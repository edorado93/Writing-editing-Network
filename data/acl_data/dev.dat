combination of recurrent neural networks and factored language models for code-switching language modeling ngoc thang vu
in this paper , we investigate the application of recurrent neural network language models ( rnnlm ) and factored language models ( flm ) to the task of language modeling for code-switching speech . we present a way to integrate partof-speech tags ( pos ) and language information ( lid ) into these models which leads to significant improvements in terms of perplexity . furthermore , a comparison between rnnlms and flms and a detailed analysis of perplexities on the different backoff levels are performed . finally , we show that recurrent neural networks and factored language models can be combined using linear interpolation to achieve the best performance . the final combined language model provides 37.8 % relative improvement in terms of perplexity on the seame development set and a relative improvement of 32.7 % on the evaluation set compared to the traditional n-gram language model . index terms : multilingual speech processing , code switching , language modeling , recurrent neural networks , factored language models

ucf-ws : domain word sense disambiguation using web selectors
this paper studies the application of the web selectors word sense disambiguation system on a specific domain . the system was primarily applied without any domain tuning , but the incorporation of domain predominant sense information was explored . results indicated that the system performs relatively the same with domain predominant sense information as without , scoring well above a random baseline , but still 5 percentage points below results of using the first sense .

supervised semantic parsing of robotic spatial commands
semeval-2014 task 6 aims to advance semantic parsing research by providing a high-quality annotated dataset to compare and evaluate approaches . the task focuses on contextual parsing of robotic commands , in which the additional context of spatial scenes can be used to guide a parser to control a robot arm . six teams submitted systems using both rule-based and statistical methods . the best performing ( hybrid ) system scored 92.5 % and 90.5 % for parsing with and without spatial context . however , the best performing statistical system scored 87.35 % and 60.84 % respectively , indicating that generalized understanding of commands given to a robot remains challenging , despite the fixed domain used for the task .

knowing the unseen : estimating vocabulary size over unseen samples
empirical studies on corpora involve making measurements of several quantities for the purpose of comparing corpora , creating language models or to make generalizations about specific linguistic phenomena in a language . quantities such as average word length are stable across sample sizes and hence can be reliably estimated from large enough samples . however , quantities such as vocabulary size change with sample size . thus measurements based on a given sample will need to be extrapolated to obtain their estimates over larger unseen samples . in this work , we propose a novel nonparametric estimator of vocabulary size . our main result is to show the statistical consistency of the estimator the first of its kind in the literature . finally , we compare our proposal with the state of the art estimators ( both parametric and nonparametric ) on large standard corpora ; apart from showing the favorable performance of our estimator , we also see that the classical good-turing estimator consistently underestimates the vocabulary size .

developing robust models for favourability analysis daoud clarke peter lane
locating documents carrying positive or negative favourability is an important application within media analysis . this paper presents some empirical results on the challenges facing a machine-learning approach to this kind of opinion mining . some of the challenges include : the often considerable imbalance in the distribution of positive and negative samples ; changes in the documents over time ; and effective training and quantification procedures for reporting results . this paper begins with three datasets generated by a media-analysis company , classifying documents in two ways : detecting the presence of favourability , and assessing negative vs. positive favourability . we then evaluate a machine-learning approach to automate the classification process . we explore the effect of using five different types of features , the robustness of the models when tested on data taken from a later time period , and the effect of balancing the input data by undersampling . we find varying choices for the optimum classifier , feature set and training strategy depending on the task and dataset .

sentence-level subjectivity detection using neuro-fuzzy models
in this work , we attempt to detect sentencelevel subjectivity by means of two supervised machine learning approaches : a fuzzy control system and adaptive neuro-fuzzy inference system . even though these methods are popular in pattern recognition , they have not been thoroughly investigated for subjectivity analysis . we present a novel pruned icf weighting coefficient , which improves the accuracy for subjectivity detection . our feature extraction algorithm calculates a feature vector based on the statistical occurrences of words in a corpus without any lexical knowledge . for this reason , these machine learning models can be applied to any language ; i.e. , there is no lexical , grammatical , syntactical analysis used in the classification process .

structured generative models for unsupervised named-entity clustering
we describe a generative model for clustering named entities which also models named entity internal structure , clustering related words by role . the model is entirely unsupervised ; it uses features from the named entity itself and its syntactic context , and coreference information from an unsupervised pronoun resolver . the model scores 86 % on the muc-7 named-entity dataset . to our knowledge , this is the best reported score for a fully unsupervised model , and the best score for a generative model .

kea : sentiment analysis of phrases within short texts
sentiment analysis has become an increasingly important research topic . this paper describes our approach to building a system for the sentiment analysis in twitter task of the semeval-2014 evaluation . the goal is to classify a phrase within a short piece of text as positive , negative or neutral . in the evaluation , classifiers trained on twitter data are tested on data from other domains such as sms , blogs as well as sarcasm . the results indicate that apart from sarcasm , classifiers built for sentiment analysis of phrases from tweets can be generalized to other short text domains quite effectively . however , in crossdomain experiments , sms data is found to generalize even better than twitter data .

semantic roles in grammar engineering
the aim of this paper is to discuss difficulties involved in adopting an existing system of semantic roles in a grammar engineering task . two typical repertoires of semantic roles are considered , namely , verbnet and sowas system . we report on experiments showing the low inter-annotator agreement when using such systems and suggest that , at least in case of languages with rich morphosyntax , an approximation of semantic roles derived from syntactic ( grammatical functions ) and morphosyntactic ( grammatical cases ) features of arguments may actually be beneficial for applications such as textual entailment .

ranking algorithms for namedentity extraction : boosting and the voted perceptron
this paper describes algorithms which rerank the top n hypotheses from a maximum-entropy tagger , the application being the recovery of named-entity boundaries in a corpus of web data . the first approach uses a boosting algorithm for ranking problems . the second approach uses the voted perceptron algorithm . both algorithms give comparable , significant improvements over the maximum-entropy baseline . the voted perceptron algorithm can be considerably more efficient to train , at some cost in computation on test examples .

social network extraction from texts : a thesis proposal
in my thesis , i propose to build a system that would enable extraction of social interactions from texts . to date i have defined a comprehensive set of social events and built a preliminary system that extracts social events from news articles . i plan to improve the performance of my current system by incorporating semantic information . using domain adaptation techniques , i propose to apply my system to a wide range of genres . by extracting linguistic constructs relevant to social interactions , i will be able to empirically analyze different kinds of linguistic constructs that people use to express social interactions . lastly , i will attempt to make convolution kernels more scalable and interpretable .

efficient deep processing of japanese
we present a broad coverage japanese grammar written in the hpsg formalism with mrs semantics . the grammar is created for use in real world applications , such that robustness and performance issues play an important role . it is connected to a pos tagging and word segmentation tool . this grammar is being developed in a multilingual context , requiring mrs structures that are easily comparable across languages .

building english-vietnamese named entity corpus with aligned bilingual news articles
named entity recognition aims to classify words in a document into pre-defined target entity classes . it is now considered to be fundamental for many natural language processing tasks such as information retrieval , machine translation , information extraction and question answering . this paper presents a workflow to build an english-vietnamese named entity corpus from an aligned bilingual corpus . the workflow is based on a state of the art named entity recognition tool to identify english named entities and map them into vietnamese text . the paper also presents a detailed discussion about several mapping errors and differences between english and vietnamese sentences that affect this task .

temporal discourse models for narrative structure
getting a machine to understand human narratives has been a classic challenge for nlp and ai . this paper proposes a new representation for the temporal structure of narratives . the representation is parsimonious , using temporal relations as surrogates for discourse relations . the narrative models , called temporal discourse models , are treestructured , where nodes include abstract events interpreted as pairs of time points and where the dominance relation is expressed by temporal inclusion . annotation examples and challenges are discussed , along with a report on progress to date in creating annotated corpora .

can you repeat that using word repetition to improve spoken term detection
we aim to improve spoken term detection performance by incorporating contextual information beyond traditional ngram language models . instead of taking a broad view of topic context in spoken documents , variability of word co-occurrence statistics across corpora leads us to focus instead the on phenomenon of word repetition within single documents . we show that given the detection of one instance of a term we are more likely to find additional instances of that term in the same document . we leverage this burstiness of keywords by taking the most confident keyword hypothesis in each document and interpolating with lower scoring hits . we then develop a principled approach to select interpolation weights using only the asr training data . using this re-weighting approach we demonstrate consistent improvement in the term detection performance across all five languages in the babel program .

hybrid decoding : decoding with partial hypotheses combination over multiple smt systems
in this paper , we present hybrid decoding a novel statistical machine translation ( smt ) decoding paradigm using multiple smt systems . in our work , in addition to component smt systems , system combination method is also employed in generating partial translation hypotheses throughout the decoding process , in which smaller hypotheses generated by each component decoder and hypotheses combination are used in the following decoding steps to generate larger hypotheses . experimental results on nist evaluation data sets for chinese-to-english machine translation ( mt ) task show that our method can not only achieve significant improvements over individual decoders , but also bring substantial gains compared with a state-of-the-art word-level system combination method .

using cross-entity inference to improve event extraction
event extraction is the task of detecting certain specified types of events that are mentioned in the source language data . the state-of-the-art research on the task is transductive inference ( e.g . cross-event inference ) . in this paper , we propose a new method of event extraction by well using cross-entity inference . in contrast to previous inference methods , we regard entitytype consistency as key feature to predict event mentions . we adopt this inference method to improve the traditional sentence-level event extraction system . experiments show that we can get 8.6 % gain in trigger ( event ) identification , and more than 11.8 % gain for argument ( role ) classification in ace event extraction .

splitting complex temporal questions for question answering systems
this paper presents a multi-layered question answering ( q.a . ) architecture suitable for enhancing current q.a . capabilities with the possibility of processing complex questions . that is , questions whose answer needs to be gathered from pieces of factual information scattered in different documents . specifically , we have designed a layer oriented to process the different types of temporal questions . complex temporal questions are first decomposed into simpler ones , according to the temporal relationships expressed in the original question . in the same way , the answers of each simple question are re-composed , fulfilling the temporal restrictions of the original complex question . using this architecture , a temporal q.a . system has been developed . in this paper , we focus on explaining the first part of the process : the decomposition of the complex questions .

discourse indicators for content selection in summarization
we present analyses aimed at eliciting which specific aspects of discourse provide the strongest indication for text importance . in the context of content selection for single document summarization of news , we examine the benefits of both the graph structure of text provided by discourse relations and the semantic sense of these relations . we find that structure information is the most robust indicator of importance . semantic sense only provides constraints on content selection but is not indicative of important content by itself . however , sense features complement structure information and lead to improved performance . further , both types of discourse information prove complementary to non-discourse features . while our results establish the usefulness of discourse features , we also find that lexical overlap provides a simple and cheap alternative to discourse for computing text structure with comparable performance for the task of content selection .

bootstrapping entity translation on weakly comparable corpora pohang , republic of korea
this paper studies the problem of mining named entity translations from comparable corpora with some asymmetry . unlike the previous approaches relying on the symmetry found in parallel corpora , the proposed method is tolerant to asymmetry often found in comparable corpora , by distinguishing different semantics of relations of entity pairs to selectively propagate seed entity translations on weakly comparable corpora . our experimental results on english-chinese corpora show that our selective propagation approach outperforms the previous approaches in named entity translation in terms of the mean reciprocal rank by up to 0.16 for organization names , and 0.14 in a low comparability case .

docent : a document-level decoder for phrase-based statistical machine translation
we describe docent , an open-source decoder for statistical machine translation that breaks with the usual sentence-bysentence paradigm and translates complete documents as units . by taking translation to the document level , our decoder can handle feature models with arbitrary discourse-wide dependencies and constitutes an essential infrastructure component in the quest for discourse-aware smt models .

a novel word segmentation approach for written languages with word boundary markers and hae-chang rim
most nlp applications work under the assumption that a user input is error-free ; thus , word segmentation ( ws ) for written languages that use word boundary markers ( wbms ) , such as spaces , has been regarded as a trivial issue . however , noisy real-world texts , such as blogs , e-mails , and sms , may contain spacing errors that require correction before further processing may take place . for the korean language , many researchers have adopted a traditional ws approach , which eliminates all spaces in the user input and re-inserts proper word boundaries . unfortunately , such an approach often exacerbates the word spacing quality for user input , which has few or no spacing errors ; such is the case , because a perfect ws model does not exist . in this paper , we propose a novel ws method that takes into consideration the initial word spacing information of the user input . our method generates a better output than the original user input , even if the user input has few spacing errors . moreover , the proposed method significantly outperforms a state-of-the-art korean ws model when the user input initially contains less than 10 % spacing errors , and performs comparably for cases containing more spacing errors . we believe that the proposed method will be a very practical pre-processing module .

unsupervised large-vocabulary word sense disambiguation
this paper introduces a graph-based algorithm for sequence data labeling , using random walks on graphs encoding label dependencies . the algorithm is illustrated and tested in the context of an unsupervised word sense disambiguation problem , and shown to significantly outperform the accuracy achieved through individual label assignment , as measured on standard senseannotated data sets .

defining a core body of knowledge for the introductory computational linguistics curriculum
discourse in and about computational linguistics depends on a shared body of knowledge . however , little content is shared across the introductory courses in this field . instead , they typically cover a diverse assortment of topics tailored to the capabilities of the students and the interests of the instructor . if the core body of knowledge could be agreed and incorporated into introductory courses several benefits would ensue , such as the proliferation of instructional materials , software support , and extension modules building on a common foundation . this paper argues that it is worthwhile to articulate a core body of knowledge , and proposes a starting point based on the acm computer science curriculum . a variety of issues specific to the multidisciplinary nature of computational linguistics are explored .

towards deeper understanding and personalisation in call
we consider in depth the semantic analysis in learning systems as well as some information retrieval techniques applied for measuring the document similarity in elearning . these results are obtained in a call project , which ended by extensive user evaluation . after several years spent in the development of call modules and prototypes , we think that much closer cooperation with real teaching experts is necessary , to find the proper learning niches and suitable wrappings of the language technologies , which could give birth to useful elearning solutions .

tightly packed tries : how to fit large models into memory , and make them load fast , too eric joanis samuel larkin
we present tightly packed tries ( tpts ) , a compact implementation of read-only , compressed trie structures with fast on-demand paging and short load times . we demonstrate the benefits of tpts for storing n-gram back-off language models and phrase tables for statistical machine translation . encoded as tpts , these databases require less space than flat text file representations of the same data compressed with the gzip utility . at the same time , they can be mapped into memory quickly and be searched directly in time linear in the length of the key , without the need to decompress the entire file . the overhead for local decompression during search is marginal .

a taxonomy , dataset , and classifier for automatic noun compound
the automatic interpretation of noun-noun compounds is an important subproblem within many natural language processing applications and is an area of increasing interest . the problem is difficult , with disagreement regarding the number and nature of the relations , low inter-annotator agreement , and limited annotated data . in this paper , we present a novel taxonomy of relations that integrates previous relations , the largest publicly-available annotated dataset , and a supervised classification method for automatic noun compound interpretation .

extracting aspects of determiner meaning from dialogue in a virtual world environment
we use data from a virtual world game for automated learning of words and grammatical constructions and their meanings . the language data are an integral part of the social interaction in the game and consist of chat dialogue , which is only constrained by the cultural context , as set by the nature of the provided virtual environment . building on previous work , where we extracted a vocabulary for concrete objects in the game by making use of the non-linguistic context , we now target np/dp grammar , in particular determiners . we assume that we have captured the meanings of a set of determiners if we can predict which determiner will be used in a particular context . to this end we train a classifier that predicts the choice of a determiner on the basis of features from the linguistic and non-linguistic context .

challenges in automating maze detection
salt is a widely used annotation approach for analyzing natural language transcripts of children . nine annotated corpora are distributed along with scoring software to provide norming data . we explore automatic identification of mazes salts version of disfluency annotations and find that cross-corpus generalization is very poor . this surprising lack of crosscorpus generalization suggests substantial differences between the corpora . this is the first paper to investigate the salt corpora from the lens of natural language processing , and to compare the utility of different corpora collected in a clinical setting to train an automatic annotation system .

mixture model pomdps for efficient handling of uncertainty in dialogue management
in spoken dialogue systems , partially observable markov decision processes ( pomdps ) provide a formal framework for making dialogue management decisions under uncertainty , but efficiency and interpretability considerations mean that most current statistical dialogue managers are only mdps . these mdp systems encode uncertainty explicitly in a single state representation . we formalise such mdp states in terms of distributions over pomdp states , and propose a new dialogue system architecture ( mixture model pomdps ) which uses mixtures of these distributions to efficiently represent uncertainty . we also provide initial evaluation results ( with real users ) for this architecture .

the human language project : building a universal corpus of the worlds languages
we present a grand challenge to build a corpus that will include all of the worlds languages , in a consistent structure that permits large-scale cross-linguistic processing , enabling the study of universal linguistics . the focal data types , bilingual texts and lexicons , relate each language to one of a set of reference languages . we propose that the ability to train systems to translate into and out of a given language be the yardstick for determining when we have successfully captured a language . we call on the computational linguistics community to begin work on this universal corpus , pursuing the many strands of activity described here , as their contribution to the global effort to document the worlds linguistic heritage before more languages fall silent .

towards event extraction from full texts on infectious diseases
event extraction approaches based on expressive structured representations of extracted information have been a significant focus of research in recent biomedical natural language processing studies . however , event extraction efforts have so far been limited to publication abstracts , with most studies further considering only the specific transcription factor-related subdomain of molecular biology of the genia corpus . to establish the broader relevance of the event extraction approach and proposed methods , it is necessary to expand on these constraints . in this study , we propose an adaptation of the event extraction approach to a subdomain related to infectious diseases and present analysis and initial experiments on the feasibility of event extraction from domain full text publications .

driving semantic parsing from the worlds response
current approaches to semantic parsing , the task of converting text to a formal meaning representation , rely on annotated training data mapping sentences to logical forms . providing this supervision is a major bottleneck in scaling semantic parsers . this paper presents a new learning paradigm aimed at alleviating the supervision burden . we develop two novel learning algorithms capable of predicting complex structures which only rely on a binary feedback signal based on the context of an external world . in addition we reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns , thus allowing our parser to scale better using less supervision . our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers .

composition of conditional random fields for transfer learning
many learning tasks have subtasks for which much training data exists . therefore , we want to transfer learning from the old , generalpurpose subtask to a more specific new task , for which there is often less data . while work in transfer learning often considers how the old task should affect learning on the new task , in this paper we show that it helps to take into account how the new task affects the old . specifically , we perform joint decoding of separately-trained sequence models , preserving uncertainty between the tasks and allowing information from the new task to affect predictions on the old task . on two standard text data sets , we show that joint decoding outperforms cascaded decoding .

exploring individual differences in student writing with a narrative composition support environment and james lester
novice writers face significant challenges as they learn to master the broad range of skills that contribute to composition . novice and expert writers differ considerably , and devising effective composition support tools for novice writers requires a clear understanding of the process and products of writing . this paper reports on a study conducted with more than one hundred middle grade students interacting with a narrative composition support environment . the texts are found to pose important challenges for state-of-the-art natural language processing techniques . furthermore , the study investigates the language usage of middle grade students , the cohesion and coherence of the resulting texts , and the relationship between students language arts skills and their writing processes . the findings suggest that composition support environments require robust nlp tools that can account for the variations in students writing in order to effectively support each phase of the writing process .

feature-rich discriminative phrase rescoring for smt fei huang and bing xiang
this paper proposes a new approach to phrase rescoring for statistical machine translation ( smt ) . a set of novel features capturing the translingual equivalence between a source and a target phrase pair are introduced . these features are combined with linear regression model and neural network to predict the quality score of the phrase translation pair . these phrase scores are used to discriminatively rescore the baseline mt systems phrase library : boost good phrase translations while prune bad ones . this approach not only significantly improves machine translation quality , but also reduces the model size by a considerable margin .

efficient solving and exploration of scope ambiguities
we present the currently most efficient solver for scope underspecification ; it also converts between different underspecification formalisms and counts readings . our tool makes the practical use of large-scale grammars with ( underspecified ) semantic output more feasible , and can be used in grammar debugging .

efficient left-to-right hierarchical phrase-based translation with
left-to-right ( lr ) decoding ( watanabe et al , 2006b ) is a promising decoding algorithm for hierarchical phrase-based translation ( hiero ) . it generates the target sentence by extending the hypotheses only on the right edge . lr decoding has complexity o ( n2b ) for input of n words and beam size b , compared too ( n3 ) for the cky algorithm . it requires a single language model ( lm ) history for each target hypothesis rather than two lm histories per hypothesis as in cky . in this paper we present an augmented lr decoding algorithm that builds on the original algorithm in ( watanabe et al , 2006b ) . unlike that algorithm , using experiments over multiple language pairs we show two new results : our lr decoding algorithm provides demonstrably more efficient decoding than cky hiero , four times faster ; and by introducing new distortion and reordering features for lr decoding , it maintains the same translation quality ( as in bleu scores ) obtained phrase-based and cky hiero with the same translation model .

altn : word alignment features for cross-lingual textual entailment
we present a supervised learning approach to cross-lingual textual entailment that explores statistical word alignment models to predict entailment relations between sentences written in different languages . our approach is language independent , and was used to participate in the clte task ( task # 8 ) organized within semeval 2013 ( negri et al , 2013 ) . the four runs submitted , one for each language combination covered by the test data ( i.e . spanish/english , german/english , french/english and italian/english ) , achieved encouraging results . in terms of accuracy , performance ranges from 38.8 % ( for german/english ) to 43.2 % ( for italian/english ) . on the italian/english and spanish/english test sets our systems ranked second among five participants , close to the top results ( respectively 43.4 % and 45.4 % ) .

using semantic unification to generate regular expressions from natural language
we consider the problem of translating natural language text queries into regular expressions which represent their meaning . the mismatch in the level of abstraction between the natural language representation and the regular expression representation make this a novel and challenging problem . however , a given regular expression can be written in many semantically equivalent forms , and we exploit this flexibility to facilitate translation by finding a form which more directly corresponds to the natural language . we evaluate our technique on a set of natural language queries and their associated regular expressions which we gathered from amazon mechanical turk . our model substantially outperforms a stateof-the-art semantic parsing baseline , yielding a 29 % absolute improvement in accuracy.1

language of vandalism : improving wikipedia vandalism detection via stylometric analysis
community-based knowledge forums , such as wikipedia , are susceptible to vandalism , i.e. , ill-intentioned contributions that are detrimental to the quality of collective intelligence . most previous work to date relies on shallow lexico-syntactic patterns and metadata to automatically detect vandalism in wikipedia . in this paper , we explore more linguistically motivated approaches to vandalism detection . in particular , we hypothesize that textual vandalism constitutes a unique genre where a group of people share a similar linguistic behavior . experimental results suggest that ( 1 ) statistical models give evidence to unique language styles in vandalism , and that ( 2 ) deep syntactic patterns based on probabilistic context free grammars ( pcfg ) discriminate vandalism more effectively than shallow lexicosyntactic patterns based on n-grams .

learning distributions over logical forms for referring expression generation nicholas fitzgerald yoav artzi luke zettlemoyer computer science & engineering
we present a new approach to referring expression generation , casting it as a density estimation problem where the goal is to learn distributions over logical expressions identifying sets of objects in the world . despite an extremely large space of possible expressions , we demonstrate effective learning of a globally normalized log-linear distribution . this learning is enabled by a new , multi-stage approximate inference technique that uses a pruning model to construct only the most likely logical forms . we train and evaluate the approach on a new corpus of references to sets of visual objects . experiments show the approach is able to learn accurate models , which generate over 87 % of the expressions people used . additionally , on the previously studied special case of single object reference , we show a 35 % relative error reduction over previous state of the art .

identification of event mentions and their semantic class
complex tasks like question answering need to be able to identify events in text and the relations among those events . we show that this event identification task and a related task , identifying the semantic class of these events , can both be formulated as classification problems in a word-chunking paradigm . we introduce a variety of linguistically motivated features for this task and then train a system that is able to identify events with a precision of 82 % and a recall of 71 % . we then show a variety of analyses of this model , and their implications for the event identification task .

metaphor detection with cross-lingual model transfer yulia tsvetkov leonid boytsov anatole gershman eric nyberg chris dyer
we show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction . our model is constructed using english resources , and we obtain state-of-the-art performance relative to previous work in this language . using a model transfer approach by pivoting through a bilingual dictionary , we show our model can identify metaphoric expressions in other languages . we provide results on three new test sets in spanish , farsi , and russian . the results support the hypothesis that metaphors are conceptual , rather than lexical , in nature .

using county demographics to infer attributes of twitter users
social media are increasingly being used to complement traditional survey methods in health , politics , and marketing . however , little has been done to adjust for the sampling bias inherent in this approach . inferring demographic attributes of social media users is thus a critical step to improving the validity of such studies . while there have been a number of supervised machine learning approaches to this problem , these rely on a training set of users annotated with attributes , which can be difficult to obtain . we instead propose training a demographic attribute classifiers that uses county-level supervision . by pairing geolocated social media with county demographics , we build a regression model mapping text to demographics . we then adopt this model to make predictions at the user level . our experiments using twitter data show that this approach is surprisingly competitive with a fully supervised approach , estimating the race of a user with 80 % accuracy .

a language independent approach for name categorization and y sistemas informaticos y sistemas informaticos y sistemas informaticos
we present a language independent approach for fine-grained categorization and discrimination of names on the basis of text semantic similarity information . the experiments are conducted for languages from the romance ( spanish ) and slavonic ( bulgarian ) language groups . despite the fact that these languages have specific characteristics as word-order and grammar , the obtained results are encouraging and show that our name entity method is scalable not only to different categories , but also to different languages . in an exhaustive experimental evaluation , we have demonstrated that our approach yields better results compared to a baseline system .

combination of arabic preprocessing schemes for statistical machine translation
statistical machine translation is quite robust when it comes to the choice of input representation . it only requires consistency between training and testing . as a result , there is a wide range of possible preprocessing choices for data used in statistical machine translation . this is even more so for morphologically rich languages such as arabic . in this paper , we study the effect of different word-level preprocessing schemes for arabic on the quality of phrase-based statistical machine translation . we also present and evaluate different methods for combining preprocessing schemes resulting in improved translation quality .

a large scale ranker-based system for search query spelling correction
this paper makes three significant extensions to a noisy channel speller designed for standard written text to target the challenging domain of search queries . first , the noisy channel model is subsumed by a more general ranker , which allows a variety of features to be easily incorporated . second , a distributed infrastructure is proposed for training and applying web scale n-gram language models . third , a new phrase-based error model is presented . this model places a probability distribution over transformations between multi-word phrases , and is estimated using large amounts of query-correction pairs derived from search logs . experiments show that each of these extensions leads to significant improvements over the stateof-the-art baseline methods .

phylogenetic grammar induction
we present an approach to multilingual grammar induction that exploits a phylogeny-structured model of parameter drift . our method does not require any translated texts or token-level alignments . instead , the phylogenetic prior couples languages at a parameter level . joint induction in the multilingual model substantially outperforms independent learning , with larger gains both from more articulated phylogenies and as well as from increasing numbers of languages . across eight languages , the multilingual approach gives error reductions over the standard monolingual dmv averaging 21.1 % and reaching as high as 39 % .

normalization and paraphrasing using symbolic methods
we describe an ongoing work in information extraction which is seen as a text normalization task . the normalized representation can be used to detect paraphrases in texts . normalization and paraphrase detection tasks are built on top of a robust analyzer for english and are exclusively achieved using symbolic methods . both grammar development rules and information extraction rules are expressed within the same formalism and are developed in an integrated way . the experiment we describe in the paper is evaluated and presents encouraging results .

learning n-best correction models from implicit user feedback in a multi-modal local search application
we describe a novel n-best correction model that can leverage implicit user feedback ( in the form of clicks ) to improve performance in a multi-modal speech-search application . the proposed model works in two stages . first , the n-best list generated by the speech recognizer is expanded with additional candidates , based on confusability information captured via user click statistics . in the second stage , this expanded list is rescored and pruned to produce a more accurate and compact n-best list . results indicate that the proposed n-best correction model leads to significant improvements over the existing baseline , as well as other traditional n-best rescoring approaches .

distributional phrase structure induction
unsupervised grammar induction systems commonly judge potential constituents on the basis of their effects on the likelihood of the data . linguistic justifications of constituency , on the other hand , rely on notions such as substitutability and varying external contexts . we describe two systems for distributional grammar induction which operate on such principles , using part-of-speech tags as the contextual features . the advantages and disadvantages of these systems are examined , including precision/recall trade-offs , error analysis , and extensibility .

a novel dependency-to-string model for statistical machine translation
dependency structure , as a first step towards semantics , is believed to be helpful to improve translation quality . however , previous works on dependency structure based models typically resort to insertion operations to complete translations , which make it difficult to specify ordering information in translation rules . in our model of this paper , we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings . the head-dependents rules require only substitution operation , thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations . large-scale experiments show that our model performs well on long distance reordering , and outperforms the stateof-the-art constituency-to-string model ( +1.47 bleu on average ) and hierarchical phrasebased model ( +0.46 bleu on average ) on two chinese-english nist test sets without resort to phrases or parse forest . for the first time , a source dependency structure based model catches up with and surpasses the state-of-theart translation models .

computational properties of environment-based disambiguation
the standard pipeline approach to semantic processing , in which sentences are morphologically and syntactically resolved to a single tree before they are interpreted , is a poor fit for applications such as natural language interfaces . this is because the environment information , in the form of the objects and events in the applications runtime environment , can not be used to inform parsing decisions unless the input sentence is semantically analyzed , but this does not occur until after parsing in the single-tree semantic architecture . this paper describes the computational properties of an alternative architecture , in which semantic analysis is performed on all possible interpretations during parsing , in polynomial time .

adaptive string similarity metrics for biomedical reference resolution the mitre corporation and james pustejovsky
in this paper we present the evaluation of a set of string similarity metrics used to resolve the mapping from strings to concepts in the umls metathesaurus . string similarity is conceived as a single component in a full reference resolution system that would resolve such a mapping . given this qualification , we obtain positive results achieving 73.6 f-measure ( 76.1 precision and 71.4 recall ) for the task of assigning the correct umls concept to a given string . our results demonstrate that adaptive string similarity methods based on conditional random fields outperform standard metrics in this domain .

chiners : a chinese named entity recognition system for the sports domain tianfang yao wei ding gregor erbach
in the investigation for chinese named entity ( ne ) recognition , we are confronted with two principal challenges . one is how to ensure the quality of word segmentation and part-of-speech ( pos ) tagging , because its consequence has an adverse impact on the performance of ne recognition . another is how to flexibly , reliably and accurately recognize nes . in order to cope with the challenges , we propose a system architecture which is divided into two phases . in the first phase , we should reduce word segmentation and pos tagging errors leading to the second phase as much as possible . for this purpose , we utilize machine learning techniques to repair such errors . in the second phase , we design finite state cascades ( fsc ) which can be automatically constructed depending on the recognition rule sets as a shallow parser for the recognition of nes . the advantages of that are reliable , accurate and easy to do maintenance for fsc . additionally , to recognize special nes , we work out the corresponding strategies to enhance the correctness of the recognition . the experimental evaluation of the system has shown that the total average recall and precision for six types of nes are 83 % and 85 % respectively .

a pac-bayesian approach to minimum perplexity language modeling
despite the overwhelming use of statistical language models in speech recognition , machine translation , and several other domains , few high probability guarantees exist on their generalization error . in this paper , we bound the test set perplexity of two popular language models the n-gram model and class-based n-grams using pac-bayesian theorems for unsupervised learning . we extend the bound to sequence clustering , wherein classes represent longer context such as phrases . the new bound is dominated by the maximum number of sequences represented by each cluster , which is polynomial in the vocabulary size . we show that we can still encourage small sample generalization by sparsifying the cluster assignment probabilities . we incorporate our bound into an efficient hmm-based sequence clustering algorithm and validate the theory with empirical results on the resource management corpus .

support vector machines for paraphrase identification and corpus construction
the lack of readily-available large corpora of aligned monolingual sentence pairs is a major obstacle to the development of statistical machine translation-based paraphrase models . in this paper , we describe the use of annotated datasets and support vector machines to induce larger monolingual paraphrase corpora from a comparable corpus of news clusters found on the world wide web . features include : morphological variants ; wordnet synonyms and hypernyms ; loglikelihood-based word pairings dynamically obtained from baseline sentence alignments ; and formal string features such as word-based edit distance . use of this technique dramatically reduces the alignment error rate of the extracted corpora over heuristic methods based on position of the sentences in the text .

sentiment classification on polarity reviews : an empirical study using rating-based features dai quoc nguyen and dat quoc nguyen and thanh vu son bao pham
we present a new feature type named rating-based feature and evaluate the contribution of this feature to the task of document-level sentiment analysis . we achieve state-of-the-art results on two publicly available standard polarity movie datasets : on the dataset consisting of 2000 reviews produced by pang and lee ( 2004 ) we obtain an accuracy of 91.6 % while it is 89.87 % evaluated on the dataset of 50000 reviews created by maas et al . ( 2011 ) . we also get a performance at 93.24 % on our own dataset consisting of 233600 movie reviews , and we aim to share this dataset for further research in sentiment polarity analysis task .

corpus expansion for statistical machine translation with
we present an approach of expanding parallel corpora for machine translation . by utilizing semantic role labeling ( srl ) on one side of the language pair , we extract srl substitution rules from existing parallel corpus . the rules are then used for generating new sentence pairs . an svm classifier is built to filter the generated sentence pairs . the filtered corpus is used for training phrase-based translation models , which can be used directly in translation tasks or combined with baseline models . experimental results on chineseenglish machine translation tasks show an average improvement of 0.45 bleu and 1.22 ter points across 5 different nist test sets .

postnominal prepositional phrase attachment in proteomics
we present a small set of attachment heuristics for postnominal pps occurring in full-text articles related to enzymes . a detailed analysis of the results suggests their utility for extraction of relations expressed by nominalizations ( often with several attached pps ) . the system achieves 82 % accuracy on a manually annotated test corpus of over 3000 pps from varied biomedical texts .

homophones and tonal patterns in english-chinese transliteration
the abundance of homophones in chinese significantly increases the number of similarly acceptable candidates in english-to-chinese transliteration ( e2c ) . the dialectal factor also leads to different transliteration practice . we compare e2c between mandarin chinese and cantonese , and report work in progress for dealing with homophones and tonal patterns despite potential skewed distributions of individual chinese characters in the training data .

learning phrasal categories
in this work we learn clusters of contextual annotations for non-terminals in the penn treebank . perhaps the best way to think about this problem is to contrast our work with that of klein and manning ( 2003 ) . that research used treetransformations to create various grammars with different contextual annotations on the non-terminals . these grammars were then used in conjunction with a cky parser . the authors explored the space of different annotation combinations by hand . here we try to automate the process to learn the right combination automatically . our results are not quite as good as those carefully created by hand , but they are close ( 84.8 vs 85.7 ) .

livetree : an integrated workbench for discourse processing
in this paper , we introduce livetree , a core component of lidas , the linguistic discourse analysis system for automatic discourse parsing with the unified linguistic discourse model ( u-ldm ) ( x et al 2004 ) . livetree is an integrated workbench for supervised and unsupervised creation , storage and manipulation of the discourse structure of text documents under the u-ldm . the livetree environment provides tools for manual and automatic u-ldm segmentation and discourse parsing . document management , grammar testing , manipulation of discourse structures and creation and editing of discourse relations are also supported .

learning event durations from event descriptions
we have constructed a corpus of news articles in which events are annotated for estimated bounds on their duration . here we describe a method for measuring inter-annotator agreement for these event duration distributions . we then show that machine learning techniques applied to this data yield coarse-grained event duration information , considerably outperforming a baseline and approaching human performance .

a confidence-based framework for disambiguating geographic terms
we describe a purely confidence-based geographic term disambiguation system that crucially relies on the notion of positive and negative context and methods for combining confidence-based disambiguation with measures of relevance to a users query .

unn-weps : web person search using co-present names and lexical chains newcastle upon tyne newcastle upon tyne
we describe a system , unn-weps for identifying individuals from web pages using data from semeval task 13. our system is based on using co-presence of person names to form seed clusters . these are then extended with pages that are deemed conceptually similar based on a lexical chaining analysis computed using rogets thesaurus . finally , a single link hierarchical agglomerative clustering algorithm merges the enhanced clusters for individual entity recognition . unn-weps achieved an average purity of 0.6 , and inverse purity of 0.73 .

bilingual-lsa based lm adaptation for spoken language translation
we propose a novel approach to crosslingual language model ( lm ) adaptation based on bilingual latent semantic analysis ( blsa ) . a blsa model is introduced which enables latent topic distributions to be efficiently transferred across languages by enforcing a one-to-one topic correspondence during training . using the proposed blsa framework crosslingual lm adaptation can be performed by , first , inferring the topic posterior distribution of the source text and then applying the inferred distribution to the target language n-gram lm via marginal adaptation . the proposed framework also enables rapid bootstrapping of lsa models for new languages based on a source lsa model from another language . on chinese to english speech and text translation the proposed blsa framework successfully reduced word perplexity of the english lm by over 27 % for a unigram lm and up to 13.6 % for a 4-gram lm . furthermore , the proposed approach consistently improved machine translation quality on both speech and text based adaptation .

improving statistical machine translation in the medical domain using the unified medical language system
texts from the medical domain are an important task for natural language processing . this paper investigates the usefulness of a large medical database ( the unified medical language system ) for the translation of dialogues between doctors and patients using a statistical machine translation system . we are able to show that the extraction of a large dictionary and the usage of semantic type information to generalize the training data significantly improves the translation performance .

speech to speech translation for nurse patient interaction
s-minds is a speech translation system , which allows an english speaker to communicate with a limited english proficiency speaker easily within a question-and-answer , interview-style format . it can handle dialogs in specific settings such as nurse-patient interaction , or medical triage . we have built and tested an english-spanish system for enabling nurse-patient interaction in a number of domains in kaiser permanente achieving a total translation accuracy of 92.8 % ( for both english and spanish ) . we will give an overview of the system as well as the quantitative and qualitatively system performance .

recognizing sublanguages in scientific journal articles through closure properties computational bioscience program
it has long been realized that sublanguages are relevant to natural language processing and text mining . however , practical methods for recognizing or characterizing them have been lacking . this paper describes a publicly available set of tools for sublanguage recognition . closure properties are used to assess the goodness of fit of two biomedical corpora to the sublanguage model . scientific journal articles are compared to general english text , and it is shown that the journal articles fit the sublanguage model , while the general english text does not . a number of examples of implications of the sublanguage characteristics for natural language processing are pointed out . the software is made publicly available at [ edited for anonymization ] .

multi-document summarisation using generic relation extraction capital markets crc limited
experiments are reported that investigate the effect of various source document representations on the accuracy of the sentence extraction phase of a multidocument summarisation task . a novel representation is introduced based on generic relation extraction ( gre ) , which aims to build systems for relation identification and characterisation that can be transferred across domains and tasks without modification of model parameters . results demonstrate performance that is significantly higher than a non-trivial baseline that uses tf*idf -weighted words and at least as good as a comparable but less general approach from the literature . analysis shows that the representations compared are complementary , suggesting that extraction performance could be further improved through system combination .

estimating probability of correctness for asr n-best lists
for a spoken dialog system to make good use of a speech recognition n-best list , it is essential to know how much trust to place in each entry . this paper presents a method for assigning a probability of correctness to each of the items on the n-best list , and to the hypothesis that the correct answer is not on the list . we find that both multinomial logistic regression and support vector machine models yields meaningful , useful probabilities across different tasks and operating conditions .

combining pomdps trained with user simulations and rule-based dialogue management in a spoken dialogue system
over several years , we have developed an approach to spoken dialogue systems that includes rule-based and trainable dialogue managers , spoken language understanding and generation modules , and a comprehensive dialogue system architecture . we present a reinforcement learning-based dialogue system that goes beyond standard rule-based models and computes on-line decisions of the best dialogue moves . the key concept of this work is that we bridge the gap between manually written dialog models ( e.g . rule-based ) and adaptive computational models such as partially observable markov decision processes ( pomdp ) based dialogue managers .

syntactic decision tree lms : random selection or intelligent design human language technology
decision trees have been applied to a variety of nlp tasks , including language modeling , for their ability to handle a variety of attributes and sparse context space . moreover , forests ( collections of decision trees ) have been shown to substantially outperform individual decision trees . in this work , we investigate methods for combining trees in a forest , as well as methods for diversifying trees for the task of syntactic language modeling . we show that our tree interpolation technique outperforms the standard method used in the literature , and that , on this particular task , restricting tree contexts in a principled way produces smaller and better forests , with the best achieving an 8 % relative reduction in word error rate over an n-gram baseline .

substring-based transliteration with conditional random fields
motivated by phrase-based translation research , we present a transliteration system where characters are grouped into substrings to be mapped atomically into the target language . we show how this substring representation can be incorporated into a conditional random field model that uses local context and phonemic information .

multiword units in an mt lexicon
multiword units significantly contribute to the robustness of mt systems as they reduce the inevitable ambiguity inherent in word to word matching . the paper focuses on a relatively little studied kind of mw units which are partially fixed and partially productive . in fact , mw units will be shown to form a continuum between completely frozen expression where the lexical elements are specified at the level of particular word forms and those which are produced by syntactic rules defined in terms of general part of speech categories . the paper will argue for the use of local grammars proposed by maurice gross to capture the productive regularity of mw units and will illustrate a uniform implementation of them in the nooj grammar development framework .

latent semantic tensor indexing for community-based question answering
retrieving similar questions is very important in community-based question answering ( cqa ) . in this paper , we propose a unified question retrieval model based on latent semantic indexing with tensor analysis , which can capture word associations among different parts of cqa triples simultaneously . thus , our method can reduce lexical chasm of question retrieval with the help of the information of question content and answer parts . the experimental result shows that our method outperforms the traditional methods .

enriching morphologically poor languages for statistical machine translation
we address the problem of translating from morphologically poor to morphologically rich languages by adding per-word linguistic information to the source language . we use the syntax of the source sentence to extract information for noun cases and verb persons and annotate the corresponding words accordingly . in experiments , we show improved performance for translating from english into greek and czech . for englishgreek , we reduce the error on the verb conjugation from 19 % to 5.4 % and noun case agreement from 9 % to 6 % .

bilingual lexicon extraction from comparable corpora using
many existing methods for bilingual lexicon learning from comparable corpora are based on similarity of context vectors . these methods suffer from noisy vectors that greatly affect their accuracy . we introduce a method for filtering this noise allowing highly accurate learning of bilingual lexicons . our method is based on the notion of in-domain terms which can be thought of as the most important contextually relevant words . we provide a method for identifying such terms . our evaluation shows that the proposed method can learn highly accurate bilingual lexicons without using orthographic features or a large initial seed dictionary . in addition , we also introduce a method for measuring the similarity between two words in different languages without requiring any initial dictionary .

increasing the coverage of a domain independent dialogue lexicon with
this paper investigates how to extend coverage of a domain independent lexicon tailored for natural language understanding . we introduce two algorithms for adding lexical entries from verbnet to the lexicon of the trips spoken dialogue system . we report results on the efficiency of the method , discussing in particular precision versus coverage issues and implications for mapping to other lexical databases .

two-stage stochastic natural language generation for email synthesisby modeling sender style and topic structure
this paper describes a two-stage process for stochastic generation of email , in which the first stage structures the emails according to sender style and topic structure ( high-level generation ) , and the second stage synthesizes text content based on the particulars of an email element and the goals of a given communication ( surface-level realization ) . synthesized emails were rated in a preliminary experiment . the results indicate that sender style can be detected . in addition we found that stochastic generation performs better if applied at the word level than at an original-sentence level ( template-based ) in terms of email coherence , sentence fluency , naturalness , and preference .

automatic fine - grained semantic classification for domain adaptation
assigning arguments of verbs to different semantic classes ( semantic typing ) , or alternatively , checking the selectional restrictions of predicates , is a fundamental component of many natural language processing tasks . however , a common experience has been that general purpose semantic classes , such as those encoded in resources like wordnet , or handcrafted subject-specific ontologies , are seldom quite right when it comes to analysing texts from a particular domain . in this paper we describe a method of automatically deriving fine-grained , domain-specific semantic classes of arguments while simultaneously clustering verbs into semantically meaningful groups : the first step in verb sense induction . we show that in a small pilot study on new examples from the same domain we are able to achieve almost perfect recall and reasonably high precision in the semantic typing of verb arguments in these texts .

learning semantic features for fmri data from definitional text
( mitchell et al , 2008 ) showed that it was possible to use a text corpus to learn the value of hypothesized semantic features characterizing the meaning of a concrete noun . the authors also demonstrated that those features could be used to decompose the spatial pattern of fmri-measured brain activation in response to a stimulus containing that noun and a picture of it . in this paper we introduce a method for learning such semantic features automatically from a text corpus , without needing to hypothesize them or provide any proxies for their presence on the text . we show that those features are effective in a more demanding classification task than that in ( mitchell et al , 2008 ) and describe their qualitative relationship to the features proposed in that paper .

extending the tool , or how to annotate historical language varieties
we present a general and simple method to adapt an existing nlp tool in order to enable it to deal with historical varieties of languages . this approach consists basically in expanding the dictionary with the old word variants and in retraining the tagger with a small training corpus . we implement this approach for old spanish . the results of a thorough evaluation over the extended tool show that using this method an almost state-of-the-art performance is obtained , adequate to carry out quantitative studies in the humanities : 94.5 % accuracy for the main part of speech and 92.6 % for lemma . to our knowledge , this is the first time that such a strategy is adopted to annotate historical language varieties and we believe that it could be used as well to deal with other non-standard varieties of languages .

rover : improving system combination with classification
we present an improved system combination technique , rover . our approach obtains significant improvements over rover , and is consistently better across varying numbers of component systems . a classifier is trained on features from the system lattices , and selects the final word hypothesis by learning cues to choose the system that is most likely to be correct at each word location . this approach achieves the best result published to date on the tc-star 2006 english speech recognition evaluation set .

the syntactically annotated ice corpus and the automatic induction of a formal grammar
the international corpus of english is a corpus of national and regional varieties of english . the mega-word british component has been constructed , grammatically tagged , and syntactically parsed . this article is a description of work that aims at the automatic induction of a wide-coverage grammar from this corpus as well as an empirical evaluation of the grammar . it first of all describes the corpus and its annotation schemes and then presents empirical statistics for the grammar . i will then evaluate the coverage and the accuracy of such a grammar when applied automatically in a parsing system . results show that the grammar enabled the parser to achieve 86.1 % recall rate and 83.5 % precision rate .

a vague sense classifier for detecting vague definitions in ontologies
vagueness is a common human knowledge and linguistic phenomenon , typically manifested by predicates that lack clear applicability conditions and boundaries such as high , expert or bad . in the context of ontologies and semantic data , the usage of such predicates within ontology element definitions ( classes , relations etc . ) can hamper the latters quality , primarily in terms of shareability and meaning explicitness . with that in mind , we present in this paper a vague word sense classifier that may help both ontology creators and consumers to automatically detect vague ontology definitions and , thus , assess their quality better .

speaker recognition with mixtures of gaussians with sparse regression
when estimating a mixture of gaussians there are usually two choices for the covariance type of each gaussian component . either diagonal or full covariance . imposing a structure though may be restrictive and lead to degraded performance and/or increased computations . in this work , several criteria to estimate the structure of regression matrices of a mixture of gaussians are introduced and evaluated . most of the criteria attempt to estimate a discriminative structure , which is suited for classification tasks . results are reported on the 1996 nist speaker recognition task and performance is compared with structural em , a well-known , non-discriminative , structurefinding algorithm .

learning with lookahead : can history-based models rival globally optimized models yoshimasa tsuruoka yusuke miyao junichi kazama
this paper shows that the performance of history-based models can be significantly improved by performing lookahead in the state space when making each classification decision . instead of simply using the best action output by the classifier , we determine the best action by looking into possible sequences of future actions and evaluating the final states realized by those action sequences . we present a perceptron-based parameter optimization method for this learning framework and show its convergence properties . the proposed framework is evaluated on partof-speech tagging , chunking , named entity recognition and dependency parsing , using standard data sets and features . experimental results demonstrate that history-based models with lookahead are as competitive as globally optimized models including conditional random fields ( crfs ) and structured perceptrons .

the telri tool catalogue : structure and prospects
in the scope of the telri concerted action a working group is investigating the formation of a tool catalogue and repository . the idea is similar to that of the acl natural language software registry , but the contents should be mostly limited to corpus processing tools available free of cost for research use . the catalogue should also offer a help-line for installing and using the software . the paper reports on the setup of this catalogue , and concentrates on the technical issues involved in its creation , storage and display . this involves the form interface on the web , the xml docbook encoding , and the xsl stylesheets used to present the catalogue either on the web or in print . the paper lists the current entries in the catalogue and discusses plans for their expansion and maintenance .

a limited-domain english to japanese medical speech translator built using regulus 2
we argue that verbal patient diagnosis is a promising application for limited-domain speech translation , and describe an architecture designed for this type of task which represents a compromise between principled linguistics-based processing on the one hand and efficient phrasal translation on the other . we propose to demonstrate a prototype system instantiating this architecture , which has been built on top of the open source regulus 2 platform . the prototype translates spoken yes-no questions about headache symptoms from english to japanese , using a vocabulary of about 200 words .

linear-time dependency analysis for japanese
we present a novel algorithm for japanese dependency analysis . the algorithm allows us to analyze dependency structures of a sentence in linear-time while keeping a state-of-the-art accuracy . in this paper , we show a formal description of the algorithm and discuss it theoretically with respect to time complexity . in addition , we evaluate its efficiency and performance empirically against the kyoto university corpus . the proposed algorithm with improved models for dependency yields the best accuracy in the previously published results on the kyoto university corpus .

automatic error detection in the japanese learners english spoken data
this paper describes a method of detecting grammatical and lexical errors made by japanese learners of english and other techniques that improve the accuracy of error detection with a limited amount of training data . in this paper , we demonstrate to what extent the proposed methods hold promise by conducting experiments using our learner corpus , which contains information on learners errors .

extensions to hmm-based statistical word alignment models
this paper describes improved hmm-based word level alignment models for statistical machine translation . we present a method for using part of speech tag information to improve alignment accuracy , and an approach to modeling fertility and correspondence to the empty word in an hmm alignment model . we present accuracy results from evaluating viterbi alignments against human-judged alignments on the canadian hansards corpus , as compared to a bigram hmm , and ibm model 4. the results show up to 16 % alignment error reduction .

i thou thee , thou traitor :
in contrast to many languages ( like russian or french ) , modern english does not distinguish formal and informal ( t/v ) address overtly , for example by pronoun choice . we describe an ongoing study which investigates to what degree the t/v distinction is recoverable in english text , and with what textual features it correlates . our findings are : ( a ) human raters can label english utterances as t or v fairly well , given sufficient context ; ( b ) , lexical cues can predict t/v almost at human level .

a powerful and general approach to context exploitation in natural
in natural language , the meaning of a lexeme often varies due to the specific surrounding context . computational approaches to natural language processing can benefit from a reliable , long-range-context-dependent representation of the meaning of each lexeme that appears in a given sentence . we have developed a general new technique that produces a context-dependent meaning representation for a lexeme in a specific surrounding context . the meaning of a lexeme in a specific context is represented by a list of semantically replaceable elements the members of which are other lexemes from our experimental lexicon . we have performed experiments with a lexicon composed of individual english words and also with a lexicon of individual words and selected phrases . the resulting lists can be used to compare the meaning of conceptual units ( individual words or frequentlyoccurring phrases ) in different contexts and also can serve as features for machine learning approaches to classify semantic roles and relationships .

collecting a why-question corpus for development and evaluation of an joanna mrozinski edward whittaker
question answering research has only recently started to spread from short factoid questions to more complex ones . one significant challenge is the evaluation : manual evaluation is a difficult , time-consuming process and not applicable within efficient development of systems . automatic evaluation requires a corpus of questions and answers , a definition of what is a correct answer , and a way to compare the correct answers to automatic answers produced by a system . for this purpose we present a wikipedia-based corpus of whyquestions and corresponding answers and articles . the corpus was built by a novel method : paid participants were contacted through a web-interface , a procedure which allowed dynamic , fast and inexpensive development of data collection methods . each question in the corpus has several corresponding , partly overlapping answers , which is an asset when estimating the correctness of answers . in addition , the corpus contains information related to the corpus collection process . we believe this additional information can be used to post-process the data , and to develop an automatic approval system for further data collection projects conducted in a similar manner .

assessing the impact of translation errors on machine translation quality with mixed-effects models
learning from errors is a crucial aspect of improving expertise . based on this notion , we discuss a robust statistical framework for analysing the impact of different error types on machine translation ( mt ) output quality . our approach is based on linear mixed-effects models , which allow the analysis of error-annotated mt output taking into account the variability inherent to the specific experimental setting from which the empirical observations are drawn . our experiments are carried out on different language pairs involving chinese , arabic and russian as target languages . interesting findings are reported , concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics .

syntactic features for evaluation of machine translation
automatic evaluation of machine translation , based on computing n-gram similarity between system output and human reference translations , has revolutionized the development of mt systems . we explore the use of syntactic information , including constituent labels and head-modifier dependencies , in computing similarity between output and reference . our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments .

improving grammaticality in statistical sentence generation : introducing a dependency spanning tree algorithm with an argument
abstract-like text summarisation requires a means of producing novel summary sentences . in order to improve the grammaticality of the generated sentence , we model a global ( sentence ) level syntactic structure . we couch statistical sentence generation as a spanning tree problem in order to search for the best dependency tree spanning a set of chosen words . we also introduce a new search algorithm for this task that models argument satisfaction to improve the linguistic validity of the generated tree . we treat the allocation of modifiers to heads as a weighted bipartite graph matching ( or assignment ) problem , a well studied problem in graph theory . using bleu to measure performance on a string regeneration task , we found an improvement , illustrating the benefit of the spanning tree approach armed with an argument satisfaction model .

event detection and summarization in weblogs with temporal collocations
this paper deals with the relationship between weblog content and time . with the proposed temporal mutual information , we analyze the collocations in time dimension , and the interesting collocations related to special events . the temporal mutual information is employed to observe the strength of term-to-term associations over time . an event detection algorithm identifies the collocations that may cause an event in a specific timestamp . an event summarization algorithm retrieves a set of collocations which describe an event . we compare our approach with the approach without considering the time interval . the experimental results demonstrate that the temporal collocations capture the real world semantics and real world events over time .

application of prize based on sentence length in chunk-based automatic evaluation of machine translation
as described in this paper , we propose a new automatic evaluation metric for machine translation . our metric is based on chunking between the reference and candidate translation . moreover , we apply a prize based on sentence-length to the metric , dissimilar from penalties in bleu or nist . we designate this metric as automatic evaluation of machine translation in which the prize is applied to a chunkbased metric ( apac ) . through metaevaluation experiments and comparison with several metrics , we confirmed that our metric shows stable correlation with human judgment .

incorporating source-language paraphrases into phrase-based smt with
to increase the model coverage , sourcelanguage paraphrases have been utilized to boost smt system performance . previous work showed that word lattices constructed from paraphrases are able to reduce out-ofvocabulary words and to express inputs in different ways for better translation quality . however , such a word-lattice-based method suffers from two problems : 1 ) path duplications in word lattices decrease the capacities for potential paraphrases ; 2 ) lattice decoding in smt dramatically increases the search space and results in poor time efficiency . therefore , in this paper , we adopt word confusion networks as the input structure to carry source-language paraphrase information . similar to previous work , we use word lattices to build word confusion networks for merging of duplicated paths and faster decoding . experiments are carried out on small- , medium- and large-scale english chinese translation tasks , and we show that compared with the word-lattice-based method , the decoding time on three tasks is reduced significantly ( up to 79 % ) while comparable translation quality is obtained on the largescale task .

annotation-based multimedia summarization and translation
this paper presents techniques for multimedia annotation and their application to video summarization and translation . our tool for annotation allows users to easily create annotation including voice transcripts , video scene descriptions , and visual/auditory object descriptions . the module for voice transcription is capable of multilingual spoken language identification and recognition . a video scene description consists of semi-automatically detected keyframes of each scene in a video clip and time codes of scenes . a visual object description is created by tracking and interactive naming of people and objects in video scenes . the text data in the multimedia annotation are syntactically and semantically structured using linguistic annotation . the proposed multimedia summarization works upon a multimodal document that consists of a video , keyframes of scenes , and transcripts of the scenes . the multimedia translation automatically generates several versions of multimedia content in different languages .

fast lr parsing using rich ( tree adjoining ) grammars
we describe an lr parser of parts-ofspeech ( and punctuation labels ) for tree adjoining grammars ( tags ) , that solves table conflicts in a greedy way , with limited amount of backtracking . we evaluate the parser using the penn treebank showing that the method yield very fast parsers with at least reasonable accuracy , confirming the intuition that lr parsing benefits from the use of rich grammars .

online learning for inexact hypergraph search
online learning algorithms like the perceptron are widely used for structured prediction tasks . for sequential search problems , like left-to-right tagging and parsing , beam search has been successfully combined with perceptron variants that accommodate search errors ( collins and roark , 2004 ; huang et al. , 2012 ) . however , perceptron training with inexact search is less studied for bottom-up parsing and , more generally , inference over hypergraphs . in this paper , we generalize the violation-fixing perceptron of huang et al . ( 2012 ) to hypergraphs and apply it to the cube-pruning parser of zhang and mcdonald ( 2012 ) . this results in the highest reported scores on wsj evaluation set ( uas 93.50 % and las 92.41 % respectively ) without the aid of additional resources .

unsupervised training for overlapping ambiguity resolution in chinese word segmentation
this paper proposes an unsupervised training approach to resolving overlapping ambiguities in chinese word segmentation . we present an ensemble of adapted nave bayesian classifiers that can be trained using an unlabelled chinese text corpus . these classifiers differ in that they use context words within windows of different sizes as features . the performance of our approach is evaluated on a manually annotated test set . experimental results show that the proposed approach achieves an accuracy of 94.3 % , rivaling the rule-based and supervised training methods .

improving word alignment with language model based confidence scores
this paper describes the statistical machine translation systems submitted to the acl-wmt 2008 shared translation task . systems were submitted for two translation directions : englishspanish and spanishenglish . using sentence pair confidence scores estimated with source and target language models , improvements are observed on the newscommentary test sets . genre-dependent sentence pair confidence score and integration of sentence pair confidence score into phrase table are also investigated .

what is the jeopardy model a quasi-synchronous grammar for qa
this paper presents a syntax-driven approach to question answering , specifically the answer-sentence selection problem for short-answer questions . rather than using syntactic features to augment existing statistical classifiers ( as in previous work ) , we build on the idea that questions and their ( correct ) answers relate to each other via loose but predictable syntactic transformations . we propose a probabilistic quasi-synchronous grammar , inspired by one proposed for machine translation ( d. smith and eisner , 2006 ) , and parameterized by mixtures of a robust nonlexical syntax/alignment model with a ( n optional ) lexical-semantics-driven log-linear model . our model learns soft alignments as a hidden variable in discriminative training . experimental results using the trec dataset are shown to significantly outperform strong state-of-the-art baselines .

projective dependency parsing with perceptron
we describe an online learning dependency parser for the conll-x shared task , based on the bottom-up projective algorithm of eisner ( 2000 ) . we experiment with a large feature set that models : the tokens involved in dependencies and their immediate context , the surfacetext distance between tokens , and the syntactic context dominated by each dependency . in experiments , the treatment of multilingual information was totally blind .

in-depth exploitation of noun and verb semantics
recognition of causality is important to achieve natural language discourse understanding . previous approaches rely on shallow linguistic features . in this work , we propose to identify causality in verbnoun pairs by exploiting deeper semantics of nouns and verbs . particularly , we acquire and employ three novel types of knowledge : ( 1 ) semantic classes of nouns with a high and low tendency to encode causality along with information regarding metonymies , ( 2 ) data-driven semantic classes of verbal events with the least tendency to encode causality , and ( 3 ) tendencies of verb frames to encode causality . using these knowledge sources , we achieve around 15 % improvement in fscore over a supervised classifier trained using linguistic features .

fast and accurate arc filtering for dependency parsing
we propose a series of learned arc filters to speed up graph-based dependency parsing . a cascade of filters identify implausible head-modifier pairs , with time complexity that is first linear , and then quadratic in the length of the sentence . the linear filters reliably predict , in context , words that are roots or leaves of dependency trees , and words that are likely to have heads on their left or right . we use this information to quickly prune arcs from the dependency graph . more than 78 % of total arcs are pruned while retaining 99.5 % of the true dependencies . these filters improve the speed of two state-ofthe-art dependency parsers , with low overhead and negligible loss in accuracy .

representations for category disambiguation
as it serves as a basis for pos tagging , category induction , and human category acquisition , we investigate the information needed to disambiguate a word in a local context , when using corpus categories . specifically , we increase the recall of an error detection method by abstracting the word to be disambiguated to a representation containing information about some of its inherent properties , namely the set of categories it can potentially have . this work thus provides insights into the relation of corpus categories to categories derived from local contexts .

multi-criteria-based strategy to stop active learning for data annotation
in this paper , we address the issue of deciding when to stop active learning for building a labeled training corpus . firstly , this paper presents a new stopping criterion , classification-change , which considers the potential ability of each unlabeled example on changing decision boundaries . secondly , a multi-criteriabased combination strategy is proposed to solve the problem of predefining an appropriate threshold for each confidence-based stopping criterion , such as max-confidence , min-error , and overalluncertainty . finally , we examine the effectiveness of these stopping criteria on uncertainty sampling and heterogeneous uncertainty sampling for active learning . experimental results show that these stopping criteria work well on evaluation data sets , and the combination strategies outperform individual criteria .

unsupervised parse selection for hpsg
parser disambiguation with precision grammars generally takes place via statistical ranking of the parse yield of the grammar using a supervised parse selection model . in the standard process , the parse selection model is trained over a hand-disambiguated treebank , meaning that without a significant investment of effort to produce the treebank , parse selection is not possible . furthermore , as treebanking is generally streamlined with parse selection models , creating the initial treebank without a model requires more resources than subsequent treebanks . in this work , we show that , by taking advantage of the constrained nature of these hpsg grammars , we can learn a discriminative parse selection model from raw text in a purely unsupervised fashion . this allows us to bootstrap the treebanking process and provide better parsers faster , and with less resources .

simple or complex assessing the readability of basque texts
in this paper we present a readability assessment system for basque , errexail , which is going to be the preprocessing module of a text simplification system . to that end we compile two corpora , one of simple texts and another one of complex texts . to analyse those texts , we implement global , lexical , morphological , morpho-syntactic , syntactic and pragmatic features based on other languages and specially considered for basque . we combine these feature types and we train our classifiers . after testing the classifiers , we detect the features that perform best and the most predictive ones .

pauses as an indicator of psycholinguistically valid multi-word expressions ( mwes )
in this paper we investigate the role of the placement of pauses in automatically extracted multi-word expression ( mwe ) candidates from a learner corpus . the aim is to explore whether the analysis of pauses might be useful in the validation of these candidates as mwes . the study is based on the assumption advanced in the area of psycholinguistics that mwes are stored holistically in the mental lexicon and are therefore produced without pauses in naturally occurring discourse . automatic mwe extraction methods are unable to capture the criterion of holistic storage and instead rely on statistics and raw frequency in the identification of mwe candidates . in this study we explore the possibility of a combination of the two approaches . we report on a study in which we analyse the placement of pauses in various instances of two very frequent automatically extracted mwe candidates from a learner corpus , i.e . the n-grams i dont know and i think i. intuitively , they are judged differently in terms of holistic storage . our study explores whether pause analysis can be used as an objective empirical criterion to support this intuition . a corpus of interview data of language learners of english forms the basis of this study .

cross linguistic name matching in english and arabic : a one to many mapping extension of the levenshtein edit distance algorithm the mitre corporation
this paper presents a solution to the problem of matching personal names in english to the same names represented in arabic script . standard string comparison measures perform poorly on this task due to varying transliteration conventions in both languages and the fact that arabic script does not usually represent short vowels . significant improvement is achieved by augmenting the classic levenshtein edit-distance algorithm with character equivalency classes .

speeding up the design of dialogue applications by using database
nowadays , most commercial and research dialogue applications for call centers are created using sophisticated and fullyfeature development platforms . surprisingly , most of them lack of some kind of acceleration strategy based on an automatic analysis of the contents or structure of the backend database . this paper describes our efforts to incorporate this kind of information which continues the work done in ( dharo et al 2006 ) . our main proposed strategies are : the generation of automatic state proposals for defining the dialogue flow network , the automatic selection of slots to be requested using mixed-initiative , the semi-automatic generation of sql statements , and the quick generation of the data model of the application and the connection with the database fields . subjective and objective evaluations demonstrate the advantages of using the accelerations and their high acceptance , both in our current proposals and in previous work .

youngim jung hyuk-chul kwon
for the implementation of the prosody prediction model , large scale annotated speech corpora have been widely applied . reliability among transcribers , however , was too low for successful learning of an automatic prosodic prediction . this paper reveals our observations on performance deterioration of the learning model due to inconsistent tagging of prosodic breaks in the established corpora . then , we suggest a method for consistent prosodic labeling among multiple transcribers . as a result , we obtain a corpus with consistent annotation of prosodic breaks . the estimated pairwise agreement of annotation of the main corpus is between 0.7477 and 0.7916 , and the value of k is between 0.7057 and 0.7569. considering the estimated k , annotation of the main corpus has reliable consistency among multiple transcribers .

priberam : a turbo semantic parser with second order features
this paper presents our contribution to the semeval-2014 shared task on broadcoverage semantic dependency parsing . we employ a feature-rich linear model , including scores for first and second-order dependencies ( arcs , siblings , grandparents and co-parents ) . decoding is performed in a global manner by solving a linear relaxation with alternating directions dual decomposition ( ad 3 ) . our system achieved the top score in the open challenge , and the second highest score in the closed track .

a system for generating descriptions of sets of objects in a rich variety
even ambitious algorithms for the generation of referring expressions that identify sets of objects are restricted in terms of efficiency or in their expressive repertoire . in this paper , we report on a system that applies a best-first searching procedure , enhancing both its effectiveness and the variety of expressions it can generate .

a combination of active learning and semi-supervised learning disambiguation : an empirical study on japanese web search query and yasuhiro takayama mitsubishi electric corporation and masaru kitsuregawa
this paper proposes to solve the bottleneck of finding training data for word sense disambiguation ( wsd ) in the domain of web queries , where a complete set of ambiguous word senses are unknown . in this paper , we present a combination of active learning and semi-supervised learning method to treat the case when positive examples , which have an expected word sense in web search result , are only given . the novelty of our approach is to use pseudo negative examples with reliable confidence score estimated by a classifier trained with positive and unlabeled examples . we show experimentally that our proposed method achieves close enough wsd accuracy to the method with the manually prepared negative examples in several japanese web search data .

domain adaptation for sentiment classification
automatic sentiment classification has been extensively studied and applied in recent years . however , sentiment is expressed differently in different domains , and annotating corpora for every possible domain of interest is impractical . we investigate domain adaptation for sentiment classifiers , focusing on online reviews for different types of products . first , we extend to sentiment classification the recently-proposed structural correspondence learning ( scl ) algorithm , reducing the relative error due to adaptation between domains by an average of 30 % over the original scl algorithm and 46 % over a supervised baseline . second , we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another . this measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains .

k-nearest neighbor monte-carlo control algorithm for pomdp-based dialogue systems
in real-world applications , modelling dialogue as a pomdp requires the use of a summary space for the dialogue state representation to ensure tractability . suboptimal estimation of the value function governing the selection of system responses can then be obtained using a gridbased approach on the belief space . in this work , the monte-carlo control technique is extended so as to reduce training over-fitting and to improve robustness to semantic noise in the user input . this technique uses a database of belief vector prototypes to choose the optimal system action . a locally weighted k-nearest neighbor scheme is introduced to smooth the decision process by interpolating the value function , resulting in higher user simulation performance .

parsing mildly non-projective dependency structures
we present parsing algorithms for various mildly non-projective dependency formalisms . in particular , algorithms are presented for : all well-nested structures of gap degree at most 1 , with the same complexity as the best existing parsers for constituency formalisms of equivalent generative power ; all well-nested structures with gap degree bounded by any constant k ; and a new class of structures with gap degree up to k that includes some ill-nested structures . the third case includes all the gap degree k structures in a number of dependency treebanks .

structured composition of semantic vectors
distributed models of semantics assume that word meanings can be discovered from the company they keep . many such approaches learn semantics from large corpora , with each document considered to be unstructured bags of words , ignoring syntax and compositionality within a document . in contrast , this paper proposes a structured vectorial semantic framework , in which semantic vectors are defined and composed in syntactic context . as such , syntax and semantics are fully interactive ; composition of semantic vectors necessarily produces a hypothetical syntactic parse . evaluations show that using relationally-clustered headwords as a semantic space in this framework improves on a syntax-only model in perplexity and parsing accuracy .

constraints on strong generative power
we consider the question how much strong generative power can be squeezed out of a formal system without increasing its weak generative power and propose some theoretical and practical constraints on this problem . we then introduce a formalism which , under these constraints , maximally squeezes strong generative power out of context-free grammar . finally , we generalize this result to formalisms beyond cfg .

automatically predicting peer-review helpfulness
identifying peer-review helpfulness is an important task for improving the quality of feedback that students receive from their peers . as a first step towards enhancing existing peerreview systems with new functionality based on helpfulness detection , we examine whether standard product review analysis techniques also apply to our new context of peer reviews . in addition , we investigate the utility of incorporating additional specialized features tailored to peer review . our preliminary results show that the structural features , review unigrams and meta-data combined are useful in modeling the helpfulness of both peer reviews and product reviews , while peer-review specific auxiliary features can further improve helpfulness prediction .

fluency constraints for minimum bayes-risk decoding of statistical machine translation lattices
a novel and robust approach to improving statistical machine translation fluency is developed within a minimum bayesrisk decoding framework . by segmenting translation lattices according to confidence measures over the maximum likelihood translation hypothesis we are able to focus on regions with potential translation errors . hypothesis space constraints based on monolingual coverage are applied to the low confidence regions to improve overall translation fluency .

reasoning in metaphor understanding : the att-meta approach and system
a detailed approach has been developed for core aspects of the task of understanding a broad class of metaphorical utterances . the utterances in question are those that depend on known metaphorical mappings but that nevertheless contain elements not mapped by those mappings . a reasoning system has been implemented that partially instantiates the theoretical approach . the system , called att-meta , will be demonstrated . the paper briefly indicates how the system works , and outlines some specific aspects of the system , approach and the overall project .

dimensionality reduction for text using domain knowledge
text documents are complex high dimensional objects . to effectively visualize such data it is important to reduce its dimensionality and visualize the low dimensional embedding as a 2-d or 3-d scatter plot . in this paper we explore dimensionality reduction methods that draw upon domain knowledge in order to achieve a better low dimensional embedding and visualization of documents . we consider the use of geometries specified manually by an expert , geometries derived automatically from corpus statistics , and geometries computed from linguistic resources .

word alignment for languages with scarce resources
this paper presents the task definition , resources , participating systems , and comparative results for the shared task on word alignment , which was organized as part of the acl 2005 workshop on building and using parallel texts . the shared task included englishinuktitut , romanianenglish , and englishhindi sub-tasks , and drew the participation of ten teams from around the world with a total of 50 systems .

named entity transliterations from large comparable corpora
in this paper , we address the problem of mining transliterations of named entities ( nes ) from large comparable corpora . we leverage the empirical fact that multilingual news articles with similar news content are rich in named entity transliteration equivalents ( netes ) . our mining algorithm , mint , uses a cross-language document similarity model to align multilingual news articles and then mines netes from the aligned articles using a transliteration similarity model . we show that our approach is highly effective on 6 different comparable corpora between english and 4 languages from 3 different language families . furthermore , it performs substantially better than a state-of-the-art competitor .

landmark classification for route directions
in order for automated navigation systems to operate effectively , the route instructions they produce must be clear , concise and easily understood by users . in order to incorporate a landmark within a coherent sentence , it is necessary to first understand how that landmark is conceptualised by travellers whether it is perceived as point-like , linelike or area-like . this paper investigates the viability of automatically classifying the conceptualisation of landmarks relative to a given city context . we use web data to learn the default conceptualisation of those landmarks , crucially analysing preposition and verb collocations in the classification .

language identification : the long and the short of the matter
language identification is the task of identifying the language a given document is written in . this paper describes a detailed examination of what models perform best under different conditions , based on experiments across three separate datasets and a range of tokenisation strategies . we demonstrate that the task becomes increasingly difficult as we increase the number of languages , reduce the amount of training data and reduce the length of documents . we also show that it is possible to perform language identification without having to perform explicit character encoding detection .

investigations into the crandem approach to word recognition
we suggest improvements to a previously proposed framework for integrating conditional random fields and hidden markov models , dubbed a crandem system ( 2009 ) . the previous authors work suggested that local label posteriors derived from the crf were too low-entropy for use in word-level automatic speech recognition . as an alternative to the log posterior representation used in their system , we explore frame-level representations derived from the crf feature functions . we also describe a weight normalization transformation that leads to increased entropy of the crf posteriors . we report significant gains over the previous crandem system on the wall street journal word recognition task .

language model adaptation for statistical machine translation with structured query models
we explore unsupervised language model adaptation techniques for statistical machine translation . the hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection . specific language models are then build from the retrieved data and interpolated with a general background model . experiments show significant improvements when translating with these adapted language models .

multiword expression filtering for building knowledge maps one washington square
this paper describes an algorithm that can be used to improve the quality of multiword expressions extracted from documents . we measure multiword expression quality by the usefulness of a multiword expression in helping ontologists build knowledge maps that allow users to search a large document corpus . our stopword based algorithm takes ngrams extracted from documents , and cleans them up to make them more suitable for building knowledge maps . running our algorithm on large corpora of documents has shown that it helps to increase the percentage of useful terms from 40 % to 70 % with an eight-fold improvement observed in some cases .

columbia nlp : sentiment detection of sentences and subjective phrases in social media
we present two supervised sentiment detection systems which were used to compete in semeval-2014 task 9 : sentiment analysis in twitter . the first system ( rosenthal and mckeown , 2013 ) classifies the polarity of subjective phrases as positive , negative , or neutral . it is tailored towards online genres , specifically twitter , through the inclusion of dictionaries developed to capture vocabulary used in online conversations ( e.g. , slang and emoticons ) as well as stylistic features common to social media . the second system ( agarwal et al. , 2011 ) classifies entire tweets as positive , negative , or neutral . it too includes dictionaries and stylistic features developed for social media , several of which are distinctive from those in the first system . we use both systems to participate in subtasks a and b of semeval2014 task 9 : sentiment analysis in twitter . we participated for the first time in subtask b : message-level sentiment detection by combining the two systems to achieve improved results compared to either system alone .

bilingual lexical cohesion trigger model for document-level
in this paper , we propose a bilingual lexical cohesion trigger model to capture lexical cohesion for document-level machine translation . we integrate the model into hierarchical phrase-based machine translation and achieve an absolute improvement of 0.85 bleu points on average over the baseline on nist chinese-english test sets .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

source language markers in europarl translations
this paper shows that it is very often possible to identify the source language of medium-length speeches in the europarl corpus on the basis of frequency counts of word n-grams ( 87.2 % 96.7 % accuracy depending on classification method ) . the paper also examines in detail which positive markers are most powerful and identifies a number of linguistic aspects as well as culture- and domain-related ones.1

an amharic stemmer : reducing words to their citation forms atelach alemu argaw
stemming is an important analysis step in a number of areas such as natural language processing ( nlp ) , information retrieval ( ir ) , machine translation ( mt ) and text classification . in this paper we present the development of a stemmer for amharic that reduces words to their citation forms . amharic is a semitic language with rich and complex morphology . the application of such a stemmer is in dictionary based cross language ir , where there is a need in the translation step , to look up terms in a machine readable dictionary ( mrd ) . we apply a rule based approach supplemented by occurrence statistics of words in a mrd and in a 3.1m words news corpus . the main purpose of the statistical supplements is to resolve ambiguity between alternative segmentations . the stemmer is evaluated on amharic text from two domains , news articles and a classic fiction text . it is shown to have an accuracy of 60 % for the old fashioned fiction text and 75 % for the news articles .

adjective-to-verb paraphrasing in japanese based on lexical constraints of verbs
this paper describes adjective-to-verb paraphrasing in japanese . in this paraphrasing , generated verbs require additional suffixes according to their difference in meaning . to determine proper suffixes for a given adjective-verb pair , we have examined the verbal features involved in the theory of lexical conceptual structure .

automated detection of language issues affecting accuracy , ambiguity and verifiability in software requirements written in natural language allan berrocal rojas , gabriela barrantes sliesarieva
most embedded systems for the avionics industry are considered safety critical systems ; as a result , strict software development standards exist to ensure critical software is built with the highest quality possible . one of such standards , do-178b , establishes a number of properties that software requirements must satisfy including : accuracy , non-ambiguity and verifiability . from a language perspective , it is possible to automate the analysis of software requirements to determine whether or not they satisfy some quality properties . this work suggests a bounded definition for three properties ( accuracy , non-ambiguity and verifiability ) considering the main characteristics that software requirements must exhibit to satisfy those objectives . a software prototype that combines natural language processing ( nlp ) techniques and specialized dictionaries was built to examine software requirements written in english with the goal of identifying whether or not they satisfy the desired properties . preliminary results are presented showing how the tool effectively identifies critical issues that are normally ignored by human reviewers .

reranking and self-training for parser adaptation
statistical parsers trained and tested on the penn wall street journal ( wsj ) treebank have shown vast improvements over the last 10 years . much of this improvement , however , is based upon an ever-increasing number of features to be trained on ( typically ) the wsj treebank data . this has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres . such worries have merit . the standard charniak parser checks in at a labeled precisionrecall f -measure of 89.7 % on the penn wsj test set , but only 82.9 % on the test set from the brown treebank corpus . this paper should allay these fears . in particular , we show that the reranking parser described in charniak and johnson ( 2005 ) improves performance of the parser on brown to 85.2 % . furthermore , use of the self-training techniques described in ( mcclosky et al , 2006 ) raise this to 87.8 % ( an error reduction of 28 % ) again without any use of labeled brown data . this is remarkable since training the parser and reranker on labeled brown data achieves only 88.4 % .

transliteration as constrained optimization
this paper introduces a new method for identifying named-entity ( ne ) transliterations in bilingual corpora . recent works have shown the advantage of discriminative approaches to transliteration : given two strings ( ws , wt ) in the source and target language , a classifier is trained to determine if wt is the transliteration of ws . this paper shows that the transliteration problem can be formulated as a constrained optimization problem and thus take into account contextual dependencies and constraints among character bi-grams in the two strings . we further explore several methods for learning the objective function of the optimization problem and show the advantage of learning it discriminately . our experiments show that the new framework results in over 50 % improvement in translating english nes to hebrew .

alena neviarouskaya helmut prendinger mitsuru ishizuka
the automatic analysis and classification of text using fine-grained attitude labels is the main task we address in our research . the developed @ am system relies on compositionality principle and a novel approach based on the rules elaborated for semantically distinct verb classes . the evaluation of our method on 1000 sentences , that describe personal experiences , showed promising results : average accuracy on fine-grained level was 62 % , on middle level 71 % , and on top level 88 % .

building nlp resources for dzongkha : a tagset and a tagged corpus
this paper describes the application of probabilistic part of speech taggers to the dzongkha language . a tag set containing 66 tags is designed , which is based on the penn treebank . a training corpus of 40,247 tokens is utilized to train the model . using the lexicon extracted from the training corpus and lexicon from the available word list , we used two statistical taggers for comparison reasons . the best result achieved was 93.1 % accuracy in a 10-fold cross validation on the training set . the winning tagger was thereafter applied to annotate a 570,247 token corpus .

improvements to the bayesian topic n -gram models
one of the language phenomena that n-gram language model fails to capture is the topic information of a given situation . we advance the previous study of the bayesian topic language model by wallach ( 2006 ) in two directions : one , investigating new priors to alleviate the sparseness problem caused by dividing all ngrams into exclusive topics , and two , developing a novel gibbs sampler that enables moving multiple n-grams across different documents to another topic . our blocked sampler can efficiently search for higher probability space even with higher order n-grams . in terms of modeling assumption , we found it is effective to assign a topic to only some parts of a document .

ua-zbsa : a headline emotion classification through web information
this paper presents a headline emotion classification approach based on frequency and co-occurrence information collected from the world wide web . the content words of a headline ( nouns , verbs , adverbs and adjectives ) are extracted in order to form different bag of word pairs with the joy , disgust , fear , anger , sadness and surprise emotions . for each pair , we compute the mutual information score which is obtained from the web occurrences of an emotion and the content words . our approach is based on the hypothesis that group of words which co-occur together across many documents with a given emotion are highly probable to express the same emotion .

the exploration of deterministic and efficient dependency parsing
in this paper , we propose a three-step multilingual dependency parser , which generalizes an efficient parsing algorithm at first phase , a root parser and postprocessor at the second and third stages . the main focus of our work is to provide an efficient parser that is practical to use with combining only lexical and part-ofspeech features toward language independent parsing . the experimental results show that our method outperforms maltparser in 13 languages . we expect that such an efficient model is applicable for most languages .

how to avoid burning ducks : combining linguistic analysis and corpus statistics for german compound processing
compound splitting is an important problem in many nlp applications which must be solved in order to address issues of data sparsity . previous work has shown that linguistic approaches for german compound splitting produce a correct splitting more often , but corpus-driven approaches work best for phrase-based statistical machine translation from german to english , a worrisome contradiction . we address this situation by combining linguistic analysis with corpus-driven statistics and obtaining better results in terms of both producing splittings according to a gold standard and statistical machine translation performance .

distributed listening : a parallel processing approach to automatic
while speech recognition systems have come a long way in the last thirty years , there is still room for improvement . although readily available , these systems are sometimes inaccurate and insufficient . the research presented here outlines a technique called distributed listening which demonstrates noticeable improvements to existing speech recognition methods . the distributed listening architecture introduces the idea of multiple , parallel , yet physically separate automatic speech recognizers called listeners . distributed listening also uses a piece of middleware called an interpreter . the interpreter resolves multiple interpretations using the phrase resolution algorithm ( pra ) . these efforts work together to increase the accuracy of the transcription of spoken utterances .

assas-band , an affix-exception-list based urdu stemmer
both inflectional and derivational morphology lead to multiple surface forms of a word . stemming reduces these forms back to its stem or root , and is a very useful tool for many applications . there has not been any work reported on urdu stemming . the current work develops an urdu stemmer or assas-band and improves the performance using more precise affix based exception lists , instead of the conventional lexical lookup employed for developing stemmers in other languages . testing shows an accuracy of 91.2 % . further enhancements are also suggested .

interactive predictive parsing using a web-based architecture
this paper introduces a web-based demonstration of an interactive-predictive framework for syntactic tree annotation , where the user is tightly integrated into the interactive parsing system . in contrast with the traditional postediting approach , both the user and the system cooperate to generate error-free annotated trees . user feedback is provided by means of natural mouse gestures and keyboard strokes .

detecting errors in corpora using support vector machines
while the corpus-based research relies on human annotated corpora , it is often said that a non-negligible amount of errors remain even in frequently used corpora such as penn treebank . detection of errors in annotated corpora is important for corpus-based natural language processing . in this paper , we propose a method to detect errors in corpora using support vector machines ( svms ) . this method is based on the idea of extracting exceptional elements that violate consistency . we propose a method of using svms to assign a weight to each element and to find errors in a pos tagged corpus . we apply the method to english and japanese pos-tagged corpora and achieve high precision in detecting errors .

automatic generation of parallel treebanks
the need for syntactically annotated data for use in natural language processing has increased dramatically in recent years . this is true especially for parallel treebanks , of which very few exist . the ones that exist are mainly hand-crafted and too small for reliable use in data-oriented applications . in this paper we introduce a novel platform for fast and robust automatic generation of parallel treebanks . the software we have developed based on this platform has been shown to handle large data sets . we also present evaluation results demonstrating the quality of the derived treebanks and discuss some possible modifications and improvements that can lead to even better results . we expect the presented platform to help boost research in the field of dataoriented machine translation and lead to advancements in other fields where parallel treebanks can be employed .

a knowledge-driven approach to text meaning processing
our goal is to be able to answer questions about text that go beyond facts explicitly stated in the text , a task which inherently requires extracting a deep level of meaning from that text . our approach treats meaning processing fundamentally as a modeling activity , in which a knowledge base of common-sense expectations guides interpretation of text , and text suggests which parts of the knowledge base might be relevant . in this paper , we describe our ongoing investigations to develop this approach into a usable method for meaning processing .

unsupervised learning of rhetorical structure with un-topic models
in this paper we investigate whether unsupervised models can be used to induce conventional aspects of rhetorical language in scientific writing . we rely on the intuition that the rhetorical language used in a document is general in nature and independent of the documents topic . we describe a bayesian latent-variable model that implements this intuition . in two empirical evaluations based on the task of argumentative zoning ( az ) , we demonstrate that our generality hypothesis is crucial for distinguishing between rhetorical and topical language and that features provided by our unsupervised model trained on a large corpus can improve the performance of a supervised az classifier .

induction of fine-grained part-of-speech taggers via elliott franco drabek
this paper presents an original approach to part-of-speech tagging of fine-grained features ( such as case , aspect , and adjective person/number ) in languages such as english where these properties are generally not morphologically marked . the goals of such rich lexical tagging in english are to provide additional features for word alignment models in bilingual corpora ( for statistical machine translation ) , and to provide an information source for part-of-speech tagger induction in new languages via tag projection across bilingual corpora . first , we present a classifier-combination approach to tagging english bitext with very fine-grained part-of-speech tags necessary for annotating morphologically richer languages such as czech and french , combining the extracted features of three major english parsers , and achieve fine-grained-tag-level syntactic analysis accuracy higher than any individual parser . second , we present experimental results for the cross-language projection of partof-speech taggers in czech and french via word-aligned bitext , achieving successful fine-grained part-of-speech tagging of these languages without any czech or french training data of any kind .

semanticnet-perception of human pragmatics
semanticnet is a semantic network of lexicons to hold human pragmatic knowledge . so far natural language processing ( nlp ) research patronized much of manually augmented lexicon resources such as wordnet . but the small set of semantic relations like hypernym , holonym , meronym and synonym etc are very narrow to capture the wide variations human cognitive knowledge . but no such information could be retrieved from available lexicon resources . semanticnet is the attempt to capture wide range of context dependent semantic inference among various themes which human beings perceive in their pragmatic knowledge , learned by day to day cognitive interactions with the surrounding physical world . semanticnet holds human pragmatics with twenty well established semantic relations for every pair of lexemes . as every pair of relations can not be defined by fixed number of certain semantic relation labels thus additionally contextual semantic affinity inference in semanticnet could be calculated by network distance and represented as a probabilistic score . semanticnet is being presently developed for bengali language .

multilingual wsd-like constraints for paraphrase extraction
the use of pivot languages and wordalignment techniques over bilingual corpora has proved an effective approach for extracting paraphrases of words and short phrases . however , inherent ambiguities in the pivot language ( s ) can lead to inadequate paraphrases . we propose a novel approach that is able to extract paraphrases by pivoting through multiple languages while discriminating word senses in the input language , i.e. , the language to be paraphrased . text in the input language is annotated with senses in the form of foreign phrases obtained from bilingual parallel data and automatic word-alignment . this approach shows 62 % relative improvement over previous work in generating paraphrases that are judged both more accurate and more fluent .

co-occurrence cluster features for lexical substitutions in context
this paper examines the influence of features based on clusters of co-occurrences for supervised word sense disambiguation and lexical substitution . cooccurrence cluster features are derived from clustering the local neighborhood of a target word in a co-occurrence graph based on a corpus in a completely unsupervised fashion . clusters can be assigned in context and are used as features in a supervised wsd system . experiments fitting a strong baseline system with these additional features are conducted on two datasets , showing improvements . cooccurrence features are a simple way to mimic topic signatures ( martnez et al , 2008 ) without needing to construct resources manually . further , a system is described that produces lexical substitutions in context with very high precision .

dissect - distributional semantics composition toolkit georgiana dinu and nghia the pham and marco baroni
we introduce dissect , a toolkit to build and explore computational models of word , phrase and sentence meaning based on the principles of distributional semantics . the toolkit focuses in particular on compositional meaning , and implements a number of composition methods that have been proposed in the literature . furthermore , dissect can be useful to researchers and practitioners who need models of word meaning ( without composition ) as well , as it supports various methods to construct distributional semantic spaces , assessing similarity and even evaluating against benchmarks , that are independent of the composition infrastructure .

the multext-east morphosyntactic specifications for slavic languages
word-level morphosyntactic descriptions , such as ncmsn designating a common masculine singular noun in the nominative , have been developed for all slavic languages , yet there have been few attempts to arrive at a proposal that would be harmonised across the languages . standardisation adds to the interchange potential of the resources , making it easier to develop multilingual applications or to evaluate language technology tools across several languages . the process of the harmonisation of morphosyntactic categories , esp . for morphologically rich slavic languages is also interesting from a language-typological perspective . the eu multext-east project developed corpora , lexica and tools for seven languages , with the focus being on morphosyntactic data , including formal , eagles-based specifications for lexical morphosyntactic descriptions . the specifications were later extended , so that they currently cover nine languages , five from the slavic family : bulgarian , croatian , czech , serbian and slovene . the paper presents these morphosyntactic specifications , giving their background and structure , including the encoding of the tables as tei feature structures . the five slavic language specifications are discussed in more depth .

experimental support for a categorical compositional distributional model of meaning
modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists . we implement the abstract categorical model of coecke et al ( 2010 ) using data from the bnc and evaluate it . the implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments . the evaluation is based on the word disambiguation task developed by mitchell and lapata ( 2008 ) for intransitive sentences , and on a similar new experiment designed for transitive sentences . our model matches the results of its competitors in the first experiment , and betters them in the second . the general improvement in results with increase in syntactic complexity showcases the compositional power of our model .

a utility-driven approach to question ranking in social qa
we generalize the task of finding question paraphrases in a question repository to a novel formulation in which known questions are ranked based on their utility to a new , reference question . we manually annotate a dataset of 60 groups of questions with a partial order relation reflecting the relative utility of questions inside each group , and use it to evaluate meaning and structure aware utility functions . experimental evaluation demonstrates the importance of using structural information in estimating the relative usefulness of questions , holding the promise of increased usability for social qa sites .

lexicalized markov grammars for sentence compression
we present a sentence compression system based on synchronous context-free grammars ( scfg ) , following the successful noisy-channel approach of ( knight and marcu , 2000 ) . we define a headdriven markovization formulation of scfg deletion rules , which allows us to lexicalize probabilities of constituent deletions . we also use a robust approach for tree-to-tree alignment between arbitrary document-abstract parallel corpora , which lets us train lexicalized models with much more data than previous approaches relying exclusively on scarcely available document-compression corpora . finally , we evaluate different markovized models , and find that our selected best model is one that exploits head-modifier bilexicalization to accurately distinguish adjuncts from complements , and that produces sentences that were judged more grammatical than those generated by previous work .

constituent parsing by classification
ordinary classification techniques can drive a conceptually simple constituent parser that achieves near state-of-the-art accuracy on standard test sets . here we present such a parser , which avoids some of the limitations of other discriminative parsers . in particular , it does not place any restrictions upon which types of features are allowed . we also present several innovations for faster training of discriminative parsers : we show how training can be parallelized , how examples can be generated prior to training without a working parser , and how independently trained sub-classifiers that have never done any parsing can be effectively combined into a working parser . finally , we propose a new figure-of-merit for bestfirst parsing with confidence-rated inferences . our implementation is freely available at : http : //cs.nyu.edu/turian/ software/parser/

an effective compositional model for lexical alignment beatrice daille emmanuel morin
the automatic compilation of bilingual dictionaries from comparable corpora has been successful for single-word terms ( swts ) , but remains disappointing for multi-word terms ( mwts ) . one of the main problems is the insufficient coverage of the bilingual dictionary . using the compositional translation method improved the results , but still shows some limits for mwts of different syntactic structures . in this paper , we propose to bridge the gap between syntactic structures through morphological links . the results show a significant improvement in the compositional translation of mwts that demonstrate the efficiency of the morphologically based-method for lexical alignment .

underspecification of meaning : the case of russian imperfective
one main problem for nlp applications is that natural language expressions are underspecified and require enrichments of different sorts to get a truthconditional interpretaton in context . underspecification applies on two levels : what is said underdetermines what is meant , and linguistic meaning underspecifies what is said . one instance of this phenomenon is aspect in russian , especially the imperfective one . it gives rise to a variety of readings , which are difficult to capture by one invariant meaning . instead , the imperfective aspect is sense-general ; its meaning has to be specified in the course of interpretation by contextual cues and pragmatic inferences . this paper advocates an account of the different imperfective readings in terms of pragmatic principles and inferential heuristics based on , and supplied by , a semantic skeleton consisting of a selectional theory of aspect . this framework might serve as basis for a rule-guided derivation of aspectual readings in russian .

language diversity across the consonant inventories : a study in the framework of complex networks
in this paper , we attempt to explain the emergence of the linguistic diversity that exists across the consonant inventories of some of the major language families of the world through a complex network based growth model . there is only a single parameter for this model that is meant to introduce a small amount of randomness in the otherwise preferential attachment based growth process . the experiments with this model parameter indicates that the choice of consonants among the languages within a family are far more preferential than it is across the families . furthermore , our observations indicate that this parameter might bear a correlation with the period of existence of the language families under investigation . these findings lead us to argue that preferential attachement seems to be an appropriate high level abstraction for language acquisition and change .

axiomatization of restricted non-projective dependency trees through finite-state constraints that analyse crossing bracketings anssi yli-jyr a
in this paper , a representation for syntactic dependency trees ( d-trees ) is defined through a finite set of axioms . the axiomatized representation constitutes a string that can encode non-projective d-trees of restricted structural complexity . upper-bounds for the structural complexity of these d-trees are fixed through the following new parameters : proper embracement depth , nested crossing depth , and non-projectivity depth . in the representation , syntactic dependencies between words are indicated with pairs of brackets . when the brackets indicate dependencies that cross each other , the crossing pairs of brackets are distinguished by assigning separate colors to each of them . these colors are allocated in a way ( yli-jyra and nykanen , 2004 ) that ensures a unique representation for each d-tree , and entails that languages whose nested crossing depth is not bounded can not be captured using a fixed number of colors . although the axiomatization is finite , it ensures that the represented dependency structures are trees . this is possible because the described d-trees have bounded non-projectivity depth . the axioms are also regular because proper embracement depth of represented d-trees is bounded . our representation suggests that extra strong generative power can be squeezed out of finite-state equivalent grammars .

phrase reordering model integrating syntactic knowledge for smt
reordering model is important for the statistical machine translation ( smt ) . current phrase-based smt technologies are good at capturing local reordering but not global reordering . this paper introduces syntactic knowledge to improve global reordering capability of smt system . syntactic knowledge such as boundary words , pos information and dependencies is used to guide phrase reordering . not only constraints in syntax tree are proposed to avoid the reordering errors , but also the modification of syntax tree is made to strengthen the capability of capturing phrase reordering . furthermore , the combination of parse trees can compensate for the reordering errors caused by single parse tree . finally , experimental results show that the performance of our system is superior to that of the state-of-the-art phrase-based smt system .

semi-supervised verb class discovery using noisy features
we cluster verbs into lexical semantic classes , using a general set of noisy features that capture syntactic and semantic properties of the verbs . the feature set was previously shown to work well in a supervised learning setting , using known english verb classes . in moving to a scenario of verb class discovery , using clustering , we face the problem of having a large number of irrelevant features for a particular clustering task . we investigate various approaches to feature selection , using both unsupervised and semi-supervised methods , comparing the results to subsets of features manually chosen according to linguistic properties . we find that the unsupervised method we tried can not be consistently applied to our data . however , the semisupervised approach ( using a seed set of sample verbs ) overall outperforms not only the full set of features , but the hand-selected features as well .

a working report on statistically modeling dative variation
dative variation is a widely observed syntactic phenomenon in world languages . it has been shown that which surface form will be used in a dative sentence is not a completely random choice , rather , it is conditioned by a wide range of linguistic factors . previous work by bresnan and colleagues adopted a statistical modeling approach to investigate the probabilistic trends in english dative alternation . in this paper , we report a similar study on mandarin chinese . we further developed bresnan et als models to suit the complexity of the chinese data . our models effectively explain away a large proportion of the variation in the data , and unveil some interesting probabilistic features of chinese grammar . among other things , we show that chinese dative variation is sensitive to heavy np shift in both left and right directions .

high performance word sense alignment by joint modeling of sense and iryna gurevych
in this paper , we present a machine learning approach for word sense alignment ( wsa ) which combines distances between senses in the graph representations of lexical-semantic resources with gloss similarities . in this way , we significantly outperform the state of the art on each of the four datasets we consider . moreover , we present two novel datasets for wsa between wiktionary and wikipedia in english and german . the latter dataset in not only of unprecedented size , but also created by the large community of wiktionary editors instead of expert annotators , making it an interesting subject of study in its own right as the first crowdsourced wsa dataset . we will make both datasets freely available along with our computed alignments .

poly-co : a multilayer perceptron approach for coreference detection
this paper presents the coreference resolution system poly-co submitted to the closed track of the conll-2011 shared task . our system integrates a multilayer perceptron classifier in a pipeline approach . we describe the heuristic used to select the pairs of coreference candidates that are feeded to the network for training , and our feature selection method . the features used in our approach are based on similarity and identity measures , filtering informations , like gender and number , and other syntactic information .

inference protocols for coreference resolution
this paper presents illinois-coref , a system for coreference resolution that participated in the conll-2011 shared task . we investigate two inference methods , best-link and all-link , along with their corresponding , pairwise and structured , learning protocols . within these , we provide a flexible architecture for incorporating linguistically-motivated constraints , several of which we developed and integrated . we compare and evaluate the inference approaches and the contribution of constraints , analyze the mistakes of the system , and discuss the challenges of resolving coreference for the ontonotes-4.0 data set .

investigating clarification strategies in a hybrid pomdp dialog manager
we investigate the clarification strategies exhibited by a hybrid pomdp dialog manager based on data obtained from a phone-based user study . the dialog manager combines task structures with a number of pomdp policies each optimized for obtaining an individual concept . we investigate the relationship between dialog length and task completion . in order to measure the effectiveness of the clarification strategies , we compute concept precisions for two different mentions of the concept in the dialog : first mentions and final values after clarifications and similar strategies , and compare this to a rulebased system on the same task . we observe an improvement in concept precision of 12.1 % for the hybrid pomdp compared to 5.2 % for the rule-based system .

towards robust context-sensitive sentence alignment for monolingual division of engineering and applied sciences
aligning sentences belonging to comparable monolingual corpora has been suggested as a first step towards training text rewriting algorithms , for tasks such as summarization or paraphrasing . we present here a new monolingual sentence alignment algorithm , combining a sentence-based tf*idf score , turned into a probability distribution using logistic regression , with a global alignment dynamic programming algorithm . our approach provides a simpler and more robust solution achieving a substantial improvement in accuracy over existing systems .

natural language understanding using temporal action logic
we consider a logicist approach to natural language understanding based on the translation of a quasi-logical form into a temporal logic , explicitly constructed for the representation of action and change , and the subsequent reasoning about this semantic structure in the context of a background knowledge theory using automated theorem proving techniques . the approach is substantiated through a proof-ofconcept question answering system implementation that uses a head-driven phrase structure grammar developed in the linguistic knowledge builder to construct minimal recursion semantics structures which are translated into a temporal action logic where both the snark automated theorem prover and the allegro prolog logic programming environment can be used for reasoning through an interchangeable compilation into first-order logic or logic programs respectively .

anaphora models and reordering for phrase-based smt christian hardmeier sara stymne j org tiedemann aaron smith joakim nivre
we describe the uppsala university systems for wmt14 . we look at the integration of a model for translating pronominal anaphora and a syntactic dependency projection model for englishfrench . furthermore , we investigate post-ordering and tunable pos distortion models for english german .

boosting for chinese named entity recognition
we report an experiment in which a highperformance boosting based ner model originally designed for multiple european languages is instead applied to the chinese named entity recognition task of the third sighan chinese language processing bakeoff . using a simple characterbased model along with a set of features that are easily obtained from the chinese input strings , the system described employs boosting , a promising and theoretically well-founded machine learning method to combine a set of weak classifiers together into a final system . even though we did no other chinese-specific tuning , and used only one-third of the msra and cityu corpora to train the system , reasonable results are obtained . our evaluation results show that 75.07 and 80.51 overall f-measures were obtained on msra and cityu test sets respectively .

using a maximum entropy-based tagger to improve a very fast vine parser
in this short paper , an off-the-shelf maximum entropy-based pos-tagger is used as a partial parser to improve the accuracy of an extremely fast linear time dependency parser that provides state-of-the-art results in multilingual unlabeled pos sequence parsing .

re-ranking models for spoken language understanding
spoken language understanding aims at mapping a natural language spoken sentence into a semantic representation . in the last decade two main approaches have been pursued : generative and discriminative models . the former is more robust to overfitting whereas the latter is more robust to many irrelevant features . additionally , the way in which these approaches encode prior knowledge is very different and their relative performance changes based on the task . in this paper we describe a machine learning framework where both models are used : a generative model produces a list of ranked hypotheses whereas a discriminative model based on structure kernels and support vector machines , re-ranks such list . we tested our approach on the media corpus ( human-machine dialogs ) and on a new corpus ( human-machine and humanhuman dialogs ) produced in the european luna project . the results show a large improvement on the state-of-the-art in concept segmentation and labeling .

randomized decoding for selection-and-ordering problems
the task of selecting and ordering information appears in multiple contexts in text generation and summarization . for instance , methods for title generation construct a headline by selecting and ordering words from the input text . in this paper , we investigate decoding methods that simultaneously optimize selection and ordering preferences . we formalize decoding as a task of finding an acyclic path in a directed weighted graph . since the problem is np-hard , finding an exact solution is challenging . we describe a novel decoding method based on a randomized color-coding algorithm . we prove bounds on the number of color-coding iterations necessary to guarantee any desired likelihood of finding the correct solution . our experiments show that the randomized decoder is an appealing alternative to a range of decoding algorithms for selection-andordering problems , including beam search and integer linear programming .

textual entailment using univariate density model and maximizing
the primary focuses of this entry this year was firstly , to develop a framework to allow multiple researchers from our group to easily contribute metrics measuring textual entailment , and secondly , to provide a baseline which we could use in our tools to evaluate and compare new metrics . a development environment tool was created to quickly allow for testing of various metrics and to easily randomize the development and test sets . for each test , this rte tool calculated two sets of results by applying the metrics to both a univariate gaussian density and by maximizing a linear discriminant function . the metrics used for the submission were a lexical similarity metric and a lexical similarity metric using synonym and antonym replacement . the two submissions for rte 2007 scored an accuracy of 61.00 % and 62.62 % .

you talking to me a predictive model for zero auxiliary constructions
as a consequence of the established practice to prefer training data obtained from written sources , nlp tools encounter problems in handling data from the spoken domain . however , accurate models of spoken data are increasingly in demand for naturalistic speech generation and machine translations in speech-like contexts ( such as chat windows and sms ) . there is a widely held assumption in the linguistic field that spoken language is an impoverished form of written language . however , we show that spoken data is not unpredictably irregular and that language models can benefit from detailed consideration of spoken language features . this paper considers one specific construction which is largely restricted to the spoken domain - the zero auxiliary and makes a predictive model of that construction for native speakers of british english . the model can predict zero auxiliary occurrence in the bnc with 96.9 % accuracy . we will demonstrate how this model can be integrated into existing parsing tools , increasing the number of successful parses for this zero auxiliary construction by around 30 % , and thus improving the performance of nlp applications which rely on parsing .

exploring cities in crime : significant concordance and co-occurrence in quantitative literary analysis
we present coocviewer , a graphical analysis tool for the purpose of quantitative literary analysis , and demonstrate its use on a corpus of crime novels . the tool displays words , their significant co-occurrences , and contains a new visualization for significant concordances . contexts of words and co-occurrences can be displayed . after reviewing previous research and current challenges in the newly emerging field of quantitative literary research , we demonstrate how coocviewer allows comparative research on literary corpora in a project-specific study , and how we can confirm or enhance our hypotheses through quantitative literary analysis .

social ( distributed ) language modeling , clustering and dialectometry
we present ongoing work in a scalable , distributed implementation of over 200 million individual language models , each capturing a single users dialect in a given language ( multilingual users have several models ) . these have a variety of practical applications , ranging from spam detection to speech recognition , and dialectometrical methods on the social graph . users should be able to view any content in their language ( even if it is spoken by a small population ) , and to browse our site with appropriately translated interface ( automatically generated , for locales with little crowd-sourced community effort ) .

nil ucm : extracting drug-drug interactions from text through combination of sequence and tree kernels
a drug-drug interaction ( ddi ) occurs when one drug affects the level or activity of another drug . semeval 2013 ddi extraction challenge is going to be held with the aim of identifying the state of the art relation extraction algorithms . in this paper we firstly review some of the existing approaches in relation extraction generally and biomedical relations especially . and secondly we will explain our svm based approaches that use lexical , morphosyntactic and parse tree features . our combination of sequence and tree kernels have shown promising performance with a best result of 0.54 f1 macroaverage on the test dataset .

getalp : propagation of a lesk measure through an ant colony algorithm
this article presents the getalp system for the participation to semeval-2013 task 12 , based on an adaptation of the lesk measure propagated through an ant colony algorithm , that yielded good results on the corpus of semeval 2007 task 7 ( wordnet 2.1 ) as well as the trial data for task 12 semeval 2013 ( babelnet 1.0 ) . we approach the parameter estimation to our algorithm from two perspectives : edogenous estimation where we maximised the sum the local lesk scores ; exogenous estimation where we maximised the f1 score on trial data . we proposed three runs of out system , exogenous estimation with babelnet 1.1.1 synset id annotations , endogenous estimation with babelnet 1.1.1 synset id annotations and endogenous estimation with wordnet 3.1 sense keys . a bug in our implementation led to incorrect results and here , we present an amended version thereof . our system arrived third on this task and a more fine grained analysis of our results reveals that the algorithms performs best on general domain texts with as little named entities as possible . the presence of many named entities leads the performance of the system to plummet greatly .

get out the vote : determining support or opposition from congressional
we investigate whether one can determine from the transcripts of u.s. congressional floor debates whether the speeches represent support of or opposition to proposed legislation . to address this problem , we exploit the fact that these speeches occur as part of a discussion ; this allows us to use sources of information regarding relationships between discourse segments , such as whether a given utterance indicates agreement with the opinion expressed by another . we find that the incorporation of such information yields substantial improvements over classifying speeches in isolation .

chinese word segmentation in ict-nlp
chinese word segmentation is always much accounted of in ict-nlp . in this bakeoff , two different systems in ictnlp participated . the one is system_ # 1 evaluated in three tracks -- pk-close , msr-close and msr-open , and system_ # 2 pk-open . through this bakeoff , the development of chinese segmentation is learned and the problems are found in our systems .

practical very large scale crfs limsi cnrs ltci cnrs limsi cnrs
conditional random fields ( crfs ) are a widely-used approach for supervised sequence labelling , notably due to their ability to handle large description spaces and to integrate structural dependency between labels . even for the simple linearchain model , taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set . in this paper , we address the issue of training very large crfs , containing up to hundreds output labels and several billion features . efficiency stems here from the sparsity induced by the use of a `1 penalty term . based on our own implementation , we compare three recent proposals for implementing this regularization strategy . our experiments demonstrate that very large crfs can be trained efficiently and that very large models are able to improve the accuracy , while delivering compact parameter sets .

scaling up from dialogue to multilogue : some principles and benchmarks
the paper considers how to scale up dialogue protocols to multilogue , settings with multiple conversationalists . we extract two benchmarks to evaluate scaled up protocols based on the long distance resolution possibilities of nonsentential utterances in dialogue and multilogue in the british national corpus . in light of these benchmarks , we then consider three possible transformations to dialogue protocols , formulated within an issue-based approach to dialogue management . we show that one such transformation yields protocols for querying and assertion that fulfill these benchmarks .

em decipherment for large vocabularies human language technology and pattern recognition
this paper addresses the problem of embased decipherment for large vocabularies . here , decipherment is essentially a tagging problem : every cipher token is tagged with some plaintext type . as with other tagging problems , this one can be treated as a hidden markov model ( hmm ) , only here , the vocabularies are large , so the usual o ( nv 2 ) exact em approach is infeasible . when faced with this situation , many people turn to sampling . however , we propose to use a type of approximate em and show that it works well . the basic idea is to collect fractional counts only over a small subset of links in the forward-backward lattice . the subset is different for each iteration of em . one option is to use beam search to do the subsetting . the second method restricts the successor words that are looked at , for each hypothesis . it does this by consulting pre-computed tables of likely n-grams and likely substitutions .

investigating a generic paraphrase-based approach for relation extraction
unsupervised paraphrase acquisition has been an active research field in recent years , but its effective coverage and performance have rarely been evaluated . we propose a generic paraphrase-based approach for relation extraction ( re ) , aiming at a dual goal : obtaining an applicative evaluation scheme for paraphrase acquisition and obtaining a generic and largely unsupervised configuration for re.we analyze the potential of our approach and evaluate an implemented prototype of it using an re dataset . our findings reveal a high potential for unsupervised paraphrase acquisition . we also identify the need for novel robust models for matching paraphrases in texts , which should address syntactic complexity and variability .

robust measurement and comparison of context similarity for finding
in cross-language information retrieval it is often important to align words that are similar in meaning in two corpora written in different languages . previous research shows that using context similarity to align words is helpful when no dictionary entry is available . we suggest a new method which selects a subset of words ( pivot words ) associated with a query and then matches these words across languages . to detect word associations , we demonstrate that a new bayesian method for estimating point-wise mutual information provides improved accuracy . in the second step , matching is done in a novel way that calculates the chance of an accidental overlap of pivot words using the hypergeometric distribution . we implemented a wide variety of previously suggested methods . testing in two conditions , a small comparable corpora pair and a large but unrelated corpora pair , both written in disparate languages , we show that our approach consistently outperforms the other systems .

medslt : a limited-domain unidirectional grammar-based medical beth ann hockey
medslt is a unidirectional medical speech translation system intended for use in doctor-patient diagnosis dialogues , which provides coverage of several different language pairs and subdomains . vocabulary ranges from about 350 to 1000 surface words , depending on the language and subdomain . we will demo both the system itself and the development environment , which uses a combination of rule-based and data-driven methods to construct efficient recognisers , generators and transfer rule sets from small corpora .

fast-champollion : a fast and robust sentence alignment algorithm the boeing company
sentence-level aligned parallel texts are important resources for a number of natural language processing ( nlp ) tasks and applications such as statistical machine translation and cross-language information retrieval . with the rapid growth of online parallel texts , efficient and robust sentence alignment algorithms become increasingly important . in this paper , we propose a fast and robust sentence alignment algorithm , i.e. , fastchampollion , which employs a combination of both length-based and lexiconbased algorithm . by optimizing the process of splitting the input bilingual texts into small fragments for alignment , fastchampollion , as our extensive experiments show , is 4.0 to 5.1 times as fast as the current baseline methods such as champollion ( ma , 2006 ) on short texts and achieves about 39.4 times as fast on long texts , and fast-champollion is as robust as champollion .

building a sense tagged corpus with open mind word expert
open mind word expert is an implemented active learning system for collecting word sense tagging from the general public over the web . it is available at http : //teach-computers.org . we expect the system to yield a large volume of high-quality training data at a much lower cost than the traditional method of hiring lexicographers . we thus propose a senseval-3 lexical sample activity where the training data is collected via open mind word expert . if successful , the collection process can be extended to create the definitive corpus of word sense information .

biomedical event detection using rules , conditional random fields and parse tree distances
this paper reports on a system developed for the bionlp'09 shared task on detection and characterisation of biomedical events . event triggers and types were recognised using a conditional random field classifier and a set of rules , while event participants were identified using a rule-based system that relied on relative distances between candidate entities and the trigger in the associated parse tree . the results on previously unseen test data were encouraging : for non-regulatory events , the fscore was almost 50 % ( with precision above 60 % ) , with the overall f-score of around 30 % ( 49 % precision ) . the performance on more complex regulatory events was poor ( f-measure of 7 % ) . among the 24 teams submitting the test results , our results were ranked 12th for the overall f-score and 8th for the f-score of non-regulation events .

learning to win by reading manuals in a monte-carlo framework
this paper presents a novel approach for leveraging automatically extracted textual knowledge to improve the performance of control applications such as games . our ultimate goal is to enrich a stochastic player with highlevel guidance expressed in text . our model jointly learns to identify text that is relevant to a given game state in addition to learning game strategies guided by the selected text . our method operates in the monte-carlo search framework , and learns both text analysis and game strategies based only on environment feedback . we apply our approach to the complex strategy game civilization ii using the official game manual as the text guide . our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart , yielding a 27 % absolute improvement and winning over 78 % of games when playing against the builtin ai of civilization ii .

combining generative and discriminative model scores for distant spoken language systems
distant supervision is a scheme to generate noisy training data for relation extraction by aligning entities of a knowledge base with text . in this work we combine the output of a discriminative at-least-one learner with that of a generative hierarchical topic model to reduce the noise in distant supervision data . the combination significantly increases the ranking quality of extracted facts and achieves state-of-the-art extraction performance in an end-to-end setting . a simple linear interpolation of the model scores performs better than a parameter-free scheme based on nondominated sorting .

simcompass : using deep learning word embeddings to assess cross-level similarity
this article presents our teams participating system at semeval-2014 task 3. using a meta-learning framework , we experiment with traditional knowledgebased metrics , as well as novel corpusbased measures based on deep learning paradigms , paired with varying degrees of context expansion . the framework enabled us to reach the highest overall performance among all competing systems .

measuring the similarity between automatically generated topics
previous approaches to the problem of measuring similarity between automatically generated topics have been based on comparison of the topics word probability distributions . this paper presents alternative approaches , including ones based on distributional semantics and knowledgebased measures , evaluated by comparison with human judgements . the best performing methods provide reliable estimates of topic similarity comparable with human performance and should be used in preference to the word probability distribution measures used previously .

the leaf projection path view of parse trees : exploring string kernels for hpsg parse selection
we present a novel representation of parse trees as lists of paths ( leaf projection paths ) from leaves to the top level of the tree . this representation allows us to achieve significantly higher accuracy in the task of hpsg parse selection than standard models , and makes the application of string kernels natural . we define tree kernels via string kernels on projection paths and explore their performance in the context of parse disambiguation . we apply svm ranking models and achieve an exact sentence accuracy of 85.40 % on the redwoods corpus .

how well do semantic relatedness measures perform
various semantic relatedness , similarity , and distance measures have been proposed in the past decade and many nlp-applications strongly rely on these semantic measures . researchers compete for better algorithms and normally only few percentage points seem to suffice in order to prove a new measure outperforms an older one . in this paper we present a metastudy comparing various semantic measures and their correlation with human judgments . we show that the results are rather inconsistent and ask for detailed analyses as well as clarification . we argue that the definition of a shared task might bring us considerably closer to understanding the concept of semantic relatedness . 59 60 cramer

generating and selecting grammatical paraphrases
natural language has a high paraphrastic power yet not all paraphrases are appropriate for all contexts . in this paper , we present a tag based surface realiser which supports both the generation and the selection of paraphrases . to deal with the combinatorial explosion typical of such an np-complete task , we introduce a number of new optimisations in a tabular , bottom-up surface realisation algorithm . we then show that one of these optimisations supports paraphrase selection .

computer science & engineering
many researchers are trying to use information extraction ( ie ) to create large-scale knowledge bases from natural language text on the web . however , the primary approach ( supervised learning of relation-specific extractors ) requires manually-labeled training data for each relation and doesnt scale to the thousands of relations encoded in web text . this paper presents luchs , a self-supervised , relation-specific ie system which learns 5025 relations more than an order of magnitude greater than any previous approach with an average f1 score of 61 % . crucial to luchss performance is an automated system for dynamic lexicon learning , which allows it to learn accurately from heuristically-generated training data , which is often noisy and sparse .

combining lexical and syntactic features for supervised word sense disambiguation
the success of supervised learning approaches to word sense disambiguation is largely dependent on the features used to represent the context in which an ambiguous word occurs . previous work has reached mixed conclusions ; some suggest that combinations of syntactic and lexical features will perform most effectively . however , others have shown that simple lexical features perform well on their own . this paper evaluates the effect of using different lexical and syntactic features both individually and in combination . we show that it is possible for a very simple ensemble that utilizes a single lexical feature and a sequence of part of speech features to result in disambiguation accuracy that is near state of the art .

exploiting rich features for detecting hedges and their scope
this paper describes our system about detecting hedges and their scope in natural language texts for our participation in conll2010 shared tasks . we formalize these two tasks as sequence labeling problems , and implement them using conditional random fields ( crfs ) model . in the first task , we use a greedy forward procedure to select features for the classifier . these features include part-ofspeech tag , word form , lemma , chunk tag of tokens in the sentence . in the second task , our system exploits rich syntactic features about dependency structures and phrase structures , which achieves a better performance than only using the flat sequence features . our system achieves the third score in biological data set for the first task , and achieves 0.5265 f1 score for the second task .

leveraging effective query modeling techniques for speech recognition and summarization
statistical language modeling ( lm ) that purports to quantify the acceptability of a given piece of text has long been an interesting yet challenging research area . in particular , language modeling for information retrieval ( ir ) has enjoyed remarkable empirical success ; one emerging stream of the lm approach for ir is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness . this paper presents a continuation of such a general line of research and the main contribution is threefold . first , we propose a principled framework which can unify the relationships among several widely-used query modeling formulations . second , on top of the successfully developed framework , we propose an extended query modeling formulation by incorporating critical query-specific information cues to guide the model estimation . third , we further adopt and formalize such a framework to the speech recognition and summarization tasks . a series of empirical experiments reveal the feasibility of such an lm framework and the performance merits of the deduced models on these two tasks .

learning structured perceptrons for coreference resolution with latent antecedents and non-local features
we investigate different ways of learning structured perceptron models for coreference resolution when using non-local features and beam search . our experimental results indicate that standard techniques such as early updates or learning as search optimization ( laso ) perform worse than a greedy baseline that only uses local features . by modifying laso to delay updates until the end of each instance we obtain significant improvements over the baseline . our model obtains the best results to date on recent shared task data for arabic , chinese , and english .

annotating anaphoric and bridging relations with mmax
we present a tool for the annotation of anaphoric and bridging relations in a corpus of written texts . based on differences as well as similarities between these phenomena , we define an annotation scheme . we then implement the scheme within an annotation tool and demonstrate its use .

coarse-to-fine syntactic machine translation using language projections
the intersection of tree transducer-based translation models with n-gram language models results in huge dynamic programs for machine translation decoding . we propose a multipass , coarse-to-fine approach in which the language model complexity is incrementally introduced . in contrast to previous orderbased bigram-to-trigram approaches , we focus on encoding-based methods , which use a clustered encoding of the target language . across various encoding schemes , and for multiple language pairs , we show speed-ups of up to 50 times over single-pass decoding while improving bleu score . moreover , our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram-to-trigram decoder .

adding high-precision links to wikipedia thanapon noraset chandra bhagavatula doug downey
wikipedias link structure is a valuable resource for natural language processing tasks , but only a fraction of the concepts mentioned in each article are annotated with hyperlinks . in this paper , we study how to augment wikipedia with additional high-precision links . we present 3w , a system that identifies concept mentions in wikipedia text , and links each mention to its referent page . 3w leverages rich semantic information present in wikipedia to achieve high precision . our experiments demonstrate that 3w can add an average of seven new links to each wikipedia article , at a precision of 0.98 .

grounding language with points and paths in continuous spaces
we present a model for generating pathvalued interpretations of natural language text . our model encodes a map from natural language descriptions to paths , mediated by segmentation variables which break the language into a discrete set of events , and alignment variables which reorder those events . within an event , lexical weights capture the contribution of each word to the aligned path segment . we demonstrate the applicability of our model on three diverse tasks : a new color description task , a new financial news task and an established direction-following task . on all three , the model outperforms strong baselines , and on a hard variant of the direction-following task it achieves results close to the state-of-the-art system described in vogel and jurafsky ( 2010 ) .

single character chinese named entity recognition
single character named entity ( scne ) is a name entity ( ne ) composed of one chinese character , such as ( zhong1 , china ) and e2 , russia . scne is very common in written chinese text . however , due to the lack of in-depth research , scne is a major source of errors in named entity recognition ( ner ) . this paper formulates the scne recognition within the sourcechannel model framework . our experiments show very encouraging results : an fscore of 81.01 % for single character location name recognition , and an f-score of 68.02 % for single character person name recognition . an alternative view of the scne recognition problem is to formulate it as a classification task . we construct two classifiers based on maximum entropy model ( me ) and vector space model ( vsm ) , respectively . we compare all proposed approaches , showing that the sourcechannel model performs the best in most cases .

automatic dating of documents and temporal text classification
the frequency of occurrence of words in natural languages exhibits a periodic and a non-periodic component when analysed as a time series . this work presents an unsupervised method of extracting periodicity information from text , enabling time series creation and filtering to be used in the creation of sophisticated language models that can discern between repetitive trends and non-repetitive writing patterns . the algorithm performs in o ( n log n ) time for input of length n. the temporal language model is used to create rules based on temporal-word associations inferred from the time series . the rules are used to guess automatically at likely document creation dates , based on the assumption that natural languages have unique signatures of changing word distributions over time . experimental results on news items spanning a nine year period show that the proposed method and algorithms are accurate in discovering periodicity patterns and in dating documents automatically solely from their content .

probabilistic coordination disambiguation in a fully-lexicalized japanese parser
this paper describes a probabilistic model for coordination disambiguation integrated into syntactic and case structure analysis . our model probabilistically assesses the parallelism of a candidate coordinate structure using syntactic/semantic similarities and cooccurrence statistics . we integrate these probabilities into the framework of fully-lexicalized parsing based on largescale case frames . this approach simultaneously addresses two tasks of coordination disambiguation : the detection of coordinate conjunctions and the scope disambiguation of coordinate structures . experimental results on web sentences indicate the effectiveness of our approach .

recognizing stances in ideological on-line debates the intelligent systems program
this work explores the utility of sentiment and arguing opinions for classifying stances in ideological debates . in order to capture arguing opinions in ideological stance taking , we construct an arguing lexicon automatically from a manually annotated corpus . we build supervised systems employing sentiment and arguing opinions and their targets as features . our systems perform substantially better than a distribution-based baseline . additionally , by employing both types of opinion features , we are able to perform better than a unigrambased system .

multiword expression identification with tree substitution grammars :
multiword expressions ( mwe ) , a known nuisance for both linguistics and nlp , blur the lines between syntax and semantics . previous work onmwe identification has relied primarily on surface statistics , which perform poorly for longer mwes and can not model discontinuous expressions . to address these problems , we show that even the simplest parsing models can effectively identify mwes of arbitrary length , and that tree substitution grammars achieve the best results . our experiments show a 36.4 % f1 absolute improvement for french over an n-gram surface statistics baseline , currently the predominant method for mwe identification . our models are useful for several nlp tasks in which mwe pre-grouping has improved accuracy .

meant : an inexpensive , high-accuracy , semi-automatic metric for evaluating translation utility via semantic frames
we introduce a novel semi-automated metric , meant , that assesses translation utility by matching semantic role fillers , producing scores that correlate with human judgment as well as hter but at much lower labor cost . as machine translation systems improve in lexical choice and fluency , the shortcomings of widespread n-gram based , fluency-oriented mt evaluation metrics such as bleu , which fail to properly evaluate adequacy , become more apparent . but more accurate , nonautomatic adequacy-oriented mt evaluation metrics like hter are highly labor-intensive , which bottlenecks the evaluation cycle . we first show that when using untrained monolingual readers to annotate semantic roles in mt output , the non-automatic version of the metric hmeant achieves a 0.43 correlation coefficient with human adequacy judgments at the sentence level , far superior to bleu at only 0.20 , and equal to the far more expensive hter . we then replace the human semantic role annotators with automatic shallow semantic parsing to further automate the evaluation metric , and show that even the semiautomated evaluation metric achieves a 0.34 correlation coefficient with human adequacy judgment , which is still about 80 % as closely correlated as hter despite an even lower labor cost for the evaluation procedure . the results show that our proposed metric is significantly better correlated with human judgment on adequacy than current widespread automatic evaluation metrics , while being much more cost effective than hter .

automatic detection of opinion bearing words and sentences
we describe a sentence-level opinion detection system . we first define what an opinion means in our research and introduce an effective method for obtaining opinion-bearing and nonopinion-bearing words . then we describe recognizing opinion-bearing sentences using these words we test the system on 3 different test sets : mpqa data , an internal corpus , and the trec2003 novelty track data . we show that our automatic method for obtaining opinion-bearing words can be used effectively to identify opinion-bearing sentences .

focused training sets to reduce noise in ner feature models
feature and context aggregation play a large role in current ner systems , allowing significant opportunities for research into optimizing these features to cater to different domains . this work strives to reduce the noise introduced into aggregated features from disparate and generic training data in order to allow for contextual features that more closely model the entities in the target data . the proposed approach trains models based on only a part of the training set that is more similar to the target domain . to this end , models are trained for an existing ner system using the top documents from the training set that are similar to the target document in order to demonstrate that this technique can be applied to improve any pre-built ner system . initial results show an improvement over the university of illinois ne tagger with a weighted average f1 score of 91.67 compared to the illinois taggers score of 91.32. this research serves as a proof-of-concept for future planned work to cluster the training documents to produce a number of more focused models from a given training set , thereby reducing noise and extracting a more representative feature set .

generating synthetic children 's acoustic models from adult models
this work focuses on generating childrens hmm-based acoustic models for speech recognition from adult acoustic models . collecting childrens speech data is more costly compared to adults speech . the patentpending method developed in this work requires only adult data to estimate synthetic childrens acoustic models in any language and works as follows : for a new language where only adult data is available , an adult male and an adult female model is trained . a linear transformation from each male hmm mean vector to its closest female mean vector is estimated . this transform is then scaled to a certain power and applied to the female model to obtain a synthetic childrens model . in a pronunciation verification task the method yields 19 % and 3.7 % relative improvement on native english and spanish childrens data , respectively , compared to the best adult model . for spanish data , the new model outperforms the available real childrens data based model by 13 % relative .

deterministic statistical mapping of sentences to underspecified semantics
we present a method for training a statistical model for mapping natural language sentences to semantic expressions . the semantics are expressions of an underspecified logical form that has properties making it particularly suitable for statistical mapping from text . an encoding of the semantic expressions into dependency trees with automatically generated labels allows application of existing methods for statistical dependency parsing to the mapping task ( without the need for separate traditional dependency labels or parts of speech ) . the encoding also results in a natural per-word semantic-mapping accuracy measure . we report on the results of training and testing statistical models for mapping sentences of the penn treebank into the semantic expressions , for which per-word semantic mapping accuracy ranges between 79 % and 86 % depending on the experimental conditions . the particular choice of algorithms used also means that our trained mapping is deterministic ( in the sense of deterministic parsing ) , paving the way for large-scale text-to-semantic mapping .

the desirability of a corpus of online book responses the netherlands
this position paper argues the need for a comprehensive corpus of online book responses . responses to books ( in traditional reviews , book blogs , on booksellers sites , etc . ) are important for understanding how readers understand literature and how literary works become popular . a sufficiently large , varied and representative corpus of online responses to books will facilitate research into these processes . this corpus should include context information about the responses and should remain open to additional material . based on a pilot study for the creation of a corpus of dutch online book response , the paper shows how linguistic tools can find differences in word usage between responses from various sites . they can also reveal response type by clustering responses based on usage of either words or their pos-tags , and can show the sentiments expressed in the responses . lsa-based similarity between book fragments and response may be able to reveal the book fragments that most affected readers . the paper argues that a corpus of book responses can be an important instrument for research into reading behavior , reader response , book reviewing and literary appreciation .

recognizing stances in online debates
this paper presents an unsupervised opinion analysis method for debate-side classification , i.e. , recognizing which stance a person is taking in an online debate . in order to handle the complexities of this genre , we mine the web to learn associations that are indicative of opinion stances in debates . we combine this knowledge with discourse information , and formulate the debate side classification task as an integer linear programming problem . our results show that our method is substantially better than challenging baseline methods .

a method for automatic pos guessing of chinese unknown words tsinghua science park , zhongguancun
this paper proposes a method for automatic pos ( part-of-speech ) guessing of chinese unknown words . it contains two models . the first model uses a machinelearning method to predict the pos of unknown words based on their internal component features . the credibility of the results of the first model is then measured . for low-credibility words , the second model is used to revise the first models results based on the global context information of those words . the experiments show that the first model achieves 93.40 % precision for all words and 86.60 % for disyllabic words , which is a significant improvement over the best results reported in previous studies , which were 89 % precision for all words and 74 % for disyllabic words . further , the second model improves the results by 0.80 % precision for all words and 1.30 % for disyllabic words .

generating music from literature
we present a system , transprose , that automatically generates musical pieces from text . transprose uses known relations between elements of music such as tempo and scale , and the emotions they evoke . further , it uses a novel mechanism to determine sequences of notes that capture the emotional activity in text . the work has applications in information visualization , in creating audio-visual e-books , and in developing music apps .

automatic part-of-speech tagging for bengali : an approach for morphologically rich languages in a poor resource scenario
this paper describes our work on building part-of-speech ( pos ) tagger for bengali . we have use hidden markov model ( hmm ) and maximum entropy ( me ) based stochastic taggers . bengali is a morphologically rich language and our taggers make use of morphological and contextual information of the words . since only a small labeled training set is available ( 45,000 words ) , simple stochastic approach does not yield very good results . in this work , we have studied the effect of using a morphological analyzer to improve the performance of the tagger . we find that the use of morphology helps improve the accuracy of the tagger especially when less amount of tagged corpora are available .

knowledge-based question answering as machine translation
a typical knowledge-based question answering ( kb-qa ) system faces two challenges : one is to transform natural language questions into their meaning representations ( mrs ) ; the other is to retrieve answers from knowledge bases ( kbs ) using generated mrs. unlike previous methods which treat them in a cascaded manner , we present a translation-based approach to solve these two tasks in one unified framework . we translate questions to answers based on cyk parsing . answers as translations of the span covered by each cyk cell are obtained by a question translation method , which first generates formal triple queries as mrs for the span based on question patterns and relation expressions , and then retrieves answers from a given kb based on triple queries generated . a linear model is defined over derivations , and minimum error rate training is used to tune feature weights based on a set of question-answer pairs . compared to a kb-qa system using a state-of-the-art semantic parser , our method achieves better results .

examining the effect of improved context sensitive morphology on arabic information retrieval
this paper explores the effect of improved morphological analysis , particularly context sensitive morphology , on monolingual arabic information retrieval ( ir ) . it also compares the effect of context sensitive morphology to noncontext sensitive morphology . the results show that better coverage and improved correctness have a dramatic effect on ir effectiveness and that context sensitive morphology further improves retrieval effectiveness , but the improvement is not statistically significant . furthermore , the improvement obtained by the use of context sensitive morphology over the use of light stemming was not significantly significant .

improving the recognizability of syntactic relations
a common task in qualitative data analysis is to characterize the usage of a linguistic entity by issuing queries over syntactic relations between words . previous interfaces for searching over syntactic structures require programming-style queries . user interface research suggests that it is easier to recognize a pattern than to compose it from scratch ; therefore , interfaces for non-experts should show previews of syntactic relations . what these previews should look like is an open question that we explored with a 400-participant mechanical turk experiment . we found that syntactic relations are recognized with 34 % higher accuracy when contextual examples are shown than a baseline of naming the relations alone . this suggests that user interfaces should display contextual examples of syntactic relations to help users choose between different relations .

amazon mechanical turk for subjectivity word sense disambiguation
amazon mechanical turk ( mturk ) is a marketplace for so-called human intelligence tasks ( hits ) , or tasks that are easy for humans but currently difficult for automated processes . providers upload tasks to mturk which workers then complete . natural language annotation is one such human intelligence task . in this paper , we investigate using mturk to collect annotations for subjectivity word sense disambiguation ( swsd ) , a coarse-grained word sense disambiguation task . we investigate whether we can use mturk to acquire good annotations with respect to gold-standard data , whether we can filter out low-quality workers ( spammers ) , and whether there is a learning effect associated with repeatedly completing the same kind of task . while our results with respect to spammers are inconclusive , we are able to obtain high-quality annotations for the swsd task . these results suggest a greater role for mturk with respect to constructing a large scale swsd system in the future , promising substantial improvement in subjectivity and sentiment analysis .

learning to differentiate better from worse translations an shafiq joty llus m alessandro moschitti preslav nakov massimo nicosia
we present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference . we integrate several layers of linguistic information encapsulated in tree-based structures , making use of both the reference and the system output simultaneously , thus bringing our ranking closer to how humans evaluate translations . most importantly , instead of deciding upfront which types of features are important , we use the learning framework of preference re-ranking kernels to learn the features automatically . the evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures . also , we show our structural kernel learning ( skl ) can be a general framework for mt evaluation , in which syntactic and semantic information can be naturally incorporated .

a joint model for quotation attribution and coreference resolution
we address the problem of automatically attributing quotations to speakers , which has great relevance in text mining and media monitoring applications . while current systems report high accuracies for this task , they either work at mentionlevel ( getting credit for detecting uninformative mentions such as pronouns ) , or assume the coreferent mentions have been detected beforehand ; the inaccuracies in this preprocessing step may lead to error propagation . in this paper , we introduce a joint model for entity-level quotation attribution and coreference resolution , exploiting correlations between the two tasks . we design an evaluation metric for attribution that captures all speakers mentions . we present results showing that both tasks benefit from being treated jointly .

the manually annotated sub-corpus : a community resource for and by the people
the manually annotated sub-corpus ( masc ) project provides data and annotations to serve as the base for a communitywide annotation effort of a subset of the american national corpus . the masc infrastructure enables the incorporation of contributed annotations into a single , usable format that can then be analyzed as it is or ported to any of a variety of other formats . masc includes data from a much wider variety of genres than existing multiply-annotated corpora of english , and the project is committed to a fully open model of distribution , without restriction , for all data and annotations produced or contributed . as such , masc is the first large-scale , open , communitybased effort to create much needed language resources for nlp . this paper describes the masc project , its corpus and annotations , and serves as a call for contributions of data and annotations from the language processing community .

senselearner : minimally supervised word sense disambiguation for all words in open text
this paper introduces senselearner a minimally supervised sense tagger that attempts to disambiguate all content words in a text using the senses from wordnet . senselearner participated in the senseval-3 english all words task , and achieved an average accuracy of 64.6 % .

chinese sentence-level sentiment classification based on fuzzy sets
this paper presents a fuzzy set theory based approach to chinese sentence-level sentiment classification . compared with traditional topic-based text classification techniques , the fuzzy set theory provides a straightforward way to model the intrinsic fuzziness between sentiment polarity classes . to approach fuzzy sentiment classification , we first propose a fine-to-coarse strategy to estimate sentence sentiment intensity . then , we define three fuzzy sets to represent the respective sentiment polarity classes , namely positive , negative and neutral sentiments . based on sentence sentiment intensities , we further build membership functions to indicate the degrees of an opinionated sentence in different fuzzy sets . finally , we determine sentence-level polarity under maximum membership principle . we show that our approach can achieve promising performance on the test set for chinese opinion analysis pilot task at ntcir-6 .

talp phrase-based statistical translation system for european language
this paper reports translation results for the exploiting parallel texts for statistical machine translation ( hlt-naacl workshop on parallel texts 2006 ) . we have studied different techniques to improve the standard phrase-based translation system . mainly we introduce two reordering approaches and add morphological information .

classification of atypical language in autism
atypical or idiosyncratic language is a characteristic of autism spectrum disorder ( asd ) . in this paper , we discuss previous work identifying language errors associated with atypical language in asd and describe a procedure for reproducing those results . we describe our data set , which consists of transcribed data from a widely used clinical diagnostic instrument ( the ados ) for children with autism , children with developmental language disorder , and typically developing children . we then present methods for automatically extracting lexical and syntactic features from transcripts of childrens speech to 1 ) identify certain syntactic and semantic errors that have previously been found to distinguish asd language from that of children with typical development ; and 2 ) perform diagnostic classification . our classifiers achieve results well above chance , demonstrating the potential for using nlp techniques to enhance neurodevelopmental diagnosis and atypical language analysis . we expect further improvement with additional data , features , and classification techniques .

using the web as an implicit training set : application to structural ambiguity resolution
recent work has shown that very large corpora can act as training data for nlp algorithms even without explicit labels . in this paper we show how the use of surface features and paraphrases in queries against search engines can be used to infer labels for structural ambiguity resolution tasks . using unsupervised algorithms , we achieve 84 % precision on pp-attachment and 80 % on noun compound coordination .

optimizing features in active machine learning for complex qualitative content analysis jasy liew suet yan
we propose a semi-automatic approach for content analysis that leverages machine learning ( ml ) being initially trained on a small set of hand-coded data to perform a first pass in coding , and then have human annotators correct machine annotations in order to produce more examples to retrain the existing model incrementally for better performance . in this active learning approach , it is equally important to optimize the creation of the initial ml model given less training data so that the model is able to capture most if not all positive examples , and filter out as many negative examples as possible for human annotators to correct . this paper reports our attempt to optimize the initial ml model through feature exploration in a complex content analysis project that uses a multidimensional coding scheme , and contains codes with sparse positive examples . while different codes respond optimally to different combinations of features , we show that it is possible to create an optimal initial ml model using only a single combination of features for codes with at least 100 positive examples in the gold standard corpus .

a flexemic tagset for polish
the article notes certain weaknesses of current efforts aiming at the standardization of pos tagsets for morphologically rich languages and argues that , in order to achieve clear mappings between tagsets , it is necessary to have clear and formal rules of delimiting poss and grammatical categories within any given tagset . an attempt at constructing such a tagset for polish is presented .

eu-bridge mt : combined machine translation
this paper describes one of the collaborative efforts within eu-bridge to further advance the state of the art in machine translation between two european language pairs , germanenglish and englishgerman . three research institutes involved in the eu-bridge project combined their individual machine translation systems and participated with a joint setup in the shared translation task of the evaluation campaign at the acl 2014 eighth workshop on statistical machine translation ( wmt 2014 ) . we combined up to nine different machine translation engines via system combination . rwth aachen university , the university of edinburgh , and karlsruhe institute of technology developed several individual systems which serve as system combination input . we devoted special attention to building syntax-based systems and combining them with the phrasebased ones . the joint setups yield empirical gains of up to 1.6 points in bleu and 1.0 points in ter on the wmt newstest2013 test set compared to the best single systems .

unsupervised discovery of generic relationships using pattern clusters and its evaluation by automatically generated sat analogy questions
we present a novel framework for the discovery and representation of general semantic relationships that hold between lexical items . we propose that each such relationship can be identified with a cluster of patterns that captures this relationship . we give a fully unsupervised algorithm for pattern cluster discovery , which searches , clusters and merges highfrequency words-based patterns around randomly selected hook words . pattern clusters can be used to extract instances of the corresponding relationships . to assess the quality of discovered relationships , we use the pattern clusters to automatically generate sat analogy questions . we also compare to a set of known relationships , achieving very good results in both methods . the evaluation ( done in both english and russian ) substantiates the premise that our pattern clusters indeed reflect relationships perceived by humans .

factorization of synchronous context-free grammars in linear time
factoring a synchronous context-free grammar into an equivalent grammar with a smaller number of nonterminals in each rule enables synchronous parsing algorithms of lower complexity . the problem can be formalized as searching for the tree-decomposition of a given permutation with the minimal branching factor . in this paper , by modifying the algorithm of uno and yagiura ( 2000 ) for the closely related problem of finding all common intervals of two permutations , we achieve a linear time algorithm for the permutation factorization problem . we also use the algorithm to analyze the maximum scfg rule length needed to cover hand-aligned data from various language pairs .

chinese word segmentation based on mixing multiple preprocessor
this paper describes the chinese word segmenter for our participation in cipssighan-2010 bake-off task of chinese word segmentation . we formalize the tasks as sequence tagging problems , and implemented them using conditional random fields ( crfs ) model . the system contains two modules : multiple preprocessor and basic segmenter . the basic segmenter is designed as a problem of character-based tagging , and using named entity recognition and chunk recognition based on boundary to preprocess . we participated in the open training on simplified chinese text and traditional chinese text , and our system achieved one rank # 5 and four rank # 2 best in all four domain corpus .

evaluation for partial event coreference jun araki eduard hovy teruko mitamura
this paper proposes an evaluation scheme to measure the performance of a system that detects hierarchical event structure for event coreference resolution . we show that each system output is represented as a forest of unordered trees , and introduce the notion of conceptual event hierarchy to simplify the evaluation process . we enumerate the desiderata for a similarity metric to measure the system performance . we examine three metrics along with the desiderata , and show that metrics extended from muc and blanc are more adequate than a metric based on simple tree matching .

feature-rich translation by quasi-synchronous lattice parsing
we present a machine translation framework that can incorporate arbitrary features of both input and output sentences . the core of the approach is a novel decoder based on lattice parsing with quasisynchronous grammar ( smith and eisner , 2006 ) , a syntactic formalism that does not require source and target trees to be isomorphic . using generic approximate dynamic programming techniques , this decoder can handle non-local features . similar approximate inference techniques support efficient parameter estimation with hidden variables . we use the decoder to conduct controlled experiments on a german-to-english translation task , to compare lexical phrase , syntax , and combined models , and to measure effects of various restrictions on nonisomorphism .

what to do when lexicalization fails : parsing german with suffix analysis
in this paper , we present an unlexicalized parser for german which employs smoothing and suffix analysis to achieve a labelled bracket f-score of 76.2 , higher than previously reported results on the negra corpus . in addition to the high accuracy of the model , the use of smoothing in an unlexicalized parser allows us to better examine the interplay between smoothing and parsing results .

context-based message expansion for disentanglement of interleaved text conversations
computational processing of text exchanged in interactive venues in which participants engage in simultaneous conversations can benefit from techniques for automatically grouping overlapping sequences of messages into separate conversations , a problem known as disentanglement . while previous methods exploit both lexical and non-lexical information that exists in conversations for this task , the inter-dependency between the meaning of a message and its temporal and social contexts is largely ignored . our approach exploits contextual properties ( both explicit and hidden ) to probabilistically expand each message to provide a more accurate message representation . extensive experimental evaluations show our approach outperforms the best previously known technique .

web-based list question answering
while research on question answering has become popular in recent years , the problem of efficiently locating a complete set of distinct answers to list questions in huge corpora or the web is still far from being solved . this paper exploits the wealth of freely available text and link structures on the web to seek complete answers to list questions . we introduce our system , fada , which relies on question parsing , web page classification/clustering , and content extraction to find reliable distinct answers with high recall .

dlut : chinese personal name disambiguation with rich
in this paper we describe a person clustering system for a given document set and report the results we have obtained on the test set of chinese personal name ( cpn ) disambiguation task of cipssighan 2010. this task consists of clustering a set of xinhua news documents that mention an ambiguous cpn according to named entity in reality . several features including named entities ( ne ) and common nouns generated from the documents and a variety of rules are employed in our system . this system achieves f = 86.36 % with b_cubed scoring metrics and f = 90.78 % with purity_based metrics .

automated whole sentence grammar correction using a noisy channel
automated grammar correction techniques have seen improvement over the years , but there is still much room for increased performance . current correction techniques mainly focus on identifying and correcting a specific type of error , such as verb form misuse or preposition misuse , which restricts the corrections to a limited scope . we introduce a novel technique , based on a noisy channel model , which can utilize the whole sentence context to determine proper corrections . we show how to use the em algorithm to learn the parameters of the noise model , using only a data set of erroneous sentences , given the proper language model . this frees us from the burden of acquiring a large corpora of corrected sentences . we also present a cheap and efficient way to provide automated evaluation results for grammar corrections by using bleu and meteor , in contrast to the commonly used manual evaluations .

the true score of statistical paraphrase generation
this article delves into the scoring function of the statistical paraphrase generation model . it presents an algorithm for exact computation and two applicative experiments . the first experiment analyses the behaviour of a statistical paraphrase generation decoder , and raises some issues with the ordering of n-best outputs . the second experiment shows that a major boost of performance can be obtained by embedding a true score computation inside a monte-carlo sampling based paraphrase generator .

the utility of a graphical representation of discourse structure in spoken dialogue systems
in this paper we explore the utility of the navigation map ( nm ) , a graphical representation of the discourse structure . we run a user study to investigate if users perceive the nm as helpful in a tutoring spoken dialogue system . from the users perspective , our results show that the nm presence allows them to better identify and follow the tutoring plan and to better integrate the instruction . it was also easier for users to concentrate and to learn from the system if the nm was present . our preliminary analysis on objective metrics further strengthens these findings .

cross language resource sharing
language resource development is crucial for language study in current approaches . many efforts have been made to model a language on very large scaled corpora . statistical and probabilistic approaches are playing a major role in taking the advantage of incorporating the context to improve their performance to a promising result in many areas of natural language processing such as machine translation , parsing , pos tagging , morphological analysis , etc . it is believed that if there are sufficient corpora for a language , we can develop many efficient language processing applications within an expectable period . however , corpus development is a labor intensive task and requires a continuous effort in maintaining the result to such a qualified level . the problem is magnified when we need to deal with the less computerized languages . the availability of the computerized language data can be varied by the availability of the standard of language encoding , number of speakers , economic scale of the speakers , and the language supporting tools . as a result , the technology gap between languages are widened as we can see in the evidence of online language populations and web contents which are mainly occupied by english , and others major languages distributed in chinese , spanish , japanese , german and french . the major concern in the less computerized languages is how to leverage the technology for those languages which will result in scaling up the number of online language populations . cross language resource sharing is one of the efforts to increase the opportunity for the access to those languages .

investigating connectivity and consistency criteria for phrase pair extraction in statistical machine translation spyros martzoukos , christophe costa florencio and christof monz
the consistency method has been established as the standard strategy for extracting high quality translation rules in statistical machine translation ( smt ) . however , no attention has been drawn to why this method is successful , other than empirical evidence . using concepts from graph theory , we identify the relation between consistency and components of graphs that represent word-aligned sentence pairs . it can be shown that phrase pairs of interest to smt form a sigma-algebra generated by components of such graphs . this construction is generalized by allowing segmented sentence pairs , which in turn gives rise to a phrase-based generative model . a by-product of this model is a derivation of probability mass functions for random partitions . these are realized as cases of constrained , biased sampling without replacement and we provide an exact formula for the probability of a segmentation of a sentence .

on jointly recognizing and aligning bilingual named entities behavior design corporation
we observe that ( 1 ) how a given named entity ( ne ) is translated ( i.e. , either semantically or phonetically ) depends greatly on its associated entity type , and ( 2 ) entities within an aligned pair should share the same type . also , ( 3 ) those initially detected nes are anchors , whose information should be used to give certainty scores when selecting candidates . from this basis , an integrated model is thus proposed in this paper to jointly identify and align bilingual named entities between chinese and english . it adopts a new mapping type ratio feature ( which is the proportion of ne internal tokens that are semantically translated ) , enforces an entity type consistency constraint , and utilizes additional monolingual candidate certainty factors ( based on those ne anchors ) . the experiments show that this novel approach has substantially raised the type-sensitive f-score of identified ne-pairs from 68.4 % to 81.7 % ( 42.1 % f-score imperfection reduction ) in our chinese-english ne alignment task .

automated planning for situated natural language generation cluster of excellence multimodal computing and interaction
we present a natural language generation approach which models , exploits , and manipulates the non-linguistic context in situated communication , using techniques from ai planning . we show how to generate instructions which deliberately guide the hearer to a location that is convenient for the generation of simple referring expressions , and how to generate referring expressions with context-dependent adjectives . we implement and evaluate our approach in the framework of the challenge on generating instructions in virtual environments , finding that it performs well even under the constraints of realtime generation .

better alignments = better translations computer & information science computer & information science
automatic word alignment is a key step in training statistical machine translation systems . despite much recent work on word alignment methods , alignment accuracy increases often produce little or no improvements in machine translation quality . in this work we analyze a recently proposed agreement-constrained em algorithm for unsupervised alignment models . we attempt to tease apart the effects that this simple but effective modification has on alignment precision and recall trade-offs , and how rare and common words are affected across several language pairs . we propose and extensively evaluate a simple method for using alignment models to produce alignments better-suited for phrase-based mt systems , and show significant gains ( as measured by bleu score ) in end-to-end translation systems for six languages pairs used in recent mt competitions .

discovering morphological paradigms from plain text using a dirichlet process mixture model sdl language weaver
we present an inference algorithm that organizes observed words ( tokens ) into structured inflectional paradigms ( types ) . it also naturally predicts the spelling of unobserved forms that are missing from these paradigms , and discovers inflectional principles ( grammar ) that generalize to wholly unobserved words . our bayesian generative model of the data explicitly represents tokens , types , inflections , paradigms , and locally conditioned string edits . it assumes that inflected word tokens are generated from an infinite mixture of inflectional paradigms ( string tuples ) . each paradigm is sampled all at once from a graphical model , whose potential functions are weighted finitestate transducers with language-specific parameters to be learned . these assumptions naturally lead to an elegant empirical bayes inference procedure that exploits monte carlo em , belief propagation , and dynamic programming . given 50100 seed paradigms , adding a 10million-word corpus reduces prediction error for morphological inflections by up to 10 % .

statistical ranking in tactical generation and csli stanford ( ca )
in this paper we describe and evaluate several statistical models for the task of realization ranking , i.e . the problem of discriminating between competing surface realizations generated for a given input semantics . three models ( and several variants ) are trained and tested : an n-gram language model , a discriminative maximum entropy model using structural information ( and incorporating the language model as a separate feature ) , and finally an svm ranker trained on the same feature set . the resulting hybrid tactical generator is part of a larger , semantic transfer mt system .

using a random forest classifier to recognise translations of biomedical terms across languages
we present a novel method to recognise semantic equivalents of biomedical terms in language pairs . we hypothesise that biomedical term are formed by semantically similar textual units across languages . based on this hypothesis , we employ a random forest ( rf ) classifier that is able to automatically mine higher order associations between textual units of the source and target language when trained on a corpus of both positive and negative examples . we apply our method on two language pairs : one that uses the same character set and another with a different script , english-french and englishchinese , respectively . we show that english-french pairs of terms are highly transliterated in contrast to the englishchinese pairs . nonetheless , our method performs robustly on both cases . we evaluate rf against a state-of-the-art alignment method , giza++ , and we report a statistically significant improvement . finally , we compare rf against support vector machines and analyse our results .

measuring lexical cohesion : beyond word repetition
this paper considers the problem of finding topical shifts in documents and in particular at what information can be leveraged to identify them . recent research on topical segmentation usually assumes that topical shifts in discourse are signalled by changes in vocabulary . this information , however , is not always a sufficient indicator of a topical shift , especially for certain genres . this paper explores an additional source of information . our hypothesis is that the type of a referring expression is an indicator of how accessible its antecedent is . the shorter and less informative the expression ( e.g. , a personal pronoun versus a lengthy post-modified noun phrase ) , the more accessible the antecedent is likely to be and the more likely it is that the topic under discussion has remained constant between the two mentions . we explore how this information can be used to augment a lexically-based topical segmenter . we test our hypothesis on two types of data , literary narratives and lecture notes . the results suggest that our similarity metric is useful : depending on the settings it either slightly improves the performance or leaves it unchanged . they also suggest that certain types of referring expressions are more useful than others .

graph connectivity measures for unsupervised parameter tuning of graph-based sense induction systems
word sense induction ( wsi ) is the task of identifying the different senses ( uses ) of a target word in a given text . this paper focuses on the unsupervised estimation of the free parameters of a graph-based wsi method , and explores the use of eight graph connectivity measures ( gcm ) that assess the degree of connectivity in a graph . given a target word and a set of parameters , gcm evaluate the connectivity of the produced clusters , which correspond to subgraphs of the initial ( unclustered ) graph . each parameter setting is assigned a score according to one of the gcm and the highest scoring setting is then selected . our evaluation on the nouns of semeval-2007 wsi task ( swsi ) shows that : ( 1 ) all gcm estimate a set of parameters which significantly outperform the worst performing parameter setting in both swsi evaluation schemes , ( 2 ) all gcm estimate a set of parameters which outperform the most frequent sense ( mfs ) baseline by a statistically significant amount in the supervised evaluation scheme , and ( 3 ) two of the measures estimate a set of parameters that performs closely to a set of parameters estimated in supervised manner .

improving coreference resolution by using conversational metadata
in this paper , we propose the use of metadata contained in documents to improve coreference resolution . specifically , we quantify the impact of speaker and turn information on the performance of our coreference system , and show that the metadata can be effectively encoded as features of a statistical resolution system , which leads to a statistically significant improvement in performance .

unsupervised all-words word sense disambiguation with grammatical dependencies
we present experiments that analyze the necessity of using a highly interconnected word/sense graph for unsupervised allwords word sense disambiguation . we show that allowing only grammatically related words to influence each others senses leads to disambiguation results on a par with the best graph-based systems , while greatly reducing the computation load . we also compare two methods for computing selectional preferences between the senses of every two grammatically related words : one using a lesk-based measure on wordnet , the other using dependency relations from the british national corpus . the best configuration uses the syntactically-constrained graph , selectional preferences computed from the corpus and a pagerank tie-breaking algorithm . we especially note good performance when disambiguating verbs with grammatically constrained links .

entity linking meets word sense disambiguation : a unified approach
entity linking ( el ) and word sense disambiguation ( wsd ) both address the lexical ambiguity of language . but while the two tasks are pretty similar , they differ in a fundamental respect : in el the textual mention can be linked to a named entity which may or may not contain the exact mention , while in wsd there is a perfect match between the word form ( better , its lemma ) and a suitable word sense . in this paper we present babelfy , a unified graph-based approach to el and wsd based on a loose identification of candidate meanings coupled with a densest subgraph heuristic which selects high-coherence semantic interpretations . our experiments show state-ofthe-art performances on both tasks on 6 different datasets , including a multilingual setting .

parallel distributed grammar engineering for practical applications
based on a detailed case study of parallel grammar development distributed across two sites , we review some of the requirements for regression testing in grammar engineering , summarize our approach to systematic competence and performance profiling , and discuss our experience with grammar development for a commercial application . if possible , the workshop presentation will be organized around a software demonstration .

language technology for agile social media science
we present an extension of the dualist tool that enables social scientists to engage directly with large twitter datasets . our approach supports collaborative construction of classifiers and associated gold standard data sets . the tool can be used to build classifier cascades that decomposes tweet streams , and provide analysis of targeted conversations . a central concern is to provide an environment in which social science researchers can rapidly develop an informed sense of what the datasets look like . the intent is that they develop , not only an informed view as to how the data could be fruitfully analysed , but also how feasible it is to analyse it in that way .

a cognitive cost model of annotations based on eye-tracking data language & information language & information applied cognitive science applied cognitive science
we report on an experiment to track complex decision points in linguistic metadata annotation where the decision behavior of annotators is observed with an eyetracking device . as experimental conditions we investigate different forms of textual context and linguistic complexity classes relative to syntax and semantics . our data renders evidence that annotation performance depends on the semantic and syntactic complexity of the decision points and , more interestingly , indicates that fullscale context is mostly negligible with the exception of semantic high-complexity cases . we then induce from this observational data a cognitively grounded cost model of linguistic meta-data annotations and compare it with existing non-cognitive models . our data reveals that the cognitively founded model explains annotation costs ( expressed in annotation time ) more adequately than non-cognitive ones .

ensemble models for dependency parsing :
previous work on dependency parsing used various kinds of combination models but a systematic analysis and comparison of these approaches is lacking . in this paper we implemented such a study for english dependency parsing and find several non-obvious facts : ( a ) the diversity of base parsers is more important than complex models for learning ( e.g. , stacking , supervised meta-classification ) , ( b ) approximate , linear-time re-parsing algorithms guarantee well-formed dependency trees without significant performance loss , and ( c ) the simplest scoring model for re-parsing ( unweighted voting ) performs essentially as well as other more complex models . this study proves that fast and accurate ensemble parsers can be built with minimal effort .

evaluating a trainable sentence planner for a spoken dialogue system
techniques for automatically training modules of a natural language generator have recently been proposed , but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rulebased approaches . in this paper we experimentally evaluate a trainable sentence planner for a spoken dialogue system by eliciting subjective human judgments . in order to perform an exhaustive comparison , we also evaluate a hand-crafted template-based generation component , two rule-based sentence planners , and two baseline sentence planners . we show that the trainable sentence planner performs better than the rule-based systems and the baselines , and as well as the handcrafted system .

empirical acquisition of differentiating relations from definitions
this paper describes a new automatic approach for extracting conceptual distinctions from dictionary definitions . a broad-coverage dependency parser is first used to extract the lexical relations from the definitions . then the relations are disambiguated using associations learned from tagged corpora . this contrasts with earlier approaches using manually developed rules for disambiguation .

compiling french-japanese terminologies from the web
we propose a method for compiling bilingual terminologies of multi-word terms ( mwts ) for given translation pairs of seed terms . traditional methods for bilingual terminology compilation exploit parallel texts , while the more recent ones have focused on comparable corpora . we use bilingual corpora collected from the web and tailor made for the seed terms . for each language , we extract from the corpus a set of mwts pertaining to the seeds semantic domain , and use a compositional method to align mwts from both sets . we increase the coverage of our system by using thesauri and by applying a bootstrap method . experimental results show high precision and indicate promising prospects for future developments .

blanc learning evalutation metrics for mt
we introduce blanc , a family of dynamic , trainable evaluation metrics for machine translation . flexible , parametrized models can be learned from past data and automatically optimized to correlate well with human judgments for different criteria ( e.g . adequacy , fluency ) using different correlation measures . towards this end , we discuss acs ( all common skipngrams ) , a practical algorithm with trainable parameters that estimates referencecandidate translation overlap by computing a weighted sum of all common skipngrams in polynomial time . we show that the bleu and rouge metric families are special cases of blanc , and we compare correlations with human judgments across these three metric families . we analyze the algorithmic complexity of acs and argue that it is more powerful in modeling both local meaning and sentence-level structure , while offering the same practicality as the established algorithms it generalizes .

parsing screenplays for extracting social networks from movies
in this paper , we present a formalization of the task of parsing movie screenplays . while researchers have previously motivated the need for parsing movie screenplays , to the best of our knowledge , there is no work that has presented an evaluation for the task . moreover , all the approaches in the literature thus far have been regular expression based . in this paper , we present an nlp and ml based approach to the task , and show that this approach outperforms the regular expression based approach by a large and statistically significant margin . one of the main challenges we faced early on was the absence of training and test data . we propose a methodology for using well structured screenplays to create training data for anticipated anomalies in the structure of screenplays .

using distributional similarity of multi-way translations to predict multiword expression compositionality
we predict the compositionality of multiword expressions using distributional similarity between each component word and the overall expression , based on translations into multiple languages . we evaluate the method over english noun compounds , english verb particle constructions and german noun compounds . we show that the estimation of compositionality is improved when using translations into multiple languages , as compared to simply using distributional similarity in the source language . we further find that string similarity complements distributional similarity .

for multi-document update summarization and marc el-b
we present smmr , a scalable sentence scoring method for query-oriented update summarization . sentences are scored thanks to a criterion combining query relevance and dissimilarity with already read documents ( history ) . as the amount of data in history increases , non-redundancy is prioritized over query-relevance . we show that smmr achieves promising results on the duc 2007 update corpus .

jointly combining implicit constraints improves temporal ordering
previous work on ordering events in text has typically focused on local pairwise decisions , ignoring globally inconsistent labels . however , temporal ordering is the type of domain in which global constraints should be relatively easy to represent and reason over . this paper presents a framework that informs local decisions with two types of implicit global constraints : transitivity ( a before b and b before c implies a before c ) and time expression normalization ( e.g . last month is before yesterday ) . we show how these constraints can be used to create a more densely-connected network of events , and how global consistency can be enforced by incorporating these constraints into an integer linear programming framework . we present results on two event ordering tasks , showing a 3.6 % absolute increase in the accuracy of before/after classification over a pairwise model .

an incremental model for coreference resolution with restrictive
we introduce an incremental model for coreference resolution that competed in the conll 2011 shared task ( open regular ) . we decided to participate with our baseline model , since it worked well with two other datasets . the benefits of an incremental over a mention-pair architecture are : a drastic reduction of the number of candidate pairs , a means to overcome the problem of underspecified items in pairwise classification and the natural integration of global constraints such as transitivity . we do not apply machine learning , instead the system uses an empirically derived salience measure based on the dependency labels of the true mentions . our experiments seem to indicate that such a system already is on par with machine learning approaches .

facilitating translation using source language paraphrase lattices
for resource-limited language pairs , coverage of the test set by the parallel corpus is an important factor that affects translation quality in two respects : 1 ) out of vocabulary words ; 2 ) the same information in an input sentence can be expressed in different ways , while current phrase-based smt systems can not automatically select an alternative way to transfer the same information . therefore , given limited data , in order to facilitate translation from the input side , this paper proposes a novel method to reduce the translation difficulty using source-side lattice-based paraphrases . we utilise the original phrases from the input sentence and the corresponding paraphrases to build a lattice with estimated weights for each edge to improve translation quality . compared to the baseline system , our method achieves relative improvements of 7.07 % , 6.78 % and 3.63 % in terms of bleu score on small , medium and largescale english-to-chinese translation tasks respectively . the results show that the proposed method is effective not only for resourcelimited language pairs , but also for resourcesufficient pairs to some extent .

skill inference with personal and skill connections
personal skill information on social media is at the core of many interesting applications . in this paper , we propose a factor graph based approach to automatically infer skills from personal profile incorporated with both personal and skill connections . we first extract personal connections with similar academic and business background ( e.g . co-major , co-university , and co-corporation ) . we then extract skill connections between skills from the same person . to well integrate various kinds of connections , we propose a joint prediction factor graph ( jpfg ) model to collectively infer personal skills with help of personal connection factor , skill connection factor , besides the normal textual attributes . evaluation on a large-scale dataset from linkedin.com validates the effectiveness of our approach .

a flexible pragmatics-driven language generator for animated agents
this paper describes the neca mnlg ; a fully implemented multimodal natural language generation module . the mnlg is deployed as part of the neca system which generates dialogues between animated agents . the generation module supports the seamless integration of full grammar rules , templates and canned text . the generator takes input which allows for the specification of syntactic , semantic and pragmatic constraints on the output .

estimating annotation cost for active learning in a multi-annotator environment
we present an empirical investigation of the annotation cost estimation task for active learning in a multi-annotator environment . we present our analysis from two perspectives : selecting examples to be presented to the user for annotation ; and evaluating selective sampling strategies when actual annotation cost is not available . we present our results on a movie review classification task with rationale annotations . we demonstrate that a combination of instance , annotator and annotation task characteristics are important for developing an accurate estimator , and argue that both correlation coefficient and root mean square error should be used for evaluating annotation cost estimators .

non-expert correction of automatically generated relation annotations
we explore a new way to collect human annotated relations in text using amazon mechanical turk . given a knowledge base of relations and a corpus , we identify sentences which mention both an entity and an attribute that have some relation in the knowledge base . each noisy sentence/relation pair is presented to multiple turkers , who are asked whether the sentence expresses the relation . we describe a design which encourages user efficiency and aids discovery of cheating . we also present results on inter-annotator agreement .

collective content selection for concept-to-text generation
a content selection component determines which information should be conveyed in the output of a natural language generation system . we present an efficient method for automatically learning content selection rules from a corpus and its related database . our modeling framework treats content selection as a collective classification problem , thus allowing us to capture contextual dependencies between input items . experiments in a sports domain demonstrate that this approach achieves a substantial improvement over context-agnostic methods .

first joint workshop on statistical parsing of morphologically rich languages exploring options for fast domain adaptation of dependency parsers
the paper explores different domain-independent techniques to adapt a dependency parser trained on a general-language corpus to parse web texts ( online reviews , newsgroup posts , weblogs ) : co-training , word clusters , and a crowd-sourced dictionary . we examine the relative utility of these techniques as well as different ways to put them together to achieve maximum parsing accuracy . while we find that co-training and word clusters produce the most promising results , there is little additive improvement when combining the two techniques , which suggests that in the absence of large grammatical discrepancies between the training and test domains , they address largely the same problem , that of unknown vocabulary , with word clusters being a somewhat more effective solution for it . our highest results were achieved by a combination of word clusters and co-training , significantly improving on the baseline , by up to 1.67 % . evaluation of the best configurations on the sancl-2012 test data ( petrov and mcdonald , 2012 ) showed that they outperform all the shared task submissions that used a single parser to parse test data , averaging the results across all the test sets .

chinese deterministic dependency analyzer : examining effects of global features and root node finder
we present a method for improving dependency structure analysis of chinese . our bottom-up deterministic analyzer adopt nivres algorithm ( nivre and scholz , 2004 ) . support vector machines ( svms ) are utilized to determine the word dependency relations . we find that there are two problems in our analyzer and propose two methods to solve them . one problem is that some operations can not be solved only using local feature . we utilize the global features to solve this . the other problem is that this bottom-up analyzer doesnt use top-down information . we supply the top-down information by constructing svms based root node finder to solve this problem . experimental evaluation on the penn chinese treebank corpus shows that the proposed extensions improve the parsing accuracy significantly .

mining metalinguistic activity in corpora to create lexical resources using information extraction techniques : the mop system carlos rodrguez penagos
this paper describes and evaluates mop , an ie system for automatic extraction of metalinguistic information from technical and scientific documents . we claim that such a system can create special databases to bootstrap compilation and facilitate update of the huge and dynamically changing glossaries , knowledge bases and ontologies that are vital to modern-day research .

fine-grained classification of named entities exploiting latent semantic kernels
we present a kernel-based approach for finegrained classification of named entities . the only training data for our algorithm is a few manually annotated entities for each class . we defined kernel functions that implicitly map entities , represented by aggregating all contexts in which they occur , into a latent semantic space derived from wikipedia . our method achieves a significant improvement over the state of the art for the task of populating an ontology of people , although requiring considerably less training instances than previous approaches .

can corpus based measures be used for comparative study of languages anil kumar singh
quantitative measurement of inter-language distance is a useful technique for studying diachronic and synchronic relations between languages . such measures have been used successfully for purposes like deriving language taxonomies and language reconstruction , but they have mostly been applied to handcrafted word lists . can we instead use corpus based measures for comparative study of languages in this paper we try to answer this question . we use three corpus based measures and present the results obtained from them and show how these results relate to linguistic and historical knowledge . we argue that the answer is yes and that such studies can provide or validate linguistic and computational insights .

nonparametric method for data-driven image captioning
we present a nonparametric density estimation technique for image caption generation . data-driven matching methods have shown to be effective for a variety of complex problems in computer vision . these methods reduce an inference problem for an unknown image to finding an existing labeled image which is semantically similar . however , related approaches for image caption generation ( ordonez et al , 2011 ; kuznetsova et al , 2012 ) are hampered by noisy estimations of visual content and poor alignment between images and human-written captions . our work addresses this challenge by estimating a word frequency representation of the visual content of a query image . this allows us to cast caption generation as an extractive summarization problem . our model strongly outperforms two state-ofthe-art caption extraction systems according to human judgments of caption relevance .

referring expression generation
in this paper we present research in which we apply ( i ) the kind of intrinsic evaluation metrics that are characteristic of current comparative hlt evaluation , and ( ii ) extrinsic , human task-performance evaluations more in keeping with nlg traditions , to 15 systems implementing a language generation task . we analyse the evaluation results and find that there are no significant correlations between intrinsic and extrinsic evaluation measures for this task .

do non-acoustic features support anger detection
most studies on speech-based emotion recognition are based on prosodic and acoustic features , only employing artificial acted corpora where the results can not be generalized to telephone-based speech applications . in contrast , we present an approach based on utterances from 1,911 calls from a deployed telephone-based speech application , taking advantage of additional dialogue features , nlu features and asr features that are incorporated into the emotion recognition process . depending on the task , non-acoustic features add 2.3 % in classification accuracy compared to using only acoustic features .

they can help : using crowdsourcing to improve the evaluation of grammatical error detection systems nitin madnania joel tetreaulta martin chodorowb alla rozovskayac aeducational testing service
despite the rising interest in developing grammatical error detection systems for non-native speakers of english , progress in the field has been hampered by a lack of informative metrics and an inability to directly compare the performance of systems developed by different researchers . in this paper we address these problems by presenting two evaluation methodologies , both based on a novel use of crowdsourcing .

dont count , predict ! a systematic comparison of
context-predicting models ( more commonly known as embeddings or neural language models ) are the new kids on the distributional semantics block . despite the buzz surrounding these models , the literature is still lacking a systematic comparison of the predictive models with classic , count-vector-based distributional semantic approaches . in this paper , we perform such an extensive evaluation , on a wide range of lexical semantics tasks and across many parameter settings . the results , to our own surprise , show that the buzz is fully justified , as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts .

human judgment as a parameter in evaluation campaigns
the relevance of human judgment in an evaluation campaign is illustrated here through the deft text mining campaigns . in a first step , testing a topic for a campaign among a limited number of human evaluators informs us about the feasibility of a task . this information comes from the results obtained by the judges , as well as from their personal impressions after passing the test . in a second step , results from individual judges , as well as their pairwise matching , are used in order to adjust the task ( choice of a marking scale for deft07 and selection of topical categories for deft08 ) . finally , the mutual comparison of competitors results , at the end of the evaluation campaign , confirms the choices we made at its starting point , and provides means to redefine the task when we shall launch a future campaign based on the same topic .

period disambiguation with maxent model
maximum entropy ( maxent ) model . a number of experiments are conducted on ptb-ii wsj corpus for the investigation of how context window , feature space and lexical information such as abbreviated and sentence-initial words affect the learning performance . such lexical information can be automatically acquired from a training corpus by a learner . our experimental results show that extending the feature space to integrate these two kinds of lexical information can eliminate 93.52 % of the remaining errors from the baseline maxent model , achieving an f-score of 99.8227 % .

methods to integrate a language model with semantic information for a word prediction component
most current word prediction systems make use of n-gram language models ( lm ) to estimate the probability of the following word in a phrase . in the past years there have been many attempts to enrich such language models with further syntactic or semantic information . we want to explore the predictive powers of latent semantic analysis ( lsa ) , a method that has been shown to provide reliable information on long-distance semantic dependencies between words in a context . we present and evaluate here several methods that integrate lsa-based information with a standard language model : a semantic cache , partial reranking , and different forms of interpolation . we found that all methods show significant improvements , compared to the 4gram baseline , and most of them to a simple cache model as well .

discriminative classifiers for deterministic dependency parsing
deterministic parsing guided by treebankinduced classifiers has emerged as a simple and efficient alternative to more complex models for data-driven parsing . we present a systematic comparison of memory-based learning ( mbl ) and support vector machines ( svm ) for inducing classifiers for deterministic dependency parsing , using data from chinese , english and swedish , together with a variety of different feature models . the comparison shows that svm gives higher accuracy for richly articulated feature models across all languages , albeit with considerably longer training times . the results also confirm that classifier-based deterministic parsing can achieve parsing accuracy very close to the best results reported for more complex parsing models .

constrained grammatical error correction using statistical machine translation
this paper describes our use of phrasebased statistical machine translation ( pbsmt ) for the automatic correction of errors in learner text in our submission to the conll 2013 shared task on grammatical error correction . since the limited training data provided for the task was insufficient for training an effective smt system , we also explored alternative ways of generating pairs of incorrect and correct sentences automatically from other existing learner corpora . our approach does not yield particularly high performance but reveals many problems that require careful attention when building smt systems for error correction .

mining wiki resources for multilingual named entity recognition
in this paper , we describe a system by which the multilingual characteristics of wikipedia can be utilized to annotate a large corpus of text with named entity recognition ( ner ) tags requiring minimal human intervention and no linguistic expertise . this process , though of value in languages for which resources exist , is particularly useful for less commonly taught languages . we show how the wikipedia format can be used to identify possible named entities and discuss in detail the process by which we use the category structure inherent to wikipedia to determine the named entity type of a proposed entity . we further describe the methods by which english language data can be used to bootstrap the ner process in other languages . we demonstrate the system by using the generated corpus as training sets for a variant of bbn 's identifinder in french , ukrainian , spanish , polish , russian , and portuguese , achieving overall f-scores as high as 84.7 % on independent , human-annotated corpora , comparable to a system trained on up to 40,000 words of human-annotated newswire .

from image descriptions to visual denotations : new similarity metrics for semantic inference over event descriptions peter young alice lai micah hodosh julia hockenmaier
we propose to use the visual denotations of linguistic expressions ( i.e . the set of images they describe ) to define novel denotational similarity metrics , which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference . to compute these denotational similarities , we construct a denotation graph , i.e . a subsumption hierarchy over constituents and their denotations , based on a large corpus of 30k images and 150k descriptive captions .

individuality and alignment in generated dialogues
it would be useful to enable dialogue agents to project , through linguistic means , their individuality or personality . equally , each member of a pair of agents ought to adjust its language ( to a greater or lesser extent ) to match that of its interlocutor . we describe crag , which generates dialogues between pairs of agents , who are linguistically distinguishable , but able to align . crag-2 makes use of openccg and an over-generation and ranking approach , guided by a set of language models covering both personality and alignment . we illustrate with examples of output , and briefly note results from user studies with the earlier crag-1 , indicating how crag-2 will be further evaluated . related work is discussed , along with current limitations and future directions .

pris at chinese language processing -- chinese personal name disambiguation
the more chinese language materials come out , the more we have to focus on the same personal name problem . in our personal name disambiguation system , the hierarchical agglomerative clustering is applied , and named entity is used as feature for document similarity calculation . we propose a two-stage strategy in which the first stage involves word segmentation and named entity recognition ( ner ) for feature extraction , and the second stage focuses on clustering .

easily identifiable discourse relations
we present a corpus study of local discourse relations based on the penn discourse tree bank , a large manually annotated corpus of explicitly or implicitly realized relations . we show that while there is a large degree of ambiguity in temporal explicit discourse connectives , overall connectives are mostly unambiguous and allow high-accuracy prediction of discourse relation type . we achieve 93.09 % accuracy in classifying the explicit relations and 74.74 % accuracy overall . in addition , we show that some pairs of relations occur together in text more often than expected by chance . this finding suggests that global sequence classification of the relations in text can lead to better results , especially for implicit relations .

using self-trained bilexical preferences to improve disambiguation
a method is described to incorporate bilexical preferences between phrase heads , such as selection restrictions , in a maximumentropy parser for dutch . the bilexical preferences are modelled as association rates which are determined on the basis of a very large parsed corpus ( about 500m words ) . we show that the incorporation of such selftrained preferences improves parsing accuracy significantly .

scaling textual inference to the web
most web-based q/a systems work by finding pages that contain an explicit answer to a question . these systems are helpless if the answer has to be inferred from multiple sentences , possibly on different pages . to solve this problem , we introduce the holmes system , which utilizes textual inference ( ti ) over tuples extracted from text . whereas previous work on ti ( e.g. , the literature on textual entailment ) has been applied to paragraph-sized texts , holmes utilizes knowledge-based model construction to scale ti to a corpus of 117 million web pages . given only a few minutes , holmes doubles recall for example queries in three disparate domains ( geography , business , and nutrition ) . importantly , holmess runtime is linear in the size of its input corpus due to a surprising property of many textual relations in the web corpusthey are approximately functional in a well-defined sense .

indirect-hmm-based hypothesis alignment for combining outputs from machine translation systems
this paper presents a new hypothesis alignment method for combining outputs of multiple machine translation ( mt ) systems . an indirect hidden markov model ( ihmm ) is proposed to address the synonym matching and word ordering issues in hypothesis alignment . unlike traditional hmms whose parameters are trained via maximum likelihood estimation ( mle ) , the parameters of the ihmm are estimated indirectly from a variety of sources including word semantic similarity , word surface similarity , and a distance-based distortion penalty . the ihmm-based method significantly outperforms the state-of-the-art ter-based alignment model in our experiments on nist benchmark datasets . our combined smt system using the proposed method achieved the best chinese-to-english translation result in the constrained training track of the 2008 nist open mt evaluation .

performance evaluation and error analysis for multimodal reference resolution in a conversation system
multimodal reference resolution is a process that automatically identifies what users refer to during multimodal human-machine conversation . given the substantial work on multimodal reference resolution ; it is important to evaluate the current state of the art , understand the limitations , and identify directions for future improvement . we conducted a series of user studies to evaluate the capability of reference resolution in a multimodal conversation system . this paper analyzes the main error sources during real-time human-machine interaction and presents key strategies for designing robust multimodal reference resolution algorithms .

the linguistic structure of english web-search queries
web-search queries are known to be short , but little else is known about their structure . in this paper we investigate the applicability of part-of-speech tagging to typical englishlanguage web search-engine queries and the potential value of these tags for improving search results . we begin by identifying a set of part-of-speech tags suitable for search queries and quantifying their occurrence . we find that proper-nouns constitute 40 % of query terms , and proper nouns and nouns together constitute over 70 % of query terms . we also show that the majority of queries are nounphrases , not unstructured collections of terms . we then use a set of queries manually labeled with these tags to train a brill tagger and evaluate its performance . in addition , we investigate classification of search queries into grammatical classes based on the syntax of part-of-speech tag sequences . we also conduct preliminary investigative experiments into the practical applicability of leveraging query-trained part-of-speech taggers for information-retrieval tasks . in particular , we show that part-of-speech information can be a significant feature in machine-learned searchresult relevance . these experiments also include the potential use of the tagger in selecting words for omission or substitution in query reformulation , actions which can improve recall .

a classification algorithm for predicting the structure of summaries
we investigate the problem of generating the structure of short domain independent abstracts . we apply a supervised machine learning approach trained over a set of abstracts collected from abstracting services and automatically annotated with a text analysis tool . we design a set of features for learning inspired from past research in content selection , information ordering , and rhetorical analysis for training an algorithm which then predicts the discourse structure of unseen abstracts . the proposed approach to the problem which combines local and contextual features is able to predict the local structure of the abstracts in just over 60 % of the cases .

feature vector quality and distributional similarity givat ram campus ,
we suggest a new goal and evaluation criterion for word similarity measures . the new criterion - meaning-entailing substitutability - fits the needs of semantic-oriented nlp applications and can be evaluated directly ( independent of an application ) at a good level of human agreement . motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results , proposing an objective measure for evaluating feature vector quality . finally , a novel feature weighting and selection function is presented , which yields superior feature vectors and better word similarity performance .

negation and modality in distributional semantics
in natural language processing , negation and modality have mostly been handled using the older , pre-statistical methodologies of formal representations subject to rule-based processing . this fits the traditional treatment of negation and modality in logic-based knowledge representation and linguistics . however , in modern-day statistics-based nlp , how exactly negation and modality should be taken into account , and what role these phenomena play overall , is much less clear . the closest statistics-based nlp gets to semantics at this time is lexical-based word distributions ( such as used in word sense disambiguation ) and topic models ( such as produced by latent dirichlet allocation ) . what exactly in such representations should a negation or a modality actually apply to what would , or should , the resulting effects be the traditional approaches are of little or no help . in this talk i argue that neither model is adequate , and that one needs a different model of semantics to be able to accommodate negation and modality . the traditional formalisms are impoverished in their absence of an explicit representation of the denotations of each symbol , and the statistics-based word distributions do not support the compositionality required of semantics since it is unclear how to link together two separate word distributions in a semantically meaningful way . a kind of hybrid , which one could call distributional semantics , should be formulated to include the necessary aspects of both : the ability to carry explicit word associations that are still partitioned so as to allow negation and modality to affect the representations in intuitively plausible ways is what is required . i present a specific model of distributional semantics that , although still rudimentary , exhibits some of the desired features . i explore the possibilities for accommodating the phenomena of negation and modality .

adding redundant features for crfs-based sentence sentiment
in this paper , we present a novel method based on crfs in response to the two special characteristics of contextual dependency and label redundancy in sentence sentiment classification . we try to capture the contextual constraints on sentence sentiment using crfs . through introducing redundant labels into the original sentimental label set and organizing all labels into a hierarchy , our method can add redundant features into training for capturing the label redundancy . the experimental results prove that our method outperforms the traditional methods like nb , svm , maxent and standard chain crfs . in comparison with the cascaded model , our method can effectively alleviate the error propagation among different layers and obtain better performance in each layer .

adaptive recursive neural network for target-dependent twitter sentiment classification
we propose adaptive recursive neural network ( adarnn ) for target-dependent twitter sentiment classification . adarnn adaptively propagates the sentiments of words to target depending on the context and syntactic relationships between them . it consists of more than one composition functions , and we model the adaptive sentiment propagations as distributions over these composition functions . the experimental studies illustrate that adarnn improves the baseline methods . furthermore , we introduce a manually annotated dataset for target-dependent twitter sentiment analysis .

training conditional random fields using incomplete annotations
we address corpus building situations , where complete annotations to the whole corpus is time consuming and unrealistic . thus , annotation is done only on crucial part of sentences , or contains unresolved label ambiguities . we propose a parameter estimation method for conditional random fields ( crfs ) , which enables us to use such incomplete annotations . we show promising results of our method as applied to two types of nlp tasks : a domain adaptation task of a japanese word segmentation using partial annotations , and a partof-speech tagging task using ambiguous tags in the penn treebank corpus .

detecting speculations and their scopes in scientific text
distinguishing speculative statements from factual ones is important for most biomedical text mining applications . we introduce an approach which is based on solving two sub-problems to identify speculative sentence fragments . the first sub-problem is identifying the speculation keywords in the sentences and the second one is resolving their linguistic scopes . we formulate the first sub-problem as a supervised classification task , where we classify the potential keywords as real speculation keywords or not by using a diverse set of linguistic features that represent the contexts of the keywords . after detecting the actual speculation keywords , we use the syntactic structures of the sentences to determine their scopes .

cascading use of soft and hard matching pattern rules for weakly supervised information extraction
current rule induction techniques based on hard matching ( i.e. , strict slot-by-slot matching ) tend to fare poorly in extracting information from natural language texts , which often exhibit great variations . the reason is that hard matching techniques result in relatively high precision but low recall . to tackle this problem , we take advantage of the newly proposed soft pattern rules which offer high recall through the use of probabilistic matching . we propose a bootstrapping framework in which soft and hard matching pattern rules are combined in a cascading manner to realize a weakly supervised rule induction scheme . the system starts with a small set of hand-tagged instances . at each iteration , we first generate soft pattern rules and utilize them to tag new training instances automatically . we then apply hard pattern rule induction on the overall tagged data to generate more precise rules , which are used to tag the data again . the process can be repeated until satisfactory results are obtained . our experimental results show that our bootstrapping scheme with two cascaded learners approaches the performance of a fully supervised information extraction system while using much fewer handtagged instances .

part of speech tagger for assamese text
assamese is a morphologically rich , agglutinative and relatively free word order indic language . although spoken by nearly 30 million people , very little computational linguistic work has been done for this language . in this paper , we present our work on part of speech ( pos ) tagging for assamese using the well-known hidden markov model . since no well-defined suitable tagset was available , we develop a tagset of 172 tags in consultation with experts in linguistics . for successful tagging , we examine relevant linguistic issues in assamese . for unknown words , we perform simple morphological analysis to determine probable tags . using a manually tagged corpus of about 10000 words for training , we obtain a tagging accuracy of nearly 87 % for test inputs .

automatic correction and extension of morphological annotations
for languages with complex morphologies , limited resources and tools , and/or lack of standard grammars , developing annotated resources can be a challenging task . annotated resources developed under time/money constraints for such languages tend to tradeoff depth of representation with degree of noise . we present two methods for automatic correction and extension of morphological annotations , and demonstrate their success on three divergent egyptian arabic corpora .

modeling the noun phrase versus sentence coordination ambiguity in dutch : evidence from surprisal theory
this paper investigates whether surprisal theory can account for differential processing difficulty in the np-/s-coordination ambiguity in dutch . surprisal is estimated using a probabilistic context-free grammar ( pcfg ) , which is induced from an automatically annotated corpus . we find that our lexicalized surprisal model can account for the reading time data from a classic experiment on this ambiguity by frazier ( 1987 ) . we argue that syntactic and lexical probabilities , as specified in a pcfg , are sufficient to account for what is commonly referred to as an np-coordination preference .

applying co-training methods to statistical parsing
we propose a novel co-training method for statistical parsing . the algorithm takes as input a small corpus ( 9695 sentences ) annotated with parse trees , a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text . the algorithm iteratively labels the entire data set with parse trees . using empirical results based on parsing the wall street journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data .

automatic generation of story highlights
in this paper we present a joint content selection and compression model for single-document summarization . the model operates over a phrase-based representation of the source document which we obtain by merging information from pcfg parse trees and dependency graphs . using an integer linear programming formulation , the model learns to select and combine phrases subject to length , coverage and grammar constraints . we evaluate the approach on the task of generating story highlightsa small number of brief , self-contained sentences that allow readers to quickly gather information on news stories . experimental results show that the models output is comparable to human-written highlights in terms of both grammaticality and content .

contexts , patterns , interrelations - new ways of presenting multi-word
this contribution presents the newest version of our wortverbindungsfelder ( fields of multi-word expressions ) , an experimental lexicographic resource that focusses on aspects of mwes that are rarely addressed in traditional descriptions : contexts , patterns and interrelations . the mwe fields use data from a very large corpus of written german ( over 6 billion word forms ) and are created in a strictly corpus-based way . in addition to traditional lexicographic descriptions , they include quantitative corpus data which is structured in new ways in order to show the usage specifics . this way of looking at mwes gives insight in the structure of language and is especially interesting for foreign language learners .

empirical methods for evaluating dialog systems paper id : sigdial_tp
we examine what purpose a dialog metric serves and then propose empirical methods for evaluating systems that meet that purpose . the methods include a protocol for conducting a wizard-of-oz experiment and a basic set of descriptive statistics for substantiating performance claims using the data collected from the experiment as an ideal benchmark or gold standard for comparative judgments . the methods also provide a practical means of optimizing the system through component analysis and cost valuation . empirical methods for evaluating dialog systems abstract we examine what purpose a dialog metric serves and then propose empirical methods for evaluating systems that meet that purpose . the methods include a protocol for conducting a wizardof-oz experiment and a basic set of descriptive statistics for substantiating performance claims using the data collected from the experiment as an ideal benchmark or gold standard for comparative judgments . the methods also provide a practical means of optimizing the system through component analysis and cost valuation .

applying an nvef word-pair identifier to
syllable-to-word ( stw ) conversion is important in chinese phonetic input methods and speech recognition . there are two major problems in the stw conversion : ( 1 ) resolving the ambiguity caused by homonyms ; ( 2 ) determining the word segmentation . this paper describes a noun-verb event-frame ( nvef ) word identifier that can be used to solve these problems effectively . our approach includes ( a ) an nvef word-pair identifier and ( b ) other word identifiers for the non-nvef portion . our experiment showed that the nvef word-pair identifier is able to achieve a 99.66 % stw accuracy for the nvef related portion , and by combining with other identifiers for the non-nvef portion , the overall stw accuracy is 96.50 % . the result of this study indicates that the nvef knowledge is very powerful for the stw conversion . in fact , numerous cases requiring disambiguation in natural language processing fall into such chicken-and-egg situation . the nvef knowledge can be employed as a general tool in such systems for disambiguating the nvef related portion independently ( thus breaking the chicken-and-egg situation ) and using that as a good fundamental basis to treat the remaining portion . this shows that the nvef knowledge is likely to be important for general nlp . to further expand its coverage , we shall extend the study of nvef to that of other co-occurrence restrictions such as noun-noun pairs , noun-adjective pairs and verb-adverb pairs .

dialog input ranking in a multi-domain environment using transferable belief model
this paper presents results of using belief functions to rank the list of candidate information provided in a noisy dialogue input . the information under consideration is the intended task to be performed and the information provided for the completion of the task . as an example , we use the task of information access in a multi-domain dialogue system . currently , the system contains knowledge of ten different domains . callers calling in are greeted with an open-ended how may i help you prompt ( thomson and wisowaty , 1999 ; chu-carroll and carpenter , 1999 ; gorin et al , 1997 ) . after receiving a reply from the caller , we extract word evidences from the recognized utterances . by using transferable belief model ( tbm ) , we in turn determine the task that the caller intends to perform as well as any information provided .

evaluation metrics for end-to-end coreference resolution systems
commonly used coreference resolution evaluation metrics can only be applied to key mentions , i.e . already annotated mentions . we here propose two variants of the b3 and ceaf coreference resolution evaluation algorithms which can be applied to coreference resolution systems dealing with system mentions , i.e . automatically determined mentions . our experiments show that our variants lead to intuitive and reliable results .

phrasal : a toolkit for statistical machine translation with facilities for extraction and incorporation of arbitrary model features
we present a new java-based open source toolkit for phrase-based machine translation . the key innovation provided by the toolkit is to use apis for integrating new features ( /knowledge sources ) into the decoding model and for extracting feature statistics from aligned bitexts . the package includes a number of useful features written to these apis including features for hierarchical reordering , discriminatively trained linear distortion , and syntax based language models . other useful utilities packaged with the toolkit include : a conditional phrase extraction system that builds a phrase table just for a specific dataset ; and an implementation of mert that allows for pluggable evaluation metrics for both training and evaluation with built in support for a variety of metrics ( e.g. , terp , bleu , meteor ) .

learning the scope of hedge cues in biomedical texts
identifying hedged information in biomedical literature is an important subtask in information extraction because it would be misleading to extract speculative information as factual information . in this paper we present a machine learning system that finds the scope of hedge cues in biomedical texts . the system is based on a similar system that finds the scope of negation cues . we show that the same scope finding approach can be applied to both negation and hedging . to investigate the robustness of the approach , the system is tested on the three subcorpora of the bioscope corpus that represent different text types .

machine translation and its philosophical accounts
this paper attempts to explore the interrelation between philosophical accounts of language and respective technological developments in the field of human language technologies . in doing so , it focuses on the interaction between analytical philosophy and machine translation development , trying to draw the emerging methodological analogies .

urdu and the parallel grammar project
we report on the role of the urdu grammar in the parallel grammar ( pargram ) project ( butt et al , 1999 ; butt et al , 2002 ) .1 the pargram project was designed to use a single grammar development platform and a unified methodology of grammar writing to develop large-scale grammars for typologically different languages . at the beginning of the project , three typologically similar european grammars were implemented . the addition of two asian languages , urdu and japanese , has shown that the basic analysis decisions made for the european languages can be applied to typologically distinct languages . however , the asian languages required the addition of a small number of new standard analyses to cover constructions and analysis techniques not found in the european languages . with these additional standards , the pargram project can now be applied to other typologically distinct languages .

improving statistical word alignment with ensemble
and cross-validation committees . on these two methods , both weighted voting and unweighted voting are compared under the word alignment task . in addition , we analyze the effect of different sizes of training sets on the bagging method . experimental results indicate that both bagging and cross-validation committees improve the word alignment results regardless of weighted voting or unweighted voting . weighted voting performs consistently better than unweighted voting on different sizes of training sets .

multimodal subjectivity analysis of multiparty conversation tno information and
we investigate the combination of several sources of information for the purpose of subjectivity recognition and polarity classification in meetings . we focus on features from two modalities , transcribed words and acoustics , and we compare the performance of three different textual representations : words , characters , and phonemes . our experiments show that character-level features outperform wordlevel features for these tasks , and that a careful fusion of all features yields the best performance .

named entity recognition for south asian languages salt lake city , utah
much work has already been done on building named entity recognition systems . however most of this work has been concentrated on english and other european languages . hence , building a named entity recognition ( ner ) system for south asian languages ( sal ) is still an open problem because they exhibit characteristics different from english . this paper builds a named entity recognizer which also identifies nested name entities for the hindi language using machine learning algorithm , trained on an annotated corpus . however , the algorithm is designed in such a manner that it can easily be ported to other south asian languages provided the necessary nlp tools like pos tagger and chunker are available for that language . i compare results of hindi data with english data of conll shared task of 2003 .

evaluating distant supervision for subjectivity and sentiment analysis on arabic twitter feeds
supervised machine learning methods for automatic subjectivity and sentiment analysis ( ssa ) are problematic when applied to social media , such as twitter , since they do not generalise well to unseen topics . a possible remedy of this problem is to apply distant supervision ( ds ) approaches , which learn from large amounts of automatically annotated data . this research empirically evaluates the performance of ds approaches for ssa on arabic twitter feeds . results for emoticon- and lexiconbased ds show a significant performance gain over a fully supervised baseline , especially for detecting subjectivity , where we achieve 95.19 % accuracy , which is a 48.47 % absolute improvement over previous fully supervised results .

topic model analysis of metaphor frequency for psycholinguistic stimuli vicky tzuyin lai
psycholinguistic studies of metaphor processing must control their stimuli not just for word frequency but also for the frequency with which a term is used metaphorically . thus , we consider the task of metaphor frequency estimation , which predicts how often target words will be used metaphorically . we develop metaphor classifiers which represent metaphorical domains through latent dirichlet allocation , and apply these classifiers to the target words , aggregating their decisions to estimate the metaphorical frequencies . training on only 400 sentences , our models are able to achieve 61.3 % accuracy on metaphor classification and 77.8 % accuracy on high vs. low metaphorical frequency estimation .

meta-learning orthographic and contextual models for language independent named entity recognition
this paper presents a named entity classification system that utilises both orthographic and contextual information . the random subspace method was employed to generate and refine attribute models . supervised and unsupervised learning techniques used in the recombination of models to produce the final results .

multilingual dependency-based syntactic and semantic parsing
our conll 2009 shared task system includes three cascaded components : syntactic parsing , predicate classification , and semantic role labeling . a pseudo-projective high-order graph-based model is used in our syntactic dependency parser . a support vector machine ( svm ) model is used to classify predicate senses . semantic role labeling is achieved using maximum entropy ( maxent ) model based semantic role classification and integer linear programming ( ilp ) based post inference . finally , we win the first place in the joint task , including both the closed and open challenges .

complexity metrics in an incremental right-corner parser
hierarchical hmm ( hhmm ) parsers make promising cognitive models : while they use a bounded model of working memory and pursue incremental hypotheses in parallel , they still achieve parsing accuracies competitive with chart-based techniques . this paper aims to validate that a right-corner hhmm parser is also able to produce complexity metrics , which quantify a readers incremental difficulty in understanding a sentence . besides defining standard metrics in the hhmm framework , a new metric , embedding difference , is also proposed , which tests the hypothesis that hhmm store elements represents syntactic working memory . results show that hhmm surprisal outperforms all other evaluated metrics in predicting reading times , and that embedding difference makes a significant , independent contribution .

exploiting language variants via grammar parsing having morphologically rich information
in this paper , the development and evaluation of the urdu parser is presented along with the comparison of existing resources for the language variants urdu/hindi . this parser was given a linguistically rich grammar extracted from a treebank . this context free grammar with sufficient encoded information is comparable with the state of the art parsing requirements for morphologically rich and closely related language variants urdu/hindi . the extended parsing model and the linguistically rich grammar together provide us promising parsing results for both the language variants . the parser gives 87 % of f-score , which outperforms the multi-path shift-reduce parser for urdu and a simple hindi dependency parser with 4.8 % and 22 % increase in recall , respectively .

policy learning for domain selection in an extensible multi-domain spoken dialogue system mathematical & computer sciences
this paper proposes a markov decision process and reinforcement learning based approach for domain selection in a multidomain spoken dialogue system built on a distributed architecture . in the proposed framework , the domain selection problem is treated as sequential planning instead of classification , such that confirmation and clarification interaction mechanisms are supported . in addition , it is shown that by using a model parameter tying trick , the extensibility of the system can be preserved , where dialogue components in new domains can be easily plugged in , without re-training the domain selection policy . the experimental results based on human subjects suggest that the proposed model marginally outperforms a non-trivial baseline .

a hybrid morphologically decomposed factored language models for
in this work , we try a hybrid methodology for language modeling where both morphological decomposition and factored language modeling ( flm ) are exploited to deal with the complex morphology of arabic language . at the end , we are able to obtain from 3.5 % to 7.0 % relative reduction in word error rate ( wer ) with respect to a traditional full-words system , and from 1.0 % to 2.0 % relative wer reduction with respect to a non-factored decomposed system .

employing word representations and regularization for domain adaptation of relation extraction thien huu nguyen
relation extraction suffers from a performance loss when a model is applied to out-of-domain data . this has fostered the development of domain adaptation techniques for relation extraction . this paper evaluates word embeddings and clustering on adapting feature-based relation extraction systems . we systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information . finally , we demonstrate the effectiveness of regularization for the adaptability of relation extractors .

reverse engineering of tree kernel feature spaces
we present a framework to extract the most important features ( tree fragments ) from a tree kernel ( tk ) space according to their importance in the target kernelbased machine , e.g . support vector machines ( svms ) . in particular , our mining algorithm selects the most relevant features based on svm estimated weights and uses this information to automatically infer an explicit representation of the input data . the explicit features ( a ) improve our knowledge on the target problem domain and ( b ) make large-scale learning practical , improving training and test time , while yielding accuracy in line with traditional tk classifiers . experiments on semantic role labeling and question classification illustrate the above claims .

graph-based clustering for semantic classication of onomatopoetic
this paper presents a method for semantic classication of onomatopoetic words like ( hum ) and ( clip clop ) which exist in every language , especially japanese being rich in onomatopoetic words . we used a graph-based clustering algorithm called newman clustering . the algorithm calculates a simple quality function to test whether a particular division is meaningful . the quality function is calculated based on the weights of edges between nodes . we combined two different similarity measures , distributional similarity , and orthographic similarity to calculate weights . the results obtained by using the web data showed a 9.0 % improvement over the baseline single distributional similarity measure .

bilingually motivated domain-adapted word segmentation for statistical machine translation yanjun ma andy way
we introduce a word segmentation approach to languages where word boundaries are not orthographically marked , with application to phrase-based statistical machine translation ( pb-smt ) . instead of using manually segmented monolingual domain-specific corpora to train segmenters , we make use of bilingual corpora and statistical word alignment techniques . first of all , our approach is adapted for the specific translation task at hand by taking the corresponding source ( target ) language into account . secondly , this approach does not rely on manually segmented training data so that it can be automatically adapted for different domains . we evaluate the performance of our segmentation approach on pb-smt tasks from two domains and demonstrate that our approach scores consistently among the best results across different data conditions .

, language and translation technology team
this paper describes our contribution to the semeval-2014 task 9 on sentiment analysis in twitter . we participated in both strands of the task , viz . classification at message-level ( subtask b ) , and polarity disambiguation of particular text spans within a message ( subtask a ) . our experiments with a variety of lexical and syntactic features show that our systems benefit from rich feature sets for sentiment analysis on user-generated content . our systems ranked ninth among 27 and sixteenth among 50 submissions for task a and b respectively .

robust ending guessing rules with application to slavonic languages
the paper studies the automatic extraction of diagnostic word endings for slavonic languages aimed to determine some grammatical , morphological and semantic properties of the underlying word . in particular , ending guessing rules are being learned from a large morphological dictionary of bulgarian in order to predict pos , gender , number , article and semantics . a simple exact high accuracy algorithm is developed and compared to an approximate one , which uses a scoring function previously proposed by mikheev for pos guessing . it is shown how the number of rules of the latter can be reduced by a factor of up to 35 , without sacrificing performance . the evaluation demonstrates coverage close to 100 % , and precision of 97-99 % for the approximate algorithm .

enhancing the expression of contrast in the sparky restaurant corpus
we show that nakatsu & whites ( 2010 ) proposed enhancements to the sparky restaurant corpus ( src ; walker et al , 2007 ) for better expressing contrast do indeed make it possible to generate better texts , including ones that make effective and varied use of contrastive connectives and discourse adverbials . after first presenting a validation experiment for naturalness ratings of src texts gathered using amazons mechanical turk , we present an initial experiment suggesting that such ratings can be used to train a realization ranker that enables higher-rated texts to be selected when the ranker is trained on a sample of generated restaurant recommendations with the contrast enhancements than without them . we conclude with a discussion of possible ways of improving the ranker in future work .

strategies for sustainable mt for basque :
we present some language technology applications that have proven to be effective tools to promote the use of basque , a european less privileged language . we also present the strategy we have followed for almost twenty years to develop those applications as the top of an integrated environment of language resources , language foundations , language tools and other applications . when we have faced a difficult task such as machine translation to basque , our strategy has worked well . we have had good results in a short time just reusing previous works for basque , reusing other open-source tools , and developing just a few new modules in collaboration with other groups . in addition , new reusable tools and formats have been produced .

search in the lost sense of query : question formulation in web search queries and its temporal changes bo pang ravi kumar
web search is an information-seeking activity . often times , this amounts to a user seeking answers to a question . however , queries , which encode users information need , are typically not expressed as full-length natural language sentences in particular , as questions . rather , they consist of one or more text fragments . as humans become more searchengine-savvy , do natural-language questions still have a role to play in web search through a systematic , large-scale study , we find to our surprise that as time goes by , web users are more likely to use questions to express their search intent .

analysis of the wikipedia category graph for nlp applications
in this paper , we discuss two graphs in wikipedia ( i ) the article graph , and ( ii ) the category graph . we perform a graphtheoretic analysis of the category graph , and show that it is a scale-free , small world graph like other well-known lexical semantic networks . we substantiate our findings by transferring semantic relatedness algorithms defined on wordnet to the wikipedia category graph . to assess the usefulness of the category graph as an nlp resource , we analyze its coverage and the performance of the transferred semantic relatedness algorithms .

language model-based document clustering using random walks
we propose a new document vector representation specifically designed for the document clustering task . instead of the traditional termbased vectors , a document is represented as an -dimensional vector , where is the number of documents in the cluster . the value at each dimension of the vector is closely related to the generation probability based on the language model of the corresponding document . inspired by the recent graph-based nlp methods , we reinforce the generation probabilities by iterating random walks on the underlying graph representation . experiments with k-means and hierarchical clustering algorithms show significant improvements over the alternative vector representation .

unsupervised modeling of twitter conversations
we propose the first unsupervised approach to the problem of modeling dialogue acts in an open domain . trained on a corpus of noisy twitter conversations , our method discovers dialogue acts by clustering raw utterances . because it accounts for the sequential behaviour of these acts , the learned model can provide insight into the shape of communication in a new medium . we address the challenge of evaluating the emergent model with a qualitative visualization and an intrinsic conversation ordering task . this work is inspired by a corpus of 1.3 million twitter conversations , which will be made publicly available . this huge amount of data , available only because twitter blurs the line between chatting and publishing , highlights the need to be able to adapt quickly to a new medium .

point-of-view mining and cognitive presence in moocs : a ( computational ) linguistics perspective
this paper explores the cognitive presence of the learners in moocs through using a ( computational ) linguistic analysis of the learners point-of-view as an indicator for cognitive presence . the linguistic analysis of the written language as a medium of interaction by the students in the context of moocs shows hallmarks of cognitive disengagement and low cognitive presence by the learners .

plaser : pronunciation learning via automatic speech recognition
plaser is a multimedia tool with instant feedback designed to teach english pronunciation for high-school students of hong kong whose mother tongue is cantonese chinese . the objective is to teach correct pronunciation and not to assess a students overall pronunciation quality . major challenges related to speech recognition technology include : allowance for non-native accent , reliable and corrective feedbacks , and visualization of errors . plaser employs hidden markov models to represent position-dependent english phonemes . they are discriminatively trained using the standard american english timit corpus together with a set of timit utterances collected from good local english speakers . there are two kinds of speaking exercises : minimal-pair exercises and word exercises . in the word exercises , plaser computes a confidence-based score for each phoneme of the given word , and paints each vowel or consonant segment in the word using a novel 3-color scheme to indicate their pronunciation accuracy . plaser was used by 900 students of grade 7 and 8 over a period of 23 months . about 80 % of the students said that they preferred using plaser over traditional english classes to learn pronunciation . a pronunciation test was also conducted before and after they used plaser .

cone : metrics for automatic evaluation of named entity
human annotation for co-reference resolution ( crr ) is labor intensive and costly , and only a handful of annotated corpora are currently available . however , corpora with named entity ( ne ) annotations are widely available . also , unlike current crr systems , state-of-the-art ner systems have very high accuracy and can generate ne labels that are very close to the gold standard for unlabeled corpora . we propose a new set of metrics collectively called cone for named entity coreference resolution ( ne-crr ) that use a subset of gold standard annotations , with the advantage that this subset can be easily approximated using ne labels when gold standard crr annotations are absent . we define cone b3 and cone ceaf metrics based on the traditional b3 and ceaf metrics and show that cone b3 and cone ceaf scores of any crr system on any dataset are highly correlated with its b3 and ceaf scores respectively . we obtain correlation factors greater than 0.6 for all crr systems across all datasets , and a best-case correlation factor of 0.8. we also present a baseline method to estimate the gold standard required by cone metrics , and show that cone b3 and cone ceaf scores using this estimated gold standard are also correlated with b3 and ceaf scores respectively . we thus demonstrate the suitability of cone b3and cone ceaf for automatic evaluation of ne-crr .

boosting statistical word alignment using
this paper proposes a semi-supervised boosting approach to improve statistical word alignment with limited labeled data and large amounts of unlabeled data . the proposed approach modifies the supervised boosting algorithm to a semisupervised learning algorithm by incorporating the unlabeled data . in this algorithm , we build a word aligner by using both the labeled data and the unlabeled data . then we build a pseudo reference set for the unlabeled data , and calculate the error rate of each word aligner using only the labeled data . based on this semisupervised boosting algorithm , we investigate two boosting methods for word alignment . in addition , we improve the word alignment results by combining the results of the two semi-supervised boosting methods . experimental results on word alignment indicate that semisupervised boosting achieves relative error reductions of 28.29 % and 19.52 % as compared with supervised boosting and unsupervised boosting , respectively .

a feature based approach to leveraging context for classifying carolyn penstein ros
on a multi-dimensional text categorization task , we compare the effectiveness of a feature based approach with the use of a stateof-the-art sequential learning technique that has proven successful for tasks such as email act classification . our evaluation demonstrates for the three separate dimensions of a well established annotation scheme that novel thread based features have a greater and more consistent impact on classification performance .

multi-human dialogue understanding for assisting
in this paper we present the dialogueunderstanding components of an architecture for assisting multi-human conversations in artifact-producing meetings : meetings in which tangible products such as project planning charts are created . novel aspects of our system include multimodal ambiguity resolution , modular ontologydriven artifact manipulation , and a meeting browser for use during and after meetings . we describe the software architecture and demonstrate the system using an example multimodal dialogue .

predicting discourse connectives for implicit discourse relation chew lim tan
existing works indicate that the absence of explicit discourse connectives makes it difficult to recognize implicit discourse relations . in this paper we attempt to overcome this difficulty for implicit relation recognition by automatically inserting discourse connectives between arguments with the use of a language model . then we propose two algorithms to leverage the information of these predicted connectives . one is to use these predicted implicit connectives as additional features in a supervised model . the other is to perform implicit relation recognition based only on these predicted connectives . results on penn discourse treebank 2.0 show that predicted discourse connectives help implicit relation recognition and the first algorithm can achieve an absolute average f-score improvement of 3 % over a state of the art baseline system .

a corpus study of evaluative and speculative language
this paper presents a corpus study of evaluative and speculative language . knowledge of such language would be useful in many applications , such as text categorization and summarization . analyses of annotator agreement and of characteristics of subjective language are performed . this study yields knowledge needed to design e ective machine learning systems for identifying subjective language .

mpowers : a multi points of view evaluation refinement studio
we present our multi point of view evaluation refinement studio ( mpowers ) , an application framework for spoken dialogue system evaluation that implements design conventions in a user-friendly interface . it ensures that all evaluator-users manipulate a unique shared corpus of data with a shared set of parameters to design and retrieve their evaluations . it therefore answers both the need for convergence among the evaluation practices and the consideration of several analytical points of view addressed by the evaluators involved in spoken dialogue system projects . after introducing the system architecture , we argue the solutions added value in supporting a both data-driven and goal-driven process . we conclude with future works and perspectives of improvement upheld by human processes .

babelnet : building a very large multilingual semantic network dipartimento di informatica simone paolo ponzetto
in this paper we present babelnet a very large , wide-coverage multilingual semantic network . the resource is automatically constructed by means of a methodology that integrates lexicographic and encyclopedic knowledge from wordnet and wikipedia . in addition machine translation is also applied to enrich the resource with lexical information for all languages . we conduct experiments on new and existing gold-standard datasets to show the high quality and coverage of the resource .

assigning time-stamps to event-clauses
we describe a procedure for arranging into a time-line the contents of news stories describing the development of some situation . we describe the parts of the system that deal with 1. breaking sentences into event-clauses and 2. resolving both explicit and implicit temporal references . evaluations show a performance of 52 % , compared to humans .

active sample selection for named entity transliteration
this paper introduces a new method for identifying named-entity ( ne ) transliterations within bilingual corpora . current state-of-theart approaches usually require annotated data and relevant linguistic knowledge which may not be available for all languages . we show how to effectively train an accurate transliteration classifier using very little data , obtained automatically . to perform this task , we introduce a new active sampling paradigm for guiding and adapting the sample selection process . we also investigate how to improve the classifier by identifying repeated patterns in the training data . we evaluated our approach using english , russian and hebrew corpora .

topic independent identification of agreement and disagreement in social media dialogue
research on the structure of dialogue has been hampered for years because large dialogue corpora have not been available . this has impacted the dialogue research communitys ability to develop better theories , as well as good off-the-shelf tools for dialogue processing . happily , an increasing amount of information and opinion exchange occur in natural dialogue in online forums , where people share their opinions about a vast range of topics . in particular we are interested in rejection in dialogue , also called disagreement and denial , where the size of available dialogue corpora , for the first time , offers an opportunity to empirically test theoretical accounts of the expression and inference of rejection in dialogue . in this paper , we test whether topic-independent features motivated by theoretical predictions can be used to recognize rejection in online forums in a topic-independent way . our results show that our theoretically motivated features achieve 66 % accuracy , an improvement over a unigram baseline of an absolute 6 % .

test collection selection and gold standard generation for a multiply-annotated opinion corpus
opinion analysis is an important research topic in recent years . however , there are no common methods to create evaluation corpora . this paper introduces a method for developing opinion corpora involving multiple annotators . the characteristics of the created corpus are discussed , and the methodologies to select more consistent testing collections and their corresponding gold standards are proposed . under the gold standards , an opinion extraction system is evaluated . the experiment results show some interesting phenomena .

making computers laugh : investigations in automatic humor recognition
humor is one of the most interesting and puzzling aspects of human behavior . despite the attention it has received in fields such as philosophy , linguistics , and psychology , there have been only few attempts to create computational models for humor recognition or generation . in this paper , we bring empirical evidence that computational approaches can be successfully applied to the task of humor recognition . through experiments performed on very large data sets , we show that automatic classification techniques can be effectively used to distinguish between humorous and non-humorous texts , with significant improvements observed over apriori known baselines .

experiments with geographic knowledge for information extraction
here we present work on using spatial knowledge in conjunction with information extraction ( ie ) . considerable volume of location data was imported in a knowledge base ( kb ) with entities of general importance used for semantic annotation , indexing , and retrieval of text . the semantic web knowledge representation standards are used , namely rdf ( s ) . an extensive upper-level ontology with more than two hundred classes is designed . with respect to the locations , the goal was to include the most important categories considering public and tasks not specially related to geography or related areas . the locations data is derived from number of publicly available resources and combined to assure best performance for domainindependent named-entity recognition in text . an evaluation and comparison to high performance ie application is given .

reranking with linguistic and semantic features for arabic optical character recognition pradeep dasigi mona diab
optical character recognition ( ocr ) systems for arabic rely on information contained in the scanned images to recognize sequences of characters and on language models to emphasize fluency . in this paper we incorporate linguistically and semantically motivated features to an existing ocr system . to do so we follow an n-best list reranking approach that exploits recent advances in learning to rank techniques . we achieve 10.1 % and 11.4 % reduction in recognition word error rate ( wer ) relative to a standard baseline system on typewritten and handwritten arabic respectively .

fipscoview : on-line visualisation of collocations extracted from multilingual parallel corpora
we introduce fipscoview , an on-line interface for dictionary-like visualisation of collocations detected from parallel corpora using a syntactically-informed extraction method .

japanese-spanish thesaurus construction using english as a pivot jessica ramrez , masayuki asahara , yuji matsumoto
we present the results of research with the goal of automatically creating a multilingual thesaurus based on the freely available resources of wikipedia and wordnet . our goal is to increase resources for natural language processing tasks such as machine translation targeting the japanese-spanish language pair . given the scarcity of resources , we use existing english resources as a pivot for creating a trilingual japanesespanish-english thesaurus . our approach consists of extracting the translation tuples from wikipedia , disambiguating them by mapping them to wordnet word senses . we present results comparing two methods of disambiguation , the first using vsm on wikipedia article texts and wordnet definitions , and the second using categorical information extracted from wikipedia , we find that mixing the two methods produces favorable results . using the proposed method , we have constructed a multilingual spanish-japanese-english thesaurus consisting of 25,375 entries . the same method can be applied to any pair of languages that are linked to english in wikipedia .

predicting dialect variation in immigrant contexts using light verb constructions
languages spoken by immigrants change due to contact with the local languages . capturing these changes is problematic for current language technologies , which are typically developed for speakers of the standard dialect only . even when dialectal variants are available for such technologies , we still need to predict which dialect is being used . in this study , we distinguish between the immigrant and the standard dialect of turkish by focusing on light verb constructions . we experiment with a number of grammatical and contextual features , achieving over 84 % accuracy ( 56 % baseline ) .

developing a typology of dialogue acts : some boundary problems
the paper gives an overview of a typology of dialogue acts used for annotating estonian spoken dialogues . several problems of the classification and determining of dialogue acts are considered . our further aim is to develop a dialogue system which can interact with the user in natural language following the norms and rules of human-human communication .

integrating dictionaries into an unsupervised model for myanmar word segmentation ye kyaw thu keihanna science city keihanna science city keihanna science city
this paper addresses the problem of word segmentation for low resource languages , with the main focus being on myanmar language . in our proposed method , we focus on exploiting limited amounts of dictionary resource , in an attempt to improve the segmentation quality of an unsupervised word segmenter . three models are proposed . in the first , a set of dictionaries ( separate dictionaries for different classes of words ) are directly introduced into the generative model . in the second , a language model was built from the dictionaries , and the n-gram model was inserted into the generative model . this model was expected to model words that did not occur in the training data . the third model was a combination of the previous two models . we evaluated our approach on a corpus of manually annotated data . our results show that the proposed methods are able to improve over a fully unsupervised baseline system . the best of our systems improved the f-score from 0.48 to 0.66. in addition to segmenting the data , one proposed method is also able to partially label the segmented corpus with pos tags .

a character-based joint model behavior design corporation
this paper presents a chinese word segmentation system for the closed track of cips-sighan word segmentation bakeoff 2010. this system adopts a character-based joint approach , which combines a character-based generative model and a character-based discriminative model . to further improve the crossdomain performance , we use an additional semi-supervised learning procedure to incorporate the unlabeled corpus . the final performance on the closed track for the simplified-character text shows that our system achieves comparable results with other state-of-the-art systems .

designing special post-processing rules for svm-based chinese word segmentation
we participated in the third international chinese word segmentation bakeoff . specifically , we evaluated our chinese word segmenter neucipseg in the close track , on all four corpora , namely academis sinica ( as ) , city university of hong kong ( cityu ) , microsoft research ( msra ) , and university of pennsylvania/university of colorado ( upenn ) . based on support vector machines ( svms ) , a basic segmenter is designed regarding chinese word segmentation as a problem of character-based tagging . moreover , we proposed postprocessing rules specially taking into account the properties of results brought out by the basic segmenter . our system achieved good ranks in all four corpora .

matching inconsistently spelled names in automatic speech recognizer output for information retrieval
many proper names are spelled inconsistently in speech recognizer output , posing a problem for applications where locating mentions of named entities is critical . we model the distortion in the spelling of a name due to the speech recognizer as the effect of a noisy channel . the models follow the framework of the ibm translation models . the model is trained using a parallel text of closed caption and automatic speech recognition output . we also test a string edit distance based method . the effectiveness of these models is evaluated on a name query retrieval task . our methods result in a 60 % improvement in f1 . we also demonstrate why the problem has not been critical in trec and tdt tasks .

consensus training for consensus decoding in machine translation
we propose a novel objective function for discriminatively tuning log-linear machine translation models . our objective explicitly optimizes the bleu score of expected n-gram counts , the same quantities that arise in forestbased consensus and minimum bayes risk decoding methods . our continuous objective can be optimized using simple gradient ascent . however , computing critical quantities in the gradient necessitates a novel dynamic program , which we also present here . assuming bleu as an evaluation measure , our objective function has two principle advantages over standard max bleu tuning . first , it specifically optimizes model weights for downstream consensus decoding procedures . an unexpected second benefit is that it reduces overfitting , which can improve test set bleu scores when using standard viterbi decoding .

collective stance classification of posts in online debate forums
online debate sites are a large source of informal and opinion-sharing dialogue on current socio-political issues . inferring users stance ( pro or con ) towards discussion topics in domains such as politics or news is an important problem , and is of utility to researchers , government organizations , and companies . predicting users stance supports identification of social and political groups , building of better recommender systems , and personalization of users information preferences to their ideological beliefs . in this paper , we develop a novel collective classification approach to stance classification , which makes use of both structural and linguistic features , and which collectively labels the posts stance across a network of the users posts . we identify both linguistic features of the posts and features that capture the underlying relationships between posts and users . we use probabilistic soft logic ( psl ) ( bach et al. , 2013 ) to model post stance by leveraging both these local linguistic features as well as the observed network structure of the posts to reason over the dataset . we evaluate our approach on 4forums ( walker et al. , 2012b ) , a collection of discussions from an online debate site on issues ranging from gun control to gay marriage . we show that our collective classification model is able to easily incorporate rich , relational information and outperforms a local model which uses only linguistic information .

chinese named entity recognition based on multiple features
this paper proposes a hybrid chinese named entity recognition model based on multiple features . it differentiates from most of the previous approaches mainly as follows . firstly , the proposed hybrid model integrates coarse particle feature ( pos model ) with fine particle feature ( word model ) , so that it can overcome the disadvantages of each other . secondly , in order to reduce the searching space and improve the efficiency , we introduce heuristic human knowledge into statistical model , which could increase the performance of ner significantly . thirdly , we use three sub-models to respectively describe three kinds of transliterated person name , that is , japanese , russian and euramerican person name , which can improve the performance of pn recognition . from the experimental results on people 's daily testing data , we can conclude that our hybrid model is better than the models which only use one kind of features . and the experiments on met-2 testing data also confirm the above conclusion , which show that our algorithm has consistence on different testing data .

nonparametric learning of phonological constraints in optimality uc san diego uc san diego
we present a method to jointly learn features and weights directly from distributional data in a log-linear framework . specifically , we propose a non-parametric bayesian model for learning phonological markedness constraints directly from the distribution of input-output mappings in an optimality theory ( ot ) setting . the model uses an indian buffet process prior to learn the feature values used in the loglinear method , and is the first algorithm for learning phonological constraints without presupposing constraint structure . the model learns a system of constraints that explains observed data as well as the phonologically-grounded constraints of a standard analysis , with a violation structure corresponding to the standard constraints . these results suggest an alternative data-driven source for constraints instead of a fully innate constraint set .

towards terascale knowledge acquisition
although vast amounts of textual data are freely available , many nlp algorithms exploit only a minute percentage of it . in this paper , we study the challenges of working at the terascale . we present an algorithm , designed for the terascale , for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method . we focus on the accuracy of these two systems as a function of processing time and corpus size .

aligning chinese-english parallel parse trees : is it feasible
we investigate the feasibility of aligning chinese and english parse trees by examining cases of incompatibility between chinese-english parallel parse trees . this work is done in the context of an annotation project wherewe construct a parallel treebank by doingword and phrase alignments simultaneously . we discuss the most common incompatibility patterns identified within vps and nps and show that most cases of incompatibility are caused by divergent syntactic annotation standards rather than inherent cross-linguistic differences in language itself . this suggests that in principle it is feasible to align the parallel parse trees with somemodification of existing syntactic annotation guidelines . we believe this has implications for the use of parallel parse trees as an important resource for machine translation models .

lexicographic semirings for exact automata encoding of sequence models
in this paper we introduce a novel use of the lexicographic semiring and motivate its use for speech and language processing tasks . we prove that the semiring allows for exact encoding of backoff models with epsilon transitions . this allows for off-line optimization of exact models represented as large weighted finite-state transducers in contrast to implicit ( on-line ) failure transition representations . we present preliminary empirical results demonstrating that , even in simple intersection scenarios amenable to the use of failure transitions , the use of the more powerful lexicographic semiring is competitive in terms of time of intersection .

from global to local similarities : a graph-based contextualization method using distributional thesauri
after recasting the computation of a distributional thesaurus in a graph-based framework for term similarity , we introduce a new contextualization method that generates , for each term occurrence in a text , a ranked list of terms that are semantically similar and compatible with the given context . the framework is instantiated by the definition of term and context , which we derive from dependency parses in this work . evaluating our approach on a standard data set for lexical substitution , we show substantial improvements over a strong non-contextualized baseline across all parts of speech . in contrast to comparable approaches , our framework defines an unsupervised generative method for similarity in context and does not rely on the existence of lexical resources as a source for candidate expansions .

metonymy recognition using different kinds of context for a memory-based learner intelligent information and communication systems ( iics )
for the metonymy resolution task at semeval-2007 , the use of a memory-based learner to train classifiers for the identification of metonymic location names is investigated . metonymy is resolved on different levels of granularity , differentiating between literal and non-literal readings on the coarse level ; literal , metonymic , and mixed readings on the medium level ; and a number of classes covering regular cases of metonymy on a fine level . different kinds of context are employed to obtain different features : 1 ) a sequence of n1 synset ids representing subordination information for nouns and for verbs , 2 ) n2 prepositions , articles , modal , and main verbs in the same sentence , and 3 ) properties of n3 tokens in a context window to the left and to the right of the location name . different classifiers were trained on the mascara data set to determine which values for the context sizes n1 , n2 , and n3 yield the highest accuracy ( n1 = 4 , n2 = 3 , and n3 = 7 , determined with the leave-oneout method ) . results from these classifiers served as features for a combined classifier . in the training phase , the combined classifier achieved a considerably higher precision for the mascara data . in the semeval submission , an accuracy of 79.8 % on the coarse , 79.5 % on the medium , and 78.5 % on the fine level is achieved ( the baseline accuracy is 79.4 % ) .

named entities translation based on comparable corpora
in this paper we present a system for translating named entities from basque to spanish based on comparable corpora . for that purpose we have tried two approaches : one based on basque linguistic features , and a language-independent tool . for both tools we have used basquespanish comparable corpora , a bilingual dictionary and the web as resources .

integrating multiple dependency corpora for inducing wide-coverage japanese ccg resources
this paper describes a method of inducing wide-coverage ccg resources for japanese . while deep parsers with corpusinduced grammars have been emerging for some languages , those for japanese have not been widely studied , mainly because most japanese syntactic resources are dependency-based . our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into ccg derivations . the method is empirically evaluated in terms of the coverage of the obtained lexicon and the accuracy of parsing .

corpus effects on the evaluation of automated transliteration systems sarvnaz karimi andrew turpin falk scholer
most current machine transliteration systems employ a corpus of known sourcetarget word pairs to train their system , and typically evaluate their systems on a similar corpus . in this paper we explore the performance of transliteration systems on corpora that are varied in a controlled way . in particular , we control the number , and prior language knowledge of human transliterators used to construct the corpora , and the origin of the source words that make up the corpora . we find that the word accuracy of automated transliteration systems can vary by up to 30 % ( in absolute terms ) depending on the corpus on which they are run . we conclude that at least four human transliterators should be used to construct corpora for evaluating automated transliteration systems ; and that although absolute word accuracy metrics may not translate across corpora , the relative rankings of system performance remains stable across differing corpora .

evaluating term extraction methods for interpreters
the study investigates term extraction methods using comparable corpora for interpreters . simultaneous interpreting requires efficient use of highly specialised domain-specific terminology in the working languages of an interpreter with limited time to prepare for new topics . we evaluate several terminology extraction methods for chinese and english using settings which replicate real-life scenarios , concerning the task difficulty , the range of terms and the amount of materials available , etc . we also investigate interpreters perception on the usefulness of automatic termlists . the results show the accuracy of the terminology extraction pipelines is not perfect , as their precision ranges from 27 % on short texts to 83 % on longer corpora for english , 24 % to 31 % on chinese . nevertheless , the use of even small corpora for specialised topics greatly facilitates interpreters in their preparation .

generating spatio-temporal descriptions in pollen forecasts
we describe our initial investigations into generating textual summaries of spatiotemporal data with the help of a prototype natural language generation ( nlg ) system that produces pollen forecasts for scotland .

text specificity and impact on quality of news summaries
in our work we use an existing classifier to quantify and analyze the level of specific and general content in news documents and their human and automatic summaries . we discover that while human abstracts contain a more balanced mix of general and specific content , automatic summaries are overwhelmingly specific . we also provide an analysis of summary specificity and the summary quality scores assigned by people . we find that too much specificity could adversely affect the quality of content in the summary . our findings give strong evidence for the need for a new task in abstractive summarization : identification and generation of general sentences .

using machine learning techniques to build a comma checker
in this paper , we describe the research using machine learning techniques to build a comma checker to be integrated in a grammar checker for basque . after several experiments , and trained with a little corpus of 100,000 words , the sys tem guesses correctly not placing com mas with a precision of 96 % and a re call of 98 % . it also gets a precision of 70 % and a recall of 49 % in the task of placing commas . finally , we have shown that these results can be improved using a bigger and a more homogeneous corpus to train , that is , a bigger corpus written by one unique author .

bootstrapping without the boot
bootstrapping methods for learning require a small amount of supervision to seed the learning process . we show that it is sometimes possible to eliminate this last bit of supervision , by trying many candidate seeds and selecting the one with the most plausible outcome . we discuss such strapping methods in general , and exhibit a particular method for strapping wordsense classifiers for ambiguous words . our experiments on the canadian hansards show that our unsupervised technique is significantly more effective than picking seeds by hand ( yarowsky , 1995 ) , which in turn is known to rival supervised methods .

two tools for creating and visualizing sub-sentential alignments of parallel text
we present two web-based , interactive tools for creating and visualizing sub-sentential alignments of parallel text . yawat is a tool to support distributed , manual word- and phrase-alignment of parallel text through an intuitive , web-based interface . kwipc is an interface for displaying words or bilingual word pairs in parallel , word-aligned context . a key element of the tools presented here is the interactive visualization : alignment information is shown only for one pair of aligned words or phrases at a time . this allows users to explore the alignment space interactively without being overwhelmed by the amount of information available .

learning sentential paraphrases from bilingual parallel corpora for text-to-text generation
previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora . however , it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases , which are more obviously learnable from monolingual parallel corpora . we extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations , including passivization , dative shift , and topicalization . we discuss how our model can be adapted to many text generation tasks by augmenting its feature set , development data , and parameter estimation routine . we illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems .

forest reranking : discriminative parsing with non-local features
conventional n-best reranking techniques often suffer from the limited scope of the nbest list , which rules out many potentially good alternatives . we instead propose forest reranking , a method that reranks a packed forest of exponentially many parses . since exact inference is intractable with non-local features , we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole treebank . our final result , an f-score of 91.7 , outperforms both 50-best and 100-best reranking baselines , and is better than any previously reported systems trained on the treebank .

annotation of regular polysemy and underspecification hector martnez alonso , bolette sandford pedersen
we present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in english , danish and spanish . this article describes the annotation process , the results in terms of inter-encoder agreement , and the sense distributions obtained with two methods : majority voting with a theory-compliant backoff strategy , and mace , an unsupervised system to choose the most likely sense from all the annotations .

coupling hierarchical word reordering and decoding in phrase-based statistical machine translation
in this paper , we start with the existing idea of taking reordering rules automatically derived from syntactic representations , and applying them in a preprocessing step before translation to make the source sentence structurally more like the target ; and we propose a new approach to hierarchically extracting these rules . we evaluate this , combined with a lattice-based decoding , and show improvements over stateof-the-art distortion models .

open entity extraction from web search query logs
in this paper we propose a completely unsupervised method for open-domain entity extraction and clustering over query logs . the underlying hypothesis is that classes defined by mining search user activity may significantly differ from those typically considered over web documents , in that they better model the user space , i.e . users perception and interests . we show that our method outperforms state of the art ( semi- ) supervised systems based either on web documents or on query logs ( 16 % gain on the clustering task ) . we also report evidence that our method successfully supports a real world application , namely keyword generation for sponsored search .

using conditional random fields to extract contexts and answers of questions from online forums
online forum discussions often contain vast amounts of questions that are the focuses of discussions . extracting contexts and answers together with the questions will yield not only a coherent forum summary but also a valuable qa knowledge base . in this paper , we propose a general framework based on conditional random fields ( crfs ) to detect the contexts and answers of questions from forum threads . we improve the basic framework by skip-chain crfs and 2d crfs to better accommodate the features of forums for better performance . experimental results show that our techniques are very promising .

towards conversational qa : automatic identification of problematic
to enable conversational qa , it is important to examine key issues addressed in conversational systems in the context of question answering . in conversational systems , understanding user intent is critical to the success of interaction . recent studies have also shown that the capability to automatically identify problematic situations during interaction can significantly improve the system performance . therefore , this paper investigates the new implications of user intent and problematic situations in the context of question answering . our studies indicate that , in basic interactive qa , there are different types of user intent that are tied to different kinds of system performance ( e.g. , problematic/error free situations ) . once users are motivated to find specific information related to their information goals , the interaction context can provide useful cues for the system to automatically identify problematic situations and user intent .

the role of sentence structure in recognizing textual entailment
recent research suggests that sentence structure can improve the accuracy of recognizing textual entailments and paraphrasing . although background knowledge such as gazetteers , wordnet and custom built knowledge bases are also likely to improve performance , our goal in this paper is to characterize the syntactic features alone that aid in accurate entailment prediction . we describe candidate features , the role of machine learning , and two final decision rules . these rules resulted in an accuracy of 60.50 and 65.87 % and average precision of 58.97 and 60.96 % in rte3test and suggest that sentence structure alone can improve entailment accuracy by 9.25 to 14.62 % over the baseline majority class .

nerdle : topic-specific question answering using wikia seeds
the wikia project maintains wikis across a diverse range of subjects from areas of popular culture . each wiki consists of collaboratively authored content and focuses on a particular topic , including franchises such as star trek , star wars and the simpsons . in this paper , we investigate the use of such wikis to create question-answering ( qa ) systems for a given topic . our key idea is to use a wiki as seed to gather large amounts of relevant text and to use semantic role labeling ( srl ) methods to extract n-ary facts from this data . by applying our method to very large amounts of topically focused text , we propose to address the coverage issues that have been noted for qa systems built using such techniques . to illustrate the strengths and weaknesses of the proposed approach , we make a web demonstrator of our system publicly available ; it provides a qa view that enables users to pose natural language questions to the system and that visualizes how questions are interpreted and matched to answers . in addition , the demonstrator provides a graph exploration view in which users can directly browse the fact base in order to inspect the scope of the extracted information .

exploiting syntactico-semantic structures for relation extraction
in this paper , we observe that there exists a second dimension to the relation extraction ( re ) problem that is orthogonal to the relation type dimension . we show that most of these second dimensional structures are relatively constrained and not difficult to identify . we propose a novel algorithmic approach to re that starts by first identifying these structures and then , within these , identifying the semantic type of the relation . in the real re problem where relation arguments need to be identified , exploiting these structures also allows reducing pipelined propagated errors . we show that this re framework provides significant improvement in re performance .

crf tagging for head recognition based on stanford parser
chinese parsing has received more and more attention , and in this paper , we use toolkit to perform parsing on the data of tsinghua chinese treebank ( tct ) used in cips , and we use conditional random fields ( crfs ) to train specific model for the head recognition . at last , we compare different results on different pos results .

spoken arabic dialect identification using phonotactic modeling
the arabic language is a collection of multiple variants , among which modern standard arabic ( msa ) has a special status as the formal written standard language of the media , culture and education across the arab world . the other variants are informal spoken dialects that are the media of communication for daily life . arabic dialects differ substantially from msa and each other in terms of phonology , morphology , lexical choice and syntax . in this paper , we describe a system that automatically identifies the arabic dialect ( gulf , iraqi , levantine , egyptian and msa ) of a speaker given a sample of his/her speech . the phonotactic approach we use proves to be effective in identifying these dialects with considerable overall accuracy 81.60 % using 30s test utterances .

wikibabel : a wiki-style platform for creation of parallel data
in this demo , we present a wiki-style platform wikibabel that enables easy collaborative creation of multilingual content in many nonenglish wikipedias , by leveraging the relatively larger and more stable content in the english wikipedia . the platform provides an intuitive user interface that maintains the user focus on the multilingual wikipedia content creation , by engaging search tools for easy discoverability of related english source material , and a set of linguistic and collaborative tools to make the content translation simple . we present two different usage scenarios and discuss our experience in testing them with real users . such integrated content creation platform in wikipedia may yield as a by-product , parallel corpora that are critical for research in statistical machine translation systems in many languages of the world .

a generalized vector space model for text retrieval based on semantic relatedness
generalized vector space models ( gvsm ) extend the standard vector space model ( vsm ) by embedding additional types of information , besides terms , in the representation of documents . an interesting type of information that can be used in such models is semantic information from word thesauri like wordnet . previous attempts to construct gvsm reported contradicting results . the most challenging problem is to incorporate the semantic information in a theoretically sound and rigorous manner and to modify the standard interpretation of the vsm . in this paper we present a new gvsm model that exploits wordnets semantic information . the model is based on a new measure of semantic relatedness between terms . experimental study conducted in three trec collections reveals that semantic information can boost text retrieval performance with the use of the proposed gvsm .

incorporating temporal and semantic information with eye gaze for automatic word acquisition in multimodal conversational systems
one major bottleneck in conversational systems is their incapability in interpreting unexpected user language inputs such as out-ofvocabulary words . to overcome this problem , conversational systems must be able to learn new words automatically during human machine conversation . motivated by psycholinguistic findings on eye gaze and human language processing , we are developing techniques to incorporate human eye gaze for automatic word acquisition in multimodal conversational systems . this paper investigates the use of temporal alignment between speech and eye gaze and the use of domain knowledge in word acquisition . our experiment results indicate that eye gaze provides a potential channel for automatically acquiring new words . the use of extra temporal and domain knowledge can significantly improve acquisition performance .

aspect extraction with automated prior knowledge learning
aspect extraction is an important task in sentiment analysis . topic modeling is a popular method for the task . however , unsupervised topic models often generate incoherent aspects . to address the issue , several knowledge-based models have been proposed to incorporate prior knowledge provided by the user to guide modeling . in this paper , we take a major step forward and show that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the web . such knowledge can then be used by a topic model to discover more coherent aspects . there are two key challenges : ( 1 ) learning quality knowledge from reviews of diverse domains , and ( 2 ) making the model fault-tolerant to handle possibly wrong knowledge . a novel approach is proposed to solve these problems . experimental results using reviews from 36 domains show that the proposed approach achieves significant improvements over state-of-the-art baselines .

multilingual subjectivity : are more languages better
while subjectivity related research in other languages has increased , most of the work focuses on single languages . this paper explores the integration of features originating from multiple languages into a machine learning approach to subjectivity analysis , and aims to show that this enriched feature set provides for more effective modeling for the source as well as the target languages . we show not only that we are able to achieve over 75 % macro accuracy in all of the six languages we experiment with , but also that by using features drawn from multiple languages we can construct high-precision meta-classifiers with a precision of over 83 % .

a supervised learning approach to automatic synonym identification based on distributional features
distributional similarity has been widely used to capture the semantic relatedness of words in many nlp tasks . however , various parameters such as similarity measures must be handtuned to make it work effectively . instead , we propose a novel approach to synonym identification based on supervised learning and distributional features , which correspond to the commonality of individual context types shared by word pairs . considering the integration with pattern-based features , we have built and compared five synonym classifiers . the evaluation experiment has shown a dramatic performance increase of over 120 % on the f-1 measure basis , compared to the conventional similarity-based classification . on the other hand , the pattern-based features have appeared almost redundant .

use of coreference in automatic searching for multiword discourse markers in the prague dependency treebank magdalna rysov ji mrovsk
the paper introduces a possibility of new research offered by a multi-dimensional annotation of the prague dependency treebank . it focuses on exploitation of the annotation of coreference for the annotation of discourse relations expressed by multiword expressions . it tries to find which aspect interlinks these linguistic areas and how we can use this interplay in automatic searching for czech expressions like despite this ( navzdory tomu ) , because of this fact ( dky tto skutenosti ) functioning as multiword discourse markers .

hybrid multilingual parsing with hpsg for srl
in this paper we present our syntactic and semantic dependency parsing system submitted to both the closed and open challenges of the conll 2009 shared task . the system extends the system of zhang , wang , & uszkoreit ( 2008 ) in the multilingual direction , and achieves 76.49 average macro f1 score on the closed joint task . substantial improvements to the open srl task have been observed that are attributed to the hpsg parses with handcrafted grammars .

towards metadata interoperability mpi for psycholinguistics mpi for psycholinguistics
within two european projects metadata interoperability is one of the central topics . while the intera project has as one of its goals to achieve an interoperability between two widely used metadata sets for the domain of language resources , the echo project created an integrated metadata domain of in total nine data providers from five different disciplines from the humanities . in both projects ad hoc techniques are used to achieve results . in the intera project , however , machine readable and iso compliant concept definitions are created as a first step towards the semantic web . in the echo project a complex ontology was realized purely relying on xml . it is argued that concept definitions should be registered in open data category repositories and that relations between them should be described as rdf assertions . yet we are missing standards that would allow us to overcome the ad hoc solutions .

towards automated related work summarization cong duy vu hoang and min-yen kan
we introduce the novel problem of automatic related work summarization . given multiple articles ( e.g. , conference/journal papers ) as input , a related work summarization system creates a topic-biased summary of related work specific to the target paper . our prototype related work summarization system , rewos , takes in set of keywords arranged in a hierarchical fashion that describes a target papers topics , to drive the creation of an extractive summary using two different strategies for locating appropriate sentences for general topics as well as detailed ones . our initial results show an improvement over generic multi-document summarization baselines in a human evaluation .

ehu-alm : similarity-feature based approach for student response basque foundation for science
we present a 5-way supervised system based on syntactic-semantic similarity features . the model deploys : text overlap measures , wordnet-based lexical similarities , graphbased similarities , corpus-based similarities , syntactic structure overlap and predicateargument overlap measures . these measures are applied to question , reference answer and student answer triplets . we take into account the negation in the syntactic and predicateargument overlap measures . our system uses the domain-specific data as one dataset to build a robust system . the results show that our system is above the median and mean on all the evaluation scenarios of the semeval2013 task # 7 .

extraction programs : a unified approach to translation rule extraction sdl language technologies division
we provide a general algorithmic schema for translation rule extraction and show that several popular extraction methods ( including phrase pair extraction , hierarchical phrase pair extraction , and ghkm extraction ) can be viewed as specific instances of this schema . this work is primarily intended as a survey of the dominant extraction paradigms , in which we make explicit the close relationship between these approaches , and establish a language for future hybridizations . this facilitates a generic and extensible implementation of alignment-based extraction methods .

why nitpicking works : evidence for occams razor in error correctors
empirical experience and observations have shown us when powerful and highly tunable classifiers such as maximum entropy classifiers , boosting and svms are applied to language processing tasks , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point . to further improve performance , various error correction mechanisms have been developed , but in practice , most of them can not be relied on to predictably improve performance on unseen data ; indeed , depending upon the test set , they are as likely to degrade accuracy as to improve it . this problem is especially severe if the base classifier has already been finely tuned . in recent work , we introduced n-fold templated piped correction , or ntpc ( nitpick ) , an intriguing error corrector that is designed to work in these extreme operating conditions . despite its simplicity , it consistently and robustly improves the accuracy of existing highly accurate base models . this paper investigates some of the more surprising claims made by ntpc , and presents experiments supporting an occams razor argument that more complex models are damaging or unnecessary in practice .

unsupervised discovery of rhyme schemes
this paper describes an unsupervised , language-independent model for finding rhyme schemes in poetry , using no prior knowledge about rhyme or pronunciation .

combining morpheme-based machine translation with post-processing morpheme prediction
this paper extends the training and tuning regime for phrase-based statistical machine translation to obtain fluent translations into morphologically complex languages ( we build an english to finnish translation system ) . our methods use unsupervised morphology induction . unlike previous work we focus on morphologically productive phrase pairs our decoder can combine morphemes across phrase boundaries . morphemes in the target language may not have a corresponding morpheme or word in the source language . therefore , we propose a novel combination of post-processing morphology prediction with morpheme-based translation . we show , using both automatic evaluation scores and linguistically motivated analyses of the output , that our methods outperform previously proposed ones and provide the best known results on the englishfinnish europarl translation task . our methods are mostly language independent , so they should improve translation into other target languages with complex morphology .

unsupervised domain relevance estimation for word sense disambiguation
this paper presents domain relevance estimation ( dre ) , a fully unsupervised text categorization technique based on the statistical estimation of the relevance of a text with respect to a certain category . we use a pre-defined set of categories ( we call them domains ) which have been previously associated to wordnet word senses . given a certain domain , dre distinguishes between relevant and non-relevant texts by means of a gaussian mixture model that describes the frequency distribution of domain words inside a large-scale corpus . then , an expectation maximization algorithm computes the parameters that maximize the likelihood of the model on the empirical data . the correct identification of the domain of the text is a crucial point for domain driven disambiguation , an unsupervised word sense disambiguation ( wsd ) methodology that makes use of only domain information . therefore , dre has been exploited and evaluated in the context of a wsd task . results are comparable to those of state-ofthe-art unsupervised wsd systems and show that dre provides an important contribution .

weakly supervised natural language learning without redundant views
we investigate single-view algorithms as an alternative to multi-view algorithms for weakly supervised learning for natural language processing tasks without a natural feature split . in particular , we apply co-training , self-training , and em to one such task and find that both selftraining and fs-em , a new variation of em that incorporates feature selection , outperform cotraining and are comparatively less sensitive to parameter changes .

submodularity for data selection in statistical machine translation
we introduce submodular optimization to the problem of training data subset selection for statistical machine translation ( smt ) . by explicitly formulating data selection as a submodular program , we obtain fast scalable selection algorithms with mathematical performance guarantees , resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible . we present a new class of submodular functions designed specifically for smt and evaluate them on two different translation tasks . our results show that our best submodular method significantly outperforms several baseline methods , including the widely-used cross-entropy based data selection method . in addition , our approach easily scales to large data sets and is applicable to other data selection problems in natural language processing .

iitb-sentiment-analysts : participation in sentiment analysis
we propose a method for using discourse relations for polarity detection of tweets . we have focused on unstructured and noisy text like tweets on which linguistic tools like parsers and pos-taggers dont work properly . we have showed how conjunctions , connectives , modals and conditionals affect the sentiments in tweets . we have also handled the commonly used abbreviations , slangs and collocations which are usually used in short text messages like tweets . this work focuses on a web based application which produces results in real time . this approach is an extension of the previous work ( mukherjee et al 2012 ) .

the automatic generation of formal annotations in a multimedia mpi for psycholinguistics
we describe in this paper the mumis project ( multimedia indexing and searching environment ) 1 , which is concerned with the development and integration of base technologies , demonstrated within a laboratory prototype , to support automated multimedia indexing and to facilitate search and retrieval from multimedia databases . we stress the role linguistically motivated annotations , coupled with domain-specific information , can play within this environment . the project will demonstrate that innovative technology components can operate on multilingual , multisource , and multimedia information and create a meaningful and queryable database .

towards incremental speech generation in dialogue systems
we present a first step towards a model of speech generation for incremental dialogue systems . the model allows a dialogue system to incrementally interpret spoken input , while simultaneously planning , realising and selfmonitoring the system response . the model has been implemented in a general dialogue system framework . using this framework , we have implemented a specific application and tested it in a wizard-of-oz setting , comparing it with a non-incremental version of the same system . the results show that the incremental version , while producing longer utterances , has a shorter response time and is perceived as more efficient by the users .

word ordering with phrase-based grammars
we describe an approach to word ordering using modelling techniques from statistical machine translation . the system incorporates a phrase-based model of string generation that aims to take unordered bags of words and produce fluent , grammatical sentences . we describe the generation grammars and introduce parsing procedures that address the computational complexity of generation under permutation of phrases . against the best previous results reported on this task , obtained using syntax driven models , we report huge quality improvements , with bleu score gains of 20+ which we confirm with human fluency judgements . our system incorporates dependency language models , large n-gram language models , and minimum bayes risk decoding .

an unsupervised aspect-sentiment model for online reviews
with the increase in popularity of online review sites comes a corresponding need for tools capable of extracting the information most important to the user from the plain text data . due to the diversity in products and services being reviewed , supervised methods are often not practical . we present an unsupervised system for extracting aspects and determining sentiment in review text . the method is simple and flexible with regard to domain and language , and takes into account the influence of aspect on sentiment polarity , an issue largely ignored in previous literature . we demonstrate its effectiveness on both component tasks , where it achieves similar results to more complex semi-supervised methods that are restricted by their reliance on manual annotation and extensive knowledge sources .

edinburghs phrase - based machine translation systems for wmt-14
this paper describes the university of edinburghs ( uedin ) phrase-based submissions to the translation and medical translation shared tasks of the 2014 workshop on statistical machine translation ( wmt ) . we participated in all language pairs . we have improved upon our 2013 system by i ) using generalized representations , specifically automatic word clusters for translations out of english , ii ) using unsupervised character-based models to translate unknown words in russianenglish and hindi-english pairs , iii ) synthesizing hindi data from closely-related urdu data , and iv ) building huge language on the common crawl corpus .

program in computational linguistics
we present the design of a professional masters program in computational linguistics . this program can be completed in one-year of full-time study , or two-three years of part-time study . originally designed for cs professionals looking for additional training , the program has evolved in flexibility to accommodate students from more diverse backgrounds and with more diverse goals .

an empirical investigation of discounting in cross-domain language models
we investigate the empirical behavior of ngram discounts within and across domains . when a language model is trained and evaluated on two corpora from exactly the same domain , discounts are roughly constant , matching the assumptions of modified kneser-ney lms . however , when training and test corpora diverge , the empirical discount grows essentially as a linear function of the n-gram count . we adapt a kneser-ney language model to incorporate such growing discounts , resulting in perplexity improvements over modified kneser-ney and jelinek-mercer baselines .

an efficient algorithm for easy-first non-directional dependency parsing
we present a novel deterministic dependency parsing algorithm that attempts to create the easiest arcs in the dependency structure first in a non-directional manner . traditional deterministic parsing algorithms are based on a shift-reduce framework : they traverse the sentence from left-to-right and , at each step , perform one of a possible set of actions , until a complete tree is built . a drawback of this approach is that it is extremely local : while decisions can be based on complex structures on the left , they can look only at a few words to the right . in contrast , our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step . this allows incorporation of features from already built structures both to the left and to the right of the attachment point . the parser learns both the attachment preferences and the order in which they should be performed . the result is a deterministic , best-first , o ( nlogn ) parser , which is significantly more accurate than best-first transition based parsers , and nears the performance of globally optimized parsing models .

an augmented three-pass system combination framework :
this paper describes the augmented threepass system combination framework of the dublin city university ( dcu ) mt group for the wmt 2010 system combination task . the basic three-pass framework includes building individual confusion networks ( cns ) , a super network , and a modified minimum bayes-risk ( mconmbr ) decoder . the augmented parts for wmt2010 tasks include 1 ) a rescoring component which is used to re-rank the n -best lists generated from the individual cns and the super network , 2 ) a new hypothesis alignment metric terp that is used to carry out english-targeted hypothesis alignment , and 3 ) more different backbone-based cns which are employed to increase the diversity of the mconmbr decoding phase . we took part in the combination tasks of englishto-czech and french-to-english . experimental results show that our proposed combination framework achieved 2.17 absolute points ( 13.36 relative points ) and 1.52 absolute points ( 5.37 relative points ) in terms of bleu score on english-toczech and french-to-english tasks respectively than the best single system . we also achieved better performance on human evaluation .

using the segmentation corpus to define an inventory of concatenative units for cantonese speech synthesis wai yi peggy wong yuk choi road , hung hom , kowloon ,
the problem of word segmentation affects all aspects of chinese language processing , including the development of text-to-speech synthesis systems . in synthesizing a hong kong cantonese text , for example , words must be identified in order to model fusion of coda [ p ] with initial [ h ] , and other similar effects that differentiate word-internal syllable boundaries from syllable edges that begin or end words . accurate segmentation is necessary also for developing any list of words large enough to identify the wordinternal cross-syllable sequences that must be recorded to model such effects using concatenated synthesis units . this paper describes our use of the segmentation corpus to constrain such units .

deep learning for chinese word segmentation and pos tagging
this study explores the feasibility of performing chinese word segmentation ( cws ) and pos tagging by deep learning . we try to avoid task-specific feature engineering , and use deep layers of neural networks to discover relevant features to the tasks . we leverage large-scale unlabeled data to improve internal representation of chinese characters , and use these improved representations to enhance supervised word segmentation and pos tagging models . our networks achieved close to state-of-theart performance with minimal computational cost . we also describe a perceptron-style algorithm for training the neural networks , as an alternative to maximum-likelihood method , to speed up the training process and make the learning algorithm easier to be implemented .

contextualizing semantic representations using syntactically enriched vector models
we present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion . it employs a systematic combination of first- and second-order context vectors . we apply our model to two different tasks and show that ( i ) it substantially outperforms previous work on a paraphrase ranking task , and ( ii ) achieves promising results on a wordsense similarity task ; to our knowledge , it is the first time that an unsupervised method has been applied to this task .

controlled language for geographical information system queries
natural language interfaces to spatial databases have not received a lot of attention in computational linguistics , in spite of the potential value of such systems for users of geographical information systems ( giss ) . this paper presents a controlled language for gis queries , solves some of the semantic problems for spatial inference in this language , and introduces a system that implements this controlled language as a novel interface for gis .

speech-enabled hybrid multilingual translation for mobile devices
this paper presents an architecture and a prototype for speech-to-speech translation on android devices , based on gf ( grammatical framework ) . from the users point of view , the advantage is that the system works off-line and yet has a lean size ; it also gives , as a bonus , grammatical information useful for language learners . from the developers point of view , the advantage is the open architecture that permits the customization of the system to new languages and for special purposes . thus the architecture can be used for controlled-language-like translators that deliver very high quality , which is the traditional strength of gf . however , this paper focuses on a general-purpose system that allows arbitrary input . it covers eight languages .

user modeling by using bag-of-behaviors for building a dialog system sensitive to the interlocutors internal state
when using spoken dialog systems in actual environments , users sometimes abandon the dialog without making any input utterance . to help these users before they give up , the system should know why they could not make an utterance . thus , we have examined a method to estimate the state of a dialog user by capturing the users non-verbal behavior even when the users utterance is not observed . the proposed method is based on vector quantization of multi-modal features such as non-verbal speech , feature points of the face , and gaze . the histogram of the vq code is used as a feature for determining the state . we call this feature the bagof-behaviors . according to the experimental results , we prove that the proposed method surpassed the results of conventional approaches and discriminated the target users states with an accuracy of more than 70 % .

exemplar-based word-space model for compositionality detection : shared task system description lexical computing ltd , uk
in this paper , we highlight the problems of polysemy in word space models of compositionality detection . most models represent each word as a single prototype-based vector without addressing polysemy . we propose an exemplar-based model which is designed to handle polysemy . this model is tested for compositionality detection and it is found to outperform existing prototype-based models . we have participated in the shared task ( biemann and giesbrecht , 2011 ) and our best performing exemplar-model is ranked first in two types of evaluations and second in two other evaluations .

the linguists search engine : an overview philip resnik aaron elkiss
the linguists search engine ( lse ) was designed to provide an intuitive , easy-touse interface that enables language researchers to seek linguistically interesting examples on the web , based on syntactic and lexical criteria . we briefly describe its user interface and architecture , as well as recent developments that include lse search capabilities for chinese .

using derivation trees for treebank error detection linguistic data consortium
this work introduces a new approach to checking treebank consistency . derivation trees based on a variant of tree adjoining grammar are used to compare the annotation of word sequences based on their structural similarity . this overcomes the problems of earlier approaches based on using strings of words rather than tree structure to identify the appropriate contexts for comparison . we report on the result of applying this approach to the penn arabic treebank and how this approach leads to high precision of error detection .

generating example contexts to illustrate a target word sense jack mostow weisi duan
learning a vocabulary word requires seeing it in multiple informative contexts . we describe a system to generate such contexts for a given word sense . rather than attempt to do word sense disambiguation on example contexts already generated or selected from a corpus , we compile information about the word sense into the context generation process . to evaluate the sense-appropriateness of the generated contexts compared to wordnet examples , three human judges chose which word sense ( s ) fit each example , blind to its source and intended sense . on average , one judge rated the generated examples as sense-appropriate , compared to two judges for the wordnet examples . although the systems precision was only half of wordnets , its recall was actually higher than wordnets , thanks to covering many senses for which wordnet lacks examples .

cluster-based language model for sentence retrieval in chinese
sentence retrieval plays a very important role in question answering system . in this paper , we present a novel cluster-based language model for sentence retrieval in chinese question answering which is motivated in part by sentence clustering and language model . sentence clustering is used to group sentences into clusters . language model is used to properly represent sentences , which is combined with sentences model , cluster/topic model and collection model . for sentence clustering , we propose two approaches that are onesentence-multi-topics and onesentence-one-topic respectively . from the experimental results on 807 chinese testing questions , we can conclude that the proposed cluster-based language model outperforms over the standard language model for sentence retrieval in chinese question answering .

proofreading human translations with an e-pen
proofreading translated text is a task aimed at checking for correctness , consistency , and appropriate writing style . while this has been typically done with a keyboard and a mouse , pen-based devices set an opportunity for making such corrections in a comfortable way , as if proofreading on physical paper . arguably , this way of interacting with a computer is very appropriate when a small number of modifications are required to achieve high-quality standards . in this paper , we propose a taxonomy of pen gestures that is tailored to machine translation review tasks , after human translator intervention . in addition , we evaluate the recognition accuracy of these gestures using a couple of popular gesture recognizers . finally , we comment on open challenges and limitations , and discuss possible avenues for future work .

the topology of synonymy and homonymy networks
semantic networks have been used successfully to explain access to the mental lexicon . topological analyses of these networks have focused on acquisition and generation . we extend this work to look at models that distinguish semantic relations . we find the scale-free properties of association networks are not found in synonymy-homonymy networks , and that this is consistent with studies of childhood acquisition of these relationships . we further find that distributional models of language acquisition display similar topological properties to these networks .

over lexical links and the follower graph
there is high demand for automated tools that assign polarity to microblog content such as tweets ( twitter posts ) , but this is challenging due to the terseness and informality of tweets in addition to the wide variety and rapid evolution of language in twitter . it is thus impractical to use standard supervised machine learning techniques dependent on annotated training examples . we do without such annotations by using label propagation to incorporate labels from a maximum entropy classifier trained on noisy labels and knowledge about word types encoded in a lexicon , in combination with the twitter follower graph . results on polarity classification for several datasets show that our label propagation approach rivals a model supervised with in-domain annotated tweets , and it outperforms the noisily supervised classifier it exploits as well as a lexicon-based polarity ratio classifier .

graph transformations in data-driven dependency parsing
transforming syntactic representations in order to improve parsing accuracy has been exploited successfully in statistical parsing systems using constituency-based representations . in this paper , we show that similar transformations can give substantial improvements also in data-driven dependency parsing . experiments on the prague dependency treebank show that systematic transformations of coordinate structures and verb groups result in a 10 % error reduction for a deterministic data-driven dependency parser . combining these transformations with previously proposed techniques for recovering nonprojective dependencies leads to state-ofthe-art accuracy for the given data set .

translation system for healthcare mark
we describe a highly interactive system for bidirectional , broad-coverage spoken language communication in the healthcare area . the paper briefly reviews the system 's interactive foundations , and then goes on to discuss in greater depth issues of practical usability . we present our translation shortcuts facility , which minimizes the need for interactive verification of sentences after they have been vetted once , considerably speeds throughput while maintaining accuracy , and allows use by minimally literate patients for whom any mode of text entry might be difficult . we also discuss facilities for multimodal input , in which handwriting , touch screen , and keyboard interfaces are offered as alternatives to speech input when appropriate . in order to deal with issues related to sheer physical awkwardness , we briefly mention facilities for hands-free or eyes-free operation of the system . finally , we point toward several directions for future improvement of the system .

optimizing textual entailment recognition using particle swarm fbk - irst
this paper introduces a new method to improve tree edit distance approach to textual entailment recognition , using particle swarm optimization . currently , one of the main constraints of recognizing textual entailment using tree edit distance is to tune the cost of edit operations , which is a difficult and challenging task in dealing with the entailment problem and datasets . we tried to estimate the cost of edit operations in tree edit distance algorithm automatically , in order to improve the results for textual entailment . automatically estimating the optimal values of the cost operations over all rte development datasets , we proved a significant enhancement in accuracy obtained on the test sets .

a case-based reasoning approach for speech corpus generation
success in speech recognition , but construction of a corpus pertaining to a specific application is a difficult task . this paper introduces a case-based reasoning system to generate natural language corpora . in comparison to traditional natural language generation approaches , this system overcomes the inflexibility of template-based methods while avoiding the linguistic sophistication of rule-based packages . the evaluation of the system indicates our approach is effective in generating users specifications or queries as 98 % of the generated sentences are grammatically correct . the study result also shows that the language model derived from the generated corpus can significantly outperform a general language model or a dictation grammar .

translators in the loop : understanding how they work with cat tools
the research that we have been carrying out at translators workplaces over the past few years has provided indications that some cat tools are not being used to their full potential or are even being ignored by the users they were ( or should have been ) designed for . since by nature humans seem to resist changing habits and procedures that do the job , it is easy to attribute that to the intransigence of older translators and shift the focus to designing new tools for digital natives . however , the cognitive demands of processing complex input in one language while producing and revising and/or assessing and revising output in another add a new dimension to the usual considerations of the human-machine loop of interaction , which may be independent of the translators age or experience . in fact , the productivity constraints that many professional translators work under means that they might be adjusting more to their tools than adjusting their tools settings to optimize their ( the translators ) performance . and if those tools have not been designed to meet their users cognitive and physical ergonomic needs , their use may actually slow down the translation process and have potentially detrimental effects on quality . maureen ehrensberger-dow is a canadian psycholinguist who has been involved in research into multilingualism and translation in switzerland for the past 15 years . she is professor of translation studies in the zurich university of applied sciences institute of translation and interpreting and principal investigator of the snsf-financed research projects capturing translation processes and the cognitive and physical ergonomics of translation . 28

interactive machine translation using hierarchical translation models
current automatic machine translation systems are not able to generate error-free translations and human intervention is often required to correct their output . alternatively , an interactive framework that integrates the human knowledge into the translation process has been presented in previous works . here , we describe a new interactive machine translation approach that is able to work with phrase-based and hierarchical translation models , and integrates error-correction all in a unified statistical framework . in our experiments , our approach outperforms previous interactive translation systems , and achieves estimated effort reductions of as much as 48 % relative over a traditional post-edition system .

simplified feature set for arabic named entity recognition
this paper introduces simplified yet effective features that can robustly identify named entities in arabic text without the need for morphological or syntactic analysis or gazetteers . a crf sequence labeling model is trained on features that primarily use character n-gram of leading and trailing letters in words and word n-grams . the proposed features help overcome some of the morphological and orthographic complexities of arabic . in comparing to results in the literature using arabic specific features such pos tags on the same dataset and same crf implementation , the results in this paper are lower by 2 f-measure points for locations , but are better by 8 points for organizations and 9 points for persons .

application of the tightness continuum measure to chinese information retrieval
most word segmentation methods employed in chinese information retrieval systems are based on a static dictionary or a model trained against a manually segmented corpus . these general segmentation approaches may not be optimal because they disregard information within semantic units . we propose a novel method for improving word-based chinese ir , which performs segmentation according to the tightness of phrases . in order to evaluate the effectiveness of our method , we employ a new test collection of 203 queries , which include a broad distribution of phrases with different tightness values . the results of our experiments indicate that our method improves ir performance as compared with a general word segmentation approach . the experiments also demonstrate the need for the development of better evaluation corpora .

unsupervised segmentation of chinese text by use of branching entropy
we propose an unsupervised segmentation method based on an assumption about language data : that the increasing point of entropy of successive characters is the location of a word boundary . a large-scale experiment was conducted by using 200 mb of unsegmented training data and 1 mb of test data , and precision of 90 % was attained with recall being around 80 % . moreover , we found that the precision was stable at around 90 % independently of the learning data size .

will my spoken dialogue system be a slow learner layla el asri
this paper presents a practical methodology for the integration of reinforcement learning during the design of a spoken dialogue system ( sds ) . it proposes a method that enables sds designers to know , in advance , the number of dialogues that their system will need in order to learn the value of each state-action couple . we ask the designer to provide a user model in a simple way . then , we run simulations with this model and we compute confidence intervals for the mean of the expected return of the state-action couples .

which side are you on identifying perspectives at the document and intelligent systems program
in this paper we investigate a new problem of identifying the perspective from which a document is written . by perspective we mean a point of view , for example , from the perspective of democrats or republicans . can computers learn to identify the perspective of a document not every sentence is written strongly from a perspective . can computers learn to identify which sentences strongly convey a particular perspective we develop statistical models to capture how perspectives are expressed at the document and sentence levels , and evaluate the proposed models on articles about the israeli-palestinian conflict . the results show that the proposed models successfully learn how perspectives are reflected in word usage and can identify the perspective of a document with high accuracy .

joint arc-factored parsing of syntactic and semantic dependencies
in this paper we introduce a joint arc-factored model for syntactic and semantic dependency parsing . the semantic role labeler predicts the full syntactic paths that connect predicates with their arguments . this process is framed as a linear assignment task , which allows to control some well-formedness constraints . for the syntactic part , we define a standard arc-factored dependency model that predicts the full syntactic tree . finally , we employ dual decomposition techniques to produce consistent syntactic and predicate-argument structures while searching over a large space of syntactic configurations . in experiments on the conll-2009 english benchmark we observe very competitive results .

semi-supervised recursive autoencoders for predicting sentiment distributions
we introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions . our method learns vector space representations for multi-word phrases . in sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets , such as movie reviews , without using any pre-defined sentiment lexica or polarity shifting rules . we also evaluate the models ability to predict sentiment distributions on a new dataset based on confessions from the experience project . the dataset consists of personal user stories annotated with multiple labels which , when aggregated , form a multinomial distribution that captures emotional reactions . our algorithm can more accurately predict distributions over such labels compared to several competitive baselines .

combining multiple models for speech information retrieval
in this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a speech information retrieval task . the formulas for combining the models are tuned on training data . then the system is evaluated on test data . the task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . also , the topics are real information needs , difficult to satisfy . information retrieval systems are not able to obtain good results on this data set , except for the case when manual summaries are included .

automatic extraction of semantic networks from text using leximancer
leximancer is a software system for performing conceptual analysis of text data in a largely language independent manner . the system is modelled on content analysis and provides unsupervised and supervised analysis using seeded concept classifiers . unsupervised ontology discovery is a key component .

generation of vp ellipsis : a corpus-based approach
we present conditions under which verb phrases are elided based on a corpus of positive and negative examples . factor that affect verb phrase ellipsis include : the distance between antecedent and ellipsis site , the syntactic relation between antecedent and ellipsis site , and the presence or absence of adjuncts . building on these results , we examine where in the generation architecture a trainable algorithm for vp ellipsis should be located . we show that the best performance is achieved when the trainable module is located after the realizer and has access to surfaceoriented features ( error rate of 7.5 % ) .

extracting parallel sentences from comparable corpora using document
the quality of a statistical machine translation ( smt ) system is heavily dependent upon the amount of parallel sentences used in training . in recent years , there have been several approaches developed for obtaining parallel sentences from non-parallel , or comparable data , such as news articles published within the same time period ( munteanu and marcu , 2005 ) , or web pages with a similar structure ( resnik and smith , 2003 ) . one resource not yet thoroughly explored is wikipedia , an online encyclopedia containing linked articles in many languages . we advance the state of the art in parallel sentence extraction by modeling the document level alignment , motivated by the observation that parallel sentence pairs are often found in close proximity . we also include features which make use of the additional annotation given by wikipedia , and features using an automatically induced lexicon model . results for both accuracy in sentence extraction and downstream improvement in an smt system are presented .

learning what to talk about in descriptive games
text generation requires a planning module to select an object of discourse and its properties . this is specially hard in descriptive games , where a computer agent tries to describe some aspects of a game world . we propose to formalize this problem as a markov decision process , in which an optimal message policy can be defined and learned through simulation . furthermore , we propose back-off policies as a novel and effective technique to fight state dimensionality explosion in this framework .

cross-lingual induction for deep broad-coverage syntax : a case study on german participles
this paper is a case study on cross-lingual induction of lexical resources for deep , broad-coverage syntactic analysis of german . we use a parallel corpus to induce a classifier for german participles which can predict their syntactic category . by means of this classifier , we induce a resource of adverbial participles from a huge monolingual corpus of german . we integrate the resource into a german lfg grammar and show that it improves parsing coverage while maintaining accuracy .

joint hebrew segmentation and parsing using a pcfg-la lattice parser
we experiment with extending a lattice parsing methodology for parsing hebrew ( goldberg and tsarfaty , 2008 ; golderg et al , 2009 ) to make use of a stronger syntactic model : the pcfg-la berkeley parser . we show that the methodology is very effective : using a small training set of about 5500 trees , we construct a parser which parses and segments unsegmented hebrew text with an f-score of almost 80 % , an error reduction of over 20 % over the best previous result for this task . this result indicates that lattice parsing with the berkeley parser is an effective methodology for parsing over uncertain inputs .

relevance feedback models for recommendation
we extended language modeling approaches in information retrieval ( ir ) to combine collaborative filtering ( cf ) and content-based filtering ( cbf ) . our approach is based on the analogy between ir and cf , especially between cf and relevance feedback ( rf ) . both cf and rf exploit users preference/relevance judgments to recommend items . we first introduce a multinomial model that combines cf and cbf in a language modeling framework . we then generalize the model to another multinomial model that approximates the polya distribution . this generalized model outperforms the multinomial model by 3.4 % for cbf and 17.4 % for cf in recommending english wikipedia articles . the performance of the generalized model for three different datasets was comparable to that of a state-of-theart item-based cf method .

a debug tool for practical grammar development
we have developed willex , a tool that helps grammar developers to work efficiently by using annotated corpora and recording parsing errors . willex has two major new functions . first , it decreases ambiguity of the parsing results by comparing them to an annotated corpus and removing wrong partial results both automatically and manually . second , willex accumulates parsing errors as data for the developers to clarify the defects of the grammar statistically . we applied willex to a large-scale hpsg-style grammar as an example .

contradiction-focused qualitative evaluation of textual entailment
in this paper we investigate the relation between positive and negative pairs in textual entailment ( te ) , in order to highlight the role of contradiction in te datasets . we base our analysis on the decomposition of text-hypothesis pairs into monothematic pairs , i.e . pairs where only one linguistic phenomenon at a time is responsible for entailment judgment and we argue that such a deeper inspection of the linguistic phenomena behind textual entailment is necessary in order to highlight the role of contradiction . we support our analysis with a number of empirical experiments , which use current available te systems .

recognizing partial textual entailment omer levy torsten zesch ido dagan iryna gurevych
textual entailment is an asymmetric relation between two text fragments that describes whether one fragment can be inferred from the other . it thus can not capture the notion that the target fragment is almost entailed by the given text . the recently suggested idea of partial textual entailment may remedy this problem . we investigate partial entailment under the faceted entailment model and the possibility of adapting existing textual entailment methods to this setting . indeed , our results show that these methods are useful for recognizing partial entailment . we also provide a preliminary assessment of how partial entailment may be used for recognizing ( complete ) textual entailment .

an efficient clustering algorithm for class-based language models takuya matsuzaki yusuke miyao crest , jst ( japan science and technology corporation )
this paper defines a general form for classbased probabilistic language models and proposes an efficient algorithm for clustering based on this . our evaluation experiments revealed that our method decreased computation time drastically , while retaining accuracy .

a unified morpho-syntactic scheme of stanford dependencies
stanford dependencies ( sd ) provide a functional characterization of the grammatical relations in syntactic parse-trees . the sd representation is useful for parser evaluation , for downstream applications , and , ultimately , for natural language understanding , however , the design of sd focuses on structurally-marked relations and under-represents morphosyntactic realization patterns observed in morphologically rich languages ( mrls ) . we present a novel extension of sd , called unified-sd ( u-sd ) , which unifies the annotation of structurally- and morphologically-marked relations via an inheritance hierarchy . we create a new resource composed of u-sdannotated constituency and dependency treebanks for the mrl modern hebrew , and present two systems that can automatically predict u-sd annotations , for gold segmented input as well as raw texts , with high baseline accuracy .

computing word-pair antonymy
knowing the degree of antonymy between words has widespread applications in natural language processing . manually-created lexicons have limited coverage and do not include most semantically contrasting word pairs . we present a new automatic and empirical measure of antonymy that combines corpus statistics with the structure of a published thesaurus . the approach is evaluated on a set of closest-opposite questions , obtaining a precision of over 80 % . along the way , we discuss what humans consider antonymous and how antonymy manifests itself in utterances .

features for dependency parsing
we propose the use of the word categories and embeddings induced from raw text as auxiliary features in dependency parsing . to induce word features , we make use of contextual , morphologic and orthographic properties of the words . to exploit the contextual information , we make use of substitute words , the most likely substitutes for target words , generated by using a statistical language model . we generate morphologic and orthographic properties of word types in an unsupervised manner . we use a co-occurrence model with these properties to embed words onto a 25dimensional unit sphere . the ai-ku system shows improvements for some of the languages it is trained on for the first shared task of statistical parsing of morphologically rich languages .

interpretation in a cognitive architecture
the work reported in this article presents a computational model of interpretation . the model proposes a cognitive architecture for intelligent agents to reason about competing analyses during interpretation and leverages the positive reinforcement principle .

part-of-speech tagging of northern sotho : disambiguating polysemous function words gertrud faa ulrich heid elsabe taljard danie prinsloo
a major obstacle to part-of-speech ( =pos ) tagging of northern sotho ( bantu , s 32 ) are ambiguous function words . many are highly polysemous and very frequent in texts , and their local context is not always distinctive . with certain taggers , this issue leads to comparatively poor results ( between 88 and 92 % accuracy ) , especially when sizeable tagsets ( over 100 tags ) are used . we use the rf-tagger ( schmid and laws , 2008 ) , which is particularly designed for the annotation of fine-grained tagsets ( e.g . including agreement information ) , and we restructure the 141 tags of the tagset proposed by taljard et al ( 2008 ) in a way to fit the rf tagger . this leads to over 94 % accuracy . error analysis in addition shows which types of phenomena cause trouble in the pos-tagging of northern sotho .

computationally rational saccadic control : an explanation of spillover effects based on sampling from noisy perception and memory
eye-movements in reading exhibit frequency spillover effects : fixation durations on a word are affected by the frequency of the previous word . we explore the idea that this effect may be an emergent property of a computationally rational eyemovement strategy that is navigating a tradeoff between processing immediate perceptual input , and continued processing of past input based on memory . we present an adaptive eye-movement control model with a minimal capacity for such processing , based on a composition of thresholded sequential samplers that integrate information from noisy perception and noisy memory . the model is applied to the list lexical decision task and shown to yield frequency spillovera robust property of human eye-movements in this task , even with parafoveal masking . we show that spillover in the model emerges in approximately optimal control policies that sometimes process memory rather than perception . we compare this model with one that is able to give priority to perception over memory , and show that the perception-priority policies in such a model do not perform as well in a range of plausible noise settings . we explain how the frequency spillover arises from a counter-intuitive but fundamental property of sequenced thresholded samplers .

optimizing to arbitrary nlp metrics using ensemble selection
while there have been many successful applications of machine learning methods to tasks in nlp , learning algorithms are not typically designed to optimize nlp performance metrics . this paper evaluates an ensemble selection framework designed to optimize arbitrary metrics and automate the process of algorithm selection and parameter tuning . we report the results of experiments that instantiate the framework for three nlp tasks , using six learning algorithms , a wide variety of parameterizations , and 15 performance metrics . based on our results , we make recommendations for subsequent machine-learning-based research for natural language learning .

exploring the sense distributions of homographs
this paper quantitatively investigates in how far local context is useful to disambiguate the senses of an ambiguous word . this is done by comparing the co-occurrence frequencies of particular context words . first , one context word representing a certain sense is chosen , and then the co-occurrence frequencies with two other context words , one of the same and one of another sense , are compared . as expected , it turns out that context words belonging to the same sense have considerably higher co-occurrence frequencies than words belonging to different senses . in our study , the sense inventory is taken from the university of south florida homograph norms , and the co-occurrence counts are based on the british national corpus .

seed selection for distantly supervised web-based relation extraction
in this paper we consider the problem of distant supervision to extract relations ( e.g . origin ( musical artist , location ) ) for entities ( e.g . the beatles ) of certain classes ( e.g . musical artist ) from web pages by using background information from the linking open data cloud to automatically label web documents which are then used as training data for relation classifiers . distant supervision approaches typically suffer from the problem of ambiguity when automatically labelling text , as well as the problem of incompleteness of background data to judge whether a mention is a true relation mention . this paper explores the hypothesis that simple statistical methods based on background data can help to filter unreliable training data and thus improve the precision of relation extractors . experiments on a web corpus show that an error reduction of 35 % can be achieved by strategically selecting seed data .

unsupervised learning summarization templates from concise summaries
we here present and compare two unsupervised approaches for inducing the main conceptual information in rather stereotypical summaries in two different languages . we evaluate the two approaches in two different information extraction settings : monolingual and cross-lingual information extraction . the extraction systems are trained on auto-annotated summaries ( containing the induced concepts ) and evaluated on humanannotated documents . extraction results are promising , being close in performance to those achieved when the system is trained on human-annotated summaries .

exploiting term importance categories and dependency relations for natural language search
in this paper , we propose a method that clearly separates terms ( words and dependency relations ) in a natural language query into important and other terms , and differently handles the terms according to their importance . the proposed method uses three types of term importance : necessary , optional , and unnecessary . the importance are detected using linguistic clues . we evaluated the proposed method using a test collection for japanese information retrieval . performance was resultantly improved by differently handling terms according to their importance .

dialogue systems that can handle face-to-face joint reference to actions in space
this talk introduces new research that works towards an overarching model of natural face-to-face conversation about spatially-located actions in the world , and then uses that model to implement a trustworthy embodied conversational agent to guide users ' ongoing , real-world activities away from the desktop . past research has demonstrated that the relationship between verbal and nonverbal behavior exists at the level of intonational phrases , conversational turns , discourse units , and the negotiation of reference to objects and actions , that mental representations of shared space are structured in such a way as to allow participants in a dialogue to draw on them , that dialogue systems must be based on models of coordination and collaboration , and that users are willing to engage in persistent , natural , trusting conversation with embodied conversational systems . in this talk , these diverse strands of research are brought together in the service of a single underlying modality-independent model of action and language , non-verbal behaviors and words , production and comprehension that can lead to a physically-located , spatially-aware , collaborative embodied conversational agent .

empirical lower bounds on translation unit error rate for the full class of inversion transduction grammars
empirical lower bounds studies in which the frequency of alignment configurations that can not be induced by a particular formalism is estimated , have been important for the development of syntax-based machine translation formalisms . the formalism that has received most attention has been inversion transduction grammars ( itgs ) ( wu , 1997 ) . all previous work on the coverage of itgs , however , concerns parse failure rates ( pfrs ) or sentence level coverage , which is not directly related to any of the evaluation measures used in machine translation . sgaard and kuhn ( 2009 ) induce lower bounds on translation unit error rates ( tuers ) for a number of formalisms , incl . normal form itgs , but not for the full class of itgs . many of the alignment configurations that can not be induced by normal form itgs can be induced by unrestricted itgs , however . this paper estimates the difference and shows that the average reduction in lower bounds on tuer is 2.48 in absolute difference ( 16.01 in average parse failure rate ) .

generating focused topic-specific sentiment lexicons
we present a method for automatically generating focused and accurate topicspecific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents . we motivate the need for such lexicons in the field of media analysis , describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon , and evaluate the quality of the generated lexicons both manually and using a trec blog track test set for opinionated blog post retrieval . although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon , they maintain , or even improve , the performance of an opinion retrieval system .

semi-supervised algorithm for human-computer dialogue mining
this paper describes the analysis of weak local coherence utterances during humancomputer conversation through the application of an emergent data mining technique , data crystallization . results reveal that by adding utterances with weak local relevance the performance of a baseline conversational partner , in terms of user satisfaction , showed betterment .

a flexible example annotation schema : translation corresponding
this paper presents work on the task of constructing an example base1 from a given bilingual corpus based on the annotation schema of translation corresponding tree ( tct ) . each tct describes a translation example ( a pair of bilingual sentences ) . it represents the syntactic structure of source language sentence , and more importantly is the facility to specify the correspondences between string ( both the source and target sentences ) and the representation tree . furthermore , syntax transformation clues are also encapsulated at each node in the tct representation to capture the differentiation of grammatical structure between the source and target languages . with this annotation schema , translation examples are effectively represented and organized in the bilingual knowledge database that we need for the portuguese to chinese machine translation system .

opensonar : user-driven development of the sonar corpus interfaces
opensonar is an online system that allows for analyzing and searching the large scale dutch reference corpus sonar . due to the size of the corpus , accessing the information contained in the dataset has proven to be difficult for less technically inclined researchers . the opensonar project aims to facilitate the use of the sonar corpus by providing a user-friendly online interface . to make sure that the resulting system is practically useful , several user groups have been identified , who drive the interface development process by providing practical use cases . the current system is already used in educational and research settings .

automatic detection of nonreferential it in spoken multi-party dialog
we present an implemented machine learning system for the automatic detection of nonreferential it in spoken dialog . the system builds on shallow features extracted from dialog transcripts . our experiments indicate a level of performance that makes the system usable as a preprocessing filter for a coreference resolution system . we also report results of an annotation study dealing with the classification of it by naive subjects .

efcient parsing of highly ambiguous context-free grammars with bit vectors
an efficient bit-vector-based cky-style parser for context-free parsing is presented . the parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences . the parser uses bit-vector operations to parallelise the basic parsing operations . the parser is particularly useful when all analyses are needed rather than just the most probable one .

ju-evora : a graph based cross-level semantic similarity analysis using discourse information
text analytics using semantic information is the latest trend of research due to its potential to represent better the texts content compared with the bag-of-words approaches . on the contrary , representation of semantics through graphs has several advantages over the traditional representation of feature vector . therefore , error tolerant graph matching techniques can be used for text comparison . nevertheless , not many methodologies exist in the literature which expresses semantic representations through graphs . the present system is designed to deal with cross level semantic similarity analysis as proposed in the semeval-2014 : semantic evaluation , international workshop on semantic evaluation , dublin , ireland .

semi-automatic construction of korean-chinese verb patterns based on translation equivalency
this paper addresses a new method of constructing korean-chinese verb patterns from existing patterns . a verb pattern is a subcategorization frame of a predicate extended by translation information . korean-chinese verb patterns are invaluable linguistic resources that only used for korean-chinese transfer but also for korean parsing . usually a verb pattern has been either hand-coded by expert lexicographers or extracted automatically from bilingual corpus . in the first case , the dependence on the linguistic intuition of lexicographers may lead to the incompleteness and the inconsistency of a dictionary . in the second case , extracted patterns can be domain-dependent . in this paper , we present a method to construct koreanchinese verb patterns semiautomatically from existing koreanchinese verb patterns that are manually written by lexicographers .

learning where to look : modeling eye movements in reading
we propose a novel machine learning task that consists in learning to predict which words in a text are fixated by a reader . in a first pilot experiment , we show that it is possible to outperform a majority baseline using a transitionbased model with a logistic regression classifier and a very limited set of features . we also show that the model is capable of capturing frequency effects on eye movements observed in human readers .

improving text classification by a sense spectrum approach to term and information science chew lim tan
experimenting with different mathematical objects for text representation is an important step of building text classification models . in order to be efficient , such objects of a formal model , like vectors , have to reasonably reproduce language-related phenomena such as word meaning inherent in index terms . we introduce an algorithm for sense-based semantic ordering of index terms which approximates cruses description of a sense spectrum . following semantic ordering , text classification by support vector machines can benefit from semantic smoothing kernels that regard semantic relations among index terms while computing document similarity . adding expansion terms to the vector representation can also improve effectiveness . this paper proposes a new kernel which discounts less important expansion terms based on lexical relatedness .

from graphs to events : a subgraph matching approach for information eextraction from biomedical text
we participated in the bionlp shared task 2011 , addressing the genia event extraction ( ge ) and the epigenetics and post-translational modifications ( epi ) tasks . a graph-based approach is employed to automatically learn rules for detecting biological events in the life-science literature . the event rules are learned by identifying the key contextual dependencies from full syntactic parsing of annotated text . event recognition is performed by searching for an isomorphism between event rules and the dependency graphs of sentences in the input texts . while we explored methods such as performance-based rule ranking to improve precision , we merged rules across multiple event types in order to increase recall . we achieved a 41.13 % f-score in detecting events of nine types in the task 1 of the ge task , and a 52.67 % f-score in identifying events across fifteen types in the core task of the epi task . our performance on both tasks is comparable to the state-of-the-art systems . our approach does not require any external domain-specific resources . the consistent performance on the two tasks supports the claim that the method generalizes well to extract events from different domains where training data is available .

converting text into agent animations : assigning gestures to text
this paper proposes a method for assigning gestures to text based on lexical and syntactic information . first , our empirical study identified lexical and syntactic information strongly correlated with gesture occurrence and suggested that syntactic structure is more useful for judging gesture occurrence than local syntactic cues . based on the empirical results , we have implemented a system that converts text into an animated agent that gestures and speaks synchronously .

in spoken dialogue systems
this paper deals with user corrections and aware sites of system errors in the toot spoken dialogue system . we rst describe our corpus , and give details on our procedure to label corrections and aware sites . then , we show that corrections and aware sites exhibit some prosodic and other properties which set them apart from `normal ' utterances . it appears that some correction types , such as simple repeats , are more likely to be correctly recognized than other types , such as paraphrases . we also present evidence that system dialogue strategy a ects users ' choice of correction type , suggesting that strategy-speci c methods of detecting or coaching users on corrections may be useful . aware sites tend to be shorter than other utterances , and are also more dif cult to recognize correctly for the asr system .

automatic discovery of feature sets for dependency parsing peter nilsson pierre nugues
this paper describes a search procedure to discover optimal feature sets for dependency parsers . the search applies to the shiftreduce algorithm and the feature sets are extracted from the parser configuration . the initial feature is limited to the first word in the input queue . then , the procedure uses a set of rules founded on the assumption that topological neighbors of significant features in the dependency graph may also have a significant contribution . the search can be fully automated and the level of greediness adjusted with the number of features examined at each iteration of the discovery procedure . using our automated feature discovery on two corpora , the swedish corpus in conll-x and the english corpus in conll 2008 , and a single parser system , we could reach results comparable or better than the best scores reported in these evaluations . the conll 2008 test set contains , in addition to a wall street journal ( wsj ) section , an out-of-domain sample from the brown corpus . with sets of 15 features , we obtained a labeled attachment score of 84.21 for swedish , 88.11 on the wsj test set , and 81.33 on the brown test set .

cipher type detection human language technology
manual analysis and decryption of enciphered documents is a tedious and error prone work . ofteneven after spending large amounts of time on a particular cipherno decipherment can be found . automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages found in libraries and archives , and to focus human effort only on a small but potentially interesting subset of them . in this work , we train a classifier that is able to predict which encipherment method has been used to generate a given ciphertext . we are able to distinguish 50 different cipher types ( specified by the american cryptogram association ) with an accuracy of 58.5 % . this is a 11.2 % absolute improvement over the best previously published classifier .

using morphosemantic information in construction of a pilot lexical semantic resource for turkish
morphological units carry vast amount of semantic information for languages with rich inflectional and derivational morphology . in this paper we show how morphosemantic information available for morphologically rich languages can be used to reduce manual effort in creating semantic resources like propbank and verbnet ; to increase performance of word sense disambiguation , semantic role labeling and related tasks . we test the consistency of these features in a pilot study for turkish and show that ; 1 ) case markers are related with semantic roles and 2 ) morphemes that change the valency of the verb follow a predictable pattern .

flsa : extending latent semantic analysis with features for dialogue act classification barbara di eugenio
we discuss feature latent semantic analysis ( flsa ) , an extension to latent semantic analysis ( lsa ) . lsa is a statistical method that is ordinarily trained on words only ; flsa adds to lsa the richness of the many other linguistic features that a corpus may be labeled with . we applied flsa to dialogue act classification with excellent results . we report results on three corpora : callhome spanish , maptask , and our own corpus of tutoring dialogues .

generating image descriptions using dependency relational patterns
this paper presents a novel approach to automatic captioning of geo-tagged images by summarizing multiple webdocuments that contain information related to an images location . the summarizer is biased by dependency pattern models towards sentences which contain features typically provided for different scene types such as those of churches , bridges , etc . our results show that summaries biased by dependency pattern models lead to significantly higher rouge scores than both n-gram language models reported in previous work and also wikipedia baseline summaries . summaries generated using dependency patterns also lead to more readable summaries than those generated without dependency patterns .

arabic native language identification
in this paper we present the first application of native language identification ( nli ) to arabic learner data . nli , the task of predicting a writers first language from their writing in other languages has been mostly investigated with english data , but is now expanding to other languages . we use l2 texts from the newly released arabic learner corpus and with a combination of three syntactic features ( cfg production rules , arabic function words and part-of-speech n-grams ) , we demonstrate that they are useful for this task . our system achieves an accuracy of 41 % against a baseline of 23 % , providing the first evidence for classifier-based detection of language transfer effects in l2 arabic . such methods can be useful for studying language transfer , developing teaching materials tailored to students native language and forensic linguistics . future directions are discussed .

exploration of term dependence in sentence retrieval
this paper focuses on the exploration of term dependence in the application of sentence retrieval . the adjacent terms appearing in query are assumed to be related with each other . these assumed dependences among query terms will be further validated for each sentence and sentences , which present strong syntactic relationship among query terms , are considered more relevant . experimental results have fully demonstrated the promising of the proposed models in improving sentence retrieval effectiveness .

asian wordet ( aw ) agency for the assessment and application of agency for the assessment and application of agency for the assessment and application of
this paper describes collaborative work on developing indonesian wordnet in the asianwordnet ( awn ) . we will describe the method to develop for collaborative editing to review and complete the translation of synset . this paper aims to create linkage among asian languages by adopting the concept of semantic relations and synset expressed in wordnet .

chinese syntactic parsing based on extended glr parsing
this paper presents an extended glr parsing algorithm with grammar pcfg* that is based on tomitas glr parsing algorithm and extends it further . we also define a new grammarpcfg* that is based on pcfg and assigns not only probability but also frequency associated with each rule . so our syntactic parsing system is implemented based on rule-based approach and statistics approach . furthermore our experiments are executed in two fields : chinese base noun phrase identification and full syntactic parsing . and the results of these two fields are compared from three ways . the experiments prove that the extended glr parsing algorithm with pcfg* is an efficient parsing method and a straightforward way to combine statistical property with rules . the experiment results of these two fields are presented in this paper .

in nlg and natural language dialog systems
reversibility is a key to efficient and maintainable nlg systems . in this paper , we present a formal definition of reversible nlg systems and develop a classification of existing natural language dialog systems in this framework .

splitting input sentence for machine translation using language model with sentence similarity
in order to boost the translation quality of corpus-based mt systems for speech translation , the technique of splitting an input sentence appears promising . in previous research , many methods used n-gram clues to split sentences . in this paper , to supplement n-gram based splitting methods , we introduce another clue using sentence similarity based on edit-distance . in our splitting method , we generate candidates for sentence splitting based on n-grams , and select the best one by measuring sentence similarity . we conducted experiments using two ebmt systems , one of which uses a phrase and the other of which uses a sentence as a translation unit . the translation results on various conditions were evaluated by objective measures and a subjective measure . the experimental results show that the proposed method is valuable for both systems .

a joint statistical model for simultaneous word spacing and spelling error correction for korean
this paper presents noisy-channel based korean preprocessor system , which corrects word spacing and typographical errors . the proposed algorithm corrects both errors simultaneously . using eojeol transition pattern dictionary and statistical data such as eumjeol n-gram and jaso transition probabilities , the algorithm minimizes the usage of huge word dictionaries .

detecting multiple facets of an event using graph-based unsupervised
we propose a new unsupervised method for topic detection that automatically identifies the different facets of an event . we use pointwise kullback-leibler divergence along with the jaccard coefficient to build a topic graph which represents the community structure of the different facets . the problem is formulated as a weighted set cover problem with dynamically varying weights . the algorithm is domainindependent and generates a representative set of informative and discriminative phrases that cover the entire event . we evaluate this algorithm on a large collection of blog postings about different news events and report promising results .

using domain similarity for performance estimation vincent van asch
many natural language processing ( nlp ) tools exhibit a decrease in performance when they are applied to data that is linguistically different from the corpus used during development . this makes it hard to develop nlp tools for domains for which annotated corpora are not available . this paper explores a number of metrics that attempt to predict the cross-domain performance of an nlp tool through statistical inference . we apply different similarity metrics to compare different domains and investigate the correlation between similarity and accuracy loss of nlp tool . we find that the correlation between the performance of the tool and the similarity metric is linear and that the latter can therefore be used to predict the performance of an nlp tool on out-of-domain data . the approach also provides a way to quantify the difference between domains .

accuracy-based scoring for dot : towards direct error minimization for dot : towards direct error minimization for data - oriented translation
in this work we present a novel technique to rescore fragments in the data-oriented translation model based on their contribution to translation accuracy . we describe three new rescoring methods , and present the initial results of a pilot experiment on a small subset of the europarl corpus . this work is a proof-of-concept , and is the first step in directly optimizing translation decisions solely on the hypothesized accuracy of potential translations resulting from those decisions .

fsa : an efficient and flexible c++ toolkit for finite state automata using on-demand computation
in this paper we present the rwth fsa toolkit an efficient implementation of algorithms for creating and manipulating weighted finite-state automata . the toolkit has been designed using the principle of on-demand computation and offers a large range of widely used algorithms . to prove the superior efficiency of the toolkit , we compare the implementation to that of other publically available toolkits . we also show that on-demand computations help to reduce memory requirements significantly without any loss in speed . to increase its flexibility , the rwth fsa toolkit supports high-level interfaces to the programming language python as well as a command-line tool for interactive manipulation of fsas . furthermore , we show how to utilize the toolkit to rapidly build a fast and accurate statistical machine translation system . future extensibility of the toolkit is ensured as it will be publically available as open source software .

geneval : a proposal for shared-task evaluation in nlg
we propose to organise a series of sharedtask nlg events , where participants are asked to build systems with similar input/output functionalities , and these systems are evaluated with a range of different evaluation techniques . the main purpose of these events is to allow us to compare different evaluation techniques , by correlating the results of different evaluations on the systems entered in the events .

factored markov translation with robust modeling information sciences institue
phrase-based translation models usually memorize local translation literally and make independent assumption between phrases which makes it neither generalize well on unseen data nor model sentencelevel effects between phrases . in this paper we present a new method to model correlations between phrases as a markov model and meanwhile employ a robust smoothing strategy to provide better generalization . this method defines a recursive estimation process and backs off in parallel paths to infer richer structures . our evaluation shows an 1.13.2 % bleu improvement over competitive baselines for chinese-english and arabic-english translation .

towards free-text semantic parsing : a unified framework based on
this article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources : framenet , verbnet and propbank . the framenet corpus contains the examples annotated with semantic roles whereas the verbnet lexicon provides the knowledge about the syntactic behavior of the verbs . we connect verbnet and framenet by mapping the framenet frames to the verbnet intersective levin classes . the propbank corpus , which is tightly connected to the verbnet lexicon , is used to increase the verb coverage and also to test the effectiveness of our approach . the results indicate that our model is an interesting step towards the design of free-text semantic parsers .

simplifica : a tool for authoring simplified texts in brazilian portuguese guided by readability assessments caroline gasperin and sandra maria alusio
simplifica is an authoring tool for producing simplified texts in portuguese . it provides functionalities for lexical and syntactic simplification and for readability assessment . this tool is the first of its kind for portuguese ; it brings innovative aspects for simplification tools in general , since the authoring process is guided by readability assessment based on the levels of literacy of the brazilian population .

language variety identification in spanish tweets
we study the problem of language variant identification , approximated by the problem of labeling tweets from spanish speaking countries by the country from which they were posted . while this task is closely related to pure language identification , it comes with additional complications . we build a balanced collection of tweets and apply techniques from language modeling . a simplified version of the task is also solved by human test subjects , who are outperformed by the automatic classification . our best automatic system achieves an overall f-score of 67.7 % on 5-class classification .

how is meaning grounded in dictionary definitions
meaning can not be based on dictionary definitions all the way down : at some point the circularity of definitions must be broken in some way , by grounding the meanings of certain words in sensorimotor categories learned from experience or shaped by evolution . this is the symbol grounding problem . we introduce the concept of a reachable set a larger vocabulary whose meanings can be learned from a smaller vocabulary through definition alone , as long as the meanings of the smaller vocabulary are themselves already grounded . we provide simple algorithms to compute reachable sets for any given dictionary .

rule filtering by pattern for efficient hierarchical translation
we describe refinements to hierarchical translation search procedures intended to reduce both search errors and memory usage through modifications to hypothesis expansion in cube pruning and reductions in the size of the rule sets used in translation . rules are put into syntactic classes based on the number of non-terminals and the pattern , and various filtering strategies are then applied to assess the impact on translation speed and quality . results are reported on the 2008 nist arabic-toenglish evaluation task .

annotating discourse connectives in the chinese treebank
in this paper we examine the issues that arise from the annotation of the discourse connectives for the chinese discourse treebank project . this project is based on the same principles as the pdtb , a project that annotates the english discourse connectives in the penn treebank . the paper begins by outlining range of discourse connectives under consideration in this project and examines the distribution of the explicit discourse connectives . we then examine the types of syntactic units that can be arguments to the discourse connectives . we show that one of the most challenging issues in this type of discourse annotation is determining the textual spans of the arguments and this is partly due to the hierarchical nature of discourse relations . finally , we discuss sense discrimination of the discourse connectives , which involves separating discourse connective from non-discourse connective senses and teasing apart the different discourse connective senses , and discourse connective variation , the use of different connectives to represent the same discourse relation . i thank aravind johi and martha palmer for their comments . all errors are my own , of course .

survey in sentiment , polarity and function analysis of citation
in this paper we proposed a survey in sentiment , polarity and function analysis of citations . this is an interesting area that has had an increased development in recent years but still has plenty of room for growth and further research . the amount of scientific information in the web makes it necessary innovate the analysis of the influence of the work of peers and leaders in the scientific community . we present an overview of general concepts , review contributions to the solution of related problems such as context identification , function and polarity classification , identify some trends and suggest possible future research directions .

construction of a german hpsg grammar from a detailed treebank
grammar extraction in deep formalisms has received remarkable attention in recent years . we recognise its value , but try to create a more precision-oriented grammar , by hand-crafting a core grammar , and learning lexical types and lexical items from a treebank . the study we performed focused on german , and we used the tiger treebank as our resource . a completely hand-written grammar in the framework of hpsg forms the inspiration for our core grammar , and is also our frame of reference for evaluation .

a semantic approach to recognizing textual entailment language computer corporation
exhaustive extraction of semantic information from text is one of the formidable goals of state-of-the-art nlp systems . in this paper , we take a step closer to this objective . we combine the semantic information provided by different resources and extract new semantic knowledge to improve the performance of a recognizing textual entailment system .

the effects of language relatedness on multilingual information retrieval : a case study with indo-european and semitic languages
we explore the effects of language relatedness within a multilingual information retrieval ( ir ) framework which can be deployed to virtually any language , focusing specifically on indo-european versus semitic languages . the semitic languages present unique challenges to ir for a number of reasons , so we set out to answer the question of whether cross-language ir for semitic languages can be boosted by manipulation of the training data ( which , in our framework , includes multilingual parallel text , some of which is morphologically analyzed ) . we attempted three measures to achieve this : first , the inclusion of genetically related ( i.e. , other semitic ) languages in the training data ; second , the inclusion of non-related languages sharing the same script , and third , the inclusion of morphological analysis for semitic languages . we find that language relatedness is a definite factor in boosting ir precision ; script similarity can probably be ruled out as a factor ; and morphological analysis can be helpful , but perhaps paradoxically not necessarily to the languages which are subjected to morphological analysis .

shallow semantic parsing of chinese
in this paper we address the question of assigning semantic roles to sentences in chinese . we show that good semantic parsing results for chinese can be achieved with a small 1100-sentence training set . in order to extract features from chinese , we describe porting the collins parser to chinese , resulting in the best performance currently reported on chinese syntactic parsing ; we include our headrules in the appendix . finally , we compare english and chinese semantic-parsing performance . while slight differences in argument labeling make a perfect comparison impossible , our results nonetheless suggest significantly better performance for chinese  work of watanabe et al. , we explore the use of the mira algorithm of crammer et al as an alternative to mert . we first show that by parallel processing and exploiting more of the parse forest , we can obtain results using mira that match or surpass mert in terms of both translation quality and computational cost . we then test the method on two classes of features that address deficiencies in the hiero hierarchical phrasebased model : first , we simultaneously train a large number of marton and resniks soft syntactic constraints , and , second , we introduce a novel structural distortion model . in both cases we obtain significant improvements in translation performance . optimizing them in combination , for a total of 56 feature weights , we improve performance by 2.6 b on a subset of the nist 2006 arabic-english evaluation data .

recognizing nested named entities in genia corpus
nested named entities ( nested nes ) , one containing another , are commonly seen in biomedical text , e.g. , accounting for 16.7 % of all named entities in genia corpus . while many works have been done in recognizing non-nested nes , nested nes have been largely neglected . in this work , we treat the task as a binary classification problem and solve it using support vector machines . for each token in nested nes , we use two schemes to set its class label : labeling as the outmost entity or the inner entity . our preliminary results show that while the outmost labeling tends to work better in recognizing the outmost entities , the inner labeling recognizes the inner nes better . this result should be useful for recognition of nested nes .

exploiting paraphrases and deferred sense commitment to interpret questions more reliably
creating correct , semantic representations of questions is essential for applications that can use formal reasoning to answer them . however , even within a restricted domain , it is hard to anticipate all the possible ways that a question might be phrased , and engineer reliable processing modules to produce a correct semantic interpretation for the reasoner . in our work on posing questions to a biology knowledge base , we address this brittleness in two ways : first , we exploit the dirt paraphrase database to introduce alternative phrasings of a question ; second , we defer word sense and semantic role commitment until question answering . resulting ambiguities are then resolved by interleaving additional interpretation with question-answering , allowing the combinatorics of alternatives to be controlled and domain knowledge to guide paraphrase and sense selection . our evaluation suggests that the resulting system is able to understand exam-style questions more reliably .

the edinburgh twitter corpus
we describe the first release of our corpus of 97 million twitter posts . we believe that this data will prove valuable to researchers working in social media , natural language processing , large-scale data processing , and similar areas .

towards automatic building of document keywords
document keywords are associated to documents as summarized versions of the documents content . considering that the number of documents is quickly growing every day , the availability of these keywords is very important . although , usually keywords are manually written . this motivated us to work on an approach to change this manual procedure for an automatic one . this paper presents a language independent approach that extracts the most relevant multiword expressions and single words from documents and propose them to describe the core content of each document .

marking time in developmental biology : annotating developmental events and their links with molecular events
current research in developmental biology aims to link developmental genetic pathways with the processes going on at cellular and tissue level . normal processes will only take place under specific sequential conditions at the level of the pathways . disrupting or altering pathways may mean disrupted or altered development . this paper is part of a larger work exploring methods of detecting and extracting information on developmental events from free text and on their relations in space and time .

unsupervised word alignment with arbitrary features
we introduce a discriminatively trained , globally normalized , log-linear variant of the lexical translation models proposed by brown et al ( 1993 ) . in our model , arbitrary , nonindependent features may be freely incorporated , thereby overcoming the inherent limitation of generative models , which require that features be sensitive to the conditional independencies of the generative process . however , unlike previous work on discriminative modeling of word alignment ( which also permits the use of arbitrary features ) , the parameters in our models are learned from unannotated parallel sentences , rather than from supervised word alignments . using a variety of intrinsic and extrinsic measures , including translation performance , we show our model yields better alignments than generative baselines in a number of language pairs .

a development environment for large-scale multi-lingual parsing systems
we describe the development environment available to linguistic developers in our lab in writing large-scale grammars for multiple languages . the environment consists of the tools that assist writing linguistic rules and running regression testing against large corpora , both of which are indispensable for realistic development of large-scale parsing systems . we also emphasize the importance of parser efficiency as an integral part of efficient parser development . the tools and methods described in this paper are actively used in the daily development of broad-coverage natural language understanding systems in seven languages ( chinese , english , french , german , japanese , korean and spanish ) .

as long as you name my name right : social circles and social sentiment in the hollywood hearings
the hollywood blacklist was based on a series of interviews conducted by the house committee on un-american activities ( huac ) , trying to identify members of the communist party . we use various nlp algorithms in order to automatically analyze a large corpus of interview transcripts and construct a network of the industry members and their naming relations . we further use algorithms for sentiment analysis in order to add a psychological dimension to the edges in the network . in particular , we test how different types of connections are manifested by different sentiment types and attitude of the interviewees . analysis of the language used in the hearings can shed new light on the motivation and role of network members .

the exploitation of spatial information in narrative discourse
we present the results of several machine learning tasks that exploit explicit spatial language to classify rhetorical relations and the spatial information of narrative events . three corpora are annotated with figure and ground ( granularity ) relationships , mereotopologically classified verbs and prepositions , and frames of reference . for rhetorical relations , nave bayesian models achieve 84.90 % and 57.87 % accuracy in classifying narration and background / elaboration relations respectively ( 16 % and 23 % above baseline ) . for the spatial information of narrative events , k* models achieve 55.68 % average accuracy ( 12 % above baseline ) for all spatial information types . this result is boosted to 71.85 % ( 28 % above baseline ) when inertial spatial reference and text sequence information are considered . overall , spatial information is shown to be central to narrative discourse structure and prediction tasks .

a hybrid approach for biomedical event extraction
in this paper we propose a system which uses hybrid methods that combine both rule-based and machine learning ( ml ) -based approaches to solve genia event extraction of bionlp shared task 2013. we apply uima1 framework to support coding . there are three main stages in model : pre-processing , trigger detection and biomedical event detection . we use dictionary and support vector machine classifier to detect event triggers . event detection is applied on syntactic patterns which are combined with features extracted for classification .

large-scale semantic parsing via schema matching and lexicon
supervised training procedures for semantic parsers produce high-quality semantic parsers , but they have difficulty scaling to large databases because of the sheer number of logical constants for which they must see labeled training data . we present a technique for developing semantic parsers for large databases based on a reduction to standard supervised training algorithms , schema matching , and pattern learning . leveraging techniques from each of these areas , we develop a semantic parser for freebase that is capable of parsing questions with an f1 that improves by 0.42 over a purely-supervised learning algorithm .

enhanced answer type inference from questions using sequential models
question classification is an important step in factual question answering ( qa ) and other dialog systems . several attempts have been made to apply statistical machine learning approaches , including support vector machines ( svms ) with sophisticated features and kernels . curiously , the payoff beyond a simple bag-ofwords representation has been small . we show that most questions reveal their class through a short contiguous token subsequence , which we call its informer span . perfect knowledge of informer spans can enhance accuracy from 79.4 % to 88 % using linear svms on standard benchmarks . in contrast , standard heuristics based on shallow pattern-matching give only a 3 % improvement , showing that the notion of an informer is non-trivial . using a novel multi-resolution encoding of the questions parse tree , we induce a conditional random field ( crf ) to identify informer spans with about 85 % accuracy . then we build a meta-classifier using a linear svm on the crf output , enhancing accuracy to 86.2 % , which is better than all published numbers .

generating expository dialogue from monologue :
generating expository dialogue from monologue is a task that poses an interesting and rewarding challenge for natural language processing . this short paper has three aims : firstly , to motivate the importance of this task , both in terms of the benefits of expository dialogue as a way to present information and in terms of potential applications ; secondly , to introduce a parallel corpus of monologues and dialogues which enables a data-driven approach to this challenge ; and , finally , to describe work-in-progress on semi-automatic construction of monologueto-dialogue ( m2d ) generation rules .

experiment with n-gram posteriors anil kumar singh
this paper describes the machine learning algorithm and the features used by limsi for the quality estimation shared task . our submission mainly aims at evaluating the usefulness for quality estimation of ngram posterior probabilities that quantify the probability for a given n-gram to be part of the system output .

automatic recognition of logical relations for english , chinese and japanese in the glarf framework
we present glarf , a framework for representing three linguistic levels and systems for generating this representation . we focus on a logical level , like lfgs f-structure , but compatible with penn treebanks . while less finegrained than typical semantic role labeling approaches , our logical structure has several advantages : ( 1 ) it includes all words in all sentences , regardless of part of speech or semantic domain ; and ( 2 ) it is easier to produce accurately . our systems achieve 90 % for english/japanese news and 74.5 % for chinese news these f-scores are nearly the same as those achieved for treebank-based parsing .

an active learning approach to finding related terms
we present a novel system that helps nonexperts find sets of similar words . the user begins by specifying one or more seed words . the system then iteratively suggests a series of candidate words , which the user can either accept or reject . current techniques for this task typically bootstrap a classifier based on a fixed seed set . in contrast , our system involves the user throughout the labeling process , using active learning to intelligently explore the space of similar words . in particular , our system can take advantage of negative examples provided by the user . our system combines multiple preexisting sources of similarity data ( a standard thesaurus , wordnet , contextual similarity ) , enabling it to capture many types of similarity groups ( synonyms of crash , types of car , etc . ) . we evaluate on a hand-labeled evaluation set ; our system improves over a strong baseline by 36 % .

stream-based translation models for statistical machine translation
typical statistical machine translation systems are trained with static parallel corpora . here we account for scenarios with a continuous incoming stream of parallel training data . such scenarios include daily governmental proceedings , sustained output from translation agencies , or crowd-sourced translations . we show incorporating recent sentence pairs from the stream improves performance compared with a static baseline . since frequent batch retraining is computationally demanding we introduce a fast incremental alternative using an online version of the em algorithm . to bound our memory requirements we use a novel data-structure and associated training regime . when compared to frequent batch retraining , our online time and space-bounded model achieves the same performance with significantly less computational overhead .

wsd as a distributed constraint optimization problem
this work models word sense disambiguation ( wsd ) problem as a distributed constraint optimization problem ( dcop ) . to model wsd as a dcop , we view information from various knowledge sources as constraints . dcop algorithms have the remarkable property to jointly maximize over a wide range of utility functions associated with these constraints . we show how utility functions can be designed for various knowledge sources . for the purpose of evaluation , we modelled all words wsd as a simple dcop problem . the results are competitive with state-of-art knowledge based systems .

better statistical machine translation through linguistic treatment of
this article describes a linguistically informed method for integrating phrasal verbs into statistical machine translation ( smt ) systems . in a case study involving english to bulgarian smt , we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task . we attribute this to the fact that , in contrast to previous work on the subject , we employ detailed linguistic information . we found out that features which describe phrasal verbs as idiomatic or compositional contribute most to the better translation quality achieved by our method .

open-domain commonsense reasoning using discourse relations from a corpus of weblog stories
we present a method of extracting opendomain commonsense knowledge by applying discourse parsing to a large corpus of personal stories written by internet authors . we demonstrate the use of a linear-time , joint syntax/discourse dependency parser for this purpose , and we show how the extracted discourse relations can be used to generate opendomain textual inferences . our evaluations of the discourse parser and inference models show some success , but also identify a number of interesting directions for future work .

hybrid stemmer for gujarati pratikkumar patel kashyap popat
in this paper we present a lightweight stemmer for gujarati using a hybrid approach . instead of using a completely unsupervised approach , we have harnessed linguistic knowledge in the form of a hand-crafted gujarati suffix list in order to improve the quality of the stems and suffixes learnt during the training phase . we used the emille corpus for training and evaluating the stemmers performance . the use of hand-crafted suffixes boosted the accuracy of our stemmer by about 17 % and helped us achieve an accuracy of 67.86 % .

supporting annotation layers for natural language processing
we demonstrate a system for flexible querying against text that has been annotated with the results of nlp processing . the system supports self-overlapping and parallel layers , integration of syntactic and ontological hierarchies , flexibility in the format of returned results , and tight integration with sql . we present a query language and its use on examples taken from the nlp literature .

umcc dlsi : sentiment analysis in twitter using polirity lexicons
this paper describes a system submitted to semeval-2014 task 4b : sentiment analysis in twitter , by the team umcc dlsi sem integrated by researchers of the university of matanzas , cuba and the university of alicante , spain . the system adopts a cascade classification process that uses two classifiers , k-nn using the lexical levenshtein metric and a dagging model trained over attributes extracted from annotated corpora and sentiment lexicons . phrases that fit the distance thresholds were automatically classified by the knn model , the others , were evaluated with the dagging model . this system achieved over 52.4 % of correctly classified instances in the twitter message-level subtask .

practical linguistic steganography using contextual synonym substitution and vertex colour coding
linguistic steganography is concerned with hiding information in natural language text . one of the major transformations used in linguistic steganography is synonym substitution . however , few existing studies have studied the practical application of this approach . in this paper we propose two improvements to the use of synonym substitution for encoding hidden bits of information . first , we use the web 1t google n-gram corpus for checking the applicability of a synonym in context , and we evaluate this method using data from the semeval lexical substitution task . second , we address the problem that arises from words with more than one sense , which creates a potential ambiguity in terms of which bits are encoded by a particular word . we develop a novel method in which words are the vertices in a graph , synonyms are linked by edges , and the bits assigned to a word are determined by a vertex colouring algorithm . this method ensures that each word encodes a unique sequence of bits , without cutting out large number of synonyms , and thus maintaining a reasonable embedding capacity .

crowdsourcing the evaluation of a domain-adapted named entity
named entity recognition systems sometimes have difficulty when applied to data from domains that do not closely match the training data . we first use a simple rule-based technique for domain adaptation . data for robust validation of the technique is then generated , and we use crowdsourcing techniques to show that this strategy produces reliable results even on data not seen by the rule designers . we show that it is possible to extract large improvements on the target data rapidly at low cost using these techniques .

assessing the costs of sampling methods in active learning for annotation
traditional active learning ( al ) techniques assume that the annotation of each datum costs the same . this is not the case when annotating sequences ; some sequences will take longer than others . we show that the al technique which performs best depends on how cost is measured . applying an hourly cost model based on the results of an annotation user study , we approximate the amount of time necessary to annotate a given sentence . this model allows us to evaluate the effectiveness of al sampling methods in terms of time spent in annotation . we acheive a 77 % reduction in hours from a random baseline to achieve 96.5 % tag accuracy on the penn treebank . more significantly , we make the case for measuring cost in assessing al methods .

qcs : a tool for querying , clustering , and summarizing documents
the qcs information retrieval ( ir ) system is presented as a tool for querying , clustering , and summarizing document sets . qcs has been developed as a modular development framework , and thus facilitates the inclusion of new technologies targeting these three ir tasks . details of the system architecture , the qcs interface , and preliminary results are presented .

subtree extractive summarization via submodular maximization
this study proposes a text summarization model that simultaneously performs sentence extraction and compression . we translate the text summarization task into a problem of extracting a set of dependency subtrees in the document cluster . we also encode obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary . in order to handle the subtree extraction problem , we investigate a new class of submodular maximization problem , and a new algorithm that has the approximation ratio 12 ( 1 e1 ) . ourexperiments with the ntcir aclia test collections show that our approach outperforms a state-of-the-art algorithm .

a fast and accurate dependency parser using neural networks
almost all current dependency parsers classify based on millions of sparse indicator features . not only do these features generalize poorly , but the cost of feature computation restricts parsing speed significantly . in this work , we propose a novel way of learning a neural network classifier for use in a greedy , transition-based dependency parser . because this classifier learns and uses just a small number of dense features , it can work very fast , while achieving an about 2 % improvement in unlabeled and labeled attachment scores on both english and chinese datasets . concretely , our parser is able to parse more than 1000 sentences per second at 92.2 % unlabeled attachment score on the english penn treebank .

learning to match names across languages the mitre corporation the mitre corporation the mitre corporation
we report on research on matching names in different scripts across languages . we explore two trainable approaches based on comparing pronunciations . the first , a cross-lingual approach , uses an automatic name-matching program that exploits rules based on phonological comparisons of the two languages carried out by humans . the second , monolingual approach , relies only on automatic comparison of the phonological representations of each pair . alignments produced by each approach are fed to a machine learning algorithm . results show that the monolingual approach results in machine-learning based comparison of person-names in english and chinese at an accuracy of over 97.0 f-measure .

a naive theory of affixation and an algorithm for extraction
we present a novel approach to the unsupervised detection of affixes , that is , to extract a set of salient prefixes and suffixes from an unlabeled corpus of a language . the underlying theory makes no assumptions on whether the language uses a lot of morphology or not , whether it is prefixing or suffixing , or whether affixes are long or short . it does however make the assumption that 1. salient affixes have to be frequent , i.e occur much more often that random segments of the same length , and that 2. words essentially are variable length sequences of random characters , e.g a character should not occur in far too many words than random without a reason , such as being part of a very frequent affix . the affix extraction algorithm uses only information from fluctation of frequencies , runs in linear time , and is free from thresholds and untransparent iterations . we demonstrate the usefulness of the approach with example case studies on typologically distant languages .

putop : turning predominant senses into a topic model for word sense
we extend on mccarthy et als predominant sense method to create an unsupervised method of word sense disambiguation that uses automatically derived topics using latent dirichlet alocation . using topicspecific synset similarity measures , we create predictions for each word in each document using only word frequency information . it is hoped that this procedure can improve upon the method for larger numbers of topics by providing more relevant training corpora for the individual topics . this method is evaluated on semeval-2007 task 1 and task 17 .

exploring content models for multi-document summarization
we present an exploration of generative probabilistic models for multi-document summarization . beginning with a simple word frequency based model ( nenkova and vanderwende , 2005 ) , we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting rouge gains along the way . our final model , hiersum , utilizes a hierarchical lda-style model ( blei et al , 2004 ) to represent content specificity as a hierarchy of topic vocabulary distributions . at the task of producing generic duc-style summaries , hiersum yields state-of-the-art rouge performance and in pairwise user evaluation strongly outperforms toutanova et al ( 2007 ) s state-of-the-art discriminative system . we also explore hiersums capacity to produce multiple topical summaries in order to facilitate content discovery and navigation .

deriving generalized knowledge from corpora using wordnet
benjamin van durme , phillip michalak and lenhart k. schubert department of computer science university of rochester rochester , ny 14627 , usa abstract existing work in the extraction of commonsense knowledge from text has been primarily restricted to factoids that serve as statements about what may possibly obtain in the world . we present an approach to deriving stronger , more general claims by abstracting over large sets of factoids . our goal is to coalesce the observed nominals for a given predicate argument into a few predominant types , obtained as wordnet synsets . the results can be construed as generically quantified sentences restricting the semantic type of an argument position of a predicate .

stacking for statistical machine translation
we propose the use of stacking , an ensemble learning technique , to the statistical machine translation ( smt ) models . a diverse ensemble of weak learners is created using the same smt engine ( a hierarchical phrase-based system ) by manipulating the training data and a strong model is created by combining the weak models on-the-fly . experimental results on two language pairs and three different sizes of training data show significant improvements of up to 4 bleu points over a conventionally trained smt model .

transductive learning for statistical machine translation
statistical machine translation systems are usually trained on large amounts of bilingual text and monolingual text in the target language . in this paper we explore the use of transductive semi-supervised methods for the effective use of monolingual data from the source language in order to improve translation quality . we propose several algorithms with this aim , and present the strengths and weaknesses of each one . we present detailed experimental evaluations on the frenchenglish europarl data set and on data from the nist chineseenglish largedata track . we show a significant improvement in translation quality on both tasks .

generalized interpolation in decision tree lm human language technology
in the face of sparsity , statistical models are often interpolated with lower order ( backoff ) models , particularly in language modeling . in this paper , we argue that there is a relation between the higher order and the backoff model that must be satisfied in order for the interpolation to be effective . we show that in n-gram models , the relation is trivially held , but in models that allow arbitrary clustering of context ( such as decision tree models ) , this relation is generally not satisfied . based on this insight , we also propose a generalization of linear interpolation which significantly improves the performance of a decision tree language model .

multilingual semantic role labelling with markov logic
this paper presents our system for the conll 2009 shared task on syntactic and semantic dependencies in multiple languages . in this work we focus only on the semantic role labelling ( srl ) task . we use markov logic to define a joint srl model and achieve the third best average performance in the closed track for srlonly systems and the sixth including for both srlonly and joint systems .

gappy phrasal alignment by agreement
we propose a principled and efficient phraseto-phrase alignment model , useful in machine translation as well as other related natural language processing problems . in a hidden semimarkov model , word-to-phrase and phraseto-word translations are modeled directly by the system . agreement between two directional models encourages the selection of parsimonious phrasal alignments , avoiding the overfitting commonly encountered in unsupervised training with multi-word units . expanding the state space to include gappy phrases ( such as french ne pas ) makes the alignment space more symmetric ; thus , it allows agreement between discontinuous alignments . the resulting system shows substantial improvements in both alignment quality and translation quality over word-based hidden markov models , while maintaining asymptotically equivalent runtime .

staying informed : supervised and semi-supervised multi-view topical analysis of ideological perspective
with the proliferation of user-generated articles over the web , it becomes imperative to develop automated methods that are aware of the ideological-bias implicit in a document collection . while there exist methods that can classify the ideological bias of a given document , little has been done toward understanding the nature of this bias on a topical-level . in this paper we address the problem of modeling ideological perspective on a topical level using a factored topic model . we develop efficient inference algorithms using collapsed gibbs sampling for posterior inference , and give various evaluations and illustrations of the utility of our model on various document collections with promising results . finally we give a metropolis-hasting inference algorithm for a semi-supervised extension with decent results .

estimating and exploiting the entropy of sense distributions
word sense distributions are usually skewed . predicting the extent of the skew can help a word sense disambiguation ( wsd ) system determine whether to consider evidence from the local context or apply the simple yet effective heuristic of using the first ( most frequent ) sense . in this paper , we propose a method to estimate the entropy of a sense distribution to boost the precision of a first sense heuristic by restricting its application to words with lower entropy . we show on two standard datasets that automatic prediction of entropy can increase the performance of an automatic first sense heuristic .

mime- nlg support for complex and unstable pre-hospital
we present the first prototype of a handover report generator developed for the mime ( managing information in medical emergencies ) project . nlg applications in the medical domain have been varied but most are deployed in clinical situations . we develop a mobile device for prehospital care which receives streamed sensor data and user input , and converts these into a handover report for paramedics .

adverseeffect relations extraction from massive clinical records
the rapid spread of electronic health records raised an interest to large-scale information extraction from clinical texts . considering such a background , we are developing a method that can extract adverse drug event and effect ( adverseeffect ) relations from massive clinical records . adverseeffect relations share some features with relations proposed in previous relation extraction studies , but they also have unique characteristics . adverseeffect relations are usually uncertain . not even medical experts can usually determine whether a symptom that arises after a medication represents an adverse effect relation or not . we propose a method to extract adverseeffect relations using a machine-learning technique with dependency features . we performed experiments to extract adverseeffect relations from 2,577 clinical texts , and obtained f1-score of 37.54 with an optimal parameters and f1-score of 34.90 with automatically tuned parameters . the results also show that dependency features increase the extraction f1-score by 3.59 .

exploiting wikipedia as external knowledge for named entity recognition
we explore the use of wikipedia as external knowledge to improve named entity recognition ( ner ) . our method retrieves the corresponding wikipedia entry for each candidate word sequence and extracts a category label from the first sentence of the entry , which can be thought of as a definition part . these category labels are used as features in a crf-based ne tagger . we demonstrate using the conll 2003 dataset that the wikipedia category labels extracted by such a simple method actually improve the accuracy of ner .

multilingual deterministic dependency parsing framework using modified finite newton method support vector machines
in this paper , we present a three-step multilingual dependency parser based on a deterministic shift-reduce parsing algorithm . different from last year , we separate the root-parsing strategy as sequential labeling task and try to link the neighbor word dependences via a near neighbor parsing . the outputs of the root and neighbor parsers were encoded as features for the shift-reduce parser . in addition , the learners we used for the two parsers and the shift-reduce parser are quite different ( conditional random fields and the modified finite-newton method support vector machines ) . we found that our method could benefit from the two-preprocessing stages . to speed up training , in this year , we employ the mfn-svm ( modified finite-newton method support vector machines ) which can be learned in linear time . the experimental results show that our method achieved the middle rank over the 23 teams . we expect that our method could be further improved via well-tuned parameter validations for different languages .

experiments with cst-based multidocument summarization
recently , with the huge amount of growing information in the web and the little available time to read and process all this information , automatic summaries have become very important resources . in this work , we evaluate deep content selection methods for multidocument summarization based on the cst model ( cross-document structure theory ) . our methods consider summarization preferences and focus on the overall main problems of multidocument treatment : redundancy , complementarity , and contradiction among different information sources . we also evaluate the impact of the cst model over superficial summarization systems . our results show that the use of cst model helps to improve informativeness and quality in automatic summaries .

stochastic contextual edit distance and probabilistic fsts
string similarity is most often measured by weighted or unweighted edit distance d ( x , y ) . ristad and yianilos ( 1998 ) defined stochastic edit distancea probability distribution p ( y | x ) whose parameters can be trained from data . we generalize this so that the probability of choosing each edit operation can depend on contextual features . we show how to construct and train a probabilistic finite-state transducer that computes our stochastic contextual edit distance . to illustrate the improvement from conditioning on context , we model typos found in social media text .

shallow discourse structure for action item detection
we investigated automatic action item detection from transcripts of multi-party meetings . unlike previous work ( gruenstein et al , 2005 ) , we use a new hierarchical annotation scheme based on the roles utterances play in the action item assignment process , and propose an approach to automatic detection that promises improved classification accuracy while enabling the extraction of useful information for summarization and reporting .

preliminary chinese term classification for ontology construction
an ontology can be seen as a representation of concepts in a specific domain . accordingly , ontology construction can be regarded as the process of organizing these concepts . if the terms which are used to label the concepts are classified before building an ontology , the work of ontology construction can proceed much more easily . part-of-speech ( pos ) tags usually carry some linguistic information of terms , so pos tagging can be seen as a kind of preliminary classification to help constructing concept nodes in ontology because features or attributes related to concepts of different pos types may be different . this paper presents a simple approach to tag domain terms for the convenience of ontology construction , referred to as term pos ( tpos ) tagging . the proposed approach makes use of segmentation and tagging results from a general pos tagging software to predict tags for extracted domain specific terms . this approach needs no training and no context information . the experimental results show that the proposed approach achieves a precision of 95.41 % for extracted terms and can be easily applied to different domains . comparing with some existing approaches , our approach shows that for some specific tasks , simple method can obtain very good performance and is thus a better choice .

deeper spoken language understanding for man-machine dialogue on broader application domains : a logical alternative to concept spotting
logus is a french-speaking spoken language understanding ( slu ) system which carries out a deeper analysis than those achieved by standard concept spotters . it is designed for multi-domain conversational systems or for systems that are working on complex application domains . based on a logical approach , the system adapts the ideas of incremental robust parsing to the issue of slu . the paper provides a detailed description of the system as well as results from two evaluation campaigns that concerned all of current french-speaking slu systems . the observed error rates suggest that our logical approach can stand comparison with concept spotters on restricted application domains , but also that its behaviour is promising for larger domains . the question of the generality of the approach is precisely addressed by our current investigations on a new task : slu for an emotional robot companion for young hospital patents .

extraction of tree adjoining grammars from a treebank for korean
we present the implementation of a system which extracts not only lexicalized grammars but also feature-based lexicalized grammars from korean sejong treebank . we report on some practical experiments where we extract tag grammars and tree schemata . above all , full-scale syntactic tags and well-formed morphological analysis in sejong treebank allow us to extract syntactic features . in addition , we modify treebank for extracting lexicalized grammars and convert lexicalized grammars into tree schemata to resolve limited lexical coverage problem of extracted lexicalized grammars .

an exponential translation model for target language morphology
this paper presents an exponential model for translation into highly inflected languages which can be scaled to very large datasets . as in other recent proposals , it predicts targetside phrases and can be conditioned on sourceside context . however , crucially for the task of modeling morphological generalizations , it estimates feature parameters from the entire training set rather than as a collection of separate classifiers . we apply it to english-czech translation , using a variety of features capturing potential predictors for case , number , and gender , and one of the largest publicly available parallel data sets . we also describe generation and modeling of inflected forms unobserved in training data and decoding procedures for a model with non-local target-side feature dependencies .

predictive text entry using syntax and semantics
most cellular telephones use numeric keypads , where texting is supported by dictionaries and frequency models . given a key sequence , the entry system recognizes the matching words and proposes a rankordered list of candidates . the ranking quality is instrumental to an effective entry . this paper describes a new method to enhance entry that combines syntax and language models . we first investigate components to improve the ranking step : language models and semantic relatedness . we then introduce a novel syntactic model to capture the word context , optimize ranking , and then reduce the number of keystrokes per character ( kspc ) needed to write a text . we finally combine this model with the other components and we discuss the results . we show that our syntax-based model reaches an error reduction in kspc of 12.4 % on a swedish corpus over a baseline using word frequencies . we also show that bigrams are superior to all the other models . however , bigrams have a memory footprint that is unfit for most devices .

a supervised learning approach towards profiling the preservation of authorial style in literary translations
recently there has been growing interest in the application of approaches from the text classification literature to fine-grained problems of textual stylometry . this paper seeks to answer a question which has concerned the translation studies community : how does a literary translators style vary across their translations of different authors this study focuses on the works of constance garnett , one of the most prolific english-language translators of russian literature , and uses supervised learning approaches to analyse her translations of three well-known russian authors , ivan turgenev , fyodor dosteyevsky and anton chekhov . this analysis seeks to identify common linguistic patterns which hold for all of the translations from the same author . based on the experimental results , it is ascertained that both document-level metrics and n-gram features prove useful for distinguishing between authorial contributions in our translation corpus and their individual efficacy increases further when these two feature types are combined , resulting in classification accuracy of greater than 90 % on the task of predicting the original author of a textual segment using a support vector machine classifier . the ratio of nouns and pronouns to total tokens are identified as distinguishing features in the document metrics space , along with occurrences of common adverbs and reporting verbs from the collection of n-gram features .

situational language training for hotel receptionists frdrique segond , thibault parmentier roberta stock , ran rosner mariola usteran muela
this paper presents the lessons learned in experimenting with thetis1 , an ec project focusing on the creation and localization of enhanced on-line pedagogical content for language learning in tourism industry . it is based on a general innovative approach to language learning that allows employees to acquire practical oral and written skills while navigating a relevant professional scenario . the approach is enabled by an underlying platform ( exills ) that integrates virtual reality with a set of linguistic , technologies to create a new form of dynamic , extensible , goal-directed e-content .

efficient , correct , unsupervised learning of context-sensitive languages
a central problem for nlp is grammar induction : the development of unsupervised learning algorithms for syntax . in this paper we present a lattice-theoretic representation for natural language syntax , called distributional lattice grammars . these representations are objective or empiricist , based on a generalisation of distributional learning , and are capable of representing all regular languages , some but not all context-free languages and some noncontext-free languages . we present a simple algorithm for learning these grammars together with a complete self-contained proof of the correctness and efficiency of the algorithm .

query-focused summaries or query-biased summaries
in the context of the document understanding conferences , the task of query-focused multi-document summarization is intended to improve agreement in content among humangenerated model summaries . query-focus also aids the automated summarizers in directing the summary at specific topics , which may result in better agreement with these model summaries . however , while query focus correlates with performance , we show that highperforming automatic systems produce summaries with disproportionally higher query term density than human summarizers do . experimental evidence suggests that automatic systems heavily rely on query term occurrence and repetition to achieve good performance .

mining a lexicon of technical terms and lay equivalents
we present a corpus-driven method for building a lexicon of semantically equivalent pairs of technical and lay medical terms . using a parallel corpus of abstracts of clinical studies and corresponding news stories written for a lay audience , we identify terms which are good semantic equivalents of technical terms for a lay audience . our method relies on measures of association . results show that , despite the small size of our corpus , a promising number of pairs are identified .

influence of parser choice on dependency-based mt
accuracy of dependency parsers is one of the key factors limiting the quality of dependencybased machine translation . this paper deals with the influence of various dependency parsing approaches ( and also different training data size ) on the overall performance of an english-to-czech dependency-based statistical translation system implemented in the treex framework . we also study the relationship between parsing accuracy in terms of unlabeled attachment score and machine translation quality in terms of bleu .

viterbi training improves unsupervised dependency parsing
we show that viterbi ( or hard ) em is well-suited to unsupervised grammar induction . it is more accurate than standard inside-outside re-estimation ( classic em ) , significantly faster , and simpler . our experiments with klein and mannings dependency model with valence ( dmv ) attain state-of-the-art performance 44.8 % accuracy on section 23 ( all sentences ) of the wall street journal corpus without clever initialization ; with a good initializer , viterbi training improves to 47.9 % . this generalizes to the brown corpus , our held-out set , where accuracy reaches 50.8 % a 7.5 % gain over previous best results . we find that classic em learns better from short sentences but can not cope with longer ones , where viterbi thrives . however , we explain that both algorithms optimize the wrong objectives and prove that there are fundamental disconnects between the likelihoods of sentences , best parses , and true parses , beyond the wellestablished discrepancies between likelihood , accuracy and extrinsic performance .

manawi : using multi-word expressions and named entities to improve
we describe the manawi 1 ( manev ) system submitted to the 2014 wmt translation shared task . we participated in the english-hindi ( en-hi ) and hindi-english ( hi-en ) language pair and achieved 0.792 for the translation error rate ( ter ) score 2 for en-hi , the lowest among the competing systems . our main innovations are ( i ) the usage of outputs from nlp tools , viz . billingual multi-word expression extractor and named-entity recognizer to improve smt quality and ( ii )

multilingual search for cultural heritage archives via combining multiple
the linguistic features of material in cultural heritage ( ch ) archives may be in various languages requiring a facility for effective multilingual search . the specialised language often associated with ch content introduces problems for automatic translation to support search applications . the multimatch project is focused on enabling users to interact with ch content across different media types and languages . we present results from a multimatch study exploring various translation techniques for the ch domain . our experiments examine translation techniques for the english language clef 2006 cross-language speech retrieval ( cl-sr ) task using spanish , french and german queries . results compare effectiveness of our query translation against a monolingual baseline and show improvement when combining a domain-specific translation lexicon with a standard machine translation system .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

adjective based inference
in this paper , we propose a fine grained classification of english adjectives geared at modeling the distinct inference patterns licensed by each adjective class . we show how it can be implemented in description logic and illustrate the predictions made by a series of examples . the proposal has been implemented using description logic as a semantic representation language and the prediction verified using the dl theorem prover racer . topics : textual entailment , adjectival semantics

using collocations for topic segmentation and link detection
we present in this paper a method for achieving in an integrated way two tasks of topic analysis : segmentation and link detection . this method combines word repetition and the lexical cohesion stated by a collocation network to compensate for the respective weaknesses of the two approaches . we report an evaluation of our method for segmentation on two corpora , one in french and one in english , and we propose an evaluation measure that specifically suits that kind of systems .

rali : smt shared task system description
thanks to the profusion of freely available tools , it recently became fairly easy to built a statistical machine translation ( smt ) engine given a bitext . the expectations we can have on the quality of such a system may however greatly vary from one pair of languages to another . we report on our experiments in building phrase-based translation engines for the four pairs of languages we had to consider for the smt sharedtask .

automatic prediction of parser accuracy
statistical parsers have become increasingly accurate , to the point where they are useful in many natural language applications . however , estimating parsing accuracy on a wide variety of domains and genres is still a challenge in the absence of gold-standard parse trees . in this paper , we propose a technique that automatically takes into account certain characteristics of the domains of interest , and accurately predicts parser performance on data from these new domains . as a result , we have a cheap ( no annotation involved ) and effective recipe for measuring the performance of a statistical parser on any given domain .

phrase chunking using entropy guided transformation learning ccero nogueira dos santos centro tecnologico do exercito
entropy guided transformation learning ( etl ) is a new machine learning strategy that combines the advantages of decision trees ( dt ) and transformation based learning ( tbl ) . in this work , we apply the etl framework to four phrase chunking tasks : portuguese noun phrase chunking , english base noun phrase chunking , english text chunking and hindi text chunking . in all four tasks , etl shows better results than decision trees and also than tbl with hand-crafted templates . etl provides a new training strategy that accelerates transformation learning . for the english text chunking task this corresponds to a factor of five speedup . for portuguese noun phrase chunking , etl shows the best reported results for the task . for the other three linguistic tasks , etl shows state-of-theart competitive results and maintains the advantages of using a rule based system .

three knowledge-free methods for automatic lexical chain extraction fg language technology
we present three approaches to lexical chaining based on the lda topic model and evaluate them intrinsically on a manually annotated set of german documents . after motivating the choice of statistical methods for lexical chaining with their adaptability to different languages and subject domains , we describe our new two-level chain annotation scheme , which rooted in the concept of cohesive harmony . also , we propose a new measure for direct evaluation of lexical chains . our three lda-based approaches outperform two knowledge-based state-of-the art methods to lexical chaining by a large margin , which can be attributed to lacking coverage of the knowledge resource . subsequent analysis shows that the three methods yield a different chaining behavior , which could be utilized in tasks that use lexical chaining as a component within nlp applications .

emotion cause detection with linguistic constructions
this paper proposes a multi-label approach to detect emotion causes . the multi-label model not only detects multi-clause causes , but also captures the long-distance information to facilitate emotion cause detection . in addition , based on the linguistic analysis , we create two sets of linguistic patterns during feature extraction . both manually generalized patterns and automatically generalized patterns are designed to extract general cause expressions or specific constructions for emotion causes . experiments show that our system achieves a performance much higher than a baseline model .

fast and robust joint models for biomedical event extraction sebastian riedel andrew mccallum
extracting biomedical events from literature has attracted much recent attention . the bestperforming systems so far have been pipelines of simple subtask-specific local classifiers . a natural drawback of such approaches are cascading errors introduced in early stages of the pipeline . we present three joint models of increasing complexity designed to overcome this problem . the first model performs joint trigger and argument extraction , and lends itself to a simple , efficient and exact inference algorithm . the second model captures correlations between events , while the third model ensures consistency between arguments of the same event . inference in these models is kept tractable through dual decomposition . the first two models outperform the previous best joint approaches and are very competitive with respect to the current state-of-theart . the third model yields the best results reported so far on the bionlp 2009 shared task , the bionlp 2011 genia task and the bionlp 2011 infectious diseases task .

refining the notions of depth and density in wordnet-based semantic similarity measures
we re-investigate the rationale for and the effectiveness of adopting the notions of depth and density in wordnet-based semantic similarity measures . we show that the intuition for including these notions in wordnet-based similarity measures does not always stand up to empirical examination . in particular , the traditional definitions of depth and density as ordinal integer values in the hierarchical structure of wordnet does not always correlate with human judgment of lexical semantic similarity , which imposes strong limitations on their contribution to an accurate similarity measure . we thus propose several novel definitions of depth and density , which yield significant improvement in degree of correlation with similarity . when used in wordnet-based semantic similarity measures , the new definitions consistently improve performance on a task of correlating with human judgment .

capturing cultural differences in expressions of intentions
the intersection of psychology and computational linguistics is capable of providing novel automated insight into the language of everyday cognition through analysis of micro-blogs . while twitter is often seen as banal or focused only on the who , what , when or where tweets can actually serve as a source for learning about the language people use to express complex cogntive states and their cultural identity . in this contribution we introduce a novel model which captures latent cultural dimensions through an individuals expressions of intentionality . we then show how these latent cultures can be used to create a culturally-sensitive model which provides enahnced detection of signals of intentionality in tweets . finally , we demonstrate how these models reveal interesting cross-cultural differences in the goals and motivations of individuals from different cultures .

dependency-based automatic evaluation for machine translation
we present a novel method for evaluating the output of machine translation ( mt ) , based on comparing the dependency structures of the translation and reference rather than their surface string forms . our method uses a treebank-based , widecoverage , probabilistic lexical-functional grammar ( lfg ) parser to produce a set of structural dependencies for each translation-reference sentence pair , and then calculates the precision and recall for these dependencies . our dependencybased evaluation , in contrast to most popular string-based evaluation metrics , will not unfairly penalize perfectly valid syntactic variations in the translation . in addition to allowing for legitimate syntactic differences , we use paraphrases in the evaluation process to account for lexical variation . in comparison with other metrics on 16,800 sentences of chinese-english newswire text , our method reaches high correlation with human scores . an experiment with two translations of 4,000 sentences from spanish-english europarl shows that , in contrast to most other metrics , our method does not display a high bias towards statistical models of translation .

automatic extraction of briefing templates dipanjan das mohit kumar
an approach to solving the problem of automatic briefing generation from non-textual events can be segmenting the task into two major steps , namely , extraction of briefing templates and learning aggregators that collate information from events and automatically fill up the templates . in this paper , we describe two novel unsupervised approaches for extracting briefing templates from human written reports . since the problem is non-standard , we define our own criteria for evaluating the approaches and demonstrate that both approaches are effective in extracting domain relevant templates with promising accuracies .

analyzing the errors of unsupervised learning
we identify four types of errors that unsupervised induction systems make and study each one in turn . our contributions include ( 1 ) using a meta-model to analyze the incorrect biases of a model in a systematic way , ( 2 ) providing an efficient and robust method of measuring distance between two parameter settings of a model , and ( 3 ) showing that local optima issues which typically plague em can be somewhat alleviated by increasing the number of training examples . we conduct our analyses on three models : the hmm , the pcfg , and a simple dependency model .

pp-attachment disambiguation boosted by a gigantic volume of unambiguous examples
on a gigantic volume of unambiguous examples extracted from raw corpus . the unambiguous examples are utilized to acquire precise lexical preferences for pp-attachment disambiguation . attachment decisions are made by a machine learning method that optimizes the use of the lexical preferences . our experiments indicate that the precise lexical preferences work effectively .

phrase-based translation model for question retrieval in community question answer archives
community-based question answer ( q & a ) has become an important issue due to the popularity of q & a archives on the web . this paper is concerned with the problem of question retrieval . question retrieval in q & a archives aims to find historical questions that are semantically equivalent or relevant to the queried questions . in this paper , we propose a novel phrase-based translation model for question retrieval . compared to the traditional word-based translation models , the phrasebased translation model is more effective because it captures contextual information in modeling the translation of phrases as a whole , rather than translating single words in isolation . experiments conducted on real q & a data demonstrate that our proposed phrasebased translation model significantly outperforms the state-of-the-art word-based translation model .

multilingual subjectivity analysis using machine translation
although research in other languages is increasing , much of the work in subjectivity analysis has been applied to english data , mainly due to the large body of electronic resources and tools that are available for this language . in this paper , we propose and evaluate methods that can be employed to transfer a repository of subjectivity resources across languages . specifically , we attempt to leverage on the resources available for english and , by employing machine translation , generate resources for subjectivity analysis in other languages . through comparative evaluations on two different languages ( romanian and spanish ) , we show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language .

towards conversation entailment : an empirical investigation
while a significant amount of research has been devoted to textual entailment , automated entailment from conversational scripts has received less attention . to address this limitation , this paper investigates the problem of conversation entailment : automated inference of hypotheses from conversation scripts . we examine two levels of semantic representations : a basic representation based on syntactic parsing from conversation utterances and an augmented representation taking into consideration of conversation structures . for each of these levels , we further explore two ways of capturing long distance relations between language constituents : implicit modeling based on the length of distance and explicit modeling based on actual patterns of relations . our empirical findings have shown that the augmented representation with conversation structures is important , which achieves the best performance when combined with explicit modeling of long distance relations .

melb-mkb : lexical substitution system based on relatives in context
in this paper we describe the melb-mkb system , as entered in the semeval-2007 lexical substitution task . the core of our system was the relatives in context unsupervised approach , which ranked the candidate substitutes by web-lookup of the word sequences built combining the target context and each substitute . our system ranked third in the final evaluation , performing close to the top-ranked system .

type-based mcmc for sampling tree fragments from forests
this paper applies type-based markov chain monte carlo ( mcmc ) algorithms to the problem of learning synchronous context-free grammar ( scfg ) rules from a forest that represents all possible rules consistent with a fixed word alignment . while type-based mcmc has been shown to be effective in a number of nlp applications , our setting , where the tree structure of the sentence is itself a hidden variable , presents a number of challenges to type-based inference . we describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges . these methods lead to improvements in both log likelihood and bleu score in our experiments .

effects of meaning-preserving corrections on language learning
we present a computational model of language learning via a sequence of interactions between a teacher and a learner . experiments learning limited sublanguages of 10 natural languages show that the learner achieves a high level of performance after a reasonable number of interactions , the teacher can produce meaning-preserving corrections of the learners utterances , and the learner can detect them . the learner does not treat corrections specially ; nonetheless in several cases , significantly fewer interactions are needed by a learner interacting with a correcting teacher than with a non-correcting teacher .

parser combination by reparsing
we present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers . we apply this idea to dependency and constituent parsing , generating results that surpass state-of-theart accuracy levels for individual parsers .

preemptive information extraction using unrestricted relation discovery
we are trying to extend the boundary of information extraction ( ie ) systems . existing ie systems require a lot of time and human effort to tune for a new scenario . preemptive information extraction is an attempt to automatically create all feasible ie systems in advance without human intervention . we propose a technique called unrestricted relation discovery that discovers all possible relations from texts and presents them as tables . we present a preliminary system that obtains reasonably good results .

predicting code-switching in multilingual communication for for advanced study
immigrant communities host multilingual speakers who switch across languages and cultures in their daily communication practices . although there are in-depth linguistic descriptions of code-switching across different multilingual communication settings , there is a need for automatic prediction of code-switching in large datasets . we use emoticons and multi-word expressions as novel features to predict code-switching in a large online discussion forum for the turkish-dutch immigrant community in the netherlands . our results indicate that multi-word expressions are powerful features to predict code-switching .

by all these lovely tokens merging conflicting tokenizations
given the contemporary trend to modular nlp architectures and multiple annotation frameworks , the existence of concurrent tokenizations of the same text represents a pervasive problem in everydays nlp practice and poses a non-trivial theoretical problem to the integration of linguistic annotations and their interpretability in general . this paper describes a solution for integrating different tokenizations using a standoff xml format , and discusses the consequences for the handling of queries on annotated corpora .

joint emotion analysis via multi-task gaussian processes
we propose a model for jointly predicting multiple emotions in natural language sentences . our model is based on a low-rank coregionalisation approach , which combines a vector-valued gaussian process with a rich parameterisation scheme . we show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset . the proposed model outperforms both singletask baselines and other multi-task approaches .

generating case markers in machine translation kristina toutanova hisami suzuki
we study the use of rich syntax-based statistical models for generating grammatical case for the purpose of machine translation from a language which does not indicate case explicitly ( english ) to a language with a rich system of surface case markers ( japanese ) . we propose an extension of n-best re-ranking as a method of integrating such models into a statistical mt system and show that this method substantially outperforms standard n-best re-ranking . our best performing model achieves a statistically significant improvement over the baseline mt system according to the bleu metric . human evaluation also confirms the results .

addressing ambiguity in unsupervised part-of-speech induction with
we study substitute vectors to solve the part-of-speech ambiguity problem in an unsupervised setting . part-of-speech tagging is a crucial preliminary process in many natural language processing applications . because many words in natural languages have more than one part-of-speech tag , resolving part-of-speech ambiguity is an important task . we claim that partof-speech ambiguity can be solved using substitute vectors . a substitute vector is constructed with possible substitutes of a target word . this study is built on previous work which has proven that word substitutes are very fruitful for part-ofspeech induction . experiments show that our methodology works for words with high ambiguity .

question answering as question-biased term extraction : a new approach toward multilingual qa
this paper regards question answering ( qa ) as question-biased term extraction ( qbte ) . this new qbte approach liberates qa systems from the heavy burden imposed by question types ( or answer types ) . in conventional approaches , a qa system analyzes a given question and determines the question type , and then it selects answers from among answer candidates that match the question type . consequently , the output of a qa system is restricted by the design of the question types . the qbte directly extracts answers as terms biased by the question . to confirm the feasibility of our qbte approach , we conducted experiments on the crl qa data based on 10-fold cross validation , using maximum entropy models ( mems ) as an ml technique . experimental results showed that the trained system achieved 0.36 in mrr and 0.47 in top5 accuracy .

a computational theory of inference for
mathematical understanding can be measured by a cognitive agents ability to explain itself , i.e. , answer relevant questions about its mathematical activities . two inference techniques , rule-based inference and path-based inference , are applied to an implemented computational cognitive agent using the sneps knowledgerepresentation , reasoning , and acting system .

using the argumentative structure of scientific literature
medline/pubmed contains structured abstracts that can provide argumentative labels . selection of abstract sentences based on the argumentative label has shown to improve the performance of information retrieval tasks . these abstracts make up less than one quarter of all the abstracts in medline/pubmed , so it is worthwhile to learn how to automatically label the non-structured ones . we have compared several machine learning algorithms trained on structured abstracts to identify argumentative labels . we have performed an intrinsic evaluation on predicting argumentative labels for non-structured abstracts and an extrinsic evaluation to predict argumentative labels on abstracts relevant to gene reference into function ( generif ) indexing . intrinsic evaluation shows that argumentative labels can be assigned effectively to structured abstracts . algorithms that model the argumentative structure seem to perform better than other algorithms . extrinsic results show that assigning argumentative labels to non-structured abstracts improves the performance on generif indexing . on the other hand , the algorithms that model the argumentative structure of the abstracts obtain lower performance in the extrinsic evaluation .

extending marie : an n -gram-based smt decoder
in this paper we present several extensions of marie1 , a freely available n -gram-based statistical machine translation ( smt ) decoder . the

learning information structure in the prague treebank
this paper investigates the automatic identification of aspects of information structure ( is ) in texts . the experiments use the prague dependency treebank which is annotated with is following the praguian approach of topic focus articulation . we automatically detect t ( opic ) and f ( ocus ) , using node attributes from the treebank as basic features and derived features inspired by the annotation guidelines . we show the performance of c4.5 , bagging , and ripper classifiers on several classes of instances such as nouns and pronouns , only nouns , only pronouns . a baseline system assigning always f ( ocus ) has an f-score of 42.5 % . our best system obtains 82.04 % .

an extensible probabilistic transformation-based approach to the third recognizing textual entailment challenge
we introduce a system for textual entailment that is based on a probabilistic model of entailment . the model is defined using some calculus of transformations on dependency trees , which is characterized by the fact that derivations in that calculus preserve the truth only with a certain probability . we also describe a possible set of transformations ( and with it implicitly a calculus ) that was successfully applied to the rte3 challenge data . however , our system can be improved in many ways and we see it as the starting point for a promising new approach to textual entailment .

fast , greedy model minimization for unsupervised tagging
model minimization has been shown to work well for the task of unsupervised part-of-speech tagging with a dictionary . in ( ravi and knight , 2009 ) , the authors invoke an integer programming ( ip ) solver to do model minimization . however , solving this problem exactly using an integer programming formulation is intractable for practical purposes . we propose a novel two-stage greedy approximation scheme to replace the ip . our method runs fast , while yielding highly accurate tagging results . we also compare our method against standard em training , and show that we consistently obtain better tagging accuracies on test data of varying sizes for english and italian .

detection of non-native sentences using machine-translated training data spoken language systems
training statistical models to detect nonnative sentences requires a large corpus of non-native writing samples , which is often not readily available . this paper examines the extent to which machinetranslated ( mt ) sentences can substitute as training data . two tasks are examined . for the native vs non-native classication task , nonnative training data yields better performance ; for the ranking task , however , models trained with a large , publicly available set of mt data perform as well as those trained with non-native data .

construction of structurally annotated spoken dialogue corpus
this paper describes the structural annotation of a spoken dialogue corpus . by statistically dealing with the corpus , the automatic acquisition of dialoguestructural rules is achieved . the dialogue structure is expressed as a binary tree and 789 dialogues consisting of 8150 utterances in the ciair speech corpus are annotated . to evaluate the scalability of the corpus for creating dialogue-structural rules , a dialogue parsing experiment was conducted .

the s-space package : an open source package for word space models
we present the s-space package , an open source framework for developing and evaluating word space algorithms . the package implements well-known word space algorithms , such as lsa , and provides a comprehensive set of matrix utilities and data structures for extending new or existing models . the package also includes word space benchmarks for evaluation . both algorithms and libraries are designed for high concurrency and scalability . we demonstrate the efficiency of the reference implementations and also provide their results on six benchmarks .

graph-based learning for statistical machine translation
current phrase-based statistical machine translation systems process each test sentence in isolation and do not enforce global consistency constraints , even though the test data is often internally consistent with respect to topic or style . we propose a new consistency model for machine translation in the form of a graph-based semi-supervised learning algorithm that exploits similarities between training and test data and also similarities between different test sentences . the algorithm learns a regression function jointly over training and test data and uses the resulting scores to rerank translation hypotheses . evaluation on two travel expression translation tasks demonstrates improvements of up to 2.6 bleu points absolute and 2.8 % in per .

a hybrid approach to content analysis for automatic essay grading
we present carmeltc , a novel hybrid text classification approach for automatic essay grading . our evaluation demonstrates that the hybrid carmeltc approach outperforms two bag of words approaches , namely lsa and a naive bayes , as well as a purely symbolic approach .

svd and clustering for unsupervised pos tagging division of applied mathematics division of applied mathematics
we revisit the algorithm of schtze ( 1995 ) for unsupervised part-of-speech tagging . the algorithm uses reduced-rank singular value decomposition followed by clustering to extract latent features from context distributions . as implemented here , it achieves state-of-the-art tagging accuracy at considerably less cost than more recent methods . it can also produce a range of finer-grained taggings , with potential applications to various tasks .

multi-tagging for lexicalized-grammar parsing
with performance above 97 % accuracy for newspaper text , part of speech ( pos ) tagging might be considered a solved problem . previous studies have shown that allowing the parser to resolve pos tag ambiguity does not improve performance . however , for grammar formalisms which use more fine-grained grammatical categories , for example tag and ccg , tagging accuracy is much lower . in fact , for these formalisms , premature ambiguity resolution makes parsing infeasible . we describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient ccg parsing . we extend this multitagging approach to the pos level to overcome errors introduced by automatically assigned pos tags . although pos tagging accuracy seems high , maintaining some pos tag ambiguity in the language processing pipeline results in more accurate ccg supertagging .

getting more mileage from web text sources for conversational speech language modeling using class-dependent mixtures
sources of training data suitable for language modeling of conversational speech are limited . in this paper , we show how training data can be supplemented with text from the web filtered to match the style and/or topic of the target recognition task , but also that it is possible to get bigger performance gains from the data by using class-dependent interpolation of n-grams .

learn to speak and to write , learn to use your mind
the aim of this talk is to show to what extent the work on text generation by computer ( tgbc ) does not address some of the fundamental problems people struggle with when generating language ( tgbp ) . we will substantiate this claim by taking two tasks on which a lot of research has been carried out during the last 15 years : discourse planning and lexicalisation .

discriminative word alignment with a function word reordering model
we address the modeling , parameter estimation and search challenges that arise from the

extracting important sentences with support vector machines
extracting sentences that contain important information from a document is a form of text summarization . the technique is the key to the automatic generation of summaries similar to those written by humans . to achieve such extraction , it is important to be able to integrate heterogeneous pieces of information . one approach , parameter tuning by machine learning , has been attracting a lot of attention . this paper proposes a method of sentence extraction based on support vector machines ( svms ) . to confirm the methods performance , we conduct experiments that compare our method to three existing methods . results on the text summarization challenge ( tsc ) corpus show that our method offers the highest accuracy . moreover , we clarify the different features effective for extracting different document genres .

a comparison of bayesian estimators for unsupervised hidden markov model pos taggers
there is growing interest in applying bayesian techniques to nlp problems . there are a number of different estimators for bayesian models , and it is useful to know what kinds of tasks each does well on . this paper compares a variety of different bayesian estimators for hidden markov model pos taggers with various numbers of hidden states on data sets of different sizes . recent papers have given contradictory results when comparing bayesian estimators to expectation maximization ( em ) for unsupervised hmm pos tagging , and we show that the difference in reported results is largely due to differences in the size of the training data and the number of states in the hmm . we invesigate a variety of samplers for hmms , including some that these earlier papers did not study . we find that all of gibbs samplers do well with small data sets and few states , and that variational bayes does well on large data sets and is competitive with the gibbs samplers . in terms of times of convergence , we find that variational bayes was the fastest of all the estimators , especially on large data sets , and that explicit gibbs sampler ( both pointwise and sentence-blocked ) were generally faster than their collapsed counterparts on large data sets .

bengali named entity recognition using support vector machine
named entity recognition ( ner ) aims to classify each word of a document into predefined target named entity classes and is nowadays considered to be fundamental for many natural language processing ( nlp ) tasks such as information retrieval , machine translation , information extraction , question answering systems and others . this paper reports about the development of a ner system for bengali using support vector machine ( svm ) . though this state of the art machine learning method has been widely applied to ner in several well-studied languages , this is our first attempt to use this method to indian languages ( ils ) and particularly for bengali . the system makes use of the different contextual information of the words along with the variety of features that are helpful in predicting the various named entity ( ne ) classes . a portion of a partially ne tagged bengali news corpus , developed from the archive of a leading bengali newspaper available in the web , has been used to develop the svm-based ner system . the training set consists of approximately 150k words and has been manually annotated with the sixteen ne tags . experimental results of the 10-fold cross validation test show the effectiveness of the proposed svm based ner system with the overall average recall , precision and f-score of 94.3 % , 89.4 % and 91.8 % , respectively . it has been shown that this system outperforms other existing bengali ner systems .

contradictions and justifications : extensions to the textual entailment task
the third pascal recognizing textual entailment challenge ( rte-3 ) contained an optional task that extended the main entailment task by requiring a system to make three-way entailment decisions ( entails , contradicts , neither ) and to justify its response . contradiction was rare in the rte-3 test set , occurring in only about 10 % of the cases , and systems found accurately detecting it difficult . subsequent analysis of the results shows a test set must contain many more entailment pairs for the three-way decision task than the traditional two-way task to have equal confidence in system comparisons . each of six human judges representing eventual end users rated the quality of a justification by assigning understandability and correctness scores . ratings of the same justification across judges differed significantly , signaling the need for a better characterization of the justification task .

multilingual dependency learning : exploiting rich features for tagging syntactic and semantic dependencies
this paper describes our system about multilingual syntactic and semantic dependency parsing for our participation in the joint task of conll-2009 shared tasks . our system uses rich features and incorporates various integration technologies . the system is evaluated on in-domain and out-of-domain evaluation data of closed challenge of joint task . for in-domain evaluation , our system ranks the second for the average macro labeled f1 of all seven languages , 82.52 % ( only about 0.1 % worse than the best system ) , and the first for english with macro labeled f1 87.69 % . and for out-of-domain evaluation , our system also achieves the second for average score of all three languages .

a bayesian mixture model for part-of-speech induction using multiple features
in this paper we present a fully unsupervised syntactic class induction system formulated as a bayesian multinomial mixture model , where each word type is constrained to belong to a single class . by using a mixture model rather than a sequence model ( e.g. , hmm ) , we are able to easily add multiple kinds of features , including those at both the type level ( morphology features ) and token level ( context and alignment features , the latter from parallel corpora ) . using only context features , our system yields results comparable to state-of-the art , far better than a similar model without the one-class-per-type constraint . using the additional features provides added benefit , and our final system outperforms the best published results on most of the 25 corpora tested .

the integration of dependency relation classification and semantic role
this paper describes a system to solve the joint learning of syntactic and semantic dependencies . an directed graphical model is put forward to integrate dependency relation classification and semantic role labeling . we present a bilayer directed graph to express probabilistic relationships between syntactic and semantic relations . maximum entropy markov models are implemented to estimate conditional probability distribution and to do inference . the submitted model yields 76.28 % macro-average f1 performance , for the joint task , 85.75 % syntactic dependencies las and 66.61 % semantic dependencies f1 .

feature decay algorithms for fast deployment of accurate statistical machine translation systems
we use feature decay algorithms ( fda ) for fast deployment of accurate statistical machine translation systems taking only about half a day for each translation direction . we develop parallel fda for solving computational scalability problems caused by the abundance of training data for smt models and lm models and still achieve smt performance that is on par with using all of the training data or better . parallel fda runs separate fda models on randomized subsets of the training data and combines the instance selections later . parallel fda can also be used for selecting the lm corpus based on the training set selected by parallel fda . the high quality of the selected training data allows us to obtain very accurate translation outputs close to the top performing smt systems . the relevancy of the selected lm corpus can reach up to 86 % reduction in the number of oov tokens and up to 74 % reduction in the perplexity . we perform smt experiments in all language pairs in the wmt13 translation task and obtain smt performance close to the top systems using significantly less resources for training and development .

a hearer-oriented evaluation of referring expression generation
this paper discusses the evaluation of a generation of referring expressions algorithm that takes structural ambiguity into account . we describe an ongoing study with human readers .

opinion mining with deep recurrent neural networks
recurrent neural networks ( rnns ) are connectionist models of sequential data that are naturally applicable to the analysis of natural language . recently , depth in space as an orthogonal notion to depth in time in rnns has been investigated by stacking multiple layers of rnns and shown empirically to bring a temporal hierarchy to the architecture . in this work we apply these deep rnns to the task of opinion expression extraction formulated as a token-level sequence-labeling task . experimental results show that deep , narrow rnns outperform traditional shallow , wide rnns with the same number of parameters . furthermore , our approach outperforms previous crf-based baselines , including the state-of-the-art semi-markov crf model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-crf , as well as without the standard layer-by-layer pre-training typically required of rnn architectures .

initial fieldwork for lwazi : a telephone-based spoken dialog system for rural south africa
this paper describes sociological fieldwork conducted in the autumn of 2008 in eleven rural communities of south africa . the goal of the fieldwork was to evaluate the potential role of automated telephony services in improving access to important government information and services . our interviews , focus group discussions and surveys revealed that lwazi , a telephone-based spoken dialog system , could greatly support current south african government efforts to effectively connect citizens to available services , provided such services be toll free , in local languages , and with content relevant to each community .

a common framework for syntactic annotation
it is widely recognized that the proliferation of annotation schemes runs counter to the need to re-use language resources , and that standards for linguistic annotation are becoming increasingly mandatory . to answer this need , we have developed a representation framework comprised of an abstract model for a variety of different annotation types ( e.g. , morpho-syntactic tagging , syntactic annotation , co-reference annotation , etc . ) , which can be instantiated in different ways depending on the annotators approach and goals . in this paper we provide an overview of our representation framework and demonstrate its applicability to syntactic annotation . we show how the framework can contribute to comparative evaluation and merging of parser output and diverse syntactic annotation schemes .

the vocal intensity of turn-initial cue phrases in dialogue
the present study explores the vocal intensity of turn-initial cue phrases in a corpus of dialogues in swedish . cue phrases convey relatively little propositional content , but have several important pragmatic functions . the majority of these entities are frequently occurring monosyllabic words such as eh , mm , ja . prosodic analysis shows that these words are produced with higher intensity than other turn-initial words are . in light of these results , it is suggested that speakers produce these expressions with high intensity in order to claim the floor . it is further shown that the difference in intensity can be measured as a dynamic inter-speaker relation over the course of a dialogue using the end of the interlocutors previous turn as a reference point .

how was your day
we describe a how was your day ( hwyd ) companion whose purpose is to establish a comforting and supportive relationship with a user via a conversation on a variety of work-related topics . the system has several fairly novel features aimed at increasing the naturalness of the interaction : a rapid short loop response primed by the results of acoustic emotion analysis , and an interruption manager , enabling the user to interrupt lengthy or apparently inappropriate system responses , prompting a replanning of behaviour on the part of the system . the long loop also takes into account the emotional state of the user , but using more conventional dialogue management and planning techniques . we describe the architecture and components of the implemented prototype hwyd system .

extending corpus-based identification of light verb constructions using a supervised learning framework yee fan tan , min-yen kan and hang cui
light verb constructions ( lvcs ) , such as make a call in english , can be said to be complex predicates in which the verb plays only a functional role . lvcs pose challenges for natural language understanding , as their semantics differ from usual predicate structures . we extend the existing corpus-based measures for identifying lvcs between verb-object pairs in english , by proposing using new features that use mutual information and assess other syntactic properties . our work also incorporates both existing and new lvc features into a machine learning approach . we experimentally show that using the proposed framework incorporating all features outperforms previous work by 17 % . as machine learning techniques model the trends found in training data , we believe the proposed lvc detection framework and statistical features is easily extendable to other languages .

context based statistical morphological analyzer and its effect on hindi deepak kumar malladi and prashanth mannem
this paper revisits the work of ( malladi and mannem , 2013 ) which focused on building a statistical morphological analyzer ( sma ) for hindi and compares the performance of sma with other existing statistical analyzer , morfette . we shall evaluate sma in various experiment scenarios and look at how it performs for unseen words . the later part of the paper presents the effect of the predicted morph features on dependency parsing and extends the work to other morphologically rich languages : hindi and telugu , without any language-specific engineering .

inter-annotator agreement for ere annotation
this paper describes a system for interannotator agreement analysis of ere annotation , focusing on entity mentions and how the higher-order annotations such as events are dependent on those entity mentions . the goal of this approach is to provide both ( 1 ) quantitative scores for the various levels of annotation , and ( 2 ) information about the types of annotation inconsistencies that might exist . while primarily designed for inter-annotator agreement , it can also be considered a system for evaluation of ere annotation .

modeling context in scenario template creation
we describe a graph-based approach to scenario template creation , which is the task of creating a representation of multiple related events , such as reports of different hurricane incidents . we argue that context is valuable to identify important , semantically similar text spans from which template slots could be generalized . to leverage context , we represent the input as a set of graphs where predicate-argument tuples are vertices and their contextual relations are edges . a context-sensitive clustering framework is then applied to obtain meaningful tuple clusters by examining their intrinsic and extrinsic similarities . the clustering framework uses expectation maximization to guide the clustering process . experiments show that : 1 ) our approach generates high quality clusters , and 2 ) information extracted from the clusters is adequate to build high coverage templates .

active learning for word sense disambiguation with methods for addressing the class imbalance problem
in this paper , we analyze the effect of resampling techniques , including undersampling and over-sampling used in active learning for word sense disambiguation ( wsd ) . experimental results show that under-sampling causes negative effects on active learning , but over-sampling is a relatively good choice . to alleviate the withinclass imbalance problem of over-sampling , we propose a bootstrap-based oversampling ( bootos ) method that works better than ordinary over-sampling in active learning for wsd . finally , we investigate when to stop active learning , and adopt two strategies , max-confidence and min-error , as stopping conditions for active learning . according to experimental results , we suggest a prediction solution by considering max-confidence as the upper bound and min-error as the lower bound for stopping conditions .

ontology-based linguistic annotation
we propose an ontology-based framework for linguistic annotation of written texts . we argue that linguistic annotation can be actually considered a special case of semantic annotation with regard to an ontology such as pursued within the context of the semantic web . furthermore , we present cream , a semantic annotation framework , as well as its concrete implementation ontomat and show how they can be used for the purpose of linguistic annotation . we demonstrate the value of our framework by applying it to the annotation of anaphoric relations in written texts .

automatic detection of deception in child-produced speech using syntactic division of engineering science , toronto ontario canada toronto ontario canada
it is important that the testimony of children be admissible in court , especially given allegations of abuse . unfortunately , children can be misled by interrogators or might offer false information , with dire consequences . in this work , we evaluate various parameterizations of five classifiers ( including support vector machines , neural networks , and random forests ) in deciphering truth from lies given transcripts of interviews with 198 victims of abuse between the ages of 4 and 7. these evaluations are performed using a novel set of syntactic features , including measures of complexity . our results show that sentence length , the mean number of clauses per utterance , and the stajnermitkov measure of complexity are highly informative syntactic features , that classification accuracy varies greatly by the age of the speaker , and that accuracy up to 91.7 % can be achieved by support vector machines given a sufficient amount of data .

raytheon bbn technologies , cambridge , ma
distant supervision usually utilizes only unlabeled data and existing knowledge bases to learn relation extraction models . however , in some cases a small amount of human labeled data is available . in this paper , we demonstrate how a state-of-theart multi-instance multi-label model can be modified to make use of these reliable sentence-level labels in addition to the relation-level distant supervision from a database . experiments show that our approach achieves a statistically significant increase of 13.5 % in f-score and 37 % in area under the precision recall curve .

separating fact from fear : tracking flu infections on twitter
twitter has been shown to be a fast and reliable method for disease surveillance of common illnesses like influenza . however , previous work has relied on simple content analysis , which conflates flu tweets that report infection with those that express concerned awareness of the flu . by discriminating these categories , as well as tweets about the authors versus about others , we demonstrate significant improvements on influenza surveillance using twitter .

unsupervised methods for developing taxonomies by combining syntactic and statistical information
this paper describes an unsupervised algorithm for placing unknown words into a taxonomy and evaluates its accuracy on a large and varied sample of words . the algorithm works by first using a large corpus to find semantic neighbors of the unknown word , which we accomplish by combining latent semantic analysis with part-of-speech information . we then place the unknown word in the part of the taxonomy where these neighbors are most concentrated , using a class-labelling algorithm developed especially for this task . this method is used to reconstruct parts of the existing wordnet database , obtaining results for common nouns , proper nouns and verbs . we evaluate the contribution made by part-of-speech tagging and show that automatic filtering using the class-labelling algorithm gives a fourfold improvement in accuracy .

modeling word segmentation
this paper describes a computational model of word segmentation and presents simulation results on realistic acquisition in particular , we explore the capacity and limitations of statistical learning mechanisms that have recently gained prominence in cognitive psychology and linguistics .

exploring english lexicon knowledge for chinese sentiment analysis yulan he harith alani
this paper presents a weakly-supervised method for chinese sentiment analysis by incorporating lexical prior knowledge obtained from english sentiment lexicons through machine translation . a mechanism is introduced to incorporate the prior information about polaritybearing words obtained from existing sentiment lexicons into latent dirichlet allocation ( lda ) where sentiment labels are considered as topics . experiments on chinese product reviews on mobile phones , digital cameras , mp3 players , and monitors demonstrate the feasibility and effectiveness of the proposed approach and show that the weakly supervised lda model performs as well as supervised classifiers such as naive bayes and support vector machines with an average of 83 % accuracy achieved over a total of 5484 review documents . moreover , the lda model is able to extract highly domain-salient polarity words from text .

a decoder for syntax-based statistical mt
this paper describes a decoding algorithm for a syntax-based translation model ( yamada and knight , 2001 ) . the model has been extended to incorporate phrasal translations as presented here . in contrast to a conventional word-to-word statistical model , a decoder for the syntaxbased model builds up an english parse tree given a sentence in a foreign language . as the model size becomes huge in a practical setting , and the decoder considers multiple syntactic structures for each word alignment , several pruning techniques are necessary . we tested our decoder in a chinese-to-english translation system , and obtained better results than ibm model 4. we also discuss issues concerning the relation between this decoder and a language model .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

automatic extraction of data deposition sentences :
research in the biomedical domain can have a major impact through open sharing of data produced . in this study , we use machine learning for the automatic identification of data deposition sentences in research articles . articles containing deposition sentences are correctly identified with 73 % f-measure . these results show the potential impact of our method for literature curation .

cross lingual adaptation : an experiment on sentiment classifications
in this paper , we study the problem of using an annotated corpus in english for the same natural language processing task in another language . while various machine translation systems are available , automated translation is still far from perfect . to minimize the noise introduced by translations , we propose to use only key reliable parts from the translations and apply structural correspondence learning ( scl ) to find a low dimensional representation shared by the two languages . we perform experiments on an englishchinese sentiment classification task and compare our results with a previous cotraining approach . to alleviate the problem of data sparseness , we create extra pseudo-examples for scl by making queries to a search engine . experiments on real-world on-line review data demonstrate the two techniques can effectively improve the performance compared to previous work .

paraphrase assessment in structured vector space :
the appropriateness of paraphrases for words depends often on context : grab can replace catch in catch a ball , but not in catch a cold . structured vector space ( svs ) ( erk and pad , 2008 ) is a model that computes word meaning in context in order to assess the appropriateness of such paraphrases . this paper investigates best-practice parameter settings for svs , and it presents a method to obtain large datasets for paraphrase assessment from corpora with wsd annotation .

graph-based ranking algorithms for sentence extraction , applied to text summarization
this paper presents an innovative unsupervised method for automatic sentence extraction using graphbased ranking algorithms . we evaluate the method in the context of a text summarization task , and show that the results obtained compare favorably with previously published results on established benchmarks .

extracting multiword translations from aligned comparable documents reinhard rapp serge sharoff
most previous attempts to identify translations of multiword expressions using comparable corpora relied on dictionaries of single words . the translation of a multiword was then constructed from the translations of its components . in contrast , in this work we try to determine the translation of a multiword unit by analyzing its contextual behaviour in aligned comparable documents , thereby not presupposing any given dictionary . whereas with this method translation results for single words are rather good , the results for multiword units are considerably worse . this is an indication that the type of multiword expressions considered here is too infrequent to provide a sufficient amount of contextual information . thus indirectly it is confirmed that it should make sense to look at the contextual behaviour of the components of a multiword expression individually , and to combine the results .

analysis and synthesis of the distribution of consonants over languages : a complex network approach
cross-linguistic similarities are reflected by the speech sound systems of languages all over the world . in this work we try to model such similarities observed in the consonant inventories , through a complex bipartite network . we present a systematic study of some of the appealing features of these inventories with the help of the bipartite network . an important observation is that the occurrence of consonants follows a two regime power law distribution . we find that the consonant inventory size distribution together with the principle of preferential attachment are the main reasons behind the emergence of such a two regime behavior . in order to further support our explanation we present a synthesis model for this network based on the general theory of preferential attachment .

revisions that improve cohesion in multi-document summaries : a preliminary study
extractive summaries produced from multiple source documents suffer from an array of problems with respect to text cohesion . in this preliminary study , we seek to understand what problems occur in such summaries and how often . we present an analysis of a small corpus of manually revised summaries and discuss the feasibility of making such repairs automatically . additionally , we present a taxonomy of the problems that occur in the corpus , as well as the operators which , when applied to the summaries , can address these concerns . this study represents a first step toward identifying and automating revision operators that could work with current summarization systems in order to repair cohesion problems in multidocument summaries .

xmg - an expressive formalism for describing tree-based grammars
in this paper1 we introduce extensible metagrammar , a system that facilitates the development of tree based grammars . this system includes both ( 1 ) a formal language adapted to the description of linguistic information and ( 2 ) a compiler for this language . it applies techniques of logic programming ( e.g . warrens abstract machine ) , thus providing an efficient and theoretically motivated framework for the processing of linguistic metadescriptions .

automatic learning of language model structure
statistical language modeling remains a challenging task , in particular for morphologically rich languages . recently , new approaches based on factored language models have been developed to address this problem . these models provide principled ways of including additional conditioning variables other than the preceding words , such as morphological or syntactic features . however , the number of possible choices for model parameters creates a large space of models that can not be searched exhaustively . this paper presents an entirely data-driven model selection procedure based on genetic search , which is shown to outperform both knowledge-based and random selection procedures on two different language modeling tasks ( arabic and turkish ) .

its not you , its me : detecting flirting and its misperception
automatically detecting human social intentions from spoken conversation is an important task for dialogue understanding . since the social intentions of the speaker may differ from what is perceived by the hearer , systems that analyze human conversations need to be able to extract both the perceived and the intended social meaning . we investigate this difference between intention and perception by using a spoken corpus of speed-dates in which both the speaker and the listener rated the speaker on flirtatiousness . our flirtationdetection system uses prosodic , dialogue , and lexical features to detect a speakers intent to flirt with up to 71.5 % accuracy , significantly outperforming the baseline , but also outperforming the human interlocuters . our system addresses lexical feature sparsity given the small amount of training data by using an autoencoder network to map sparse lexical feature vectors into 30 compressed features . our analysis shows that humans are very poor perceivers of intended flirtatiousness , instead often projecting their own intended behavior onto their interlocutors .

interpreting semantic relations in noun compounds via verb semantics su nam kim and timothy baldwin
we propose a novel method for automatically interpreting compound nouns based on a predefined set of semantic relations . first we map verb tokens in sentential contexts to a fixed set of seed verbs using wordnet : :similarity and mobys thesaurus . we then match the sentences with semantic relations based on the semantics of the seed verbs and grammatical roles of the head noun and modifier . based on the semantics of the matched sentences , we then build a classifier using timbl . the performance of our final system at interpreting ncs is 52.6 % .

bart : a multilingual anaphora resolution system
bart ( versley et al , 2008 ) is a highly modular toolkit for coreference resolution that supports state-of-the-art statistical approaches and enables efficient feature engineering . for the semeval task 1 on coreference resolution , bart runs have been submitted for german , english , and italian . bart relies on a maximum entropy-based classifier for pairs of mentions . a novel entitymention approach based on semantic trees is at the moment only supported for english .

a syntax-directed translator with extended domain of locality
a syntax-directed translator first parses the source-language input into a parsetree , and then recursively converts the tree into a string in the target-language . we model this conversion by an extended treeto-string transducer that have multi-level trees on the source-side , which gives our system more expressive power and flexibility . we also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation . the model is then extended to the general log-linear framework in order to rescore with other features like n-gram language models . we devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring . initial experimental results on english-to-chinese translation are presented .

improving classification-based natural language understanding with
although data-driven techniques are commonly used for natural language understanding in dialogue systems , their efficacy is often hampered by the lack of appropriate annotated training data in sufficient amounts . we present an approach for rapid and cost-effective annotation of training data for classification-based language understanding in conversational dialogue systems . experiments using a webaccessible conversational character that interacts with a varied user population show that a dramatic improvement in natural language understanding and a substantial reduction in expert annotation effort can be achieved by leveraging non-expert annotation .

detecting metaphor by contextual analogy
as one of the most challenging issues in nlp , metaphor identification and its interpretation have seen many models and methods proposed . this paper presents a study on metaphor identification based on the semantic similarity between literal and non literal meanings of words that can appear at the same context .

a new feature selection score for multinomial naive bayes text classification based on kl-divergence
we define a new feature selection score for text classification based on the kl-divergence between the distribution of words in training documents and their classes . the score favors words that have a similar distribution in documents of the same class but different distributions in documents of different classes . experiments on two standard data sets indicate that the new method outperforms mutual information , especially for smaller categories .

annotating emotion in dialogue mary mcgee wood
communication behaviour is affected by emotion . here we discuss how dialogue is affected by participants emotion and how expressions of emotion are manifested in its content .

creating robust supervised classifiers via web-scale n-gram data
in this paper , we systematically assess the value of using web-scale n-gram data in state-of-the-art supervised nlp classifiers . we compare classifiers that include or exclude features for the counts of various n-grams , where the counts are obtained from a web-scale auxiliary corpus . we show that including n-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering , spelling correction , noun compound bracketing , and verb part-of-speech disambiguation . more importantly , when operating on new domains , or when labeled training data is not plentiful , we show that using web-scale n-gram features is essential for achieving robust performance .

resources for urdu language processing
urdu is spoken by more than 100 million speakers . this paper summarizes the corpus and lexical resources being developed for urdu by the crulp , in pakistan .

a resource-poor approach for linking ontology classes to
the applicability of ontologies for natural language processing depends on the ability to link ontological concepts and relations to their realisations in texts . we present a general , resource-poor account to create such a linking automatically by extracting wikipedia articles corresponding to ontology classes . we evaluate our approach in an experiment with the music ontology . we consider linking as a promising starting point for subsequent steps of information extraction . 381 382 reiter , hartung , and frank

critical reflections on evaluation practices in coreference resolution gordana ilic holen
in this paper we revisit the task of quantitative evaluation of coreference resolution systems . we review the most commonly used metrics ( muc , b3 , ceaf and blanc ) on the basis of their evaluation of coreference resolution in five texts from the ontonotes corpus . we examine both the correlation between the metrics and the degree to which our human judgement of coreference resolution agrees with the metrics . in conclusion we claim that loss of information value is an essential factor , insufficiently adressed in current metrics , in human perception of the degree of success or failure of coreference resolution . we thus conjecture that including a layer of mention information weight could improve both the coreference resolution and its evaluation .

confidence-based active learning methods for machine translation
the paper presents experiments with active learning methods for the acquisition of training data in the context of machine translation . we propose a confidencebased method which is superior to the state-of-the-art method both in terms of quality and complexity . additionally , we discovered that oracle selection techniques that use real quality scores lead to poor results , making the effectiveness of confidence-driven methods of active learning for machine translation questionable .

a study on convolution kernels for shallow semantic parsing
in this paper we have designed and experimented novel convolution kernels for automatic classification of predicate arguments . their main property is the ability to process structured representations . support vector machines ( svms ) , using a combination of such kernels and the flat feature kernel , classify propbank predicate arguments with accuracy higher than the current argument classification stateof-the-art . additionally , experiments on framenet data have shown that svms are appealing for the classification of semantic roles even if the proposed kernels do not produce any improvement .

japanese named entity recognition using structural natural language processing
this paper presents an approach that uses structural information for japanese named entity recognition ( ner ) . our ner system is based on support vector machine ( svm ) , and utilizes four types of structural information : cache features , coreference relations , syntactic features and caseframe features , which are obtained from structural analyses . we evaluated our approach on crl ne data and obtained a higher f-measure than existing approaches that do not use structural information . we also conducted experiments on irex ne data and an ne-annotated web corpus and confirmed that structural information improves the performance of ner .

creative discovery in lexical ontologies
compound terms play a surprisingly key role in the organization of lexical ontologies . however , their inclusion forces one to address the issues of completeness and consistency that naturally arise from this organizational role . in this paper we show how creative exploration in the space of literal compounds can reveal not only additional compound terms to systematically balance an ontology , but can also discover new and potentially innovative concepts in their own right .

constraint-based computational semantics : a comparison between ltag and lrs
this paper compares two approaches to computational semantics , namely semantic unification in lexicalized tree adjoining grammars ( ltag ) and lexical resource semantics ( lrs ) in hpsg . there are striking similarities between the frameworks that make them comparable in many respects . we will exemplify the differences and similarities by looking at several phenomena . we will show , first of all , that many intuitions about the mechanisms of semantic computations can be implemented in similar ways in both frameworks . secondly , we will identify some aspects in which the frameworks intrinsically differ due to more general differences between the approaches to formal grammar adopted by ltag and hpsg .

speech summarization without lexical features for mandarin broadcast news
we present the first known empirical study on speech summarization without lexical features for mandarin broadcast news . we evaluate acoustic , lexical and structural features as predictors of summary sentences . we find that the summarizer yields good performance at the average fmeasure of 0.5646 even by using the combination of acoustic and structural features alone , which are independent of lexical features . in addition , we show that structural features are superior to lexical features and our summarizer performs surprisingly well at the average f-measure of 0.3914 by using only acoustic features . these findings enable us to summarize speech without placing a stringent demand on speech recognition accuracy .

frontiers in linguistic annotation for lower-density languages
the languages that are most commonly subject to linguistic annotation on a large scale tend to be those with the largest populations or with recent histories of linguistic scholarship . in this paper we discuss the problems associated with lowerdensity languages in the context of the development of linguistically annotated resources . we frame our work with three key questions regarding the definition of lower-density languages ; increasing available resources and reducing data requirements . a number of steps forward are identified for increasing the number lowerdensity language corpora with linguistic annotations .

workshop on statistical machine translation juan antonio p prompsit language engineering ,
this paper describes the system jointly developed by members of the departament de llenguatges i sistemes inform`atics at universitat dalacant and the prompsit language engineering company for the shared translation task of the 2014 workshop on statistical machine translation . we present a phrase-based statistical machine translation system whose phrase table is enriched with information obtained from dictionaries and shallowtransfer rules like those used in rule-based machine translation . the novelty of our approach lies in the fact that the transfer rules used were not written by humans , but automatically inferred from a parallel corpus .

monolingual marginal matching for translation model adaptation hal daume iii
when using a machine translation ( mt ) model trained on old-domain parallel data to translate new-domain text , one major challenge is the large number of out-of-vocabulary ( oov ) and new-translation-sense words . we present a method to identify new translations of both known and unknown source language words that uses new-domain comparable document pairs . starting with a joint distribution of source-target word pairs derived from the old-domain parallel corpus , our method recovers a new joint distribution that matches the marginal distributions of the new-domain comparable document pairs , while minimizing the divergence from the old-domain distribution . adding learned translations to our french-english mt model results in gains of about 2 bleu points over strong baselines .

latent-descriptor clustering for unsupervised pos induction division of applied mathematics
we present a novel approach to distributionalonly , fully unsupervised , pos tagging , based on an adaptation of the em algorithm for the estimation of a gaussian mixture . in this approach , which we call latent-descriptor clustering ( ldc ) , word types are clustered using a series of progressively more informative descriptor vectors . these descriptors , which are computed from the immediate left and right context of each word in the corpus , are updated based on the previous state of the cluster assignments . the ldc algorithm is simple and intuitive . using standard evaluation criteria for unsupervised pos tagging , ldc shows a substantial improvement in performance over state-of-the-art methods , along with a several-fold reduction in computational cost .

combining intra- and multi-sentential rhetorical parsing for document-level discourse analysis
we propose a novel approach for developing a two-stage document-level discourse parser . our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two conditional random fields : one for intrasentential parsing and the other for multisentential parsing . we present two approaches to combine these two stages of discourse parsing effectively . a set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art , often by a wide margin .

a web-enabled and speech-enhanced parallel corpus of greek - bulgarian cultural texts
this paper reports on completed work carried out in the framework of an eu-funded project aimed at ( a ) developing a bilingual collection of cultural texts in greek and bulgarian , ( b ) creating a number of accompanying resources that will facilitate study of the primary texts across languages , and ( c ) integrating a system which aims to provide web-enabled and speech-enhanced access to digitized bilingual cultural heritage resources . this simple user interface , which incorporates advanced search mechanisms , also offers innovative accessibility for visually impaired greek and bulgarian users . the rationale behind the work ( and the relative resource ) was to promote the comparative study of the cultural heritage of the two countries .

disambiguating sentiment ambiguous adjectives ministry of education , china
sentiment ambiguous adjectives cause major difficulties for existing algorithms of sentiment analysis . we present an evaluation task designed to provide a framework for comparing different approaches in this problem . we define the task , describe the data creation , list the participating systems and discuss their results . there are 8 teams and 16 systems .

automatic processing of diabetic patients hospital documentation
the paper presents a rule-based information extraction ( ie ) system for polish medical texts . we select the most important information from diabetic patients records . most data being processed are free-form texts , only a part is in table form . the work has three goals : to test classical ie methods on texts in polish , to create relational database containing the extracted data , and to prepare annotated data for further ie experiments .

marieke van erp , antal van den bosch , sander wubben , steve hunt
an approach is presented to the automatic discovery of labels of relations between pairs of ontological classes . using a hyperlinked encyclopaedic resource , we gather evidence for likely predicative labels by searching for sentences that describe relations between terms . the terms are instances of the pair of ontological classes under consideration , drawn from a populated knowledge base . verbs or verb phrases are automatically extracted , yielding a ranked list of candidate relations . human judges rate the extracted relations . the extracted relations provide a basis for automatic ontology discovery from a non-relational database . the approach is demonstrated on a database from the natural history domain .

an algorithm for open text semantic parsing
this paper describes an algorithm for open text shallow semantic parsing . the algorithm relies on a frame dataset ( framenet ) and a semantic network ( wordnet ) , to identify semantic relations between words in open text , as well as shallow semantic features associated with concepts in the text . parsing semantic structures allows semantic units and constituents to be accessed and processed in a more meaningful way than syntactic parsing , moving the automation of understanding natural language text to a higher level .

a framework for figurative language detection based on sense
various text mining algorithms require the process of feature selection . high-level semantically rich features , such as figurative language uses , speech errors etc. , are very promising for such problems as e.g . writing style detection , but automatic extraction of such features is a big challenge . in this paper , we propose a framework for figurative language use detection . this framework is based on the idea of sense differentiation . we describe two algorithms illustrating the mentioned idea . we show then how these algorithms work by applying them to russian language data .

multi-component word sense disambiguation
this paper describes the system mc-wsd presented for the english lexical sample task . the system is based on a multicomponent architecture . it consists of one classifier with two components . one is trained on the data provided for the task . the second is trained on this data and , additionally , on an external training set extracted from the wordnet glosses . the goal of the additional component is to lessen sparse data problems by exploiting the information encoded in the ontology .

the cmu-ark german-english translation system
this paper describes the german-english translation system developed by the ark research group at carnegie mellon university for the sixth workshop on machine translation ( wmt11 ) . we present the results of several modeling and training improvements to our core hierarchical phrase-based translation system , including : feature engineering to improve modeling of the derivation structure of translations ; better handing of oovs ; and using development set translations into other languages to create additional pseudoreferences for training .

time-efficient creation of an accurate sentence fusion corpus
sentence fusion enables summarization and question-answering systems to produce output by combining fully formed phrases from different sentences . yet there is little data that can be used to develop and evaluate fusion techniques . in this paper , we present a methodology for collecting fusions of similar sentence pairs using amazons mechanical turk , selecting the input pairs in a semiautomated fashion . we evaluate the results using a novel technique for automatically selecting a representative sentence from multiple responses . our approach allows for rapid construction of a high accuracy fusion corpus .

emma : a novel evaluation metric for morphological analysis
we present a novel evaluation metric for morphological analysis ( emma ) that is both linguistically appealing and empirically sound . emma uses a graphbased assignment algorithm , optimized via integer linear programming , to match morphemes of predicted word analyses to the analyses of a morphologically rich answer key . this is necessary especially for unsupervised morphology analysis systems which do not have access to linguistically motivated morpheme labels . across 3 languages , emma scores of 14 systems have a substantially greater positive correlation with mean average precision in an information retrieval ( ir ) task than do scores from the metric currently used by the morpho challenge ( mc ) competition series . we compute emma and mc metric scores for 93 separate system-language pairs from the 2007 , 2008 , and 2009 mc competitions , demonstrating that emma is not susceptible to two types of gaming that have plagued recent mc competitions : ambiguity hijacking and shared morpheme padding .

merging word senses
wordnet , a widely used sense inventory for word sense disambiguation ( wsd ) , is often too fine-grained for many natural language applications because of its narrow sense distinctions . we present a semi-supervised approach to learn similarity between wordnet synsets using a graph based recursive similarity definition . we seed our framework with sense similarities of all the word-sense pairs , learnt using supervision on humanlabelled sense clusterings . finally we discuss our method to derive coarse sense inventories at arbitrary granularities and show that the coarse-grained sense inventory obtained significantly boosts the disambiguation of nouns on standard test sets .

target-centric features for translation quality estimation
we describe the dcu-mixed and dcusvr submissions to the wmt-14 quality estimation task 1.1 , predicting sentencelevel perceived post-editing effort . feature design focuses on target-side features as we hypothesise that the source side has little effect on the quality of human translations , which are included in task 1.1 of this years wmt quality estimation shared task . we experiment with features of the quest framework , features of our past work , and three novel feature sets . despite these efforts , our two systems perform poorly in the competition . follow up experiments indicate that the poor performance is due to improperly optimised parameters .

the mile corpus for less commonly taught languages
this paper describes a small , structured english corpus that is designed for translation into less commonly taught languages ( lctls ) , and a set of re-usable tools for creation of similar corpora . the corpus systematically explores meanings that are known to affect morphology or syntax in the worlds languages . each sentence is associated with a feature structure showing the elements of meaning that are represented in the sentence . the corpus is highly structured so that it can support machine learning with only a small amount of data . as part of the reflex program , the corpus will be translated into multiple lctls , resulting in parallel corpora can be used for training of mt and other language technologies . only the untranslated english corpus is described in this paper .

dependency parsing with reference to slovene , spanish and swedish natural language processing natural language processing
we describe a parser used in the conll 2006 shared task , multingual dependency parsing . the parser first identifies syntactic dependencies and then labels those dependencies using a maximum entropy classifier . we consider the impact of feature engineering and the choice of machine learning algorithm , with particular focus on slovene , spanish and swedish .

schema and variation : digitizing printed dictionaries
in this paper we show how to exploit typographical and textual features of raw text for creating a finegrain xml schema markup with special focus on capturing linguistic variation in dictionaries . we use declarative programming techniques and contextfree grammars implemented in prolog .

improved word alignment using a symmetric lexicon model
word-aligned bilingual corpora are an important knowledge source for many tasks in natural language processing . we improve the well-known ibm alignment models , as well as the hidden-markov alignment model using a symmetric lexicon model . this symmetrization takes not only the standard translation direction from source to target into account , but also the inverse translation direction from target to source . we present a theoretically sound derivation of these techniques . in addition to the symmetrization , we introduce a smoothed lexicon model . the standard lexicon model is based on full-form words only . we propose a lexicon smoothing method that takes the word base forms explicitly into account . therefore , it is especially useful for highly inflected languages such as german . we evaluate these methods on the germanenglish verbmobil task and the frenchenglish canadian hansards task . we show statistically significant improvements of the alignment quality compared to the best system reported so far .

two knives cut better than one : chinese word segmentation with dual decomposition
there are two dominant approaches to chinese word segmentation : word-based and character-based models , each with respective strengths . prior work has shown that gains in segmentation performance can be achieved from combining these two types of models ; however , past efforts have not provided a practical technique to allow mainstream adoption . we propose a method that effectively combines the strength of both segmentation schemes using an efficient dual-decomposition algorithm for joint inference . our method is simple and easy to implement . experiments on sighan 2003 and 2005 evaluation datasets show that our method achieves the best reported results to date on 6 out of 7 datasets .

combining outputs of multiple japanese named entity chunkers and computer sciences ,
in this paper , we propose a method for learning a classifier which combines outputs of more than one japanese named entity extractors . the proposed combination method belongs to the family of stacked generalizers , which is in principle a technique of combining outputs of several classifiers at the first stage by learning a second stage classifier to combine those outputs at the first stage . individual models to be combined are based on maximum entropy models , one of which always considers surrounding contexts of a fixed length , while the other considers those of variable lengths according to the number of constituent morphemes of named entities . as an algorithm for learning the second stage classifier , we employ a decision list learning method . experimental evaluation shows that the proposed method achieves improvement over the best known results with japanese named entity extractors based on maximum entropy models .

dynamic feature selection for dependency parsing he he hal daume iii
feature computation and exhaustive search have significantly restricted the speed of graph-based dependency parsing . we propose a faster framework of dynamic feature selection , where features are added sequentially as needed , edges are pruned early , and decisions are made online for each sentence . we model this as a sequential decision-making problem and solve it by imitation learning techniques . we test our method on 7 languages . our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features , while computing fewer than 30 % of the feature templates .

natural language models for predicting programming comments
statistical language models have successfully been used to describe and analyze natural language documents . recent work applying language models to programming languages is focused on the task of predicting code , while mainly ignoring the prediction of programmer comments . in this work , we predict comments from java source files of open source projects , using topic models and n-grams , and we analyze the performance of the models given varying amounts of background data on the project being predicted . we evaluate models on their comment-completion capability in a setting similar to codecompletion tools built into standard code editors , and show that using a comment completion tool can save up to 47 % of the comment typing .

coping with problems in grammars automatically extracted from treebanks
we report in this paper on an experiment on automatic extraction of a tree adjoining grammar from the wsj corpus of the penn treebank . we use an automatic tool developed by ( xia , 2001 ) properly adapted to our particular need . rather than addressing general aspects of the automatic extraction we focus on the problems we have found to extract a linguistically ( and computationally ) sound grammar and approaches to handle them .

together we can : bilingual bootstrapping for wsd
recent work on bilingual word sense disambiguation ( wsd ) has shown that a resource deprived language ( l1 ) can benefit from the annotation work done in a resource rich language ( l2 ) via parameter projection . however , this method assumes the presence of sufficient annotated data in one resource rich language which may not always be possible . instead , we focus on the situation where there are two resource deprived languages , both having a very small amount of seed annotated data and a large amount of untagged data . we then use bilingual bootstrapping , wherein , a model trained using the seed annotated data of l1 is used to annotate the untagged data of l2 and vice versa using parameter projection . the untagged instances of l1 and l2 which get annotated with high confidence are then added to the seed data of the respective languages and the above process is repeated . our experiments show that such a bilingual bootstrapping algorithm when evaluated on two different domains with small seed sizes using hindi ( l1 ) and marathi ( l2 ) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost .

combining multiple models for speech information retrieval
in this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a speech information retrieval task . the formulas for combining the models are tuned on training data . then the system is evaluated on test data . the task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . also , the topics are real information needs , difficult to satisfy . information retrieval systems are not able to obtain good results on this data set , except for the case when manual summaries are included .

route communication in dialogue : a matter of principles
the present study uses the dialogue paradigm to explore route communication . it revolves around the analysis of a corpus of route instructions produced in real-time interaction with the follower . it explores the variation in forming route instructions and the factors that contribute in it . the results show that visual co-presence influences the performance , conversation patterns and configuration of instructions . most importantly , the results suggest an analogy between the choices of instructiongivers and the communicative actions of their partners .

a novel approach to automatic gazetteer generation using
gazetteers or entity dictionaries are important knowledge resources for solving a wide range of nlp problems , such as entity extraction . we introduce a novel method to automatically generate gazetteers from seed lists using an external knowledge resource , the wikipedia . unlike previous methods , our method exploits the rich content and various structural elements of wikipedia , and does not rely on language- or domainspecific knowledge . furthermore , applying the extended gazetteers to an entity extraction task in a scientific domain , we empirically observed a significant improvement in system accuracy when compared with those using seed gazetteers .

zipfian corruptions for robust pos tagging
inspired by robust generalization and adversarial learning we describe a novel approach to learning structured perceptrons for part-ofspeech ( pos ) tagging that is less sensitive to domain shifts . the objective of our method is to minimize average loss under random distribution shifts . we restrict the possible target distributions to mixtures of the source distribution and random zipfian distributions . our algorithm is used for pos tagging and evaluated on the english web treebank and the danish dependency treebank with an average 4.4 % error reduction in tagging accuracy .

improving speech synthesis quality by reducing pitch peaks in the source recordings
we present a method for improving the perceived naturalness of corpus-based speech synthesizers . it consists in removing pronounced pitch peaks in the original recordings , which typically lead to noticeable discontinuities in the synthesized speech . we perceptually evaluated this method using two concatenative and two hmm-based synthesis systems , and found that using it on the source recordings managed to improve the naturalness of the synthesizers and had no effect on their intelligibility .

alternative approaches for generating bodies of grammar rules
we compare two approaches for describing and generating bodies of rules used for natural language parsing . in todays parsers rule bodies do not exist a priori but are generated on the fly , usually with methods based on n-grams , which are one particular way of inducing probabilistic regular languages . we compare two approaches for inducing such languages . one is based on n-grams , the other on minimization of the kullback-leibler divergence . the inferred regular languages are used for generating bodies of rules inside a parsing procedure . we compare the two approaches along two dimensions : the quality of the probabilistic regular language they produce , and the performance of the parser they were used to build . the second approach outperforms the first one along both dimensions .

chinese word segmentation as lmr tagging
in this paper we present chinese word segmentation algorithms based on the socalled lmr tagging . our lmr taggers are implemented with the maximum entropy markov model and we then use transformation-based learning to combine the results of the two lmr taggers that scan the input in opposite directions . our system achieves f-scores of and on the academia sinica corpus and the hong kong city university corpus respectively .

unsupervised word segmentation for sesotho using adaptor grammars
this paper describes a variety of nonparametric bayesian models of word segmentation based on adaptor grammars that model different aspects of the input and incorporate different kinds of prior knowledge , and applies them to the bantu language sesotho . while we find overall word segmentation accuracies lower than these models achieve on english , we also find some interesting differences in which factors contribute to better word segmentation . specifically , we found little improvement to word segmentation accuracy when we modeled contextual dependencies , while modeling morphological structure did improve segmentation accuracy .

modelling the internal variability of mwes
the issue of flexibility of multiword expressions ( mwes ) is crucial towards their identification and extraction in running text , as well as their better understanding from a linguistic perspective . if we project a large mwe lexicon onto a corpus , projecting fixed forms suffers from low recall , while an unconstrained flexible search for lemmas yields a loss in precision . in this talk , i will describe a method aimed at maximising precision in the identification of mwes in flexible mode , building on the idea that internal variability can be modelled via so-called variation patterns . i will discuss the advantages and limitations of using variation patterns , compare their performance to that of association measures , and explore their usability in mwe extraction , too . about the speaker malvina nissim is a tenured researcher in computational linguistics at the university of bologna . her research focuses on the computational handling of several lexical semantics and discourse phenomena , such as the choice of referring expressions , semantic relations within compounds and in argument structure , multiword expressions , and , more recently , on the annotation and automatic detection of modality . she is also a co-founder and promoter of the senso comune project , devoted to the creation of a common knowledge base for italian via crowdsourcing . she graduated in linguistics from the university of pisa , and obtained her phd in linguistics from the university of pavia . before joining the university of bologna she was a post-doc at the university of edinburgh and at the institute for cognitive science and technology in rome . 51

a reconfigurable stochastic tagger for languages with complex tag
we present a case study of a complex stochastic disambiguator of alternatives of morphosyntactic tags which allows for using incomplete disambiguation , shorthand tag notation , external tagset definition and external definition of multivalued context features . the tagger bases on naive bayes modeling and allows for using almost as general context features as in classical trigram taggers as well as more specific ones . its preliminary results for polish still do not meet our expectations . possible sources of the taggers failures can be : inhomogeneity of the training corpus in preparation , lack of the automatic search of probability models , too general conditional independence assumptions in defining the class of interpretable models . automatization of high-quality morphosyntactic tagging for strongly inflective languages , such as slavic languages , seems to be a much harder task than so called part-of-speech ( pos ) tagging for weaker inflective languages . an important factor increasing the complexity is the very design of the tagset . usually , the tags assigned to wordlike segments in the former task are long lists of subsequent attribute values , e.g . pos , number , case , gender , person etc . ( hajic and hladk , 1998 ; wolinski and przepirkowski , 2001 ) , so they provide much more information than almost atomic labels used for pos tagging ( manning and schtze , 1999 ) . to make the matter harder , many formal descriptions become easier when the tag attribute values are allowed to form rsrl-like type hierarchies ( przepirkowski et al , 2002 ) .

multilingual lexical network from the archives of the digital
we are describing the construction process of a specialized multilingual lexical resource dedicated for the archive of the digital silk road dsr . the dsr project creates digital archives of cultural heritage along the historical silk road ; more than 116 of basic references on silk road have been digitized and made available online . these books are written in various languages and attract people from different linguistic background , therefore , we are trying to build a multilingual repository for the terminology of the dsr to help its users , and increase the accessibility of these books . the construction of a terminological database using a classical approach is difficult and expensive . instead , we are introducing specialized lexical resources that can be constructed by the community and its resources ; we call it multilingual preterminological graphs mpgs . we build such graphs by analyzing the access log files of the website of the digital silk road . we aim at making this graph as a seed repository so multilingual volunteers can contribute . we have used the access log files of the dsr since its beginning in 2003 , and obtained an initial graph of around 116,000 terms . as an application , we have used this graph to obtain a preterminological multilingual database that has a number of applications .

extractive summarization using supervised and semi-supervised
it is difficult to identify sentence importance from a single point of view . in this paper , we propose a learning-based approach to combine various sentence features . they are categorized as surface , content , relevance and event features . surface features are related to extrinsic aspects of a sentence . content features measure a sentence based on contentconveying words . event features represent sentences by events they contained . relevance features evaluate a sentence from its relatedness with other sentences . experiments show that the combined features improved summarization performance significantly . although the evaluation results are encouraging , supervised learning approach requires much labeled data . therefore we investigate co-training by combining labeled and unlabeled data .

multilingual alignments by monolingual string differences
we propose a method to obtain subsentential alignments from several languages simultaneously . the method handles several languages at once , and avoids the complexity explosion due to the usual pair-bypair processing . it can be used for different units ( characters , morphemes , words , chunks ) . an evaluation of word alignments with a trilingual machine translation corpus has been conducted . a comparison of the results with those obtained by state of the art alignment software is reported .

nuggeteer : automatic nugget-based evaluation
the trec definition and relationship questions are evaluated on the basis of information nuggets that may be contained in system responses . human evaluators provide informal descriptions of each nugget , and judgements ( assignments of nuggets to responses ) for each response submitted by participants . while human evaluation is the most accurate way to compare systems , approximate automatic evaluation becomes critical during system development . we present nuggeteer , a new automatic evaluation tool for nugget-based tasks . like the first such tool , pourpre , nuggeteer uses words in common between candidate answer and answer key to approximate human judgements . unlike pourpre , but like human assessors , nuggeteer creates a judgement for each candidatenugget pair , and can use existing judgements instead of guessing . this creates a more readily interpretable aggregate score , and allows developers to track individual nuggets through the variants of their system . nuggeteer is quantitatively comparable in performance to pourpre , and provides qualitatively better feedback to developers .

composite tense recognition and tagging in serbian
the technology of finite-state transducers is implemented to recognize , lemmatize and tag composite tenses in serbian in a way that connects the auxiliary and main verb . the suggested approach uses a morphological electronic dictionary of simple words and appropriate local grammars .

biber redux : reconsidering dimensions of variation in american english
genre classification has been found to improve performance in many applications of statistical nlp , including language modeling for spoken language , domain adaptation of statistical parsers , and machine translation . it has also been found to benefit retrieval of spoken or written documents . at its base , however , classification assumes separability . this paper revisits an assumption that genre variation is continuous along multiple dimensions , and an early use of principal component analysis to find these dimensions . results on a very heterogeneous corpus of post1990s american english reveal four major dimensions , three of which echo those found in prior work and the fourth depending on features not used in the earlier study . the resulting model can provide a basis for more detailed analysis of sub-genres and the relation between genre and situations of language use , as well as a means to predict distributional properties of new genres .

word segmentation as general chunking
during language acquisition , children learn to segment speech into phonemes , syllables , morphemes , and words . we examine word segmentation specifically , and explore the possibility that children might have generalpurpose chunking mechanisms to perform word segmentation . the voting experts ( ve ) and bootstrapped voting experts ( bve ) algorithms serve as computational models of this chunking ability . ve finds chunks by searching for a particular information-theoretic signature : low internal entropy and high boundary entropy . bve adds to ve the ability to incorporate information about word boundaries previously found by the algorithm into future segmentations . we evaluate the general chunking model on phonemicallyencoded corpora of child-directed speech , and show that it is consistent with empirical results in the developmental literature . we argue that it offers a parsimonious alternative to specialpurpose linguistic models .

relagent : entity detection and normalization for diseases in clinical records : a linguistically driven approach relagent tech pvt ltd relagent tech pvt ltd
we refined the performance of cocoa/peaberry , a linguistically motivated system , on extracting disease entities from clinical notes in the training and development sets for task 7. entities were identified in noun chunks by use of dictionaries , and events ( the left atrium is dilated ) through our own parser and predicate-argument structures . we also developed a module to map the extracted entities to the snomed subset of umls . the module is based on direct matching against umls entries through regular expressions derived from a small set of morphological transformations , along with priority rules when multiple umls entries were matched . the performance on training and development sets was 81.0 % and 83.3 % respectively ( task a ) , and the umls matching scores were respectively 75.3 % and 78.2 % ( task b ) . however , the performance against the test set was low by comparison , 72.0 % for task a and 63.9 % for task b , even while the pure umls mapping score was reasonably high ( relaxed score in task b = 91.2 % ) . we speculate that our moderate performance on the test set derives primarily from chunking/parsing errors .

human pause and resume behaviours for unobtrusive humanlike in-car spoken dialogue systems
this paper presents a first , largely qualitative analysis of a set of human-human dialogues recorded specifically to provide insights in how humans handle pauses and resumptions in situations where the speakers can not see each other , but have to rely on the acoustic signal alone . the work presented is part of a larger effort to find unobtrusive human dialogue behaviours that can be mimicked and implemented in-car spoken dialogue systems within in the eu project get home safe , a collaboration between kth , dfki , nuance , ibm and daimler aiming to find ways of driver interaction that minimizes safety issues , . the analysis reveals several human temporal , semantic/pragmatic , and structural behaviours that are good candidates for inclusion in spoken dialogue systems .

discovering relations between noun categories
traditional approaches to relation extraction from text require manually defining the relations to be extracted . we propose here an approach to automatically discovering relevant relations , given a large text corpus plus an initial ontology defining hundreds of noun categories ( e.g. , athlete , musician , instrument ) . our approach discovers frequently stated relations between pairs of these categories , using a two step process . for each pair of categories ( e.g. , musician and instrument ) it first coclusters the text contexts that connect known instances of the two categories , generating a candidate relation for each resulting cluster . it then applies a trained classifier to determine which of these candidate relations is semantically valid . our experiments apply this to a text corpus containing approximately 200 million web pages and an ontology containing 122 categories from the nell system [ carlson et al , 2010b ] , producing a set of 781 proposed candidate relations , approximately half of which are semantically valid . we conclude this is a useful approach to semi-automatic extension of the ontology for large-scale information extraction systems such as nell .

automatically predicting sentence translation difficulty
in this paper we introduce translation difficulty index ( tdi ) , a measure of difficulty in text translation . we first define and quantify translation difficulty in terms of tdi . we realize that any measure of tdi based on direct input by translators is fraught with subjectivity and adhocism . we , rather , rely on cognitive evidences from eye tracking . tdi is measured as the sum of fixation ( gaze ) and saccade ( rapid eye movement ) times of the eye . we then establish that tdi is correlated with three properties of the input sentence , viz . length ( l ) , degree of polysemy ( dp ) and structural complexity ( sc ) . we train a support vector regression ( svr ) system to predict tdis for new sentences using these features as input . the prediction done by our framework is well correlated with the empirical gold standard data , which is a repository of < l , dp , sc > and tdi pairs for a set of sentences . the primary use of our work is a way of binning sentences ( to be translated ) in easy , medium and hard categories as per their predicted tdi .

sentence level dialect identification for machine translation system selection
in this paper we study the use of sentencelevel dialect identification in optimizing machine translation system selection when translating mixed dialect input . we test our approach on arabic , a prototypical diglossic language ; and we optimize the combination of four different machine translation systems . our best result improves over the best single mt system baseline by 1.0 % bleu and over a strong system selection baseline by 0.6 % bleu on a blind test set .

deep linguistic processing for spoken dialogue systems
we describe a framework for deep linguistic processing for natural language understanding in task-oriented spoken dialogue systems . the goal is to create domaingeneral processing techniques that can be shared across all domains and dialogue tasks , combined with domain-specific optimization based on an ontology mapping from the generic lf to the application ontology . this framework has been tested in six domains that involve tasks such as interactive planning , coordination operations , tutoring , and learning .

lexpagerank : prestige in multi-document text summarization
multidocument extractive summarization relies on the concept of sentence centrality to identify the most important sentences in a document . centrality is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence . we are now considering an approach for computing sentence importance based on the concept of eigenvector centrality ( prestige ) that we call lexpagerank . in this model , a sentence connectivity matrix is constructed based on cosine similarity . if the cosine similarity between two sentences exceeds a particular predefined threshold , a corresponding edge is added to the connectivity matrix . we provide an evaluation of our method on duc 2004 data . the results show that our approach outperforms centroid-based summarization and is quite successful compared to other summarization systems .

a provably correct learning algorithm for latent-variable pcfgs
we introduce a provably correct learning algorithm for latent-variable pcfgs . the algorithm relies on two steps : first , the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample ; second , the use of em applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition . experiments on parsing and a language modeling problem show that the algorithm is efficient and effective in practice .

early deletion of fillers in processing conversational speech
this paper evaluates the benefit of deleting fillers ( e.g . you know , like ) early in parsing conversational speech . readability studies have shown that disfluencies ( fillers and speech repairs ) may be deleted from transcripts without compromising meaning ( jones et al , 2003 ) , and deleting repairs prior to parsing has been shown to improve its accuracy ( charniak and johnson , 2001 ) . we explore whether this strategy of early deletion is also beneficial with regard to fillers . reported experiments measure the effect of early deletion under in-domain and out-of-domain parser training conditions using a state-of-the-art parser ( charniak , 2000 ) . while early deletion is found to yield only modest benefit for in-domain parsing , significant improvement is achieved for out-of-domain adaptation . this suggests a potentially broader role for disfluency modeling in adapting text-based tools for processing conversational speech .

unsupervised resolution of objects and relations on the web
the task of identifying synonymous relations and objects , or synonym resolution ( sr ) , is critical for high-quality information extraction . the bulk of previous sr work assumed strong domain knowledge or hand-tagged training examples . this paper investigates sr in the context of unsupervised information extraction , where neither is available . the paper presents a scalable , fully-implemented system for sr that runs in o ( kn log n ) time in the number of extractions n and the maximum number of synonyms per word , k. the system , called resolver , introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them . given two million assertions extracted from the web , resolver resolves objects with 78 % precision and an estimated 68 % recall and resolves relations with 90 % precision and 35 % recall .

constituent reordering and syntax models for english-tojapanese statistical machine translation
we present a constituent parsing-based reordering technique that improves the performance of the state-of-the-art english-to-japanese phrase translation system that includes distortion models by 4.76 bleu points . the phrase translation model with reordering applied at the pre-processing stage outperforms a syntax-based translation system that incorporates a phrase translation model , a hierarchical phrase-based translation model and a tree-to-string grammar . we also show that combining constituent reordering and the syntax model improves the translation quality by additional 0.84 bleu points .

selective phrase pair extraction for improved statistical machine translation
phrase-based statistical machine translation systems depend heavily on the knowledge represented in their phrase translation tables . however , the phrase pairs included in these tables are typically selected using simple heuristics that potentially leave much room for improvement . in this paper , we present a technique for selecting the phrase pairs to include in phrase translation tables based on their estimated quality according to a translation model . this method not only reduces the size of the phrase translation table , but also improves translation quality as measured by the bleu metric .

the benefit of stochastic pp attachment to a rule-based parser
to study pp attachment disambiguation as a benchmark for empirical methods in natural language processing it has often been reduced to a binary decision problem ( between verb or noun attachment ) in a particular syntactic configuration . a parser , however , must solve the more general task of deciding between more than two alternatives in many different contexts . we combine the attachment predictions made by a simple model of lexical attraction with a full-fledged parser of german to determine the actual benefit of the subtask to parsing . we show that the combination of data-driven and rule-based components can reduce the number of all parsing errors by 14 % and raise the attachment accuracy for dependency parsing of german to an unprecedented 92 % .

verifiably effective arabic dialect identification
several recent papers on arabic dialect identification have hinted that using a word unigram model is sufficient and effective for the task . however , most previous work was done on a standard fairly homogeneous dataset of dialectal user comments . in this paper , we show that training on the standard dataset does not generalize , because a unigram model may be tuned to topics in the comments and does not capture the distinguishing features of dialects . we show that effective dialect identification requires that we account for the distinguishing lexical , morphological , and phonological phenomena of dialects . we show that accounting for such can improve dialect detection accuracy by nearly 10 % absolute .

efficient confirmation strategy for large-scale text retrieval systems with spoken dialogue interface
adequate confirmation for keywords is indispensable in spoken dialogue systems to eliminate misunderstandings caused by speech recognition errors . spoken language also inherently includes out-ofdomain phrases and redundant expressions such as disfluency , which do not contribute to task achievement . it is necessary to appropriately make confirmation for important portions . however , a set of keywords necessary to achieve the tasks can not be predefined in retrieval for a largescale knowledge base unlike conventional database query tasks . in this paper , we describe two statistical measures for identifying portions to be confirmed . a relevance score represents the matching degree with the target knowledge base . a significance score detects portions that consequently affect the retrieval results . these measures are defined based on information that is automatically derived from the target knowledge base . an experimental evaluation shows that our method improved the success rate of retrieval by generating confirmation more efficiently than using a conventional confidence measure .

extracting biomedical events and modifications using subgraph matching with noisy training data
the genia event ( ge ) extraction task of the bionlp shared task addresses the extraction of biomedical events from the natural language text of the published literature . in our submission , we modified an existing system for learning of event patterns via dependency parse subgraphs to utilise a more accurate parser and significantly more , but noisier , training data . we explore the impact of these two aspects of the system and conclude that the change in parser limits recall to an extent that can not be offset by the large quantities of training data . however , our extensions of the system to extract modification events shows promise .

enhancing mention detection using projection via aligned corpora
the research question treated in this paper is centered on the idea of exploiting rich resources of one language to enhance the performance of a mention detection system of another one . we successfully achieve this goal by projecting information from one language to another via a parallel corpus . we examine the potential improvement using various degrees of linguistic information in a statistical framework and we show that the proposed technique is effective even when the target language model has access to a significantly rich feature set . experimental results show up to 2.4f improvement in performance when the system has access to information obtained by projecting mentions from a resource-richlanguage mention detection system via a parallel corpus .

using comparable corpora to adapt mt models to new domains
in previous work we showed that when using an smt model trained on old-domain data to translate text in a new-domain , most errors are due to unseen source words , unseen target translations , and inaccurate translation model scores ( irvine et al. , 2013a ) . in this work , we target errors due to inaccurate translation model scores using new-domain comparable corpora , which we mine from wikipedia . we assume that we have access to a large olddomain parallel training corpus but only enough new-domain parallel data to tune model parameters and do evaluation . we use the new-domain comparable corpora to estimate additional feature scores over the phrase pairs in our baseline models . augmenting models with the new features improves the quality of machine translations in the medical and science domains by up to 1.3 bleu points over very strong baselines trained on the 150 million word canadian hansard dataset .

pos-tagging different varieties of occitan with single-dialect resources
in this study , we tackle the question of pos-tagging written occitan , a lesser-resourced language with multiple dialects each containing several varieties . for pos-tagging , we use a supervised machine learning approach , requiring annotated training and evaluation corpora and optionally a lexicon , all of which were prepared as part of the study . although we evaluate two dialects of occitan , lengadocian and gascon , the training material and lexicon concern only lengadocian . we concluded that reasonable results ( > 89 % accuracy ) are possible with a very limited training corpus ( 2500 tokens ) , as long as it is compensated by intensive use of the lexicon . results are much lower across dialects , and pointers are provided for improvement . finally , we compare the relative contribution of more training material vs. a larger lexicon , and conclude that within our configuration , spending effort on lexicon construction yields higher returns .

finding word substitutions using a distributional similarity baseline and immediate context overlap
this paper deals with the task of finding generally applicable substitutions for a given input term . we show that the output of a distributional similarity system baseline can be filtered to obtain terms that are not simply similar but frequently substitutable . our filter relies on the fact that when two terms are in a common entailment relation , it should be possible to substitute one for the other in their most frequent surface contexts . using the google 5-gram corpus to find such characteristic contexts , we show that for the given task , our filter improves the precision of a distributional similarity system from 41 % to 56 % on a test set comprising common transitive verbs .

an annotation framework for dense event ordering
todays event ordering research is heavily dependent on annotated corpora . current corpora influence shared evaluations and drive algorithm development . partly due to this dependence , most research focuses on partial orderings of a documents events . for instance , the tempeval competitions and the timebank only annotate small portions of the event graph , focusing on the most salient events or on specific types of event pairs ( e.g. , only events in the same sentence ) . deeper temporal reasoners struggle with this sparsity because the entire temporal picture is not represented . this paper proposes a new annotation process with a mechanism to force annotators to label connected graphs . it generates 10 times more relations per document than the timebank , and our timebank-dense corpus is larger than all current corpora . we hope this process and its dense corpus encourages research on new global models with deeper reasoning .

optimal rank reduction for linear context-free rewriting systems with fan-out two
linear context-free rewriting systems ( lcfrss ) are a grammar formalism capable of modeling discontinuous phrases . many parsing applications use lcfrss where the fan-out ( a measure of the discontinuity of phrases ) does not exceed 2. we present an efficient algorithm for optimal reduction of the length of production right-hand side in lcfrss with fan-out at most 2. this results in asymptotical running time improvement for known parsing algorithms for this class .

hallucinating phrase translations for low resource mt
we demonstrate that hallucinating phrasal translations can significantly improve the quality of machine translation in low resource conditions . our hallucinated phrase tables consist of entries composed from multiple unigram translations drawn from the baseline phrase table and from translations that are induced from monolingual corpora . the hallucinated phrase table is very noisy . its translations are low precision but high recall . we counter this by introducing 30 new feature functions ( including a variety of monolinguallyestimated features ) and by aggressively pruning the phrase table . our analysis evaluates the intrinsic quality of our hallucinated phrase pairs as well as their impact in end-to-end spanish-english and hindi-english mt .

unsupervised bilingual pos tagging with markov random fields
in this paper , we give a treatment to the problem of bilingual part-of-speech induction with parallel data . we demonstrate that nave optimization of log-likelihood with joint mrfs suffers from a severe problem of local maxima , and suggest an alternative using contrastive estimation for estimation of the parameters . our experiments show that estimating the parameters this way , using overlapping features with joint mrfs performs better than previous work on the 1984 dataset .

formatting time-aligned asr transcripts for readability
we address the problem of formatting the output of an automatic speech recognition ( asr ) system for readability , while preserving wordlevel timing information of the transcript . our system enriches the asr transcript with punctuation , capitalization and properly written dates , times and other numeric entities , and our approach can be applied to other formatting tasks . the method we describe combines hand-crafted grammars with a class-based language model trained on written text and relies on weighted finite state transducers ( wfsts ) for the preservation of start and end time of each word .

relative clause extraction for syntactic simplification
this paper investigates non-destructive simplification , a type of syntactic text simplification which focuses on extracting embedded clauses from structurally complex sentences and rephrasing them without affecting their original meaning . this process reduces the average sentence length and complexity to make text simpler . although relevant for human readers with low reading skills or language disabilities , the process has direct applications in nlp . in this paper we analyse the extraction of relative clauses through a tagging approach . a dataset covering three genres was manually annotated and used to develop and compare several approaches for automatically detecting appositions and non-restrictive relative clauses . the best results are obtained by a ml model developed using crfsuite , followed by a rule based method .

evaluating an off-the-shelf pos-tagger on early modern german text
the goal of this study is to evaluate an offthe-shelf pos-tagger for modern german on historical data from the early modern period ( 1650-1800 ) . with no specialised tagger available for this particular stage of the language , our findings will be of particular interest to smaller , humanities-based projects wishing to add pos annotations to their historical data but which lack the means or resources to train a pos tagger themselves . our study assesses the effects of spelling variation on the performance of the tagger , and investigates to what extent tagger performance can be improved by using normalised input , where spelling variants in the corpus are standardised to a modern form . our findings show that adding such a normalisation layer improves tagger performance considerably .

less destructive cleaning of web documents by using standoff annotation
standoff annotation , that is , the separation of primary data and markup , can be an interesting option to annotate web pages since it does not demand the removal of annotations already present in web pages . we will present a standoff serialization that allows for annotating wellformed web pages with multiple annotation layers in a single instance , easing processing and analyzing of the data .

multilevel coarse-to-fine pcfg parsing
we present a pcfg parsing algorithm that uses a multilevel coarse-to-fine ( mlctf ) scheme to improve the efficiency of search for the best parse . our approach requires the user to specify a sequence of nested partitions or equivalence classes of the pcfg nonterminals . we define a sequence of pcfgs corresponding to each partition , where the nonterminals of each pcfg are clusters of nonterminals of the original source pcfg . we use the results of parsing at a coarser level ( i.e. , grammar defined in terms of a coarser partition ) to prune the next finer level . we present experiments showing that with our algorithm the work load ( as measured by the total number of constituents processed ) is decreased by a factor of ten with no decrease in parsing accuracy compared to standard cky parsing with the original pcfg . we suggest that the search space over mlctf algorithms is almost totally unexplored so that future work should be able to improve significantly on these results .

using reinforcement learning to build a better model of dialogue state
given the growing complexity of tasks that spoken dialogue systems are trying to handle , reinforcement learning ( rl ) has been increasingly used as a way of automatically learning the best policy for a system to make . while most work has focused on generating better policies for a dialogue manager , very little work has been done in using rl to construct a better dialogue state . this paper presents a rl approach for determining what dialogue features are important to a spoken dialogue tutoring system . our experiments show that incorporating dialogue factors such as dialogue acts , emotion , repeated concepts and performance play a significant role in tutoring and should be taken into account when designing dialogue systems .

classification for aspect based sentiment analysis
this paper describes our seemgo system for the task of aspect based sentiment analysis in semeval-2014 . the subtask of aspect term extraction is cast as a sequence labeling problem modeled with conditional random fields that obtains the f-score of 0.683 for laptops and 0.791 for restaurants by exploiting both word-based features and context features . the other three subtasks are solved by the maximum entropy model , with the occurrence counts of unigram and bigram words of each sentence as features . the subtask of aspect category detection obtains the best result when applying the boosting method on the maximum entropy model , with the precision of 0.869 for restaurants . the maximum entropy model also shows good performance in the subtasks of both aspect term and aspect category polarity classification .

is there syntactic adaptation in language comprehension
in this paper we investigate the manner in which the human language comprehension system adapts to shifts in probability distributions over syntactic structures , given experimentally controlled experience with those structures . we replicate a classic reading experiment , and present a model of the behavioral data that implements a form of bayesian belief update over the course of the experiment .

multilingual propbank annotation tools :
this paper demonstrates two annotation tools related to propbank : cornerstone and jubilee . propbank is a corpus in which the arguments of each verb predicate are annotated with their semantic roles . propbank annotation also requires the choice of a sense id for each predicate , defined in the corresponding frameset file . jubilee expedites the annotation process by displaying several resources of syntactic and semantic information simultaneously ; easy access to each of these resources allows the annotator to quickly absorb and apply the necessary syntactic and semantic information pertinent to each predicate for consistent and efficient annotation . cornerstone is a user-friendly xml editor , customized to allow frame authors to create and edit frameset files . both tools have been successfully adapted to many propbank projects ; they run platform independently , are light enough to run as x11 applications and support multiple languages such as arabic , chinese , english , hindi and korean .

a machine learning based approach to evaluating retrieval systems
test collections are essential to evaluate information retrieval ( ir ) systems . the relevance assessment set has been recognized as the key bottleneck in test collection building , especially on very large sized document collections . this paper addresses the problem of efficiently selecting documents to be included in the assessment set . we will show how machine learning techniques can fit this task . this leads to smaller pools than traditional round robin pooling , thus reduces significantly the manual assessment workload . experimental results on trec collections1 consistently demonstrate the effectiveness of our approach according to different evaluation criteria .

learning to lemmatise polish noun phrases
we present a novel approach to noun phrase lemmatisation where the main phase is cast as a tagging problem . the idea draws on the observation that the lemmatisation of almost all polish noun phrases may be decomposed into transformation of singular words ( tokens ) that make up each phrase . we perform evaluation , which shows results similar to those obtained earlier by a rule-based system , while our approach allows to separate chunking from lemmatisation .

bioar : anaphora resolution for relating protein names
the need for associating , or grounding , protein names in the literature with the entries of proteome databases such as swiss-prot is well-recognized . the protein names in the biomedical literature show a high degree of morphological and syntactic variations , and various anaphoric expressions including null anaphors . we present a biomedical anaphora resolution system , bioar , in order to address the variations of protein names and to further associate them with swiss-prot entries as the actual entities in the world . the system shows the performance of 59.5 % 75.0 % precision and 40.7 % 56.3 % recall , depending on the specific types of anaphoric expressions . we apply bioar to the protein names in the biological interactions as extracted by our biomedical information extraction system , or bioie , in order to construct protein pathways automatically .

unsupervised learning of selectional restrictions and detection of
metonymic language is a pervasive phenomenon . metonymic type shifting , or argument type coercion , results in a selectional restriction violation where the arguments semantic class differs from the class the predicate expects . in this paper we present an unsupervised method that learns the selectional restriction of arguments and enables the detection of argument coercion . this method also generates an enhanced probabilistic resolution of logical metonymies . the experimental results indicate substantial improvements the detection of coercions and the ranking of metonymic interpretations .

improving mt word alignment using aligned multi-stage parses
we use hand-coded rules and graph-aligned logical dependencies to reorder english text towards chinese word order . we obtain a 1.5 % higher f-score for giza++ compared to running with unprocessed text . we describe this research and its implications for smt .

network analysis of korean word associations
korean word associations ( korwa ) were collected to build a semantic network for the korean language . a graphic representation approach of applying coefficients to complex networks allows us to discern the semantic structures within words . a semantic network of the korwa was found to exhibit the scalefree property in its degree distribution . the growth of the network around hub words was also confirmed through two experimental phases . as an issue for further research , we suggest that the present results may yield insights for computational neurolinguistics , as a semantic network of word association norms can bridge the gap between information about lexical co-occurrences derived from a corpora and anatomical networks as a basis for mapping out neural activations .

filling the gap : semi-supervised learning for opinion detection across domains
we investigate the use of semi-supervised learning ( ssl ) in opinion detection both in sparse data situations and for domain adaptation . we show that co-training reaches the best results in an in-domain setting with small labeled data sets , with a maximum absolute gain of 33.5 % . for domain transfer , we show that self-training gains an absolute improvement in labeling accuracy for blog data of 16 % over the supervised approach with target domain training data .

automatic detection and correction of errors in dependency treebanks
annotated corpora are essential for almost all nlp applications . whereas they are expected to be of a very high quality because of their importance for the followup developments , they still contain a considerable number of errors . with this work we want to draw attention to this fact . additionally , we try to estimate the amount of errors and propose a method for their automatic correction . whereas our approach is able to find only a portion of the errors that we suppose are contained in almost any annotated corpus due to the nature of the process of its creation , it has a very high precision , and thus is in any case beneficial for the quality of the corpus it is applied to . at last , we compare it to a different method for error detection in treebanks and find out that the errors that we are able to detect are mostly different and that our approaches are complementary .

first joint workshop on statistical parsing of morphologically rich languages parsing german : how much morphology do we need
we investigate how the granularity of pos tags influences pos tagging , and furthermore , how pos tagging performance relates to parsing results . for this , we use the standard pipeline approach , in which a parser builds its output on previously tagged input . the experiments are performed on two german treebanks , using three pos tagsets of different granularity , and six different pos taggers , together with the berkeley parser . our findings show that less granularity of the pos tagset leads to better tagging results . however , both too coarse-grained and too fine-grained distinctions on pos level decrease parsing performance .

a fast method for parallel document identification
we present a fast method to identify homogeneous parallel documents . the method is based on collecting counts of identical low-frequency words between possibly parallel documents . the candidate with the most shared low-frequency words is selected as the parallel document . the method achieved 99.96 % accuracy when tested on the europarl corpus of parliamentary proceedings , failing only in anomalous cases of truncated or otherwise distorted documents . while other work has shown similar performance on this type of dataset , our approach presented here is faster and does not require training . apart from proposing an efficient method for parallel document identification in a restricted domain , this paper furnishes evidence that parliamentary proceedings may be inappropriate for testing parallel document identification systems in general .

simple supervised document geolocation with geodesic grids
we investigate automatic geolocation ( i.e . identification of the location , expressed as latitude/longitude coordinates ) of documents . geolocation can be an effective means of summarizing large document collections and it is an important component of geographic information retrieval . we describe several simple supervised methods for document geolocation using only the documents raw text as evidence . all of our methods predict locations in the context of geodesic grids of varying degrees of resolution . we evaluate the methods on geotagged wikipedia articles and twitter feeds . for wikipedia , our best method obtains a median prediction error of just 11.8 kilometers . twitter geolocation is more challenging : we obtain a median error of 479 km , an improvement on previous results for the dataset .

from relevance rankings for cross-language retrieval
we present an approach to cross-language retrieval that combines dense knowledgebased features and sparse word translations . both feature types are learned directly from relevance rankings of bilingual documents in a pairwise ranking framework . in large-scale experiments for patent prior art search and cross-lingual retrieval in wikipedia , our approach yields considerable improvements over learningto-rank with either only dense or only sparse features , and over very competitive baselines that combine state-of-the-art machine translation and retrieval .

evaluating cross-language annotation transfer in the multisemcor corpus
in this paper we illustrate and evaluate an approach to the creation of high quality linguistically annotated resources based on the exploitation of aligned parallel corpora . this approach is based on the assumption that if a text in one language has been annotated and its translation has not , annotations can be transferred from the source text to the target using word alignment as a bridge . the transfer approach has been tested in the creation of the multisemcor corpus , an english/italian parallel corpus created on the basis of the english semcor corpus . in multisemcor texts are aligned at the word level and semantically annotated with a shared inventory of senses . we present some experiments carried out to evaluate the different steps involved in the methodology . the results of the evaluation suggest that the cross-language annotation transfer methodology is a promising solution allowing for the exploitation of existing ( mostly english ) annotated resources to bootstrap the creation of annotated corpora in new ( resourcepoor ) languages with greatly reduced human effort .

structure-preserving pipelines for digital libraries
most existing hlt pipelines assume the input is pure text or , at most , html and either ignore ( logical ) document structure or remove it . we argue that identifying the structure of documents is essential in digital library and other types of applications , and show that it is relatively straightforward to extend existing pipelines to achieve ones in which the structure of a document is preserved .

incremental n-gram approach for language identification in
a multilingual person writing a sentence or a piece of text tends to switch between languages s/he is proficient in . this alteration between languages , commonly known as code-switching , presents us with the problem of determining the correct language of each word in the text . my method uses a variety of techniques based upon the observed differences in the formation of words in these languages . my system was able to obtain third position in both tweet and token level for the main test dataset as well as first position in the token level evaluation for the surprise dataset both consisting of nepali-english codeswitched texts .

dutch word sense disambiguation : optimizing the localness of context
we describe a new version of the dutch word sense disambiguation system trained and tested on a corrected version of the senseval-2 data . the system is an ensemble of word experts ; each word expert is a memory-based classifier of which the parameters are automatically determined through cross-validation on training material . the original best-performing system , which used only local context features for disambiguation , is further refined by performing additional parallel crossvalidation experiments for optimizing algorithmic parameters and the amount of local context available to each of the word experts memory-based kernels . this procedure produces an accuracy of 84.8 % on test material , improving on a baseline score of 77.2 % and the previous senseval-2 score of 84.2 % . we show that cross-validation overfits ; had the local context been held constant at two left and right neighbouring words , the system would have scored 85.0 % .

designing agreement features for realization ranking
this paper shows that incorporating linguistically motivated features to ensure correct animacy and number agreement in an averaged perceptron ranking model for ccg realization helps improve a state-ofthe-art baseline even further . traditionally , these features have been modelled using hard constraints in the grammar . however , given the graded nature of grammaticality judgements in the case of animacy we argue a case for the use of a statistical model to rank competing preferences . though subject-verb agreement is generally viewed to be syntactic in nature , a perusal of relevant examples discussed in the theoretical linguistics literature ( kathol , 1999 ; pollard and sag , 1994 ) points toward the heterogeneous nature of english agreement . compared to writing grammar rules , our method is more robust and allows incorporating information from diverse sources in realization . we also show that the perceptron model can reduce balanced punctuation errors that would otherwise require a post-filter . the full model yields significant improvements in bleu scores on section 23 of the ccgbank and makes many fewer agreement errors .

attribute-based and value-based clustering : an evaluation
in most research on concept acquisition from corpora , concepts are modeled as vectors of relations extracted from syntactic structures . in the case of modifiers , these relations often specify values of attributes , as in ( attr red ) ; this is unlike what typically proposed in theories of knowledge representation , where concepts are typically defined in terms of their attributes ( e.g. , color ) . we compared models of concepts based on values with models based on attributes , using lexical clustering as the basis for comparison . we find that attribute-based models work better than value-based ones , and result in shorter descriptions ; but that mixed models including both the best attributes and the best values work best of all .

using lexico-semantic information for query expansion in passage retrieval for question answering
in this paper we investigate the use of several types of lexico-semantic information for query expansion in the passage retrieval component of our qa system . we have used four corpus-based methods to acquire semantically related words , and we have used one hand-built resource . we evaluate our techniques on the dutch clef qa track.1 in our experiments expansions that try to bridge the terminological gap between question and document collection do not result in any improvements . however , expansions bridging the knowledge gap show modest improvements .

temporal information processing of a new language : fast porting with minimal resources
we describe the semi-automatic adaptation of a timeml annotated corpus from english to portuguese , a language for which timeml annotated data was not available yet . in order to validate this adaptation , we use the obtained data to replicate some results in the literature that used the original english data . the fact that comparable results are obtained indicates that our approach can be used successfully to rapidly create semantically annotated resources for new languages .

the stringnet lexico-grammatical
this demo introduces a suite of web-based english lexical knowledge resources , called stringnet and stringnet navigator , designed to provide access to the immense territory of multiword expressions that falls between what the lexical entries encode in lexicons on the one hand and what productive grammar rules cover on the other . stringnets content consists of 1.6 billion hybrid n-grams , strings in which word forms and parts of speech grams can cooccur . subordinate and super-ordinate relations among hybrid n-grams are indexed , making stringnet a navigable web rather than a list . applications include error detection and correction tools and web browser-based tools that detect patterns in the webpages that a user browses .

classification-based contextual preferences
this paper addresses context matching in textual inference . we formulate the task under the contextual preferences framework which broadly captures contextual aspects of inference . we propose a generic classificationbased scheme under this framework which coherently attends to context matching in inference and may be employed in any inferencebased task . as a test bed for our scheme we use the name-based text categorization ( tc ) task . we define an integration of contextual preferences into the tc setting and present a concrete self-supervised model which instantiates the generic scheme and is applied to address context matching in the tc task . experiments on standard tc datasets show that our approach outperforms the state of the art in context modeling for name-based tc .

translating into morphologically rich languages with synthetic phrases
translation into morphologically rich languages is an important but recalcitrant problem in mt . we present a simple and effective approach that deals with the problem in two phases . first , a discriminative model is learned to predict inflections of target words from rich source-side annotations . then , this model is used to create additional sentencespecific word- and phrase-level translations that are added to a standard translation model as synthetic phrases . our approach relies on morphological analysis of the target language , but we show that an unsupervised bayesian model of morphology can successfully be used in place of a supervised analyzer . we report significant improvements in translation quality when translating from english to russian , hebrew and swahili .

paraphrase alignment for synonym evidence discovery
we describe a new unsupervised approach for synonymy discovery by aligning paraphrases in monolingual domain corpora . for that purpose , we identify phrasal terms that convey most of the concepts within domains and adapt a methodology for the automatic extraction and alignment of paraphrases to identify paraphrase casts from which valid synonyms are discovered . results performed on two different domain corpora show that general synonyms as well as synonymic expressions can be identified with a 67.27 % precision .

mttk : an alignment toolkit for statistical machine translation
the mttk alignment toolkit for statistical machine translation can be used for word , phrase , and sentence alignment of parallel documents . it is designed mainly for building statistical machine translation systems , but can be exploited in other multi-lingual applications . it provides computationally efficient alignment and estimation procedures that can be used for the unsupervised alignment of parallel text collections in a language independent fashion . mttk version 1.0 is available under the open source educational community license .

evaluation in a translation learner corpus
the realisation that fully automatic translation in many settings is still far from producing output that is equal or superior to human translation has lead to an intense interest in translation evaluation in the mt community . however , research in this field , by now , has not only largely ignored the tremendous amount of relevant knowledge available in a closely related discipline , namely translation studies , but also failed to provide a deeper understanding of the nature of '' translation errors '' and '' translation quality '' . this paper presents an empirical take on the latter concept , translation quality , by comparing human and automatic evaluations of learner translations in the kopte corpus . we will show that translation studies provide sophisticated concepts for translation quality estimation and error annotation . moreover , by applying well-established mt evaluation scores , namely bleu and meteor , to kopte learner translations that were graded by a human expert , we hope to shed light on properties ( and potential shortcomings ) of these scores .

towards an annotated corpus of discourse relations in hindi
we describe our initial efforts towards developing a large-scale corpus of hindi texts annotated with discourse relations . adopting the lexically grounded approach of the penn discourse treebank ( pdtb ) , we present a preliminary analysis of discourse connectives in a small corpus . we describe how discourse connectives are represented in the sentence-level dependency annotation in hindi , and discuss how the discourse annotation can enrich this level for research and applications . the ultimate goal of our work is to build a hindi discourse relation bank along the lines of the pdtb . our work will also contribute to the cross-linguistic understanding of discourse connectives .

on large scale graphs using map-reduce
label propagation , a standard algorithm for semi-supervised classification , suffers from scalability issues involving memory and computation when used with largescale graphs from real-world datasets . in this paper we approach label propagation as solution to a system of linear equations which can be implemented as a scalable parallel algorithm using the map-reduce framework . in addition to semi-supervised classification , this approach to label propagation allows us to adapt the algorithm to make it usable for ranking on graphs and derive the theoretical connection between label propagation and pagerank . we provide empirical evidence to that effect using two natural language tasks lexical relatedness and polarity induction . the version of the label propagation algorithm presented here scales linearly in the size of the data with a constant main memory requirement , in contrast to the quadratic cost of both in traditional approaches .

an hmm-based approach to automatic phrasing for mandarin textto-speech synthesis
automatic phrasing is essential to mandarin textto-speech synthesis . we select word format as target linguistic feature and propose an hmmbased approach to this issue . then we define four states of prosodic positions for each word when employing a discrete hidden markov model . the approach achieves high accuracy of roughly 82 % , which is very close to that from manual labeling . our experimental results also demonstrate that this approach has advantages over those part-ofspeech-based ones .

trend survey on japanese natural language processing studies over the last decade
using natural language processing , we carried out a trend survey on japanese natural language processing studies that have been done over the last ten years . we determined the changes in the number of papers published for each research organization and on each research area as well as the relationship between research organizations and research areas . this paper is useful for both recognizing trends in japanese nlp and constructing a method of supporting trend surveys using nlp .

hla hla htay , kavi narayana murthy
in myanmar language , sentences are clearly delimited by a unique sentence boundary marker but are written without necessarily pausing between words with spaces . it is therefore non-trivial to segment sentences into words . word tokenizing plays a vital role in most natural language processing applications . we observe that word boundaries generally align with syllable boundaries . working directly with characters does not help . it is therefore useful to syllabify texts first . syllabification is also a non-trivial task in myanmar . we have collected 4550 syllables from available sources . we have evaluated our syllable inventory on 2,728 sentences spread over 258 pages and observed a coverage of 99.96 % . in the second part , we build word lists from available sources such as dictionaries , through the application of morphological rules , and by generating syllable n-grams as possible words and manually checking .

a logic-based semantic approach to recognizing textual entailment language computer corporation united states of america
this paper proposes a knowledge representation model and a logic proving setting with axioms on demand successfully used for recognizing textual entailments . it also details a lexical inference system which boosts the performance of the deep semantic oriented approach on the rte data . the linear combination of two slightly different logical systems with the third lexical inference system achieves 73.75 % accuracy on the rte 2006 data .

adapting a semantic question answering system to the web intelligent information and communication systems ( iics )
this paper describes how a question answering ( qa ) system developed for smallsized document collections of several million sentences was modified in order to work with a monolingual subset of the web . the basic qa system relies on complete sentence parsing , inferences , and semantic representation matching . the extensions and modifications needed for useful and quick answers from web documents are discussed . the main extension is a two-level approach that first accesses a web search engine and downloads some of its document hits and then works similar to the basic qa system . most modifications are restrictions like a maximal number of documents and a maximal length of investigated document parts ; they ensure acceptable answer times . the resulting web qa system is evaluated on the german test collection from qa @ clef 2004. several parameter settings and strategies for accessing the web search engine are investigated . the main results are : precision-oriented extensions and experimentally derived parameter settings are needed to achieve similar performance on the web as on small-sized document collections that show higher homogeneity and quality of the contained texts ; adapting a semantic qa system to the web is feasible , but answering a question is still expensive in terms of bandwidth and cpu time .

simple effective decipherment via combinatorial optimization
we present a simple objective function that when optimized yields accurate solutions to both decipherment and cognate pair identification problems . the objective simultaneously scores a matching between two alphabets and a matching between two lexicons , each in a different language . we introduce a simple coordinate descent procedure that efficiently finds effective solutions to the resulting combinatorial optimization problem . our system requires only a list of words in both languages as input , yet it competes with and surpasses several state-of-the-art systems that are both substantially more complex and make use of more information .

estimating linear models for compositional distributional semantics
in distributional semantics studies , there is a growing attention in compositionally determining the distributional meaning of word sequences . yet , compositional distributional models depend on a large set of parameters that have not been explored . in this paper we propose a novel approach to estimate parameters for a class of compositional distributional models : the additive models . our approach leverages on two main ideas . firstly , a novel idea for extracting compositional distributional semantics examples . secondly , an estimation method based on regression models for multiple dependent variables . experiments demonstrate that our approach outperforms existing methods for determining a good model for compositional distributional semantics .

a cross-lingual ilp solution to zero anaphora resolution
we present an ilp-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by denis and baldridge ( 2007 ) , but revises it and extends it into a three-way ilp problem also incorporating subject detection . we show that this new model outperforms several baselines and competing models , as well as a direct translation of the denis / baldridge model , for both italian and japanese zero anaphora . we incorporate our model in complete anaphoric resolvers for both italian and japanese , showing that our approach leads to improved performance also when not used in isolation , provided that separate classifiers are used for zeros and for explicitly realized anaphors .

reducing smt rule table with monolingual key phrase
this paper presents an effective approach to discard most entries of the rule table for statistical machine translation . the rule table is filtered by monolingual key phrases , which are extracted from source text using a technique based on term extraction . experiments show that 78 % of the rule table is reduced without worsening translation performance . in most cases , our approach results in measurable improvements in bleu score .

generating student feedback from time-series data using reinforcement
we describe a statistical natural language generation ( nlg ) method for summarisation of time-series data in the context of feedback generation for students . in this paper , we initially present a method for collecting time-series data from students ( e.g . marks , lectures attended ) and use example feedback from lecturers in a datadriven approach to content selection . we show a novel way of constructing a reward function for our reinforcement learning agent that is informed by the lecturers method of providing feedback . we evaluate our system with undergraduate students by comparing it to three baseline systems : a rule-based system , lecturerconstructed summaries and a brute force system . our evaluation shows that the feedback generated by our learning agent is viewed by students to be as good as the feedback from the lecturers . our findings suggest that the learning agent needs to take into account both the student and lecturers preferences .

sub-sentential alignment using substring co-occurrence counts
in this paper , we will present an efficient method to compute the co-occurrence counts of any pair of substring in a parallel corpus , and an algorithm that make use of these counts to create subsentential alignments on such a corpus . this algorithm has the advantage of being as general as possible regarding the segmentation of text .

combination of machine learning methods for optimum chinese word segmentation
this article presents our recent work for participation in the second international chinese word segmentation bakeoff . our system performs two procedures : out-ofvocabulary extraction and word segmentation . we compose three out-of-vocabulary extraction modules : character-based tagging with different classifiers maximum entropy , support vector machines , and conditional random fields . we also compose three word segmentation modules character-based tagging by maximum entropy classifier , maximum entropy markov model , and conditional random fields . all modules are based on previously proposed methods . we submitted three systems which are different combination of the modules .

hierarchy extraction based on inclusion of appearance eiko yamamoto kyoko kanzaki hitoshi isahara
in this paper , we propose a method of automatically extracting word hierarchies based on the inclusion relation of appearance patterns from corpora . we apply a complementary similarity measure to find a hierarchical word structure . this similarity measure was developed for the recognition of degraded machineprinted text in the field and can be applied to estimate one-to-many relations . our purpose is to extract word hierarchies from corpora automatically . as the initial task , we attempt to extract hierarchies of abstract nouns cooccurring with adjectives in japanese and compare with hierarchies in the edr electronic dictionary .

linguistics in computational linguistics :
as my title suggests , this position paper focuses on the relevance of linguistics in nlp instead of asking the inverse question . although the question about the role of computational linguistics in the study of language may theoretically be much more interesting than the selected topic , i feel that my choice is more appropriate for the purpose and context of this workshop . this position paper starts with some retrospective observations clarifying my view on the ambivalent and multi-facetted relationship between linguistics and computational linguistics as it has evolved from both applied and theoretical research on language processing . in four brief points i will then strongly advocate a strengthened relationship from which both sides benefit . first , i will observe that recent developments in both deep linguistic processing and statistical nlp suggest a certain plausible division of labor between the two paradigms . second , i want to propose a systematic approach to research on hybrid systems which determines optimal combinations of the paradigms and continuously monitors the division of labor as both paradigm progress . concrete examples illustrating the proposal are taken from our own research . third , i will argue that a central vision of computational linguistics is still alive , the dream of a formalized reusable linguistic knowledge source embodying the core competence of a language that can be utilized for wide range of applications .

sense and similarity : a study of sense-level similarity measures and torsten zesch
in this paper , we investigate the difference between word and sense similarity measures and present means to convert a state-of-the-art word similarity measure into a sense similarity measure . in order to evaluate the new measure , we create a special sense similarity dataset and re-rate an existing word similarity dataset using two different sense inventories from wordnet and wikipedia . we discover that word-level measures were not able to differentiate between different senses of one word , while sense-level measures actually increase correlation when shifting to sense similarities . sense-level similarity measures improve when evaluated with a re-rated sense-aware gold standard , while correlation with word-level similarity measures decreases .

a semantic scattering model for the automatic interpretation of genitives
this paper addresses the automatic classification of the semantic relations expressed by the english genitives . a learning model is introduced based on the statistical analysis of the distribution of genitives semantic relations on a large corpus . the semantic and contextual features of the genitives noun phrase constituents play a key role in the identification of the semantic relation . the algorithm was tested on a corpus of approximately 2,000 sentences and achieved an accuracy of 79 % , far better than 44 % accuracy obtained with c5.0 , or 43 % obtained with a naive bayes algorithm , or 27 % accuracy with a support vector machines learner on the same corpus .

pos-tag based poetry generation with wordnet
in this paper we present the preliminary work of a basque poetry generation system . basically , we have extracted the pos-tag sequences from some verse corpora and calculated the probability of each sequence . for the generation process we have defined 3 different experiments : based on a strophe from the corpora , we ( a ) replace each word with other according to its pos-tag and suffixes , ( b ) replace each noun and adjective with another equally inflected word and ( c ) replace only nouns with semantically related ones ( inflected ) . finally we evaluate those strategies using a turing test-like evaluation .

inference rules and their application to recognizing textual entailment
in this paper , we explore ways of improving an inference rule collection and its application to the task of recognizing textual entailment . for this purpose , we start with an automatically acquired collection and we propose methods to refine it and obtain more rules using a hand-crafted lexical resource . following this , we derive a dependency-based structure representation from texts , which aims to provide a proper base for the inference rule application . the evaluation of our approach on the recognizing textual entailment data shows promising results on precision and the error analysis suggests possible improvements .

improving lsa-based summarization with anaphora resolution
we propose an approach to summarization exploiting both lexical information and the output of an automatic anaphoric resolver , and using singular value decomposition ( svd ) to identify the main terms . we demonstrate that adding anaphoric information results in significant performance improvements over a previously developed system , in which only lexical terms are used as the input to svd . however , we also show that how anaphoric information is used is crucial : whereas using this information to add new terms does result in improved performance , simple substitution makes the performance worse .

udel : named entity recognition and reference regeneration from surface text
this report describes the methods and results of a system developed for the grec named entity recognition and grec named entity regeneration challenges 2010. we explain our process of automatically annotating surface text , as well as how we use this output to select improved referring expressions for named entities .

generalizing tree transformations for inductive dependency parsing jens nilsson joakim nivre
previous studies in data-driven dependency parsing have shown that tree transformations can improve parsing accuracy for specific parsers and data sets . we investigate to what extent this can be generalized across languages/treebanks and parsers , focusing on pseudo-projective parsing , as a way of capturing non-projective dependencies , and transformations used to facilitate parsing of coordinate structures and verb groups . the results indicate that the beneficial effect of pseudo-projective parsing is independent of parsing strategy but sensitive to language or treebank specific properties . by contrast , the construction specific transformations appear to be more sensitive to parsing strategy but have a constant positive effect over several languages .

cfilt-core : semantic textual similarity using universal networking
this paper describes the system that was submitted in the *sem 2013 semantic textual similarity shared task . the task aims to find the similarity score between a pair of sentences . we describe a universal networking language ( unl ) based semantic extraction system for measuring the semantic similarity . our approach combines syntactic and word level similarity measures along with the unl based semantic similarity measures for finding similarity scores between sentences .

translating unknown words by analogical learning
unknown words are a well-known hindrance to natural language applications . in particular , they drastically impact machine translation quality . an easy way out commercial translation systems usually offer their users is the possibility to add unknown words and their translations into a dedicated lexicon . recently , stroppa and yvon ( 2005 ) have shown how analogical learning alone deals nicely with morphology in different languages . in this study we show that analogical learning offers as well an elegant and effective solution to the problem of identifying potential translations of unknown words .

boosting-based system combination for machine translation
in this paper , we present a simple and effective method to address the issue of how to generate diversified translation systems from a single statistical machine translation ( smt ) engine for system combination . our method is based on the framework of boosting . first , a sequence of weak translation systems is generated from a baseline system in an iterative manner . then , a strong translation system is built from the ensemble of these weak translation systems . to adapt boosting to smt system combination , several key components of the original boosting algorithms are redesigned in this work . we evaluate our method on chinese-to-english machine translation ( mt ) tasks in three baseline systems , including a phrase-based system , a hierarchical phrasebased system and a syntax-based system . the experimental results on three nist evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems .

generating varied narrative probability exercises
this paper presents genpex , a system for automatic generation of narrative probability exercises . generation of exercises in genpex is done in two steps . first , the system creates a specification of a solvable probability problem , based on input from the user ( a researcher or test developer ) who selects a specific question type and a narrative context for the problem . then , a text expressing the probability problem is generated . the user can tune the generated text by setting the values of some linguistic variation parameters . by varying the mathematical content of the exercise , its narrative context and the linguistic parameter settings , many different exercises can be produced . here we focus on the natural language generation part of genpex . after describing how the system works , we briefly present our first evaluation results , and discuss some aspects requiring further investigation .

analysis in twitter
we present two systems developed at the university of ottawa for the semeval 2013 task 2. the first system ( for task a ) classifies the polarity / sentiment orientation of one target word in a twitter message . the second system ( for task b ) classifies the polarity of whole twitter messages . our two systems are very simple , based on supervised classifiers with bag-ofwords feature representation , enriched with information from several sources . we present a few additional results , besides results of the submitted runs .

ngram-based statistical machine translation enhanced with multiple weighted reordering hypotheses
this paper describes the 2007 ngram-based statistical machine translation system developed at the talp research center of the upc ( universitat polite`cnica de catalunya ) in barcelona . emphasis is put on improvements and extensions of the previous years system , being highlighted and empirically compared . mainly , these include a novel word ordering strategy based on : ( 1 ) statistically monotonizing the training source corpus and ( 2 ) a novel reordering approach based on weighted reordering graphs . in addition , this system introduces a target language model based on statistical classes , a feature for out-of-domain units and an improved optimization procedure . the paper provides details of this system participation in the acl 2007 second workshop on statistical machine translation . results on three pairs of languages are reported , namely from spanish , french and german into english ( and the other way round ) for both the in-domain and out-of-domain tasks .

feature weighting for co-occurrence-based classification of words
the paper comparatively studies methods of feature weighting in application to the task of cooccurrence-based classification of words according to their meaning . we explore parameter optimization of several weighting methods frequently used for similar problems such as text classification . we find that successful application of all the methods crucially depends on a number of parameters ; only a carefully chosen weighting procedure allows to obtain consistent improvement on a classifier learned from non-weighted data .

on the subjectivity of human authored short summaries
we address the issue of human subjectivity when authoring summaries , aiming at a simple , robust evaluation of machine generated summaries . applying a cross comprehension test on human authored short summaries from broadcast news , the level of subjectivity is gauged among four authors . the instruction set is simple , thus there is enough room for subjectivity . however the approach is robust because the test does not use the absolute score , relying instead on relative comparison , effectively alleviating the subjectivity . finally we illustrate the application of the above scheme when evaluating the informativeness of machine generated summaries .

xmeant : better semantic mt evaluation without reference translations
we introduce xmeanta new cross-lingual version of the semantic frame based mt evaluation metric meantwhich can correlate even more closely with human adequacy judgments than monolingual meant and eliminates the need for expensive human references . previous work established that meant reflects translation adequacy with state-of-the-art accuracy , and optimizing mt systems against meant robustly improves translation quality . however , to go beyond tuning weights in the loglinear smt model , a cross-lingual objective function that can deeply integrate semantic frame criteria into the mt training pipeline is needed . we show that cross-lingual xmeant outperforms monolingual meant by ( 1 ) replacing the monolingual context vector model in meant with simple translation probabilities , and ( 2 ) incorporating bracketing itg constraints .

toward plot units : automatic affect state analysis amit goyal and ellen riloff and hal daume iii and nathan gilbert
we present a system called aesop that automatically produces affect states associated with characters in a story . this research represents a first step toward the automatic generation of plot unit structures from text . aesop incorporates several existing sentiment analysis tools and lexicons to evaluate the effectiveness of current sentiment technology on this task . aesop also includes two novel components : a method for acquiring patient polarity verbs , which impart negative affect on their patients , and affect projection rules to propagate affect tags from surrounding words onto the characters in the story . we evaluate aesop on a small collection of fables .

learning to model domain-specific utterance sequences for extractive
this paper proposes a novel extractive summarization method for contact center dialogues . we use a particular type of hidden markov model ( hmm ) called class speaker hmm ( cshmm ) , which processes operator/caller utterance sequences of multiple domains simultaneously to model domain-specific utterance sequences and common ( domainwide ) sequences at the same time . we applied the cshmm to call summarization of transcripts in six different contact center domains and found that our method significantly outperforms competitive baselines based on the maximum coverage of important words using integer linear programming .

ends-based dialogue processing
we describe a reusable and scalable dialogue toolbox and its application in multiple systems . our main claim is that ends-based representation and processing throughout the complete dialogue backbone it essential to our approach .

recovering coherent intepretations using semantic integration of partial parses
this paper describes a chunk-based parser/semantic analyzer used by a language learning model . the language learning model requires an analyzer that robustly responds to extragrammaticality , ungrammaticality and other problems associated with transcribed language . the analyzer produces globally coherent analyses by semantically integrating the partial parses . each resulting semantically integrated analysis is ranked by its semantic compatibility using a novel metric called semantic density .

learning to transform linguistic graphs
we argue in favor of the the use of labeled directed graph to represent various types of linguistic structures , and illustrate how this allows one to view nlp tasks as graph transformations . we present a general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method : identification of non-local depenencies ( using penn treebank data ) and semantic role labeling ( using proposition bank data ) .

henry-core : domain adaptation and stacking for text similarity educational testing service
this paper describes a system for automatically measuring the semantic similarity between two texts , which was the aim of the 2013 semantic textual similarity ( sts ) task ( agirre et al , 2013 ) . for the 2012 sts task , heilman and madnani ( 2012 ) submitted the perp system , which performed competitively in relation to other submissions . however , approaches including word and n-gram features also performed well ( bar et al , 2012 ; saric et al , 2012 ) , and the 2013 sts task focused more on predicting similarity for text pairs from new domains . therefore , for the three variations of our system that we were allowed to submit , we used stacking ( wolpert , 1992 ) to combine perp with word and ngram features and applied the domain adaptation approach outlined by daume iii ( 2007 ) to facilitate generalization to new domains . our submissions performed well at most subtasks , particularly at measuring the similarity of news headlines , where one of our submissions ranked 2nd among 89 from 34 teams , but there is still room for improvement .

dependency trees and the strong generative capacity of ccg
we propose a novel algorithm for extracting dependencies from the derivations of a large fragment of ccg . unlike earlier proposals , our dependency structures are always tree-shaped . we then use these dependency trees to compare the strong generative capacities of ccg and tag and obtain surprising results : both formalisms generate the same languages of derivation trees but the mechanisms they use to bring the words in these trees into a linear order are incomparable .

hierarchical a parsing with bridge outside scores
hierarchical a ( ha ) uses of a hierarchy of coarse grammars to speed up parsing without sacrificing optimality . ha prioritizes search in refined grammars using viterbi outside costs computed in coarser grammars . we present bridge hierarchical a ( bha ) , a modified hierarchial a algorithm which computes a novel outside cost called a bridge outside cost . these bridge costs mix finer outside scores with coarser inside scores , and thus constitute tighter heuristics than entirely coarse scores . we show that bha substantially outperforms ha when the hierarchy contains only very coarse grammars , while achieving comparable performance on more refined hierarchies .

a comparison of unsupervised methods for part-of-speech tagging in chinese
we conduct a series of part-of-speech ( pos ) tagging experiments using expectation maximization ( em ) , variational bayes ( vb ) and gibbs sampling ( gs ) against the chinese penn treebank . we want to first establish a baseline for unsupervised pos tagging in chinese , which will facilitate future research in this area . secondly , by comparing and analyzing the results between chinese and english , we highlight some of the strengths and weaknesses of each of the algorithms in pos tagging task and attempt to explain the differences based on some preliminary linguistics analysis . comparing to english , we find that all algorithms perform rather poorly in chinese in 1-to-1 accuracy result but are more competitive in many-to-1 accuracy . we attribute one possible explanation of this to the algorithms inability to correctly produce tags that match the desired tag count distribution .

coarse-to-fine n-best parsing and maxent discriminative reranking
discriminative reranking is one method for constructing high-performance statistical parsers . a discriminative reranker requires a source of candidate parses for each sentence . this paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser . this method generates 50-best lists that are of substantially higher quality than previously obtainable . we used these parses as the input to a maxent reranker that selects the best parse from the set of parses for each sentence , obtaining an f-score of 91.0 % on sentences of length 100 or less .

pos tagging for historical texts with sparse training data
this paper presents a method for part-ofspeech tagging of historical data and evaluates it on texts from different corpora of historical german ( 15th18th century ) . spelling normalization is used to preprocess the texts before applying a pos tagger trained on modern german corpora . using only 250 manually normalized tokens as training data , the tagging accuracy of a manuscript from the 15th century can be raised from 28.65 % to 74.89 % .

the impact of machine translation quality on human post-editing
we investigate the effect of four different competitive machine translation systems on post-editor productivity and behaviour . the study involves four volunteers postediting automatic translations of news stories from english to german . we see significant difference in productivity due to the systems ( about 20 % ) , and even bigger variance between post-editors .

qualitative evaluation of automatically calculated acception
in the context of the papillon project , which aims at creating a multilingual lexical database ( mldb ) , we have developed jeminie , an adaptable system that helps automatically building interlingual lexical databases from existing lexical resources . in this article , we present a taxonomy of criteria for evaluating a mldb , that motivates the need for arbitrary compositions of criteria to evaluate a whole mldb . a quality measurement method is proposed , that is adaptable to different contexts and available lexical resources .

building a biowordnet by using wordnets data formats and wordnets software infrastructure a failure story
in this paper , we describe our efforts to build on wordnet resources , using wordnet lexical data , the data format that it comes with and wordnets software infrastructure in order to generate a biomedical extension of wordnet , the biowordnet . we began our efforts on the assumption that the software resources were stable and reliable . in the course of our work , it turned out that this belief was far too optimistic . we discuss the stumbling blocks that we encountered , point out an error in the wordnet software with implications for research based on it , and conclude that building on the legacy of wordnet data structures and its associated software might preclude sustainable extensions that go beyond the domain of general english .

automatic detection of tags for political blogs khairun-nisa hassanali vasileios hatzivassiloglou
this paper describes a technique for automatically tagging political blog posts using svms and named entity recognition . we compare the quality of the tags detected by this approach to earlier approaches in other domains , observing effects from the political domain and benefits from nlp techniques complementary to the core svm method .

towards a methodology for named entities annotation
today , the named entity recognition task is considered as fundamental , but it involves some specific difficulties in terms of annotation . those issues led us to ask the fundamental question of what the annotators should annotate and , even more important , for which purpose . we thus identify the applications using named entity recognition and , according to the real needs of those applications , we propose to semantically define the elements to annotate . finally , we put forward a number of methodological recommendations to ensure a coherent and reliable annotation scheme .

the triumph of hope over experience
it is remarkable if any relationship at all persists between computational linguists ( cl ) and that part of general linguistics comprising the mainstream of mit transformational-generative ( tg ) theoretical syntax . if the lines are still open , it represents something of a tribute to cl practitioners tolerance a triumph of hope and goodwill over the experience of abuse because the tg community has shown considerable hostility toward cl and everything it stands for over the past fifty years . i offer some brief historical notes , and hint at prospects for a better basis for collaboration in the future .

the rwth aachen german-english machine translation system for
this paper describes the statistical machine translation ( smt ) systems developed at rwth aachen university for the germanenglish translation task of the acl 2014 eighth workshop on statistical machine translation ( wmt 2014 ) . both hierarchical and phrase-based smt systems are applied employing hierarchical phrase reordering and word class language models . for the phrase-based system , we run discriminative phrase training . in addition , we describe our preprocessing pipeline for germanenglish .

alpage : transition-based semantic graph parsing with syntactic univ paris diderot , sorbonne paris cit
this paper describes the systems deployed by the alpage team to participate to the semeval-2014 task on broad-coverage semantic dependency parsing . we developed two transition-based dependency parsers with extended sets of actions to handle non-planar acyclic graphs . for the open track , we worked over two orthogonal axes lexical and syntactic in order to provide our models with lexical and syntactic features such as word clusters , lemmas and tree fragments of different types .

colourful language : measuring wordcolour associations
since many real-world concepts are associated with colour , for example danger with red , linguistic information is often complimented with the use of appropriate colours in information visualization and product marketing . yet , there is no comprehensive resource that captures conceptcolour associations . we present a method to create a large wordcolour association lexicon by crowdsourcing . we focus especially on abstract concepts and emotions to show that even though they can not be physically visualized , they too tend to have strong colour associations . finally , we show how wordcolour associations manifest themselves in language , and quantify usefulness of co-occurrence and polarity cues in automatically detecting colour associations.1

semi-supervised learning for automatic prosodic event detection using co-training algorithm
most of previous approaches to automatic prosodic event detection are based on supervised learning , relying on the availability of a corpus that is annotated with the prosodic labels of interest in order to train the classification models . however , creating such resources is an expensive and time-consuming task . in this paper , we exploit semi-supervised learning with the co-training algorithm for automatic detection of coarse level representation of prosodic events such as pitch accents , intonational phrase boundaries , and break indices . we propose a confidence-based method to assign labels to unlabeled data and demonstrate improved results using this method compared to the widely used agreement-based method . in addition , we examine various informative sample selection methods . in our experiments on the boston university radio news corpus , using only a small amount of the labeled data as the initial training set , our proposed labeling method combined with most confidence sample selection can effectively use unlabeled data to improve performance and finally reach performance closer to that of the supervised method using all the training data .

shallow semantic trees for smt
we present a translation model enriched with shallow syntactic and semantic information about the source language . base-phrase labels and semantic role labels are incorporated into an hierarchical model by creating shallow semantic trees . results show an increase in performance of up to 6 % in bleu scores for english-spanish translation over a standard phrase-based smt baseline .

semantic structure from correspondence analysis
a common problem for clustering techniques is that clusters overlap , which makes graphing the statistical structure in the data difficult . a related problem is that we often want to see the distribution of factors ( variables ) as well as classes ( objects ) . correspondence analysis ( ca ) offers a solution to both these problems . the structure that ca discovers may be an important step in representing similarity . we have performed an analysis for italian verbs and nouns , and confirmed that similar structures are found for english .

with equivalent language model state maintenance
we describe a scalable decoder for parsingbased machine translation . the decoder is written in java and implements all the essential algorithms described in chiang ( 2007 ) : chart-parsing , m-gram language model integration , beam- and cube-pruning , and unique k-best extraction . additionally , parallel and distributed computing techniques are exploited to make it scalable . we also propose an algorithm to maintain equivalent language model states that exploits the back-off property of m-gram language models : instead of maintaining a separate state for each distinguished sequence of state words , we merge multiple states that can be made equivalent for language model probability calculations due to back-off . we demonstrate experimentally that our decoder is more than 30 times faster than a baseline decoder written in python . we propose to release our decoder as an opensource toolkit .

hindi and telugu to english clir using query expansion
this paper presents the experiments of language technologies research centre ( ltrc ) as part of their participation in clef2 2007 indian language to english ad-hoc cross language document retrieval task . in this paper we discuss our hindi and telugu to english clir system and the experiments using clef 2007 dataset . we used a variant of tfidf algorithm in combination with a bilingual lexicon for query translation . we also explored the role of a document summary in fielded queries and two different boolean formulations of query translations . we find that a hybrid boolean formulation using a combination of boolean and and boolean or operators improves ranking of documents . we also find that simple disjunctive combination of translated query keywords results in maximum recall .

collective opinion target extraction in chinese microblogs
microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style . in this paper , we study the problem of extracting opinion targets of chinese microblog messages . such fine-grained word-level task has not been well investigated in microblogs yet . we propose an unsupervised label propagation algorithm to address the problem . the opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets . topics in microblogs are identified by hashtags or using clustering algorithms . experimental results on chinese microblogs show the effectiveness of our framework and algorithms .

online large-margin training for statistical machine translation
we achieved a state of the art performance in statistical machine translation by using a large number of features with an online large-margin training algorithm . the millions of parameters were tuned only on a small development set consisting of less than 1k sentences . experiments on arabic-toenglish translation indicated that a model trained with sparse binary features outperformed a conventional smt system with a small number of features .

studying the history of ideas using topic models
how can the development of ideas in a scientific field be studied over time we apply unsupervised topic modeling to the acl anthology to analyze historical trends in the field of computational linguistics from 1978 to 2006. we induce topic clusters using latent dirichlet allocation , and examine the strength of each topic over time . our methods find trends in the field including the rise of probabilistic methods starting in 1988 , a steady increase in applications , and a sharp decline of research in semantics and understanding between 1978 and 2001 , possibly rising again after 2001. we also introduce a model of the diversity of ideas , topic entropy , using it to show that coling is a more diverse conference than acl , but that both conferences as well as emnlp are becoming broader over time . finally , we apply jensen-shannon divergence of topic distributions to show that all three conferences are converging in the topics they cover .

sentiklue : updating a polarity classifier in 48 hours
sentiklue is an update of the klue polarity classifier which achieved good and robust results in semeval-2013 with a simple feature set implemented in 48 hours .

prior derivation models for formally syntax-based translation using linguistically syntactic parsing and tree kernels
this paper presents an improved formally syntax-based smt model , which is enriched by linguistically syntactic knowledge obtained from statistical constituent parsers . we propose a linguistically-motivated prior derivation model to score hypothesis derivations on top of the baseline model during the translation decoding . moreover , we devise a fast training algorithm to achieve such improved models based on tree kernel methods . experiments on an english-to-chinese task demonstrate that our proposed models outperformed the baseline formally syntaxbased models , while both of them achieved significant improvements over a state-of-theart phrase-based smt system .

archivus : a multimodal system for multimedia meeting browsing and
this paper presents archivus , a multimodal language-enabled meeting browsing and retrieval system . the prototype is in an early stage of development , and we are currently exploring the role of natural language for interacting in this relatively unfamiliar and complex domain . we briefly describe the design and implementation status of the system , and then focus on how this system is used to elicit useful data for supporting hypotheses about multimodal interaction in the domain of meeting retrieval and for developing nlp modules for this specific domain .

finding short definitions of terms on web pages
we present a system that finds short definitions of terms on web pages . it employs a maximum entropy classifier , but it is trained on automatically generated examples ; hence , it is in effect unsupervised . we use rouge-w to generate training examples from encyclopedias and web snippets , a method that outperforms an alternative centroid-based one . after training , our system can be used to find definitions of terms that are not covered by encyclopedias . the system outperforms a comparable publicly available system , as well as a previously published form of our system .

annotation and data mining of the penn discourse treebank
the penn discourse treebank ( pdtb ) is a new resource built on top of the penn wall street journal corpus , in which discourse connectives are annotated along with their arguments . its use of standoff annotation allows integration with a stand-off version of the penn treebank ( syntactic structure ) and propbank ( verbs and their arguments ) , which adds value for both linguistic discovery and discourse modeling . here we describe the pdtb and some experiments in linguistic discovery based on the pdtb alone , as well as on the linked ptb and pdtb corpora .

sentence-level rewriting detection
writers usually need iterations of revisions and edits during their writings . to better understand the process of rewriting , we need to know what has changed between the revisions . prior work mainly focuses on detecting corrections within sentences , which is at the level of words or phrases . this paper proposes to detect revision changes at the sentence level . looking at revisions at a higher level allows us to have a different understanding of the revision process . this paper also proposes an approach to automatically detect sentence revision changes . the proposed approach shows high accuracy in an evaluation using first and final draft essays from an undergraduate writing class .

atypical prosodic structure as an indicator of reading level and text difficulty
automatic assessment of reading ability builds on applying speech recognition tools to oral reading , measuring words correct per minute . this work looks at more fine-grained analysis that accounts for effects of prosodic context using a large corpus of read speech from a literacy study . experiments show that lower-level readers tend to produce relatively more lengthening on words that are not likely to be final in a prosodic phrase , i.e . in less appropriate locations . the results have implications for automatic assessment of text difficulty in that locations of atypical prosodic lengthening are indicative of difficult lexical items and syntactic constructions .

prototype-driven learning for sequence models
we investigate prototype-driven learning for primarily unsupervised sequence modeling . prior knowledge is specified declaratively , by providing a few canonical examples of each target annotation label . this sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model . on part-of-speech induction in english and chinese , as well as an information extraction task , prototype features provide substantial error rate reductions over competitive baselines and outperform previous work . for example , we can achieve an english part-of-speech tagging accuracy of 80.5 % using only three examples of each tag and no dictionary constraints . we also compare to semi-supervised learning and discuss the systems error trends .

automatic coupling of answer extraction and information retrieval
information retrieval ( ir ) and answer extraction are often designed as isolated or loosely connected components in question answering ( qa ) , with repeated overengineering on ir , and not necessarily performance gain for qa . we propose to tightly integrate them by coupling automatically learned features for answer extraction to a shallow-structured ir model . our method is very quick to implement , and significantly improves ir for qa ( measured in mean average precision and mean reciprocal rank ) by 10 % -20 % against an uncoupled retrieval baseline in both document and passage retrieval , which further leads to a downstream 20 % improvement in qa f1 .

multilingual term extraction from domain-specific corpora using morphological structure
morphologically complex terms composed from greek or latin elements are frequent in scientific and technical texts . word forming units are thus relevant cues for the identification of terms in domainspecific texts . this article describes a method for the automatic extraction of terms relying on the detection of classical prefixes and word-initial combining forms . word-forming units are identified using a regular expression . the system then extracts terms by selecting words which either begin or coalesce with these elements . next , terms are grouped in families which are displayed as a weighted list in html format .

two is bigger ( and better ) than one : the wikipedia bitaxonomy project dipartimento di informatica
we present wibi , an approach to the automatic creation of a bitaxonomy for wikipedia , that is , an integrated taxonomy of wikipage pages and categories . we leverage the information available in either one of the taxonomies to reinforce the creation of the other taxonomy . our experiments show higher quality and coverage than state-of-the-art resources like dbpedia , yago , menta , wikinet and wikitaxonomy . wibi is available at http : //wibitaxonomy.org .

grounding spatial named entities for information extraction and question answering
the task of named entity annotation of unseen text has recently been successfully automated with near-human performance . but the full task involves more than annotation , i.e . identifying the scope of each ( continuous ) text span and its class ( such as place name ) . it also involves grounding the named entity ( i.e . establishing its denotation with respect to the world or a model ) . the latter aspect has so far been neglected . in this paper , we show how geo-spatial named entities can be grounded using geographic coordinates , and how the results can be visualized using off-the-shelf software . we use this to compare a textual surrogate of a newspaper story , with a visual surrogate based on geographic coordinates .

co-training for predicting emotions with spoken dialogue data
natural language processing applications often require large amounts of annotated training data , which are expensive to obtain . in this paper we investigate the applicability of co-training to train classifiers that predict emotions in spoken dialogues . in order to do so , we have first applied the wrapper approach with forward selection and nave bayes , to reduce the dimensionality of our feature set . our results show that co-training can be highly effective when a good set of features are chosen .

classifying what-type questions by head noun tagging
classifying what-type questions into proper semantic categories is found more challenging than classifying other types in question answering systems . in this paper , we propose to classify what-type questions by head noun tagging . the approach highlights the role of head nouns as the category discriminator of whattype questions . to reduce the semantic ambiguities of head noun , we integrate local syntactic feature , semantic feature and category dependency among adjacent nouns with conditional random fields ( crfs ) . experiments on standard question classification data set show that the approach achieves state-of-the-art performances .

using mmil for the high level semantic annotation of the lina maria rojas-barahona
the multimodal interface language formalism ( mmil ) has been selected as the high level semantic ( hls ) formalism for annotating the french media dialogue corpus . this corpus is composed of human-machine dialogues in the domain of hotel reservation and tourist information . utterances in dialogues have been previously annotated with a concept-value flat semantics for studying and evaluating spoken language understanding modules in dialogue systems . we are now interested in investigating the use of more complex representations to improve the understanding capability . the mmil intermediate language is a high level semantic formalism that bears relevant linguistic information , from syntax up to discourse . this representation should increase the expressivity of the current annotation though at the expense of the annotation process complexity . in this paper we present our first attempt in defining the annotation guidelines for the hls annotation of the media corpus and its effect on the annotation process itself , revealed by annotators disagreements due to the different levels of hierarchy and the granularity of the features defined in mmil .

resources report on languages of indonesia agency for the assessment and application of technology ( bppt )
in this paper , we report a survey of language resources in indonesia , primarily of indigenous languages . we look at the official indonesian language ( bahasa indonesia ) and 726 regional languages of indonesia ( bahasa nusantara ) and list all the available lrs that we can gathered . this paper suggests that the smaller regional languages may remain relatively unstudied , and unknown , but they are still worthy of our attention . various lrs of these endangered languages are being built and collected by regional language centers for study and its preservation . we will also briefly report its presence on the internet .

learning word-level dialectal variation as phonological replacement rules using a limited parallel corpus
this paper explores two different methods of learning dialectal morphology from a small parallel corpus of standard and dialect-form text , given that a computational description of the standard morphology is available . the goal is to produce a model that translates individual lexical dialectal items to their standard dialect counterparts in order to facilitate dialectal use of available nlp tools that only assume standard-form input . the results show that a learning method based on inductive logic programming quickly converges to the correct model with respect to many phonological and morphological differences that are regular in nature .

minimum bayes risk decoding for bleu human language technology and pattern recognition
we present a minimum bayes risk ( mbr ) decoder for statistical machine translation . the approach aims to minimize the expected loss of translation errors with regard to the bleu score . we show that mbr decoding on n -best lists leads to an improvement of translation quality . we report the performance of the mbr decoder on four different tasks : the tcstar epps spanish-english task 2006 , the nist chinese-english task 2005 and the gale arabic-english and chinese-english task 2006. the absolute improvement of the bleu score is between 0.2 % for the tcstar task and 1.1 % for the gale chineseenglish task .

a deep architecture for semantic parsing
many successful approaches to semantic parsing build on top of the syntactic analysis of text , and make use of distributional representations or statistical models to match parses to ontology-specific queries . this paper presents a novel deep learning architecture which provides a semantic parsing system through the union of two neural models of language semantics . it allows for the generation of ontology-specific queries from natural language statements and questions without the need for parsing , which makes it especially suitable to grammatically malformed or syntactically atypical text , such as tweets , as well as permitting the development of semantic parsers for resourcepoor languages .

discourse structure for context question answering
in a real-world setting , questions are not asked in isolation , but rather in a cohesive manner that involves a sequence of related questions to meet users information needs . the capability to interpret and answer questions based on context is important . in this paper , we discuss the role of discourse modeling in context question answering . in particular , we motivate a semantic-rich discourse representation and discuss the impact of refined discourse structure on question answering .

chinese-uyghur sentence alignment : an approach based on anchor sentences
this paper , which builds on previous studies on sentence alignment , introduces a sentence alignment method in which some sentences are used as anchors and a two step proce-dure is applied . in the first step , some lexical information such as proper names , technical terms , numbers and punctuation marks , loca-tion information and length information are used to generate anchor sentences that satisfy some conditions . in the second step , texts are divided into several segments by using the anchor sentences as boundaries , and then the sentences in each segment are aligned by us-ing a length-based approach . by applying this segmentation technique , the method avoids complex computation and error spreading . experimental results show that the precision of the method is 94.6 % on the average for chinese-uyghur sentence alignment for multi-domain texts .

named entity recognition for indian languages
abstract stub this paper talks about a new approach to recognize named entities for indian languages . phonetic matching technique is used to match the strings of different languages on the basis of their similar sounding property . we have tested our system with a comparable corpus of english and hindi language data . this approach is language independent and requires only a set of rules appropriate for a language .

semantic case role detection for information extraction
if information extraction wants to make its results more accurate , it will have to resort increasingly to a coherent implementation of natural language semantics . in this paper , we will focus on the extraction of semantic case roles from texts . after setting the essential theoretical framework , we will argue that it is possible to detect case roles on the basis of morphosyntactic and lexical surface phenomena . we will give a concise overview of our methodology and of a preliminary test that seems to confirm our hypotheses .

corry : a system for coreference resolution
corry is a system for coreference resolution in english . it supports both local ( soon et al . ( 2001 ) -style ) and global ( integer linear programming , denis and baldridge ( 2007 ) style ) models of coreference . corry relies on a rich linguistically motivated feature set , which has , however , been manually reduced to 64 features for efficiency reasons . three runs have been submitted for the semeval task 1 on coreference resolution ( recasens et al , 2010 ) , optimizing corrys performance for blanc ( recasens and hovy , in prep ) , muc ( vilain et al , 1995 ) and ceaf ( luo , 2005 ) . corry runs have shown the best performance level among all the systems in their track for the corresponding metric .

frame semantic enhancement of lexical-semantic resources
semframe generates framenet-like frames , complete with semantic roles and evoking lexical units . this output can enhance framenet by suggesting new frames , as well as additional lexical units that evoke existing frames . semframe output can also support the addition of frame semantic relationships to wordnet .

how the statistical revolution changes ( computational ) linguistics
this paper discusses some of the ways that the statistical revolution has changed and continues to change the relationship between linguistics and computational linguistics . i claim that it is more useful in parsing to make an open world assumption about possible linguistic structures , rather than the closed world assumption usually made in grammar-based approaches to parsing , and i sketch two different ways in which grammar-based approaches might be modified to achieve this . i also describe some of the ways in which probabilistic models are starting to have a significant impact on psycholinguistics and language acquisition . in language acquisition bayesian techniques may let us empirically evaluate the role of putative universals in universal grammar .

are two heads better than one crowdsourced translation via a
crowdsourcing is a viable mechanism for creating training data for machine translation . it provides a low cost , fast turnaround way of processing large volumes of data . however , when compared to professional translation , naive collection of translations from non-professionals yields low-quality results . careful quality control is necessary for crowdsourcing to work well . in this paper , we examine the challenges of a two-step collaboration process with translation and post-editing by non-professionals . we develop graphbased ranking models that automatically select the best output from multiple redundant versions of translations and edits , and improves translation quality closer to professionals .

prosody-based topic segmentation for mandarin broadcast news
automatic topic segmentation , separation of a discourse stream into its constituent stories or topics , is a necessary preprocessing step for applications such as information retrieval , anaphora resolution , and summarization . while significant progress has been made in this area for text sources and for english audio sources , little work has been done in automatic , acoustic feature-based segmentation of other languages . in this paper , we focus on prosody-based topic segmentation of mandarin chinese . as a tone language , mandarin presents special challenges for applicability of intonation-based techniques , since the pitch contour is also used to establish lexical identity . we demonstrate that intonational cues such as reduction in pitch and intensity at topic boundaries and increase in duration and pause still provide significant contrasts in mandarin chinese . we also build a decision tree classifier that , based only on word and local context prosodic information without reference to term similarity , cue phrase , or sentence-level information , achieves boundary classification accuracy of 89-95.8 % on a large standard test set .

competitive generative models with structure learning for nlp
in this paper we show that generative models are competitive with and sometimes superior to discriminative models , when both kinds of models are allowed to learn structures that are optimal for discrimination . in particular , we compare bayesian networks and conditional loglinear models on two nlp tasks . we observe that when the structure of the generative model encodes very strong independence assumptions ( a la naive bayes ) , a discriminative model is superior , but when the generative model is allowed to weaken these independence assumptions via learning a more complex structure , it can achieve very similar or better performance than a corresponding discriminative model . in addition , as structure learning for generative models is far more efficient , they may be preferable for some tasks .

carsim : a system to visualize written road accident reports as animated
this paper describes a system to create animated 3d scenes of car accidents from reports written in swedish . the system has been developed using news reports of varying size and complexity . the text-to-scene conversion process consists of two stages . an information extraction module creates a structured representation of the accident and a visual simulator generates and animates the scene . we first describe the overall structure of the textto-scene conversion and the structure of the representation . we then explain the information extraction and visualization modules . we show snapshots of the car animation output and we conclude with the results we obtained .

towards interactive text understanding
this position paper argues for an interactive approach to text understanding . the proposed model extends an existing semantics-based text authoring system by using the input text as a source of information to assist the user in re-authoring its content . the approach permits a reliable deep semantic analysis by combining automatic information extraction with a minimal amount of human intervention .

summarizing complex events : a cross-modal solution of storylines
the rapid development of web2.0 leads to significant information redundancy . especially for a complex news event , it is difficult to understand its general idea within a single coherent picture . a complex event often contains branches , intertwining narratives and side news which are all called storylines . in this paper , we propose a novel solution to tackle the challenging problem of storylines extraction and reconstruction . specifically , we first investigate two requisite properties of an ideal storyline . then a unified algorithm is devised to extract all effective storylines by optimizing these properties at the same time . finally , we reconstruct all extracted lines and generate the high-quality story map . experiments on real-world datasets show that our method is quite efficient and highly competitive , which can bring about quicker , clearer and deeper comprehension to readers .

automatic generation of translation dictionaries using intermediary
we describe a method which uses one or more intermediary languages in order to automatically generate translation dictionaries . such a method could potentially be used to efficiently create translation dictionaries for language groups which have as yet had little interaction . for any given word in the source language , our method involves first translating into the intermediary language ( s ) , then into the target language , back into the intermediary language ( s ) and finally back into the source language . the relationship between a word and the number of possible translations in another language is most often 1-to-many , and so at each stage , the number of possible translations grows exponentially . if we arrive back at the same starting point i.e . the same word in the source language , then we hypothesise that the meanings of the words in the chain have not diverged significantly . hence we backtrack through the link structure to the target language word and accept this as a suitable translation . we have tested our method by using english as an intermediary language to automatically generate a spanish-to-germandictionary , and the results are encouraging .

phrasefix : statistical post-editing of tectomt
we present two english-to-czech systems that took part in the wmt 2013 shared task : tectomt and phrasefix . the former is a deep-syntactic transfer-based system , the latter is a more-or-less standard statistical post-editing ( spe ) applied on top of tectomt . in a brief survey , we put spe in context with other system combination techniques and evaluate spe vs. another simple system combination technique : using synthetic parallel data from tectomt to train a statistical mt system ( smt ) . we confirm that phrasefix ( spe ) improves the output of tectomt , and we use this to analyze errors in tectomt . however , we also show that extending data for smt is more effective .

dialogue management based on entities and constraints
this paper introduces a new dialogue management framework for goal-directed conversations . a declarative specification defines the domain-specific elements and guides the dialogue manager , which communicates with the knowledge sources to complete the specified goal . the user is viewed as another knowledge source . the dialogue manager finds the next action by a mixture of rule-based reasoning and a simple statistical model . implementation in the flight-reservation domain demonstrates that the framework enables the developer to easily build a conversational dialogue system .

better word alignments with supervised itg models
this work investigates supervised word alignment methods that exploit inversion transduction grammar ( itg ) constraints . we consider maximum margin and conditional likelihood objectives , including the presentation of a new normal form grammar for canonicalizing derivations . even for non-itg sentence pairs , we show that it is possible learn itg alignment models by simple relaxations of structured discriminative learning objectives . for efficiency , we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext cky parsing . finally , we introduce many-to-one block alignment features , which significantly improve our itg models . altogether , our method results in the best reported aer numbers for chinese-english and a performance improvement of 1.1 bleu over giza++ alignments .

a rudimentary lexicon and semantics help bootstrap phoneme abdellah fourtassi emmanuel dupoux
infants spontaneously discover the relevant phonemes of their language without any direct supervision . this acquisition is puzzling because it seems to require the availability of high levels of linguistic structures ( lexicon , semantics ) , that logically suppose the infants having a set of phonemes already . we show how this circularity can be broken by testing , in realsize language corpora , a scenario whereby infants would learn approximate representations at all levels , and then refine them in a mutually constraining way . we start with corpora of spontaneous speech that have been encoded in a varying number of detailed context-dependent allophones . we derive , in an unsupervised way , an approximate lexicon and a rudimentary semantic representation . despite the fact that all these representations are poor approximations of the ground truth , they help reorganize the fine grained categories into phoneme-like categories with a high degree of accuracy . one of the most fascinating facts about human infants is the speed at which they acquire their native language . during the first year alone , i.e. , before they are able to speak , infants achieve impressive landmarks regarding three key language components . first , they tune in on the phonemic categories of their language ( werker and tees , 1984 ) . second , they learn to segment the continuous speech stream into discrete units ( jusczyk and aslin , 1995 ) .

the design of a proofreading software service
web applications have the opportunity to check spelling , style , and grammar using a software service architecture . a software service authoring aid can offer contextual spell checking , detect real word errors , and avoid poor grammar checker suggestions through the use of large language models . here we present after the deadline , an open source authoring aid , used in production on wordpress.com , a blogging platform with over ten million writers . we discuss the benefits of the software service environment and how it affected our choice of algorithms . we summarize our design principles as speed over accuracy , simplicity over complexity , and do what works .

untangling the cross-lingual link structure of wikipedia
wikipedia articles in different languages are connected by interwiki links that are increasingly being recognized as a valuable source of cross-lingual information . unfortunately , large numbers of links are imprecise or simply wrong . in this paper , techniques to detect such problems are identified . we formalize their removal as an optimization task based on graph repair operations . we then present an algorithm with provable properties that uses linear programming and a region growing technique to tackle this challenge . this allows us to transform wikipedia into a much more consistent multilingual register of the worlds entities and concepts .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

toward a psycholinguistically-motivated model of language processing
psycholinguistic studies suggest a model of human language processing that 1 ) performs incremental interpretation of spoken utterances or written text , 2 ) preserves ambiguity by maintaining competing analyses in parallel , and 3 ) operates within a severely constrained short-term memory store possibly constrained to as few as four distinct elements . this paper describes a relatively simple model of language as a factored statistical time-series process that meets all three of the above desiderata ; and presents corpus evidence that this model is sufficient to parse naturally occurring sentences using human-like bounds on memory .

softmax-margin crfs : training log-linear models with cost functions
we describe a method of incorporating taskspecific cost functions into standard conditional log-likelihood ( cll ) training of linear structured prediction models . recently introduced in the speech recognition community , we describe the method generally for structured models , highlight connections to cll and max-margin learning for structured prediction ( taskar et al , 2003 ) , and show that the method optimizes a bound on risk . the approach is simple , efficient , and easy to implement , requiring very little change to an existing cll implementation . we present experimental results comparing with several commonly-used methods for training structured predictors for named-entity recognition .

learning probabilistic paradigms for morphology in a latent class model
this paper introduces the probabilistic paradigm , a probabilistic , declarative model of morphological structure . we describe an algorithm that recursively applies latent dirichlet allocation with an orthogonality constraint to discover morphological paradigms as the latent classes within a suffix-stem matrix . we apply the algorithm to data preprocessed in several different ways , and show that when suffixes are distinguished for part of speech and allomorphs or gender/conjugational variants are merged , the model is able to correctly learn morphological paradigms for english and spanish . we compare our system with linguistica ( goldsmith 2001 ) , and discuss the advantages of the probabilistic paradigm over linguisticas signature representation .

information extraction using the structured language model
the paper presents a data-driven approach to information extraction ( viewed as template lling ) using the structured language model ( slm ) as a statistical parser . the task of template lling is cast as constrained parsing using the slm . the model is automatically trained from a set of sentences annotated with frame/slot labels and spans . training proceeds in stages : rst a constrained syntactic parser is trained such that the parses on training data meet the speci ed semantic spans , then the non-terminal labels are enriched to contain semantic information and nally a constrained syntactic+semantic parser is trained on the parse trees resulting from the previous stage . despite the small amount of training data used , the model is shown to outperform the slot level accuracy of a simple semantic grammar authored manually for the mipad | personal information management | task .

a generative entity-mention model for linking entities
linking entities with knowledge base ( entity linking ) is a key issue in bridging the textual data with the structural knowledge base . due to the name variation problem and the name ambiguity problem , the entity linking decisions are critically depending on the heterogenous knowledge of entities . in this paper , we propose a generative probabilistic model , called entitymention model , which can leverage heterogenous entity knowledge ( including popularity knowledge , name knowledge and context knowledge ) for the entity linking task . in our model , each name mention to be linked is modeled as a sample generated through a three-step generative story , and the entity knowledge is encoded in the distribution of entities in document p ( e ) , the distribution of possible names of a specific entity p ( s|e ) , and the distribution of possible contexts of a specific entity p ( c|e ) . to find the referent entity of a name mention , our method combines the evidences from all the three distributions p ( e ) , p ( s|e ) and p ( c|e ) . experimental results show that our method can significantly outperform the traditional methods .

translation memory retrieval methods
translation memory ( tm ) systems are one of the most widely used translation technologies . an important part of tm systems is the matching algorithm that determines what translations get retrieved from the bank of available translations to assist the human translator . although detailed accounts of the matching algorithms used in commercial systems cant be found in the literature , it is widely believed that edit distance algorithms are used . this paper investigates and evaluates the use of several matching algorithms , including the edit distance algorithm that is believed to be at the heart of most modern commercial tm systems . this paper presents results showing how well various matching algorithms correlate with human judgments of helpfulness ( collected via crowdsourcing with amazons mechanical turk ) . a new algorithm based on weighted n-gram precision that can be adjusted for translator length preferences consistently returns translations judged to be most helpful by translators for multiple domains and language pairs .

clam : quickly deploy nlp command-line tools on the web
in this paper we present the software clam ; the computational linguistics application mediator . clam is a tool that allows you to quickly and transparently transform command-line nlp tools into fully-fledged restful webservices with which automated clients can communicate , as well as a generic webapplication interface for human end-users .

the upv-unige-ciaosenso wsd system
the ciaosenso wsd system is based on conceptual density , wordnet domains and frequences of wordnet senses . this paper describes the upvunige-ciaosenso wsd system , we participated in the english all-word task with , and its versions used for the english lexical sample and the wordnet gloss disambiguation tasks . in the last an additional goal was to check if the disambiguation of glosses , that has been performed during our tests on the semcor corpus , was done properly or not .

improved decipherment of homophonic ciphers human language technology and pattern recognition
in this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in nuhn et al . ( 2013 ) : an improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the zodiac-408 cipher from several million down to less than one hundred : the search effort is reduced from several hours of computation time to just a few seconds on a single cpu . these improvements allow us to successfully decipher the second part of the famous beale cipher ( see ( ward et al. , 1885 ) and e.g . ( king , 1993 ) ) : having 182 different cipher symbols while having a length of just 762 symbols , the decipherment is way more challenging than the decipherment of the previously deciphered zodiac408 cipher ( length 408 , 54 different symbols ) . to the best of our knowledge , this cipher has not been deciphered automatically before .

elixirfm implementation of functional arabic morphology
functional arabic morphology is a formulation of the arabic inflectional system seeking the working interface between morphology and syntax . elixirfm is its high-level implementation that reuses and extends the functional morphology library for haskell . inflection and derivation are modeled in terms of paradigms , grammatical categories , lexemes and word classes . the computation of analysis or generation is conceptually distinguished from the general-purpose linguistic model . the lexicon of elixirfm is designed with respect to abstraction , yet is no more complicated than printed dictionaries . it is derived from the open-source buckwalter lexicon and is enhanced with information sourcing from the syntactic annotations of the prague arabic dependency treebank .

device-dependent readability for improved text understanding
readability is used to provide users with highquality service in text recommendation or text visualization . with the increasing use of handheld devices , reading device is regarded as an important factor for readability . therefore , this paper investigates the relationship between readability and reading devices such as a smart phone , a tablet , and paper . we suggest readability factors that are strongly related with the readability of a specific device by showing the correlations between various factors in each device and human-rated readability . our experimental results show that each device has its own readability characteristics , and thus different weights should be imposed on readability factors according to the device type . in order to prove the usefulness of the results , we apply the device-dependent readability to news article recommendation .

a scaleable automated quality assurance technique for semantic computational bioscience program computational bioscience program
this paper presents an evaluation of an automated quality assurance technique for a type of semantic representation known as a predicate argument structure . these representations are crucial to the development of an important class of corpus known as a proposition bank . previous work ( cohen and hunter , 2006 ) proposed and tested an analytical technique based on a simple discovery procedure inspired by classic structural linguistic methodology . cohen and hunter applied the technique manually to a small set of representations . here we test the feasibility of automating the technique , as well as the ability of the technique to scale to a set of semantic representations and to a corpus many times larger than that used by cohen and hunter . we conclude that the technique is completely automatable , uncovers missing sense distinctions and other bad semantic representations , and does scale well , performing at an accuracy of 69 % for identifying bad representations . we also report on the implications of our findings for the correctness of the semantic representations in propbank .

fine-grained semantic typing of emerging entities
methods for information extraction ( ie ) and knowledge base ( kb ) construction have been intensively studied . however , a largely under-explored case is tapping into highly dynamic sources like news streams and social media , where new entities are continuously emerging . in this paper , we present a method for discovering and semantically typing newly emerging out-ofkb entities , thus improving the freshness and recall of ontology-based ie and improving the precision and semantic rigor of open ie . our method is based on a probabilistic model that feeds weights into integer linear programs that leverage type signatures of relational phrases and type correlation or disjointness constraints . our experimental evaluation , based on crowdsourced user studies , show our method performing significantly better than prior work .

definition and analysis of intermediate entailment levels
in this paper we define two intermediate models of textual entailment , which correspond to lexical and lexical-syntactic levels of representation . we manually annotated a sample from the rte dataset according to each model , compared the outcome for the two models , and explored how well they approximate the notion of entailment . we show that the lexicalsyntactic model outperforms the lexical model , mainly due to a much lower rate of false-positives , but both models fail to achieve high recall . our analysis also shows that paraphrases stand out as a dominant contributor to the entailment task . we suggest that our models and annotation methods can serve as an evaluation scheme for entailment at these levels .

improving the estimation of word importance for news multi-document
we introduce a supervised model for predicting word importance that incorporates a rich set of features . our model is superior to prior approaches for identifying words used in human summaries . moreover we show that an extractive summarizer using these estimates of word importance is comparable in automatic evaluation with the state-of-the-art .

stacking classifiers for anti-spam filtering of e-mail
we evaluate empirically a scheme for combining classifiers , known as stacked generalization , in the context of anti-spam filtering , a novel cost-sensitive application of text categorization . unsolicited commercial email , or spam , floods mailboxes , causing frustration , wasting bandwidth , and exposing minors to unsuitable content . using a public corpus , we show that stacking can improve the efficiency of automatically induced anti-spam filters , and that such filters can be used in reallife applications .

efficient non-parametric estimation of multiple embeddings per word in vector space
there is rising interest in vector-space word embeddings and their use in nlp , especially given recent methods for their fast estimation at very large scale . nearly all this work , however , assumes a single vector per word typeignoring polysemy and thus jeopardizing their usefulness for downstream tasks . we present an extension to the skip-gram model that efficiently learns multiple embeddings per word type . it differs from recent related work by jointly performing word sense discrimination and embedding learning , by non-parametrically estimating the number of senses per word type , and by its efficiency and scalability . we present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours .

ai-ku : using substitute vectors and co-occurrence modeling for word
word sense induction aims to discover different senses of a word from a corpus by using unsupervised learning approaches . once a sense inventory is obtained for an ambiguous word , word sense discrimination approaches choose the best-fitting single sense for a given context from the induced sense inventory . however , there may not be a clear distinction between one sense and another , although for a context , more than one induced sense can be suitable . graded word sense method allows for labeling a word in more than one sense . in contrast to the most common approach which is to apply clustering or graph partitioning on a representation of first or second order co-occurrences of a word , we propose a system that creates a substitute vector for each target word from the most likely substitutes suggested by a statistical language model . word samples are then taken according to probabilities of these substitutes and the results of the co-occurrence model are clustered . this approach outperforms the other systems on graded word sense induction task in semeval-2013 .

a maximum entropy approach to combining word alignments
this paper presents a new approach to combining outputs of existing word alignment systems . each alignment link is represented with a set of feature functions extracted from linguistic features and input alignments . these features are used as the basis of alignment decisions made by a maximum entropy approach . the learning method has been evaluated on three language pairs , yielding significant improvements over input alignments and three heuristic combination methods . the impact of word alignment on mt quality is investigated , using a phrase-based mt system .

interpreter for highly portable spoken dialogue system
recently the technology for speech recognition and language processing for spoken dialogue systems has been improved , and speech recognition systems and dialogue systems have been developed to the extent of practical usage . in order to become more practical , not only those fundamental techniques but also the techniques of portability and expansibility should be developed . in our previous research , we demonstrated the portability of the speech recognition module to a developed portal spoken dialogue system . and we constructed a dialogue strategy design tool of dialogue script for controlling the dialogue strategy . in this paper , we report a highly portable interpreter using a commercial electronic dictionary . we apply this to three domains/tasks and confirm the validity of the interpreter for each domain/task .

duluth-wsi : senseclusters applied to the
the duluth-wsi systems in semeval-2 built word cooccurrence matrices from the task test data to create a second order cooccurrence representation of those test instances . the senses of words were induced by clustering these instances , where the number of clusters was automatically predicted . the duluth-mix system was a variation of wsi that used the combination of training and test data to create the co-occurrence matrix . the duluth-r system was a series of random baselines .

for automatic evaluation of machine translation lehrstuhl fur informatik vi
evaluation measures for machine translation depend on several common methods , such as preprocessing , tokenization , handling of sentence boundaries , and the choice of a reference length . in this paper , we describe and review some new approaches to them and compare these to state-of-the-art methods . we experimentally look into their impact on four established evaluation measures . for this purpose , we study the correlation between automatic and human evaluation scores on three mt evaluation corpora . these experiments confirm that the tokenization method , the reference length selection scheme , and the use of sentence boundaries we introduce will increase the correlation between automatic and human evaluation scores . we find that ignoring case information and normalizing evaluator scores has a positive effect on the sentence level correlation as well .

a latent variable model for generative dependency parsing
we propose a generative dependency parsing model which uses binary latent variables to induce conditioning features . to define this model we use a recently proposed class of bayesian networks for structured prediction , incremental sigmoid belief networks . we demonstrate that the proposed model achieves state-of-the-art results on three different languages . we also demonstrate that the features induced by the isbns latent variables are crucial to this success , and show that the proposed model is particularly good on long dependencies .

improving probabilistic latent semantic analysis with principal component analysis
probabilistic latent semantic analysis ( plsa ) models have been shown to provide a better model for capturing polysemy and synonymy than latent semantic analysis ( lsa ) . however , the parameters of a plsa model are trained using the expectation maximization ( em ) algorithm , and as a result , the trained model is dependent on the initialization values so that performance can be highly variable . in this paper we present a method for using lsa analysis to initialize a plsa model . we also investigated the performance of our method for the tasks of text segmentation and retrieval on personal-size corpora , and present results demonstrating the efficacy of our proposed approach .

mctest : a challenge dataset for the open-domain machine comprehension of text
we present mctest , a freely available set of stories and associated questions intended for research on the machine comprehension of text . previous work on machine comprehension ( e.g. , semantic modeling ) has made great strides , but primarily focuses either on limited-domain datasets , or on solving a more restricted goal ( e.g. , open-domain relation extraction ) . in contrast , mctest requires machines to answer multiple-choice reading comprehension questions about fictional stories , directly tackling the high-level goal of open-domain machine comprehension . reading comprehension can test advanced abilities such as causal reasoning and understanding the world , yet , by being multiple-choice , still provide a clear metric . by being fictional , the answer typically can be found only in the story itself . the stories and questions are also carefully limited to those a young child would understand , reducing the world knowledge that is required for the task . we present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions . by screening workers ( with grammar tests ) and stories ( with grading ) , we have ensured that the data is the same quality as another set that we manually edited , but at one tenth the editing cost . by being open-domain , yet carefully restricted , we hope mctest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text .

discovering commonsense entailment rules implicit in sentences
reasoning about ordinary human situations and activities requires the availability of diverse types of knowledge , including expectations about the probable results of actions and the lexical entailments for many predicates . we describe initial work to acquire such a collection of conditional ( ifthen ) knowledge by exploiting presuppositional discourse patterns ( such as ones involving but , yet , and hoping to ) and abstracting the matched material into general rules .

incremental semantic construction in a dialogue system
this paper describes recent work on the dyndial project towards incremental semantic interpretation in dialogue . we outline our domain-general grammar-based approach , using a variant of dynamic syntax integrated with type theory with records and a davidsonian event-based semantics . we describe a java-based implementation of the parser , used within the jindigo framework to produce an incremental dialogue system capable of handling inherently incremental phenomena such as split utterances , adjuncts , and mid-sentence clarification requests or backchannels .

evaluation without references : language technology
current metrics for evaluating machine translation quality have the huge drawback that they require human-quality reference translations . we propose a truly automatic evaluation metric based on ibm1 lexicon probabilities which does not need any reference translations . several variants of ibm1 scores are systematically explored in order to find the most promising directions . correlations between the new metrics and human judgments are calculated on the data of the third , fourth and fifth shared tasks of the statistical machine translation workshop . five different european languages are taken into account : english , spanish , french , german and czech . the results show that the ibm1 scores are competitive with the classic evaluation metrics , the most promising being ibm1 scores calculated on morphemes and pos-4grams .

experiments with pos-based restructuring and alignment-based reordering for statistical machine translation
this paper presents the methods which are based on the part-of-speech ( pos ) and auto alignment information to improve the quality of machine translation result and the word alignment . we utilize different types of pos tag to restructure source sentences and use an alignment-based reordering method to improve the alignment . after applying the reordering method , we use two phrase tables in the decoding part to keep the translation performance . our experiments on korean-chinese show that our methods can improve the alignment and translation results . since the proposed approach reduces the size of the phrase table , multi-tables are considered . the combination of all these methods together would get the best translation result .

heterogeneous automatic mt evaluation through non-parametric metric combinations
combining different metrics into a single measure of quality seems the most direct and natural way to improve over the quality of individual metrics . recently , several approaches have been suggested ( kulesza and shieber , 2004 ; liu and gildea , 2007 ; albrecht and hwa , 2007a ) . although based on different assumptions , these approaches share the common characteristic of being parametric . their models involve a number of parameters whose weight must be adjusted . as an alternative , in this work , we study the behaviour of non-parametric schemes , in which metrics are combined without having to adjust their relative importance . besides , rather than limiting to the lexical dimension , we work on a wide set of metrics operating at different linguistic levels ( e.g. , lexical , syntactic and semantic ) . experimental results show that non-parametric methods are a valid means of putting different quality dimensions together , thus tracing a possible path towards heterogeneous automatic mt evaluation .

putting a value on comparable data
machine translation began in 1947 with an influential memo by warren weaver . in that memo , weaver noted that human code-breakers could transform ciphers into natural language ( e.g. , into turkish ) without access to parallel ciphertext/plaintext data , and without knowing the plaintext languages syntax and semantics . simple word- and letter-statistics seemed to be enough for the task . weaver then predicted that such statistical methods could also solve a tougher problem , namely language translation . this raises the question : can sufficient translation knowledge be derived from comparable ( non-parallel ) data in this talk , i will discuss initial work in treating foreign language as a code for english , where we assume the code to involve both word substitutions and word transpositions . in doing so , i will quantitatively estimate the value of non-parallel data , versus parallel data , in terms of end-to-end accuracy of trained translation systems . because we still know very little about solving word-based codes , i will also describe successful techniques and lessons from the realm of letter-based ciphers , where the nonparallel resources are ( 1 ) enciphered text , and ( 2 ) unrelated plaintext . as an example , i will describe how we decoded the copiale cipher with limited computer-like knowledge of the plaintext language . the talk will wrap up with challenges in exploiting comparable data at all levels : letters , words , phrases , syntax , and semantics .

using a mixture of n-best lists from multiple mt systems in rank-sum-based confidence measure for mt outputs
this paper addressees the problem of eliminating unsatisfactory outputs from machine translation ( mt ) systems . the authors intend to eliminate unsatisfactory mt outputs by using confidence measures . confidence measures for mt outputs include the rank-sum-based confidence measure ( rscm ) for statistical machine translation ( smt ) systems . rscm can be applied to non-smt systems but does not always work well on them . this paper proposes an alternative rscm that adopts a mixture of the n-best lists from multiple mt systems instead of a single-systems n-best list in the existing rscm . in most cases , the proposed rscm proved to work better than the existing rscm on two non-smt systems and to work as well as the existing rscm on an smt system .

nlp serving the cause of language learning
e-learning paves the way to a new type of course , more student centred , granulized , on demand , and highly interactive . natural language processing ( nlp ) technologies associated with other multimedia technologies can help to address the major issues raised by this new type of courses : interaction , personalization and reliable information access . this paper presents exills , a true elearning solution which integrates natural language processing tools and virtual reality1 . exills is unique in that , , unlike most of the language learning systems , it focuses on improving learners performance rather than learners competence .

using names and topics for new event detection
new event detection ( ned ) involves monitoring chronologically-ordered news streams to automatically detect the stories that report on new events . we compare two stories by finding three cosine similarities based on names , topics and the full text . these additional comparisons suggest treating the ned problem as a binary classification problem with the comparison scores serving as features . the classifier models we learned show statistically significant improvement over the baseline vector space model system on all the collections we tested , including the latest tdt5 collection . the presence of automatic speech recognizer ( asr ) output of broadcast news in news streams can reduce performance and render our named entity recognition based approaches ineffective . we provide a solution to this problem achieving statistically significant improvements .

story link detection based on dynamic information extending
topic detection and tracking refers to automatic techniques for locating topically related materials in streams of data . as the core technology of it , story link detection is to determine whether two stories are about the same topic . to overcome the limitation of the story length and the topic dynamic evolution problem in data streams , this paper presents a method of applying dynamic information extending to improve the performance of link detection . the proposed method uses previous latest related story to extend current processing story , generates new dynamic models for computing the similarity between the current two stories . the work is evaluated on the tdt4 chinese corpus , and the experimental results indicate that story link detection using this method can make much better performance on all evaluation metrics .

a pronoun anaphora resolution system based on factorial hidden markov models
this paper presents a supervised pronoun anaphora resolution system based on factorial hidden markov models ( fhmms ) . the basic idea is that the hidden states of fhmms are an explicit short-term memory with an antecedent buffer containing recently described referents . thus an observed pronoun can find its antecedent from the hidden buffer , or in terms of a generative model , the entries in the hidden buffer generate the corresponding pronouns . a system implementing this model is evaluated on the ace corpus with promising performance .

effective use of prosody in parsing conversational speech
we identify a set of prosodic cues for parsing conversational speech and show how such features can be effectively incorporated into a statistical parsing model . on the switchboard corpus of conversational speech , the system achieves improved parse accuracy over a state-of-the-art system which uses only lexical and syntactic features . since removal of edit regions is known to improve downstream parse accuracy , we explore alternatives for edit detection and show that pcfgs are not competitive with more specialized techniques .

an approach based on multilingual thesauri and model combination for bilingual lexicon extraction
this paper focuses on exploiting different models and methods in bilingual lexicon extraction , either from parallel or comparable corpora , in specialized domains . first , a special attention is given to the use of multilingual thesauri , and different search strategies based on such thesauri are investigated . then , a method to combine the different models for bilingual lexicon extraction is presented . our results show that the combination of the models significantly improves results , and that the use of the hierarchical information contained in our thesaurus , umls/mesh , is of primary importance . lastly , methods for bilingual terminology extraction and thesaurus enrichment are discussed .

staying on topic : an indicator of power in political debates
we study the topic dynamics of interactions in political debates using the 2012 republican presidential primary debates as data . we show that the tendency of candidates to shift topics changes over the course of the election campaign , and that it is correlated with their relative power . we also show that our topic shift features help predict candidates relative rankings .

information-based machine translation
this paper describes an approach to machine translation that places linguistic information at its foundation . the difficulty of translation from english to japanese is illustrated with data that shows the influence of various linguistic contextual factors . next , a method for natural language transfer is presented that integrates translation examples ( represented as typed feature structures with source-target indices ) with linguistic rules and constraints . the method has been implemented , and the results of an evaluation are presented .

incremental dependency parsing using online learning
we describe an incremental parser that was trained to minimize cost over sentences rather than over individual parsing actions . this is an attempt to use the advantages of the two top-scoring systems in the conll-x shared task . in the evaluation , we present the performance of the parser in the multilingual task , as well as an evaluation of the contribution of bidirectional parsing and beam search to the parsing performance .

markov random topic fields
most approaches to topic modeling assume an independence between documents that is frequently violated . we present an topic model that makes use of one or more user-specified graphs describing relationships between documents . these graph are encoded in the form of a markov random field over topics and serve to encourage related documents to have similar topic structures . experiments on show upwards of a 10 % improvement in modeling performance .

named entity recognition in biomedical texts using an hmm model
although there exists a huge number of biomedical texts online , there is a lack of tools good enough to help people get information or knowledge from them . named entity recognition ( ner ) becomes very important for further processing like information retrieval , information extraction and knowledge discovery . we introduce a hidden markov model ( hmm ) for ner , with a word similarity-based smoothing . our experiment shows that the word similarity-based smoothing can improve the performance by using huge unlabeled data . while many systems have laboriously hand-coded rules for all kinds of word features , we show that word similarity is a potential method to automatically get word formation , prefix , suffix and abbreviation information automatically from biomedical texts , as well as useful word distribution information .

multi-step natural language understanding
while natural language as an interaction modality is increasingly being accepted by users , remaining technological challenges still hinder its widespread employment . tools that better support the design , development and improvement of these types of applications are required . this demo presents a prototyping framework for spoken dialog system ( sds ) design which combines existing language technology components for automatic speech recognition ( asr ) , dialog management ( dm ) , and text-to-speech synthesis ( tts ) with a multi-step component for natural language understanding ( nlu ) .

developing guidelines for the annotation of anaphors in the
this paper describes the ctb coreference annotation guidelines for annotating pronominal anaphoric expressions in the penn chinese treebank . the goals of the annotation are : to provide training data for learning-based pronoun resolution tools , and to provide a \gold '' standard to be used in the evaluation of pronoun resolution algorithms . the choices that were made concerning the coindexing of pronominal anaphors and their antecedents are discussed , as are some questions that arose in trying to categorize those pronominal expressions that did not refer to speci c nominal entities in the text .

supersense tagging of unknown nouns in wordnet
we present a new framework for classifying common nouns that extends namedentity classification . we used a fixed set of 26 semantic labels , which we called supersenses . these are the labels used by lexicographers developing wordnet . this framework has a number of practical advantages . we show how information contained in the dictionary can be used as additional training data that improves accuracy in learning new nouns . we also define a more realistic evaluation procedure than cross-validation .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

subcategorization acquisition and evaluation for chinese verbs
this paper describes the technology and an experiment of subcategorization acquisition for chinese verbs . the scf hypotheses are generated by means of linguistic heuristic information and filtered via statistical methods . evaluation on the acquisition of 20 multi-pattern verbs shows that our experiment achieved the similar precision and recall with former researches . besides , simple application of the acquired lexicon to a pcfg parser indicates great potentialities of subcategorization information in the fields of nlp . credits this research is sponsored by national natural science foundation ( grant no . 60373101 and 603750 19 ) , and high-tech research and development program ( grant no . 2002aa117010-09 ) .

improved modeling of out-of-vocabulary words using morphological
we present a class-based language model that clusters rare words of similar morphology together . the model improves the prediction of words after histories containing outof-vocabulary words . the morphological features used are obtained without the use of labeled data . the perplexity improvement compared to a state of the art kneser-ney model is 4 % overall and 81 % on unknown histories .

a lexical network with a morphological model in it mvs publishing solutions
the french lexical network ( fr-ln ) is a global model of the french lexicon presently under construction . the fr-ln accounts for lexical knowledge as a lexical network structured by paradigmatic and syntagmatic relations holding between lexical units . this paper describes how morphological knowledge is presently being introduced into the fr-ln through the implementation and lexicographic exploitation of a dynamic morphological model . section 1 presents theoretical and practical justifications for the approach which we believe allows for a cognitively sound description of morphological data within semantically-oriented lexical databases . section 2 gives an overview of the structure of the dynamic morphological model , which is constructed through two complementary processes : a morphological processsection 3and a lexicographic processsection 4 .

revisiting word neighborhoods for speech recognition
word neighborhoods have been suggested but not thoroughly explored as an explanatory variable for errors in automatic speech recognition ( asr ) . we revisit the definition of word neighborhoods , propose new measures using a fine-grained articulatory representation of word pronunciations , and consider new neighbor weighting functions . we analyze the significance of our measures as predictors of errors in an isolated-word asr system and a continuous-word asr system . we find that our measures are significantly better predictors of asr errors than previously used neighborhood density measures .

woz simulation of interactive question answering
qaciad ( question answering challenge for information access dialogue ) is an evaluation framework for measuring interactive question answering ( qa ) technologies . it assumes that users interactively collect information using a qa system for writing a report on a given topic and evaluates , among other things , the capabilities needed under such circumstances . this paper reports an experiment for examining the assumptions made by qaciad . in this experiment , dialogues under the situation that qaciad assumes are collected using woz ( wizard of oz ) simulating , which is frequently used for collecting dialogue data for designing speech dialogue systems , and then analyzed . the results indicate that the setting of qaciad is real and appropriate and that one of the important capabilities for future interactive qa systems is providing cooperative and helpful responses .

bioex : a novel user-interface that accesses images from abstract sentences
images ( i.e. , figures or tables ) are important experimental results that are typically reported in bioscience full-text articles . biologists need to access the images to validate research facts and to formulate or to test novel research hypotheses . we designed , evaluated , and implemented a novel user-interface , bioex , that allows biologists to access images that appear in a full-text article directly from the abstract of the article .

sentiment retrieval using generative models
ranking documents or sentences according to both topic and sentiment relevance should serve a critical function in helping users when topics and sentiment polarities of the targeted text are not explicitly given , as is often the case on the web . in this paper , we propose several sentiment information retrieval models in the framework of probabilistic language models , assuming that a user both inputs query terms expressing a certain topic and also specifies a sentiment polarity of interest in some manner . we combine sentiment relevance models and topic relevance models with model parameters estimated from training data , considering the topic dependence of the sentiment . our experiments prove that our models are effective .

improved arabic base phrase chunking with a new enriched pos tag set
base phrase chunking ( bpc ) or shallow syntactic parsing is proving to be a task of interest to many natural language processing applications . in this paper , a bpc system is introduced that improves over state of the art performance in bpc using a new part of speech tag ( pos ) set . the new pos tag set , erts , reflects some of the morphological features specific to modern standard arabic . erts explicitly encodes definiteness , number and gender information increasing the number of tags from 25 in the standard ldc reduced tag set to 75 tags . for the bpc task , we introduce a more language specific set of definitions for the base phrase annotations . we employ a support vector machine approach for both the pos tagging and the bpc processes . the pos tagging performance using this enriched tag set , erts , is at 96.13 % accuracy . in the bpc experiments , we vary the feature set alng two factors : the pos tag set and a set of explicitly encoded morphological features . using the erts pos tagset , bpc achieves the highest overall f=1 of 96.33 % on 10 different chunk types outperforming the use of the standard pos tag set even when explicit morphological features are present .

an experiment setup for collecting data for adaptive output planning in a multimodal dialogue system
we describe a wizard-of-oz experiment setup for the collection of multimodal interaction data for a music player application . this setup was developed and used to collect experimental data as part of a project aimed at building a flexible multimodal dialogue system which provides an interface to an mp3 player , combining speech and screen input and output . besides the usual goal of woz data collection to get realistic examples of the behavior and expectations of the users , an equally important goal for us was to observe natural behavior of multiple wizards in order to guide our system development . the wizards responses were therefore not constrained by a script . one of the challenges we had to address was to allow the wizards to produce varied screen output a in real time . our setup includes a preliminary screen output planning module , which prepares several versions of possible screen output . the wizards were free to speak , and/or to select a screen output .

the szeged corpus : a pos tagged and syntactically annotated hungarian natural language corpus
the szeged corpus is a manually annotated natural language corpus currently comprising 1.2 million word entries , 145 thousand different word forms , and an additional 225 thousand punctuation marks . with this , it is the largest manually processed hungarian textual database that serves as a reference material for research in natural language processing as well as a learning database for machine learning algorithms and other software applications . language processing of the corpus texts so far included morpho-syntactic analysis , pos tagging and shallow syntactic parsing . semantic information was also added to a preselected section of the corpus to support automated information extraction . the present state of the szeged corpus is the result of three national projects and the cooperation of the university of szeged , department of informatics , morphologic ltd. budapest , and the research institute for linguistics at the hungarian academy of sciences . corpus texts have gone through different phases of natural language processing ( nlp ) and analysis . extensive and accurate manual annotation of the texts , incorporating over 124 person-months of manual work , is a great value of the corpus .

an entity-level approach to information extraction
we present a generative model of template-filling in which coreference resolution and role assignment are jointly determined . underlying template roles first generate abstract entities , which in turn generate concrete textual mentions . on the standard corporate acquisitions dataset , joint resolution in our entity-level model reduces error over a mention-level discriminative approach by up to 20 % .

evaluating contribution of deep syntactic information to shallow semantic analysis sumire uematsu junichi tsujii
this paper presents shallow semantic parsing based only on hpsg parses . an hpsg-framenet map was constructed from a semantically annotated corpus , and semantic parsing was performed by mapping hpsg dependencies to framenet relations . the semantic parsing was evaluated in a senseval-3 task ; the results suggested that there is a high contribution of syntactic information to semantic analysis .

an effective two-stage model for exploiting non-local dependencies in named entity recognition
this paper shows that a simple two-stage approach to handle non-local dependencies in named entity recognition ( ner ) can outperform existing approaches that handle non-local dependencies , while being much more computationally efficient . ner systems typically use sequence models for tractable inference , but this makes them unable to capture the long distance structure present in text . we use a conditional random field ( crf ) based ner system using local features to make predictions and then train another crf which uses both local information and features extracted from the output of the first crf . using features capturing non-local dependencies from the same document , our approach yields a 12.6 % relative error reduction on the f1 score , over state-of-theart ner systems using local-information alone , when compared to the 9.3 % relative error reduction offered by the best systems that exploit non-local information . our approach also makes it easy to incorporate non-local information from other documents in the test corpus , and this gives us a 13.3 % error reduction over ner systems using local-information alone . additionally , our running time for inference is just the inference time of two sequential crfs , which is much less than that of other more complicated approaches that directly model the dependencies and do approximate inference .

automatic set expansion for list question answering
this paper explores the use of set expansion ( se ) to improve question answering ( qa ) when the expected answer is a list of entities belonging to a certain class . given a small set of seeds , se algorithms mine textual resources to produce an extended list including additional members of the class represented by the seeds . we explore the hypothesis that a noise-resistant se algorithm can be used to extend candidate answers produced by a qa system and generate a new list of answers that is better than the original list produced by the qa system . we further introduce a hybrid approach which combines the original answers from the qa system with the output from the se algorithm . experimental results for several state-of-the-art qa systems show that the hybrid system performs better than the qa systems alone when tested on list question data from past trec evaluations .

chained machine translation using morphemes as pivot language and technology of china
as the smallest meaning-bearing elements of the languages which have rich morphology information , morphemes are often integrated into state-of-the-art statistical machine translation to improve translation quality . the paper proposes an approach which novelly uses morphemes as pivot language in a chained machine translation system . a machine translation based method is used therein to find the mapping relations between morphemes and words . experiments show the effectiveness of our approach , achieving 18.6 percent increase in bleu score over the baseline phrase-based machine translation system .

the parallel grammar project
we report on the parallel grammar ( pargram ) project which uses the xle parser and grammar development platform for six languages : english , french , german , japanese , norwegian , and urdu.1

gennext : a consolidated domain adaptable nlg system
we introduce gennext , an nlg system designed specifically to adapt quickly and easily to different domains . given a domain corpus of historical texts , gennext allows the user to generate a template bank organized by semantic concept via derived discourse representation structures in conjunction with general and domain-specific entity tags . based on various features collected from the training corpus , the system statistically learns template representations and document structure and produces wellformed texts ( as evaluated by crowdsourced and expert evaluations ) . in addition to domain adaptation , gennexts hybrid approach significantly reduces complexity as compared to traditional nlg systems by relying on templates ( consolidating micro-planning and surface realization ) and minimizing the need for domain experts . in this description , we provide details of gennexts theoretical perspective , architecture and evaluations of output .

multiword lexical acquisition and dictionary formalization
in this paper , we present the current state of development of a large-scale lexicon built at label1 for portuguese . we will concentrate on multiword expressions ( mwe ) , particularly on multiword nouns , ( i ) illustrating their most relevant morphological features , and ( ii ) pointing out the methods and techniques adopted to generate the inflected forms from lemmas . moreover , we describe a corpus-based aproach for the acquisition of new multiword nouns , which led to a significant enlargement of the existing lexicon . evaluation results concerning lexical coverage in the corpus are also discussed .

rule and tree ensembles for unrestricted coreference resolution cicero nogueira dos santos informatica aplicada ppgia davi lopes carvalho informatica aplicada ppgia
in this paper , we describe a machine learning system based on rule and tree ensembles for unrestricted coreference resolution . we use entropy guided transformation learning ( etl ) and decision trees as the base learners , and , respectively , etl committee and random forest as ensemble algorithms . our system is evaluated on the closed track of the conll 2011 shared task : modeling unrestricted coreference in ontonotes . a preliminary version of our system achieves the 6th best score out of 21 competitors in the conll 2011 shared task . here , we depict the system architecture and our experimental results and findings .

an efficient indexer for large n-gram corpora
we introduce a new publicly available tool that implements efficient indexing and retrieval of large n-gram datasets , such as the web1t 5-gram corpus . our tool indexes the entire web1t dataset with an index size of only 100 mb and performs a retrieval of any n-gram with a single disk access . with an increased index size of 420 mb and duplicate data , it also allows users to issue wild card queries provided that the wild cards in the query are contiguous . furthermore , we also implement some of the smoothing algorithms that are designed specifically for large datasets and are shown to yield better language models than the traditional ones on the web1t 5gram corpus ( yuret , 2008 ) . we demonstrate the effectiveness of our tool and the smoothing algorithms on the english lexical substitution task by a simple implementation that gives considerable improvement over a basic language model .

investigating loss functions and optimization methods for discriminative
discriminative models have been of interest in the nlp community in recent years . previous research has shown that they are advantageous over generative models . in this paper , we investigate how different objective functions and optimization methods affect the performance of the classifiers in the discriminative learning framework . we focus on the sequence labelling problem , particularly pos tagging and ner tasks . our experiments show that changing the objective function is not as effective as changing the features included in the model .

conceptual structuring through term variations
term extraction systems are now an integral part of the compiling of specialized dictionaries and updating of term banks . in this paper , we present a term detection approach that discovers , structures , and infers conceptual relationships between terms for french . conceptual relationships are deduced from specific types of term variations , morphological and syntagmatic , and are expressed through lexical functions . the linguistic precision of the conceptual structuring through morphological variations is of 95 % .

validation and evaluation of automatically acquired multiword expressions for grammar engineering
this paper focuses on the evaluation of methods for the automatic acquisition of multiword expressions ( mwes ) for robust grammar engineering . first we investigate the hypothesis that mwes can be detected by the distinct statistical properties of their component words , regardless of their type , comparing 3 statistical measures : mutual information ( mi ) , 2 and permutation entropy ( pe ) . our overall conclusion is that at least two measures , mi and pe , seem to differentiate mwes from non-mwes . we then investigate the influence of the size and quality of different corpora , using the bnc and the web search engines google and yahoo . we conclude that , in terms of language usage , web generated corpora are fairly similar to more carefully built corpora , like the bnc , indicating that the lack of control and balance of these corpora are probably compensated by their size . finally , we show a qualitative evaluation of the results of automatically adding extracted mwes to existing linguistic resources . we argue that such a process improves qualitatively , if a more compositional approach to grammar/lexicon automated extension is adopted .

branch and bound algorithm for dependency parsing with non-local features
graph based dependency parsing is inefficient when handling non-local features due to high computational complexity of inference . in this paper , we proposed an exact and efficient decoding algorithm based on the branch and bound ( b & b ) framework where nonlocal features are bounded by a linear combination of local features . dynamic programming is used to search the upper bound . experiments are conducted on english ptb and chinese ctb datasets . we achieved competitive unlabeled attachment score ( uas ) when no additional resources are available : 93.17 % for english and 87.25 % for chinese . parsing speed is 177 words per second for english and 97 words per second for chinese . our algorithm is general and can be adapted to nonprojective dependency parsing or other graphical models .

the life and death of discourse entities : identifying singleton mentions
a discourse typically involves numerous entities , but few are mentioned more than once . distinguishing discourse entities that die out after just one mention ( singletons ) from those that lead longer lives ( coreferent ) would benefit nlp applications such as coreference resolution , protagonist identification , topic modeling , and discourse coherence . we build a logistic regression model for predicting the singleton/coreferent distinction , drawing on linguistic insights about how discourse entity lifespans are affected by syntactic and semantic features . the model is effective in its own right ( 78 % accuracy ) , and incorporating it into a state-of-the-art coreference resolution system yields a significant improvement .

unknown word sense detection as outlier detection
we address the problem of unknown word sense detection : the identification of corpus occurrences that are not covered by a given sense inventory . we model this as an instance of outlier detection , using a simple nearest neighbor-based approach to measuring the resemblance of a new item to a training set . in combination with a method that alleviates data sparseness by sharing training data across lemmas , the approach achieves a precision of 0.77 and recall of 0.82 .

on the equivalence of weighted finite-state transducers
although they can be topologically different , two distinct transducers may actually recognize the same rational relation . being able to test the equivalence of transducers allows to implement such operations as incremental minimization and iterative composition . this paper presents an algorithm for testing the equivalence of deterministic weighted finite-state transducers , and outlines an implementation of its applications in a prototype weighted finite-state calculus tool .

the effect of higher-order dependency features in discriminative
higher-order dependency features are known to improve dependency parser accuracy . we investigate the incorporation of such features into a cube decoding phrase-structure parser . we find considerable gains in accuracy on the range of standard metrics . what is especially interesting is that we find strong , statistically significant gains on dependency recovery on out-of-domain tests ( brown vs. wsj ) . this suggests that higher-order dependency features are not simply overfitting the training material .

application of localized similarity for web documents
in this paper we present a novel approach to automatic creation of anchor texts for hyperlinks in a document pointing to similar documents . methods used in this approach rank parts of a document based on the similarity to a presumably related document . ranks are then used to automatically construct the best anchor text for a link inside original document to the compared document . a number of different methods from information retrieval and natural language processing are adapted for this task . automatically constructed anchor texts are manually evaluated in terms of relatedness to linked documents and compared to baseline consisting of originally inserted anchor texts . additionally we use crowdsourcing for evaluation of original anchors and automatically constructed anchors . results show that our best adapted methods rival the precision of the baseline method .

a graph approach to spelling correction in domain-centric search
spelling correction for keyword-search queries is challenging in restricted domains such as personal email ( or desktop ) search , due to the scarcity of query logs , and due to the specialized nature of the domain . for that task , this paper presents an algorithm that is based on statistics from the corpus data ( rather than the query log ) . this algorithm , which employs a simple graph-based approach , can incorporate different types of data sources with different levels of reliability ( e.g. , email subject vs. email body ) , and can handle complex spelling errors like splitting and merging of words . an experimental study shows the superiority of the algorithm over existing alternatives in the email domain .

a graph-based approach for contextual text normalization
the informal nature of social media text renders it very difficult to be automatically processed by natural language processing tools . text normalization , which corresponds to restoring the non-standard words to their canonical forms , provides a solution to this challenge . we introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text . the contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus . the graph encodes the relative positions of the words with respect to each other , as well as their part-ofspeech tags . the lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity . unlike most of the recent approaches that are based on generating normalization dictionaries , the proposed approach performs normalization by considering the context of the non-standard words in the input text . our results show that it achieves state-ofthe-art f-score performance on standard datasets . in addition , the system can be tuned to achieve very high precision without sacrificing much from recall .

hit-cir : an unsupervised wsd system based on domain most frequent sense estimation
this paper presents an unsupervised system for all-word domain specific word sense disambiguation task . this system tags target word with the most frequent sense which is estimated using a thesaurus and the word distribution information in the domain . the thesaurus is automatically constructed from bilingual parallel corpus using paraphrase technique . the recall of this system is 43.5 % on semeval2 task 17 english data set .

integrating uima with alveo , a human communication science virtual andrew mackinlay denis burnham
this paper describes two aspects of alveo , a new virtual laboratory for human communication science ( hcs ) . as a platform for hcs researchers , the integration of the unstructured information management architecture ( uima ) with alveo was one of the aims during the development phase and we report on the choices that were made for the implementation . user acceptance testing ( uat ) constituted an integral part of the development and evolution of alveo and we present the distributed testing organisation , the test development process and the evolution of the tests . we conclude with some lessons learned regarding multi-site collaborative work on the development and deployment of hlt research infrastructure .

shallow semantic parsing using support vector machines
in this paper , we propose a machine learning algorithm for shallow semantic parsing , extending the work of gildea and jurafsky ( 2002 ) , surdeanu et al ( 2003 ) and others . our algorithm is based on support vector machines which we show give an improvement in performance over earlier classifiers . we show performance improvements through a number of new features and measure their ability to generalize to a new test set drawn from the aquaint corpus .

learning hebrew roots : machine learning with linguistic constraints
the morphology of semitic languages is unique in the sense that the major word-formation mechanism is an inherently non-concatenative process of interdigitation , whereby two morphemes , a root and a pattern , are interwoven . identifying the root of a given word in a semitic language is an important task , in some cases a crucial part of morphological analysis . it is also a non-trivial task , which many humans find challenging . we present a machine learning approach to the problem of extracting roots of hebrew words . given the large number of potential roots ( thousands ) , we address the problem as one of combining several classifiers , each predicting the value of one of the roots consonants . we show that when these predictors are combined by enforcing some fairly simple linguistics constraints , high accuracy , which compares favorably with human performance on this task , can be achieved .

latent features in automatic tense translation between chinese
on the task of determining the tense to use when translating a chinese verb into english , current systems do not perform as well as human translators . the main focus of the present paper is to identify features that human translators use , but which are not currently automatically extractable . the goal is twofold : to test a particular hypothesis about what additional information human translators might be using , and as a pilot to determine where to focus effort on developing automatic extraction methods for features that are somewhat beyond the reach of current feature extraction . the paper shows that incorporating several latent features into the tense classifier boosts the tense classifiers performance , and a tense classifier using only the latent features outperforms one using only the surface features . our findings confirm the utility of the latent features in automatic tense classification , explaining the gap between automatic classification systems and the human brain .

finding parallel texts on the web using cross-language information retrieval
discovering parallel corpora on the web is a challenging task . in this paper , we use cross-language information retrieval techniques in combination with structural features to retrieve candidate page pairs from a commercial search engine . the candidate page pairs are then filtered using techniques described by resnik and smith ( 2003 ) to determine if they are translations . the results allow the comparison of efficiency of different parameter settings and provide an estimate for the percentage of pages that are parallel for a certain language pair .

hiddenvariable models for discriminative reranking
we describe a new method for the representation of nlp structures within reranking approaches . we make use of a conditional loglinear model , with hidden variables representing the assignment of lexical items to word clusters or word senses . the model learns to automatically make these assignments based on a discriminative training criterion . training and decoding with the model requires summing over an exponential number of hidden variable assignments : the required summations can be computed efficiently and exactly using dynamic programming . as a case study , we apply the model to parse reranking . the model gives an f measure improvement of 1.25 % beyond the base parser , and an 0.25 % improvement beyond the collins ( 2000 ) reranker . although our experiments are focused on parsing , the techniques described generalize naturally to nlp structures other than parse trees .

brazilian portuguese complex predicates magali sanches duran carlos ramisch sandra maria alusio aline villavicencio
semantic role labeling annotation task depends on the correct identification of predicates , before identifying arguments and assigning them role labels . however , most predicates are not constituted only by a verb : they constitute complex predicates ( cps ) not yet available in a computational lexicon . in order to create a dictionary of cps , this study employs a corpus-based methodology . searches are guided by pos tags instead of a limited list of verbs or nouns , in contrast to similar studies . results include ( but are not limited to ) light and support verb constructions . these cps are classified into idiomatic and less idiomatic . this paper presents an in-depth analysis of this phenomenon , as well as an original resource containing a set of 773 annotated expressions . both constitute an original and rich contribution for nlp tools in brazilian portuguese that perform tasks involving semantics .

re-ranking models based-on small training data for spoken language
the design of practical language applications by means of statistical approaches requires annotated data , which is one of the most critical constraint . this is particularly true for spoken dialog systems since considerably domain-specific conceptual annotation is needed to obtain accurate language understanding models . since data annotation is usually costly , methods to reduce the amount of data are needed . in this paper , we show that better feature representations serve the above purpose and that structure kernels provide the needed improved representation . given the relatively high computational cost of kernel methods , we apply them to just re-rank the list of hypotheses provided by a fast generative model . experiments with support vector machines and different kernels on two different dialog corpora show that our re-ranking models can achieve better results than state-of-the-art approaches when small data is available .

an integrated approach for arabic-english named entity translation hany hassan jeffrey sorensen giza - egypt
translation of named entities ( nes ) , such as person names , organization names and location names is crucial for cross lingual information retrieval , machine translation , and many other natural language processing applications . newly named entities are introduced on daily basis in newswire and this greatly complicates the translation task . also , while some names can be translated , others must be transliterated , and , still , others are mixed . in this paper we introduce an integrated approach for named entity translation deploying phrase-based translation , word-based translation , and transliteration modules into a single framework . while arabic based , the approach introduced here is a unified approach that can be applied to ne translation for any language pair .

an integrated approach to robust processing of situated spoken dialogue
spoken dialogue is notoriously hard to process with standard nlp technologies . natural spoken dialogue is replete with disfluent , partial , elided or ungrammatical utterances , all of which are very hard to accommodate in a dialogue system . furthermore , speech recognition is known to be a highly error-prone task , especially for complex , open-ended discourse domains . the combination of these two problems ill-formed and/or misrecognised speech inputs raises a major challenge to the development of robust dialogue systems . we present an integrated approach for addressing these two issues , based on a incremental parser for combinatory categorial grammar . the parser takes word lattices as input and is able to handle illformed and misrecognised utterances by selectively relaxing its set of grammatical rules . the choice of the most relevant interpretation is then realised via a discriminative model augmented with contextual information . the approach is fully implemented in a dialogue system for autonomous robots . evaluation results on a wizard of oz test suite demonstrate very significant improvements in accuracy and robustness compared to the baseline .

thumbs up or thumbs down semantic orientation applied to unsupervised classification of reviews
this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended ( thumbs up ) or not recommended ( thumbs down ) . the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations ( e.g. , subtle nuances ) and a negative semantic orientation when it has bad associations ( e.g. , very cavalier ) . in this paper , the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word excellent minus the mutual information between the given phrase and the word poor . a review is classified as recommended if the average semantic orientation of its phrases is positive . the algorithm achieves an average accuracy of 74 % when evaluated on 410 reviews from epinions , sampled from four different domains ( reviews of automobiles , banks , movies , and travel destinations ) . the accuracy ranges from 84 % for automobile reviews to 66 % for movie reviews .

a transformational-based learner for dependency grammars
nlp systems will be more portable among medical domains if acquisition of semantic lexicons can be facilitated . we are pursuing lexical acquisition through the syntactic relationships of words in medical corpora . therefore we require a syntactic parser which is flexible , portable , captures head-modifier pairs and does not require a large training set . we have designed a dependency grammar parser that learns through a transformational-based algorithm . we propose a novel design for templates and transformations which capitalize on the dependency structure directly and produces human-readable rules . our parser achieved a 77 % accurate parse training on only 830 sentences . further work will evaluate the usefulness of this parse for lexical acquisition .

exploiting syntactic patterns as clues in zero-anaphora resolution
we approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zeroanaphora resolution . for the former problem , syntactic patterns of the appearance of zero-pronouns and their antecedents are useful clues . taking japanese as a target language , we empirically demonstrate that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora , which consequently improves the overall performance of zeroanaphora resolution .

exploiting shallow linguistic information for relation extraction from biomedical literature
we propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information . we use a combination of kernel functions to integrate two different information sources : ( i ) the whole sentence where the relation appears , and ( ii ) the local contexts around the interacting entities . we performed experiments on extracting gene and protein interactions from two different data sets . the results show that our approach outperforms most of the previous methods based on syntactic and semantic information .

faster phrase-based decoding by refining feature state
we contribute a faster decoding algorithm for phrase-based machine translation . translation hypotheses keep track of state , such as context for the language model and coverage of words in the source sentence . most features depend upon only part of the state , but traditional algorithms , including cube pruning , handle state atomically . for example , cube pruning will repeatedly query the language model with hypotheses that differ only in source coverage , despite the fact that source coverage is irrelevant to the language model . our key contribution avoids this behavior by placing hypotheses into equivalence classes , masking the parts of state that matter least to the score . moreover , we exploit shared words in hypotheses to iteratively refine language model scores rather than handling language model state atomically . since our algorithm and cube pruning are both approximate , improvement can be used to increase speed or accuracy . when tuned to attain the same accuracy , our algorithm is 4.07.7 times as fast as the moses decoder with cube pruning .

a generate and rank approach to sentence paraphrasing
we present a method that paraphrases a given sentence by first generating candidate paraphrases and then ranking ( or classifying ) them . the candidates are generated by applying existing paraphrasing rules extracted from parallel corpora . the ranking component considers not only the overall quality of the rules that produced each candidate , but also the extent to which they preserve grammaticality and meaning in the particular context of the input sentence , as well as the degree to which the candidate differs from the input . we experimented with both a maximum entropy classifier and an svr ranker . experimental results show that incorporating features from an existing paraphrase recognizer in the ranking component improves performance , and that our overall method compares well against a state of the art paraphrase generator , when paraphrasing rules apply to the input sentences . we also propose a new methodology to evaluate the ranking components of generate-and-rank paraphrase generators , which evaluates them across different combinations of weights for grammaticality , meaning preservation , and diversity . the paper is accompanied by a paraphrasing dataset we constructed for evaluations of this kind .

domain adaptation for machine translation by mining unseen words hal daume iii
we show that unseen words account for a large part of the translation error when moving to new domains . using an extension of a recent approach to mining translations from comparable corpora ( haghighi et al , 2008 ) , we are able to find translations for otherwise oov terms . we show several approaches to integrating such translations into a phrasebased translation system , yielding consistent improvements in translations quality ( between 0.5 and 1.5 bleu points ) on four domains and two language pairs .

segmenting email message text into zones
in the early days of email , widely-used conventions for indicating quoted reply content and email signatures made it easy to segment email messages into their functional parts . today , the explosion of different email formats and styles , coupled with the ad hoc ways in which people vary the structure and layout of their messages , means that simple techniques for identifying quoted replies that used to yield 95 % accuracy now find less than 10 % of such content . in this paper , we describe zebra , an svm-based system for segmenting the body text of email messages into nine zone types based on graphic , orthographic and lexical cues . zebra performs this task with an accuracy of 87.01 % ; when the number of zones is abstracted to two or three zone classes , this increases to 93.60 % and 91.53 % respectively .

glml : annotating argument
in this paper we introduce a methodology for annotating compositional operations in natural language text , and describe a mark-up language , glml , based on generative lexicon , for identifying such relations . while most annotation systems capture surface relationships , glml captures the compositional history of the argument selection relative to the predicate . we provide a brief overview of gl before moving on to our proposed methodology for annotating with glml . there are three main tasks described in the paper : ( i ) compositional mechanisms of argument selection ; ( ii ) qualia in modification constructions ; ( iii ) type selection in modification of dot objects . we explain what each task includes and provide a description of the annotation interface . we also include the xml format for glml including examples of annotated sentences .

a hands-on introductory nlp course
we describe our first attempts to re-engineer the curriculum of our introductory nlp course by using two important building blocks : ( 1 ) access to an easy-to-learn programming language and framework to build hands-on programming assignments with real-world data and corpora and , ( 2 ) incorporation of interesting ideas from recent nlp research publications into assignment and examination problems . we believe that these are extremely important components of a curriculum aimed at a diverse audience consisting primarily of firstyear graduate students from both linguistics and computer science . based on overwhelmingly positive student feedback , we find that our attempts were hugely successful .

attention and interaction control in a human-human-computer dialogue setting
this paper presents a simple , yet effective model for managing attention and interaction control in multimodal spoken dialogue systems . the model allows the user to switch attention between the system and other humans , and the system to stop and resume speaking . an evaluation in a tutoring setting shows that the users attention can be effectively monitored using head pose tracking , and that this is a more reliable method than using push-to-talk .

xrce-m : a hybrid system for named entity metonymy resolution
this paper describes our participation to the metonymy resolution at semeval 2007 ( task # 8 ) . in order to perform named entity metonymy resolution , we developed a hybrid system based on a robust parser that extracts deep syntactic relations combined with a non-supervised distributional approach , also relying on the relations extracted by the parser .

dynamically integrating cross-domain translation memory into phrase-based machine translation during decoding
our previous work focuses on combining translation memory ( tm ) and statistical machine translation ( smt ) when the tm database and the smt training set are the same . however , the tm database will deviate from the smt training set in the real task when time goes by . in this work , we concentrate on the task when the tm database and the smt training set are different and even from different domains . firstly , we dynamically merge the matched tm phrase-pairs into the smt phrase table to meet the real application . secondly , we propose an improved integrated model to distinguish the original and the newly-added phrase-pairs . thirdly , a simple but effective tm adaptation method is adopted to favor the consistent translations in cross-domain test . our experiments have shown that merging the tm phrasepairs achieves significant improvements . furthermore , the proposed approaches are significantly better than the tm , the smt and previous integration works for both in-domain and cross-domain tests .

mime - nlg in pre-hospital care
the cross-disciplinary mime project aims to develop a mobile medical monitoring system that improves handover transactions in rural pre-hospital scenarios between the first person on scene and ambulance clinicians . nlg is used to produce a textual handover report at any time , summarising data from novel medical sensors , as well as observations and actions recorded by the carer . we describe the mime project with a focus on the nlg algorithm and an initial evaluation of the generated reports .

paraphrase generation as monolingual translation : data and evaluation
in this paper we investigate the automatic generation and evaluation of sentential paraphrases . we describe a method for generating sentential paraphrases by using a large aligned monolingual corpus of news headlines acquired automatically from google news and a standard phrase-based machine translation ( pbmt ) framework . the output of this system is compared to a word substitution baseline . human judges prefer the pbmt paraphrasing system over the word substitution system . we demonstrate that bleu correlates well with human judgements provided that the generated paraphrased sentence is sufficiently different from the source sentence .

bootstrapping coreference classifiers with multiple machine learning algorithms
successful application of multi-view cotraining algorithms relies on the ability to factor the available features into views that are compatible and uncorrelated . this can potentially preclude their use on problems such as coreference resolution that lack an obvious feature split . to bootstrap coreference classifiers , we propose and evaluate a single-view weakly supervised algorithm that relies on two different learning algorithms in lieu of the two different views required by co-training . in addition , we investigate a method for ranking unlabeled instances to be fed back into the bootstrapping loop as labeled data , aiming to alleviate the problem of performance deterioration that is commonly observed in the course of bootstrapping .

situated models of meaning for sports video retrieval
situated models of meaning ground words in the non-linguistic context , or situation , to which they refer . applying such models to sports video retrieval requires learning appropriate representations for complex events . we propose a method that uses data mining to discover temporal patterns in video , and pair these patterns with associated closed captioning text . this paired corpus is used to train a situated model of meaning that significantly improves video retrieval performance .

answering definition questions using web knowledge bases
is capable of mining textual definitions from large collections of documents . in order to automatically identify definition sentences from a large collection of documents , we utilize the existing definitions in the web knowledge bases instead of hand-crafted rules or annotated corpus . effective methods are adopted to make full use of web knowledge bases , and they promise high quality response to definition questions . we applied our system in the trec 2004 definition question-answering task and achieved an encouraging performance with the fmeasure score of 0.404 , which was ranked second among all the submitted runs .

feature subsumption for opinion analysis
lexical features are key to many approaches to sentiment analysis and opinion detection . a variety of representations have been used , including single words , multi-word ngrams , phrases , and lexicosyntactic patterns . in this paper , we use a subsumption hierarchy to formally define different types of lexical features and their relationship to one another , both in terms of representational coverage and performance . we use the subsumption hierarchy in two ways : ( 1 ) as an analytic tool to automatically identify complex features that outperform simpler features , and ( 2 ) to reduce a feature set by removing unnecessary features . we show that reducing the feature set improves performance on three opinion classification tasks , especially when combined with traditional feature selection .

looking for candidate translational equivalents in specialized , comparable
previous attempts at identifying translational equivalents in comparable corpora have dealt with very large general language corpora and words . we address this task in a specialized domain , medicine , starting from smaller non-parallel , comparable corpora and an initial bilingual medical lexicon . we compare the distributional contexts of source and target words , testing several weighting factors and similarity measures . on a test set of frequently occurring words , for the best combination ( the jaccard similarity measure with or without tf : idf weighting ) , the correct translation is ranked first for 20 % of our test words , and is found in the top 10 candidates for 50 % of them . an additional reverse-translation filtering step improves the precision of the top candidate translation up to 74 % , with a 33 % recall .

modeling latent biographic attributes in conversational genres
this paper presents and evaluates several original techniques for the latent classification of biographic attributes such as gender , age and native language , in diverse genres ( conversation transcripts , email ) and languages ( arabic , english ) . first , we present a novel partner-sensitive model for extracting biographic attributes in conversations , given the differences in lexical usage and discourse style such as observed between same-gender and mixedgender conversations . then , we explore a rich variety of novel sociolinguistic and discourse-based features , including mean utterance length , passive/active usage , percentage domination of the conversation , speaking rate and filler word usage . cumulatively up to 20 % error reduction is achieved relative to the standard boulis and ostendorf ( 2005 ) algorithm for classifying individual conversations on switchboard , and accuracy for gender detection on the switchboard corpus ( aggregate ) and gulf arabic corpus exceeds 95 % .

a discriminative model for joint morphological disambiguation and
most previous studies of morphological disambiguation and dependency parsing have been pursued independently . morphological taggers operate on n-grams and do not take into account syntactic relations ; parsers use the pipeline approach , assuming that morphological information has been separately obtained . however , in morphologically-rich languages , there is often considerable interaction between morphology and syntax , such that neither can be disambiguated without the other . in this paper , we propose a discriminative model that jointly infers morphological properties and syntactic structures . in evaluations on various highly-inflected languages , this joint model outperforms both a baseline tagger in morphological disambiguation , and a pipeline parser in head selection .

a new approach to lexical disambiguation of arabic text
we describe a model for the lexical analysis of arabic text , using the lists of alternatives supplied by a broad-coverage morphological analyzer , sama , which include stable lemma ids that correspond to combinations of broad word sense categories and pos tags . we break down each of the hundreds of thousands of possible lexical labels into its constituent elements , including lemma id and part-of-speech . features are computed for each lexical token based on its local and document-level context and used in a novel , simple , and highly efficient two-stage supervised machine learning algorithm that overcomes the extreme sparsity of label distribution in the training data . the resulting system achieves accuracy of 90.6 % for its first choice , and 96.2 % for its top two choices , in selecting among the alternatives provided by the sama lexical analyzer . we have successfully used this system in applications such as an online reading helper for intermediate learners of the arabic language , and a tool for improving the productivity of arabic treebank annotators .

automatic expansion of feature-level opinion lexicons
in most tasks related to opinion mining and sentiment analysis , it is necessary to compute the semantic orientation ( i.e. , positive or negative evaluative implications ) of certain opinion expressions . recent works suggest that semantic orientation depends on application domains . moreover , we think that semantic orientation depends on the specific targets ( features ) that an opinion is applied to . in this paper , we introduce a technique to build domainspecific , feature-level opinion lexicons in a semi-supervised manner : we first induce a lexicon starting from a small set of annotated documents ; then , we expand it automatically from a larger set of unannotated documents , using a new graph-based ranking algorithm . our method was evaluated in three different domains ( headphones , hotels and cars ) , using a corpus of product reviews which opinions were annotated at the feature level . we conclude that our method produces feature-level opinion lexicons with better accuracy and recall that domain-independent opinion lexicons using only a few annotated documents .

unimelb : topic modelling-based word sense induction jey han lau , paul cook and timothy baldwin
this paper describes our system for shared task 13 word sense induction for graded and non-graded senses of semeval-2013 . the task is on word sense induction ( wsi ) , and builds on earlier semeval wsi tasks in exploring the possibility of multiple senses being compatible to varying degrees with a single contextual instance : participants are asked to grade senses rather than selecting a single sense like most word sense disambiguation ( wsd ) settings . the evaluation measures are designed to assess how well a system perceives the different senses in a contextual instance . we adopt a previously-proposed wsi methodology for the task , which is based on a hierarchical dirichlet process ( hdp ) , a nonparametric topic model . our system requires no parameter tuning , uses the english ukwac as an external resource , and achieves encouraging results over the shared task .

collocations and the structure of proper names
this paper establishes a connection between two apparently very different kinds of probabilistic models . latent dirichlet allocation ( lda ) models are used as topic models to produce a lowdimensional representation of documents , while probabilistic context-free grammars ( pcfgs ) define distributions over trees . the paper begins by showing that lda topic models can be viewed as a special kind of pcfg , so bayesian inference for pcfgs can be used to infer topic models as well . adaptor grammars ( ags ) are a hierarchical , non-parameteric bayesian extension of pcfgs . exploiting the close relationship between lda and pcfgs just described , we propose two novel probabilistic models that combine insights from lda and ag models . the first replaces the unigram component of lda topic models with multi-word sequences or collocations generated by an ag . the second extension builds on the first one to learn aspects of the internal structure of proper names .

byeong man kim
in this work , we apply a clustering technique to integrate the contents of items into the item-based collaborative filtering framework . the group rating information that is obtained from the clustering result provides a way to introduce content information into collaborative recommendation and solves the cold start problem . extensive experiments have been conducted on movielens data to analyze the characteristics of our technique . the results show that our approach contributes to the improvement of prediction quality of the item-based collaborative filtering , especially for the cold start problem .

a structural support vector method for extracting contexts and answers of questions from online forums
this paper addresses the issue of extracting contexts and answers of questions from post discussion of online forums . we propose a novel and unified model by customizing the structural support vector machine method . our customization has several attractive properties : ( 1 ) it gives a comprehensive graphical representation of thread discussion . ( 2 ) it designs special inference algorithms instead of generalpurpose ones . ( 3 ) it can be readily extended to different task preferences by varying loss functions . experimental results on a real data set show that our methods are both promising and flexible .

the inside-outside recursive neural network model for
we propose the first implementation of an infinite-order generative dependency model . the model is based on a new recursive neural network architecture , the inside-outside recursive neural network . this architecture allows information to flow not only bottom-up , as in traditional recursive neural networks , but also topdown . this is achieved by computing content as well as context representations for any constituent , and letting these representations interact . experimental results on the english section of the universal dependency treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting , and tends to choose more accurate parses in k-best lists . in addition , reranking with this model achieves state-of-the-art unlabelled attachment scores and unlabelled exact match scores .

stanfords multi-pass sieve coreference resolution system at the
this paper details the coreference resolution system submitted by stanford at the conll2011 shared task . our system is a collection of deterministic coreference resolution models that incorporate lexical , syntactic , semantic , and discourse information . all these models use global document-level information by sharing mention attributes , such as gender and number , across mentions in the same cluster . we participated in both the open and closed tracks and submitted results using both predicted and gold mentions . our system was ranked first in both tracks , with a score of 57.8 in the closed track and 58.3 in the open track .

nouns are vectors , adjectives are matrices : representing adjective-noun constructions in semantic space
we propose an approach to adjective-noun composition ( an ) for corpus-based distributional semantics that , building on insights from theoretical linguistics , represents nouns as vectors and adjectives as data-induced ( linear ) functions ( encoded as matrices ) over nominal vectors . our model significantly outperforms the rivals on the task of reconstructing an vectors not seen in training . a small post-hoc analysis further suggests that , when the model-generated an vector is not similar to the corpus-observed an vector , this is due to anomalies in the latter . we show moreover that our approach provides two novel ways to represent adjective meanings , alternative to its representation via corpus-based co-occurrence vectors , both outperforming the latter in an adjective clustering task .

automatic extraction of complex predicates in bengali dipankar das santanu pal tapabrata mondal tanmoy chakraborty
this paper presents the automatic extraction of complex predicates ( cps ) in bengali with a special focus on compound verbs ( verb + verb ) and conjunct verbs ( noun /adjective + verb ) . the lexical patterns of compound and conjunct verbs are extracted based on the information of shallow morphology and available seed lists of verbs . lexical scopes of compound and conjunct verbs in consecutive sequence of complex predicates ( cps ) have been identified . the fine-grained error analysis through confusion matrix highlights some insufficiencies of lexical patterns and the impacts of different constraints that are used to identify the complex predicates ( cps ) . system achieves f-scores of 75.73 % , and 77.92 % for compound verbs and 89.90 % and 89.66 % for conjunct verbs respectively on two types of bengali corpus .

event detection and summarization in weblogs with temporal collocations
this paper deals with the relationship between weblog content and time . with the proposed temporal mutual information , we analyze the collocations in time dimension , and the interesting collocations related to special events . the temporal mutual information is employed to observe the strength of term-to-term associations over time . an event detection algorithm identifies the collocations that may cause an event in a specific timestamp . an event summarization algorithm retrieves a set of collocations which describe an event . we compare our approach with the approach without considering the time interval . the experimental results demonstrate that the temporal collocations capture the real world semantics and real world events over time .

dependency parsing of japanese spoken monologue based on clause boundaries
spoken monologues feature greater sentence length and structural complexity than do spoken dialogues . to achieve high parsing performance for spoken monologues , it could prove effective to simplify the structure by dividing a sentence into suitable language units . this paper proposes a method for dependency parsing of japanese monologues based on sentence segmentation . in this method , the dependency parsing is executed in two stages : at the clause level and the sentence level . first , the dependencies within a clause are identified by dividing a sentence into clauses and executing stochastic dependency parsing for each clause . next , the dependencies over clause boundaries are identified stochastically , and the dependency structure of the entire sentence is thus completed . an experiment using a spoken monologue corpus shows this method to be effective for efficient dependency parsing of japanese monologue sentences .

use and acquisition of semantic language model
semantic language model is a technique that utilizes the semantic structure of an utterance to better rank the likelihood of words composing the sentence . when used in a conversational system , one can dynamically integrate the dialog state and domain semantics into the semantic language model to better guide the speech recognizer executing the decoding process . we describe one such application that employs semantic language model to cope with spontaneous speech in a robust manner . the semantic language model , though can be manually crafted without data , can benefit significantly from data driven machine learning techniques . an example based approach is also described here to demonstrate a viable approach .

unal : discriminating between literal and figurative
in this paper we describe the system used to participate in the sub task 5b in the phrasal semantics challenge ( task 5 ) in semeval 2013. this sub task consists in discriminating literal and figurative usage of phrases with compositional and non-compositional meanings in context . the proposed approach is based on part-of-speech tags , stylistic features and distributional statistics gathered from the same development-training-test text collection . the system obtained a relative improvement in accuracy against the most-frequentclass baseline of 49.8 % in the unseen contexts ( lexsample ) setting and 8.5 % in unseen phrases ( allwords ) .

simple parser for indian languages in a dependency framework akshar bharati , mridul gupta , vineet yadav , karthik gali and dipti misra sharma
this paper is an attempt to show that an intermediary level of analysis is an effective way for carrying out various nlp tasks for linguistically similar languages . we describe a process for developing a simple parser for doing such tasks . this parser uses a grammar driven approach to annotate dependency relations ( both inter and intra chunk ) at an intermediary level . ease in identifying a particular dependency relation dictates the degree of analysis reached by the parser . to establish efficiency of the simple parser we show the improvement in its results over previous grammar driven dependency parsing approaches for indian languages like hindi . we also propose the possibility of usefulness of the simple parser for indian languages that are similar in nature .

a little goes a long way : quick authoring of semantic knowledge sources for interpretation carolyn penstein rose
in this paper we present an evaluation of carmel-tools , a novel behavior oriented approach to authoring and maintaining domain specific knowledge sources for robust sentence-level language understanding . carmel-tools provides a layer of abstraction between the author and the knowledge sources , freeing up the author to focus on the desired language processing behavior that is desired in the target system rather than the linguistic details of the knowledge sources that would make this behavior possible . furthermore , carmeltools offers greater flexibility in output representation than the context-free rewrite rules produced by previous semantic authoring tools , allowing authors to design their own predicate language representations .

sucre : a modular system for coreference resolution
this paper presents sucre , a new software tool for coreference resolution and its feature engineering . it is able to separately do noun , pronoun and full coreference resolution . sucre introduces a new approach to the feature engineering of coreference resolution based on a relational database model and a regular feature definition language . sucre successfully participated in semeval-2010 task 1 on coreference resolution in multiple languages ( recasens et al , 2010 ) for gold and regular closed annotation tracks of six languages . it obtained the best results in several categories , including the regular closed annotation tracks of english and german .

generating elliptic coordination
in this paper , we focus on the task of generating elliptic sentences . we extract from the data provided by the surface realisation ( sr ) task ( belz et al , 2011 ) 2398 input whose corresponding output sentence contain an ellipsis . we show that 9 % of the data contains an ellipsis and that both coverage and bleu score markedly decrease for elliptic input ( from 82.3 % coverage for non-elliptic sentences to 65.3 % for elliptic sentences and from 0.60 bleu score to 0.47 ) . we argue that elided material should be represented using phonetically empty nodes and we introduce a set of rewrite rules which permits adding these empty categories to the sr data . finally , we evaluate an existing surface realiser on the resulting dataset . we show that , after rewriting , the generator achieves a coverage of 76 % and a bleu score of 0.74 on the elliptical data .

part-of-speech tagset and corpus development for igbo , an african
this project aims to develop linguistic resources to support computational nlp research on the igbo language . the starting point for this project is the development of a new part-of-speech tagging scheme based on the eagles tagset guidelines , adapted to incorporate additional language internal features . the tags are currently being used in a part-of-speech annotation task for the development of pos tagged igbo corpus . the proposed tagset has 59 tags .

non-projective dependency parsing in expected linear time
we present a novel transition system for dependency parsing , which constructs arcs only between adjacent words but can parse arbitrary non-projective trees by swapping the order of words in the input . adding the swapping operation changes the time complexity for deterministic parsing from linear to quadratic in the worst case , but empirical estimates based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora . evaluation on data from five languages shows state-of-the-art accuracy , with especially good results for the labeled exact match score .

automatic extraction of arabic multiword expressions
in this paper we investigate the automatic acquisition of arabic multiword expressions ( mwe ) . we propose three complementary approaches to extract mwes from available data resources . the first approach relies on the correspondence asymmetries between arabic wikipedia titles and titles in 21 different languages . the second approach collects english mwes from princeton wordnet 3.0 , translates the collection into arabic using google translate , and utilizes different search engines to validate the output . the third uses lexical association measures to extract mwes from a large unannotated corpus . we experimentally explore the feasibility of each approach and measure the quality and coverage of the output against gold standards .

towards learning rules from natural texts
in this paper , we consider the problem of inductively learning rules from specific facts extracted from texts . this problem is challenging due to two reasons . first , natural texts are radically incomplete since there are always too many facts to mention . second , natural texts are systematically biased towards novelty and surprise , which presents an unrepresentative sample to the learner . our solutions to these two problems are based on building a generative observation model of what is mentioned and what is extracted given what is true . we first present a multiple-predicate bootstrapping approach that consists of iteratively learning if-then rules based on an implicit observation model and then imputing new facts implied by the learned rules . second , we present an iterative ensemble colearning approach , where multiple decisiontrees are learned from bootstrap samples of the incomplete training data , and facts are imputed based on weighted majority .

paraquery : making sense of paraphrase collections educational testing service
pivoting on bilingual parallel corpora is a popular approach for paraphrase acquisition . although such pivoted paraphrase collections have been successfully used to improve the performance of several different nlp applications , it is still difficult to get an intrinsic estimate of the quality and coverage of the paraphrases contained in these collections . we present paraquery , a tool that helps a user interactively explore and characterize a given pivoted paraphrase collection , analyze its utility for a particular domain , and compare it to other popular lexical similarity resources all within a single interface .

a new string-to-dependency machine translation algorithm with a target dependency language model
in this paper , we propose a novel string-todependency algorithm for statistical machine translation . with this new framework , we employ a target dependency language model during decoding to exploit long distance word relations , which are unavailable with a traditional n-gram language model . our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in bleu and 2.53 point improvement in ter compared to a standard hierarchical string-tostring system on the nist 04 chinese-english evaluation set .

lattice-based system combination for statistical machine translation
current system combination methods usually use confusion networks to find consensus translations among different systems . requiring one-to-one mappings between the words in candidate translations , confusion networks have difficulty in handling more general situations in which several words are connected to another several words . instead , we propose a lattice-based system combination model that allows for such phrase alignments and uses lattices to encode all candidate translations . experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on chinese-to-english translation test sets .

improving pronoun resolution using statistics-based semantic compatibility information
in this paper we focus on how to improve pronoun resolution using the statisticsbased semantic compatibility information . we investigate two unexplored issues that influence the effectiveness of such information : statistics source and learning framework . specifically , we for the first time propose to utilize the web and the twin-candidate model , in addition to the previous combination of the corpus and the single-candidate model , to compute and apply the semantic information . our study shows that the semantic compatibility obtained from the web can be effectively incorporated in the twin-candidate learning model and significantly improve the resolution of neutral pronouns .

improving parsing and pp attachment performance with sense information
to date , parsers have made limited use of semantic information , but there is evidence to suggest that semantic features can enhance parse disambiguation . this paper shows that semantic classes help to obtain significant improvement in both parsing and pp attachment tasks . we devise a gold-standard sense- and parse tree-annotated dataset based on the intersection of the penn treebank and semcor , and experiment with different approaches to both semantic representation and disambiguation . for the bikel parser , we achieved a maximal error reduction rate over the baseline parser of 6.9 % and 20.5 % , for parsing and pp-attachment respectively , using an unsupervised wsd strategy . this demonstrates that word sense information can indeed enhance the performance of syntactic disambiguation .

topic modeling on historical newspapers
in this paper , we explore the task of automatic text processing applied to collections of historical newspapers , with the aim of assisting historical research . in particular , in this first stage of our project , we experiment with the use of topical models as a means to identify potential issues of interest for historians .

building a semantically annotated corpus for congestive heart and renal failure from clinical records and the literature
narrative information in electronic health records ( ehrs ) and literature articles contains a wealth of clinical information about treatment , diagnosis , medication and family history . this often includes detailed phenotype information for specific diseases , which in turn can help to identify risk factors and thus determine the susceptibility of different patients . such information can help to improve healthcare applications , including clinical decision support systems ( cds ) . clinical text mining ( tm ) tools can provide efficient automated means to extract and integrate vital information hidden within the vast volumes of available text . development or adaptation of tm tools is reliant on the availability of annotated training corpora , although few such corpora exist for the clinical domain . in response , we have created a new annotated corpus ( phenochf ) , focussing on the identification of phenotype information for a specific clinical sub-domain , i.e. , congestive heart failure ( chf ) . the corpus is unique in this domain , in its integration of information from both ehrs ( 300 discharge summaries ) and literature articles ( 5 full-text papers ) . the annotation scheme , whose design was guided by a domain expert , includes both entities and relations pertinent to chf . two further domain experts performed the annotation , resulting in high quality annotation , with agreement rates up to 0.92 f-score .

clause identification and classification in bengali
this paper reports about the development of clause identification and classification techniques for bengali language . a syntactic rule based model has been used to identify the clause boundary . for clause type identification a conditional random field ( crf ) based statistical model has been used . the clause identification system and clause classification system demonstrated 73 % and 78 % precision values respectively .

from prosodic trees to syntactic trees
this paper describes an ongoing effort to parse the hebrew bible . the parser consults the bracketing information extracted from the cantillation marks of the masoetic text . we first constructed a cantillation treebank which encodes the prosodic structures of the text . it was found that many of the prosodic boundaries in the cantillation trees correspond , directly or indirectly , to the phrase boundaries of the syntactic trees we are trying to build . all the useful boundary information was then extracted to help the parser make syntactic decisions , either serving as hard constraints in rule application or used probabilistically in tree ranking . this has greatly improved the accuracy and efficiency of the parser and reduced the amount of manual work in building a hebrew treebank .

discriminative modeling of extraction sets for machine translation
we present a discriminative model that directly predicts which set of phrasal translation rules should be extracted from a sentence pair . our model scores extraction sets : nested collections of all the overlapping phrase pairs consistent with an underlying word alignment . extraction set models provide two principle advantages over word-factored alignment models . first , we can incorporate features on phrase pairs , in addition to word links . second , we can optimize for an extraction-based loss function that relates directly to the end task of generating translations . our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines , as well as providing up to a 1.4 improvement in bleu score in chinese-to-english translation experiments .

using minimal recursion semantics for entailment recognition
this paper describes work on using minimal recursion semantics ( mrs ) representations for the task of recognising textual entailment . i use entailment data from a semeval-2010 shared task to develop and evaluate an entailment recognition heuristic . i compare my results to the shared task winner , and discuss differences in approaches . finally , i run my system with multiple mrs representations per sentence , and show that this improves the recognition results for positive entailment sentence pairs .

toward categorization of sign language corpora
this paper addresses the notion of parallel , noisy parallel and comparable corpora in the sign language research field . as it is quite a new field , the categorization of sign language corpora is not well established , and does not rely on a straightforward basis . nevertheless , several kinds of corpora are now available and could raise interesting issues , provided that adapted tools and techniques are developed .

beam search for solving substitution ciphers human language technology and pattern recognition
in this paper we address the problem of solving substitution ciphers using a beam search approach . we present a conceptually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ciphers and is able to use high order n-gram language models . we show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3-gram language models has 38.6 % decipherment error , while our approach achieves 4.13 % decipherment error in a fraction of time by using a 6-gram language model . we also apply our approach to the famous zodiac-408 cipher and obtain slightly better ( and near to optimal ) results than previously published . unlike the previous state-of-the-art approach that uses additional word lists to evaluate possible decipherments , our approach only uses a letterbased 6-gram language model . furthermore we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the gigaword corpus of 7.8 % to 6.0 % error rate .

cross-lingual distributional profiles of concepts for measuring semantic distance
we present the idea of estimating semantic distance in one , possibly resource-poor , language using a knowledge source in another , possibly resource-rich , language . we do so by creating cross-lingual distributional profiles of concepts , using a bilingual lexicon and a bootstrapping algorithm , but without the use of any sense-annotated data or word-aligned corpora . the cross-lingual measures of semantic distance are evaluated on two tasks : ( 1 ) estimating semantic distance between words and ranking the word pairs according to semantic distance , and ( 2 ) solving readers digest word power problems . in task ( 1 ) , cross-lingual measures are superior to conventional monolingual measures based on a wordnet . in task ( 2 ) , cross-lingual measures are able to solve more problems correctly , and despite scores being affected by many tied answers , their overall performance is again better than the best monolingual measures .

antecedent recovery : experiments with a trace tagger
this paper explores the problem of finding non-local dependencies . first , we isolate a set of features useful for this task . second , we develop both a two-step approach which combines a trace tagger with a state-of-the-art lexicalized parser and a one-step approach which finds nonlocal dependencies while parsing . we find that the former outperforms the latter because it makes better use of the features we isolate .

the role of syntax in vector space models of compositional semantics karl moritz hermann and phil blunsom
modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of natural language processing . in this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by combinatory categorial grammar to introduce combinatory categorial autoencoders . this model leverages the ccg combinatory operators to guide a non-linear transformation of meaning within a sentence . we use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks , demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general .

hybrid document indexing with spectral embedding
document representation has a large impact on the performance of document retrieval and clustering algorithms . we propose a hybrid document indexing scheme that combines the traditional bagof-words representation with spectral embedding . this method accounts for the specifics of the document collection and also uses semantic similarity information based on a large scale statistical analysis . clustering experiments showed improvements over the traditional tf-idf representation and over the spectral methods based solely on the document collection .

automatic inference of the temporal location of situations in chinese text
chinese is a language that does not have morphological tense markers that provide explicit grammaticalization of the temporal location of situations ( events or states ) . however , in many nlp applications such as machine translation , information extraction and question answering , it is desirable to make the temporal location of the situations explicit . we describe a machine learning framework where different sources of information can be combined to predict the temporal location of situations in chinese text . our experiments show that this approach significantly outperforms the most frequent tense baseline . more importantly , the high training accuracy shows promise that this challenging problem is solvable to a level where it can be used in practical nlp applications with more training data , better modeling techniques and more informative and generalizable features .

fast and robust multilingual dependency parsing with a generative latent variable model
we use a generative history-based model to predict the most likely derivation of a dependency parse . our probabilistic model is based on incremental sigmoid belief networks , a recently proposed class of latent variable models for structure prediction . their ability to automatically induce features results in multilingual parsing which is robust enough to achieve accuracy well above the average for each individual language in the multilingual track of the conll-2007 shared task . this robustness led to the third best overall average labeled attachment score in the task , despite using no discriminative methods . we also demonstrate that the parser is quite fast , and can provide even faster parsing times without much loss of accuracy .

evaluation of a japanese cfg derived from a syntactically annotated corpus with respect to dependency measures tomoya noro chimato koike taiichi hashimoto takenobu tokunaga hozumi tanaka
parsing is one of the important processes for natural language processing and , in general , a large-scale cfg is used to parse a wide variety of sentences . for many languages , a cfg is derived from a large-scale syntactically annotated corpus , and many parsing algorithms using cfgs have been proposed . however , we could not apply them to japanese since a japanese syntactically annotated corpus has not been available as of yet . in order to solve the problem , we have been building a large-scale japanese syntactically annotated corpus . in this paper , we show the evaluation results of a cfg derived from our corpus and compare it with results of some japanese dependency analyzers .

solving the problem of cascading errors : approximate bayesian inference for linguistic annotation pipelines
the end-to-end performance of natural language processing systems for compound tasks , such as question answering and textual entailment , is often hampered by use of a greedy 1-best pipeline architecture , which causes errors to propagate and compound at each stage . we present a novel architecture , which models these pipelines as bayesian networks , with each low level task corresponding to a variable in the network , and then we perform approximate inference to find the best labeling . our approach is extremely simple to apply but gains the benefits of sampling the entire distribution over labels at each stage in the pipeline . we apply our method to two tasks semantic role labeling and recognizing textual entailment and achieve useful performance gains from the superior pipeline architecture .

rating computer-generated questions with mechanical turk
we use amazon mechanical turk to rate computer-generated reading comprehension questions about wikipedia articles . such application-specific ratings can be used to train statistical rankers to improve systems final output , or to evaluate technologies that generate natural language . we discuss the question rating scheme we developed , assess the quality of the ratings that we gathered through amazon mechanical turk , and show evidence that these ratings can be used to improve question generation .

semtag , the loria toolbox for tag-based parsing and generation
in this paper , we introduce semtag , a toolbox for tag-based parsing and generation . this environment supports the development of wide-coverage grammars and differs from existing environments for tag such as xtag , ( xtag-researchgroup , 2001 ) in that it includes a semantic dimension . semtag is open-source and freely available .

selecting relevant text subsets from web-data for building topic specic
in this paper we present a scheme to select relevant subsets of sentences from a large generic corpus such as text acquired from the web . a relative entropy ( r.e ) based criterion is used to incrementally select sentences whose distribution matches the domain of interest . experimental results show that by using the proposed subset selection scheme we can get significant performance improvement in both word error rate ( wer ) and perplexity ( ppl ) over the models built from the entire web-corpus by using just 10 % of the data . in addition incremental data selection enables us to achieve significant reduction in the vocabulary size as well as number of n-grams in the adapted language model . to demonstrate the gains from our method we provide a comparative analysis with a number of methods proposed in recent language modeling literature for cleaning up text .

resolving lexical ambiguity in tensor regression models of meaning and computer science mile end road
this paper provides a method for improving tensor-based compositional distributional models of meaning by the addition of an explicit disambiguation step prior to composition . in contrast with previous research where this hypothesis has been successfully tested against relatively simple compositional models , in our work we use a robust model trained with linear regression . the results we get in two experiments show the superiority of the prior disambiguation method and suggest that the effectiveness of this approach is modelindependent .

different sense granularities for different applications martha palmer , olga babko-malaya , hoa trang dang
this paper describes an hierarchical approach to wordnet sense distinctions that provides different types of automatic word sense disambiguation ( wsd ) systems , which perform at varying levels of accuracy . for tasks where fine-grained sense distinctions may not be essential , an accurate coarse-grained wsd system may be sufficient . the paper discusses the criteria behind the three different levels of sense granularity , as well as the machine learning approach used by the wsd system .

contextual modeling for meeting translation using unsupervised word
in this paper we investigate the challenges of applying statistical machine translation to meeting conversations , with a particular view towards analyzing the importance of modeling contextual factors such as the larger discourse context and topic/domain information on translation performance . we describe the collection of a small corpus of parallel meeting data , the development of a statistical machine translation system in the absence of genre-matched training data , and we present a quantitative analysis of translation errors resulting from the lack of contextual modeling inherent in standard statistical machine translation systems . finally , we demonstrate how the largest source of translation errors ( lack of topic/domain knowledge ) can be addressed by applying documentlevel , unsupervised word sense disambiguation , resulting in performance improvements over the baseline system .

towards the interpretation of utterance sequences in a dialogue system ingrid zukerman and patrick ye and kapil kumar gupta and enes makalic
this paper describes a probabilistic mechanism for the interpretation of sentence sequences developed for a spoken dialogue system mounted on a robotic agent . the mechanism receives as input a sequence of sentences , and produces an interpretation which integrates the interpretations of individual sentences . for our evaluation , we collected a corpus of hypothetical requests to a robot . our mechanism exhibits good performance for sentence pairs , but requires further improvements for sentence sequences .

instance-based ontology population exploiting named-entity substitution
we present an approach to ontology population based on a lexical substitution technique . it consists in estimating the plausibility of sentences where the named entity to be classified is substituted with the ones contained in the training data , in our case , a partially populated ontology . plausibility is estimated by using web data , while the classification algorithm is instance-based . we evaluated our method on two different ontology population tasks . experiments show that our solution is effective , outperforming existing methods , and it can be applied to practical ontology population problems .

aligning words in english-hindi parallel corpora niraj aswani robert gaizauskas
in this paper , we describe a word alignment algorithm for english-hindi parallel data . the system was developed to participate in the shared task on word alignment for languages with scarce resources at the acl 2005 workshop , on building and using parallel texts : data driven machine translation and beyond . our word alignment algorithm is based on a hybrid method which performs local word grouping on hindi sentences and uses other methods such as dictionary lookup , transliteration similarity , expected english words and nearest aligned neighbours . we trained our system on the training data provided to obtain a list of named entities and cognates and to collect rules for local word grouping in hindi sentences . the system scored 77.03 % precision and 60.68 % recall on the shared task unseen test data .

a repository of free lexical resources for african languages :
we report on a project which we believe to have the potential to become home to , among others , bilingual dictionaries for african languages . kept in a well-structured xml format with several possible degrees of conformance , the dictionaries will be able to get usable even in their early versions , which will be then subject to supervised improvement as user feedback accumulates . the project is freedict , part of sourceforge , a well-known internet repository of open source content . we demonstrate a possible process of dictionary development on the example of one of freedict dictionaries , a swahili-english dictionary that we maintain and have been developing through subsequent stages of increasing complexity and machineprocessability . the aim of the paper is to show that even a small bilingual lexical resource can be submitted to this project and gradually developed into a machineprocessable form that can then interact with other freedict resources . we also present the immediate benefits of locating bilingual african dictionaries in this project . we have found freedict to be a very promising project with a lot of potential , and the present paper is meant to spread the news about it , in the hope to create an active community of linguists and lexicographers of various backgrounds , where common research subprojects can be fruitfully carried out .

a simple and effective hierarchical phrase reordering model
while phrase-based statistical machine translation systems currently deliver state-of-theart performance , they remain weak on word order changes . current phrase reordering models can properly handle swaps between adjacent phrases , but they typically lack the ability to perform the kind of long-distance reorderings possible with syntax-based systems . in this paper , we present a novel hierarchical phrase reordering model aimed at improving non-local reorderings , which seamlessly integrates with a standard phrase-based system with little loss of computational efficiency . we show that this model can successfully handle the key examples often used to motivate syntax-based systems , such as the rotation of a prepositional phrase around a noun phrase . we contrast our model with reordering models commonly used in phrase-based systems , and show that our approach provides statistically significant bleu point gains for two language pairs : chinese-english ( +0.53 on mt05 and +0.71 on mt08 ) and arabic-english ( +0.55 on mt05 ) .

scanning methods and language modeling for binary switch typing
we present preliminary experiments of a binary-switch , static-grid typing interface making use of varying language model contributions . our motivation is to quantify the degree to which language models can make the simplest scanning interfaces such as showing one symbol at a time rather than a scanning a grid competitive in terms of typing speed . we present a grid scanning method making use of optimal huffman binary codes , and demonstrate the impact of higher order language models on its performance . we also investigate the scanning methods of highlighting just one cell in a grid at any given time or showing one symbol at a time without a grid , and show that they yield commensurate performance when using higher order n-gram models , mainly due to lower error rate and a lower rate of missed targets .

finding middle ground multi-objective natural language generation from time-series data
a natural language generation ( nlg ) system is able to generate text from nonlinguistic data , ideally personalising the content to a users specific needs . in some cases , however , there are multiple stakeholders with their own individual goals , needs and preferences . in this paper , we explore the feasibility of combining the preferences of two different user groups , lecturers and students , when generating summaries in the context of student feedback generation . the preferences of each user group are modelled as a multivariate optimisation function , therefore the task of generation is seen as a multi-objective ( mo ) optimisation task , where the two functions are combined into one . this initial study shows that treating the preferences of each user group equally smooths the weights of the mo function , in a way that preferred content of the user groups is not presented in the generated summary .

multilingual legal terminology on the jibiki platform : the lexalp project
this paper presents the particular use of jibiki ( papillons web server development platform ) for the lexalp1 project . lexalps goal is to harmonise the terminology on spatial planning and sustainable development used within the alpine convention2 , so that the member states are able to cooperate and communicate efficiently in the four official languages ( french , german , italian and slovene ) . to this purpose , lexalp uses the jibiki platform to build a term bank for the contrastive analysis of the specialised terminology used in six different national legal systems and four different languages . in this paper we present how a generic platform like jibiki can cope with a new kind of dictionary .

construction of bilingual multimodal corpora of referring expressions in
this paper presents on-going work on constructing bilingual multimodal corpora of referring expressions in collaborative problem solving for english and japanese . the corpora were collected from dialogues in which two participants collaboratively solved tangram puzzles with a puzzle simulator . extra-linguistic information such as operations on puzzle pieces , mouse cursor position and piece positions were recorded in synchronisation with utterances . the speech data was transcribed and time-aligned with the extra-linguistic information . referring expressions in utterances that refer to puzzle pieces were annotated in terms of their spans , their referents and their other attributes . the japanese corpus has already been completed , but the english counterpart is still undergoing annotation . we have conducted a preliminary comparative analysis of both corpora , mainly with respect to task completion time , task success rates and attributes of referring expressions . these corpora showed significant differences in task completion time and success rate .

accelerated estimation of conditional random fields using a pseudo-likelihood-inspired perceptron variant
we discuss a simple estimation approach for conditional random fields ( crfs ) . the approach is derived heuristically by defining a variant of the classic perceptron algorithm in spirit of pseudo-likelihood for maximum likelihood estimation . the resulting approximative algorithm has a linear time complexity in the size of the label set and contains a minimal amount of tunable hyper-parameters . consequently , the algorithm is suitable for learning crfbased part-of-speech ( pos ) taggers in presence of large pos label sets . we present experiments on five languages . despite its heuristic nature , the algorithm provides surprisingly competetive accuracies and running times against reference methods .

an hdp model for inducing combinatory categorial grammars
we introduce a novel nonparametric bayesian model for the induction of combinatory categorial grammars from pos-tagged text . it achieves state of the art performance on a number of languages , and induces linguistically plausible lexicons .

language modeling for determiner selection
we present a method for automatic determiner selection , based on an existing language model . we train on the penn treebank and also use additional data from the north american news text corpus . our results are a significant improvement over previous best .

the talp-upc ngram-based statistical machine translation system for
this paper reports on the participation of the talp research center of the upc ( universitat politcnica de catalunya ) to the acl wmt 2008 evaluation campaign . this years system is the evolution of the one we employed for the 2007 campaign . main updates and extensions involve linguistically motivated word reordering based on the reordering patterns technique . in addition , this system introduces a target language model , based on linguistic classes ( part-of-speech ) , morphology reduction for an inflectional language ( spanish ) and an improved optimization procedure . results obtained over the development and test sets on spanish to english ( and the other way round ) translations for both the traditional europarl and a challenging news stories tasks are analyzed and commented .

efficient extraction of oracle-best translations from hypergraphs
hypergraphs are used in several syntaxinspired methods of machine translation to compactly encode exponentially many translation hypotheses . the hypotheses closest to given reference translations therefore can not be found via brute force , particularly for popular measures of closeness such as bleu . we develop a dynamic program for extracting the so called oracle-best hypothesis from a hypergraph by viewing it as the problem of finding the most likely hypothesis under an n-gram language model trained from only the reference translations . we further identify and remove massive redundancies in the dynamic program state due to the sparsity of n-grams present in the reference translations , resulting in a very efficient program . we present runtime statistics for this program , and demonstrate successful application of the hypotheses thus found as the targets for discriminative training of translation system components .

ctemp : a chinese temporal parser for extracting and normalizing temporal information
information extraction , question answering and summarization . in this paper , we present a temporal parser for extracting and normalizing temporal expressions from chinese texts . an integrated temporal framework is proposed , which includes basic temporal concepts and the classification of temporal expressions . the identification of temporal expressions is fulfilled by powerful chart-parsing based on grammar rules and constraint rules . we evaluated the system on a substantial corpus and obtained promising results .

discovering implicit discourse relations through brown cluster pair
sentences form coherent relations in a discourse without discourse connectives more frequently than with connectives . senses of these implicit discourse relations that hold between a sentence pair , however , are challenging to infer . here , we employ brown cluster pairs to represent discourse relation and incorporate coreference patterns to identify senses of implicit discourse relations in naturally occurring text . our system improves the baseline performance by as much as 25 % . feature analyses suggest that brown cluster pairs and coreference patterns can reveal many key linguistic characteristics of each type of discourse relation .

identifying pronominal verbs : towards automatic disambiguation of the clitic se in portuguese magali sanches duran , carolina evaristo scarton , sandra maria alusio , carlos ramisch
a challenging topic in portuguese language processing is the multifunctional and ambiguous use of the clitic pronoun se , which impacts nlp tasks such as syntactic parsing , semantic role labeling and machine translation . aiming to give a step forward towards the automatic disambiguation of se , our study focuses on the identification of pronominal verbs , which correspond to one of the six uses of se as a clitic pronoun , when se is considered a constitutive particle of the verb lemma to which it is bound , as a multiword unit . our strategy to identify such verbs is to analyze the results of a corpus search and to rule out all the other possible uses of se . this process evidenced the features needed in a computational lexicon to automatically perform the disambiguation task . the availability of the resulting lexicon of pronominal verbs on the web enables their inclusion in broader lexical resources , such as the portuguese versions of wordnet , propbank and verbnet . moreover , it will allow the revision of parsers and dictionaries already in use .

an empirical study of vietnamese noun phrase chunking with discriminative sequence models
this paper presents an empirical work for vietnamese np chunking task . we show how to build an annotation corpus of np chunking and how discriminative sequence models are trained using the corpus . experiment results using 5 fold cross validation test show that discriminative sequence learning are well suitable for vietnamese chunking . in addition , by empirical experiments we show that the part of speech information contribute significantly to the performance of there learning models .

