thai grapheme-based speech recognition
in this paper we present the results for building a grapheme-based speech recognition system for thai . we experiment with different settings for the initial context independent system , different number of acoustic models and different contexts for the speech unit . in addition , we investigate the potential of an enhanced tree clustering method as a way of sharing parameters across models . we compare our system with two phoneme-based systems ; one that uses a hand-crafted dictionary and another that uses an automatically generated dictionary . experiment results show that the grapheme-based system with enhanced tree clustering outperforms the phoneme-based system using an automatically generated dictionary , and has comparable results to the phoneme-based system with the handcrafted dictionary .

middleware for creating and combining multi-dimensional nlp markup
we present the heart of gold middleware by demonstrating three xmlbased integration scenarios where multidimensional markup produced online by multilingual natural language processing ( nlp ) components is combined to deliver rich , robust linguistic markup for use in nlp-based applications like information extraction , question answering and semantic web . the scenarios include ( 1 ) robust deep-shallow integration , ( 2 ) shallow processing cascades , and ( 3 ) treebank storage of multi-dimensionally annotated texts .

using hypothesis selection based features for confusion network mt
this paper describes the development operated into many , an open source system combination software based on confusion networks developed at lium . the hypotheses from chinese-english mt systems were combined with a new version of the software . many has been updated in order to use word confidence score and to boost n-grams occurring in input hypotheses . in this paper we propose either to use an adapted language model or adding some additional features in the decoder to boost certain n-grams probabilities . experimental results show that the updates yielded significant improvements in terms of bleu score .

extracting mwes from italian corpora : a case study for refining the pos-pattern methodology
an established method for mwe extraction is the combined use of previously identified pos-patterns and association measures . however , the selection of such pospatterns is rarely debated . focusing on italian mwes containing at least one adjective , we set out to explore how candidate pos-patterns listed in relevant literature and lexicographic sources compare with pos sequences exhibited by statistically significant n-grams including an adjective position extracted from a large corpus of italian . all literature-derived patterns are foundand new meaningful candidate patterns emergeamong the top-ranking trigrams for three association measures . we conclude that a final solid set to be used for mwe extraction will have to be further refined through a combination of association measures as well as manual inspection .

a syntactic time-series model for parsing fluent and disfluent speech
this paper describes an incremental approach to parsing transcribed spontaneous speech containing disfluencies with a hierarchical hidden markov model ( hhmm ) . this model makes use of the right-corner transform , which has been shown to increase non-incremental parsing accuracy on transcribed spontaneous speech ( miller and schuler , 2008 ) , using trees transformed in this manner to train the hhmm parser . not only do the representations used in this model align with structure in speech repairs , but as an hmm-like timeseries model , it can be directly integrated into conventional speech recognition systems run on continuous streams of audio . a system implementing this model is evaluated on the standard task of parsing the switchboard corpus , and achieves an improvement over the standard baseline probabilistic cyk parser .

summarizing textual information about locations in a geo-spatial information display system congxing cai eduard hovy
this demo describes the summarization of textual material about locations in the context of a geo-spatial information display system . when the amount of associated textual data is large , it is organized and summarized before display . a hierarchical summarization framework , conditioned on the small space available for display , has been fully implemented . snapshots of the system , with narrative descriptions , demonstrate our results .

optimizing grammars for minimum dependency length
we examine the problem of choosing word order for a set of dependency trees so as to minimize total dependency length . we present an algorithm for computing the optimal layout of a single tree as well as a numerical method for optimizing a grammar of orderings over a set of dependency types . a grammar generated by minimizing dependency length in unordered trees from the penn treebank is found to agree surprisingly well with english word order , suggesting that dependency length minimization has influenced the evolution of english .

distributed training strategies for the structured perceptron ryan mcdonald keith hall gideon mann
perceptron training is widely applied in the natural language processing community for learning complex structured models . like all structured prediction learning frameworks , the structured perceptron can be costly to train as training complexity is proportional to inference , which is frequently non-linear in example sequence length . in this paper we investigate distributed training strategies for the structured perceptron as a means to reduce training times when computing clusters are available . we look at two strategies and provide convergence bounds for a particular mode of distributed structured perceptron training based on iterative parameter mixing ( or averaging ) . we present experiments on two structured prediction problems namedentity recognition and dependency parsing to highlight the efficiency of this method .

multiple-stream language models for statistical machine translation
we consider using online language models for translating multiple streams which naturally arise on the web . after establishing that using just one stream can degrade translations on different domains , we present a series of simple approaches which tackle the problem of maintaining translation performance on all streams in small space . by exploiting the differing throughputs of each stream and how the decoder translates prior test points from each stream , we show how translation performance can equal specialised , per-stream language models , but do this in a single language model using far less space . our results hold even when adding three billion tokens of additional text as a background language model .

ubbnbc wsd system description
the nave bayes classification proves to be a good performing tool in word sense disambiguation , although it has not yet been applied to the romanian language . the aim of this paper is to present our wsd system , based on the nbc algorithm , that performed quite well in senseval 3 .

a linguistic discovery program that verbalizes its discoveries
we describe a discovery program , called univauto ( universals authoringtool ) , whose domain of application is the study of language universals , a classic trend in contemporary linguistics . accepting as input information about languages , presented in terms of feature-values , the discoveries of another human agent arising from the same data , as well as some additional data , the program discovers the universals in the data , compares them with the discoveries of the human agent and , if appropriate , generates a report in english on its discoveries . running univauto on the data from the seminal paper of greenberg ( 1966 ) on word order universals , the system has produced several linguistically valuable texts , two of which are published in a refereed linguistic journal .

its time for a semantic engine
a common computational goal is to encapsulate the modeling of a target phenomenon within a unified and comprehensive engine , which addresses a broad range of the required processing tasks . this goal is followed in common modeling of the morphological and syntactic levels of natural language , where most processing tasks are encapsulated within morphological analyzers and syntactic parsers . in this talk i suggest that computational modeling of the semantic level should also focus on encapsulating the various processing tasks within a unified module ( engine ) . the input/output specification of such engine ( api ) can be based on the textual entailment paradigm , which will be described in brief and suggested as an attractive framework for applied semantic inference . the talk will illustrate an initial proposal for the engines api , designed to be embedded within the prominent language processing applications . finally , i will sketch the entailment formalism and efficient inference algorithm developed at bar-ilan university , which illustrates a principled transformational ( rather than interpretational ) approach towards developing a comprehensive semantic engine .

creating reverse bilingual dictionaries khang nhut lam
bilingual dictionaries are expensive resources and not many are available when one of the languages is resource-poor . in this paper , we propose algorithms for creation of new reverse bilingual dictionaries from existing bilingual dictionaries in which english is one of the two languages . our algorithms exploit the similarity between word-concept pairs using the english wordnet to produce reverse dictionary entries . since our algorithms rely on available bilingual dictionaries , they are applicable to any bilingual dictionary as long as one of the two languages has wordnet type lexical ontology .

language independent transliteration mining system using finite state
we propose a named entities transliteration mining system using finite state automata ( fsa ) . we compare the proposed approach with a baseline system that utilizes the editex technique to measure the length-normalized phonetic based edit distance between the two words . we submitted three standard runs in news2010 shared task and ranked first for english to arabic ( wm-enar ) and obtained an fmeasure of 0.915 , 0.903 , and 0.874 respectively .

on automated evaluation of readability of summaries :
readability of a summary is usually graded manually on five aspects of readability : grammaticality , coherence and structure , focus , referential clarity and non-redundancy . in the context of automated metrics for evaluation of summary quality , content evaluations have been presented through the last decade and continue to evolve , however a careful examination of readability aspects of summary quality has not been as exhaustive . in this paper we explore alternative evaluation metrics for grammaticality and coherence and structure that are able to strongly correlate with manual ratings . our results establish that our methods are able to perform pair-wise ranking of summaries based on grammaticality , as strongly as rouge is able to distinguish for content evaluations . we observed that none of the five aspects of readability are independent of each other , and hence by addressing the individual criterion of evaluation we aim to achieve automated appreciation of readability of summaries .

parma : a predicate argument
we introduce parma , a system for crossdocument , semantic predicate and argument alignment . our system combines a number of linguistic resources familiar to researchers in areas such as recognizing textual entailment and question answering , integrating them into a simple discriminative model . parma achieves state of the art results on an existing and a new dataset . we suggest that previous efforts have focussed on data that is biased and too easy , and we provide a more difficult dataset based on translation data with a low baseline which we beat by 17 % f1 .

evaluation of restricted domain question-answering systems
question-answering ( qa ) evaluation efforts have largely been tailored to open-domain systems . the trec qa test collections contain newswire articles and the accompanying queries cover a wide variety of topics . while some apprehension about the limitations of restricteddomain systems is no doubt justified , the strict promotion of unlimited domain qa evaluations may have some unintended consequences . simply applying the open domain qa evaluation paradigm to a restricted-domain system poses problems in the areas of test question development , answer key creation , and test collection construction . this paper examines the evaluation requirements of restricted domain systems . it incorporates evaluation criteria identified by users of an operational qa system in the aerospace engineering domain . while the paper demonstrates that user-centered task-based evaluations are required for restricted domain systems , these evaluations are found to be equally applicable to open domain systems .

atilf ( analyse et traitement jacques dendien atilf ( analyse et traitement atilf ( analyse et traitement
this paper presents one of the main computerized resources of the research laboratory atilf ( analyse et traitement informatique de la langue franaise ) available via the web : the computerized dictionary tlfi ( trsor de la langue franaise informatis ) . its highly detailed xml structure ( over 3,6 million tags ) is powered by stella : the extended capacities and potentialities of this software allows high level queries as well as hypernavigation through and between different databases .

improving a method for quantifying readers impressions of news articles with a regression equation
in this paper , we focus on the impressions that people gain from reading articles in japanese newspapers , and we propose a method for extracting and quantifying these impressions in real numbers . the target impressions are limited to those represented by three bipolar scales , happy sad , glad angry , and peaceful strained , and the strength of each impression is computed as a real number between 1 and 7. first , we implement a method for computing impression values of articles using an impression lexicon . this lexicon represents a correlation between the words appearing in articles and the influence of these words on the readers impressions , and is created from a newspaper database using a word co-occurrence based method . we considered that some gaps would occur between values computed by such an unsupervised method and those judged by the readers , and we conducted experiments with 900 subjects to identify what gaps actually occurred . consequently , we propose a new approach that uses regression equations to correct impression values computed by the method . our investigation shows that accuracy is improved by a range of 23.2 % to 42.7 % by using regression equations .

creating an annotated corpus for generating walking directions
this work describes first steps towards building a system that synchronously generates multimodal ( textual and visual ) route directions for pedestrians . we pursue a corpus-based approach for building a generation model that produces natural instructions in multiple languages . we conducted an empirical study to collect verbal route directions , and annotated the acquired texts on different levels . here we describe the experimental setting and an analysis of the collected data .

correcting a pos-tagged corpus using three complementary methods
the quality of the part-of-speech ( pos ) annotation in a corpus is crucial for the development of pos taggers . in this paper , we experiment with three complementary methods for automatically detecting errors in the pos annotation for the icelandic frequency dictionary corpus . the first two methods are language independent and we argue that the third method can be adapted to other morphologically complex languages . once possible errors have been detected , we examine each error candidate and hand-correct the corresponding pos tag if necessary . overall , based on the three methods , we handcorrect the pos tagging of 1,334 tokens ( 0.23 % of the tokens ) in the corpus . furthermore , we re-evaluate existing state-ofthe-art pos taggers on icelandic text using the corrected corpus .

evaluating the impact of alternative dependency graph encodings on solving event extraction tasks
in state-of-the-art approaches to information extraction ( ie ) , dependency graphs constitute the fundamental data structure for syntactic structuring and subsequent knowledge elicitation from natural language documents . the top-performing systems in the bionlp 2009 shared task on event extraction all shared the idea to use dependency structures generated by a variety of parsers either directly or in some converted manner and optionally modified their output to fit the special needs of ie . as there are systematic differences between various dependency representations being used in this competition , we scrutinize on different encoding styles for dependency information and their possible impact on solving several ie tasks . after assessing more or less established dependency representations such as the stanford and conll-x dependencies , we will then focus on trimming operations that pave the way to more effective ie . our evaluation study covers data from a number of constituency- and dependency-based parsers and provides experimental evidence which dependency representations are particularly beneficial for the event extraction task . based on empirical findings from our study we were able to achieve the performance of 57.2 % f-score on the development data set of the bionlp shared task 2009 .

a stochastic finite-state morphological parser for turkish hasim sak & tunga gung
this paper presents the first stochastic finite-state morphological parser for turkish . the non-probabilistic parser is a standard finite-state transducer implementation of two-level morphology formalism . a disambiguated text corpus of 200 million words is used to stochastize the morphotactics transducer , then it is composed with the morphophonemics transducer to get a stochastic morphological parser . we present two applications to evaluate the effectiveness of the stochastic parser ; spelling correction and morphology-based language modeling for speech recognition .

nlp and the humanities : the revival of an old liaison
this paper present an overview of some emerging trends in the application of nlp in the domain of the so-called digital humanities and discusses the role and nature of metadata , the annotation layer that is so characteristic of documents that play a role in the scholarly practises of the humanities . it is explained how metadata are the key to the added value of techniques such as text and link mining , and an outline is given of what measures could be taken to increase the chances for a bright future for the old ties between nlp and the humanities . there is no data like metadata !

confidence estimation for information extraction
information extraction techniques automatically create structured databases from unstructured data sources , such as the web or newswire documents . despite the successes of these systems , accuracy will always be imperfect . for many reasons , it is highly desirable to accurately estimate the confidence the system has in the correctness of each extracted field . the information extraction system we evaluate is based on a linear-chain conditional random field ( crf ) , a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary , overlapping features of the input in a markov model . we implement several techniques to estimate the confidence of both extracted fields and entire multi-field records , obtaining an average precision of 98 % for retrieving correct fields and 87 % for multi-field records .

using word-pair identifier to improve chinese input system
this paper presents a word-pair ( wp ) identifier that can be used to resolve homonym/segmentation ambiguities and perform syllable-to-word ( stw ) conversion effectively for improving chinese input systems . the experiment results show the following : ( 1 ) the wp identifier is able to achieve tonal ( syllables with four tones ) and toneless ( syllables without four tones ) stw accuracies of 98.5 % and 90.7 % , respectively , among the identified word-pairs ; ( 2 ) while applying the wp identifier , together with the microsoft input method editor 2003 and an optimized bigram model , the tonal and toneless stw improvements of the two input systems are 27.5 % /18.9 % and 22.1 % /18.8 % , respectively .

syntactic phrase reordering for english-to-arabic statistical machine
syntactic reordering of the source language to better match the phrase structure of the target language has been shown to improve the performance of phrase-based statistical machine translation . this paper applies syntactic reordering to english-to-arabic translation . it introduces reordering rules , and motivates them linguistically . it also studies the effect of combining reordering with arabic morphological segmentation , a preprocessing technique that has been shown to improve arabic-english and englisharabic translation . we report on results in the news text domain , the un text domain and in the spoken travel domain .

match : an architecture for multimodal dialogue systems
mobile interfaces need to allow the user and system to adapt their choice of communication modes according to user preferences , the task at hand , and the physical and social environment . we describe a multimodal application architecture which combines finite-state multimodal language processing , a speech-act based multimodal dialogue manager , dynamic multimodal output generation , and user-tailored text planning to enable rapid prototyping of multimodal interfaces with flexible input and adaptive output . our testbed application match ( multimodal access to city help ) provides a mobile multimodal speech-pen interface to restaurant and subway information for new york city .

improving email speech acts analysis via n-gram selection
in email conversational analysis , it is often useful to trace the the intents behind each message exchange . in this paper , we consider classification of email messages as to whether or not they contain certain intents or email-acts , such as propose a meeting or commit to a task . we demonstrate that exploiting the contextual information in the messages can noticeably improve email-act classification . more specifically , we describe a combination of n-gram sequence features with careful message preprocessing that is highly effective for this task . compared to a previous study ( cohen et al , 2004 ) , this representation reduces the classification error rates by 26.4 % on average . finally , we introduce ciranda : a new open source toolkit for email speech act prediction .

domain adaptation via pseudo in-domain data selection
we explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain . these sentences may be selected with simple cross-entropy based methods , of which we present three . as these sentences are not themselves identical to the in-domain data , we call them pseudo in-domain subcorpora . these subcorpora 1 % the size of the original can then used to train small domain-adapted statistical machine translation ( smt ) systems which outperform systems trained on the entire corpus . performance is further improved when we use these domain-adapted models in combination with a true in-domain model . the results show that more training data is not always better , and that best results are attained via proper domain-relevant data selection , as well as combining in- and general-domain systems during decoding .

left-corner transitions on dependency parsing
we propose a transition system for dependency parsing with a left-corner parsing strategy . unlike parsers with conventional transition systems , such as arc-standard or arc-eager , a parser with our system correctly predicts the processing difficulties people have , such as of center-embedding . we characterize our transition system by comparing its oracle behaviors with those of other transition systems on treebanks of 18 typologically diverse languages . a crosslinguistical analysis confirms the universality of the claim that a parser with our system requires less memory for parsing naturally occurring sentences .

using the wikipedia link structure to correct the wikipedia link
one of the valuable features of any collaboratively constructed semantic resource ( csr ) is its ability to as a system continuously correct itself . wikipedia is an excellent example of such a process , with vandalism and misinformation being removed or reverted in astonishing time by a coalition of human editors and machine bots . however , some errors are harder to spot than others , a problem which can lead to persistent unchecked errors , particularly on more obscure , less viewed article pages . in this paper we discuss the problems of incorrect link targets in wikipedia , and propose a method of automatically highlighting and correcting them using only the semantic information found in this encyclopaedias link structure .

unsupervised classification of sentiment and objectivity in chinese text
we address the problem of sentiment and objectivity classification of product reviews in chinese . our approach is distinctive in that it treats both positive / negative sentiment and subjectivity / objectivity not as distinct classes but rather as a continuum ; we argue that this is desirable from the perspective of would-be customers who read the reviews . we use novel unsupervised techniques , including a one-word 'seed ' vocabulary and iterative retraining for sentiment processing , and a criterion of 'sentiment density ' for determining the extent to which a document is opinionated . the classifier achieves up to 87 % f-measure for sentiment polarity detection .

latent mixture of discriminative experts for multimodal prediction modeling
during face-to-face conversation , people naturally integrate speech , gestures and higher level language interpretations to predict the right time to start talking or to give backchannel feedback . in this paper we introduce a new model called latent mixture of discriminative experts which addresses some of the key issues with multimodal language processing : ( 1 ) temporal synchrony/asynchrony between modalities , ( 2 ) micro dynamics and ( 3 ) integration of different levels of interpretation . we present an empirical evaluation on listener nonverbal feedback prediction ( e.g. , head nod ) , based on observable behaviors of the speaker . we confirm the importance of combining four types of multimodal features : lexical , syntactic structure , eye gaze , and prosody . we show that our latent mixture of discriminative experts model outperforms previous approaches based on conditional random fields ( crfs ) and latent-dynamic crfs .

thinkminers : disorder recognition using conditional random fields and distributional semantics ankur parikh avinesh pvs joy mustafi lalit agarwalla ashish mungi
in 2014 , semeval organized multiple challenges on natural language processing and information retrieval . one of the task was analysis of the clinical text . this challenge is further divided into two tasks . the task a of the challenge was to extract disorder mention spans in the clinical text and the task b was to map each of the disorder mentions to a unique unified medical language system concept unique identifier . we participated in the task a and developed a clinical disorder recognition system . the proposed system consists of a conditional random fields based approach to recognize disorder entities . the semeval challenge organizers manually annotated disorder entities in 298 clinical notes , of which 199 notes were used for training and 99 for development . on the test data , our system achieved the fmeasure of 0.844 for entity recognition in relaxed and 0.689 in strict evaluation .

understanding complex natural language explanations in tutorial
we describe the why2-atlas intelligent tutoring system for qualitative physics that interacts with students via natural language dialogue . we focus on the issue of analyzing and responding to multisentential explanations . we explore an approach that combines a statistical classifier , multiple semantic parsers and a formal reasoner for achieving a deeper understanding of these explanations in order to provide appropriate feedback on them .

graeme blackwood jamie brunning william byrne
this paper describes the cambridge university engineering department submission to the fifth workshop on statistical machine translation . we report results for the french-english and spanish-english shared translation tasks in both directions . the cued system is based on hifst , a hierarchical phrase-based decoder implemented using weighted finite-state transducers . in the french-english task , we investigate the use of context-dependent alignment models . we also show that lattice minimum bayes-risk decoding is an effective framework for multi-source translation , leading to large gains in bleu score .

formalism-independent parser evaluation with ccg and depbank
a key question facing the parsing community is how to compare parsers which use different grammar formalisms and produce different output . evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance . in this paper we evaluate a ccg parser on depbank , and demonstrate the difficulties in converting the parser output into depbank grammatical relations . in addition we present a method for measuring the effectiveness of the conversion , which provides an upper bound on parsing accuracy . the ccg parser obtains an f-score of 81.9 % on labelled dependencies , against an upper bound of 84.8 % . we compare the ccg parser against the rasp parser , outperforming rasp by over 5 % overall and on the majority of dependency types .

global topology of word co-occurrence networks : beyond the two-regime power-law
word co-occurrence networks are one of the most common linguistic networks studied in the past and they are known to exhibit several interesting topological characteristics . in this article , we investigate the global topological properties of word co-occurrence networks and , in particular , present a detailed study of their spectrum . our experiments reveal certain universal trends found across the networks for seven different languages from three different language families , which are neither reported nor explained by any of the previous studies and models of word-cooccurrence networks . we hypothesize that since word co-occurrences are governed by syntactic properties of a language , the network has much constrained topology than that predicted by the previously proposed growth model . a deeper empirical and theoretical investigation into the evolution of these networks further suggests that they have a coreperiphery structure , where the core hardly evolves with time and new words are only attached to the periphery of the network . these properties are fundamental to the nature of word co-occurrence across languages .

machine translation with lattices and forests
traditional 1-best translation pipelines suffer a major drawback : the errors of 1best outputs , inevitably introduced by each module , will propagate and accumulate along the pipeline . in order to alleviate this problem , we use compact structures , lattice and forest , in each module instead of 1-best results . we integrate both lattice and forest into a single tree-to-string system , and explore the algorithms of lattice parsing , lattice-forest-based rule extraction and decoding . more importantly , our model takes into account all the probabilities of different steps , such as segmentation , parsing , and translation . the main advantage of our model is that we can make global decision to search for the best segmentation , parse-tree and translation in one step . medium-scale experiments show an improvement of +0.9 bleu points over a state-of-the-art forest-based baseline .

multiple aspect ranking using the good grief algorithm
we address the problem of analyzing multiple related opinions in a text . for instance , in a restaurant review such opinions may include food , ambience and service . we formulate this task as a multiple aspect ranking problem , where the goal is to produce a set of numerical scores , one for each aspect . we present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks . this algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions , such as agreement and contrast . we prove that our agreementbased joint model is more expressive than individual ranking models . our empirical results further confirm the strength of the model : the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model .

a rose is a roos is a ruusu : querying translations for web image search
we query web image search engines with words ( e.g. , spring ) but need images that correspond to particular senses of the word ( e.g. , flexible coil ) . querying with polysemous words often yields unsatisfactory results from engines such as google images . we build an image search engine , idiom , which improves the quality of returned images by focusing search on the desired sense . our algorithm , instead of searching for the original query , searches for multiple , automatically chosen translations of the sense in several languages . experimental results show that idiom outperforms google images and other competing algorithms returning 22 % more relevant images .

an evaluation exercise for romanian word sense disambiguation for artificial intelligence
this paper presents the task definition , resources , participating systems , and comparative results for a romanian word sense disambiguation task , which was organized as part of the senseval-3 evaluation exercise . five teams with a total of seven systems were drawn to this task .

discourse level opinion interpretation intelligent systems program
this work proposes opinion frames as a representation of discourse-level associations which arise from related opinion topics . we illustrate how opinion frames help gather more information and also assist disambiguation . finally we present the results of our experiments to detect these associations .

construction of a chinese idiom knowledge base and its applications linguistics of ministry of education linguistics of ministry of education ,
idioms are not only interesting but also distinctive in a language for its continuity and metaphorical meaning in its context . this paper introduces the construction of a chinese idiom knowledge base by the institute of computational linguistics at peking university and describes an experiment that aims at the automatic emotion classification of chinese idioms . in the process , we expect to know more about how the constituents in a fossilized composition like an idiom function so as to affect its semantic or grammatical properties . as an important chinese language resource , our idiom knowledge base will play a major role in applications such as linguistic research , teaching chinese as a foreign language and even as a tool for preserving this non-material chinese cultural and historical heritage .

improving decoding generalization for tree-to-string translation
to address the parse error issue for tree-tostring translation , this paper proposes a similarity-based decoding generation ( sdg ) solution by reconstructing similar source parse trees for decoding at the decoding time instead of taking multiple source parse trees as input for decoding . experiments on chinese-english translation demonstrated that our approach can achieve a significant improvement over the standard method , and has little impact on decoding speed in practice . our approach is very easy to implement , and can be applied to other paradigms such as tree-to-tree models .

speech retrieval in unknown languages : a pilot study
most cross-lingual speech retrieval assumes intensive knowledge about all involved languages . however , such resource may not exist for some less popular languages . some applications call for speech retrieval in unknown languages . in this work , we leverage on a quasi-language-independent subword recognizer trained on multiple languages , to obtain an abstracted representation of speech data in an unknown language . languageindependent query expansion is achieved either by allowing a wide lattice output for an audio query , or by taking advantage of distinctive features in speech articulation to propose subwords most similar to the given subwords in a query . we propose using a retrieval model based on finite state machines for fuzzy matching of speech sound patterns , and further for speech retrieval . a pilot study of speech retrieval in unknown languages is presented , using english , spanish and russian as training languages , and croatian as the unknown target language .

automatically creating datasets for measures of semantic relatedness
semantic relatedness is a special form of linguistic distance between words . evaluating semantic relatedness measures is usually performed by comparison with human judgments . previous test datasets had been created analytically and were limited in size . we propose a corpus-based system for automatically creating test datasets.1 experiments with human subjects show that the resulting datasets cover all degrees of relatedness . as a result of the corpus-based approach , test datasets cover all types of lexical-semantic relations and contain domain-specific words naturally occurring in texts .

a preliminary study on probabilistic models for chinese abbreviations
chinese abbreviations are widely used in the modern chinese texts . they are a special form of unknown words , including many named entities . this results in difficulty for correct chinese processing . in this study , the chinese abbreviation problem is regarded as an error recovery problem in which the suspect root words are the errors to be recovered from a set of candidates . such a problem is mapped to an hmm-based generation model for both abbreviation identification and root word recovery , and is integrated as part of a unified word segmentation model when the input extends to a complete sentence . two major experiments are conducted to test the abbreviation models . in the first experiment , an attempt is made to guess the abbreviations of the root words . an accuracy rate of 72 % is observed . in contrast , a second experiment is conducted to guess the root words from abbreviations . some submodels could achieve as high as 51 % accuracy with the simple hmm-based model .

computational lexicography : a feature-based approach in designing an e-dictionary of chinese classifiers
chinese noun classifiers are obligatory as a category in association with nouns . conventional dictionaries include classifiers as lexical entries but explanations given are very brief and thus hardly helpful for l2 learners . this paper presents a new design of an e-dictionary of chinese classifiers . the design is based on both theoretical studies of chinese classifiers and empirical studies of chinese classifier acquisition by both children and adults . my main argument with regards to chinese classifier acquisition is that cognitive strategies with a bottom-up approach are the key to the understanding of the complexity of classifier and noun associations . the noun-dependent semantic features of classifiers are evidence to support my argument . these features are categorically defined and stored in a separated database in an e-learning environment linked to the e-dictionary . the aim of making such a design is to provide a platform for l2 learners to explore and learn with a bottom-up approach the associations of classifiers with nouns . the computational agent-based model that automatically links noun features to that of classifiers is the technical part of the design that will be described in detail in the paper . future development of the e-dictionary will be discussed as well .

off-topic detection in conversational telephone speech
in a context where information retrieval is extended to spoken documents including conversations , it will be important to provide users with the ability to seek informational content , rather than socially motivated small talk that appears in many conversational sources . in this paper we present a preliminary study aimed at automatically identifying irrelevance in the domain of telephone conversations . we apply a standard machine learning algorithm to build a classifier that detects offtopic sections with better-than-chance accuracy and that begins to provide insight into the relative importance of features for identifying utterances as on topic or not .

detecting the noteworthiness of utterances in human meetings
our goal is to make note-taking easier in meetings by automatically detecting noteworthy utterances in verbal exchanges and suggesting them to meeting participants for inclusion in their notes . to show feasibility of such a process we conducted a wizard of oz study where the wizard picked automatically transcribed utterances that he judged as noteworthy , and suggested their contents to the participants as notes . over 9 meetings , participants accepted 35 % of these suggestions . further , 41.5 % of their notes at the end of the meeting contained wizard-suggested text . next , in order to perform noteworthiness detection automatically , we annotated a set of 6 meetings with a 3-level noteworthiness annotation scheme , which is a break from the binary in summary/ not in summary labeling typically used in speech summarization . we report kappa of 0.44 for the 3way classification , and 0.58 when two of the 3 labels are merged into one . finally , we trained an svm classifier on this annotated data ; this classifiers performance lies between that of trivial baselines and inter-annotator agreement .

learning to predict case markers in japanese
japanese case markers , which indicate the grammatical relation of the complement np to the predicate , often pose challenges to the generation of japanese text , be it done by a foreign language learner , or by a machine translation ( mt ) system . in this paper , we describe the task of predicting japanese case markers and propose machine learning methods for solving it in two settings : ( i ) monolingual , when given information only from the japanese sentence ; and ( ii ) bilingual , when also given information from a corresponding english source sentence in an mt context . we formulate the task after the well-studied task of english semantic role labelling , and explore features from a syntactic dependency structure of the sentence . for the monolingual task , we evaluated our models on the kyoto corpus and achieved over 84 % accuracy in assigning correct case markers for each phrase . for the bilingual task , we achieved an accuracy of 92 % per phrase using a bilingual dataset from a technical domain . we show that in both settings , features that exploit dependency information , whether derived from gold-standard annotations or automatically assigned , contribute significantly to the prediction of case markers.1

nbest dependency parsing with linguistically rich models
we try to improve the classifier-based deterministic dependency parsing in two ways : by introducing a better search method based on a non-deterministic nbest algorithm and by devising a series of linguistically richer models . it is experimentally shown on a conll 2007 shared task that this results in a system with higher performance while still keeping it simple enough for an efficient implementation .

using very simple statistics for review search : an exploration
we report on work in progress on using very simple statistics in an unsupervised fashion to re-rank search engine results when review-oriented queries are issued ; the goal is to bring opinionated or subjective results to the top of the results list . we find that our proposed technique performs comparably to methods that rely on sophisticated pre-encoded linguistic knowledge , and that both substantially improve the initial results produced by the yahoo ! search engine .

the ( non ) utility of predicate-argument frequencies for pronoun
state-of-the-art pronoun interpretation systems rely predominantly on morphosyntactic contextual features . while the use of deep knowledge and inference to improve these models would appear technically infeasible , previous work has suggested that predicate-argument statistics mined from naturally-occurring data could provide a useful approximation to such knowledge . we test this idea in several system configurations , and conclude from our results and subsequent error analysis that such statistics offer little or no predictive information above that provided by morphosyntax .

enhancing linguistically oriented automatic keyword extraction
this paper presents experiments on how the performance of automatic keyword extraction can be improved , as measured by keywords previously assigned by professional indexers . the keyword extraction algorithm consists of three prediction models that are combined to decide what words or sequences of words in the documents are suitable as keywords . the models , in turn , are built using different definitions of what constitutes a term in a written document .

unsupervised multiword segmentation of large corpora using prediction-driven decomposition of n-grams
we present a new , efficient unsupervised approach to the segmentation of corpora into multiword units . our method involves initial decomposition of common n-grams into segments which maximize within-segment predictability of words , and then further refinement of these segments into a multiword lexicon . evaluating in four large , distinct corpora , we show that this method creates segments which correspond well to known multiword expressions ; our model is particularly strong with regards to longer ( 3+ word ) multiword units , which are often ignored or minimized in relevant work .

a rule-augmented statistical phrase-based translation system cong duy vu hoang
interactive or incremental statistical machine translation ( imt ) aims to provide a mechanism that allows the statistical models involved in the translation process to be incrementally updated and improved . the source of knowledge normally comes from users who either post-edit the entire translation or just provide the translations for wrongly translated domain-specific terminologies . most of the existing work on imt uses batch learning paradigm which does not allow translation systems to make use of the new input instantaneously . we introduce an adaptive mt framework with a rule definition language ( rdl ) for users to amend mt results through translation rules or patterns . experimental results show that our system acknowledges user feedback via rdl which improves the translations of the baseline system on three test sets for vietnamese to english translation .

incremental structured prediction using a global learning and
this tutorial discusses a framework for incremental left-to-right structured predication , which makes use of global discriminative learning and beam-search decoding . the method has been applied to a wide range of nlp tasks in recent years , and

a brazilian portuguese phonological-prosodic algorithm applied to language acquisition : a case study
the paper presents a system for transcribing and annotating phonological information in brazilian portuguese , including syllabification . an application of this system for the assessment of language understanding and production is described , following a child longitudinally , comparing expected production with observed production .

improving pivot-based statistical machine translation using random walk
this paper proposes a novel approach that utilizes a machine learning method to improve pivot-based statistical machine translation ( smt ) . for language pairs with few bilingual data , a possible solution in pivot-based smt using another language as a `` bridge '' to generate source-target translation . however , one of the weaknesses is that some useful sourcetarget translations can not be generated if the corresponding source phrase and target phrase connect to different pivot phrases . to alleviate the problem , we utilize markov random walks to connect possible translation phrases between source and target language . experimental results on european parliament data , spoken language data and web data show that our method leads to significant improvements on all the tasks over the baseline system .

reformatting web documents via header trees
we propose a new method for reformatting web documents by extracting semantic structures from web pages . our approach is to extract trees that describe hierarchical relations in documents . we developed an algorithm for this task by employing the em algorithm and clustering techniques . preliminary experiments showed that our approach was more effective than baseline methods .

factored statistical machine translation for grammatical error
this paper describes our ongoing work on grammatical error correction ( gec ) . focusing on all possible error types in a real-life environment , we propose a factored statistical machine translation ( smt ) model for this task . we consider error correction as a series of language translation problems guided by various linguistic information , as factors that influence translation results . factors included in our study are morphological information , i.e . word stem , prefix , suffix , and part-of-speech ( pos ) information . in addition , we also experimented with different combinations of translation models ( tm ) , phrase-based and factor-based , trained on various datasets to boost the overall performance . empirical results show that the proposed model yields an improvement of 32.54 % over a baseline phrase-based smt model . the system participated in the conll 2014 shared task and achieved the 7 th and 5 th f0.5 scores 1 on the official test set among the thirteen participating teams .

gertjan van noord
in our attempts to construct a wide coverage hpsg parser for dutch , techniques to improve the overall robustness of the parser are required at various steps in the parsing process . straightforward but important aspects include the treatment of unknown words , and the treatment of input for which no full parse is available . another important means to improve the parser 's performance on unexpected input is the ability to learn from your errors . in our methodology we apply the parser to large quantities of text ( preferably from different types of corpora ) , and we then apply error mining techniques to identify potential errors , and furthermore we apply machine learning techniques to correct some of those errors ( semi- ) automatically , in particular those errors that are due to missing or incomplete lexical entries . evaluating the robustness of a parser is notoriously hard . we argue against coverage as a meaningful evaluation metric . more generally , we argue against evaluation metrics that do not take into account accuracy . we propose to use variance of accuracy across sentences ( and more generally across corpora ) as a measure for robustness .

parsing with generative models of predicate-argument structure
the model used by the ccg parser of hockenmaier and steedman ( 2002b ) would fail to capture the correct bilexical dependencies in a language with freer word order , such as dutch . this paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure , as in the model of clark et al ( 2002 ) , and defines a generative model for ccg derivations that captures these dependencies , including bounded and unbounded long-range dependencies .

may all your wishes come true : a study of wishes and how to recognize them
a wish is a desire or hope for something to happen . in december 2007 , people from around the world offered up their wishes to be printed on confetti and dropped from the sky during the famous new years eve ball drop in new york citys times square . we present an in-depth analysis of this collection of wishes . we then leverage this unique resource to conduct the first study on building general wish detectors for natural language text . wish detection complements traditional sentiment analysis and is valuable for collecting business intelligence and insights into the worlds wants and desires . we demonstrate the wish detectors effectiveness on domains as diverse as consumer product reviews and online political discussions .

reformulating discourse connectives for non-expert readers advaith siddharthan napoleon katsos
in this paper we report a behavioural experiment documenting that different lexicosyntactic formulations of the discourse relation of causation are deemed more or less acceptable by different categories of readers . we further report promising results for automatically selecting the formulation that is most appropriate for a given category of reader using supervised learning . this investigation is embedded within a longer term research agenda aimed at summarising scientific writing for lay readers using appropriate paraphrasing .

a joint model of word segmentation and phonological variation for
word-final /t/-deletion refers to a common phenomenon in spoken english where words such as /west/ west are pronounced as [ wes ] wes in certain contexts . phonological variation like this is common in naturally occurring speech . current computational models of unsupervised word segmentation usually assume idealized input that is devoid of these kinds of variation . we extend a non-parametric model of word segmentation by adding phonological rules that map from underlying forms to surface forms to produce a mathematically well-defined joint model as a first step towards handling variation and segmentation in a single model . we analyse how our model handles /t/-deletion on a large corpus of transcribed speech , and show that the joint model can perform word segmentation and recover underlying /t/s . we find that bigram dependencies are important for performing well on real data and for learning appropriate deletion probabilities for different contexts.1

urdu and hindi : translation and sharing of linguistic resources
hindi and urdu share a common phonology , morphology and grammar but are written in different scripts . in addition , the vocabularies have also diverged significantly especially in the written form . in this paper we show that we can get reasonable quality translations ( we estimated the translation error rate at 18 % ) between the two languages even in absence of a parallel corpus . linguistic resources such as treebanks , part of speech tagged data and parallel corpora with english are limited for both these languages . we use the translation system to share linguistic resources between the two languages . we demonstrate improvements on three tasks and show : statistical machine translation from urdu to english is improved ( 0.8 in bleu score ) by using a hindi-english parallel corpus , hindi part of speech tagging is improved ( upto 6 % absolute ) by using an urdu part of speech corpus and a hindi-english word aligner is improved by using a manually word aligned urduenglish corpus ( upto 9 % absolute in fmeasure ) .

an empirical study of the impact of idioms on phrase based statistical machine translation of english to brazilian-portuguese
this paper describes an experiment to evaluate the impact of idioms on statistical machine translation ( smt ) process using the language pair english/brazilianportuguese . our results show that on sentences containing idioms a standard smt system achieves about half the bleu score of the same system when applied to sentences that do not contain idioms . we also provide a short error analysis and outline our planned work to overcome this limitation .

multilingual authoring : the namic approach via di tor vergata ,
with increasing amounts of electronic information available , and the increase in the variety of languages used to produce documents of the same type , the problem of how to manage similar documents in different languages arises . this paper proposes an approach to processing/structuring text so that multilingual authoring ( creating hypertext links ) can be effectively carried out . this work , funded by the european union , is applied to the multilingual authoring of news agency text . we have applied methods from natural language processing , especially information extraction technology , to both monolingual and multilingual authoring .

discriminative features in reversible stochastic attribute-value grammars
reversible stochastic attribute-value grammars ( de kok et al , 2011 ) use one model for parse disambiguation and fluency ranking . such a model encodes preferences with respect to syntax , fluency , and appropriateness of logical forms , as weighted features . reversible models are built on the premise that syntactic preferences are shared between parse disambiguation and fluency ranking . given that reversible models also use features that are specific to parsing or generation , there is the possibility that the model is trained to rely on these directional features . if this is true , the premise that preferences are shared between parse disambiguation and fluency ranking does not hold . in this work , we compare and apply feature selection techniques to extract the most discriminative features from directional and reversible models . we then analyse the contributions of different classes of features , and show that reversible models do rely on task-independent features .

analysis of statistical and morphological classes to generate weighted reordering hypotheses on a statistical machine translation system
one main challenge of statistical machine translation ( smt ) is dealing with word order . the main idea of the statistical machine reordering ( smr ) approach is to use the powerful techniques of smt systems to generate a weighted reordering graph for smt systems . this technique supplies reordering constraints to an smt system , using statistical criteria . in this paper , we experiment with different graph pruning which guarantees the translation quality improvement due to reordering at a very low increase of computational cost . the smr approach is capable of generalizing reorderings , which have been learned during training , by using word classes instead of words themselves . we experiment with statistical and morphological classes in order to choose those which capture the most probable reorderings . satisfactory results are reported in the wmt07 es/en task . our system outperforms in terms of bleu the wmt07 official baseline system .

hedgehunter : a system for hedge detection and uncertainty
with the dramatic growth of scientific publishing , information extraction ( ie ) systems are becoming an increasingly important tool for large scale data analysis . hedge detection and uncertainty classification are important components of a high precision ie system . this paper describes a two part supervised system which classifies words as hedge or nonhedged and sentences as certain or uncertain in biomedical and wikipedia data . in the first stage , our system trains a logistic regression classifier to detect hedges based on lexical and part-of-speech collocation features . in the second stage , we use the output of the hedge classifier to generate sentence level features based on the number of hedge cues , the identity of hedge cues , and a bag-of-words feature vector to train a logistic regression classifier for sentence level uncertainty . with the resulting classification , an ie system can then discard facts and relations extracted from these sentences or treat them as appropriately doubtful . we present results for in domain training and testing and cross domain training and testing based on a simple union of training sets .

combining linguistic and machine learning techniques for email
this paper shows that linguistic techniques along with machine learning can extract high quality noun phrases for the purpose of providing the gist or summary of email messages . we describe a set of comparative experiments using several machine learning algorithms for the task of salient noun phrase extraction . three main conclusions can be drawn from this study : ( i ) the modifiers of a noun phrase can be semantically as important as the head , for the task of gisting , ( ii ) linguistic filtering improves the performance of machine learning algorithms , ( iii ) a combination of classifiers improves accuracy .

comparing rating scales and preference judgements in language evaluation anja belz eric kow
rating-scale evaluations are common in nlp , but are problematic for a range of reasons , e.g . they can be unintuitive for evaluators , inter-evaluator agreement and self-consistency tend to be low , and the parametric statistics commonly applied to the results are not generally considered appropriate for ordinal data . in this paper , we compare rating scales with an alternative evaluation paradigm , preferencestrength judgement experiments ( pjes ) , where evaluators have the simpler task of deciding which of two texts is better in terms of a given quality criterion . we present three pairs of evaluation experiments assessing text fluency and clarity for different data sets , where one of each pair of experiments is a rating-scale experiment , and the other is a pje . we find the pje versions of the experiments have better evaluator self-consistency and interevaluator agreement , and a larger proportion of variation accounted for by system differences , resulting in a larger number of significant differences being found .

automatic allocation of training data for rapid prototyping of speech understanding based on multiple model combination
the optimal choice of speech understanding method depends on the amount of training data available in rapid prototyping . a statistical method is ultimately chosen , but it is not clear at which point in the increase in training data a statistical method become effective . our framework combines multiple automatic speech recognition ( asr ) and language understanding ( lu ) modules to provide a set of speech understanding results and selects the best result among them . the issue is how to allocate training data to statistical modules and the selection module in order to avoid overfitting in training and obtain better performance . this paper presents an automatic training data allocation method that is based on the change in the coefficients of the logistic regression functions used in the selection module . experimental evaluation showed that our allocation method outperformed baseline methods that use a single asr module and a single lu module at every point while training data increase .

semi-supervised conditional random fields for improved sequence
we present a new semi-supervised training procedure for conditional random fields ( crfs ) that can be used to train sequence segmentors and labelers from a combination of labeled and unlabeled training data . our approach is based on extending the minimum entropy regularization framework to the structured prediction case , yielding a training objective that combines unlabeled conditional entropy with labeled conditional likelihood . although the training objective is no longer concave , it can still be used to improve an initial model ( e.g . obtained from supervised training ) by iterative ascent . we apply our new training algorithm to the problem of identifying gene and protein mentions in biological texts , and show that incorporating unlabeled data improves the performance of the supervised crf in this case .

word sense disambiguation improves statistical machine translation yee seng chan and hwee tou ng
recent research presents conflicting evidence on whether word sense disambiguation ( wsd ) systems can help to improve the performance of statistical machine translation ( mt ) systems . in this paper , we successfully integrate a state-of-the-art wsd system into a state-of-the-art hierarchical phrase-based mt system , hiero . we show for the first time that integrating a wsd system improves the performance of a state-ofthe-art statistical mt system on an actual translation task . furthermore , the improvement is statistically significant .

unsupervised segmentation of words using prior distributions of morph
we present a language-independent and unsupervised algorithm for the segmentation of words into morphs . the algorithm is based on a new generative probabilistic model , which makes use of relevant prior information on the length and frequency distributions of morphs in a language . our algorithm is shown to outperform two competing algorithms , when evaluated on data from a language with agglutinative morphology ( finnish ) , and to perform well also on english data .

jumping distance based chinese person name disambiguation
in this paper , we describe a chinese person name disambiguation system for news articles and report the results obtained on the data set of the clp 2010 bakeoff-31 . the main task of the bakeoff is to identify different persons from the news stories that contain the same person-name string . compared to the traditional methods , two additional features are used in our system : 1 ) n-grams co-occurred with target name string ; 2 ) jumping distance among the n-grams . on the basis , we propose a two-stage clustering algorithm to improve the low recall .

infomagnets : making sense of corpus data jaime arguello carolyn ros
we introduce a new interactive corpus exploration tool called infomagnets . infomagnets aims at making exploratory corpus analysis accessible to researchers who are not experts in text mining . as evidence of its usefulness and usability , it has been used successfully in a research context to uncover relationships between language and behavioral patterns in two distinct domains : tutorial dialogue ( kumar et al , submitted ) and on-line communities ( arguello et al , 2006 ) . as an educational tool , it has been used as part of a unit on protocol analysis in an educational research methods course .

weakly-supervised learning with cost-augmented contrastive estimation
we generalize contrastive estimation in two ways that permit adding more knowledge to unsupervised learning . the first allows the modeler to specify not only the set of corrupted inputs for each observation , but also how bad each one is . the second allows specifying structural preferences on the latent variable used to explain the observations . they require setting additional hyperparameters , which can be problematic in unsupervised learning , so we investigate new methods for unsupervised model selection and system combination . we instantiate these ideas for part-of-speech induction without tag dictionaries , improving over contrastive estimation as well as strong benchmarks from the pascal 2012 shared task .

a bottom-up merging algorithm for chinese unknown word extraction
statistical methods for extracting chinese unknown words usually suffer a problem that superfluous character strings with strong statistical associations are extracted as well . to solve this problem , this paper proposes to use a set of general morphological rules to broaden the coverage and on the other hand , the rules are appended with different linguistic and statistical constraints to increase the precision of the representation . to disambiguate rule applications and reduce the complexity of the rule matching , a bottom-up merging algorithm for extraction is proposed , which merges possible morphemes recursively by consulting above the general rules and dynamically decides which rule should be applied first according to the priorities of the rules . effects of different priority strategies are compared in our experiment , and experimental results show that the performance of proposed method is very promising .

designing testsuites for grammar-based systems in applications
in complex grammar-based systems , even small changes may have an unforeseeable impact on overall system performance . regression testing of the system and its components becomes crucial for the grammar engineers developing the system . as part of this regression testing , the testsuites themselves must be designed to accurately assess coverage and progress and to help rapidly identify problems . we describe a system of passage-query pairs divided into three types of phenomenon-based testsuites ( sanity , query , basic correct ) . these allow for rapid development and for specific coverage assessment . in addition , real-world testsuites allow for overall performance and coverage assessment . these testsuites are used in conjunction with the more traditional representation-based regression testsuites used by grammar engineers .

preliminary lexical framework for english-arabic semantic resource construction
this paper describes preliminary work concerning the creation of a framework to aid in lexical semantic resource construction . the framework consists of 9 stages during which various lexical resources are collected , studied , and combined into a single combinatory lexical resource . to evaluate the general framework it was applied to a small set of english and arabic resources , automatically combining them into a single lexical knowledge base that can be used for query translation and disambiguation in crosslanguage information retrieval .

koosho : japanese text input environment based on aerial hand writing
hand gesture-based input systems have been in active research , yet most of them focus only on single character recognition . we propose koosho : an environment for japanese input based on aerial hand gestures . the system provides an integrated experience of character input , kana-kanji conversion , and search result visualization . to achieve faster input , users only have to input consonant , which is then converted directly to kanji sequences by direct consonant decoding . the system also shows suggestions to complete the user input . the comparison with voice recognition and a screen keyboard showed that koosho can be a more practical solution compared to the existing system .

the sentimental factor : improving review classification via philip
sentiment classification is the task of labeling a review document according to the polarity of its prevailing opinion ( favorable or unfavorable ) . in approaching this problem , a model builder often has three sources of information available : a small collection of labeled documents , a large collection of unlabeled documents , and human understanding of language . ideally , a learning method will utilize all three sources . to accomplish this goal , we generalize an existing procedure that uses the latter two . we extend this procedure by re-interpreting it as a naive bayes model for document sentiment . viewed as such , it can also be seen to extract a pair of derived features that are linearly combined to predict sentiment . this perspective allows us to improve upon previous methods , primarily through two strategies : incorporating additional derived features into the model and , where possible , using labeled data to estimate their relative influence .

a projection extension algorithm for statistical machine translation
in this paper , we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrasebased models . the units of translation are blocks pairs of phrases . during decoding , we use a block unigram model and a word-based trigram language model . during training , the blocks are learned from source interval projections using an underlying high-precision word alignment . the system performance is significantly increased by applying a novel block extension algorithm using an additional highrecall word alignment . the blocks are further filtered using unigram-count selection criteria . the system has been successfully test on a chinese-english and an arabicenglish translation task .

handling information access dialogue through qa technologies a novel challenge for open-domain question answering
a novel challenge for evaluating open-domain question answering technologies is proposed . in this challenge , question answering systems are supposed to be used interactively to answer a series of related questions , whereas in the conventional setting , systems answer isolated questions one by one . such an interaction occurs in the case of gathering information for a report on a specific topic , or when browsing information of interest to the user . in this paper , first , we explain the design of the challenge . we then discuss its reality and show how the capabilities measured by the challenge are useful and important in practical situations , and that the difficulty of the challenge is proper for evaluating the current state of open-domain question answering technologies .

towards an optimal lexicalization in a natural-sounding portable natural language generator for dialog systems
in contrast to the latest progress in speech recognition , the state-of-the-art in natural language generation for spoken language dialog systems is lagging behind . the core dialog managers are now more sophisticated ; and natural-sounding and flexible output is expected , but not achieved with current simple techniques such as template-based systems . portability of systems across subject domains and languages is another increasingly important requirement in dialog systems . this paper presents an outline of legend , a system that is both portable and generates natural-sounding output . this goal is achieved through the novel use of existing lexical resources such as framenet and wordnet .

stream-based randomised language models for smt
randomised techniques allow very big language models to be represented succinctly . however , being batch-based they are unsuitable for modelling an unbounded stream of language whilst maintaining a constant error rate . we present a novel randomised language model which uses an online perfect hash function to efficiently deal with unbounded text streams . translation experiments over a text stream show that our online randomised model matches the performance of batch-based lms without incurring the computational overhead associated with full retraining . this opens up the possibility of randomised language models which continuously adapt to the massive volumes of texts published on the web each day .

al-bayan : an arabic question answering system for the holy quran heba abdelnasser reham mohamed maha ragab alaa mohamed bassant farouk nagwa el-makky
recently , question answering ( qa ) has been one of the main focus of natural language processing research . however , arabic question answering is still not in the mainstream . the challenges of the arabic language and the lack of resources have made it difficult to provide arabic qa systems with high accuracy . while low accuracies may be accepted for general purpose systems , it is critical in some fields such as religious affairs . therefore , there is a need for specialized accurate systems that target these critical fields . in this paper , we propose al-bayan , a new arabic qa system specialized for the holy quran . the system accepts an arabic question about the quran , retrieves the most relevant quran verses , then extracts the passage that contains the answer from the quran and its interpretation books ( tafseer ) . evaluation results on a collected dataset show that the overall system can achieve 85 % accuracy using the top-3 results .

grammatical error detection and correction using a single maximum entropy model
this paper describes the system of shanghai jiao tong unvierity team in the conll-2014 shared task . error correction operations are encoded as a group of predefined labels and therefore the task is formulized as a multi-label classification task . for training , labels are obtained through a strict rule-based approach . for decoding , errors are detected and corrected according to the classification results . a single maximum entropy model is used for the classification implementation incorporated with an improved feature selection algorithm . our system achieved precision of 29.83 , recall of 5.16 and f 0.5 of 15.24 in the official evaluation .

determining recurrent sound correspondences by inducing translation models
i present a novel approach to the determination of recurrent sound correspondences in bilingual wordlists . the idea is to relate correspondences between sounds in wordlists to translational equivalences between words in bitexts ( bilingual corpora ) . my method induces models of sound correspondence that are similar to models developed for statistical machine translation . the experiments show that the method is able to determine recurrent sound correspondences in bilingual wordlists in which less than 30 % of the pairs are cognates . by employing the discovered correspondences , the method can identify cognates with higher accuracy than the previously reported algorithms .

tense and aspect assignment in narrative discourse
we describe a method for assigning english tense and aspect in a system that realizes surface text for symbolically encoded narratives . our testbed is an encoding interface in which propositions that are attached to a timeline must be realized from several temporal viewpoints . this involves a mapping from a semantic encoding of time to a set of tense/aspect permutations . the encoding tool realizes each permutation to give a readable , precise description of the narrative so that users can check whether they have correctly encoded actions and statives in the formal representation . our method selects tenses and aspects for individual event intervals as well as subintervals ( with multiple reference points ) , quoted and unquoted speech ( which reassign the temporal focus ) , and modal events such as conditionals .

dialogue strategy learning in healthcare : a systematic approach for learning dialogue models from data
we aim to build dialogue agents that optimize the dialogue strategy , specifically through learning the dialogue model components from dialogue data . in this paper , we describe our current research on automatically learning dialogue strategies in the healthcare domain . we go through our systematic approach of learning dialogue model components from data , specifically user intents and the user model , as well as the agent reward function . we demonstrate our experiments on healthcare data from which we learned the dialogue model components . we conclude by describing our current research for automatically learning dialogue features that can be used in representing dialogue states and learning the reward function .

a generative blog post retrieval model that uses query expansion based on external collections
user generated content is characterized by short , noisy documents , with many spelling errors and unexpected language usage . to bridge the vocabulary gap between the users information need and documents in a specific user generated content environment , the blogosphere , we apply a form of query expansion , i.e. , adding and reweighing query terms . since the blogosphere is noisy , query expansion on the collection itself is rarely effective but external , edited collections are more suitable . we propose a generative model for expanding queries using external collections in which dependencies between queries , documents , and expansion documents are explicitly modeled . different instantiations of our model are discussed and make different ( in ) dependence assumptions . results using two external collections ( news andwikipedia ) show that external expansion for retrieval of user generated content is effective ; besides , conditioning the external collection on the query is very beneficial , and making candidate expansion terms dependent on just the document seems sufficient .

computational analysis to explore authors depiction of characters cecilia ovesdotter alm
this study involves automatically identifying the sociolinguistic characteristics of fictional characters in plays by analyzing their written speech . we discuss three binary classification problems : predicting the characters gender ( male vs. female ) , age ( young vs. old ) , and socioeconomic standing ( upper-middle class vs. lower class ) . the text corpus used is an annotated collection of august strindberg and henrik ibsen plays , translated into english , which are in the public domain . these playwrights were chosen for their known attention to relevant socioeconomic issues in their work . linguistic and textual cues are extracted from the characters lines ( turns ) for modeling purposes . we report on the dataset as well as the performance and important features when predicting each of the sociolinguistic characteristics , comparing intra- and inter-author testing .

a machine learning approach to german pronoun resolution
this paper presents a novel ensemble learning approach to resolving german pronouns . boosting , the method in question , combines the moderately accurate hypotheses of several classifiers to form a highly accurate one . experiments show that this approach is superior to a single decision-tree classifier . furthermore , we present a standalone system that resolves pronouns in unannotated text by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process . although the system performs well within a limited textual domain , further research is needed to make it effective for open-domain question answering and text summarisation .

acquisition of desires before beliefs : a computational investigation
the acquisition of belief verbs lags behind the acquisition of desire verbs in children . some psycholinguistic theories attribute this lag to conceptual differences between the two classes , while others suggest that syntactic differences are responsible . through computational experiments , we show that a probabilistic verb learning model exhibits the pattern of acquisition , even though there is no difference in the model in the difficulty of the semantic or syntactic properties of belief vs. desire verbs . our results point to the distributional properties of various verb classes as a potentially important , and heretofore unexplored , factor in the observed developmental lag of belief verbs .

extractive summaries for educational science content
this paper describes an extractive summarizer for educational science content called cogent . cogent extends mead based on strategies elicited from an empirical study with domain and instructional experts . cogent implements a hybrid approach integrating both domain independent sentence scoring features and domain-aware features . initial evaluation results indicate that cogent outperforms existing summarizers and generates summaries that closely resemble those generated by human experts .

integrating bayesian co-segmentation models with pivot-based smt
recent research on multilingual statistical machine translation ( smt ) focuses on the usage of pivot languages in order to overcome resource limitations for certain language pairs . this paper proposes a new method to translate a dialect language into a foreign language by integrating transliteration approaches based on bayesian co-segmentation ( bcs ) models with pivot-based smt approaches . the advantages of the proposed method with respect to standard smt approaches are three fold : ( 1 ) it uses a standard language as the pivot language and acquires knowledge about the relation between dialects and the standard language automatically , ( 2 ) it reduces the translation task complexity by using monotone decoding techniques , ( 3 ) it reduces the number of features in the log-linear model that have to be estimated from bilingual data . experimental results translating four japanese dialects ( kumamoto , kyoto , okinawa , osaka ) into four indo-european languages ( english , german , russian , hindi ) and two asian languages ( chinese , korean ) revealed that the proposed method improves the translation quality of dialect translation tasks and outperforms standard pivot translation approaches concatenating smt engines for the majority of the investigated language pairs .

directional pps and reference frames in drt
in this paper , i will argue that the wellknown ambiguity of directional prepositions between the intrinsic and relative readings is not lexical , but can be interpreted as a framework assignment ambiguity of the type observed in the temporal domain . a drt semantics will then be constructed around a unified model of framework assignment which can be applied across the board to all three universal frame types .

learner characteristics and feedback in tutorial dialogue
tutorial dialogue has been the subject of increasing attention in recent years , and it has become evident that empirical studies of human-human tutorial dialogue can contribute important insights to the design of computational models of dialogue . this paper reports on a corpus study of human-human tutorial dialogue transpiring in the course of problemsolving in a learning environment for introductory computer science . analyses suggest that the choice of corrective tutorial strategy makes a significant difference in the outcomes of both student learning gains and selfefficacy gains . the findings reveal that tutorial strategies intended to maximize student motivational outcomes ( e.g. , self-efficacy gain ) may not be the same strategies that maximize cognitive outcomes ( i.e. , learning gain ) . in light of recent findings that learner characteristics influence the structure of tutorial dialogue , we explore the importance of understanding the interaction between learner characteristics and tutorial dialogue strategy choice when designing tutorial dialogue systems .

crowdsourcing annotation of non-local semantic
this paper reports on a study of crowdsourcing the annotation of non-local ( or implicit ) frame-semantic roles , i.e. , roles that are realized in the previous discourse context . we describe two annotation setups ( marking and gap filling ) and find that gap filling works considerably better , attaining an acceptable quality relatively cheaply . the produced data is available for research purposes .

efficient implementation of beam-search incremental parsers
beam search incremental parsers are accurate , but not as fast as they could be . we demonstrate that , contrary to popular belief , most current implementations of beam parsers in fact run in o ( n2 ) , rather than linear time , because each statetransition is actually implemented as an o ( n ) operation . we present an improved implementation , based on tree structured stack ( tss ) , in which a transition is performed in o ( 1 ) , resulting in a real lineartime algorithm , which is verified empirically . we further improve parsing speed by sharing feature-extraction and dotproduct across beam items . practically , our methods combined offer a speedup of 2x over strong baselines on penn treebank sentences , and are orders of magnitude faster on much longer sentences .

the infinite tree
historically , unsupervised learning techniques have lacked a principled technique for selecting the number of unseen components . research into non-parametric priors , such as the dirichlet process , has enabled instead the use of infinite models , in which the number of hidden categories is not fixed , but can grow with the amount of training data . here we develop the infinite tree , a new infinite model capable of representing recursive branching structure over an arbitrarily large set of hidden categories . specifically , we develop three infinite tree models , each of which enforces different independence assumptions , and for each model we define a simple direct assignment sampling inference procedure . we demonstrate the utility of our models by doing unsupervised learning of part-of-speech tags from treebank dependency skeleton structure , achieving an accuracy of 75.34 % , and by doing unsupervised splitting of part-of-speech tags , which increases the accuracy of a generative dependency parser from 85.11 % to 87.35 % .

coreference handling in xmg
we claim that existing specification languages for tree based grammars fail to adequately support identifier managment . we then show that xmg ( extensible metagrammar ) provides a sophisticated treatment of identifiers which is effective in supporting a linguist-friendly grammar design .

unsupervised and constrained dirichlet process mixture models for verb
in this work , we apply dirichlet process mixture models ( dpmms ) to a learning task in natural language processing ( nlp ) : lexical-semantic verb clustering . we thoroughly evaluate a method of guiding dpmms towards a particular clustering solution using pairwise constraints . the quantitative and qualitative evaluation performed highlights the benefits of both standard and constrained dpmms compared to previously used approaches . in addition , it sheds light on the use of evaluation measures and their practical application .

the stanford corenlp natural language processing toolkit
we describe the design and use of the stanford corenlp toolkit , an extensible pipeline that provides core natural language analysis . this toolkit is quite widely used , both in the research nlp community and also among commercial and government users of open source nlp technology . we suggest that this follows from a simple , approachable design , straightforward interfaces , the inclusion of robust and good quality analysis components , and not requiring use of a large amount of associated baggage .

a generalized language model as the combination of skipped n-grams and modified kneser-ney smoothing paul georg wagner and till speicher
we introduce a novel approach for building language models based on a systematic , recursive exploration of skip n-gram models which are interpolated using modified kneser-ney smoothing . our approach generalizes language models as it contains the classical interpolation with lower order models as a special case . in this paper we motivate , formalize and present our approach . in an extensive empirical experiment over english text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1 % and 12.7 % in comparison to traditional language models using modified kneser-ney smoothing . furthermore , we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements . finally , we also show that the strength of our approach lies in its ability to cope in particular with sparse training data . using a very small training data set of only 736 kb text we yield improvements of even 25.7 % reduction of perplexity .

solving relational similarity problems using the web as a corpus
we present a simple linguistically-motivated method for characterizing the semantic relations that hold between two nouns . the approach leverages the vast size of the web in order to build lexically-specific features . the main idea is to look for verbs , prepositions , and coordinating conjunctions that can help make explicit the hidden relations between the target nouns . using these features in instance-based classifiers , we demonstrate state-of-the-art results on various relational similarity problems , including mapping noun-modifier pairs to abstract relations like time , location and container , characterizing noun-noun compounds in terms of abstract linguistic predicates like cause , use , and from , classifying the relations between nominals in context , and solving sat verbal analogy problems . in essence , the approach puts together some existing ideas , showing that they apply generally to various semantic tasks , finding that verbs are especially useful features .

towards automatic addressee identification in multi-party dialogues rieks op den akker
the paper is about the issue of addressing in multi-party dialogues . analysis of addressing behavior in face to face meetings results in the identification of several addressing mechanisms . from these we extract several utterance features and features of non-verbal communicative behavior of a speaker , like gaze and gesturing , that are relevant for observers to identify the participants the speaker is talking to . a method for the automatic prediction of the addressee of speech acts is discussed .

chinese syntactic reordering for statistical machine translation
syntactic reordering approaches are an effective method for handling word-order differences between source and target languages in statistical machine translation ( smt ) systems . this paper introduces a reordering approach for translation from chinese to english . we describe a set of syntactic reordering rules that exploit systematic differences between chinese and english word order . the resulting system is used as a preprocessor for both training and test sentences , transforming chinese sentences to be much closer to english in terms of their word order . we evaluated the reordering approach within the moses phrase-based smt system ( koehn et al , 2007 ) . the reordering approach improved the bleu score for the moses system from 28.52 to 30.86 on the nist 2006 evaluation data . we also conducted a series of experiments to analyze the accuracy and impact of different types of reordering rules .

a unified model for topics , events and users on twitter
with the rapid growth of social media , twitter has become one of the most widely adopted platforms for people to post short and instant message . on the one hand , people tweets about their daily lives , and on the other hand , when major events happen , people also follow and tweet about them . moreover , peoples posting behaviors on events are often closely tied to their personal interests . in this paper , we try to model topics , events and users on twitter in a unified way . we propose a model which combines an lda-like topic model and the recurrent chinese restaurant process to capture topics and events . we further propose a duration-based regularization component to find bursty events . we also propose to use event-topic affinity vectors to model the association between events and topics . our experiments shows that our model can accurately identify meaningful events and the event-topic affinity vectors are effective for event recommendation and grouping events by topics .

imposing hierarchical browsing structures onto spoken documents
this paper studies the problem of imposing a known hierarchical structure onto an unstructured spoken document , aiming to help browse such archives . we formulate our solutions within a dynamic-programming-based alignment framework and use minimum errorrate training to combine a number of global and hierarchical constraints . this pragmatic approach is computationally efficient . results show that it outperforms a baseline that ignores the hierarchical and global features and the improvement is consistent on transcripts with different wers . directly imposing such hierarchical structures onto raw speech without using transcripts yields competitive results .

hierarchical bayesian domain adaptation
multi-task learning is the problem of maximizing the performance of a system across a number of related tasks . when applied to multiple domains for the same task , it is similar to domain adaptation , but symmetric , rather than limited to improving performance on a target domain . we present a more principled , better performing model for this problem , based on the use of a hierarchical bayesian prior . each domain has its own domain-specific parameter for each feature but , rather than a constant prior over these parameters , the model instead links them via a hierarchical bayesian global prior . this prior encourages the features to have similar weights across domains , unless there is good evidence to the contrary . we show that the method of ( daume iii , 2007 ) , which was presented as a simple preprocessing step , is actually equivalent , except our representation explicitly separates hyperparameters which were tied in his work . we demonstrate that allowing different values for these hyperparameters significantly improves performance over both a strong baseline and ( daume iii , 2007 ) within both a conditional random field sequence model for named entity recognition and a discriminatively trained dependency parser .

computational metaphor identification univ of california , irvine univ of california , irvine univ of california , irvine
most computational approaches to metaphor have focused on discerning between metaphorical and literal text . recent work on computational metaphor identification ( cmi ) instead seeks to identify overarching conceptual metaphors by mapping selectional preferences between source and target corpora . this paper explores using semantic role labeling ( srl ) in cmi . its goals are two-fold : first , to demonstrate that semantic roles can effectively be used to identify conceptual metaphors , and second , to compare srl to the current use of typed dependency parsing in cmi . the results show that srl can be used to identify potential metaphors and that it overcomes some of the limitations of using typed dependencies , but also that srl introduces its own set of complications . the paper concludes by suggesting future directions , both for evaluating the use of srl in cmi , and for fostering critical and creative thinking about metaphors .

transducing sentences to syntactic feature vectors : an alternative way to parse fabio massimo zanzotto
classification and learning algorithms use syntactic structures as proxies between source sentences and feature vectors . in this paper , we explore an alternative path to use syntax in feature spaces : the distributed representation parsers ( drp ) . the core of the idea is straightforward : drps directly obtain syntactic feature vectors from sentences without explicitly producing symbolic syntactic interpretations . results show that drps produce feature spaces significantly better than those obtained by existing methods in the same conditions and competitive with those obtained by existing methods with lexical information .

combining natural and artificial examples to improve implicit discourse relation identification
this paper presents the first experiments on identifying implicit discourse relations ( i.e. , relations lacking an overt discourse connective ) in french . given the little amount of annotated data for this task , our system resorts to additional data automatically labeled using unambiguous connectives , a method introduced by ( marcu and echihabi , 2002 ) . we first show that a system trained solely on these artificial data does not generalize well to natural implicit examples , thus echoing the conclusion made by ( sporleder and lascarides , 2008 ) for english . we then explain these initial results by analyzing the different types of distribution difference between natural and artificial implicit data . this finally leads us to propose a number of very simple methods , all inspired from work on domain adaptation , for combining the two types of data . through various experiments on the french annodis corpus , we show that our best system achieves an accuracy of 41.7 % , corresponding to a 4.4 % significant gain over a system solely trained on manually labeled data .

automatically detecting action items in audio meeting recordings william morgan pi-chuan chang surabhi gupta
identification of action items in meeting recordings can provide immediate access to salient information in a medium notoriously difficult to search and summarize . to this end , we use a maximum entropy model to automatically detect action itemrelated utterances from multi-party audio meeting recordings . we compare the effect of lexical , temporal , syntactic , semantic , and prosodic features on system performance . we show that on a corpus of action item annotations on the icsi meeting recordings , characterized by high imbalance and low inter-annotator agreement , the system performs at an f measure of 31.92 % . while this is low compared to better-studied tasks on more mature corpora , the relative usefulness of the features towards this task is indicative of their usefulness on more consistent annotations , as well as to related tasks .

how to train your multi bottom-up tree transducer
the local multi bottom-up tree transducer is introduced and related to the ( non-contiguous ) synchronous tree sequence substitution grammar . it is then shown how to obtain a weighted local multi bottom-up tree transducer from a bilingual and biparsed corpus . finally , the problem of non-preservation of regularity is addressed . three properties that ensure preservation are introduced , and it is discussed how to adjust the rule extraction process such that they are automatically fulfilled .

an automatic filter for non-parallel texts
numerous cross-lingual applications , including state-of-the-art machine translation systems , require parallel texts aligned at the sentence level . however , collections of such texts are often polluted by pairs of texts that are comparable but not parallel . bitext maps can help to discriminate between parallel and comparable texts . bitext mapping algorithms use a larger set of document features than competing approaches to this task , resulting in higher accuracy . in addition , good bitext mapping algorithms are not limited to documents with structural mark-up such as web pages . the task of filtering non-parallel text pairs represents a new application of bitext mapping algorithms .

computing paraphrasability of syntactic variants using web snippets
in a broad range of natural language processing tasks , large-scale knowledge-base of paraphrases is anticipated to improve their performance . the key issue in creating such a resource is to establish a practical method of computing semantic equivalence and syntactic substitutability , i.e. , paraphrasability , between given pair of expressions . this paper addresses the issues of computing paraphrasability , focusing on syntactic variants of predicate phrases . our model estimates paraphrasability based on traditional distributional similarity measures , where the web snippets are used to overcome the data sparseness problem in handling predicate phrases . several feature sets are evaluated through empirical experiments .

wosit : a word sense induction toolkit for search result clustering and diversification dipartimento di informatica
in this demonstration we present wosit , an api for word sense induction ( wsi ) algorithms . the toolkit provides implementations of existing graph-based wsi algorithms , but can also be extended with new algorithms . the main mission of wosit is to provide a framework for the extrinsic evaluation of wsi algorithms , also within end-user applications such as web search result clustering and diversification .

conll-x shared task on multilingual dependency parsing communication & cognition
each year the conference on computational natural language learning ( conll ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems . the tenth conll ( conll-x ) saw a shared task on multilingual dependency parsing . in this paper , we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured . we also give an overview of the parsing approaches that participants took and the results that they achieved . finally , we try to draw general conclusions about multi-lingual parsing : what makes a particular language , treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser acknowledgement many thanks to amit dubey and yuval krymolowski , the other two organizers of the shared task , for discussions , converting treebanks , writing software and helping with the papers.

combining parallel treebanks and geo-tagging
this paper describes a new kind of semantic annotation in parallel treebanks . we build french-german parallel treebanks of mountaineering reports , a text genre that abounds with geographical names which we classify and ground with reference to a large gazetteer of swiss toponyms . we discuss the challenges in obtaining a high recall and precision in automatic grounding , and sketch how we represent the grounding information in our treebank .

framenet-based semantic parsing using maximum entropy models
as part of its description of lexico-semantic predicate frames or conceptual structures , the framenet project defines a set of semantic roles specific to the core predicate of a sentence . recently , researchers have tried to automatically produce semantic interpretations of sentences using this information . building on prior work , we describe a new method to perform such interpretations . we define sentence segmentation first and show how maximum entropy re-ranking helps achieve a level of 76.2 % f-score ( answer among topfive candidates ) or 61.5 % ( correct answer ) .

linking events and their participants in discourse
we describe the semeval-2010 shared task on linking events and their participants in discourse . this task is an extension to the classical semantic role labeling task . while semantic role labeling is traditionally viewed as a sentence-internal task , local semantic argument structures clearly interact with each other in a larger context , e.g. , by sharing references to specific discourse entities or events . in the shared task we looked at one particular aspect of cross-sentence links between argument structures , namely linking locally uninstantiated roles to their co-referents in the wider discourse context ( if such co-referents exist ) . this task is potentially beneficial for a number of nlp applications , such as information extraction , question answering or text summarization .

query-based sentence fusion is better defined and leads to more preferred results than generic sentence
we show that question-based sentence fusion is a better defined task than generic sentence fusion ( q-based fusions are shorter , display less variety in length , yield more identical results and have higher normalized rouge scores ) . moreover , we show that in a qa setting , participants strongly prefer q-based fusions over generic ones , and have a preference for union over intersection fusions .

mixture model-based minimum bayes risk decoding using multiple machine translation systems
we present mixture model-based minimum bayes risk ( mmmbr ) decoding , an approach that makes use of multiple smt systems to improve translation accuracy . unlike existing mbr decoding methods defined on the basis of single smt systems , an mmmbr decoder reranks translation outputs in the combined search space of multiple systems using the mbr decision rule and a mixture distribution of component smt models for translation hypotheses . mmmbr decoding is a general method that is independent of specific smt models and can be applied to various commonly used search spaces . experimental results on the nist chinese-to-english mt evaluation tasks show that our approach brings significant improvements to single system-based mbr decoding and outperforms a stateof-the-art system combination method .

collective named entity disambiguation using graph ranking and clique partitioning approaches and robert gaizauskas
disambiguating named entities ( ne ) in running text to their correct interpretations in a specific knowledge base ( kb ) is an important problem in nlp . this paper presents two collective disambiguation approaches using a graph representation where possible kb candidates for ne textual mentions are represented as nodes and the coherence relations between different ne candidates are represented by edges . each node has a local confidence score and each edge has a weight . the first approach uses page-rank ( pr ) to rank all nodes and selects a candidate based on pr score combined with local confidence score . the second approach uses an adapted clique partitioning technique to find the most weighted clique and expands this clique until all ne textual mentions are disambiguated . experiments on 27,819 ne textual mentions show the effectiveness of both approaches , outperforming both baseline and state-of-the-art approaches .

paraphrase-driven learning for open question answering
we study question answering as a machine learning problem , and induce a function that maps open-domain questions to queries over a database of web extractions . given a large , community-authored , question-paraphrase corpus , we demonstrate that it is possible to learn a semantic lexicon and linear ranking function without manually annotating questions . our approach automatically generalizes a seed lexicon and includes a scalable , parallelized perceptron parameter estimation scheme . experiments show that our approach more than quadruples the recall of the seed lexicon , with only an 8 % loss in precision .

corrective modeling for non-projective dependency parsing
we present a corrective model for recovering non-projective dependency structures from trees generated by state-of-theart constituency-based parsers . the continuity constraint of these constituencybased parsers makes it impossible for them to posit non-projective dependency trees . analysis of the types of dependency errors made by these parsers on a czech corpus show that the correct governor is likely to be found within a local neighborhood of the governor proposed by the parser . our model , based on a maxent classifier , improves overall dependency accuracy by .7 % ( a 4.5 % reduction in error ) with over 50 % accuracy for non-projective structures .

fbk-irst : a multi-phase kernel based approach for drug-drug interaction detection and classification that exploits linguistic information
this paper presents the multi-phase relation extraction ( re ) approach which was used for the ddi extraction task of semeval 2013. as a preliminary step , the proposed approach indirectly ( and automatically ) exploits the scope of negation cues and the semantic roles of involved entities for reducing the skewness in the training data as well as discarding possible negative instances from the test data . then , a state-of-the-art hybrid kernel is used to train a classifier which is later applied on the instances of the test data not filtered out by the previous step . the official results of the task show that our approach yields an f-score of 0.80 for ddi detection and an f-score of 0.65 for ddi detection and classification . our system obtained significantly higher results than all the other participating teams in this shared task and has been ranked 1st .

combining multiple forms of evidence while filtering
this paper studies how to go beyond relevance and enable a filtering system to learn more interesting and detailed data driven user models from multiple forms of evidence . we carry out a user study using a real time web based personal news filtering system , and collect extensive multiple forms of evidence , including explicit and implicit user feedback . we explore the graphical modeling approach to combine these forms of evidence . to test whether the approach can help us understand the domain better , we use graph structure learning algorithm to derive the causal relationships between different forms of evidence . to test whether the approach can help the system improve the performance , we use the graphical inference algorithms to predict whether a user likes a document based on multiple forms of evidence . the results show that combining multiple forms of evidence using graphical models can help us better understand the filtering problem , improve filtering system performance , and handle various data missing situations naturally .

combining multiple models for speech information retrieval
in this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a speech information retrieval task . the formulas for combining the models are tuned on training data . then the system is evaluated on test data . the task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . also , the topics are real information needs , difficult to satisfy . information retrieval systems are not able to obtain good results on this data set , except for the case when manual summaries are included .

a prototype text to british sign language ( bsl ) translation system
we demonstrate a text to sign language translation system for investigating sign language ( sl ) structure and assisting in production of sign narratives and informative presentations1 . the system is demonstrable on a conventional pc laptop computer .

constructing an english valency lexicon
this paper presents the english valency lexicon engvallex , built within the functional generative description framework . the form of the lexicon , as well as the process of its semi-automatic creation is described . the lexicon describes valency for verbs and also includes links to other lexical sources , namely propbank . basic statistics about the lexicon are given . the lexicon will be later used for annotation of the wall street journal section of the penn treebank in praguian formalisms .

semantic transparency for chinese word segmentation richard tzong-han tsai and hsi-chuan hung
this paper exploits unlabeled text data to improve new word identification and chinese word segmentation performance . our contributions are twofold . first , for new words that lack semantic transparency , such as person , location , or transliteration names , we calculate association metrics of adjacent character segments on unlabeled data and encode this information as features . second , we construct an internal dictionary by using an initial model to extract words from both the unlabeled training and test set to maintain balanced coverage on the training and test set . in comparison to the baseline model which only uses n-gram features , our approach increases new word recall up to 6.0 % . additionally , our approaches reduce segmentation errors up to 32.3 % . our system achieves state-of-the-art performance for both the closed and open tasks of the 2006 sighan bakeoff .

acquiring knowledge from the web to be used as selectors for noun
this paper presents a method of acquiring knowledge from the web for noun sense disambiguation . words , called selectors , are acquired which take the place of an instance of a target word in its local context . the selectors serve for the system to essentially learn the areas or concepts of wordnet that the sense of a target word should be a part of . the correct sense is chosen based on a combination of the strength given from similarity and relatedness measures over wordnet and the probability of a selector occurring within the local context . our method is evaluated using the coarse-grained all-words task from semeval 2007. experiments reveal that pathbased similarity measures perform just as well as information content similarity measures within our system . overall , the results show our system is out-performed only by systems utilizing training data or substantially more annotated data .

topological field parsing of german jackie chi kit cheung
freer-word-order languages such as german exhibit linguistic phenomena that present unique challenges to traditional cfg parsing . such phenomena produce discontinuous constituents , which are not naturally modelled by projective phrase structure trees . in this paper , we examine topological field parsing , a shallow form of parsing which identifies the major sections of a sentence in relation to the clausal main verb and the subordinating heads . we report the results of topological field parsing of german using the unlexicalized , latent variable-based berkeley parser ( petrov et al , 2006 ) without any language- or model-dependent adaptation , we achieve state-of-the-art results on the tuba-d/z corpus , and a modified negra corpus that has been automatically annotated with topological fields ( becker and frank , 2002 ) . we also perform a qualitative error analysis of the parser output , and discuss strategies to further improve the parsing results .

computing confidence scores for all sub parse trees
computing confidence scores for applications , such as dialogue system , information retrieving and extraction , is an active research area . however , its focus has been primarily on computing word- , concept- , or utterance-level confidences . motivated by the need from sophisticated dialogue systems for more effective dialogs , we generalize the confidence annotation to all the subtrees , the first effort in this line of research . the other contribution of this work is that we incorporated novel long distance features to address challenges in computing multi-level confidence scores . using conditional maximum entropy ( cme ) classifier with all the selected features , we reached an annotation error rate of 26.0 % in the swbd corpus , compared with a subtree error rate of 41.91 % , a closely related benchmark with the charniak parser from ( kahn et al , 2005 ) .

learning optimal dialogue management rules by using reinforcement learning and inductive logic programming
developing dialogue systems is a complex process . in particular , designing efficient dialogue management strategies is often difficult as there are no precise guidelines to develop them and no sure test to validate them . several suggestions have been made recently to use reinforcement learning to search for the optimal management strategy for specific dialogue situations . these approaches have produced interesting results , including applications involving real world dialogue systems . however , reinforcement learning suffers from the fact that it is state based . in other words , the optimal strategy is expressed as a decision table specifying which action to take in each specific state . it is therefore difficult to see whether there is any generality across states . this limits the analysis of the optimal strategy and its potential for re-use in other dialogue situations . in this paper we tackle this problem by learning rules that generalize the state-based strategy . these rules are more readable than the underlying strategy and therefore easier to explain and re-use .

computing word similarity and identifying cognates with pair hidden markov models
we present a system for computing similarity between pairs of words . our system is based on pair hidden markov models , a variation on hidden markov models that has been used successfully for the alignment of biological sequences . the parameters of the model are automatically learned from training data that consists of word pairs known to be similar . our tests focus on the identification of cognates words of common origin in related languages . the results show that our system outperforms previously proposed techniques .

melodi : semantic similarity of words and compositional phrases using latent vector weighting
in this paper we present our system for the semeval 2013 task 5a on semantic similarity of words and compositional phrases . our system uses a dependency-based vector space model , in combination with a technique called latent vector weighting . the system computes the similarity between a particular noun instance and the head noun of a particular noun phrase , which was weighted according to the semantics of the modifier . the system is entirely unsupervised ; one single parameter , the similarity threshold , was tuned using the training data .

agile corpus annotation in practice : an overview of manual and automatic annotation of cvs
this paper describes work testing agile data annotation by moving away from the traditional , linear phases of corpus creation towards iterative ones and by recognizing the potential for sources of error occurring throughout the annotation process .

umcc_dlsi : reinforcing a ranking algorithm with sense frequencies and multidimensional semantic resources to solve multilingual word sense disambiguation
this work introduces a new unsupervised approach to multilingual word sense disambiguation . its main purpose is to automatically choose the intended sense ( meaning ) of a word in a particular context for different languages . it does so by selecting the correct babel synset for the word and the various wiki page titles that mention the word . babelnet contains all the output information that our system needs , in its babel synset . through babel synset , we find all the possible synsets for the word in wordnet . using these synsets , we apply the disambiguation method ppr+freq to find what we need . to facilitate the work with wordnet , we use the isr-wn which offers the integration of different resources to wordnet . our system , recognized as the best in the competition , obtains results around 69 % of recall .

community evaluation and exchange of word vectors
vector space word representations are useful for many natural language processing applications . the diversity of techniques for computing vector representations and the large number of evaluation benchmarks makes reliable comparison a tedious task both for researchers developing new vector space models and for those wishing to use them . we present a website and suite of offline tools that that facilitate evaluation of word vectors on standard lexical semantics benchmarks and permit exchange and archival by users who wish to find good vectors for their applications .

hr-wsd : system description for all-words word sense disambiguation
the document describes the knowledgebased domain-wsd system using heuristic rules ( knowledge-base ) . this hrwsd system delivered the best performance ( 55.9 % ) among all chinese systems in semeval-2010 task 17 : all-words wsd on a specific domain .

picking the amateurs mind predicting chess player strength from and language processing
results from psychology show a connection between a speakers expertise in a task and the language he uses to talk about it . in this paper , we present an empirical study on using linguistic evidence to predict the expertise of a speaker in a task : playing chess . instructional chess literature claims that the mindsets of amateur and expert players differ fundamentally ( silman , 1999 ) ; psychological science has empirically arrived at similar results ( e.g. , pfau and murphy ( 1988 ) ) . we conduct experiments on automatically predicting chess player skill based on their natural language game commentary . we make use of annotated chess games , in which players provide their own interpretation of game in prose . based on a dataset collected from an online chess forum , we predict player strength through svm classification and ranking . we show that using textual and chess-specific features achieves both high classification accuracy and significant correlation . finally , we compare our findings to claims from the chess literature and results from psychology .

the cisp annotation schema uncovers hypotheses and explanations in full-text scientific journal articles
increasingly , as full-text scientific papers are becoming available , scientific queries have shifted from looking for facts to looking for arguments . researchers want to know when their colleagues are proposing theories , outlining evidentiary relations , or explaining discrepancies . we show here that sentence-level annotation with the cisp schema adapts well to a corpus of biomedical articles , and we present preliminary results arguing that the cisp schema is uniquely suited to recovering common types of scientific arguments about hypotheses , explanations , and evidence .

a machine learning approach to extract temporal information from texts in swedish and generate animated 3d scenes
carsim is a program that automatically converts narratives into 3d scenes . carsim considers authentic texts describing road accidents , generally collected from web sites of swedish newspapers or transcribed from hand-written accounts by victims of accidents . one of the programs key features is that it animates the generated scene to visualize events . to create a consistent animation , carsim extracts the participants mentioned in a text and identifies what they do . in this paper , we focus on the extraction of temporal relations between actions . we first describe how we detect time expressions and events . we then present a machine learning technique to order the sequence of events identified in the narratives . we finally report the results we obtained .

efficient online summarization of microblogging streams
the large amounts of data generated on microblogging services are making summarization challenging . previous research has mostly focused on working in batches or with filtered streams . input data has to be saved and analyzed several times , in order to detect underlying events and then summarize them . we improve the efficiency of this process by designing an online abstractive algorithm . processing is done in a single pass , removing the need to save any input data and improving the running time . an online approach is also able to generate the summaries in real time , using the latest information . the algorithm we propose uses a word graph , along with optimization techniques such as decaying windows and pruning . it outperforms the baseline in terms of summary quality , as well as time and memory efficiency .

integrating information extraction and automatic hyperlinking
this paper presents a novel information system integrating advanced information extraction technology and automatic hyper-linking . extracted entities are mapped into a domain ontology that relates concepts to a selection of hyperlinks . for information extraction , we use sprout , a generic platform for the development and use of multilingual text processing components . by combining finite-state and unification-based formalisms , the grammar formalism used in sprout offers both processing efficiency and a high degree of decalrativeness . the extralink demo system showcases the extraction of relevant concepts from german texts in the tourism domain , offering the direct connection to associated web documents on demand .

implications for generating clarification requests in task-oriented
clarification requests ( crs ) in conversation ensure and maintain mutual understanding and thus play a crucial role in robust dialogue interaction . in this paper , we describe a corpus study of crs in task-oriented dialogue and compare our findings to those reported in two prior studies . we find that cr behavior in task-oriented dialogue differs significantly from that in everyday conversation in a number of ways . moreover , the dialogue type , the modality and the channel quality all influence the decision of when to clarify and at which level of the grounding process . finally we identify formfunction correlations which can inform the generation of crs .

the excitement open platform for textual inferences
this paper presents the excitement open platform ( eop ) , a generic architecture and a comprehensive implementation for textual inference in multiple languages . the platform includes state-of-art algorithms , a large number of knowledge resources , and facilities for experimenting and testing innovative approaches . the eop is distributed as an open source software .

encoding syntactic dependencies by vector permutation
distributional approaches are based on a simple hypothesis : the meaning of a word can be inferred from its usage . the application of that idea to the vector space model makes possible the construction of a wordspace in which words are represented by mathematical points in a geometric space . similar words are represented close in this space and the definition of word usage depends on the definition of the context used to build the space , which can be the whole document , the sentence in which the word occurs , a fixed window of words , or a specific syntactic context . however , in its original formulation wordspace can take into account only one definition of context at a time . we propose an approach based on vector permutation and random indexing to encode several syntactic contexts in a single wordspace . moreover , we propose some operations in this space and report the results of an evaluation performed using the gems 2011 shared evaluation data .

extraction and approximation of numerical attributes from the web
we present a novel framework for automated extraction and approximation of numerical object attributes such as height and weight from the web . given an object-attribute pair , we discover and analyze attribute information for a set of comparable objects in order to infer the desired value . this allows us to approximate the desired numerical values even when no exact values can be found in the text . our framework makes use of relation defining patterns and wordnet similarity information . first , we obtain from the web and wordnet a list of terms similar to the given object . then we retrieve attribute values for each term in this list , and information that allows us to compare different objects in the list and to infer the attribute value range . finally , we combine the retrieved data for all terms from the list to select or approximate the requested value . we evaluate our method using automated question answering , wordnet enrichment , and comparison with answers given in wikipedia and by leading search engines . in all of these , our framework provides a significant improvement .

guessing the grammatical function of a non-root f-structure in lfg josef van genabith
lexical-functional grammar ( kaplan and bresnan , 1982 ) f-structures are bilexical labelled dependency representations . we show that the naive bayes classifier is able to guess missing grammatical function labels ( i.e . bilexical dependency labels ) with reasonably high accuracy ( 8291 % ) . in the experiments we use f-structure parser output for english and german europarl data , automatically broken by replacing grammatical function labels with a generic unknown label and asking the classifier to restore the label .

generalizing hierarchical phrase-based translation using rules with adjacent nonterminals
hierarchical phrase-based translation ( hiero , ( chiang , 2005 ) ) provides an attractive framework within which both short- and longdistance reorderings can be addressed consistently and efciently . however , hiero is generally implemented with a constraint preventing the creation of rules with adjacent nonterminals , because such rules introduce computational and modeling challenges . we introduce methods to address these challenges , and demonstrate that rules with adjacent nonterminals can improve hiero 's generalization power and lead to signicant performance gains in chinese-english translation .

sentiment classification using rough set based hybrid feature basant agarwal namita mittal
sentiment analysis means to extract opinion of users from review documents . sentiment classification using machine learning ( ml ) methods faces the problem of high dimensionality of feature vector . therefore , a feature selection method is required to eliminate the irrelevant and noisy features from the feature vector for efficient working of ml algorithms . rough set theory based feature selection method finds the optimal feature subset by eliminating the redundant features . in this paper , rough set theory ( rst ) based feature selection method is applied for sentiment classification . a hybrid feature selection method based on rst and information gain ( ig ) is proposed for sentiment classification . proposed methods are evaluated on four standard datasets viz . movie review , product ( book , dvd and electronics ) review dataset . experimental results show that hybrid feature selection method outperforms than other feature selection methods for sentiment classification .

pku : combining supervised classifiers with features selection
this paper presents the word sense disambiguation system of peking university which was designed for the semeval-2007 competition . the system participated in the web track of task 11 english lexical sample task via english-chinese parallel text . the system is a hybrid model by combining two supervised learning algorithms svm and me . and the method of entropy-based feature chosen was experimented . we obtained precision ( and recall ) of 81.5 % .

of cross-lingual question answering strategies
this article presents a bilingual question answering system , which is able to process questions and documents both in french and in english . two cross-lingual strategies are described and evaluated . first , we study the contribution of biterms translation , and the influence of the completion of the translation dictionaries . then , we propose a strategy for transferring the question analysis from one language to the other , and we study its influence on the performance of our system .

assessing the correlation between contextual patterns and
the tagging of biological entities , and in particular gene and protein names , is an essential step in the analysis of textual information in molecular biology and biomedicine . the problem is harder than was originally thought because of the highly dynamic nature of the research area , in which new genes and their functions are constantly being discovered , and because of the lack of commonly accepted standards . an impressive collection of techniques has been used to detect protein and gene names in the last fourfive years , ranging from typical nlp to purely bioinformatics approaches . we explore here the relationship between protein/gene names and expressions used to characterize protein/gene function . these expressions are captured in a collection of patterns derived from an original set of manually derived expressions , extended to cover lexical variants and filtered with known cases of association patterns/ names . applying these patterns to a large collection of curated sentences , we found a significant number of patterns with a very strong tendency to appear only in sentences in which a protein/gene name is simultaneously present . this approach is part of a larger effort to incorporate contextual information so as to make biological information less ambiguous .

time-aware personalized hashtag recommendation on social media
the task of recommending hashtags for microblogs has been received considerable attention in recent years , and many applications can reap enormous benefits from it . various approaches have been proposed to study the problem from different aspects . however , the impacts of temporal and personal factors have rarely been considered in the existing methods . in this paper , we propose a novel method that extends the translation based model and incorporates the temporal and personal factors . to overcome the limitation of only being able to recommend hashtags that exist in the training data of the existing methods , the proposed method also incorporates extraction strategies into it . the results of experiments on the data collected from real world microblogging services by crawling demonstrate that the proposed method outperforms state-of-the-art methods that do not consider these aspects . the relative improvement of the proposed method over the method without considering these aspects is around 47.8 % in f1-score .

revisiting readability : a unified framework for predicting text quality
we combine lexical , syntactic , and discourse features to produce a highly predictive model of human readers judgments of text readability . this is the first study to take into account such a variety of linguistic factors and the first to empirically demonstrate that discourse relations are strongly associated with the perceived quality of text . we show that various surface metrics generally expected to be related to readability are not very good predictors of readability judgments in our wall street journal corpus . we also establish that readability predictors behave differently depending on the task : predicting text readability or ranking the readability . our experiments indicate that discourse relations are the one class of features that exhibits robustness across these two tasks .

a transitive model for extracting translation equivalents of web queries through anchor text mining
one of the existing difficulties of cross-language information retrieval ( clir ) and web search is the lack of appropriate translations of new terminology and proper names . different from conventional approaches , in our previous research we developed an approach for exploiting web anchor texts as live bilingual corpora and reducing the existing difficulties of query term translation . although web anchor texts , undoubtedly , are very valuable multilingual and wide-scoped hypertext resources , not every particular pair of languages contains sufficient anchor texts in the web to extract corresponding translations in the language pair . for more generalized applications , in this paper we extend our previous approach by adding a phase of transitive ( indirect ) translation via an intermediate ( third ) language , and propose a transitive model to further exploit anchor-text mining in term translation extraction applications . preliminary experimental results show that many query translations which can not be obtained using the previous approach can be extracted with the improved approach .

a new approach to improving multilingual summarization using a genetic algorithm
automated summarization methods can be defined as language-independent , if they are not based on any languagespecific knowledge . such methods can be used for multilingual summarization defined by mani ( 2001 ) as processing several languages , with summary in the same language as input . in this paper , we introduce muse , a languageindependent approach for extractive summarization based on the linear optimization of several sentence ranking measures using a genetic algorithm . we tested our methodology on two languagesenglish and hebrewand evaluated its performance with rouge-1 recall vs. stateof-the-art extractive summarization approaches . our results show that muse performs better than the best known multilingual approach ( textrank1 ) in both languages . moreover , our experimental results on a bilingual ( english and hebrew ) document collection suggest that muse does not need to be retrained on each language and the same model can be used across at least two different languages .

referring expression generation through attribute-based heuristics
in this paper , we explore a corpus of human-produced referring expressions to see to what extent we can learn the referential behaviour the corpus represents . despite a wide variation in the way subjects refer across a set of ten stimuli , we demonstrate that component elements of the referring expression generation process appear to generalise across participants to a significant degree . this leads us to propose an alternative way of thinking of referring expression generation , where each attribute in a description is provided by a separate heuristic .

great : a finite-state machine translation toolkit implementing a grammatical inference approach for transducer inference ( giati )
great is a finite-state toolkit which is devoted to machine translation and that learns structured models from bilingual data . the training procedure is based on grammatical inference techniques to obtain stochastic transducers that model both the structure of the languages and the relationship between them . the inference of grammars from natural language causes the models to become larger when a less restrictive task is involved ; even more if a bilingual modelling is being considered . great has been successful to implement the giati learning methodology , using different scalability issues to be able to deal with corpora of high volume of data . this is reported with experiments on the europarl corpus , which is a state-of-theart task in statistical machine translation .

kul : a data-driven approach to temporal parsing of documents
this paper describes a system for temporal processing of text , which participated in the temporal evaluations 2013 campaign . the system employs a number of machine learning classifiers to perform the core tasks of : identification of time expressions and events , recognition of their attributes , and estimation of temporal links between recognized events and times . the central feature of the proposed system is temporal parsing an approach which identifies temporal relation arguments ( eventevent and event-timex pairs ) and the semantic label of the relation as a single decision .

discriminative reranking for machine translation franz josef och
this paper describes the application of discriminative reranking techniques to the problem of machine translation . for each sentence in the source language , we obtain from a baseline statistical machine translation system , a ranked best list of candidate translations in the target language . we introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the bleu metric . we provide experimental results on the nist 2003 chinese-english large data track evaluation . we also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation .

a tag-based noisy channel model of speech repairs
this paper describes a noisy channel model of speech repairs , which can identify and correct repairs in speech transcripts . a syntactic parser is used as the source model , and a novel type of tag-based transducer is the channel model . the use of tag is motivated by the intuition that the reparandum is a rough copy of the repair . the model is trained and tested on the switchboard disfluency-annotated corpus .

exploring representation-learning approaches to domain adaptation
most supervised language processing systems show a significant drop-off in performance when they are tested on text that comes from a domain significantly different from the domain of the training data . sequence labeling systems like partof-speech taggers are typically trained on newswire text , and in tests their error rate on , for example , biomedical data can triple , or worse . we investigate techniques for building open-domain sequence labeling systems that approach the ideal of a system whose accuracy is high and constant across domains . in particular , we investigate unsupervised techniques for representation learning that provide new features which are stable across domains , in that they are predictive in both the training and out-of-domain test data . in experiments , our novel techniques reduce error by as much as 29 % relative to the previous state of the art on out-of-domain text .

lemmatization and lexicalized statistical parsing of morphologically rich languages : the case of french
this paper shows that training a lexicalized parser on a lemmatized morphologically-rich treebank such as the french treebank slightly improves parsing results . we also show that lemmatizing a similar in size subset of the english penn treebank has almost no effect on parsing performance with gold lemmas and leads to a small drop of performance when automatically assigned lemmas and pos tags are used . this highlights two facts : ( i ) lemmatization helps to reduce lexicon data-sparseness issues for french , ( ii ) it also makes the parsing process sensitive to correct assignment of pos tags to unknown words .

modeling scientific impact with topical influence regression james foulds padhraic smyth
when reviewing scientific literature , it would be useful to have automatic tools that identify the most influential scientific articles as well as how ideas propagate between articles . in this context , this paper introduces topical influence , a quantitative measure of the extent to which an article tends to spread its topics to the articles that cite it . given the text of the articles and their citation graph , we show how to learn a probabilistic model to recover both the degree of topical influence of each article and the influence relationships between articles . experimental results on corpora from two well-known computer science conferences are used to illustrate and validate the proposed approach .

probabilistic human-computer trust handling and wolfgang minker
human-computer trust has shown to be a critical factor in influencing the complexity and frequency of interaction in technical systems . particularly incomprehensible situations in human-computer interaction may lead to a reduced users trust in the system and by that influence the style of interaction . analogous to human-human interaction , explaining these situations can help to remedy negative effects . in this paper we present our approach of augmenting task-oriented dialogs with selected explanation dialogs to foster the humancomputer trust relationship in those kinds of situations . we have conducted a webbased study testing the effects of different goals of explanations on the components of human-computer trust . subsequently , we show how these results can be used in our probabilistic trust handling architecture to augment pre-defined task-oriented dialogs .

a hierarchical phrase-based model for statistical machine translation
we present a statistical phrase-based translation model that uses hierarchical phrases phrases that contain subphrases . the model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information . thus it can be seen as a shift to the formal machinery of syntaxbased translation systems without any linguistic commitment . in our experiments using bleu as a metric , the hierarchical phrasebased model achieves a relative improvement of 7.5 % over pharaoh , a state-of-the-art phrase-based system .

assembling the kazakh language corpus
this paper presents the kazakh language corpus ( klc ) , which is one of the first attempts made within a local research community to assemble a kazakh corpus . klc is designed to be a large scale corpus containing over 135 million words and conveying five stylistic genres : literary , publicistic , official , scientific and informal . along with its primary part klc comprises such parts as : ( i ) annotated sub-corpus , containing segmented documents encoded in the extensible markup language ( xml ) that marks complete morphological , syntactic , and structural characteristics of texts ; ( ii ) as well as a sub-corpus with the annotated speech data . klc has a web-based corpus management system that helps to navigate the data and retrieve necessary information . klc is also open for contributors , who are willing to make suggestions , donate texts and help with annotation of existing materials .

dialogue act modeling for non-visual web access
speech-enabled dialogue systems have the potential to enhance the ease with which blind individuals can interact with the web beyond what is possible with screen readers - the currently available assistive technology which narrates the textual content on the screen and provides shortcuts to navigate the content . in this paper , we present a dialogue act model towards developing a speech enabled browsing system . the model is based on the corpus data that was collected in a wizard-of-oz study with 24 blind individuals who were assigned a gamut of browsing tasks . the development of the model included extensive experiments with assorted feature sets and classifiers ; the outcomes of the experiments and the analysis of the results are presented .

what to do about bad language on the internet
the rise of social media has brought computational linguistics in ever-closer contact with bad language : text that defies our expectations about vocabulary , spelling , and syntax . this paper surveys the landscape of bad language , and offers a critical review of the nlp communitys response , which has largely followed two paths : normalization and domain adaptation . each approach is evaluated in the context of theoretical and empirical work on computer-mediated communication . in addition , the paper presents a quantitative analysis of the lexical diversity of social media text , and its relationship to other corpora .

a new chinese natural language understanding architecture based on multilayer search mechanism
a classical chinese natural language understanding ( nlu ) architecture usually includes several nlu components which are executed with some mechanism . a new multilayer search mechanism ( msm ) which integrates and quantifies these components into a uniform multilayer treelike architecture is presented in this paper . the mechanism gets the optimal result with search algorithms . the components in msm affect each other . at last , the performance of each component is enhanced . we built a practical system cup ( chinese understanding platform ) based on msm with three layers . by the experiments on word segmentation , a better performance was achieved . in theory the normal cascade and feedback mechanism are just some special cases of msm .

errors in wikis : new challenges and new opportunities a discussion
this discussion document concerns the challenges to assessments of reliability posed by wikis and the potential for language processing techniques for aiding readers to decide whether to trust particular text .

incremental hypothesis alignment with flexible matching for building
this paper describes the incremental hypothesis alignment algorithm used in the bbn submissions to the wmt09 system combination task . the alignment algorithm used a sentence specific alignment order , flexible matching , and new shift heuristics . these refinements yield more compact confusion networks compared to using the pair-wise or incremental ter alignment algorithms . this should reduce the number of spurious insertions in the system combination output and the system combination weight tuning converges faster . system combination experiments on the wmt09 test sets from five source languages to english are presented . the best bleu scores were achieved by combing the english outputs of three systems from all five source languages .

tagging icelandic text using a linguistic and a statistical tagger
we describe our linguistic rule-based tagger icetagger , and compare its tagging accuracy to the tnt tagger , a state-of-theart statistical tagger , when tagging icelandic , a morphologically complex language . evaluation shows that the average tagging accuracy is 91.54 % and 90.44 % , obtained by icetagger and tnt , respectively . when tag profile gaps in the lexicon , used by the tnt tagger , are filled with tags produced by our morphological analyser icemorphy , tnts tagging accuracy increases to 91.18 % .

classifying ellipsis in dialogue : a machine learning approach
this paper presents a machine learning approach to bare sluice disambiguation in dialogue . we extract a set of heuristic principles from a corpus-based sample and formulate them as probabilistic horn clauses . we then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different machine learning algorithms : slipper , a rule-based learning algorithm , and timbl , a memory-based system . both learners perform well , yielding similar success rates of approx 90 % . the results show that the features in terms of which we formulate our heuristic principles have significant predictive power , and that rules that closely resemble our horn clauses can be learnt automatically from these features .

cross-lingual semantic relatedness using encyclopedic knowledge
in this paper , we address the task of crosslingual semantic relatedness . we introduce a method that relies on the information extracted from wikipedia , by exploiting the interlanguage links available between wikipedia versions in multiple languages . through experiments performed on several language pairs , we show that the method performs well , with a performance comparable to monolingual measures of relatedness .

learning the scope of negation via shallow semantic parsing
in this paper we present a simplified shallow semantic parsing approach to learning the scope of negation ( son ) . this is done by formulating it as a shallow semantic parsing problem with the negation signal as the predicate and the negation scope as its arguments . our parsing approach to son learning differs from the state-of-the-art chunking ones in two aspects . first , we extend son learning from the chunking level to the parse tree level , where structured syntactic information is available . second , we focus on determining whether a constituent , rather than a word , is negated or not , via a simplified shallow semantic parsing framework . evaluation on the bioscope corpus shows that structured syntactic information is effective in capturing the domination relationship between a negation signal and its dominated arguments . it also shows that our parsing approach much outperforms the state-of-the-art chunking ones .

answer validation by information distance calculation
in this paper , an information distance based approach is proposed to perform answer validation for question answering system . to validate an answer candidate , the approach calculates the conditional information distance between the question focus and the candidate under certain condition pattern set . heuristic methods are designed to extract question focus and generate proper condition patterns from question . general search engines are employed to estimate the kolmogorov complexity , hence the information distance . experimental results show that our approach is stable and flexible , and outperforms traditional tfidf methods .

grammar error detection with best approximated parse
in this paper , we propose that grammar error detection be disambiguated in generating the connected parse ( s ) of optimal merit for the full input utterance , in overcoming the cheapest error . the detected error ( s ) are described as violated grammatical constraints in a framework for modeltheoretic syntax ( mts ) . we present a parsing algorithm for mts , which only relies on a grammar of well-formedness , in that the process does not require any extragrammatical resources , additional rules for constraint relaxation or error handling , or any recovery process .

a generalized alignment-free phrase extraction
in this paper , we present a phrase extraction algorithm using a translation lexicon , a fertility model , and a simple distortion model . except these models , we do not need explicit word alignments for phrase extraction . for each phrase pair ( a block ) , a bilingual lexicon based score is computed to estimate the translation quality between the source and target phrase pairs ; a fertility score is computed to estimate how good the lengths are matched between phrase pairs ; a center distortion score is computed to estimate the relative position divergence between the phrase pairs . we presented the results and our experience in the shared tasks on frenchenglish .

computational mechanisms for pun generation
computer pun-generators have so far relied on arbitrary semantic content , not linked to the immediate context . the mechanisms used , although tractable , may be of limited applicability . integrating puns into normal text may involve complex search .

space characters in chinese semi-structured texts
space characters can have an important role in disambiguating text . however , few , if any , chinese information extraction systems make full use of space characters . however , it seems that treatment of space characters is necessary , especially in cases of extracting information from semi-structured documents . this investigation aims to address the importance of space characters in chinese information extraction by parsing some semi-structured documents with two similar grammars - one with treatment for space characters , the other ignoring it . this paper also introduces two post processing filters to further improve treatment of space characters . results show that the grammar that takes account of spaces clearly out-performs the one that ignores them , and so concludes that space characters can play a useful role in information extraction .

reranking for biomedical named-entity recognition kazuhiro yoshida junichi tsujii
this paper investigates improvement of automatic biomedical named-entity recognition by applying a reranking method to the coling 2004 jnlpba shared task of bioentity recognition . our system has a common reranking architecture that consists of a pipeline of two statistical classifiers which are based on log-linear models . the architecture enables the reranker to take advantage of features which are globally dependent on the label sequences , and features from the labels of other sentences than the target sentence . the experimental results show that our system achieves the labeling accuracies that are comparable to the best performance reported for the same task , thanks to the 1.55 points of f-score improvement by the reranker .

predicting sentences using n-gram language models
we explore the benefit that users in several application areas can experience from a tab-complete editing assistance function . we develop an evaluation metric and adapt n -gram language models to the problem of predicting the subsequent words , given an initial text fragment . using an instance-based method as baseline , we empirically study the predictability of call-center emails , personal emails , weather reports , and cooking recipes .

utexas : natural language semantics using distributional semantics and
we represent natural language semantics by combining logical and distributional information in probabilistic logic . we use markov logic networks ( mln ) for the rte task , and probabilistic soft logic ( psl ) for the sts task . the system is evaluated on the sick dataset . our best system achieves 73 % accuracy on the rte task , and a pearsons correlation of 0.71 on the sts task .

opinion graphs for polarity and discourse classification
this work shows how to construct discourse-level opinion graphs to perform a joint interpretation of opinions and discourse relations . specifically , our opinion graphs enable us to factor in discourse information for polarity classification , and polarity information for discourse-link classification . this inter-dependent framework can be used to augment and improve the performance of local polarity and discourse-link classifiers .

mining the web for relations between digital devices using a probabilistic maximum margin model
searching and reading the web is one of the principal methods used to seek out information to resolve problems about technology in general and digital devices in particular . this paper addresses the problem of text mining in the digital devices domain . in particular , we address the task of detecting semantic relations between digital devices in the text of web pages . we use a nave bayes model trained to maximize the margin and compare its performance with several other comparable methods . we construct a novel dataset which consists of segments of text extracted from the web , where each segment contains pairs of devices . we also propose a novel , inexpensive and very effective way of getting people to label text data using a web service , the mechanical turk . our results show that the maximum margin model consistently outperforms the other methods .

sentence generation as a planning problem
we translate sentence generation from tag grammars with semantic and pragmatic information into a planning problem by encoding the contribution of each word declaratively and explicitly . this allows us to exploit the performance of off-the-shelf planners . it also opens up new perspectives on referring expression generation and the relationship between language and action .

modeling semantic relevance for question-answer pairs in web social communities
quantifying the semantic relevance between questions and their candidate answers is essential to answer detection in social media corpora . in this paper , a deep belief network is proposed to model the semantic relevance for question-answer pairs . observing the textual similarity between the community-driven questionanswering ( cqa ) dataset and the forum dataset , we present a novel learning strategy to promote the performance of our method on the social community datasets without hand-annotating work . the experimental results show that our method outperforms the traditional approaches on both the cqa and the forum corpora .

extracting opinion targets and opinion words from online reviews with graph co-ranking
extracting opinion targets and opinion words from online reviews are two fundamental tasks in opinion mining . this paper proposes a novel approach to collectively extract them with graph coranking . first , compared to previous methods which solely employed opinion relations among words , our method constructs a heterogeneous graph to model two types of relations , including semantic relations and opinion relations . next , a co-ranking algorithm is proposed to estimate the confidence of each candidate , and the candidates with higher confidence will be extracted as opinion targets/words . in this way , different relations make cooperative effects on candidates confidence estimation . moreover , word preference is captured and incorporated into our coranking algorithm . in this way , our coranking is personalized and each candidates confidence is only determined by its preferred collocations . it helps to improve the extraction precision . the experimental results on three data sets with different sizes and languages show that our approach achieves better performance than state-of-the-art methods .

tjp : using twitter to analyze the polarity of contexts tawunrat chalothorn jeremy ellman
this paper presents our system , tjp , which participated in semeval 2013 task 2 part a : contextual polarity disambiguation . the goal of this task is to predict whether marked contexts are positive , neutral or negative . however , only the scores of positive and negative class will be used to calculate the evaluation result using f-score . we chose to work as constrained , which used only the provided training and development data without additional sentiment annotated resources . our approach considered unigram , bigram and trigram using nave bayes training model with the objective of establishing a simpleapproach baseline . our system achieved fscore 81.23 % and f-score 78.16 % in the results for sms messages and tweets respectively .

cambridge : parser evaluation using textual entailment by grammatical relation comparison
this paper describes the cambridge submission to the semeval-2010 parser evaluation using textual entailment ( pete ) task . we used a simple definition of entailment , parsing both t and h with the c & c parser and checking whether the core grammatical relations ( subject and object ) produced for h were a subset of those for t. this simple system achieved the top score for the task out of those systems submitted . we analyze the errors made by the system and the potential role of the task in parser evaluation .

automated skimming in response to questions for nonvisual readers
this paper presents factors in designing a system for automatically skimming text documents in response to a question . the system will take a potentially complex question and a single document and return a web page containing links to text related to the question . the goal is that these text areas be those that visual readers would spend the most time on when skimming for the answer to a question . to identify these areas , we had visual readers skim for an answer to a complex question while being tracked by an eye-tracking system . analysis of these results indicates that text with semantic connections to the question are of interest , but these connections are much looser than can be identified with traditional question-answering or information retrieval techniques . instead , we are expanding traditional semantic treatments by using a web search . the goal of this system is to give nonvisual readers information similar to what visual readers get when skimming through a document in response to a question .

understanding verbs based on overlapping verbs senses
natural language can be easily understood by everyone irrespective of their differences in age or region or qualification . the existence of a conceptual base that underlies all natural languages is an accepted claim as pointed out by schank in his conceptual dependency ( cd ) theory . inspired by the cd theory and theories in indian grammatical tradition , we propose a new set of meaning primitives in this paper . we claim that this new set of primitives captures the meaning inherent in verbs and help in forming an inter-lingual and computable ontological classification of verbs . we have identified seven primitive overlapping verb senses which substantiate our claim . the percentage of coverage of these primitives is 100 % for all verbs in sanskrit and hindi and 3750 verbs in english .

an information theoretic approach to bilingual word clustering
we present an information theoretic objective for bilingual word clustering that incorporates both monolingual distributional evidence as well as cross-lingual evidence from parallel corpora to learn high quality word clusters jointly in any number of languages . the monolingual component of our objective is the average mutual information of clusters of adjacent words in each language , while the bilingual component is the average mutual information of the aligned clusters . to evaluate our method , we use the word clusters in an ner system and demonstrate a statistically significant improvement in f1 score when using bilingual word clusters instead of monolingual clusters .

citation resolution : a method for evaluating context-based citation
wouldnt it be helpful if your text editor automatically suggested papers that are relevant to your research wouldnt it be even better if those suggestions were contextually relevant in this paper we name a system that would accomplish this a context-based citation recommendation ( cbcr ) system . we specifically present citation resolution , a method for the evaluation of cbcr systems which exclusively uses readily-available scientific articles . exploiting the human judgements that are already implicit in available resources , we avoid purpose-specific annotation . we apply this evaluation to three sets of methods for representing a document , based on a ) the contents of the document , b ) the surrounding contexts of citations to the document found in other documents , and c ) a mixture of the two .

chinese named entity recognition with cascaded hybrid model
we propose a high-performance cascaded hybrid model for chinese ner . firstly , we use boosting , a standard and theoretically wellfounded machine learning method to combine a set of weak classifiers together into a base system . secondly , we introduce various types of heuristic human knowledge into markov logic networks ( mlns ) , an effective combination of first-order logic and probabilistic graphical models to validate boosting ner hypotheses . experimental results show that the cascaded hybrid model significantly outperforms the state-of-the-art boosting model .

generating and visualizing a soccer knowledge base
this demo abstract describes the smartweb ontology-based annotation system ( soba ) . a key feature of soba is that all information is extracted and stored with respect to the smartweb integrated ontology ( swinto ) . in this way , other components of the systems , which use the same ontology , can access this information in a straightforward way . we will show how information extracted by soba is visualized within its original context , thus enhancing the browsing experience of the end user .

k-qard : a practical korean question answering framework for
we present a korean question answering framework for restricted domains , called k-qard . k-qard is developed to achieve domain portability and robustness , and the framework is successfully applied to build question answering systems for several domains .

learning latent word representations for domain adaptation using supervised word clustering
domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain . in this paper , we develop a novel representation learning approach to address domain adaptation for text classification with automatically induced discriminative latent features , which are generalizable across domains while informative to the prediction task . specifically , we propose a hierarchical multinomial naive bayes model with latent variables to conduct supervised word clustering on labeled documents from both source and target domains , and then use the produced cluster distribution of each word as its latent feature representation for domain adaptation . we train this latent graphical model using a simple expectation-maximization ( em ) algorithm . we empirically evaluate the proposed method with both cross-domain document categorization tasks on reuters-21578 dataset and cross-domain sentiment classification tasks on amazon product review dataset . the experimental results demonstrate that our proposed approach achieves superior performance compared with alternative methods .

predicting cognitively salient modifiers of the constitutive parts of concepts
when subjects describe concepts in terms of their characteristic properties , they often produce composite properties , e. g. , rabbits are said to have long ears , not just ears . we present a set of simple methods to extract the modifiers of composite properties ( in particular : parts ) from corpora . we achieve our best performance by combining evidence about the association between the modifier and the part both within the context of the target concept and independently of it . we show that this performance is relatively stable across languages ( italian and german ) and for production vs. perception of properties .

cross-instance tuning of unsupervised document clustering algorithms and sanjeev khudanpur
in unsupervised learning , where no training takes place , one simply hopes that the unsupervised learner will work well on any unlabeled test collection . however , when the variability in the data is large , such hope may be unrealistic ; a tuning of the unsupervised algorithm may then be necessary in order to perform well on new test collections . in this paper , we show how to perform such a tuning in the context of unsupervised document clustering , by ( i ) introducing a degree of freedom , , into two leading informationtheoretic clustering algorithms , through the use of generalized mutual information quantities ; and ( ii ) selecting the value of based on clusterings of similar , but supervised document collections ( crossinstance tuning ) . one option is to perform a tuning that directly minimizes the error on the supervised data sets ; another option is to use strapping ( eisner and karakos , 2005 ) , which builds a classifier that learns to distinguish good from bad clusterings , and then selects the with the best predicted clustering on the test set . experiments from the 20 newsgroups corpus show that , although both techniques improve the performance of the baseline algorithms , strapping is clearly a better choice for cross-instance tuning . this work was partially supported by the darpa gale program ( contract no hr0011-06-2-0001 ) and by the jhu wse/apl partnership fund .

a re-examination of lexical association measures
we review lexical association measures ( ams ) that have been employed by past work in extracting multiword expressions . our work contributes to the understanding of these ams by categorizing them into two groups and suggesting the use of rank equivalence to group ams with the same ranking performance . we also examine how existing ams can be adapted to better rank english verb particle constructions and light verb constructions . specifically , we suggest normalizing ( pointwise ) mutual information and using marginal frequencies to construct penalization terms . we empirically validate the effectiveness of these modified ams in detection tasks in english , performed on the penn treebank , which shows significant improvement over the original ams .

multidisciplinary instruction with the natural language toolkit
the natural language toolkit ( nltk ) is widely used for teaching natural language processing to students majoring in linguistics or computer science . this paper describes the design of nltk , and reports on how it has been used effectively in classes that involve different mixes of linguistics and computer science students . we focus on three key issues : getting started with a course , delivering interactive demonstrations in the classroom , and organizing assignments and projects . in each case , we report on practical experience and make recommendations on how to use nltk to maximum effect .

a comparison and semi-quantitative analysis of words and character-bigrams as features in chinese text categorization
words and character-bigrams are both used as features in chinese text processing tasks , but no systematic comparison or analysis of their values as features for chinese text categorization has been reported heretofore . we carry out here a full performance comparison between them by experiments on various document collections ( including a manually word-segmented corpus as a golden standard ) , and a semi-quantitative analysis to elucidate the characteristics of their behavior ; and try to provide some preliminary clue for feature term choice ( in most cases , character-bigrams are better than words ) and dimensionality setting in text categorization systems .

automatic evaluation method for machine translation using
as described in this paper , we propose a new automatic evaluation method for machine translation using noun-phrase chunking . our method correctly determines the matching words between two sentences using corresponding noun phrases . moreover , our method determines the similarity between two sentences in terms of the noun-phrase order of appearance . evaluation experiments were conducted to calculate the correlation among human judgments , along with the scores produced using automatic evaluation methods for mt outputs obtained from the 12 machine translation systems in ntcir7 . experimental results show that our method obtained the highest correlations among the methods in both sentence-level adequacy and fluency .

muck : a toolkit for extracting and visualizing semantic dimensions of large text collections
users with large text collections are often faced with one of two problems ; either they wish to retrieve a semanticallyrelevant subset of data from the collection for further scrutiny ( needle-in-a-haystack ) or they wish to glean a high-level understanding of how a subset compares to the parent corpus in the context of aforementioned semantic dimensions ( forestfor-the-trees ) . in this paper , i describe muck 1 , an open-source toolkit that addresses both of these problems through a distributed text processing engine with an interactive visualization interface .

integrating joint n-gram features into a discriminative training framework
phonetic string transduction problems , such as letter-to-phoneme conversion and name transliteration , have recently received much attention in the nlp community . in the past few years , two methods have come to dominate as solutions to supervised string transduction : generative joint n-gram models , and discriminative sequence models . both approaches benefit from their ability to consider large , flexible spans of source context when making transduction decisions . however , they encode this context in different ways , providing their respective models with different information . to combine the strengths of these two systems , we include joint n-gram features inside a state-of-the-art discriminative sequence model . we evaluate our approach on several letter-to-phoneme and transliteration data sets . our results indicate an improvement in overall performance with respect to both the joint n-gram approach and traditional feature sets for discriminative models .

corpus annotation by generation
as the interest in annotated corpora is spreading , there is increasing concern with using existing language technology for corpus processing . in this paper we explore the idea of using natural language generation systems for corpus annotation . resources for generation systems often focus on areas of linguistic variability that are under-represented in analysis-directed approaches . therefore , making use of generation resources promises some significant extensions in the kinds of annotation information that can be captured . we focus here on exploring the use of the kpml ( komet-penman multilingual ) generation system for corpus annotation . we describe the kinds of linguistic information covered in kpml and show the steps involved in creating a standard xml corpus representation from kpmls generation output .

inducing history representations for broad coverage statistical parsing
we present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser . the resulting statistical parser achieves performance ( 89.1 % f-measure ) on the penn treebank which is only 0.6 % below the best current parser for this task , despite using a smaller vocabulary size and less prior linguistic knowledge . crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history , and no use of hard independence assumptions .

corpus-based semantic lexicon induction with web-based corroboration
various techniques have been developed to automatically induce semantic dictionaries from text corpora and from the web . our research combines corpus-based semantic lexicon induction with statistics acquired from the web to improve the accuracy of automatically acquired domain-specific dictionaries . we use a weakly supervised bootstrapping algorithm to induce a semantic lexicon from a text corpus , and then issue web queries to generate co-occurrence statistics between each lexicon entry and semantically related terms . the web statistics provide a source of independent evidence to confirm , or disconfirm , that a word belongs to the intended semantic category . we evaluate this approach on 7 semantic categories representing two domains . our results show that the web statistics dramatically improve the ranking of lexicon entries , and can also be used to filter incorrect entries .

retrieval for information extraction tasks irisa cnrs
this paper describes the information extraction techniques developed in the framework of the participation of irisatexmex to the following bionlp-st13 tasks : bacterial biotope subtasks 1 and 2 , and graph regulation network . the approaches developed are general-purpose ones and do not rely on specialized preprocessing , nor specialized external data , and they are expected to work independently of the domain of the texts processed . they are classically based on machine learning techniques , but we put the emphasis on the use of similarity measures inherited from the information retrieval domain ( okapi-bm25 ( robertson et al , 1998 ) , language modeling ( hiemstra , 1998 ) ) . through the good results obtained for these tasks , we show that these simple settings are competitive provided that the representation and similarity chosen are well suited for the task .

leveraging lexical cohesion and disruption for topic segmentation
topic segmentation classically relies on one of two criteria , either finding areas with coherent vocabulary use or detecting discontinuities . in this paper , we propose a segmentation criterion combining both lexical cohesion and disruption , enabling a trade-off between the two . we provide the mathematical formulation of the criterion and an efficient graph based decoding algorithm for topic segmentation . experimental results on standard textual data sets and on a more challenging corpus of automatically transcribed broadcast news shows demonstrate the benefit of such a combination . gains were observed in all conditions , with segments of either regular or varying length and abrupt or smooth topic shifts . long segments benefit more than short segments . however the algorithm has proven robust on automatic transcripts with short segments and limited vocabulary reoccurrences .

harmonizing lexical data for their linking to knowledge objects in the linked data framework
in this position paper we discuss some of the experiences we made in describing lexical data using representation formalisms that are compatible for the publication of such data in the linked data framework . while we see a huge potential in the emerging linguistic linked open data , also supporting the publication of less-resourced language data on the same platform as for mainstream languages , we are wondering if , parallel to the widening of linking language data to both other language data and encyclopaedic knowledge present in the linked data cloud , it would not be beneficial to give more focus more on harmonization and merging of rdf encoded lexical data , instead of establishing links between such resources in the linked data .

addressing the resource bottleneck to create large-scale annotated texts
large-scale linguistically annotated resources have become available in recent years . this is partly due to sophisticated automatic and semiautomatic approaches that work well on specific tasks such as part-ofspeech tagging . for more complex linguistic phenomena like anaphora resolution there are no tools that result in high-quality annotations without massive user intervention . annotated corpora of the size needed for modern computational linguistics research can not however be created by small groups of hand annotators . the anawiki project strikes a balance between collecting high-quality annotations from experts and applying a game-like approach to collecting linguistic annotation from the general web population . more generally , anawiki is a project that explores to what extend expert annotations can be substituted by a critical mass of non-expert judgements . 375 376 chamberlain , poesio , and kruschwitz

large-scale expected bleu training of phrase-based reordering models
recent work by cherry ( 2013 ) has shown that directly optimizing phrase-based reordering models towards bleu can lead to significant gains . their approach is limited to small training sets of a few thousand sentences and a similar number of sparse features . we show how the expected bleu objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements . a comparison to likelihood training demonstrates that expected bleu is vastly more effective . our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 bleu in a single-reference setting on a french-english wmt 2012 setup .

an application of ontological semantics to information assurance
the paper deals with the latest application of natural language processing ( nlp ) , specifically of ontological semantics ( onse ) to natural language information assurance and security ( nl ias ) . it demonstrates how the existing ideas , methods , and resources of ontological semantics can be applied to detect deception in nl text ( and , eventually , in data and other media as well ) . after stating the problem , the paper proceeds to a brief

on the information conveyed by discourse markers
discourse connectives play an important role in making a text coherent and helping humans to infer relations between spans of text . using the penn discourse treebank , we investigate what information relevant to inferring discourse relations is conveyed by discourse connectives , and whether the specificity of discourse relations reflects general cognitive biases for establishing coherence . we also propose an approach to measure the effect of a discourse marker on sense identification according to the different levels of a relation sense hierarchy . this will open a way to the computational modeling of discourse processing .

a web survey on the use of active learning to support annotation of text data
as supervised machine learning methods for addressing tasks in natural language processing ( nlp ) prove increasingly viable , the focus of attention is naturally shifted towards the creation of training data . the manual annotation of corpora is a tedious and time consuming process . to obtain high-quality annotated data constitutes a bottleneck in machine learning for nlp today . active learning is one way of easing the burden of annotation . this paper presents a first probe into the nlp research community concerning the nature of the annotation projects undertaken in general , and the use of active learning as annotation support in particular .

identifying important features for graph retrieval
infographics , such as bar charts and line graphs , occur often in popular media and are a rich knowledge source that should be accessible to users . unfortunately , information retrieval research has focused on the retrieval of text documents and images , with almost no attention specifically directed toward the retrieval of information graphics . our work is the first to directly tackle the retrieval of infographics and to design a system that takes into account their unique characteristics . learning-to-rank algorithms are applied on a large set of features to develop several models for infographics retrieval . evaluation of the models shows that features pertaining to the structure and the content of graphics should be taken into account when retrieving graphics and that doing so results in a model with better performance than a baseline model that relies on matching query words with words in the graphic .

person name entity recognition for arabic
named entity recognition ( ner ) is nowadays an important task , which is responsible for the identification of proper names in text and their classification as different types of named entity such as people , locations , and organizations . in this paper , we present our attempt at the recognition and extraction of the most important proper name entity , that is , the person name , for the arabic language . we developed the system , person name entity recognition for arabic ( pera ) , using a rule-based approach . the system consists of a lexicon , in the form of gazetteer name lists , and a grammar , in the form of regular expressions , which are responsible for recognizing person name entities . the pera system is evaluated using a corpus that is tagged in a semi-automated way . the system performance results achieved were satisfactory and confirm to the targets set forth for the precision , recall , and fmeasure .

improved features and grammar selection for syntax-based mt
we present the carnegie mellon university stat-xfer group submission to the wmt 2010 shared translation task . updates to our syntax-based smt system mainly fell in the areas of new feature formulations in the translation model and improved filtering of scfg rules . compared to our wmt 2009 submission , we report a gain of 1.73 bleu by using the new features and decoding environment , and a gain of up to 0.52 bleu from improved grammar selection .

a handsome set of metrics to measure utterance classification performance in spoken dialog systems
we present a set of metrics describing classification performance for individual contexts of a spoken dialog system as well as for the entire system . we show how these metrics can be used to train and tune system components and how they are related to caller experience , a subjective measure describing how well a caller was treated by the dialog system .

making relative sense : from word-graphs to semantic frames robert porzel berenike loos vanessa micelli
scaling up from controlled single domain spoken dialogue systems towards conversational , multi-domain and multimodal dialogue systems poses new challenges for the reliable processing of less restricted user utterances . in this paper we explore the feasibility to employ a general purpose ontology for various tasks involved in processing the users utterances .

probabilistic frame induction jackie chi kit cheung
in natural-language discourse , related events tend to appear near each other to describe a larger scenario . such structures can be formalized by the notion of a frame ( a.k.a . template ) , which comprises a set of related events and prototypical participants and event transitions . identifying frames is a prerequisite for information extraction and natural language generation , and is usually done manually . methods for inducing frames have been proposed recently , but they typically use ad hoc procedures and are difficult to diagnose or extend . in this paper , we propose the first probabilistic approach to frame induction , which incorporates frames , events , and participants as latent topics and learns those frame and event transitions that best explain the text . the number of frame components is inferred by a novel application of a split-merge method from syntactic parsing . in end-to-end evaluations from text to induced frames and extracted facts , our method produces state-of-the-art results while substantially reducing engineering effort .

exploring semantic constraints for document retrieval
in this paper , we explore the use of structured content as semantic constraints for enhancing the performance of traditional term-based document retrieval in special domains . first , we describe a method for automatic extraction of semantic content in the form of attribute-value ( av ) pairs from natural language texts based on domain models constructed from a semistructured web resource . then , we explore the effect of combining a state-ofthe-art term-based ir system and a simple constraint-based search system that uses the extracted av pairs . our evaluation results have shown that such combination produces some improvement in ir performance over the term-based ir system on our test collection .

utilizing extra-sentential context for parsing
syntactic consistency is the preference to reuse a syntactic construction shortly after its appearance in a discourse . we present an analysis of the wsj portion of the penn treebank , and show that syntactic consistency is pervasive across productions with various lefthand side nonterminals . then , we implement a reranking constituent parser that makes use of extra-sentential context in its feature set . using a linear-chain conditional random field , we improve parsing accuracy over the generative baseline parser on the penn treebank wsj corpus , rivalling a similar model that does not make use of context . we show that the context-aware and the context-ignorant rerankers perform well on different subsets of the evaluation data , suggesting a combined approach would provide further improvement . we also compare parses made by models , and suggest that context can be useful for parsing by capturing structural dependencies between sentences as opposed to lexically governed dependencies .

systemt : an algebraic approach to declarative information extraction
as information extraction ( ie ) becomes more central to enterprise applications , rule-based ie engines have become increasingly important . in this paper , we describe systemt , a rule-based ie system whose basic design removes the expressivity and performance limitations of current systems based on cascading grammars . systemt uses a declarative rule language , aql , and an optimizer that generates high-performance algebraic execution plans for aql rules . we compare systemts approach against cascading grammars , both theoretically and with a thorough experimental evaluation . our results show that systemt can deliver result quality comparable to the state-of-theart and an order of magnitude higher annotation throughput .

confidence-based rewriting of machine translation output lingua et machina , le chesnay , france
numerous works in statistical machine translation ( smt ) have attempted to identify better translation hypotheses obtained by an initial decoding using an improved , but more costly scoring function . in this work , we introduce an approach that takes the hypotheses produced by a state-ofthe-art , reranked phrase-based smt system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phraselevel confidence . in the medical domain , we obtain a 1.9 bleu improvement over a reranked baseline exploiting the same scoring function , corresponding to a 5.4 bleu improvement over the original moses baseline . we show that if an indication of which phrases require rewriting is provided , our automatic rewriting procedure yields an additional improvement of 1.5 bleu . various analyses , including a manual error analysis , further illustrate the good performance and potential for improvement of our approach in spite of its simplicity .

problems of reusing an existing mt system
this paper describes an attempt to recycle parts of the czech-to-russian machine translation system ( mt ) in the new czech-to-english mt system . the paper describes the overall architecture of the new system and the details of the modules which have been added . a special attention is paid to the problem of named entity recognition and to the method of automatic acquisition of lexico-syntactic information for the bilingual dictionary of the system . the paper concentrates on the problems encountered in the process of reusing existing modules and their solution .

swatcs : combining simple classifiers with estimated accuracy
this paper is an overview of the swatcs system submitted to semeval-2013 task 2a : contextual polarity disambiguation . the sentiment of individual phrases within a tweet are labeled using a combination of classifiers trained on a range of lexical features . the classifiers are combined by estimating the accuracy of the classifiers on each tweet . performance is measured when using only the provided training data , and separately when including external data .

serial combination of rules and statistics : a case study in czech
a hybrid system is described which combines the strength of manual rulewriting and statistical learning , obtaining results superior to both methods if applied separately . the combination of a rule-based system and a statistical one is not parallel but serial : the rule-based system performing partial disambiguation with recall close to 100 % is applied first , and a trigram hmm tagger runs on its results . an experiment in czech tagging has been performed with encouraging results .

two languages are better than one ( for syntactic parsing )
we show that jointly parsing a bitext can substantially improve parse quality on both sides . in a maximum entropy bitext parsing model , we define a distribution over source trees , target trees , and node-to-node alignments between them . features include monolingual parse scores and various measures of syntactic divergence . using the translated portion of the chinese treebank , our model is trained iteratively to maximize the marginal likelihood of training tree pairs , with alignments treated as latent variables . the resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 f1 at predicting english side trees and 1.8 f1 at predicting chinese side trees ( the highest published numbers on these corpora ) . moreover , these improved trees yield a 2.4 bleu increase when used in a downstream mt evaluation .

semi-automated named entity annotation linguistic data consortium , division of oncology , childrens hospital of philadelphia philadelphia pa
we investigate a way to partially automate corpus annotation for named entity recognition , by requiring only binary decisions from an annotator . our approach is based on a linear sequence model trained using a k-best mira learning algorithm . we ask an annotator to decide whether each mention produced by a high recall tagger is a true mention or a false positive . we conclude that our approach can reduce the effort of extending a seed training corpus by up to 58 % .

automatic feature selection for agenda-based dependency parsing
in this paper we present an in-depth study on automatic feature selection for beam-search dependency parsers . the search strategy is inherited from the one implemented in maltoptimizer , but searches in a much larger set of feature templates that could lead to a higher number of combinations . our models provide results that are on par with models trained with a larger set of feature templates , and this implies that our models provide faster training and parsing times . moreover , the results establish the state of the art for some of the languages .

asymmetric features of human generated translation
distinct properties of translated text have been the subject of research in linguistics for many year ( baker , 1993 ) . in recent years computational methods have been developed to empirically verify the linguistic theories about translated text ( baroni and bernardini , 2006 ) . while many characteristics of translated text are more apparent in comparison to the original text , most of the prior research has focused on monolingual features of translated and original text . the contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level , rather than using monolingual statistics at the document level . we show that these bilingual features outperform the monolingual features used in prior work ( kurokawa et al. , 2009 ) for the task of classifying translation direction .

a survey on the role of negation in sentiment analysis
this paper presents a survey on the role of negation in sentiment analysis . negation is a very common linguistic construction that affects polarity and , therefore , needs to be taken into consideration in sentiment analysis . we will present various computational approaches modeling negation in sentiment analysis . we will , in particular , focus on aspects , such as level of representation used for sentiment analysis , negation word detection and scope of negation . we will also discuss limits and challenges of negation modeling on that task .

using large-scale parser output to guide grammar development
this paper reports on guiding parser development by extracting information from output of a large-scale parser applied to wikipedia documents . data-driven parser improvement is especially important for applications where the corpus may differ from that originally used to develop the core grammar and where efficiency concerns affect whether a new construction should be added , or existing analyses modified . the large size of the corpus in question also brings scalability concerns to the foreground .

reduced n-gram models for english and chinese corpora le q ha , p hanna , d w stewart and f j smith
statistical language models should improve as the size of the n-grams increases from 3 to 5 or higher . however , the number of parameters and calculations , and the storage requirement increase very rapidly if we attempt to store all possible combinations of n-grams . to avoid these problems , the reduced n-grams approach previously developed by oboyle ( 1993 ) can be applied . a reduced n-gram language model can store an entire corpuss phrase-history length within feasible storage limits . another theoretical advantage of reduced n-grams is that they are closer to being semantically complete than traditional models , which include all n-grams . in our experiments , the reduced n-gram zipf curves are first presented , and compared with previously obtained conventional n-grams for both english and chinese . the reduced n-gram model is then applied to large english and chinese corpora . for english , we can reduce the model sizes , compared to 7-gram traditional model sizes , with factors of 14.6 for a 40-million-word corpus and 11.0 for a 500-million-word corpus while obtaining 5.8 % and 4.2 % improvements in perplexities . for chinese , we gain a 16.9 % perplexity reductions and we reduce the model size by a factor larger than 11.2. this paper is a step towards the modeling of english and chinese using semantically complete phrases in an n-gram model .

gplsi : word coarse-grained disambiguation aided by basic level ruben izquierdo armando suarez
we present a corpus-based supervised learning system for coarse-grained sense disambiguation . in addition to usual features for training in word sense disambiguation , our system also uses base level concepts automatically obtained from wordnet . base level concepts are some synsets that generalize a hyponymy subhierarchy , and provides an extra level of abstraction as well as relevant information about the context of a word to be disambiguated . our experiments proved that using this type of features results on a significant improvement of precision . our system has achieved almost 0.8 f1 ( fifth place ) in the coarsegrained english all-words task using a very simple set of features plus base level concepts annotation .

unsupervised morphological segmentation with log-linear models
morphological segmentation breaks words into morphemes ( the basic semantic units ) . it is a key component for natural language processing systems . unsupervised morphological segmentation is attractive , because in every language there are virtually unlimited supplies of text , but very few labeled resources . however , most existing model-based systems for unsupervised morphological segmentation use directed generative models , making it difficult to leverage arbitrary overlapping features that are potentially helpful to learning . in this paper , we present the first log-linear model for unsupervised morphological segmentation . our model uses overlapping features such as morphemes and their contexts , and incorporates exponential priors inspired by the minimum description length ( mdl ) principle . we present efficient algorithms for learning and inference by combining contrastive estimation with sampling . our system , based on monolingual features only , outperforms a state-of-the-art system by a large margin , even when the latter uses bilingual information such as phrasal alignment and phonetic correspondence . on the arabic penn treebank , our system reduces f1 error by 11 % compared to morfessor .

extracting semantic orientations of phrases from dictionary
we propose a method for extracting semantic orientations of phrases ( pairs of an adjective and a noun ) : positive , negative , or neutral . given an adjective , the semantic orientation classification of phrases can be reduced to the classification of words . we construct a lexical network by connecting similar/related words . in the network , each node has one of the three orientation values and the neighboring nodes tend to have the same value . we adopt the potts model for the probability model of the lexical network . for each adjective , we estimate the states of the nodes , which indicate the semantic orientations of the adjective-noun pairs . unlike existing methods for phrase classification , the proposed method can classify phrases consisting of unseen words . we also propose to use unlabeled data for a seed set of probability computation . empirical evaluation shows the effectiveness of the proposed method .

lexical morphology in machine translation : a feasibility study
this paper presents a feasibility study for implementing lexical morphology principles in a machine translation system in order to solve unknown words . multilingual symbolic treatment of word-formation is seducing but requires an in-depth analysis of every step that has to be performed . the construction of a prototype is firstly presented , highlighting the methodological issues of such approach . secondly , an evaluation is performed on a large set of data , showing the benefits and the limits of such approach .

disambiguating noun compounds with latent semantic indexing
technical terms in text often appear as noun compounds , a frequently occurring yet highly ambiguous construction whose interpretation relies on extra-syntactic information . several statistical methods for disambiguating compounds have been reported in the literature , often with quite impressive results . however , a striking feature of all these approaches is that they rely on the existence of previously seen unambiguous compounds , meaning they are prone to the problem of sparse data . this difficulty has been overcome somewhat through the use of hand-crafted knowledge resources to collect statistics on concepts rather than noun tokens , but domain-independence has been sacrificed by doing so . we report here on work investigating the application of latent semantic indexing to provide a robust domain-independent source of the extra-syntactic knowledge necessary for noun compound disambiguation .

typed tensor decomposition of knowledge bases for relation extraction
while relation extraction has traditionally been viewed as a task relying solely on textual data , recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data , the performance of relation extraction can be improved significantly . following this new paradigm , we propose a tensor decomposition approach for knowledge base embedding that is highly scalable , and is especially suitable for relation extraction . by leveraging relational domain knowledge about entity type information , our learning algorithm is significantly faster than previous approaches and is better able to discover new relations missing from the database . in addition , when applied to a relation extraction task , our approach alone is comparable to several existing systems , and improves the weighted mean average precision of a state-of-theart method by 10 points when used as a subcomponent .

an evaluation framework for plagiarism detection martin potthast benno stein web technology & information systems alberto barrn-cedeo paolo rosso
we present an evaluation framework for plagiarism detection.1 the framework provides performance measures that address the specifics of plagiarism detection , and the pan-pc-10 corpus , which contains 64 558 artificial and 4 000 simulated plagiarism cases , the latter generated via amazons mechanical turk . we discuss the construction principles behind the measures and the corpus , and we compare the quality of our corpus to existing corpora . our analysis gives empirical evidence that the construction of tailored training corpora for plagiarism detection can be automated , and hence be done on a large scale .

preposition semantic classification via penn treebank and framenet
this paper reports on experiments in classifying the semantic role annotations assigned to prepositional phrases in both the penn treebank and framenet . in both cases , experiments are done to see how the prepositions can be classified given the datasets role inventory , using standard word-sense disambiguation features . in addition to using traditional word collocations , the experiments incorporate class-based collocations in the form of wordnet hypernyms . for treebank , the word collocations achieve slightly better performance : 78.5 % versus 77.4 % when separate classifiers are used per preposition . when using a single classifier for all of the prepositions together , the combined approach yields a significant gain at 85.8 % accuracy versus 81.3 % for wordonly collocations . for framenet , the combined use of both collocation types achieves better performance for the individual classifiers : 70.3 % versus 68.5 % . however , classification using a single classifier is not effective due to confusion among the fine-grained roles .

investigating speaker gaze and pointing behaviour
can speaker gaze and speaker arm movements be used as a practical information source for naturalistic conversational humancomputer interfaces to investigate this question , we recorded ( with eye tracking and motion capture ) a corpus of interactions with a ( wizarded ) system . in this paper , we describe the recording , analysis infrastructure that we built for such studies , and analysis we performed on these data . we find that with some initial calibration , a minimally invasive , stationary camera-based setting provides data of sufficient quality to support interaction .

national library of medicine
fda drug package inserts provide comprehensive and authoritative information about drugs . dailymed database is a repository of structured product labels extracted from these package inserts . most salient information about drugs remains in free text portions of these labels . extracting information from these portions can improve the safety and quality of drug prescription . in this paper , we present a study that focuses on resolution of coreferential information from drug labels contained in dailymed . we generalized and expanded an existing rule-based coreference resolution module for this purpose . enhancements include resolution of set/instance anaphora , recognition of appositive constructions and wider use of umls semantic knowledge . we obtained an improvement of 40 % over the baseline with unweighted average f 1 -measure using b-cubed , muc , and ceaf metrics . the results underscore the importance of set/instance anaphora and appositive constructions in this type of text and point out the shortcomings in coreference annotation in the dataset .

semantic chunk annotation for complex questions using conditional
this paper presents a crf ( conditional random field ) model for semantic chunk annotation in a chinese question and answering system ( scacqa ) . the model was derived from a corpus of real world questions , which are collected from some discussion groups on the internet . the questions are supposed to be answered by other people , so some of the questions are very complex . mutual information was adopted for feature selection . the training data collection consists of 14000 sentences and the testing data collection consists of 4000 sentences . the result shows an f-score of 93.07 % . 2008. licensed under the creative commons attribution-noncommercial-share alike 3.0 unported license ( http : //creativecommons.org/licenses/by-ncsa/3.0/ ) . some rights reserved .

non-monotonic parsing of fluent umm i mean disfluent sentences
parsing disfluent sentences is a challenging task which involves detecting disfluencies as well as identifying the syntactic structure of the sentence . while there have been several studies recently into solely detecting disfluencies at a high performance level , there has been relatively little work into joint parsing and disfluency detection that has reached that state-ofthe-art performance in disfluency detection . we improve upon recent work in this joint task through the use of novel features and learning cascades to produce a model which performs at 82.6 f-score . it outperforms the previous best in disfluency detection on two different evaluations .

reranking the berkeley and brown parsers ahmet engin ural
the brown and the berkeley parsers are two state-of-the-art generative parsers . since both parsers produce n-best lists , it is possible to apply reranking techniques to the output of both of these parsers , and to their union . we note that the standard reranker feature set distributed with the brown parser does not do well with the berkeley parser , and propose an extended set that does better . an ablation experiment shows that different parsers benefit from different reranker features .

a rule based approach for automatic clause boundary detection and classification in hindi
a complex sentence , divided into clauses , can be analyzed more easily than the complex sentence itself . we present here , the task of identification and classification of clauses in hindi text . to the best of our knowledge , not much work has been done on clause boundary identification for hindi , which makes this task more important . we have built a rule based system using linguistic cues such as coordinating conjunct , subordinating conjunct etc . our system gives 91.53 % and 80.63 % f1-scores for identification and classification for finite clauses respectively , and 60.57 % accuracy for non-finite clauses .

demonstration of an always-on companion for isolated older adults
we summarize the status of an ongoing project to develop and evaluate a companion for isolated older adults . four key scientific issues in the project are : embodiment , interaction paradigm , engagement and relationship . the system architecture is extensible and handles realtime behaviors . the system supports multiple activities , including discussing the weather , playing cards , telling stories , exercise coaching and video conferencing . a live , working demo system will be presented at the meeting .

jmwe : a java toolkit for detecting multi-word expressions
jmwe is a java library for implementing and testing algorithms that detect multi-word expression ( mwe ) tokens in text . it provides ( 1 ) a detector api , including implementations of several detectors , ( 2 ) facilities for constructing indices of mwe types that may be used by the detectors , and ( 3 ) a testing framework for measuring the performance of a mwe detector . the software is available for free download . jmwe is a java library for constructing and testing multi-word expression ( mwe ) token detectors . the original goal of the library was to detect tokens ( instances ) of mwe types in a token stream , given a list of types such as those that can be extracted from an electronic dictionary such as wordnet ( fellbaum , 1998 ) . the purpose of the library is not to discover new mwe types , but rather find instances of a set of given types in a given text . the library also supports mwe detectors that are not list-based . the functionality of the library is basic , but it is a necessary foundation for any system that wishes to use mwes in later stages of language processing . it is a natural complement to software for discovering mwe types , such as mwetoolkit ( ramisch et al. , 2010 ) or the nsp package ( banerjee and pedersen , 2003 ) . jmwe is available online for free download ( finlayson and kulkarni , 2011a ) .

cedit semantic networks manual annotation tool
we present a demonstration of an annotation tool designed to annotate texts into a semantic network formalism called multinet . the tool is based on a java swing gui and allows the annotators to edit nodes and relations in the network , as well as links between the nodes in the network and the nodes from the previous layer of annotation . the data processed by the tool in this presentation are from the english version of the wall street journal .

a deep learning approach to machine transliteration
in this paper we present a novel transliteration technique which is based on deep belief networks . common approaches use finite state machines or other methods similar to conventional machine translation . instead of using conventional nlp techniques , the approach presented here builds on deep belief networks , a technique which was shown to work well for other machine learning problems . we show that deep belief networks have certain properties which are very interesting for transliteration and possibly also for translation and that a combination with conventional techniques leads to an improvement over both components on an arabic-english transliteration task .

topic models + word alignment = a flexible framework for extracting bilingual dictionary from comparable corpus
we propose a flexible and effective framework for extracting a bilingual dictionary from comparable corpora . our approach is based on a novel combination of topic modeling and word alignment techniques . intuitively , our approach works by converting a comparable document-aligned corpus into a parallel topic-aligned corpus , then learning word alignments using co-occurrence statistics . this topicaligned corpus is similar in structure to the sentence-aligned corpus frequently used in statistical machine translation , enabling us to exploit advances in word alignment research . unlike many previous work , our framework does not require any languagespecific knowledge for initialization . furthermore , our framework attempts to handle polysemy by allowing multiple translation probability models for each word . on a large-scale wikipedia corpus , we demonstrate that our framework reliably extracts high-precision translation pairs on a wide variety of comparable data conditions .

overcoming the memory bottleneck in distributed training of latent variable models of text
large unsupervised latent variable models ( lvms ) of text , such as latent dirichlet allocation models or hidden markov models ( hmms ) , are constructed using parallel training algorithms on computational clusters . the memory required to hold lvm parameters forms a bottleneck in training more powerful models . in this paper , we show how the memory required for parallel lvm training can be reduced by partitioning the training corpus to minimize the number of unique words on any computational node . we present a greedy document partitioning technique for the task . for large corpora , our approach reduces memory consumption by over 50 % , and trains the same models up to three times faster , when compared with existing approaches for parallel lvm training .

unite competing sentiment classifiers with random forest
in this paper , we describe how we created a meta-classifier to detect the message-level sentiment of tweets . we participated in semeval-2014 task 9b by combining the results of several existing classifiers using a random forest . the results of 5 other teams from the competition as well as from 7 generalpurpose commercial classifiers were used to train the algorithm . this way , we were able to get a boost of up to 3.24 f1 score points .

# tagspace : semantic embeddings from hashtags
we describe a convolutional neural network that learns feature representations for short textual posts using hashtags as a supervised signal . the proposed approach is trained on up to 5.5 billion words predicting 100,000 possible hashtags . as well as strong performance on the hashtag prediction task itself , we show that its learned representation of text ( ignoring the hashtag labels ) is useful for other tasks as well . to that end , we present results on a document recommendation task , where it also outperforms a number of baselines .

mining wikipedia revision histories for improving sentence compression elif yamangil rani nelken
a well-recognized limitation of research on supervised sentence compression is the dearth of available training data . we propose a new and bountiful resource for such training data , which we obtain by mining the revision history of wikipedia for sentence compressions and expansions . using only a fraction of the available wikipedia data , we have collected a training corpus of over 380,000 sentence pairs , two orders of magnitude larger than the standardly used ziff-davis corpus . using this newfound data , we propose a novel lexicalized noisy channel model for sentence compression , achieving improved results in grammaticality and compression rate criteria with a slight decrease in importance .

a japanese predicate argument structure analysis using decision lists keihanna science city ,
this paper describes a new automatic method for japanese predicate argument structure analysis . the method learns relevant features to assign case roles to the argument of the target predicate using the features of the words located closest to the target predicate under various constraints such as dependency types , words , semantic categories , parts of speech , functional words and predicate voices . we constructed decision lists in which these features were sorted by their learned weights . using our method , we integrated the tasks of semantic role labeling and zero-pronoun identification , and achieved a 17 % improvement compared with a baseline method in a sentence level performance analysis .

semantic types of some generic relation arguments :
this paper presents an approach to detection of the semantic types of relation arguments employing the wordnet hierarchy . using the semeval-2007 data , we show that the method allows to generalize relation arguments with high precision for such generic relations as origin-entity , content-container , instrument-agency and some other .

towards tracking semantic change by visual analytics
this paper presents a new approach to detecting and tracking changes in word meaning by visually modeling and representing diachronic development in word contexts . previous studies have shown that computational models are capable of clustering and disambiguating senses , a more recent trend investigates whether changes in word meaning can be tracked by automatic methods . the aim of our study is to offer a new instrument for investigating the diachronic development of word senses in a way that allows for a better understanding of the nature of semantic change in general . for this purpose we combine techniques from the field of visual analytics with unsupervised methods from natural language processing , allowing for an interactive visual exploration of semantic change .

reducing weight undertraining in structured discriminative learning
discriminative probabilistic models are very popular in nlp because of the latitude they afford in designing features . but training involves complex trade-offs among weights , which can be dangerous : a few highlyindicative features can swamp the contribution of many individually weaker features , causing their weights to be undertrained . such a model is less robust , for the highly-indicative features may be noisy or missing in the test data . to ameliorate this weight undertraining , we introduce several new feature bagging methods , in which separate models are trained on subsets of the original features , and combined using a mixture model or a product of experts . these methods include the logarithmic opinion pools used by smith et al ( 2005 ) . we evaluate feature bagging on linear-chain conditional random fields for two natural-language tasks . on both tasks , the feature-bagged crf performs better than simply training a single crf on all the features .

word segmentation and named entity recognition for sighan
we have participated in three open tracks of chinese word segmentation and named entity recognition tasks of sighan bakeoff3 . we take a probabilistic feature based maximum entropy ( me ) model as our basic frame to combine multiple sources of knowledge . our named entity recognizer achieved the highest f measure for msra , and word segmenter achieved the medium f measure for msra . we find effective combining of the external multi-knowledge is crucial to improve performance of word segmentation and named entity recognition .

evaluating multilanguage-comparability of subjectivity analysis division of electrical and computer engineering
subjectivity analysis is a rapidly growing field of study . along with its applications to various nlp tasks , much work have put efforts into multilingual subjectivity learning from existing resources . multilingual subjectivity analysis requires language-independent criteria for comparable outcomes across languages . this paper proposes to measure the multilanguage-comparability of subjectivity analysis tools , and provides meaningful comparisons of multilingual subjectivity analysis from various points of view .

acquistion of the morphological structure of the lexicon based on lexical similarity and formal analogy
the paper presents a computational model aiming at making the morphological structure of the lexicon emerge from the formal and semantic regularities of the words it contains . the model is purely lexemebased . the proposed morphological structure consists of ( 1 ) binary relations that connect each headword with words that are morphologically related , and especially with the members of its morphological family and its derivational series , and of ( 2 ) the analogies that hold between the words . the model has been tested on the lexicon of french using the tlfi machine readable dictionary .

robust approach to abbreviating terms : a discriminative latent variable model with global information
the present paper describes a robust approach for abbreviating terms . first , in order to incorporate non-local information into abbreviation generation tasks , we present both implicit and explicit solutions : the latent variable model , or alternatively , the label encoding approach with global information . although the two approaches compete with one another , we demonstrate that these approaches are also complementary . by combining these two approaches , experiments revealed that the proposed abbreviation generator achieved the best results for both the chinese and english languages . moreover , we directly apply our generator to perform a very different task from tradition , the abbreviation recognition . experiments revealed that the proposed model worked robustly , and outperformed five out of six state-of-the-art abbreviation recognizers .

pair language models for deriving alternative pronunciations and spellings from pronunciation dictionaries
pronunciation dictionaries provide a readily available parallel corpus for learning to transduce between character strings and phoneme strings or vice versa . translation models can be used to derive character-level paraphrases on either side of this transduction , allowing for the automatic derivation of alternative pronunciations or spellings . we examine finitestate and smt-based methods for these related tasks , and demonstrate that the tasks have different characteristics finding alternative spellings is harder than alternative pronunciations and benefits from round-trip algorithms when the other does not . we also show that we can increase accuracy by modeling syllable stress .

domain-independent shallow sentence ordering
we present a shallow approach to the sentence ordering problem . the employed features are based on discourse entities , shallow syntactic analysis , and temporal precedence relations retrieved from verbocean . we show that these relatively simple features perform well in a machine learning algorithm on datasets containing sequences of events , and that the resulting models achieve optimal performance with small amounts of training data . the model does not yet perform well on datasets describing the consequences of events , such as the destructions after an earthquake .

combining constituent parsers
combining the 1-best output of multiple parsers via parse selection or parse hybridization improves f-score over the best individual parser ( henderson and brill , 1999 ; sagae and lavie , 2006 ) . we propose three ways to improve upon existing methods for parser combination . first , we propose a method of parse hybridization that recombines context-free productions instead of constituents , thereby preserving the structure of the output of the individual parsers to a greater extent . second , we propose an efficient lineartime algorithm for computing expected f-score using minimum bayes risk parse selection . third , we extend these parser combination methods from multiple 1-best outputs to multiple n-best outputs . we present results on wsj section 23 and also on the english side of a chinese-english parallel corpus .

breaking out of local optima with count transforms and model recombination : a study in grammar induction
many statistical learning problems in nlp call for local model search methods . but accuracy tends to suffer with current techniques , which often explore either too narrowly or too broadly : hill-climbers can get stuck in local optima , whereas samplers may be inefficient . we propose to arrange individual local optimizers into organized networks . our building blocks are operators of two types : ( i ) transform , which suggests new places to search , via non-random restarts from already-found local optima ; and ( ii ) join , which merges candidate solutions to find better optima . experiments on grammar induction show that pursuing different transforms ( e.g. , discarding parts of a learned model or ignoring portions of training data ) results in improvements . groups of locally-optimal solutions can be further perturbed jointly , by constructing mixtures . using these tools , we designed several modular dependency grammar induction networks of increasing complexity . our complete system achieves 48.6 % accuracy ( directed dependency macro-average over all 19 languages in the 2006/7 conll data ) more than 5 % higher than the previous state-of-the-art .

rtg based surface realisation for tag
surface realisation with grammars integrating flat semantics is known to be np complete . in this paper , we present a new algorithm for surface realisation based on feature based tree adjoining grammar ( ftag ) which draws on the observation that an ftag can be translated into a regular tree grammar describing its derivation trees . we carry out an extensive testing of several variants of this algorithm using an automatically produced testsuite and compare the results obtained with those obtained using geni , another ftag based surface realiser .

unal-nlp : cross-lingual phrase sense disambiguation with syntactic dependency trees
in this paper we describe our participation in the semeval 2014 , task 5 , consisting of the construction of a translation assistance system that translates l1 fragments , written in l2 context , to their correct l2 translation . our approach consists of a bilingual parallel corpus , a system of syntactic features extraction and a statistical memory-based classification algorithm . our system ranked 4th and 6th among the 10 participating systems that used the english-spanish data set .

computational lexicography of multi-word units : how efficient can it be
the morphosyntactic treatment of multiword units is particularly challenging in morphologically rich languages . we present a comparative study of two formalisms meant for lexicalized description of mwus in polish . we show their expressive power and describe encoding experiments , involving novice and expert lexicographers , and allowing to evaluate the accuracy and efficiency of both implementations .

identifying the hidden dimension for unsupervised text classification
while traditional work on text clustering has largely focused on grouping documents by topic , it is conceivable that a user may want to cluster documents along other dimensions , such as the authors mood , gender , age , or sentiment . without knowing the users intention , a clustering algorithm will only group documents along the most prominent dimension , which may not be the one the user desires . to address this problem , we propose a novel way of incorporating user feedback into a clustering algorithm , which allows a user to easily specify the dimension along which she wants the data points to be clustered via inspecting only a small number of words . this distinguishes our method from existing ones , which typically require a large amount of effort on the part of humans in the form of document annotation or interactive construction of the feature space . we demonstrate the viability of our method on several challenging sentiment datasets .

multi-domain sentiment relevance classification with automatic
sentiment relevance ( sr ) aims at identifying content that does not contribute to sentiment analysis . previously , automatic sr classification has been studied in a limited scope , using a single domain and feature augmentation techniques that require large hand-crafted databases . in this paper , we present experiments on sr classification with automatically learned feature representations on multiple domains . we show that a combination of transfer learning and in-task supervision using features learned unsupervisedly by the stacked denoising autoencoder significantly outperforms a bag-of-words baseline for in-domain and cross-domain classification .

joint coreference resolution and named-entity linking with multi-pass sieves
many errors in coreference resolution come from semantic mismatches due to inadequate world knowledge . errors in named-entity linking ( nel ) , on the other hand , are often caused by superficial modeling of entity context . this paper demonstrates that these two tasks are complementary . we introduce neco , a new model for named entity linking and coreference resolution , which solves both problems jointly , reducing the errors made on each . neco extends the stanford deterministic coreference system by automatically linking mentions to wikipedia and introducing new nel-informed mention-merging sieves . linking improves mention-detection and enables new semantic attributes to be incorporated from freebase , while coreference provides better context modeling by propagating named-entity links within mention clusters . experiments show consistent improvements across a number of datasets and experimental conditions , including over 11 % reduction in muc coreference error and nearly 21 % reduction in f1 nel error on ace 2004 newswire data .

ranking help message candidates based on robust grammar verification results and utterance history in spoken dialogue systems
we address an issue of out-of-grammar ( oog ) utterances in spoken dialogue systems by generating help messages for novice users . help generation for oog utterances is a challenging problem because language understanding ( lu ) results based on automatic speech recognition ( asr ) results for such utterances are always erroneous as important words are often misrecognized or missed from such utterances . we first develop grammar verification for oog utterances on the basis of a weighted finite-state transducer ( wfst ) . it robustly identifies a grammar rule that a user intends to utter , even when some important words are missed from the asr result . we then adopt a ranking algorithm , rankboost , whose features include the grammar verification results and the utterance history representing the users experience .

linguistic features in data-driven dependency parsing
this article investigates the effect of a set of linguistically motivated features on argument disambiguation in data-driven dependency parsing of swedish . we present results from experiments with gold standard features , such as animacy , definiteness and finiteness , as well as corresponding experiments where these features have been acquired automatically and show significant improvements both in overall parse results and in the analysis of specific argument relations , such as subjects , objects and predicatives .

user-controlled , robust natural language generation from an evolving
in this paper we describe a natural language generation system which produces complex sentences from a biology knowledge base . the nlg system allows domain experts to discover errors in the knowledge base and generates certain parts of answers in response to users questions in an e-textbook application . the system allows domain experts to customise its lexical resources and to set parameters which influence syntactic constructions in generated sentences . the system is capable of dealing with certain types of incomplete inputs arising from a knowledge base which is constantly edited and includes a referring expression generation module which keeps track of discourse history . our referring expression module is available for download as the open source antfarm tool1 .

automatic measuring of english language proficiency using mt evaluation technology
assisting in foreign language learning is one of the major areas in which natural language processing technology can contribute . this paper proposes a computerized method of measuring communicative skill in english as a foreign language . the proposed method consists of two parts . the first part involves a test sentence selection part to achieve precise measurement with a small test set . the second part is the actual measurement , which has three steps . step one asks proficiency-known human subjects to translate japanese sentences into english . step two gauges the match between the translations of the subjects and correct translations based on the n-gram overlap or the edit distance between translations . step three learns the relationship between proficiency and match . by regression it finds a straight-line fitting for the scatter plot representing the proficiency and matches of the subjects . then , it estimates proficiency of proficiency-unknown users by using the line and the match .

topical pagerank : a model of scientific expertise for bibliographic search james jardine simone teufel
we model scientific expertise as a mixture of topics and authority . authority is calculated based on the network properties of each topic network . themedpagerank , our combination of lda-derived topics with pagerank differs from previous models in that topics influence both the bias and transition probabilities of pagerank . it also incorporates the age of documents . our model is general in that it can be applied to all tasks which require an estimate of documentdocument , document query , documenttopic and topicquery similarities . we present two evaluations , one on the task of restoring the reference lists of 10,000 articles , the other on the task of automatically creating reading lists that mimic reading lists created by experts . in both evaluations , our system beats state-of-the-art , as well as google scholar and google search indexed againt the corpus . our experiments also allow us to quantify the beneficial effect of our two proposed modifications to pagerank .

the tao of chi : towards effective human-computer interaction robert porzel manja baudis
end-to-end evaluations of conversational dialogue systems with naive users are currently uncovering severe usability problems that result in low task completion rates . preliminary analyses suggest that these problems are related to the systems dialogue management and turntaking behavior . we present the results of experiments designed to take a detailed look at the effects of that behavior . based on the resulting findings , we spell out a set of criteria which lie orthogonal to dialogue quality , but nevertheless constitute an integral part of a more comprehensive view on dialogue felicity as a function of dialogue quality and efficiency .

learning multi character alignment rules and classification of training data for transliteration
we address the issues of transliteration between indian languages and english , especially for named entities . we use an em algorithm to learn the alignment between the languages . we find that there are lot of ambiguities in the rules mapping the characters in the source language to the corresponding characters in the target language . some of these ambiguities can be handled by capturing context by learning multi-character based alignments and use of character n-gram models . we observed that a word in the source script may have actually originated from different languages . instead of learning one model for the language pair , we propose that one may use multiple models and a classifier to decide which model to use . a contribution of this work is that the models and classifiers are learned in a completely unsupervised manner . using our system we were able to get quite accurate transliteration models .

snap : a multi-stage xml-pipeline for aspect based sentiment analysis
this paper describes the snap system , which participated in task 4 of semeval2014 : aspect based sentiment analysis . we use an xml-based pipeline that combines several independent components to perform each subtask . key resources used by the system are bing lius sentiment lexicon , stanford corenlp , rftagger , several machine learning algorithms and wordnet . snap achieved satisfactory results in the evaluation , placing in the top half of the field for most subtasks .

estimating strictly piecewise distributions
strictly piecewise ( sp ) languages are a subclass of regular languages which encode certain kinds of long-distance dependencies that are found in natural languages . like the classes in the chomsky and subregular hierarchies , there are many independently converging characterizations of the sp class ( rogers et al , to appear ) . here we define sp distributions and show that they can be efficiently estimated from positive data .

determining term subjectivity and term orientation for opinion mining
opinion mining is a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about , but with the opinion it expresses . to aid the extraction of opinions from text , recent work has tackled the issue of determining the orientation of subjective terms contained in text , i.e . deciding whether a term that carries opinionated content has a positive or a negative connotation . this is believed to be of key importance for identifying the orientation of documents , i.e . determining whether a document expresses a positive or negative opinion about its subject matter . we contend that the plain determination of the orientation of terms is not a realistic problem , since it starts from the nonrealistic assumption that we already know whether a term is subjective or not ; this would imply that a linguistic resource that marks terms as subjective or objective is available , which is usually not the case . in this paper we confront the task of deciding whether a given term has a positive connotation , or a negative connotation , or has no subjective connotation at all ; this problem thus subsumes the problem of determining subjectivity and the problem of determining orientation . we tackle this problem by testing three different variants of a semi-supervised method previously proposed for orientation detection . our results show that determining subjectivity and orientation is a much harder problem than determining orientation alone .

argument optionality in the lingo grammar matrix
we present a library of implemented hpsg analyses for argument optionality based on typological studies of this phenomenon in the worlds languages , developed in the context of a grammar customization system that pairs a crosslinguistic core grammar with extensions for non-universal phenomena on the basis of user input of typological properties . our analyses are compatible with multiple intersecting phenomena , including person , number , gender , tense , aspect and morphological rule formulation . we achieve 80-100 % coverage on test suites from 10 natural languages .

hybrid text simplification using synchronous dependency grammars with hand-written and automatically harvested rules
we present an approach to text simplification based on synchronous dependency grammars . the higher level of abstraction afforded by dependency representations allows for a linguistically sound treatment of complex constructs requiring reordering and morphological change , such as conversion of passive voice to active . we present a synchronous grammar formalism in which it is easy to write rules by hand and also acquire them automatically from dependency parses of aligned english and simple english sentences . the grammar formalism is optimised for monolingual translation in that it reuses ordering information from the source sentence where appropriate . we demonstrate the superiority of our approach over a leading contemporary system based on quasi-synchronous tree substitution grammars , both in terms of expressivity and performance .

parsing formal languages using natural language parsing techniques jens nilsson welf lowe johan hall joakim nivre
program analysis tools used in software maintenance must be robust and ought to be accurate . many data-driven parsing approaches developed for natural languages are robust and have quite high accuracy when applied to parsing of software . we show this for the programming languages java , c/c++ , and python . further studies indicate that post-processing can almost completely remove the remaining errors . finally , the training data for instantiating the generic data-driven parser can be generated automatically for formal languages , as opposed to the manually development of treebanks for natural languages . hence , our approach could improve the robustness of software maintenance tools , probably without showing a significant negative effect on their accuracy .

using clustering to improve retrieval evaluation without
retrieval evaluation without relevance judgments is a hard but also very meaningful work . in this paper , we use clustering technique to improve the performance of judgment free retrieval evaluation . by using one system to represent all the systems that are similar to it , we can largely reduce the negative effect of similar retrieval results in retrieval evaluation . experimental results demonstrated that our method outperformed all the previous judgment free evaluation methods significantly . its overall average performance outperformed the best previous result by 20.5 % . besides , our work is a general framework that can be applied to any other judgment free evaluation method for performance improvement .

hierarchical phrase-based machine translation with word-based
hierarchical phrase-based machine translation can capture global reordering with synchronous context-free grammar , but has little ability to evaluate the correctness of word orderings during decoding . we propose a method to integrate word-based reordering model into hierarchical phrasebased machine translation to overcome this weakness . our approach extends the synchronous context-free grammar rules of hierarchical phrase-based model to include reordered source strings , allowing efficient calculation of reordering model scores during decoding . our experimental results on japanese-to-english basic travel expression corpus showed that the bleu scores obtained by our proposed system were better than those obtained by a standard hierarchical phrase-based machine translation system .

inferring the semantics of temporal prepositions in italian tommaso caselli valeria quochi
in this work we report on the results of a preliminary corpus study of italian on the semantics of temporal prepositions , which is part of a wider project on the automatic recognition of temporal relations . the corpus data collected supports our hypothesis that each temporal preposition can be associated with one prototypical temporal relation , and that deviations from the prototype can be explained as determined by the occurrence of different semantic patterns . the motivation behind this approach is to improve methods for temporal annotation of texts for content based access to information . the corpus study described in this paper led to the development of a preliminary set of heuristics for automatic annotation of temporal relations in text/discourse .

a probabilistic generative model for an intermediate
we present a probabilistic model extension to the tesni`ere dependency structure ( tds ) framework formulated in ( sangati and mazza , 2009 ) . this representation incorporates aspects from both constituency and dependency theory . in addition , it makes use of junction structures to handle coordination constructions . we test our model on parsing the english penn wsj treebank using a re-ranking framework . this technique allows us to efficiently test our model without needing a specialized parser , and to use the standard evaluation metric on the original phrase structure version of the treebank . we obtain encouraging results : we achieve a small improvement over state-of-the-art results when re-ranking a small number of candidate structures , on all the evaluation metrics except for chunking .

improving english-spanish statistical machine translation : experiments in
we describe the experiments of the uc berkeley team on improving english-spanish machine translation of news text , as part of the wmt08 shared translation task . we experiment with domain adaptation , combining a small in-domain news bi-text and a large out-of-domain one from the europarl corpus , building two separate phrase translation models and two separate language models . we further add a third phrase translation model trained on a version of the news bi-text augmented with monolingual sentencelevel syntactic paraphrases on the sourcelanguage side , and we combine all models in a log-linear model using minimum error rate training . finally , we experiment with different tokenization and recasing rules , achieving 35.09 % bleu score on the wmt07 news test data when translating from english to spanish , which is a sizable improvement over the highest bleu score achieved on that dataset at wmt07 : 33.10 % ( in fact , by our system ) . on the wmt08 english to spanish news translation , we achieve 21.92 % , which makes our team the second best on bleu score .

on using articulatory features for discriminative speaker adaptation
this paper presents a way to perform speaker adaptation for automatic speech recognition using the stream weights in a multi-stream setup , which included acoustic models for articulatory features such as rounded or voiced . we present supervised speaker adaptation experiments on a spontaneous speech task and compare the above stream-based approach to conventional approaches , in which the models , and not stream combination weights , are being adapted . in the approach we present , stream weights model the importance of features such as voiced for word discrimination , which offers a descriptive interpretation of the adaptation parameters .

sentiment propagation via implicature constraints intelligent systems program
opinions may be expressed implicitly via inference over explicit sentiments and events that positively/negatively affect entities ( goodfor/badfor events ) . we investigate how such inferences may be exploited to improve sentiment analysis , given goodfor/badfor event information . we apply loopy belief propagation to propagate sentiments among entities . the graph-based model improves over explicit sentiment classification by 10 points in precision and , in an evaluation of the model itself , we find it has an 89 % chance of propagating sentiments correctly .

power of confidence : how poll scores impact topic dynamics in political debates
in this paper , we investigate how topic dynamics during the course of an interaction correlate with the power differences between its participants . we perform this study on the us presidential debates and show that a candidates power , modeled after their poll scores , affects how often he/she attempts to shift topics and whether he/she succeeds . we ensure the validity of topic shifts by confirming , through a simple but effective method , that the turns that shift topics provide substantive topical content to the interaction .

extending nlp tools repositories for the interaction with language data resources repositories
this short paper presents some motivations behind the organization of the acl/eacl01 workshop on sharing tools and resources for research and education , concentrating on the possible connection of tools and resources repositories . taking some papers printed in this volume and the acl natural language software registry as a basis , we outline some of the steps to be done on the side of nlp tool repositories in order to achieve this goal .

unsupervised models for coreference resolution
we present a generative model for unsupervised coreference resolution that views coreference as an em clustering process . for comparison purposes , we revisit haghighi and kleins ( 2007 ) fully-generative bayesian model for unsupervised coreference resolution , discuss its potential weaknesses and consequently propose three modifications to their model . experimental results on the ace data sets show that our model outperforms their original model by a large margin and compares favorably to the modified model .

inducing neural models of script knowledge
induction of common sense knowledge about prototypical sequence of events has recently received much attention ( e.g. , chambers and jurafsky ( 2008 ) ; regneri et al . ( 2010 ) ) . instead of inducing this knowledge in the form of graphs , as in much of the previous work , in our method , distributed representations of event realizations are computed based on distributed representations of predicates and their arguments , and then these representations are used to predict prototypical event orderings . the parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated . we show that this approach results in a substantial boost in performance on the event ordering task with respect to the previous approaches , both on natural and crowdsourced texts .

thumbs up sentiment classification using machine learning
we consider the problem of classifying documents not by topic , but by overall sentiment , e.g. , determining whether a review is positive or negative . using movie reviews as data , we find that standard machine learning techniques definitively outperform human-produced baselines . however , the three machine learning methods we employed ( naive bayes , maximum entropy classification , and support vector machines ) do not perform as well on sentiment classification as on traditional topic-based categorization . we conclude by examining factors that make the sentiment classification problem more challenging .

improving text retrieval precision and answer accuracy in question answering systems
question answering ( qa ) systems are often built modularly , with a text retrieval component feeding forward into an answer extraction component . conventional wisdom suggests that , the higher the quality of the retrieval results used as input to the answer extraction module , the better the extracted answers , and hence system accuracy , will be . this turns out to be a poor assumption , because text retrieval and answer extraction are tightly coupled . improvements in retrieval quality can be lost at the answer extraction module , which can not necessarily recognize the additional answer candidates provided by improved retrieval . going forward , to improve accuracy on the qa task , systems will need greater coordination between text retrieval and answer extraction modules .

urdu word segmentation nadir durrani sarmad hussain
word segmentation is the foremost obligatory task in almost all the nlp applications where the initial phase requires tokenization of input into words . urdu is amongst the asian languages that face word segmentation challenge . however , unlike other asian languages , word segmentation in urdu not only has space omission errors but also space insertion errors . this paper discusses how orthographic and linguistic features in urdu trigger these two problems . it also discusses the work that has been done to tokenize input text . we employ a hybrid solution that performs an n-gram ranking on top of rule based maximum matching heuristic . our best technique gives an error detection of 85.8 % and overall accuracy of 95.8 % . further issues and possible future directions are also discussed .

a priority model for named entities
we introduce a new approach to named entity classification which we term a priority model . we also describe the construction of a semantic database called semcat consisting of a large number of semantically categorized names relevant to biomedicine . we used semcat as training data to investigate name classification techniques . we generated a statistical language model and probabilistic contextfree grammars for gene and protein name classification , and compared the results with the new model . for all three methods , we used a variable order markov model to predict the nature of strings not represented in the training data . the priority model achieves an f-measure of 0.958-0.960 , consistently higher than the statistical language model and probabilistic context-free grammar .

tracking sentiment in mail : how genders differ on emotional axes
with the widespread use of email , we now have access to unprecedented amounts of text that we ourselves have written . in this paper , we show how sentiment analysis can be used in tandem with effective visualizations to quantify and track emotions in many types of mail . we create a large wordemotion association lexicon by crowdsourcing , and use it to compare emotions in love letters , hate mail , and suicide notes . we show that there are marked differences across genders in how they use emotion words in work-place email . for example , women use many words from the joysadness axis , whereas men prefer terms from the feartrust axis . finally , we show visualizations that can help people track emotions in their emails .

resumptive pronoun detection for modern standard arabic to english mt
many languages , including modern standard arabic ( msa ) , insert resumptive pronouns in relative clauses , whereas many others , such as english , do not , using empty categories instead . this discrepancy is a source of difficulty when translating between these languages because there are words in one language that correspond to empty categories in the other , and these words must either be inserted or deleteddepending on translation direction . in this paper , we first examine challenges presented by resumptive pronouns in msa-english translations and review resumptive pronoun translations generated by a popular online msa-english mt engine . we then present what is , to the best of our knowledge , the first system for automatic identification of resumptive pronouns . the system achieves 91.9 f1 and 77.8 f1 on arabic treebank data when using gold standard parses and automatic parses , respectively .

automatic construction of an english-chinese bilingual framenet
we propose a method of automatically constructing an english-chinese bilingual framenet where the english framenet lexical entries are linked to the appropriate chinese word senses . this resource can be used in machine translation and cross-lingual ir systems . we coerce the english framenet into chinese using a bilingual lexicon , frame context in framenet and taxonomy structure in hownet . our approach does not require any manual mapping between framenet and hownet semantic roles . evaluation results show that we achieve a promising 82 % average fmeasure for the most ambiguous lexical entries .

linguistic indicators of severity and progress in online text-based therapy for depression
mental illnesses such as depression and anxiety are highly prevalent , and therapy is increasingly being offered online . this new setting is a departure from face-toface therapy , and offers both a challenge and an opportunity it is not yet known what features or approaches are likely to lead to successful outcomes in such a different medium , but online text-based therapy provides large amounts of data for linguistic analysis . we present an initial investigation into the application of computational linguistic techniques , such as topic and sentiment modelling , to online therapy for depression and anxiety . we find that important measures such as symptom severity can be predicted with comparable accuracy to face-to-face data , using general features such as discussion topic and sentiment ; however , measures of patient progress are captured only by finergrained lexical features , suggesting that aspects of style or dialogue structure may also be important .

active learning for multilingual statistical machine translation
statistical machine translation ( smt ) models require bilingual corpora for training , and these corpora are often multilingual with parallel text in multiple languages simultaneously . we introduce an active learning task of adding a new language to an existing multilingual set of parallel text and constructing high quality mt systems , from each language in the collection into this new target language . we show that adding a new language using active learning to the europarl corpus provides a significant improvement compared to a random sentence selection baseline . we also provide new highly effective sentence selection methods that improve al for phrase-based smt in the multilingual and single language pair setting .

top-down nearly-context-sensitive parsing
we present a new syntactic parser that works left-to-right and top down , thus maintaining a fully-connected parse tree for a few alternative parse hypotheses . all of the commonly used statistical parsers use context-free dynamic programming algorithms and as such work bottom up on the entire sentence . thus they only find a complete fully connected parse at the very end . in contrast , both subjective and experimental evidence show that people understand a sentence word-to-word as they go along , or close to it . the constraint that the parser keeps one or more fully connected syntactic trees is intended to operationalize this cognitive fact . our parser achieves a new best result for topdown parsers of 89.4 % , a 20 % error reduction over the previous single-parser best result for parsers of this type of 86.8 % ( roark , 2001 ) . the improved performance is due to embracing the very large feature set available in exchange for giving up dynamic programming .

using higher-level linguistic knowledge for speech recognition error
speech interface is often required in many application environments such as telephonebased information retrieval , car navigation systems , and user-friendly interfaces , but the low speech recognition rate makes it difficult to extend its application to new fields . several approaches to increase the accuracy of the recognition rate have been researched by error correction of the recognition results , but previous approaches were mainly lexical-oriented ones in post error correction . we suggest an improved syllable-based model and a new semantic-oriented approach to correct both semantic and lexical errors , which is also more accurate for especially domain-specific speech error correction . through extensive experiments using a speech-driven in-vehicle telematics information retrieval , we demonstrate the superior performance of our approach and some advantages over previous lexical-oriented approaches .

modelling semantic role plausibility in human sentence processing
we present the psycholinguistically motivated task of predicting human plausibility judgements for verb-role-argument triples and introduce a probabilistic model that solves it . we also evaluate our model on the related role-labelling task , and compare it with a standard role labeller . for both tasks , our model benefits from classbased smoothing , which allows it to make correct argument-specific predictions despite a severe sparse data problem . the standard labeller suffers from sparse data and a strong reliance on syntactic cues , especially in the prediction task .

modelling atypical syntax processing
we evaluate the inferences that can be drawn from dissociations in syntax processing identified in developmental disorders and acquired language deficits . we use an srn to simulate empirical data from dick et al ( 2001 ) on the relative difficulty of comprehending different syntactic constructions under normal conditions and conditions of damage . we conclude that task constraints and internal computational constraints interact to predict patterns of difficulty . difficulty is predicted by frequency of constructions , by the requirement of the task to focus on local vs. global sequence information , and by the ability of the system to maintain sequence information . we generate a testable prediction on the empirical pattern that should be observed under conditions of developmental damage .

automatic sanskrit segmentizer using finite state transducers
in this paper , we propose a novel method for automatic segmentation of a sanskrit string into different words . the input for our segmentizer is a sanskrit string either encoded as a unicode string or as a roman transliterated string and the output is a set of possible splits with weights associated with each of them . we followed two different approaches to segment a sanskrit text using sandhi rules extracted from a parallel corpus of manually sandhi split text . while the first approach augments the finite state transducer used to analyze sanskrit morphology and traverse it to segment a word , the second approach generates all possible segmentations and validates each constituent using a morph analyzer .

capturing disjunction in lexicalization with extensible dependency grammar jorge marques pelizzoni langue & dialogue - loria - france maria das graas volpe nunes
in spite of its potential for bidirectionality , extensible dependency grammar ( xdg ) has so far been used almost exclusively for parsing . this paper represents one of the first steps towards an xdg-based integrated generation architecture by tackling what is arguably the most basic among generation tasks : lexicalization . herein we present a constraint-based account of disjunction in lexicalization , i.e . a way to enable an xdg grammar to generate all paraphrases along the lexicalization axis , of course realizing a given input semantics . our model is ( i ) efficient , yielding strong propagation , ( ii ) modular and ( iii ) favourable to synergy inasmuch as it allows collaboration between modules , notably semantics and syntax . we focus on constraints ensuring wellformedness and completeness and avoiding over-redundancy .

joint optimization for machine translation system combination
system combination has emerged as a powerful method for machine translation ( mt ) . this paper pursues a joint optimization strategy for combining outputs from multiple mt systems , where word alignment , ordering , and lexical selection decisions are made jointly according to a set of feature functions combined in a single log-linear model . the decoding algorithm is described in detail and a set of new features that support this joint decoding approach is proposed . the approach is evaluated in comparison to state-of-the-art confusion-network-based system combination methods using equivalent features and shown to outperform them significantly .

an efficient , generic approach to extracting multi-word expressions from dependency trees centrum voor computerlingustiek
the varro toolkit offers an intuitive mechanism for extracting syntactically motivated multi-word expressions ( mwes ) from dependency treebanks by looking for recurring connected subtrees instead of subsequences in strings . this approach can find mwes that are in varying orders and have words inserted into their components . this paper also proposes description length gain as a statistical correlation measure well-suited to tree structures .

bayesian reordering model with feature selection and mahesan niranjan
in phrase-based statistical machine translation systems , variation in grammatical structures between source and target languages can cause large movements of phrases . modeling such movements is crucial in achieving translations of long sentences that appear natural in the target language . we explore generative learning approach to phrase reordering in arabic to english . formulating the reordering problem as a classification problem and using naive bayes with feature selection , we achieve an improvement in the bleu score over a lexicalized reordering model . the proposed model is compact , fast and scalable to a large corpus .

propagation strategies for building temporal ontologies
in this paper , we propose to build temporal ontologies from wordnet . the underlying idea is that each synset is augmented with its temporal connotation . for that purpose , temporal classifiers are iteratively learned from an initial set of time-sensitive synsets and different propagation strategies to give rise to different tempowordnets .

joshua : an open source toolkit for parsing-based machine translation
we describe joshua , an open source toolkit for statistical machine translation . joshua implements all of the algorithms required for synchronous context free grammars ( scfgs ) : chart-parsing , ngram language model integration , beamand cube-pruning , and k-best extraction . the toolkit also implements suffix-array grammar extraction and minimum error rate training . it uses parallel and distributed computing techniques for scalability . we demonstrate that the toolkit achieves state of the art translation performance on the wmt09 french-english translation task .

evaluating n-gram based evaluation metrics for automatic keyphrase extraction su nam kim , timothy baldwin
this paper describes a feasibility study of n-gram-based evaluation metrics for automatic keyphrase extraction . to account for near-misses currently ignored by standard evaluation metrics , we adapt various evaluation metrics developed for machine translation and summarization , and also the r-precision evaluation metric from keyphrase evaluation . in evaluation , the r-precision metric is found to achieve the highest correlation with human annotations . we also provide evidence that the degree of semantic similarity varies with the location of the partially-matching component words .

ranking the annotators : an agreement study on argumentation structure applied computational linguistics
we investigate methods for evaluating agreement among a relatively large group of annotators who have not received extensive training and differ in terms of ability and motivation . we show that it is possible to isolate a reliable subgroup of annotators , so that aspects of the difficulty of the underlying task can be studied . our task is to annotate the argumentative structure of short texts .

almut silja hildebrand
in this paper we present our entry to the wmt13 shared task : quality estimation ( qe ) for machine translation ( mt ) . we participated in the 1.1 , 1.2 and 1.3 sub-tasks with our qe system trained on features from diverse information sources like mt decoder features , n-best lists , mono- and bi-lingual corpora and giza training models . our system shows competitive results in the workshop shared task .

arabic spelling correction using supervised learning
in this work , we address the problem of spelling correction in the arabic language utilizing the new corpus provided by qalb ( qatar arabic language bank ) project which is an annotated corpus of sentences with errors and their corrections . the corpus contains edit , add before , split , merge , add after , move and other error types . we are concerned with the first four error types as they contribute more than 90 % of the spelling errors in the corpus . the proposed system has many models to address each error type on its own and then integrating all the models to provide an efficient and robust system that achieves an overall recall of 0.59 , precision of 0.58 and f1 score of 0.58 including all the error types on the development set . our system participated in the qalb 2014 shared task automatic arabic error correction and achieved an f1 score of 0.6 , earning the sixth place out of nine participants .

alignment by agreement
we present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models . compared to the standard practice of intersecting predictions of independently-trained models , joint training provides a 32 % reduction in aer . moreover , a simple and efficient pair of hmm aligners provides a 29 % reduction in aer over symmetrized ibm model 4 predictions .

extended phraseological information in a valence dictionary for nlp applications
the aim of this paper is to propose a far-reaching extension of the phraseological component of a valence dictionary for polish . the dictionary is the basis of two different parsers of polish ; its format has been designed so as to maximise the readability of the information it contains and its re-applicability . we believe that the extension proposed here follows this approach and , hence , may be an inspiration in the design of valence dictionaries for other languages .

sentence ordering driven by local and global coherence for summary generation
in summarization , sentence ordering is conducted to enhance summary readability by accommodating text coherence . we propose a grouping-based ordering framework that integrates local and global coherence concerns . summary sentences are grouped before ordering is applied on two levels : group-level and sentence-level . different algorithms for grouping and ordering are discussed . the preliminary results on single-document news datasets demonstrate the advantage of our method over a widely accepted method .

a chinese efficient analyser integrating word segmentation , part-of-speech tagging , partial parsing and full parsing
this paper introduces an efficient analyser for the chinese language , which efficiently and effectively integrates word segmentation , part-of-speech tagging , partial parsing and full parsing . the chinese efficient analyser is based on a hidden markov model ( hmm ) and an hmm-based tagger . that is , all the components are based on the same hmm-based tagging engine . one advantage of using the same single engine is that it largely decreases the code size and makes the maintenance easy . another advantage is that it is easy to optimise the code and thus improve the speed while speed plays a critical important role in many applications . finally , the performances of all the components can benefit from the optimisation of existing algorithms and/or adoption of better algorithms to a single engine . experiments show that all the components can achieve state-of-art performances with high efficiency for the chinese language . the layout of this paper is as follows . section 2 describes the chinese efficient analyser . section 3 presents the hmm and the hmm-based tagger .

multiword noun compound bracketing using wikipedia
this research suggests two contributions in relation to the multiword noun compound bracketing problem : first , demonstrate the usefulness of wikipedia for the task , and second , present a novel bracketing method relying on a word association model . the intent of the association model is to represent combined evidence about the possibly lexical , relational or coordinate nature of links between all pairs of words within a compound . as for wikipedia , it is promoted for its encyclopedic nature , meaning it describes terms and named entities , as well as for its size , large enough for corpus-based statistical analysis . both types of information will be used in measuring evidence about lexical units , noun relations and noun coordinates in order to feed the association model in the bracketing algorithm . using a gold standard of around 4800 multiword noun compounds , we show performances of 73 % in a strict match evaluation , comparing favourably to results reported in the literature using unsupervised approaches .

unsupervised word sense induction using distributional statistics
word sense induction is an unsupervised task to find and characterize different senses of polysemous words . this work investigates two unsupervised approaches that focus on using distributional word statistics to cluster the contextual information of the target words using two different algorithms involving latent dirichlet allocation and spectral clustering . using a large corpus for achieving this task , we quantitatively analyze our clusters on the semeval-2010 dataset and also perform a qualitative analysis of our induced senses . our results indicate that our methods successfully characterized the senses of the target words and were also able to find unconventional senses for those words .

positive results for parsing with a bounded stack using a model-based
statistical parsing models have recently been proposed that employ a bounded stack in timeseries ( left-to-right ) recognition , using a rightcorner transform defined over training trees to minimize stack use ( schuler et al , 2008 ) . corpus results have shown that a vast majority of naturally-occurring sentences can be parsed in this way using a very small stack bound of three to four elements . this suggests that the standard cubic-time cky chart-parsing algorithm , which implicitly assumes an unbounded stack , may be wasting probability mass on trees whose complexity is beyond human recognition or generation capacity . this paper first describes a version of the rightcorner transform that is defined over entire probabilistic grammars ( cast as infinite sets of generable trees ) , in order to ensure a fair comparison between bounded-stack and unbounded pcfg parsing using a common underlying model ; then it presents experimental results that show a bounded-stack right-corner parser using a transformed version of a grammar significantly outperforms an unboundedstack cky parser using the original grammar .

training data modification for smt
generally speaking , statistical machine translation systems would be able to attain better performance with more training sets . unfortunately , well-organized training sets are rarely available in the real world . consequently , it is necessary to focus on modifying the training set to obtain high accuracy for an smt system . if the smt system trained the translation model , the translation pair would have a low probability when there are many variations for target sentences from a single source sentence . if we decreased the number of variations for the translation pair , we could construct a superior translation model . this paper describes the effects of modification on the training corpus when consideration is given to synonymous sentence groups . we attempt three types of modification : compression of the training set , replacement of source and target sentences with a selected sentence from the synonymous sentence group , and replacement of the sentence on only one side with the selected sentence from the synonymous sentence group . as a result , we achieve improved performance with the replacement of source-side sentences .

a suite of shallow processing tools for portuguese : joao ricardo silva
in this paper we present lx-suite , a set of tools for the shallow processing of portuguese . this suite comprises several modules , namely : a sentence chunker , a tokenizer , a pos tagger , featurizers and lemmatizers .

dead parrots make bad pets : exploring modifier effects in noun phrases
sometimes modifiers have a strong effect on core aspects of the meaning of the nouns they are attached to : a parrot is a desirable pet , but a dead parrot is , at the very least , a rather unusual household companion . in order to stimulate computational research into the impact of modification on phrase meaning , we collected and made available a large dataset containing subject ratings for a variety of noun phrases and the categories they might belong to . we propose to use compositional distributional semantics to model these data , experimenting with numerous distributional semantic spaces , phrase composition methods and asymmetric similarity measures . our models capture a statistically significant portion of the data , although much work is still needed before we achieve a full computational account of modification effects .

enhancing multi-lingual information extraction via cross-media inference and fusion
we describe a new information fusion approach to integrate facts extracted from cross-media objects ( videos and texts ) into a coherent common representation including multi-level knowledge ( concepts , relations and events ) . beyond standard information fusion , we exploited video extraction results and significantly improved text information extraction . we further extended our methods to multi-lingual environment ( english , arabic and chinese ) by presenting a case study on cross-lingual comparable corpora acquisition based on video comparison .

shrinking exponential language models
in ( chen , 2009 ) , we show that for a variety of language models belonging to the exponential family , the test set cross-entropy of a model can be accurately predicted from its training set cross-entropy and its parameter values . in this work , we show how this relationship can be used to motivate two heuristics for shrinking the size of a language model to improve its performance . we use the first heuristic to develop a novel class-based language model that outperforms a baseline word trigram model by 28 % in perplexity and 1.9 % absolute in speech recognition word-error rate on wall street journal data . we use the second heuristic to motivate a regularized version of minimum discrimination information models and show that this method outperforms other techniques for domain adaptation .

renoun : fact extraction for nominal attributes steven euijong whang , rahul gupta , alon halevy
search engines are increasingly relying on large knowledge bases of facts to provide direct answers to users queries . however , the construction of these knowledge bases is largely manual and does not scale to the long and heavy tail of facts . open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations . we describe renoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail . renouns approach is based on leveraging a large ontology of noun attributes mined from a text corpus and from user queries . renoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontology . renoun then generalizes from this seed set to produce a much larger set of extractions that are then scored . we describe experiments that show that we extract facts with high precision and for attributes that can not be extracted with verb-based techniques .

learning bilingual word representations by marginalizing alignments cisky karl moritz hermann
we present a probabilistic model that simultaneously learns alignments and distributed representations for bilingual data . by marginalizing over word alignments the model captures a larger semantic context than prior work relying on hard alignments . the advantage of this approach is demonstrated in a cross-lingual classification task , where we outperform the prior published state of the art .

language models and reranking for machine translation
complex language models can not be easily integrated in the first pass decoding of a statistical machine translation system the decoder queries the lm a very large number of times ; the search process in the decoding builds the hypotheses incrementally and can not make use of lms that analyze the whole sentence . we present in this paper the language computers system for wmt06 that employs lmpowered reranking on hypotheses generated by phrase-based smt systems

automated pyramid scoring of summaries using distributional semantics
the pyramid method for content evaluation of automated summarizers produces scores that are shown to correlate well with manual scores used in educational assessment of students summaries . this motivates the development of a more accurate automated method to compute pyramid scores . of three methods tested here , the one that performs best relies on latent semantics .

cross-lingual dependency parsing of related languages with rich morphosyntactic tagsets
this paper addresses cross-lingual dependency parsing using rich morphosyntactic tagsets . in our case study , we experiment with three related slavic languages : croatian , serbian and slovene . four different dependency treebanks are used for monolingual parsing , direct cross-lingual parsing , and a recently introduced crosslingual parsing approach that utilizes statistical machine translation and annotation projection . we argue for the benefits of using rich morphosyntactic tagsets in cross-lingual parsing and empirically support the claim by showing large improvements over an impoverished common feature representation in form of a reduced part-of-speech tagset . in the process , we improve over the previous state-of-the-art scores in dependency parsing for all three languages .

recognition of affect , judgment , and appreciation in text
the main task we address in our research is classification of text using fine-grained attitude labels . the developed @ am system relies on the compositionality principle and a novel approach based on the rules elaborated for semantically distinct verb classes . the evaluation of our method on 1000 sentences , that describe personal experiences , showed promising results : average accuracy on the finegrained level ( 14 labels ) was 62 % , on the middle level ( 7 labels ) 71 % , and on the top level ( 3 labels ) 88 % .

empirical analysis of aggregation methods for collective annotation
we investigate methods for aggregating the judgements of multiple individuals in a linguistic annotation task into a collective judgement . we define several aggregators that take the reliability of annotators into account and thus go beyond the commonly used majority vote , and we empirically analyse their performance on new datasets of crowdsourced data .

towards identifying the resolvability of threads in moocs
one important function of the discussion forums of massive open online courses ( moocs ) is for students to post problems they are unable to resolve and receive help from their peers and instructors . there are a large proportion of threads that are not resolved to the satisfaction of the students for various reasons . in this paper , we attack this problem by firstly constructing a conceptual model validated using a structural equation modeling technique , which enables us to understand the factors that influence whether a problem thread is satisfactorily resolved . we then demonstrate the robustness of these findings using a predictive model that illustrates how accurately those factors can be used to predict whether a thread is resolved or unresolved . experiments conducted on one mooc show that thread resolveability connects closely to our proposed five dimensions and that the predictive ensemble model gives better performance over several baselines .

evaluating information content by factoid analysis : human annotation and stability
we present a new approach to intrinsic summary evaluation , based on initial experiments in van halteren and teufel ( 2003 ) , which combines two novel aspects : comparison of information content ( rather than string similarity ) in gold standard and system summary , measured in shared atomic information units which we call factoids , and comparison to more than one gold standard summary ( in our data : 20 and 50 summaries respectively ) . in this paper , we show that factoid annotation is highly reproducible , introduce a weighted factoid score , estimate how many summaries are required for stable system rankings , and show that the factoid scores can not be sufficiently approximated by unigrams and the duc information overlap measure .

a prolog datamodel for state chart xml
scxml was proposed as one description language for dialog control in the w3c multimodal architecture but lacks the facilities required for grounding and reasoning . this prohibits the application of many dialog modeling techniques for multimodal applications following this w3c standard . by extending scxml with a prolog datamodel and scripting language , we enable those techniques to be employed again . thereby bridging the gap between respective dialog modeling research and a standardized architecture to access and coordinate modalities .

predicting barge-in utterance errors by using implicitly supervised asr accuracy and barge-in rate per user
modeling of individual users is a promising way of improving the performance of spoken dialogue systems deployed for the general public and utilized repeatedly . we define implicitly-supervised asr accuracy per user on the basis of responses following the systems explicit confirmations . we combine the estimated asr accuracy with the users barge-in rate , which represents how well the user is accustomed to using the system , to predict interpretation errors in barge-in utterances . experimental results showed that the estimated asr accuracy improved prediction performance . since this asr accuracy and the barge-in rate are obtainable at runtime , they improve prediction performance without the need for manual labeling .

accenting unknown words in a specialized language
we propose two internal methods for accenting unknown words , which both learn on a reference set of accented words the contexts of occurrence of the various accented forms of a given letter . one method is adapted from pos tagging , the other is based on finite state transducers . we show experimental results for letter e on the french version of the medical subject headings thesaurus . with the best training set , the tagging method obtains a precision-recall breakeven point of 84.24.4 % and the transducer method 83.84.5 % ( with a baseline at 64 % ) for the unknown words that contain this letter . a consensus combination of both increases precision to 92.03.7 % with a recall of 75 % . we perform an error analysis and discuss further steps that might help improve over the current performance .

which system differences matter
we investigate how to jointly explain the performance and behavioral differences of two spoken dialogue systems . the join evaluation and differences identification ( jedi ) , finds differences between systems relevant to performance by formulating the problem as a multi-task feature selection question . jedi provides evidence on the usefulness of a recent method , `1/`p-regularized regression ( obozinski et al , 2007 ) . we evaluate against manually annotated success criteria from real users interacting with five different spoken user interfaces that give bus schedule information .

robox : ccg with structured perceptron for supervised semantic parsing of robotic spatial commands
we use a combinatory categorial grammar ( ccg ) parser with a structured perceptron learner to address shared task 6 of semeval-2014 , supervised semantic parsing of robotic spatial commands . our system reaches an accuracy of 79 % ignoring spatial context and 87 % using the spatial planner , showing that ccg can successfully be applied to the task .

interlinear glossing and its role in theoretical and descriptive studies of african and other lesser-documented languages
in a manuscript william labov ( 1987 ) states that although linguistics is a field with a long historical tradition and with a high degree of consensus on basic categories , it experiences a fundamental devision concerning the role that quantitative methods should play as part of the research progress . linguists differ in the role they assign to the use of natural language examples in linguistic research and in the publication of its results . in this paper we suggest that the general availability of richly annotated , multi-lingual data directly suited for scientific publications could have a positive impact on the way we think about language , and how we approach linguistics.we encourage the systematic generation of linguistic data beyond what emerges from fieldwork and other descriptive studies and introduce an online glossing tool for textual data annotation . we argue that the availability of such an online tool will facilitate the generation of in-depth annotated linguistic examples as part of linguistic research . this in turn will allow the build-up of linguistic resources which can be used independent of the research focus and of the theoretical framework applied . the tool we would like to present is a non-expert-user system designed in particular for the work with lesser documented languages . it has been used for the documentation of several african languages , and has served for two projects involving universities in africa .

linguastream : an integrated environment for computational linguistics experimentation
by presenting the linguastream platform , we introduce different methodological principles and analysis models , which make it possible to build hybrid experimental nlp systems by articulating corpus processing tasks .

detecting turnarounds in sentiment analysis : thwarting
thwarting and sarcasm are two uncharted territories in sentiment analysis , the former because of the lack of training corpora and the latter because of the enormous amount of world knowledge it demands . in this paper , we propose a working definition of thwarting amenable to machine learning and create a system that detects if the document is thwarted or not . we focus on identifying thwarting in product reviews , especially in the camera domain . an ontology of the camera domain is created . thwarting is looked upon as the phenomenon of polarity reversal at a higher level of ontology compared to the polarity expressed at the lower level . this notion of thwarting defined with respect to an ontology is novel , to the best of our knowledge . a rule based implementation building upon this idea forms our baseline . we show that machine learning with annotated corpora ( thwarted/nonthwarted ) is more effective than the rule based system . because of the skewed distribution of thwarting , we adopt the areaunder-the-curve measure of performance . to the best of our knowledge , this is the first attempt at the difficult problem of thwarting detection , which we hope will at least provide a baseline system to compare against .

using n-gram based features for machine translation
conventional confusion network based system combination for machine translation ( mt ) heavily relies on features that are based on the measure of agreement of words in different translation hypotheses . this paper presents two new features that consider agreement of n-grams in different hypotheses to improve the performance of system combination . the first one is based on a sentence specific online n-gram language model , and the second one is based on n-gram voting . experiments on a large scale chinese-to-english mt task show that both features yield significant improvements on the translation performance , and a combination of them produces even better translation results .

ukwabelana - an open-source morphological zulu corpus
zulu is an indigenous language of south africa , and one of the eleven official languages of that country . it is spoken by about 11 million speakers . although it is similar in size to some western languages , e.g . swedish , it is considerably under-resourced . this paper presents a new open-source morphological corpus for zulu named ukwabelana corpus . we describe the agglutinating morphology of zulu with its multiple prefixation and suffixation , and also introduce our labeling scheme . further , the annotation process is described and all single resources are explained . these comprise a list of 10,000 labeled and 100,000 unlabeled word types , 3,000 part-of-speech ( pos ) tagged and 30,000 raw sentences as well as a morphological zulu grammar , and a parsing algorithm which hypothesizes possible word roots and enumerates parses that conform to the zulu grammar . we also provide a pos tagger which assigns the grammatical category to a morphologically analyzed word type . as it is hoped that the corpus and all resources will be of benefit to any person doing research on zulu or on computer-aided analysis of languages .

online learning for interactive statistical machine translation
state-of-the-art machine translation ( mt ) systems are still far from being perfect . an alternative is the so-called interactive machine translation ( imt ) framework . in this framework , the knowledge of a human translator is combined with a mt system . the vast majority of the existing work on imt makes use of the well-known batch learning paradigm . in the batch learning paradigm , the training of the imt system and the interactive translation process are carried out in separate stages . this paradigm is not able to take advantage of the new knowledge produced by the user of the imt system . in this paper , we present an application of the online learning paradigm to the imt framework . in the online learning paradigm , the training and prediction stages are no longer separated . this feature is particularly useful in imt since it allows the user feedback to be taken into account . the online learning techniques proposed here incrementally update the statistical models involved in the translation process .

dramatically reducing training data size through vocabulary
our field has seen significant improvements in the quality of machine translation systems over the past several years . the single biggest factor in this improvement has been the accumulation of ever larger stores of data . however , we now find ourselves the victims of our own success , in that it has become increasingly difficult to train on such large sets of data , due to limitations in memory , processing power , and ultimately , speed ( i.e. , data to models takes an inordinate amount of time ) . some teams have dealt with this by focusing on data cleaning to arrive at smaller data sets , domain adaptation to arrive at data more suited to the task at hand , or by specifically focusing on data reduction by keeping only as much data as is needed for building models e.g. , ( eck et al , 2005 ) . this paper focuses on techniques related to the latter efforts . we have developed a very simple n-gram counting method that reduces the size of data sets dramatically , as much as 90 % , and is applicable independent of specific dev and test data . at the same time it reduces model sizes , improves training times , and , because it attempts to preserve contexts for all n-grams in a corpus , the cost in quality is minimal ( as measured by bleu ) . further , unlike other methods created specifically for data reduction that have similar effects on the data , our method scales to very large data , up to tens to hundreds of millions of parallel sentences .

gleu : automatic evaluation of sentence-level fluency
in evaluating the output of language technology applicationsmt , natural language generation , summarisationautomatic evaluation techniques generally conflate measurement of faithfulness to source content with fluency of the resulting text . in this paper we develop an automatic evaluation metric to estimate fluency alone , by examining the use of parser outputs as metrics , and show that they correlate with human judgements of generated text fluency . we then develop a machine learner based on these , and show that this performs better than the individual parser metrics , approaching a lower bound on human performance . we finally look at different language models for generating sentences , and show that while individual parser metrics can be fooled depending on generation method , the machine learner provides a consistent estimator of fluency .

multiple alignment of citation sentences with conditional random fields and posterior decoding
in scientific literature , sentences that cite related work can be a valuable resource for applications such as summarization , synonym identification , and entity extraction . in order to determine which equivalent entities are discussed in the various citation sentences , we propose aligning the words within these sentences according to semantic similarity . this problem is partly analogous to the problem of multiple sequence alignment in the biosciences , and is also closely related to the word alignment problem in statistical machine translation . in this paper we address the problem of multiple citation concept alignment by combining and modifying the crf based pairwise word alignment system of blunsom & cohn ( 2006 ) and a posterior decoding based multiple sequence alignment algorithm of schwartz & pachter ( 2007 ) . we evaluate the algorithm on hand-labeled data , achieving results that improve on a baseline .

linking tweets to news : a framework to enrich short text data in
many current natural language processing [ nlp ] techniques work well assuming a large context of text as input data . however they become ineffective when applied to short texts such as twitter feeds . to overcome the issue , we want to find a related newswire document to a given tweet to provide contextual support for nlp tasks . this requires robust modeling and understanding of the semantics of short texts . the contribution of the paper is two-fold : 1. we introduce the linking-tweets-tonews task as well as a dataset of linked tweet-news pairs , which can benefit many nlp applications ; 2. in contrast to previous research which focuses on lexical features within the short texts ( text-to-word information ) , we propose a graph based latent variable model that models the inter short text correlations ( text-to-text information ) . this is motivated by the observation that a tweet usually only covers one aspect of an event . we show that using tweet specific feature ( hashtag ) and news specific feature ( named entities ) as well as temporal constraints , we are able to extract text-to-text correlations , and thus completes the semantic picture of a short text . our experiments show significant improvement of our new model over baselines with three evaluation metrics in the new task .

semantic density analysis : comparing word meaning across time and phonetic space
this paper presents a new statistical method for detecting and tracking changes in word meaning , based on latent semantic analysis . by comparing the density of semantic vector clusters this method allows researchers to make statistical inferences on questions such as whether the meaning of a word changed across time or if a phonetic cluster is associated with a specific meaning . possible applications of this method are then illustrated in tracing the semantic change of dog , do , and deer in early english and examining and comparing phonaesthemes .

extraction of nominal multiword expressions in french
multiword expressions ( mwes ) can be extracted automatically from large corpora using association measures , and tools like mwetoolkit allow researchers to generate training data for mwe extraction given a tagged corpus and a lexicon . we use mwetoolkit on a sample of the french europarl corpus together with the french lexicon dela , and use weka to train classifiers for mwe extraction on the generated training data . a manual evaluation shows that the classifiers achieve 6075 % precision and that about half of the mwes found are novel and not listed in the lexicon . we also investigate the impact of the patterns used to generate the training data and find that this can affect the trade-off between precision and novelty .

an api for measuring the relatedness of words in wikipedia simone paolo ponzetto andmichael strube
we present an api for computing the semantic relatedness of words in wikipedia .

interpretation of compound nominalisations using corpus and web
we present two novel paraphrase tests for automatically predicting the inherent semantic relation of a given compound nominalisation as one of subject , direct object , or prepositional object . we compare these to the usual verbargument paraphrase test using corpus statistics , and frequencies obtained by scraping the google search engine interface . we also implemented a more robust statistical measure than maximum likelihood estimation the confidence interval . a significant reduction in data sparseness was achieved , but this alone is insufficient to provide a substantial performance improvement .

smoothing a lexicon-based pos tagger for arabic and hebrew saib mansour khalil sima'an yoad winter computer science , technion illc computer science , technion
we propose an enhanced part-of-speech ( pos ) tagger of semitic languages that treats modern standard arabic ( henceforth arabic ) and modern hebrew ( henceforth hebrew ) using the same probabilistic model and architectural setting . we start out by porting an existing hidden markov model pos tagger for hebrew to arabic by exchanging a morphological analyzer for hebrew with buckwalter 's ( 2002 ) morphological analyzer for arabic . this gives state-of-theart accuracy ( 96.12 % ) , comparable to habash and rambows ( 2005 ) analyzerbased pos tagger on the same arabic datasets . however , further improvement of such analyzer-based tagging methods is hindered by the incomplete coverage of standard morphological analyzer ( bar haim et al , 2005 ) . to overcome this coverage problem we supplement the output of buckwalter 's analyzer with synthetically constructed analyses that are proposed by a model which uses character information ( diab et al , 2004 ) in a way that is similar to nakagawa 's ( 2004 ) system for chinese and japanese . a version of this extended model that ( unlike nakagawa ) incorporates synthetically constructed analyses also for known words achieves 96.28 % accuracy on the standard arabic test set .

sparsity in dependency grammar induction
a strong inductive bias is essential in unsupervised grammar induction . we explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types . specifically , we investigate sparsity-inducing penalties on the posterior distributions of parent-child pos tag pairs in the posterior regularization ( pr ) framework of graa et al ( 2007 ) . in experiments with 12 languages , we achieve substantial gains over the standard expectation maximization ( em ) baseline , with average improvement in attachment accuracy of 6.3 % . further , our method outperforms models based on a standard bayesian sparsity-inducing prior by an average of 4.9 % . on english in particular , we show that our approach improves on several other state-of-the-art techniques .

evaluating the results of a memory-based word-expert approach
in this paper , we evaluate the results of the antwerp university word sense disambiguation system in the english all words task of senseval-2 . in this approach , specialized memory-based wordexperts were trained per word-pos combination . through optimization by crossvalidation of the individual component classifiers and the voting scheme for combining them , the best possible word-expert was determined . in the competition , this word-expert architecture resulted in accuracies of 63.6 % ( fine-grained ) and 64.5 % ( coarse-grained ) on the senseval-2 test data . in order to better understand these results , we investigated whether classifiers trained on different information sources performed differently on the different part-of-speech categories . furthermore , the results were evaluated in terms of the available number of training items , the number of senses , and the sense distributions in the data set . we conclude that there is no information source which is optimal over all word-experts . selecting the optimal classifier/voter for each single word-expert , however , leads to major accuracy improvements . we furthermore show that accuracies do not so much depend on the available number of training items , but largely on polysemy and sense distributions .

automatic construction of japanese katakana variant list from large corpus
this paper presents a method to construct japanese katakana variant list from large corpus . our method is useful for information retrieval , information extraction , question answering , and so on , because katakana words tend to be used as loan words and the transliteration causes several variations of spelling . our method consists of three steps . at step 1 , our system collects katakana words from large corpus . at step 2 , our system collects candidate pairs of katakana variants from the collected katakana words using a spelling similarity which is based on the edit distance . at step 3 , our system selects variant pairs from the candidate pairs using a semantic similarity which is calculated by a vector space model of a context of each katakana word . we conducted experiments using 38 years of japanese newspaper articles and constructed japanese katakana variant list with the performance of 97.4 % recall and 89.1 % precision . estimating from this precision , our system can extract 178,569 variant pairs from the corpus .

fine-grained tree-to-string translation rule extraction xianchao wu takuya matsuzaki junichi tsujii
tree-to-string translation rules are widely used in linguistically syntax-based statistical machine translation systems . in this paper , we propose to use deep syntactic information for obtaining fine-grained translation rules . a head-driven phrase structure grammar ( hpsg ) parser is used to obtain the deep syntactic information , which includes a fine-grained description of the syntactic property and a semantic representation of a sentence . we extract fine-grained rules from aligned hpsg tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems . extensive experiments on largescale bidirectional japanese-english translations testified the effectiveness of our approach .

probabilistic head-driven parsing for discourse structure
we describe a data-driven approach to building interpretable discourse structures for appointment scheduling dialogues . we represent discourse structures as headed trees and model them with probabilistic head-driven parsing techniques . we show that dialogue-based features regarding turn-taking and domain specific goals have a large positive impact on performance . our best model achieves an f score of 43.2 % for labelled discourse relations and 67.9 % for unlabelled ones , significantly beating a right-branching baseline that uses the most frequent relations .

a self-adaptive classifier for efficient text-stream processing
a self-adaptive classifier for efficient text-stream processing is proposed . the proposed classifier adaptively speeds up its classification while processing a given text stream for various nlp tasks . the key idea behind the classifier is to reuse results for past classification problems to solve forthcoming classification problems . a set of classification problems commonly seen in a text stream is stored to reuse the classification results , while the set size is controlled by removing the least-frequently-used or least-recently-used classification problems . experimental results with twitter streams confirmed that the proposed classifier applied to a state-of-the-art base-phrase chunker and dependency parser speeds up its classification by factors of 3.2 and 5.7 , respectively .

person identification from text and speech genre samples the mitre corporation roberta evans sabin
in this paper , we describe experiments conducted on identifying a person using a novel unique correlated corpus of text and audio samples of the persons communication in six genres . the text samples include essays , emails , blogs , and chat . audio samples were collected from individual interviews and group discussions and then transcribed to text . for each genre , samples were collected for six topics . we show that we can identify the communicant with an accuracy of 71 % for six fold cross validation using an average of 22,000 words per individual across the six genres . for person identification in a particular genre ( train on five genres , test on one ) , an average accuracy of 82 % is achieved . for identification from topics ( train on five topics , test on one ) , an average accuracy of 94 % is achieved . we also report results on identifying a persons communication in a genre using text genres only as well as audio genres only .

creating a bi-lingual entailment corpus through translations with
this paper reports on experiments in the creation of a bi-lingual textual entailment corpus , using non-experts workforce under strict cost and time limitations ( $ 100 , 10 days ) . to this aim workers have been hired for translation and validation tasks , through the crowdflower channel to amazon mechanical turk . as a result , an accurate and reliable corpus of 426 english/spanish entailment pairs has been produced in a more cost-effective way compared to other methods for the acquisition of translations based on crowdsourcing . focusing on two orthogonal dimensions ( i.e . reliability of annotations made by non experts , and overall corpus creation costs ) , we summarize the methodology we adopted , the achieved results , the main problems encountered , and the lessons learned .

unknown word extraction for chinese documents
there is no blank to mark word boundaries in chinese text . as a result , identifying words is difficult , because of segmentation ambiguities and occurrences of unknown words . conventionally unknown words were extracted by statistical methods because statistical methods are simple and efficient . however the statistical methods without using linguistic knowledge suffer the drawbacks of low precision and low recall , since character strings with statistical significance might be phrases or partial phrases instead of words and low frequency new words are hardly identifiable by statistical methods . in addition to statistical information , we try to use as much information as possible , such as morphology , syntax , semantics , and world knowledge . the identification system fully utilizes the context and content information of unknown words in the steps of detection process , extraction process , and verification process . a practical unknown word extraction system was implemented which online identifies new words , including low frequency new words , with high precision and high recall rates .

adding semantic role annotation to a corpus of written dutch
we present an approach to automatic semantic role labeling ( srl ) carried out in the context of the dutch language corpus initiative ( d-coi ) project . adapting earlier research which has mainly focused on english to the dutch situation poses an interesting challenge especially because there is no semantically annotated dutch corpus available that can be used as training data . our automatic srl approach consists of three steps : bootstrapping from a syntactically annotated corpus by means of a rulebased tagger developed for this purpose , manual correction on the basis of the propbank guidelines which have been adapted to dutch and training a machine learning system on the manually corrected data .

automatic detection of grammar elements that decrease readability
this paper proposes an automatic method of detecting grammar elements that decrease readability in a japanese sentence . the method consists of two components : ( 1 ) the check list of the grammar elements that should be detected ; and ( 2 ) the detector , which is a search program of the grammar elements from a sentence . by defining a readability level for every grammar element , we can find which part of the sentence is difficult to read .

an analysis of annotation of verb-noun idiomatic combinations in a parallel dependency corpus
while working on valency lexicons for czech and english , it was necessary to define treatment of multiword entities ( mwes ) with the verb as the central lexical unit . morphological , syntactic and semantic properties of such mwes had to be formally specified in order to create lexicon entries and use them in treebank annotation . such a formal specification has also been used for automated quality control of the annotation vs. the lexicon entries . we present a corpus-based study , concentrating on multilayer specification of verbal mwes , their properties in czech and english , and a comparison between the two languages using the parallel czech-english dependency treebank ( pcedt ) . this comparison revealed interesting differences in the use of verbal mwes in translation ( discovering that such mwes are actually rarely translated as mwes , at least between czech and english ) as well as some inconsistencies in their annotation . adding mwe-based checks should thus result in better quality control of future treebank/lexicon annotation . since czech and english are typologically different languages , we believe that our findings will also contribute to a better understanding of verbal mwes and possibly their more unified treatment across languages .

simulating the acquisition of object names
naming requires recognition . recognition requires the ability to categorize objects and events . infants under six months of age are capable of making fine-grained discriminations of object boundaries and three-dimensional space . at 8 to 10 months , a childs object categories are sufficiently stable and flexible to be used as the foundation for labeling and referencing actions . what mechanisms in the brain underlie the unfolding of these capacities in this article , we describe a neural network model which attempts to simulate , in a biologically plausible way , the process by which infants learn how to recognize objects and words through exposure to visual stimuli and vocal sounds .

compositional matrix-space models of language fzi forschungszentrum informatik
we propose cmsms , a novel type of generic compositional models for syntactic and semantic aspects of natural language , based on matrix multiplication . we argue for the structural and cognitive plausibility of this model and show that it is able to cover and combine various common compositional nlp approaches ranging from statistical word space models to symbolic grammar formalisms .

multi-component tree adjoining grammars , dependency graph models , and linguistic analyses
recent work identifies two properties that appear particularly relevant to the characterization of graph-based dependency models of syntactic structure : the absence of interleaving substructures ( well-nestedness ) and a bound on a type of discontinuity ( gap-degree 1 ) successfully describe more than 99 % of the structures in two dependency treebanks . 2 bodirsky et al establish that every dependency structure with these two properties can be recast as a lexicalized tree adjoining grammar ( ltag ) derivation and vice versa . however , multicomponent extensions of tag ( mc-tag ) , argued to be necessary on linguistic grounds , induce dependency structures that do not conform to these two properties . in this paper , we observe that several types of mc-tag as used for linguistic analysis are more restrictive than the formal system is in principle . in particular , tree-local mc-tag , tree-local mc-tag with flexible composition , and special cases of set-local tag as used to describe certain linguistic phenomena satisfy the well-nested and gap degree 1 criteria . we also observe that gap degree can distinguish between prohibited and allowed wh-extractions in english , and report some preliminary work comparing the predictions of the graph approach and the mctag approach to scrambling .

a log-linear model with an n-gram reference distribution for accurate hpsg
this paper describes a log-linear model with an n-gram reference distribution for accurate probabilistic hpsg parsing . in the model , the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries , which are provided by the discriminative method with machine learning features of word and pos n-gram as defined in the ccg/hpsg/cdg supertagging . recently , supertagging becomes well known to drastically improve the parsing accuracy and speed , but supertagging techniques were heuristically introduced , and hence the probabilistic models for parse trees were not well defined . we introduce the supertagging probabilities as a reference distribution for the log-linear model of the probabilistic hpsg . this is the first model which properly incorporates the supertagging probabilities into parse trees probabilistic model .

analytical approaches to combining mt technologies
the talk will report on recent and ongoing work dedicated to analytical methods for a systematic combination of observed strengths of translation technologies . the focus will be on different ways of exploiting existing data on mt output and performance measures for system combination and for gaining insights on strengths and weaknesses of existing technologies .

the iucl+ system : word-level language identification via extended
we describe the iucl+ system for the shared task of the first workshop on computational approaches to code switching , in which participants were challenged to label each word in twitter texts as a named entity or one of two candidate languages . our system combines character n-gram probabilities , lexical probabilities , word label transition probabilities and existing named entity recognition tools within a markovmodel framework that weights these components and assigns a label . our approach is language-independent , and we submitted results for all data sets ( five test sets and three surprise sets , covering four language pairs ) , earning the highest accuracy score on the tweet level on two language pairs ( mandarin-english , arabicdialects 1 & 2 ) and one of the surprise sets ( arabic-dialects ) .

unifying tense , aspect and modality across languages
this paper computes the semantic representation of while as the pragmatically most relevant one which speakers select from a variety of grammatical constructions in which while may occur in current english . the semantic representation of while provides the condition for translating it into the adequate german equivalent . this computation is implemented in a unificationbased formalism and may thus be applied in a machine translation system .

random walk factoid annotation for collective discourse ben king rahul jha the new yorker magazine
in this paper , we study the problem of automatically annotating the factoids present in collective discourse . factoids are information units that are shared between instances of collective discourse and may have many different ways of being realized in words . our approach divides this problem into two steps , using a graph-based approach for each step : ( 1 ) factoid discovery , finding groups of words that correspond to the same factoid , and ( 2 ) factoid assignment , using these groups of words to mark collective discourse units that contain the respective factoids . we study this on two novel data sets : the new yorker caption contest data set , and the crossword clues data set .

from pathways to biomolecular events : opportunities and challenges
the construction of pathways is a major focus of present-day biology . typical pathways involve large numbers of entities of various types whose associations are represented as reactions involving arbitrary numbers of reactants , outputs and modifiers . until recently , few information extraction approaches were capable of resolving the level of detail in text required to support the annotation of such pathway representations . we argue that event representations of the type popularized by the bionlp shared task are potentially applicable for pathway annotation support . as a step toward realizing this possibility , we study the mapping from a formal pathway representation to the event representation in order to identify remaining challenges in event extraction for pathway annotation support . following initial analysis , we present a detailed study of protein association and dissociation reactions , proposing a new event class and representation for the latter and , as a step toward its automatic extraction , introduce a manually annotated resource incorporating the type among a total of nearly 1300 annotated event instances . as a further practical contribution , we introduce the first pathway-to-event conversion software for sbml/celldesigner pathways and discuss the opportunities arising from the ability to convert the substantial existing pathway resources to events .

tools for non-native readers : the case for translation and simplification
one of the populations that often needs some form of help to read everyday documents is non-native speakers . this paper discusses aid at the word and word string levels and focuses on the possibility of using translation and simplification . seen from the perspective of the non-native as an ever-learning reader , we show how translation may be of more harm than help in understanding and retaining the meaning of a word while simplification holds promise . we conclude that if reading everyday documents can be considered as a learning activity as well as a practical necessity , then our study reinforces the arguments that defend the use of simplification to make documents that non-natives need to read more accessible .

leveraging structural relations for fluent compressions at multiple compression rates
prior approaches to sentence compression have taken low level syntactic constraints into account in order to maintain grammaticality . we propose and successfully evaluate a more comprehensive , generalizable feature set that takes syntactic and structural relationships into account in order to sustain variable compression rates while making compressed sentences more coherent , grammatical and readable .

bayesian inference for pcfgs via markov chain monte carlo
this paper presents two markov chain monte carlo ( mcmc ) algorithms for bayesian inference of probabilistic context free grammars ( pcfgs ) from terminal strings , providing an alternative to maximum-likelihood estimation using the inside-outside algorithm . we illustrate these methods by estimating a sparse grammar describing the morphology of the bantu language sesotho , demonstrating that with suitable priors bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the inside-outside algorithm only produce a trivial grammar .

taxonomy learning factoring the structure of a taxonomy into a semantic classification decision
the paper examines different possibilities to take advantage of the taxonomic organization of a thesaurus to improve the accuracy of classifying new words into its classes . the results of the study demonstrate that taxonomic similarity between nearest neighbors , in addition to their distributional similarity to the new word , may be useful evidence on which classification decision can be based .

bengali , hindi and telugu to english ad-hoc bilingual task
this paper presents the experiments carried out at jadavpur university as part of participation in the clef 2007 ad-hoc bilingual task . this is our first participation in the clef evaluation task and we have considered bengali , hindi and telugu as query languages for the retrieval from english document collection . we have discussed our bengali , hindi and telugu to english clir system as part of the ad-hoc bilingual task , english ir system for the ad-hoc monolingual task and the associated experiments at clef . query construction was manual for telugu-english ad-hoc bilingual task , while it was automatic for all other tasks .

evaluation of commonsense knowledge with mechanical turk
efforts to automatically acquire world knowledge from text suffer from the lack of an easy means of evaluating the resulting knowledge . we describe initial experiments using mechanical turk to crowdsource evaluation to nonexperts for little cost , resulting in a collection of factoids with associated quality judgements . we describe the method of acquiring usable judgements from the public and the impact of such large-scale evaluation on the task of knowledge acquisition .

a semantic kernel for predicate argument classification alessandro moschitti and cosmin adrian bejan
automatically deriving semantic structures from text is a challenging task for machine learning . the flat feature representations , usually used in learning models , can only partially describe structured data . this makes difficult the processing of the semantic information that is embedded into parse-trees . in this paper a new kernel for automatic classification of predicate arguments has been designed and experimented . it is based on subparse-trees annotated with predicate argument information from propbank corpus . this kernel , exploiting the convolution properties of the parse-tree kernel , enables us to learn which syntactic structures can be associated with the arguments defined in propbank . support vector machines ( svms ) using such a kernel classify arguments with a better accuracy than svms based on linear kernel .

multi-document summarization of evaluative text deptartment of computer science
we present and compare two approaches to the task of summarizing evaluative arguments . the first is a sentence extractionbased approach while the second is a language generation-based approach . we evaluate these approaches in a user study and find that they quantitatively perform equally well . qualitatively , however , we find that they perform well for different but complementary reasons . we conclude that an effective method for summarizing evaluative arguments must effectively synthesize the two approaches .

intermediary semantic representation through proposition structures both authors equally contributed to this paper
we propose an intermediary-level semantic representation , providing a higher level of abstraction than syntactic parse trees , while not committing to decisions in cases such as quantification , grounding or verbspecific roles assignments . the proposal is centered around the proposition structure of the text , and includes also implicit propositions which can be inferred from the syntax but are not transparent in parse trees , such as copular relations introduced by appositive constructions . other benefits over dependency-trees are explicit marking of logical relations between propositions , explicit marking of multiword predicate such as light-verbs , and a consistent representation for syntacticallydifferent but semantically-similar structures . the representation is meant to serve as a useful input layer for semanticoriented applications , as well as to provide a better starting point for further levels of semantic analysis such as semantic-rolelabeling and semantic-parsing .

the acl anthology searchbench
we describe a novel application for structured search in scientific digital libraries . the acl anthology searchbench is meant to become a publicly available research tool to query the content of the acl anthology . the application provides search in both its bibliographic metadata and semantically analyzed full textual content . by combining these two features , very efficient and focused queries are possible . at the same time , the application serves as a showcase for the recent progress in natural language processing ( nlp ) research and language technology . the system currently indexes the textual content of 7,500 anthology papers from 20022009 with predicateargument-like semantic structures . it also provides useful search filters based on bibliographic metadata . it will be extended to provide the full anthology content and enhanced functionality based on further nlp techniques .

a multimodal interface for access to content in the home
in order to effectively access the rapidly increasing range of media content available in the home , new kinds of more natural interfaces are needed . in this paper , we explore the application of multimodal interface technologies to searching and browsing a database of movies . the resulting system allows users to access movies using speech , pen , remote control , and dynamic combinations of these modalities . an experimental evaluation , with more than 40 users , is presented contrasting two variants of the system : one combining speech with traditional remote control input and a second where the user has a tablet display supporting speech and pen input .

language model information retrieval with document expansion
language model information retrieval depends on accurate estimation of document models . in this paper , we propose a document expansion technique to deal with the problem of insufficient sampling of documents . we construct a probabilistic neighborhood for each document , and expand the document with its neighborhood information . the expanded document provides a more accurate estimation of the document model , thus improves retrieval accuracy . moreover , since document expansion and pseudo feedback exploit different corpus structures , they can be combined to further improve performance . the experiment results on several different data sets demonstrate the effectiveness of the proposed document expansion method .

overview of bionlp09 shared task on event extraction
the paper presents the design and implementation of the bionlp09 shared task , and reports the final results with analysis . the shared task consists of three sub-tasks , each of which addresses bio-molecular event extraction at a different level of specificity . the data was developed based on the genia event corpus . the shared task was run over 12 weeks , drawing initial interest from 42 teams . of these teams , 24 submitted final results . the evaluation results are encouraging , indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges .

ccg chart realization from disjunctive inputs
this paper presents a novel algorithm for efficiently generating paraphrases from disjunctive logical forms . the algorithm is couched in the framework of combinatory categorial grammar ( ccg ) and has been implemented as an extension to the openccg surface realizer . the algorithm makes use of packed representations similar to those initially proposed by shemtov ( 1997 ) , generalizing the approach in a more straightforward way than in the algorithm ultimately adopted therein .

dynamic knowledge-base alignment for coreference resolution
coreference resolution systems can benefit greatly from inclusion of global context , and a number of recent approaches have demonstrated improvements when precomputing an alignment to external knowledge sources . however , since alignment itself is a challenging task and is often noisy , existing systems either align conservatively , resulting in very few links , or combine the attributes of multiple candidates , leading to a conflation of entities . our approach instead performs joint inference between within-document coreference and entity linking , maintaining ranked lists of candidate entities that are dynamically merged and reranked during inference . further , we incorporate a large set of surface string variations for each entity by using anchor texts from the web that link to the entity . these forms of global context enables our system to improve classifier-based coreference by 1.09 b3 f1 points , and improve over the previous state-of-art by 0.41 points , thus introducing a new state-of-art result on the ace 2004 data .

do we need chinese word segmentation for statistical machine translation chair of computer science vi
in chinese texts , words are not separated by white spaces . this is problematic for many natural language processing tasks . the standard approach is to segment the chinese character sequence into words . here , we investigate chinese word segmentation for statistical machine translation . we pursue two goals : the first one is the maximization of the final translation quality ; the second is the minimization of the manual effort for building a translation system . the commonly used method for getting the word boundaries is based on a word segmentation tool and a predefined monolingual dictionary . to avoid the dependence of the translation system on an external dictionary , we have developed a system that learns a domainspecific dictionary from the parallel training corpus . this method produces results that are comparable with the predefined dictionary . further more , our translation system is able to work without word segmentation with only a minor loss in translation quality .

rebuilding the oxford dictionary of english as a semantic network
this paper describes a project to develop a lexicon for use both as an electronic dictionary and as a database for a range of nlp tasks . it proposes that a lexicon for such open-ended application may be derived from a human-user dictionary , retaining and enhancing the richness of its editorial content but abandoning its entry-list structure in favour of networks of relationships between discrete lexical objects , where each object represents a discrete lexeme-meaning unit .

thai sentence-breaking for large-scale smt
thai language text presents challenges for integration into large-scale multilanguage statistical machine translation ( smt ) systems , largely stemming from the nominal lack of punctuation and inter-word space . for thai sentence breaking , we describe a monolingual maximum entropy classifier with features that may be applicable to other languages such as arabic , khmer and lao . we apply this sentence breaker to our largevocabulary , general-purpose , bidirectional thai-english smt system , and achieve bleu scores of around 0.20 , reaching our threshold of releasing it as a free online service .

inducing a multilingual dictionary from a parallel multitext in related languages
dictionaries and word translation models are used by a variety of systems , especially in machine translation . we build a multilingual dictionary induction system for a family of related resource-poor languages . we assume only the presence of a single medium-length multitext ( the bible ) . the techniques rely upon lexical and syntactic similarity of languages as well as on the fact that building dictionaries for several pairs of languages provides information about other pairs .

typed graph models for semi-supervised learning of name ethnicity
this paper presents an original approach to semi-supervised learning of personal name ethnicity from typed graphs of morphophonemic features and first/last-name co-occurrence statistics . we frame this as a general solution to an inference problem over typed graphs where the edges represent labeled relations between features that are parameterized by the edge types . we propose a framework for parameter estimation on different constructions of typed graphs for this problem using a gradient-free optimization method based on grid search . results on both in-domain and out-of-domain data show significant gains over 30 % accuracy improvement using the techniques presented in the paper .

an exploration of embeddings for generalized phrases
deep learning embeddings have been successfully used for many natural language processing problems . embeddings are mostly computed for word forms although lots of recent papers have extended this to other linguistic units like morphemes and word sequences . in this paper , we define the concept of generalized phrase that includes conventional linguistic phrases as well as skip-bigrams . we compute embeddings for generalized phrases and show in experimental evaluations on coreference resolution and paraphrase identification that such embeddings perform better than word form embeddings .

a look inside the distributionally similar terms
we analyzed the details of aweb-derived distributional data of japanese nominal terms with two aims . one aim is to examine if distributionally similar terms can be in fact equated with semantically similar terms , and if so to what extent . the other is to investigate into what kind of semantic relations constitute ( strongly ) distributionally similar terms . our results show that over 85 % of the pairs of the terms derived from the highly similar terms turned out to be semantically similar in some way . the ratio of classmate , synonymous , hypernym-hyponym , and meronymic relations are about 62 % , 17 % , 8 % and 1 % of the classified data , respectively .

a statistical model for domain-independent text segmentation
we propose a statistical method that finds the maximum-probability segmentation of a given text . this method does not require training data because it estimates probabilities from the given text . therefore , it can be applied to any text in any domain . an experiment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system .

a comparison of tutor and student behavior in speech versus text based
this paper describes preliminary work in exploring the relative effectiveness of speech versus text based tutoring . most current tutorial dialogue systems are text based ( evens et al. , 2001 ; rose and aleven , 2002 ; zinn et al. , 2002 ; aleven et al , 2001 ; vanlehn et al , 2002 ) . however , prior studies have shown considerable benefits of tutoring through spoken interactions ( lemke , 1990 ; chi et al , 1994 ; hausmann and chi , 2002 ) . thus , we are currently developing a speech based dialogue system that uses a text based system for tutoring conceptual physics ( vanlehn et al , 2002 ) as its back-end . in order to explore the relative effectiveness between these two input modalities in our task domain , we have started by collecting parallel human-human tutoring corpora both for text based and speech based tutoring . in both cases , students interact with the tutor through a web interface . we present here a comparison between the two on a number of features of dialogue that have been demonstrated to correlate reliably with learning gains with students interacting with the tutor using the text based interface ( rose et al , submitted ) .

intelligent systems program
this paper describes extensions to a corpus annotation scheme for the manual annotation of attributions , as well as opinions , emotions , sentiments , speculations , evaluations and other private states in language . it discusses the scheme with respect to the pie in the sky check list of desirable semantic information for annotation . we believe that the scheme is a good foundation for adding private state annotations to other layers of semantic meaning .

search engine statistics beyond the n-gram : application to noun compound bracketing
in order to achieve the long-range goal of semantic interpretation of noun compounds , it is often necessary to first determine their syntactic structure . this paper describes an unsupervised method for noun compound bracketing which extracts statistics from web search engines using a 2 measure , a new set of surface features , and paraphrases . on a gold standard , the system achieves results of 89.34 % ( baseline 66.80 % ) , which is a sizable improvement over the state of the art ( 80.70 % ) .

contextual bearing on linguistic variation in social media marina del rey , ca
microtexts , like sms messages , twitter posts , and facebook status updates , are a popular medium for real-time communication . in this paper , we investigate the writing conventions that different groups of users use to express themselves in microtexts . our empirical study investigates properties of lexical transformations as observed within twitter microtexts . the study reveals that different populations of users exhibit different amounts of shortened english terms and different shortening styles . the results reveal valuable insights into how human language technologies can be effectively applied to microtexts .

large margin synchronous generation and its application to sentence compression
this paper presents a tree-to-tree transduction method for text rewriting . our model is based on synchronous tree substitution grammar , a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches . we describe an algorithm for decoding in this framework and show how the model can be trained discriminatively within a large margin framework . experimental results on sentence compression bring significant improvements over a state-of-the-art model .

growing finely-discriminating taxonomies from seeds of varying quality and size
concept taxonomies offer a powerful means for organizing knowledge , but this organization must allow for many overlapping and fine-grained perspectives if a general-purpose taxonomy is to reflect concepts as they are actually employed and reasoned about in everyday usage . we present here a means of bootstrapping finely-discriminating taxonomies from a variety of different starting points , or seeds , that are acquired from three different sources : wordnet , conceptnet and the web at large .

improving name tagging
information extraction systems incorporate multiple stages of linguistic analysis . although errors are typically compounded from stage to stage , it is possible to reduce the errors in one stage by harnessing the results of the other stages . we demonstrate this by using the results of coreference analysis and relation extraction to reduce the errors produced by a chinese name tagger . we use an n-best approach to generate multiple hypotheses and have them re-ranked by subsequent stages of processing . we obtained thereby a reduction of 24 % in spurious and incorrect name tags , and a reduction of 14 % in missed tags .

experts retrieval with multiword-enhanced author topic model
in this paper , we propose a multiwordenhanced author topic model that clusters authors with similar interests and expertise , and apply it to an information retrieval system that returns a ranked list of authors related to a keyword . for example , we can retrieve eugene charniak via search for statistical parsing . the existing works on author topic modeling assume a bag-of-words representation . however , many semantic atomic concepts are represented by multiwords in text documents . this paper presents a pre-computation step as a way to discover these multiwords in the corpus automatically and tags them in the termdocument matrix . the key advantage of this method is that it retains the simplicity and the computational efficiency of the unigram model . in addition to a qualitative evaluation , we evaluate the results by using the topic models as a component in a search engine . we exhibit improved retrieval scores when the documents are represented via sets of latent topics and authors .

a uniform approach to analogies , synonyms , antonyms ,
recognizing analogies , synonyms , antonyms , and associations appear to be four distinct tasks , requiring distinct nlp algorithms . in the past , the four tasks have been treated independently , using a wide variety of algorithms . these four semantic classes , however , are a tiny sample of the full range of semantic phenomena , and we can not afford to create ad hoc algorithms for each semantic phenomenon ; we need to seek a unified approach . we propose to subsume a broad range of phenomena under analogies . to limit the scope of this paper , we restrict our attention to the subsumption of synonyms , antonyms , and associations . we introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs , and we show that it can solve multiple-choice sat analogy questions , toefl synonym questions , esl synonym-antonym questions , and similar-associated-both questions from cognitive psychology .

a chinese word segmentation system based on cascade model
this paper introduces the system of word segmentation and analyzes its evaluation results in the fourth sighan bakeoff1 . a novel method has been used in the system , which main idea is : firstly , the main problems of ws have been classified , and then a cascaded model has been used to gradually optimize the system . the core of this ws system is the segmentation of ambiguous words and the internal information extraction of unknown words . the experiments show that the performance is satisfying , with the riv-measure 96.8 % in ncc open test in the sighan bakeoff 2007 .

using frame semantics in natural language processing
we summarize our experience using framenet in two rather different projects in natural language processing ( nlp ) . we conclude that nlp can benefit from framenet in different ways , but we sketch some problems that need to be overcome .

chinese named entity recognition with conditional random fields
we present a chinese named entity recognition ( ner ) system submitted to the close track of sighan bakeoff2006 . we define some additional features via doing statistics in training corpus . our system incorporates basic features and additional features based on conditional random fields ( crfs ) . in order to correct inconsistently results , we perform the postprocessing procedure according to n-best results given by the crfs model . our final system achieved a f-score of 85.14 at msra , 89.03 at cityu , and 76.27 at ldc .

generating artificial errors for grammatical error correction
this paper explores the generation of artificial errors for correcting grammatical mistakes made by learners of english as a second language . artificial errors are injected into a set of error-free sentences in a probabilistic manner using statistics from a corpus . unlike previous approaches , we use linguistic information to derive error generation probabilities and build corpora to correct several error types , including open-class errors . in addition , we also analyse the variables involved in the selection of candidate sentences . experiments using the nucle corpus from the conll 2013 shared task reveal that : 1 ) training on artificially created errors improves precision at the expense of recall and 2 ) different types of linguistic information are better suited for correcting different error types .

context-aware learning for sentence-level sentiment analysis with posterior regularization
this paper proposes a novel context-aware method for analyzing sentiment at the level of individual sentences . most existing machine learning approaches suffer from limitations in the modeling of complex linguistic structures across sentences and often fail to capture nonlocal contextual cues that are important for sentiment interpretation . in contrast , our approach allows structured modeling of sentiment while taking into account both local and global contextual information . specifically , we encode intuitive lexical and discourse knowledge as expressive constraints and integrate them into the learning of conditional random field models via posterior regularization . the context-aware constraints provide additional power to the crf model and can guide semi-supervised learning when labeled data is limited . experiments on standard product review datasets show that our method outperforms the state-of-theart methods in both the supervised and semi-supervised settings .

evaluation technology from speaker identification to affective analysis : a multi-step system for analyzing childrens stories
we propose a multi-step system for the analysis of childrens stories that is intended to be part of a larger text-to-speechbased storytelling system . a hybrid approach is adopted , where pattern-based and statistical methods are used along with utilization of external knowledge sources . this system performs the following story analysis tasks : identification of characters in each story ; attribution of quotes to specific story characters ; identification of character age , gender and other salient personality attributes ; and finally , affective analysis of the quoted material . the different types of analyses were evaluated using several datasets . for the quote attribution , as well as for the gender and age estimation , substantial improvement over baseline was realized , whereas results for personality attribute estimation and valence estimation are more modest .

estimating word alignment quality for smt reordering tasks sara stymne j org tiedemann joakim nivre
previous studies of the effect of word alignment on translation quality in smt generally explore link level metrics only and mostly do not show any clear connections between alignment and smt quality . in this paper , we specifically investigate the impact of word alignment on two pre-reordering tasks in translation , using a wider range of quality indicators than previously done . experiments on germanenglish translation show that reordering may require alignment models different from those used by the core translation system . sparse alignments with high precision on the link level , for translation units , and on the subset of crossing links , like intersected hmm models , are preferred . unlike smt performance the desired alignment characteristics are similar for small and large training data for the pre-reordering tasks . moreover , we confirm previous research showing that the fuzzy reordering score is a useful and cheap proxy for performance on smt reordering tasks .

naive bayes word sense induction do kook choe
we introduce an extended naive bayes model for word sense induction ( wsi ) and apply it to a wsi task . the extended model incorporates the idea the words closer to the target word are more relevant in predicting its sense . the proposed model is very simple yet effective when evaluated on semeval-2010 wsi data .

an empirical study of active learning with support vector machines for japanese word segmentation
we explore how active learning with support vector machines works well for a non-trivial task in natural language processing . we use japanese word segmentation as a test case . in particular , we discuss how the size of a pool affects the learning curve . it is found that in the early stage of training with a larger pool , more labeled examples are required to achieve a given level of accuracy than those with a smaller pool . in addition , we propose a novel technique to use a large number of unlabeled examples effectively by adding them gradually to a pool . the experimental results show that our technique requires less labeled examples than those with the technique in previous research . to achieve 97.0 % accuracy , the proposed technique needs 59.3 % of labeled examples that are required when using the previous technique and only 17.4 % of labeled examples with random sampling .

turn-taking cues in a human tutoring corpus
most spoken dialogue systems are still lacking in their ability to accurately model the complex process that is human turntaking . this research analyzes a humanhuman tutoring corpus in order to identify prosodic turn-taking cues , with the hopes that they can be used by intelligent tutoring systems to predict student turn boundaries . results show that while there was variation between subjects , three features were significant turn-yielding cues overall . in addition , a positive relationship between the number of cues present and the probability of a turn yield was demonstrated .

bootstrapping a multilingual part-of-speech tagger in one person-day
this paper presents a method for bootstrapping a fine-grained , broad-coverage part-of-speech ( pos ) tagger in a new language using only one personday of data acquisition effort . it requires only three resources , which are currently readily available in 60-100 world languages : ( 1 ) an online or hard-copy pocket-sized bilingual dictionary , ( 2 ) a basic library reference grammar , and ( 3 ) access to an existing monolingual text corpus in the language . the algorithm begins by inducing initial lexical pos distributions from english translations in a bilingual dictionary without pos tags . it handles irregular , regular and semi-regular morphology through a robust generative model using weighted levenshtein alignments . unsupervised induction of grammatical gender is performed via global modeling of contextwindow feature agreement . using a combination of these and other evidence sources , interactive training of context and lexical prior models are accomplished for fine-grained pos tag spaces . experiments show high accuracy , fine-grained tag resolution with minimal new human effort .

automatic construction of predicate-argument structure patterns for biomedical information extraction akane yakushiji yusuke miyao tomoko ohta yuka tateisi junichi tsujii
this paper presents a method of automatically constructing information extraction patterns on predicate-argument structures ( pass ) obtained by full parsing from a smaller training corpus . because pass represent generalized structures for syntactical variants , patterns on pass are expected to be more generalized than those on surface words . in addition , patterns are divided into components to improve recall and we introduce a support vector machine to learn a prediction model using pattern matching results . in this paper , we present experimental results and analyze them on how well protein-protein interactions were extracted from medline abstracts . the results demonstrated that our method improved accuracy compared to a machine learning approach using surface word/part-of-speech patterns .

fast full parsing by linear-chain conditional random fields yoshimasa tsuruoka junichi tsujii sophia ananiadou
this paper presents a chunking-based discriminative approach to full parsing . we convert the task of full parsing into a series of chunking tasks and apply a conditional random field ( crf ) model to each level of chunking . the probability of an entire parse tree is computed as the product of the probabilities of individual chunking results . the parsing is performed in a bottom-up manner and the best derivation is efficiently obtained by using a depthfirst search algorithm . experimental results demonstrate that this simple parsing framework produces a fast and reasonably accurate parser .

creating multilingual translation lexicons with regional variations using web corpora
the purpose of this paper is to automatically create multilingual translation lexicons with regional variations . we propose a transitive translation approach to determine translation variations across languages that have insufficient corpora for translation via the mining of bilingual search-result pages and clues of geographic information obtained from web search engines . the experimental results have shown the feasibility of the proposed approach in efficiently generating translation equivalents of various terms not covered by general translation dictionaries . it also revealed that the created translation lexicons can reflect different cultural aspects across regions such as taiwan , hong kong and mainland china .

using wizard-of-oz simulations to bootstrap reinforcement-learningbased dialog management systems
this paper describes a method for bootstrapping a reinforcement learningbased dialog manager using a wizard-ofoz trial . the state space and action set are discovered through the annotation , and an initial policy is generated using a supervised learning algorithm . the method is tested and shown to create an initial policy which performs significantly better and with less effort than a handcrafted policy , and can be generated using a small number of dialogs .

detecting code-switching in a multilingual alpine heritage corpus
this paper describes experiments in detecting and annotating code-switching in a large multilingual diachronic corpus of swiss alpine texts . the texts are in english , french , german , italian , romansh and swiss german . because of the multilingual authors ( mountaineers , scientists ) and the assumed multilingual readers , the texts contain numerous code-switching elements . when building and annotating the corpus , we faced issues of language identification on the sentence and subsentential level . we present our strategy for language identification and for the annotation of foreign language fragments within sentences . we report 78 % precision on detecting a subset of code-switches with correct language labels and 92 % unlabeled precision .

textrank : bringing order into texts
in this paper , we introduce textrank a graph-based ranking model for text processing , and show how this model can be successfully used in natural language applications . in particular , we propose two innovative unsupervised methods for keyword and sentence extraction , and show that the results obtained compare favorably with previously published results on established benchmarks .

relating wordnet senses for word sense disambiguation
the granularity of word senses in current general purpose sense inventories is often too fine-grained , with narrow sense distinctions that are irrelevant for many nlp applications . this has particularly been a problem with wordnet which is widely used for word sense disambiguation ( wsd ) . there have been several attempts to group wordnet senses given a number of different information sources in order to reduce granularity . we propose relating senses as a matter of degree to permit a softer notion of relationships between senses compared to fixed groupings so that granularity can be varied according to the needs of the application . we compare two such approaches with a gold-standard produced by humans for this work . we also contrast this goldstandard and another used in previous research with the automatic methods for relating senses for use with back-off methods for wsd .

identifying and ranking topic clusters in the blogosphere
the blogosphere is a huge collaboratively constructed resource containing diverse and rich information . this diversity and richness presents a significant research challenge to the information retrieval community . this paper addresses this challenge by proposing a method for identification of topic clusters within the blogosphere where topic clusters represent the concept of grouping together blogs sharing a common interest i.e . topic , the algorithm takes into account both the hyperlinked social network of blogs along with the content in the blog posts . additionally we use various forms and parts-of-speech of the topic to provide a broader coverage of the blogosphere . the next step of the method is to assign topic-specific ranks to each blog in the cluster using a metric called topic discussion rank , that helps in identifying the most influential blog for a specific topic . we also perform an experimental evaluation of our method on real blog data and show that the proposed method reaches a high level of accuracy .

automatic identification of chinese event descriptive
this paper gives a new definition of chinese clause called event descriptive clause and proposes an automatic method to identify these clauses in chinese sentence . by analyzing the characteristics of the clause , the recognition task is formulated as a classification of chinese punctuations . the maximum entropy classifier is trained and two kinds of useful features and their combinations are explored in the task . meanwhile , a simple rule-based post processing phase is also proposed to improve the recognition performance . ultimately , we obtain 81.32 % f-score on the test set .

the far reach of multiword expressions in educational technology educational testing service
multiword expressions as they appear as nominal compounds , collocational forms , and idioms are now leveraged in educational technology in assessment and instruction contexts . the talk will focus on how multiword expression identification is used in different kinds of educational applications , including automated essay evaluation , and teacher professional development in curriculum development for english language learners . recent approaches developed to resolve polarity for noun-noun compounds in a sentiment system being designed to handle evaluation of argumentation ( sentiment ) in testtaker writing ( beigman-klebanov , burstein , and madnani , to appear ) will also be described . about the speaker jill burstein is a managing principal research scientist in the research & development division at educational testing service in princeton , new jersey . her background and expertise is in computational linguistics with a focus on educational applications for writing , reading , and teacher professional development . she holds 13 patents for educational technology inventions . jills inventions include e-rater , an automated essay scoring and evaluation system . and , in more recent work , she has leveraged natural language processing to develop language musesm , a teacher professional development application that supports teachers in the development of languagebased instruction that aids english learner content understanding and language skills development . she received her b.a . in linguistics and spanish from new york university , and her m.a .

automatic idiom identification in wiktionary computer science & engineering
online resources , such as wiktionary , provide an accurate but incomplete source of idiomatic phrases . in this paper , we study the problem of automatically identifying idiomatic dictionary entries with such resources . we train an idiom classifier on a newly gathered corpus of over 60,000 wiktionary multi-word definitions , incorporating features that model whether phrase meanings are constructed compositionally . experiments demonstrate that the learned classifier can provide high quality idiom labels , more than doubling the number of idiomatic entries from 7,764 to 18,155 at precision levels of over 65 % . these gains also translate to idiom detection in sentences , by simply using known word sense disambiguation algorithms to match phrases to their definitions . in a set of wiktionary definition example sentences , the more complete set of idioms boosts detection recall by over 28 percentage points .

context comparison of bursty events in web search and online media
in this paper , we conducted a systematic comparative analysis of language in different contexts of bursty topics , including web search , news media , blogging , and social bookmarking . we analyze ( 1 ) the content similarity and predictability between contexts , ( 2 ) the coverage of search content by each context , and ( 3 ) the intrinsic coherence of information in each context . our experiments show that social bookmarking is a better predictor to the bursty search queries , but news media and social blogging media have a much more compelling coverage . this comparison provides insights on how the search behaviors and social information sharing behaviors of users are correlated to the professional news media in the context of bursty events .

a unified framework for scope learning via simplified shallow semantic parsing
this paper approaches the scope learning problem via simplified shallow semantic parsing . this is done by regarding the cue as the predicate and mapping its scope into several constituents as the arguments of the cue . evaluation on the bioscope corpus shows that the structural information plays a critical role in capturing the relationship between a cue and its dominated arguments . it also shows that our parsing approach significantly outperforms the state-of-the-art chunking ones . although our parsing approach is only evaluated on negation and speculation scope learning here , it is portable to other kinds of scope learning .

speech translation with grammatical framework
grammatical framework ( gf ) is a grammar formalism which supports interlinguabased translation , library-based grammar engineering , and compilation to speech recognition grammars . we show how these features can be used in the construction of portable high-precision domain-specific speech translators .

sconeedit : a text-guided domain knowledge editor
we will demonstrate sconeedit , a new tool for exploring and editing knowledge bases ( kbs ) that leverages interaction with domain texts . the tool provides an annotated view of user-selected text , allowing a user to see which concepts from the text are in the kb and to edit the kb directly from this text view . alongside the text view , sconeedit provides a navigable kb view of the knowledge base , centered on concepts that appear in the text . this unified tool gives the user a text-driven way to explore a kb and add new knowledge .

using bilingual information for cross-language document
cross-language document summarization is defined as the task of producing a summary in a target language ( e.g . chinese ) for a set of documents in a source language ( e.g . english ) . existing methods for addressing this task make use of either the information from the original documents in the source language or the information from the translated documents in the target language . in this study , we propose to use the bilingual information from both the source and translated documents for this task . two summarization methods ( simfusion and corank ) are proposed to leverage the bilingual information in the graph-based ranking framework for cross-language summary extraction . experimental results on the duc2001 dataset with manually translated reference chinese summaries show the effectiveness of the proposed methods .

karolina owczarzak josef van genabith andy way
we present a method for evaluating the quality of machine translation ( mt ) output , using labelled dependencies produced by a lexical-functional grammar ( lfg ) parser . our dependencybased method , in contrast to most popular string-based evaluation metrics , does not unfairly penalize perfectly valid syntactic variations in the translation , and the addition of wordnet provides a way to accommodate lexical variation . in comparison with other metrics on 16,800 sentences of chinese-english newswire text , our method reaches high correlation with human scores .

computationally efficient m-estimation of log-linear structure models
we describe a new loss function , due to jeon and lin ( 2006 ) , for estimating structured log-linear models on arbitrary features . the loss function can be seen as a ( generative ) alternative to maximum likelihood estimation with an interesting information-theoretic interpretation , and it is statistically consistent . it is substantially faster than maximum ( conditional ) likelihood estimation of conditional random fields ( lafferty et al , 2001 ; an order of magnitude or more ) . we compare its performance and training time to an hmm , a crf , an memm , and pseudolikelihood on a shallow parsing task . these experiments help tease apart the contributions of rich features and discriminative training , which are shown to be more than additive .

classifying dialogue acts in one-on-one live chats su nam kim , lawrence cavedon and timothy baldwin
we explore the task of automatically classifying dialogue acts in 1-on-1 online chat forums , an increasingly popular means of providing customer service . in particular , we investigate the effectiveness of various features and machine learners for this task . while a simple bag-of-words approach provides a solid baseline , we find that adding information from dialogue structure and inter-utterance dependency provides some increase in performance ; learners that account for sequential dependencies ( crfs ) show the best performance . we report our results from testing using a corpus of chat dialogues derived from online shopping customer-feedback data .

a bayesian model for joint unsupervised induction
we propose a joint model for unsupervised induction of sentiment , aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model , we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level . we deviate from the traditional view of discourse , as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task ; consequently , the induced discourse relations play the role of opinion and aspect shifters . the quantitative analysis that we conducted indicated that the integration of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure .

a corpus-based analysis of geometric constraints on projective
this paper presents a corpus-based method for automatic evaluation of geometric constraints on projective prepositions . the method is used to find an appropriate model of geometric constraints for a twodimensional domain . two simple models are evaluated against the uses of projective prepositions in a corpus of natural language dialogues to find the best parameters of these models . both models cover more than 96 % of the data correctly . an extra treatment of negative uses of projective prepositions ( e.g . a is not above b ) improves both models getting close to full coverage .

aid is out there : looking for help from tweets during a large scale disaster
the 2011 great east japan earthquake caused a wide range of problems , and as countermeasures , many aid activities were carried out . many of these problems and aid activities were reported via twitter . however , most problem reports and corresponding aid messages were not successfully exchanged between victims and local governments or humanitarian organizations , overwhelmed by the vast amount of information . as a result , victims could not receive necessary aid and humanitarian organizations wasted resources on redundant efforts . in this paper , we propose a method for discovering matches between problem reports and aid messages . our system contributes to problem-solving in a large scale disaster situation by facilitating communication between victims and humanitarian organizations .

unsupervised learning of generalized names
we present an algorithm , nomen , for learning generalized names in text . examples of these are names of diseases and infectious agents , such as bacteria and viruses . these names exhibit certain properties that make their identi cation more complex than that of regular proper names . nomen uses a novel form of bootstrapping to grow sets of textual instances and of their contextual patterns . the algorithm makes use of competing evidence to boost the learning of several categories of names simultaneously . we present results of the algorithm on a large corpus . we also investigate the relative merits of several evaluation strategies .

addressing how-to questions using a spoken dialogue system : a viable approach
in this document , we illustrate how complex questions such as procedural ( how-to ) ones can be addressed in an interactive format by means of a spoken dialogue system . the advantages of interactivity and in particular of spoken dialogue with respect to standard question answering settings are numerous . first , addressing user needs that do not necessarily arise in front of a computer ; moreover , a spoken or multimodal answer format can often be better suited to the users need . finally , the procedural nature of the information itself makes iterative question formulation and answer production particularly appealing .

linear mixture models for robust machine translation multilingual text processing
as larger and more diverse parallel texts become available , how can we leverage heterogeneous data to train robust machine translation systems that achieve good translation quality on various test domains this challenge has been addressed so far by repurposing techniques developed for domain adaptation , such as linear mixture models which combine estimates learned on homogeneous subdomains . however , learning from large heterogeneous corpora is quite different from standard adaptation tasks with clear domain distinctions . in this paper , we show that linear mixture models can reliably improve translation quality in very heterogeneous training conditions , even if the mixtures do not use any domain knowledge and attempt to learn generic models rather than adapt them to the target domain . this surprising finding opens new perspectives for using mixture models in machine translation beyond clear cut domain adaptation tasks .

chunk-based verb reordering in vso sentences for arabic-english statistical machine translation
in arabic-to-english phrase-based statistical machine translation , a large number of syntactic disfluencies are due to wrong long-range reordering of the verb in vso sentences , where the verb is anticipated with respect to the english word order . in this paper , we propose a chunk-based reordering technique to automatically detect and displace clause-initial verbs in the arabic side of a word-aligned parallel corpus . this method is applied to preprocess the training data , and to collect statistics about verb movements . from this analysis , specific verb reordering lattices are then built on the test sentences before decoding them . the application of our reordering methods on the training and test sets results in consistent bleu score improvements on the nist-mt 2009 arabicenglish benchmark .

the benefits of errors : learning an ot grammar with a structured candidate set
we compare three recent proposals adding a topology to ot : mccarthys persistent ot , smolenskys ics and bros sa-ot . to test their learnability , constraint rankings are learnt from sa-ots output . the errors in the output , being more than mere noise , follow from the topology . thus , the learner has to reconstructs her competence having access only to the teachers performance .

incremental joint extraction of entity mentions and relations
we present an incremental joint framework to simultaneously extract entity mentions and relations using structured perceptron with efficient beam-search . a segment-based decoder based on the idea of semi-markov chain is adopted to the new framework as opposed to traditional token-based tagging . in addition , by virtue of the inexact search , we developed a number of new and effective global features as soft constraints to capture the interdependency among entity mentions and relations . experiments on automatic content extraction ( ace ) corpora demonstrate that our joint model significantly outperforms a strong pipelined baseline , which attains better performance than the best-reported end-to-end system .

automating analysis of social media communication
a growing body of research analyzes the linguistic and discourse properties of communication in online social media . most of the analysis , especially at the discourse level , is done manually by human researchers . this talk explores how the findings and techniques of computer-mediated discourse analysis ( cmda ) , a paradigm i have been developing and teaching for 18 years , can inform computational approaches to communication in social media . i start by reviewing established automation approaches , which mainly focus on structural linguistic phenomena , and emergent approaches , such as machine learning models that identify semantically- and pragmatically-richer phenomena , through the lens of cmda , pointing out the strengths and limitations of each . the basic problem is that patterns in the discourse of social media users can be identified by humans that do not appear to lend themselves to reliable automated identification using existing approaches . to begin to address this problem , i draw on examples of recent work on twitter , wikipedia , and web-based discussion forums to suggest an approach that synthesizes linguistically-informed manual analysis and existing automated techniques . i consider how such an approach could scale up , while still making use of human analysts , and i identify a number of real-world problems that automated cmda could help address .

amparo elizabeth cano basave
latent topics derived by topic models such as latent dirichlet allocation ( lda ) are the result of hidden thematic structures which provide further insights into the data . the automatic labelling of such topics derived from social media poses however new challenges since topics may characterise novel events happening in the real world . existing automatic topic labelling approaches which depend on external knowledge sources become less applicable here since relevant articles/concepts of the extracted topics may not exist in external sources . in this paper we propose to address the problem of automatic labelling of latent topics learned from twitter as a summarisation problem . we introduce a framework which apply summarisation algorithms to generate topic labels . these algorithms are independent of external sources and only rely on the identification of dominant terms in documents related to the latent topic . we compare the efficiency of existing state of the art summarisation algorithms . our results suggest that summarisation algorithms generate better topic labels which capture event-related context compared to the top-n terms returned by lda .

generating textual summaries of bar charts
information graphics , such as bar charts and line graphs , play an important role in multimodal documents . this paper presents a novel approach to producing a brief textual summary of a simple bar chart . it outlines our approach to augmenting the core message of the graphic to produce a brief summary . our method simultaneously constructs both the discourse and sentence structures of the textual summary using a bottom-up approach . the result is then realized in natural language . an evaluation study validates our generation methodology .

authorship attribution and verification with many authors and limited
most studies in statistical or machine learning based authorship attribution focus on two or a few authors . this leads to an overestimation of the importance of the features extracted from the training data and found to be discriminating for these small sets of authors . most studies also use sizes of training data that are unrealistic for situations in which stylometry is applied ( e.g. , forensics ) , and thereby overestimate the accuracy of their approach in these situations . a more realistic interpretation of the task is as an authorship verification problem that we approximate by pooling data from many different authors as negative examples . in this paper , we show , on the basis of a new corpus with 145 authors , what the effect is of many authors on feature selection and learning , and show robustness of a memory-based learning approach in doing authorship attribution and verification with many authors and limited training data when compared to eager learning methods such as svms and maximum entropy learning .

part-of-speech tagging using virtual evidence and negative training
we present a part-of-speech tagger which introduces two new concepts : virtual evidence in the form of an observed child node , and negative training data to learn the conditional probabilities for the observed child . associated with each word is a flexible feature-set which can include binary flags , neighboring words , etc . the conditional probability of tag given word + features is implemented using a factored language-model with back-off to avoid data sparsity problems . this model remains within the framework of dynamic bayesian networks ( dbns ) and is conditionally-structured , but resolves the label bias problem inherent in the conditional markov model ( cmm ) .

discriminative reordering with chinese grammatical relations features
the prevalence in chinese of grammatical structures that translate into english in different word orders is an important cause of translation difficulty . while previous work has used phrase-structure parses to deal with such ordering problems , we introduce a richer set of chinese grammatical relations that describes more semantically abstract relations between words . using these chinese grammatical relations , we improve a phrase orientation classifier ( introduced by zens and ney ( 2006 ) ) that decides the ordering of two phrases when translated into english by adding path features designed over the chinese typed dependencies . we then apply the log probability of the phrase orientation classifier as an extra feature in a phrase-based mt system , and get significant bleu point gains on three test sets : mt02 ( +0.59 ) , mt03 ( +1.00 ) and mt05 ( +0.77 ) . our chinese grammatical relations are also likely to be useful for other nlp tasks .

speech recognition grammar compilation in grammatical framework
this paper describes how grammar-based language models for speech recognition systems can be generated from grammatical framework ( gf ) grammars . context-free grammars and finite-state models can be generated in several formats : gsl , srgs , jsgf , and htk slf . in addition , semantic interpretation code can be embedded in the generated context-free grammars . this enables rapid development of portable , multilingual and easily modifiable speech recognition applications .

example-based spoken dialogue system using woz system log yukiko yamaguchi yasuyoshi inagaki
this paper proposes a new framework for a spoken dialogue system based on dialogue examples between human subjects and the wizard of oz ( woz ) system . using this framework and a model of information retrieval dialogue , a spoken dialogue system for retrieving shop information while driving in a car has been designed . the system refers to the dialogue examples to find an example that is suitable for generating a query or a reply . the authors have also constructed a large-scale dialogue database using a woz system , which enables efficient collection of dialogue examples .

event detection and summarization in weblogs with temporal collocations
this paper deals with the relationship between weblog content and time . with the proposed temporal mutual information , we analyze the collocations in time dimension , and the interesting collocations related to special events . the temporal mutual information is employed to observe the strength of term-to-term associations over time . an event detection algorithm identifies the collocations that may cause an event in a specific timestamp . an event summarization algorithm retrieves a set of collocations which describe an event . we compare our approach with the approach without considering the time interval . the experimental results demonstrate that the temporal collocations capture the real world semantics and real world events over time .

the types and distributions of errors in a wide coverage surface realizer evaluation
recent empirical experiments on surface realizers have shown that grammars for generation can be effectively evaluated using large corpora . evaluation metrics are usually reported as single averages across all possible types of errors and syntactic forms . but the causes of these errors are diverse , and the extent to which the accuracy of generation over individual syntactic phenomena is unknown . this article explores the types of errors , both computational and linguistic , inherent in the evaluation of a surface realizer when using large corpora . we analyze data from an earlier wide coverage experiment on the fuf/surge surface realizer with the penn treebank in order to empirically classify the sources of errors and describe their frequency and distribution . this both provides a baseline for future evaluations and allows designers of nlg applications needing off-the-shelf surface realizers to choose on a quantitative basis .

reducing grounded learning tasks to grammatical inference
it is often assumed that grounded learning tasks are beyond the scope of grammatical inference techniques . in this paper , we show that the grounded task of learning a semantic parser from ambiguous training data as discussed in kim and mooney ( 2010 ) can be reduced to a probabilistic context-free grammar learning task in a way that gives state of the art results . we further show that additionally letting our model learn the languages canonical word order improves its performance and leads to the highest semantic parsing f-scores previously reported in the literature.1

anatomy of annotation schemes : mapping to graf
in this paper , we apply the annotation scheme design methodology defined in ( bunt , 2010 ) and demonstrate its use for generating a mapping from an existing annotation scheme to a representation in graf format . the most important features of this methodology are ( 1 ) the distinction of the abstract and concrete syntax of an annotation language ; ( 2 ) the specification of a formal semantics for the abstract syntax ; and ( 3 ) the formalization of the relation between abstract and concrete syntax , which guarantees that any concrete syntax inherits the semantics of the abstract syntax , and thus guarantees meaning-preserving mappings between representation formats . by way of illustration , we apply this mapping strategy to annotations from isotimeml , propbank , and framenet .

annotating attribution in the penn discourse treebank
an emerging task in text understanding and generation is to categorize information as fact or opinion and to further attribute it to the appropriate source . corpus annotation schemes aim to encode such distinctions for nlp applications concerned with such tasks , such as information extraction , question answering , summarization , and generation . we describe an annotation scheme for marking the attribution of abstract objects such as propositions , facts and eventualities associated with discourse relations and their arguments annotated in the penn discourse treebank . the scheme aims to capture the source and degrees of factuality of the abstract objects . key aspects of the scheme are annotation of the text spans signalling the attribution , and annotation of features recording the source , type , scopal polarity , and determinacy of attribution .

a web application for the diagnostic evaluation of machine translation over specific linguistic phenomena
this paper presents a web application and a web service for the diagnostic evaluation of machine translation ( mt ) . these web-based tools are built on top of delic4mt , an opensource software package that assesses the performance of mt systems over user-defined linguistic phenomena ( lexical , morphological , syntactic and semantic ) . the advantage of the web-based scenario is clear ; compared to the standalone tool , the user does not need to carry out any installation , configuration or maintenance of the tool .

paraphrase recognition via dissimilarity signicance classication
we propose a supervised , two-phase framework to address the problem of paraphrase recognition ( pr ) . unlike most pr systems that focus on sentence similarity , our framework detects dissimilarities between sentences and makes its paraphrase judgment based on the significance of such dissimilarities . the ability to differentiate significant dissimilarities not only reveals what makes two sentences a nonparaphrase , but also helps to recall additional paraphrases that contain extra but insignificant information . experimental results show that while being accurate at discerning non-paraphrasing dissimilarities , our implemented system is able to achieve higher paraphrase recall ( 93 % ) , at an overall performance comparable to the alternatives .

fast and simple semantic class assignment for biomedical text computational bioscience program computational bioscience program computational bioscience program computational bioscience program
a simple and accurate method for assigning broad semantic classes to text strings is presented . the method is to map text strings to terms in ontologies based on a pipeline of exact matches , normalized strings , headword matching , and stemming headwords . the results of three experiments evaluating the technique are given . five semantic classes are evaluated against the craft corpus of full-text journal articles . twenty semantic classes are evaluated against the corresponding full ontologies , i.e . by reflexive matching . one semantic class is evaluated against a structured test suite . precision , recall , and f-measure on the corpus when evaluating against only the ontologies in the corpus is micro-averaged 67.06/78.49/72.32 and macro-averaged 69.84/83.12/75.31 . accuracy on the corpus when evaluating against all twenty semantic classes ranges from 77.12 % to 95.73 % . reflexive matching is generally successful , but reveals a small number of errors in the implementation .

complexity assumptions in ontology verbalisation
we describe the strategy currently pursued for verbalising owl ontologies by sentences in controlled natural language ( i.e. , combining generic rules for realising logical patterns with ontology-specific lexicons for realising atomic terms for individuals , classes , and properties ) and argue that its success depends on assumptions about the complexity of terms and axioms in the ontology . we then show , through analysis of a corpus of ontologies , that although these assumptions could in principle be violated , they are overwhelmingly respected in practice by ontology developers .

automatic generation of semantic fields for annotating web images
the overwhelming amounts of multimedia contents have triggered the need for automatically detecting the semantic concepts within the media contents . with the development of photo sharing websites such as flickr , we are able to obtain millions of images with usersupplied tags . however , user tags tend to be noisy , ambiguous and incomplete . in order to improve the quality of tags to annotate web images , we propose an approach to build semantic fields for annotating the web images . the main idea is that the images are more likely to be relevant to a given concept , if several tags to the image belong to the same semantic field as the target concept . semantic fields are determined by a set of highly semantically associated terms with high tag co-occurrences in the image corpus and in different corpora and lexica such as wordnet and wikipedia . we conduct experiments on the nuswide web image corpus and demonstrate superior performance on image annotation as compared to the state-ofthe-art approaches .

wsd system based on specialized hidden markov model ( upv-shmm-eaw )
we present a supervised approach to word sense disambiguation ( wsd ) based on specialized hidden markov models . we used as training data the semcor corpus and the test data set provided by senseval 2 competition and as dictionary the wordnet 1.6. we evaluated our system on the english all-word task of the senseval-3 competition .

semi-supervised training of a kernel pca-based model for word sense disambiguation
in this paper , we introduce a new semi-supervised learning model for word sense disambiguation based on kernel principal component analysis ( kpca ) , with experiments showing that it can further improve accuracy over supervised kpca models that have achieved wsd accuracy superior to the best published individual models . although empirical results with supervised kpca models demonstrate significantly better accuracy compared to the state-of-the-art achieved by either nave bayes or maximum entropy models on senseval-2 data , we identify specific sparse data conditions under which supervised kpca models deteriorate to essentially a most-frequent-sense predictor . we discuss the potential of kpca for leveraging unannotated data for partially-unsupervised training to address these issues , leading to a composite model that combines both the supervised and semi-supervised models .

jmdict : a japanese-multilingual dictionary
the jmdict project has at its aim the compilation of a multilingual lexical database with japanese as the pivot language . using an xml structure designed to cater for a mix of languages and a rich set of lexicographic information , it has reached a size of approximately 100,000 entries , with most entries having translations in english , french and german . the compilation involves information re-use , with the french and german translations being drawn from separately maintained lexicons . material from other languages is also being included . the file is freely available for research purposes and for incorporation in dictionary application software , and is available in several www server systems .

hungarian corpus of light verb constructions
the precise identification of light verb constructions is crucial for the successful functioning of several nlp applications . in order to facilitate the development of an algorithm that is capable of recognizing them , a manually annotated corpus of light verb constructions has been built for hungarian . basic annotation guidelines and statistical data on the corpus are also presented in the paper . it is also shown how applications in the fields of machine translation and information extraction can make use of such a corpus and an algorithm .

source language categorization for improving a speech into sign language translation system
this paper describes a categorization module for improving the performance of a spanish into spanish sign language ( lse ) translation system . this categorization module replaces spanish words with associated tags . when implementing this module , several alternatives for dealing with non-relevant words have been studied . nonrelevant words are spanish words not relevant in the translation process . the categorization module has been incorporated into a phrase-based system and a statistical finite state transducer ( sfst ) . the evaluation results reveal that the bleu has increased from 69.11 % to 78.79 % for the phrase-based system and from 69.84 % to 75.59 % for the sfst .

cross-narrative temporal ordering of medical events
cross-narrative temporal ordering of medical events is essential to the task of generating a comprehensive timeline over a patients history . we address the problem of aligning multiple medical event sequences , corresponding to different clinical narratives , comparing the following approaches : ( 1 ) a novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding , and ( 2 ) dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms . the cross-narrative coreference and temporal relation weights used in both these approaches are learned from a corpus of clinical narratives . we present results using both approaches and observe that the finite state transducer approach performs performs significantly better than the dynamic programming one by 6.8 % for the problem of multiple-sequence alignment .

machine learning for coreference resolution : from local classification to global ranking
in this paper , we view coreference resolution as a problem of ranking candidate partitions generated by different coreference systems . we propose a set of partition-based features to learn a ranking model for distinguishing good and bad partitions . our approach compares favorably to two state-of-the-art coreference systems when evaluated on three standard coreference data sets .

melb-yb : preposition sense disambiguation using rich semantic
this paper describes a maxent-based preposition sense disambiguation system entry to the preposition sense disambiguation task of the semeval 2007. this system uses a wide variety of semantic and syntactic features to perform the disambiguation task and achieves a precision of 69.3 % over the test data .

a robust and extensible exemplar-based model of thematic fit
this paper presents a new , exemplar-based model of thematic fit . in contrast to previous models , it does not approximate thematic fit as argument plausibility or fit with verb selectional preferences , but directly as semantic role plausibility for a verb-argument pair , through similaritybased generalization from previously seen verb-argument pairs . this makes the model very robust for data sparsity . we argue that the model is easily extensible to a model of semantic role ambiguity resolution during online sentence comprehension . the model is evaluated on human semantic role plausibility judgments . its predictions correlate significantly with the human judgments . it rivals two state-of-theart models of thematic fit and exceeds their performance on previously unseen or lowfrequency items .

a generative constituent-context model for improved grammar induction
we present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts . parameter search with em produces higher quality analyses than previously exhibited by unsupervised systems , giving the best published unsupervised parsing results on the atis corpus . experiments on penn treebank sentences of comparable length show an even higher f1 of 71 % on nontrivial brackets . we compare distributionally induced and actual part-of-speech tags as input data , and examine extensions to the basic model . we discuss errors made by the system , compare the system to previous models , and discuss upper bounds , lower bounds , and stability for this task .

the peoples web meets linguistic knowledge : automatic sense alignment of wikipedia and wordnet
we propose a method to automatically alignwordnet synsets andwikipedia articles to obtain a sense inventory of higher coverage and quality . for eachwordnet synset , we first extract a set of wikipedia articles as alignment candidates ; in a second step , we determine which article ( if any ) is a valid alignment , i.e . is about the same sense or concept . in this paper , we go significantly beyond stateof-the-art word overlap approaches , and apply a threshold-based personalized pagerank method for the disambiguation step . we show that wordnet synsets can be aligned to wikipedia articles with a performance of up to 0.78 f1-measure based on a comprehensive , well-balanced reference dataset consisting of 1,815 manually annotated sense alignment candidates . the fully-aligned resource as well as the reference dataset is publicly available.1

study on romanian language
standard methods for part-of-speech tagging suffer from data sparseness when used on highly inflectional languages ( which require large lexical tagset inventories ) . for this reason , a number of alternative methods have been proposed over the years . one of the most successful methods used for this task , fdoohg7lhuhg7djjlqj 7xil , 1999 ) , exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions . a second phase is aimed at recovering the full set of morpho-syntactic features . in this paper we present an alternative method to tiered tagging , based on local optimizations with neural networks and we show how , by properly encoding the input sequence in a general neural network architecture , we achieve results similar to the tiered tagging methodology , significantly faster and without requiring extensive linguistic knowledge as implied by the previously mentioned method .

got you ! : automatic vandalism detection in wikipedia with web-based shallow syntactic-semantic modeling
discriminating vandalism edits from non-vandalism edits in wikipedia is a challenging task , as ill-intentioned edits can include a variety of content and be expressed in many different forms and styles . previous studies are limited to rule-based methods and learning based on lexical features , lacking in linguistic analysis . in this paper , we propose a novel web-based shallow syntacticsemantic modeling method , which utilizes web search results as resource and trains topic-specific n-tag and syntactic n-gram language models to detect vandalism . by combining basic task-specific and lexical features , we have achieved high f-measures using logistic boosting and logistic model trees classifiers , surpassing the results reported by major wikipedia vandalism detection systems .

a new yardstick and tool for personalized vocabulary building thomas k landauer kirill kireyev
the goal of this research is to increase the value of each individual student 's vocabulary by finding words that the student doesnt know , needs to , and is ready to learn . to help identify such words , a better model of how well any given word is expected to be known was created . this is accomplished by using a semantic language model , lsa , to track how every word changes with the addition of more and more text from an appropriate corpus . we define the maturity of a word as the degree to which it has become similar to that after training on the entire corpus . an individual students average vocabulary level can then be placed on the wordmaturity scale by an adaptive test . finally , the words that the student did or did not know on the test can be used to predict what other words the same student knows by using multiple maturity models trained on random samples of typical educational readings . this detailed information can be used to generate highly customized vocabulary teaching and testing exercises , such as cloze tests .

web and corpus methods for malay count classifier prediction
we examine the capacity of web and corpus frequency methods to predict preferred count classifiers for nouns in malay . the observed f-score for the web model of 0.671 considerably outperformed corpus-based frequency and machine learning models . we expect that this is a fruitful extension for webascorpus approaches to lexicons in languages other than english , but further research is required in other south-east and east asian languages .

japanese dependency parsing using co-occurrence information and a combination of case elements
in this paper , we present a method that improves japanese dependency parsing by using large-scale statistical information . it takes into account two kinds of information not considered in previous statistical ( machine learning based ) parsing methods : information about dependency relations among the case elements of a verb , and information about co-occurrence relations between a verb and its case element . this information can be collected from the results of automatic dependency parsing of large-scale corpora . the results of an experiment in which our method was used to rerank the results obtained using an existing machine learning based parsing method showed that our method can improve the accuracy of the results obtained using the existing method .

automatically generating annotator rationales to improve sentiment classification ainur yessenalina yejin choi claire cardie
one of the central challenges in sentimentbased text categorization is that not every portion of a document is equally informative for inferring the overall sentiment of the document . previous research has shown that enriching the sentiment labels with human annotators rationales can produce substantial improvements in categorization performance . we explore methods to automatically generate annotator rationales for document-level sentiment classification . rather unexpectedly , we find the automatically generated rationales just as helpful as human rationales .

a study on the semantic relatedness of query and document terms in
the use of lexical semantic knowledge in information retrieval has been a field of active study for a long time . collaborative knowledge bases like wikipedia and wiktionary , which have been applied in computational methods only recently , offer new possibilities to enhance information retrieval . in order to find the most beneficial way to employ these resources , we analyze the lexical semantic relations that hold among query and document terms and compare how these relations are represented by a measure for semantic relatedness . we explore the potential of different indicators of document relevance that are based on semantic relatedness and compare the characteristics and performance of the knowledge bases wikipedia , wiktionary and wordnet .

corpus representativeness for syntactic information acquisition
this paper refers to part of our research in the area of automatic acquisition of computational lexicon information from corpus . the present paper reports the ongoing research on corpus representativeness . for the task of inducing information out of text , we wanted to fix a certain degree of confidence on the size and composition of the collection of documents to be observed . the results show that it is possible to work with a relatively small corpus of texts if it is tuned to a particular domain . even more , it seems that a small tuned corpus will be more informative for real parsing than a general corpus .

an mdl-based approach to extracting subword units for and computer science
we address a key problem in grapheme-tophoneme conversion : the ambiguity in mapping grapheme units to phonemes . rather than using single letters and phonemes as units , we propose learning chunks , or subwords , to reduce ambiguity . this can be interpreted as learning a lexicon of subwords that has minimum description length . we implement an algorithm to build such a lexicon , as well as a simple decoder that uses these subwords .

automatic evaluation of translation quality for distant language pairs
automatic evaluation of machine translation ( mt ) quality is essential to developing highquality mt systems . various evaluation metrics have been proposed , and bleu is now used as the de facto standard metric . however , when we consider translation between distant language pairs such as japanese and english , most popular metrics ( e.g. , bleu , nist , per , and ter ) do not work well . it is well known that japanese and english have completely different word orders , and special care must be paid to word order in translation . otherwise , translations with wrong word order often lead to misunderstanding and incomprehensibility . for instance , smt-based japanese-to-english translators tend to translate a because b as b because a. thus , word order is the most important problem for distant language translation . however , conventional evaluation metrics do not significantly penalize such word order mistakes . therefore , locally optimizing these metrics leads to inadequate translations . in this paper , we propose an automatic evaluation metric based on rank correlation coefficients modified with precision . our meta-evaluation of the ntcir-7 patmt je task data shows that this metric outperforms conventional metrics .

unsupervised language model adaptation incorporating named entity information
language model ( lm ) adaptation is important for both speech and language processing . it is often achieved by combining a generic lm with a topic-specific model that is more relevant to the target document . unlike previous work on unsupervised lm adaptation , this paper investigates how effectively using named entity ( ne ) information , instead of considering all the words , helps lm adaptation . we evaluate two latent topic analysis approaches in this paper , namely , clustering and latent dirichlet allocation ( lda ) . in addition , a new dynamically adapted weighting scheme for topic mixture models is proposed based on lda topic analysis . our experimental results show that the ne-driven lm adaptation framework outperforms the baseline generic lm . the best result is obtained using the lda-based approach by expanding the named entities with syntactically filtered words , together with using a large number of topics , which yields a perplexity reduction of 14.23 % compared to the baseline generic lm .

quality estimation for machine translation using the joint method of evaluation criteria and statistical modeling
this paper is to introduce our participation in the wmt13 shared tasks on quality estimation for machine translation without using reference translations . we submitted the results for task 1.1 ( sentence-level quality estimation ) , task 1.2 ( system selection ) and task 2 ( word-level quality estimation ) . in task 1.1 , we used an enhanced version of bleu metric without using reference translations to evaluate the translation quality . in task 1.2 , we utilized a probability model nave bayes ( nb ) as a classification algorithm with the features borrowed from the traditional evaluation metrics . in task 2 , to take the contextual information into account , we employed a discriminative undirected probabilistic graphical model conditional random field ( crf ) , in addition to the nb algorithm . the training experiments on the past wmt corpora showed that the designed methods of this paper yielded promising results especially the statistical models of crf and nb . the official results show that our crf model achieved the highest f-score 0.8297 in binary classification of task 2 .

a quantitative view of feedback lexical markers in conversational french laurent prevot brigitte bigi
this paper presents a quantitative description of the lexical items used for linguistic feedback in the corpus of interactional data ( cid ) . the paper includes the raw figures for feedback lexical item as well as more detailed figures concerning interindividual variability . this effort is a first step before a broader analysis including more discourse situations and featuring communicative function annotation . index terms : feedback , backchannel , corpus , french language 1 objectives conversational feedback is mostly performed through short utterances such as yeah , mh , okay not produced by the main speaker but by one of the other participants of a conversation . such utterances are among the most frequent in conversational data ( stolcke et al , 2000 ) . they also have been described in psycho-linguistic models of communication as a crucial communicative tool for achieving coordination or alignment in dialogue ( clark , 1996 ) . the general objective of the project ( anr cofee : conversational feedback ) 1 ( prevot and bertrand , 2012 ) in which this work takes place is to propose a fine grained model of the form/function relationship concerning feedback behaviors in conversation . the present study is first exploration aiming at knowing better the distribution of these items in one of our corpus . more precisely , we would to verify how much interindividual variability we will face in further study and whether we can identify a structure in this variability ( e.g speaker profiles ) . second , we tried 1see the project website : http : //cofee.hypotheses.org to check there some strong trends in terms of evolution of use of these items in the course of the conversation .

disambiguating temporalcontrastive discourse connectives for machine
temporalcontrastive discourse connectives ( although , while , since , etc . ) signal various types of relations between clauses such as temporal , contrast , concession and cause . they are often ambiguous and therefore difficult to translate from one language to another . we discuss several new and translation-oriented experiments for the disambiguation of a specific subset of discourse connectives in order to correct some of the translation errors made by current statistical machine translation systems .

an intermediate representation for the interpretation of temporal expressions
the interpretation of temporal expressions in text is an important constituent task for many practical natural language processing tasks , including question-answering , information extraction and text summarisation . although temporal expressions have long been studied in the research literature , it is only more recently , with the impetus provided by exercises like the ace program , that attention has been directed to broad-coverage , implemented systems . in this paper , we describe our approach to intermediate semantic representations in the interpretation of temporal expressions .

efficient parsing for head-split dependency trees
head splitting techniques have been successfully exploited to improve the asymptotic runtime of parsing algorithms for projective dependency trees , under the arc-factored model . in this article we extend these techniques to a class of non-projective dependency trees , called well-nested dependency trees with block-degree at most 2 , which has been previously investigated in the literature . we define a structural property that allows head splitting for these trees , and present two algorithms that improve over the runtime of existing algorithms at no significant loss in coverage .

quantitative analysis of treebanks using frequent subtree mining methods centrum voor computerlingustiek , ku leuven
the first task of statistical computational linguistics , or any other type of datadriven processing of language , is the extraction of counts and distributions of phenomena . this is much more difficult for the type of complex structured data found in treebanks and in corpora with sophisticated annotation than for tokenized texts . recent developments in data mining , particularly in the extraction of frequent subtrees from treebanks , offer some solutions . we have applied a modified version of the treeminer algorithm to a small treebank and present some promising results .

query expansion using domain information in compounds
this paper describes a query expansion strategy for domain specific information retrieval . components of compounds are used selectively . only parts belonging to the same domain as the compound itself will be used in expanded queries .

a comparison of techniques to automatically identify complex words
identifying complex words ( cws ) is an important , yet often overlooked , task within lexical simplification ( the process of automatically replacing cws with simpler alternatives ) . if too many words are identified then substitutions may be made erroneously , leading to a loss of meaning . if too few words are identified then those which impede a users understanding may be missed , resulting in a complex final text . this paper addresses the task of evaluating different methods for cw identification . a corpus of sentences with annotated cws is mined from simple wikipedia edit histories , which is then used as the basis for several experiments . firstly , the corpus design is explained and the results of the validation experiments using human judges are reported . experiments are carried out into the cw identification techniques of : simplifying everything , frequency thresholding and training a support vector machine . these are based upon previous approaches to the task and show that thresholding does not perform significantly differently to the more nave technique of simplifying everything . the support vector machine achieves a slight increase in precision over the other two methods , but at the cost of a dramatic trade off in recall .

a self-learning agent for exchanging pop trivia
this paper describes a self-learning software agent who collects and learns knowledge from the web and also exchanges her knowledge via dialogues with the users . the agent is built on top of information extraction , web mining , question answering and dialogue system technologies , and users can freely formulate their questions within the gossip domain and obtain the answers in multiple ways : textual response , graph-based visualization of the related concepts and speech output .

implicitly supervised language model adaptation for meeting
we describe the use of meeting metadata , acquired using a computerized meeting organization and note-taking system , to improve automatic transcription of meetings . by applying a two-step language model adaptation process based on notes and agenda items , we were able to reduce perplexity by 9 % and word error rate by 4 % relative on a set of ten meetings recorded in-house . this approach can be used to leverage other types of metadata .

assessing interpretable , attribute-related meaning representations for adjective-noun phrases in a similarity prediction task
we present a distributional vector space model that incorporates latent dirichlet allocation in order to capture the semantic relation holding between adjectives and nouns along interpretable dimensions of meaning : the meaning of adjective-noun phrases is characterized in terms of ontological attributes that are prominent in their compositional semantics . the model is evaluated in a similarity prediction task based on paired adjective-noun phrases from the mitchell and lapata ( 2010 ) benchmark data . comparing our model against a high-dimensional latent word space , we observe qualitative differences that shed light on different aspects of similarity conveyed by both models and suggest integrating their complementary strengths .

context and learning in novelty detection
we demonstrate the value of using context in a new-information detection system that achieved the highest precision scores at the text retrieval conferences novelty track in 2004. in order to determine whether information within a sentence has been seen in material read previously , our system integrates information about the context of the sentence with novel words and named entities within the sentence , and uses a specialized learning algorithm to tune the system parameters .

iterative scaling and coordinate descent methods for maximum entropy
maximum entropy ( maxent ) is useful in many areas . iterative scaling ( is ) methods are one of the most popular approaches to solve maxent . with many variants of is methods , it is difficult to understand them and see the differences . in this paper , we create a general and unified framework for is methods . this framework also connects is and coordinate descent ( cd ) methods . besides , we develop a cd method for maxent . results show that it is faster than existing iterative scaling methods .

mining clinical documents
early recognition of distinguishing patterns of a novel pandemic disease is important . we introduce a methodological approach based on popular data mining techniques to extract key features and temporal patterns of swine ( h1n1 ) flu that is discriminated from swine flu like symptoms .

natural logic for textual inference
this paper presents the first use of a computational model of natural logica system of logical inference which operates over natural languagefor textual inference . most current approaches to the pascal rte textual inference task achieve robustness by sacrificing semantic precision ; while broadly effective , they are easily confounded by ubiquitous inferences involving monotonicity . at the other extreme , systems which rely on first-order logic and theorem proving are precise , but excessively brittle . this work aims at a middle way . our system finds a low-cost edit sequence which transforms the premise into the hypothesis ; learns to classify entailment relations across atomic edits ; and composes atomic entailments into a top-level entailment judgment . we provide the first reported results for any system on the fracas test suite . we also evaluate on rte3 data , and show that hybridizing an existing rte system with our natural logic system yields significant performance gains .

using context vectors in improving a machine translation system with bridge language samira tofighi zahabi somayeh bakhshaei shahram khadivi
mapping phrases between languages as translation of each other by using an intermediate language ( pivot language ) may generate translation pairs that are wrong . since a word or a phrase has different meanings in different contexts , we should map source and target phrases in an intelligent way . we propose a pruning method based on the context vectors to remove those phrase pairs that connect to each other by a polysemous pivot phrase or by weak translations . we use context vectors to implicitly disambiguate the phrase senses and to recognize irrelevant phrase translation pairs . using the proposed method a relative improvement of 2.8 percent in terms of bleu score is achieved .

shared task system description : measuring the compositionality of bigrams using statistical methodologies
the measurement of relative compositionality of bigrams is crucial to identify multi-word expressions ( mwes ) in natural language processing ( nlp ) tasks . the article presents the experiments carried out as part of the participation in the shared task distributional semantics and compositionality ( disco ) organized as part of the disco workshop in aclhlt 2011. the experiments deal with various collocation based statistical approaches to compute the relative compositionality of three types of bigram phrases ( adjective-noun , verbsubject and verb-object combinations ) . the experimental results in terms of both fine-grained and coarse-grained compositionality scores have been evaluated with the human annotated gold standard data . reasonable results have been obtained in terms of average point difference and coarse precision .

identifying cognates by phonetic and semantic similarity
i present a method of identifying cognates in the vocabularies of related languages . i show that a measure of phonetic similarity based on multivalued features performs better than orthographic measures , such as the longest common subsequence ratio ( lcsr ) or dices coefficient . i introduce a procedure for estimating semantic similarity of glosses that employs keyword selection and wordnet . tests performed on vocabularies of four algonquian languages indicate that the method is capable of discovering on average nearly 75 % percent of cognates at 50 % precision .

grammatical error correction as multiclass classification with single model
this paper describes our system in the shared task of conll-2013 . we illustrate that grammatical error detection and correction can be transformed into a multiclass classification task and implemented as a single-model system regardless of various error types with the aid of maximum entropy modeling . our system achieves the f1 score of 17.13 % on the standard test set .

story tracking : linking similar news over time and across languages bruno pouliquen & ralf steinberger
the europe media monitor system ( emm ) gathers and aggregates an average of 50,000 newspaper articles per day in over 40 languages . to manage the information overflow , it was decided to group similar articles per day and per language into clusters and to link daily clusters over time into stories . a story automatically comes into existence when related groups of articles occur within a 7-day window . while cross-lingual links across 19 languages for individual news clusters have been displayed since 2004 as part of a freely accessible online application ( http : //press.jrc.it/newsexplorer ) , the newest development is work on linking entire stories across languages . the evaluation of the monolingual aggregation of historical clusters into stories and of the linking of stories across languages yielded mostly satisfying results .

improved smoothing for n-gram language models based on ordinary counts
kneser-ney ( 1995 ) smoothing and its variants are generally recognized as having the best perplexity of any known method for estimating n-gram language models . kneser-ney smoothing , however , requires nonstandard n-gram counts for the lowerorder models used to smooth the highestorder model . for some applications , this makes kneser-ney smoothing inappropriate or inconvenient . in this paper , we introduce a new smoothing method based on ordinary counts that outperforms all of the previous ordinary-count methods we have tested , with the new method eliminating most of the gap between kneser-ney and those methods .

investigating the relationship between word segmentation performance and retrieval performance in chinese ir
it is commonly believed that word segmentation accuracy is monotonically related to retrieval performance in chinese information retrieval . in this paper we show that , for chinese , the relationship between segmentation and retrieval performance is in fact nonmonotonic ; that is , at around 70 % word segmentation accuracy an over-segmentation phenomenon begins to occur which leads to a reduction in information retrieval performance . we demonstrate this effect by presenting an empirical investigation of information retrieval on chinese trec data , using a wide variety of word segmentation algorithms with word segmentation accuracies ranging from 44 % to 95 % . it appears that the main reason for the drop in retrieval performance is that correct compounds and collocations are preserved by accurate segmenters , while they are broken up by less accurate ( but reasonable ) segmenters , to a surprising advantage . this suggests that words themselves might be too broad a notion to conveniently capture the general semantic meaning of chinese text .

semantic relatedness from automatically generated semantic
we introduce a novel approach to measuring semantic relatedness of terms based on an automatically generated , large-scale semantic network . we present promising first results that indicate potential competitiveness with approaches based on manually created resources .

named entity recognition in wikipedia
named entity recognition ( ner ) is used in many domains beyond the newswire text that comprises current gold-standard corpora . recent work has used wikipedias link structure to automatically generate near gold-standard annotations . until now , these resources have only been evaluated on newswire corpora or themselves . we present the first ner evaluation on a wikipedia gold standard ( wg ) corpus . our analysis of cross-corpus performance on wg shows that wikipedia text may be a harder ner domain than newswire . we find that an automatic annotation of wikipedia has high agreement with wg and , when used as training data , outperforms newswire models by up to 7.7 % .

structured penalties for log-linear language models
language models can be formalized as loglinear regression models where the input features represent previously observed contexts up to a certain length m. the complexity of existing algorithms to learn the parameters by maximum likelihood scale linearly in nd , where n is the length of the training corpus and d is the number of observed features . we present a model that grows logarithmically in d , making it possible to efficiently leverage longer contexts . we account for the sequential structure of natural language using treestructured penalized objectives to avoid overfitting and achieve better generalization .

construction of an infrastructure for providing users with suitable language resources hitomi tohyama shunsuke kozawa kiyotaka uchimoto
our research organization has been constructing a large scale database named shachi by collecting detailed meta information on language resources ( lrs ) in asia and western countries . the metadata database contains more than 2,000 compiled lrs such as corpora , dictionaries , thesauruses and lexicons , forming a large scale metadata of lrs archive . its metadata , an extended version of olac metadata set conforming to dublin core , have been collected semi-automatically . this paper explains the design and the structure of the metadata database , as well as the realization of the catalogue search tool .

towards developing generation algorithms for text-to-text applications
we describe a new sentence realization framework for text-to-text applications . this framework uses idl-expressions as a representation formalism , and a generation mechanism based on algorithms for intersecting idl-expressions with probabilistic language models . we present both theoretical and empirical results concerning the correctness and efficiency of these algorithms .

fast and robust arabic error correction system
in this paper we describe the implementation of an arabic error correction system developed for the emnlp2014 shared task on automatic error correction for arabic text . we proposed a novel algorithm , where we find some correction rules and calculate their probability based on the training data , they we rank the correction rules , then we apply them on the text to maximize the overall fscore for the provided data . the system achieves and f-score of 0.6573 on the test data .

using maximum entropy models to discriminate between similar
dslrae is a hierarchical classifier for similar written languages and varieties based on maximum-entropy ( maxent ) classifiers . in the first level , the text is classified into a language group using a simple token-based maxent classifier . at the second level , a group-specific maxent classifier is applied to classify the text as one of the languages or varieties within the previously identified group . for each group of languages , the classifier uses a different kind and combination of knowledge-poor features : token or character n-grams and white lists of tokens . features were selected according to the results of applying ten-fold cross-validation over the training dataset . the system presented in this article 1 has been ranked second in the discriminating similar language ( dsl ) shared task co-located within the vardial workshop at coling 2014 ( zampieri et al. , 2014 ) .

segment predictability as a cue in word segmentation : application to
several computational simulations of how children solve the word segmentation problem have been proposed , but most have been applied only to a limited number of languages . one model with some experimental support uses distributional statistics of sound sequence predictability ( saffran et al 1996 ) . however , the experimental design does not fully specify how predictability is best measured or modeled in a simulation . saffran et al ( 1996 ) assume transitional probability , but brent ( 1999a ) claims mutual information ( mi ) is more appropriate . both assume predictability is measured locally , relative to neighboring segment-pairs . this paper replicates brents ( 1999a ) mutualinformation model on a corpus of childdirected speech in modern greek , and introduces a variant model using a global threshold . brents finding regarding the superiority of mi is confirmed ; the relative performance of local comparisons and global thresholds depends on the evaluation metric .

automatic set instance extraction using the web
an important and well-studied problem is the production of semantic lexicons from a large corpus . in this paper , we present a system named asia ( automatic set instance acquirer ) , which takes in the name of a semantic class as input ( e.g. , car makers ) and automatically outputs its instances ( e.g. , ford , nissan , toyota ) . asia is based on recent advances in webbased set expansion - the problem of finding all instances of a set given a small number of seed instances . this approach effectively exploits web resources and can be easily adapted to different languages . in brief , we use languagedependent hyponym patterns to find a noisy set of initial seeds , and then use a state-of-the-art language-independent set expansion system to expand these seeds . the proposed approach matches or outperforms prior systems on several englishlanguage benchmarks . it also shows excellent performance on three dozen additional benchmark problems from english , chinese and japanese , thus demonstrating language-independence .

hybrid text summarization : combining external relevance measures with structural analysis
in this paper , a novel linguistically advanced text summarization system is described for reducing the minimum size of highly readable variable sized summaries of digitized text documents produced by text summarization methods that use discourse analysis to rank sentences for inclusion in the final summary . the basic algorithm used in fxpals palsumm text summarization system combines text structure methods that preserve readability and correct reference resolution with statistical methods to reduce overall summary length while promoting the inclusion of important material .

eumssi : a platform for multimodal analysis and recommendation gesellschaft zur forderung der and media solutions
the eumssi project ( event understanding through multimodal social stream interpretation ) aims at developing technologies for aggregating data presented as unstructured information in sources of very different nature . the multimodal analytics will help organize , classify and cluster cross-media streams , by enriching its associated metadata in an interactive manner , so that the data resulting from analysing one media helps reinforce the aggregation of information from other media , in a cross-modal semantic representation framework . once all the available descriptive information has been collected , an interpretation component will dynamically reason over the semantic representation in order to derive implicit knowledge . finally the enriched information will be fed to a hybrid recommendation system , which will be at the basis of two well-motivated use-cases . in this paper we give a brief overview of eumssis main goals and how we are approaching its implementation using uima to integrate and combine various layers of annotations coming from different sources .

the nvi clustering evaluation measure
clustering is crucial for many nlp tasks and applications . however , evaluating the results of a clustering algorithm is hard . in this paper we focus on the evaluation setting in which a gold standard solution is available . we discuss two existing information theory based measures , v and vi , and show that they are both hard to use when comparing the performance of different algorithms and different datasets . the v measure favors solutions having a large number of clusters , while the range of scores given by vi depends on the size of the dataset . we present a new measure , nvi , which normalizes vi to address the latter problem . we demonstrate the superiority of nvi in a large experiment involving an important nlp application , grammar induction , using real corpus data in english , german and chinese .

parsing linear context-free rewriting systems
we describe four different parsing algorithms for linear context-free rewriting systems . the algorithms are described as deduction systems , and possible optimizations are discussed .

enunciative and modal variations in newswire texts in french : from guideline to automatic annotation
in this paper we present the development of a corpus of french newswire texts annotated with enunciative and modal commitment information . the annotation scheme we propose is based on the detection of predicative cues - referring to an enunciative and/or modal variation - and their scope at a sentence level . we describe how we have improved our annotation guideline by using the evaluation ( in terms of precision , recall and f-measure ) of a first round of annotation produced by two expert annotators and by our automatic annotation system .

phrase table training for precision and recall : what makes a good phrase and a good phrase pair yonggang deng , jia xu+ and yuqing gao
in this work , the problem of extracting phrase translation is formulated as an information retrieval process implemented with a log-linear model aiming for a balanced precision and recall . we present a generic phrase training algorithm which is parameterized with feature functions and can be optimized jointly with the translation engine to directly maximize the end-to-end system performance . multiple data-driven feature functions are proposed to capture the quality and confidence of phrases and phrase pairs . experimental results demonstrate consistent and significant improvement over the widely used method that is based on word alignment matrix only .

discovering latent structure in task-oriented dialogues
a key challenge for computational conversation models is to discover latent structure in task-oriented dialogue , since it provides a basis for analysing , evaluating , and building conversational systems . we propose three new unsupervised models to discover latent structures in task-oriented dialogues . our methods synthesize hidden markov models ( for underlying state ) and topic models ( to connect words to states ) . we apply them to two real , non-trivial datasets : human-computer spoken dialogues in bus query service , and humanhuman text-based chats from a live technical support service . we show that our models extract meaningful state representations and dialogue structures consistent with human annotations . quantitatively , we show our models achieve superior performance on held-out log likelihood evaluation and an ordering task .

word-based dialect identification with georeferenced rules
we present a novel approach for ( written ) dialect identification based on the discriminative potential of entire words . we generate swiss german dialect words from a standard german lexicon with the help of hand-crafted phonetic/graphemic rules that are associated with occurrence maps extracted from a linguistic atlas created through extensive empirical fieldwork . in comparison with a charactern-gram approach to dialect identification , our model is more robust to individual spelling differences , which are frequently encountered in non-standardized dialect writing . moreover , it covers the whole swiss german dialect continuum , which trained models struggle to achieve due to sparsity of training data .

decision detection using hierarchical graphical models
we investigate hierarchical graphical models ( hgms ) for automatically detecting decisions in multi-party discussions . several types of dialogue act ( da ) are distinguished on the basis of their roles in formulating decisions . hgms enable us to model dependencies between observed features of discussions , decision das , and subdialogues that result in a decision . for the task of detecting decision regions , an hgm classifier was found to outperform non-hierarchical graphical models and support vector machines , raising the f1-score to 0.80 from 0.55 .

limited-domain speech-to-speech translation
this paper describes a prototype system for near-real-time spontaneous , bidirectional translation between spoken english and pashto , a language presenting many technological challenges because of its lack of resources , including both data and expert knowledge . development of the prototype is ongoing , and we propose to demonstrate a fully functional version which shows the basic capabilities , though not yet their final depth and breadth .

discourse chunking and its application to sentence compression
in this paper we consider the problem of analysing sentence-level discourse structure . we introduce discourse chunking ( i.e. , the identification of intra-sentential nucleus and satellite spans ) as an alternative to full-scale discourse parsing . our experiments show that the proposed modelling approach yields results comparable to state-of-the-art while exploiting knowledge-lean features and small amounts of discourse annotations . we also demonstrate how discourse chunking can be successfully applied to a sentence compression task .

a structured vector space model for hidden attribute meaning in adjective-noun phrases
we present an approach to model hidden attributes in the compositional semantics of adjective-noun phrases in a distributional model . for the representation of adjective meanings , we reformulate the pattern-based approach for attribute learning of almuhareb ( 2006 ) in a structured vector space model ( vsm ) . this model is complemented by a structured vector space representing attribute dimensions of noun meanings . the combination of these representations along the lines of compositional semantic principles exposes the underlying semantic relations in adjective-noun phrases . we show that our compositional vsm outperforms simple pattern-based approaches by circumventing their inherent sparsity problems .

an effective discourse parser that uses rich linguistic information display advertising sciences
this paper presents a first-order logic learning approach to determine rhetorical relations between discourse segments . beyond linguistic cues and lexical information , our approach exploits compositional semantics and segment discourse structure data . we report a statistically significant improvement in classifying relations over attribute-value learning paradigms such as decision trees , ripper and naive bayes . for discourse parsing , our modified shift-reduce parsing model that uses our relation classifier significantly outperforms a right-branching majority-class baseline .

using generation for grammar analysis and error detection
we demonstrate that the bidirectionality of deep grammars , allowing them to generate as well as parse sentences , can be used to automatically and effectively identify errors in the grammars . the system is tested on two implemented hpsg grammars : jacy for japanese , and the erg for english . using this system , we were able to increase generation coverage in jacy by 18 % ( 45 % to 63 % ) with only four weeks of grammar development .

a tool for multi-word expression extraction in modern greek using syntactic parsing
this paper presents a tool for extracting multi-word expressions from corpora in modern greek , which is used together with a parallel concordancer to augment the lexicon of a rule-based machinetranslation system . the tool is part of a larger extraction system that relies , in turn , on a multilingual parser developed over the past decade in our laboratory . the paper reviews the various nlp modules and resources which enable the retrieval of greek multi-word expressions and their translations : the greek parser , its lexical database , the extraction and concordancing system .

extracting synchronous grammar rules from word-level alignments in linear time
we generalize uno and yagiuras algorithm for finding all common intervals of two permutations to the setting of two sequences with many-to-many alignment links across the two sides . we show how to maximally decompose a word-aligned sentence pair in linear time , which can be used to generate all possible phrase pairs or a synchronous context-free grammar ( scfg ) with the simplest rules possible . we also use the algorithm to precisely analyze the maximum scfg rule length needed to cover hand-aligned data from various language pairs .

bigram hmm with context distribution clustering for unsupervised chinese part-of-speech tagging
this paper presents an unsupervised chinese part-of-speech ( pos ) tagging model based on the first-order hmm . unlike the conventional hmm , the number of hidden states is not fixed and will be increased to fit the training data . in favor of sparse distribution , the dirichlet priors are introduced with variational inference method . to reduce the emission variables , words are represented by their contexts and clustered based on the distributional similarities between contexts . experiment results show the output state sequence of hmm are highly correlated to the latent annotations of gold pos tags , in context of clustering similarity measures . the other experiments on a real application , unsupervised dependency parsing , reveal that the output sequence can replace the manually annotated tags without loss of accuracies .

bypassed alignment graph for learning coordination in japanese
past work on english coordination has focused on coordination scope disambiguation . in japanese , detecting whether coordination exists in a sentence is also a problem , and the state-of-the-art alignmentbased method specialized for scope disambiguation does not perform well on japanese sentences . to take the detection of coordination into account , this paper introduces a bypass to the alignment graph used by this method , so as to explicitly represent the non-existence of coordinate structures in a sentence . we also present an effective feature decomposition scheme based on the distance between words in conjuncts .

tag , dynamic programming , and the perceptron for efficient , feature - rich parsing
we describe a parsing approach that makes use of the perceptron algorithm , in conjunction with dynamic programming methods , to recover full constituent-based parse trees . the formalism allows a rich set of parse-tree features , including pcfgbased features , bigram and trigram dependency features , and surface features . a severe challenge in applying such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved . we show that efficient training is feasible , using a tree adjoining grammar ( tag ) based parsing formalism . a lower-order dependency parsing model is used to restrict the search space of the full model , thereby making it efficient . experiments on the penn wsj treebank show that the model achieves state-of-the-art performance , for both constituent and dependency accuracy .

voice-rate : a dialog system for consumer ratings
voice-rate is an automated dialog system which provides access to over one million ratings of products and businesses . by calling a toll-free number , consumers can access ratings for products , national businesses such as airlines , and local businesses such as restaurants . voice-rate also has a facility for recording and analyzing ratings that are given over the phone . the service has been primed with ratings taken from a variety of web sources , and we are augmenting these with user ratings . voice-rate can be accessed by dialing 1-877-456-data .

robust sub-sentential alignment of phrase-structure trees
data-oriented translation ( dot ) , based on dataoriented parsing ( dop ) , is a language-independent mt engine which exploits parsed , aligned bitexts to produce very high quality translations . however , data acquisition constitutes a serious bottleneck as dot requires parsed sentences aligned at both sentential and sub-structural levels . manual substructural alignment is time-consuming , error-prone and requires considerable knowledge of both source and target languages and how they are related . automating this process is essential in order to carry out the large-scale translation experiments necessary to assess the full potential of dot . we present a novel algorithm which automatically induces sub-structural alignments between context-free phrase structure trees in a fast and consistent fashion requiring little or no knowledge of the language pair . we present results from a number of experiments which indicate that our method provides a serious alternative to manual alignment .

a preliminary look into the use of named entity information for bioscience text tokenization
tokenization in the bioscience domain is often difficult . new terms , technical terminology , and nonstandard orthography , all common in bioscience text , contribute to this difficulty . this paper will introduce the tasks of tokenization , normalization before introducing bacchant , a system built for bioscience text normalization . casting tokenization / normalization as a problem of punctuation classification motivates using machine learning methods in the implementation of this system . the evaluation of bacchant 's performance included error analysis of the system 's performance inside and outside of named entities ( nes ) from the genia corpus , which led to the creation of a normalization system trained solely on data from inside nes , bacchant-n. evaluation of this new system indicated that normalization systems trained on data inside nes perform better than systems trained both inside and outside nes , motivating a merging of tokenization and named entity tagging processes as opposed to the standard pipelining approach .

assisting translators in indirect lexical transfer
we present the design and evaluation of a translators amenuensis that uses comparable corpora to propose and rank nonliteral solutions to the translation of expressions from the general lexicon . using distributional similarity and bilingual dictionaries , the method outperforms established techniques for extracting translation equivalents from parallel corpora .

argumentation-relevant metaphors in test-taker essays
this article discusses metaphor annotation in a corpus of argumentative essays written by test-takers during a standardized examination for graduate school admission . the quality of argumentation being the focus of the project , we developed a metaphor annotation protocol that targets metaphors that are relevant for the writers arguments . the reliability of the protocol is =0.58 , on a set of 116 essays ( the total of about 30k content-word tokens ) . we found a moderate-to-strong correlation ( r=0.51-0.57 ) between the percentage of metaphorically used words in an essay and the writing quality score . we also describe encouraging findings regarding the potential of metaphor identification to contribute to automated scoring of essays .

learning to recognize features of valid textual entailments
this paper advocates a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment . current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text , using a locally decomposable matching score . we argue that there are significant weaknesses in this approach , including flawed assumptions of monotonicity and locality . instead we propose a pipelined approach where alignment is followed by a classification step , in which we extract features representing high-level characteristics of the entailment problem , and pass the resulting feature vector to a statistical classifier trained on development data . we report results on data from the 2005 pascal rte challenge which surpass previously reported results for alignment-based systems .

using paraphrases of deep semantic representions to support regression testing in spoken dialogue systems
rule-based spoken dialogue systems require a good regression testing framework if they are to be maintainable . we argue that there is a tension between two extreme positions when constructing the database of test examples . on the one hand , if the examples consist of input/output tuples representing many levels of internal processing , they are finegrained enough to catch most processing errors , but unstable under most system modifications . if the examples are pairs of user input and final system output , they are much more stable , but too coarse-grained to catch many errors . in either case , there are fairly severe difficulties in judging examples correctly . we claim that a good compromise can be reached by implementing a paraphrasing mechanism which maps internal semantic representations into surface forms , and carrying out regression testing using paraphrases of semantic forms rather than the semantic forms themselves . we describe an implementation of the idea using the open source regulus toolkit , where paraphrases are produced using regulus grammars compiled in generation mode . paraphrases can also be used at runtime to produce confirmations . by compiling the paraphrase grammar a second time , as a recogniser , it is possible in a simple and natural way to guarantee that confirmations are always within system coverage .

robust biomedical event extraction with dual decomposition and minimal sebastian riedel andrew mccallum
we present a joint model for biomedical event extraction and apply it to four tracks of the bionlp 2011 shared task . our model decomposes into three sub-models that concern ( a ) event triggers and outgoing arguments , ( b ) event triggers and incoming arguments and ( c ) protein-protein bindings . for efficient decoding we employ dual decomposition . our results are very competitive : with minimal adaptation of our model we come in second for two of the tasksright behind a version of the system presented here that includes predictions of the stanford event extractor as features . we also show that for the infectious diseases task using data from the genia track is a very effective way to improve accuracy .

unsupervised syntactic alignment with inversion transduction grammars
syntactic machine translation systems currently use word alignments to infer syntactic correspondences between the source and target languages . instead , we propose an unsupervised itg alignment model that directly aligns syntactic structures . our model aligns spans in a source sentence to nodes in a target parse tree . we show that our model produces syntactically consistent analyses where possible , while being robust in the face of syntactic divergence . alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline .

sense information for disambiguation : confluence of supervised and unsupervised methods
for senseval-2 , we disambiguated the lexical sample using two different sense inventories . official senseval-2 results were generated using wordnet , and separately using the new oxford dictionary of english ( node ) . since our initial submission , we have implemented additional routines and have now examined the differences in the features used for making sense selections . we report here the contribution of default sense selection , idiomatic usage , syntactic and semantic clues , subcategorization patterns , word forms , syntactic usage , context , selectional preferences , and topics or subject fields . we also compare the differences between wordnet and node . finally , we compare these features to those identified as significant in supervised learning approaches .

better binarization for the cky parsing
we present a study on how grammar binarization empirically affects the efficiency of the cky parsing . we argue that binarizations affect parsing efficiency primarily by affecting the number of incomplete constituents generated , and the effectiveness of binarization also depends on the nature of the input . we propose a novel binarization method utilizing rich information learnt from training corpus . experimental results not only show that different binarizations have great impacts on parsing efficiency , but also confirm that our learnt binarization outperforms other existing methods . furthermore we show that it is feasible to combine existing parsing speed-up techniques with our binarization to achieve even better performance .

answering why-questions in closed domains
in this paper we will present a system for question answering called getaruns , in its deep version applicable to closed domains , that is to say domains for which the lexical semantics is fully specified and does not have to be induced . in addition , no ontology is needed : semantic relations are derived from linguistic relations encoded in the syntax . the main tenet of the system is that it is possible to produce consistent semantic representations using a strict linguistic approach without resorting to extralinguistic knowledge sources . the paper will briefly present the low level component which is responsible for pronominal binding , quantifier raising and temporal interpretation . then it will discuss in more detail the high level component where a discourse model is created from text . the system has been evaluated on a wide variety of texts from closed domains , producing full and accurate parsing , semantics and anaphora resolution for all sentences .

co-parsing with competitive models
we present an asymmetric approach to a run-time combination of two parsers where one component serves as a predictor to the other one . predictions are integrated by means of weighted constraints and therefore are subject to preferential decisions . previously , the same architecture has been successfully used with predictors providing partial or inferior information about the parsing problem . it has now been applied to a situation where the predictor produces exactly the same type of information at a fully competitive quality level . results show that the combined system outperforms its individual components , even though their performance in isolation is already fairly high .

fast tweet retrieval with compact binary codes
the most widely used similarity measure in the field of natural language processing may be cosine similarity . however , in the context of twitter , the large scale of massive tweet data inevitably makes it expensive to perform cosine similarity computations among tremendous data samples . in this paper , we exploit binary coding to tackle the scalability issue , which compresses each data sample into a compact binary code and hence enables highly efficient similarity computations via hamming distances between the generated codes . in order to yield semantics sensitive binary codes for tweet data , we design a binarized matrix factorization model and further improve it in two aspects . first , we force the projection directions employed by the model nearly orthogonal to reduce the redundant information in their resulting binary bits . second , we leverage the tweets neighborhood information to encourage similar tweets to have adjacent binary codes . evaluated on a tweet dataset using hashtags to create gold labels in an information retrieval scenario , our proposed model shows significant performance gains over competing methods .

a unified account of the semantics of discourse particles
the paper investigates discourse particles on the example of german doch , assigning to them very specific semantic interpretations that still cover a wide range of their uses . the analysis highlights the role of discourse particles in managing the common ground and crucially takes into account that discourse particles can refer not only to utterances they are a part of and to previously uttered utterances , but also to felicity conditions of these utterances .

advanced dynamic programming in
dynamic programming ( dp ) is an important class of algorithms widely used in many areas of speech and language processing . recently there have been a series of work trying to formalize many instances of dp algorithms under algebraic and graph-theoretic frameworks . this tutorial surveys two such frameworks , namely semirings and directed hypergraphs , and draws connections between them . we formalize two particular types of dp algorithms under each of these frameworks : the viterbi-style topological algorithms and the dijkstra-style best-first algorithms . wherever relevant , we also discuss typical applications of these algorithms in natural language processing .

a comparative study for query translation using
in cross language information retrieval ( clir ) , query terms can be translated to the document language using bilingual dictionaries ( bds ) or statistical translation models ( stms ) . combining different translation resources can also be used to improve the performance . unfortunately , the most studies on combining multiple resources use simple methods such as linear combination . in this paper , we drew up a comparative study between linear combination and confidence measures to combine multiple translation resources for the purpose of clir . we show that the linear combination method is unable to combine correctly different types of resources such as bds and stms . while the confidence measure method is able to re-weight the translation candidate more radically than in linear combination . it reconsiders each translation candidate proposed by different resources with respect to additional features . we tested the two methods on different test clir collections and the results show that the confidence measure outperforms the linear combination method .

whats great and whats not : learning to classify the scope of negation for improved sentiment analysis
automatic detection of linguistic negation in free text is a critical need for many text processing applications , including sentiment analysis . this paper presents a negation detection system based on a conditional random field modeled using features from an english dependency parser . the scope of negation detection is limited to explicit rather than implied negations within single sentences . a new negation corpus is presented that was constructed for the domain of english product reviews obtained from the open web , and the proposed negation extraction system is evaluated against the reviews corpus as well as the standard bioscope negation corpus , achieving 80.0 % and 75.5 % f1 scores , respectively . the impact of accurate negation detection on a state-of-the-art sentiment analysis system is also reported .

speech-input multi-target machine translation
in order to simultaneously translate speech into multiple languages an extension of stochastic finite-state transducers is proposed . in this approach the speech translation model consists of a single network where acoustic models ( in the input ) and the multilingual model ( in the output ) are embedded . the multi-target model has been evaluated in a practical situation , and the results have been compared with those obtained using several mono-target models . experimental results show that the multi-target one requires less amount of memory . in addition , a single decoding is enough to get the speech translated into multiple languages .

calendar expressions in texts
temporal expressions that refer to a part of a calendar area in terms of common calendar divisions are studied . our claim is that such a calendar expression '' ( ce ) can be described by a succession of operators operating on a calendar base ( cb ) . these operators are categorized : a pointing operator that transform a cb into a ce ; a focalizing/shifting operator that reduces or shifts the ce into another ce , and finally a zoning operator that provides the wanted ce from this last ce . relying on these operators , a set of annotations is presented which are used to automatically annotate biographic texts . a software application , plugged in the platformnavitext , is described that builds a calendar view of a biographic text . 365 366 battistelli , couto , minel , and schwer

combining multiple models for speech information retrieval
in this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a speech information retrieval task . the formulas for combining the models are tuned on training data . then the system is evaluated on test data . the task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . also , the topics are real information needs , difficult to satisfy . information retrieval systems are not able to obtain good results on this data set , except for the case when manual summaries are included .

issue framing as a generalizable phenomenon
framingportraying an issue from one perspective to the necessary exclusion of alternative perspectivesis a central concept in political communication . it is also a powerful political tool , as evidenced through experiments and single-issue studies beyond the lab . yet compared to its significance , we know very little about framing as a generalizable phenomenon . do framing dynamics , such as the evolution of one frame into another , play out the same way for all issues under what conditions does framing influence public opinion and policy understanding the general patterns of framing dynamics and effects is thus hugely important . it is also a serious challenge , thanks to the volume of text data , the dynamic nature of language , and variance in applicable frames across issues ( e.g. , the innocence frame of the death penalty debate is irrelevant for discussing smoking bans ) . to address this challenge , i describe a collaborative project with justin gross , philip resnik , and noah smith . we advance a unified policy frames codebook , in which issue-specific frames ( e.g. , innocence ) are nested within high-level categories of frames ( e.g. , fairness ) that cross cut issues . through manual annotation bolstered by supervised learning , we can track the relative use of different frame cues within a given issue over time and in an apples-to-apples way across issues . preliminary findings suggest our work may help unlock the black box of framing , pointing to generalizable conditions under which we should expect to see different types of framing dynamics and framing effects . 71

towards nlg for physiological data monitoring with body area networks hadi banaee , mobyen uddin ahmed and amy loutfi
this position paper presents an on-going work on a natural language generation framework that is particularly tailored for summary text generation from body area networks . we present an overview of the main challenges when considering this type of sensor devices used for at home monitoring of health parameters . this paper describes the first steps towards the implementation of a system which collects information from heart rate and respiration rate using a wearable sensor . the paper further outlines the direction for future work and in particular the challenges for nlg in this application domain .

a practical text summarizer by paragraph extraction for thai
in this paper , we propose a practical approach for extracting the most relevant paragraphs from the original document to form a summary for thai text . the idea of our approach is to exploit both the local and global properties of paragraphs . the local property can be considered as clusters of significant words within each paragraph , while the global property can be though of as relations of all paragraphs in a document . these two properties are combined for ranking and extracting summaries . experimental results on real-world data sets are encouraging .

you are what you say : using meeting participants speech to detect their roles and expertise
our goal is to automatically detect the functional roles that meeting participants play , as well as the expertise they bring to meetings . to perform this task , we build decision tree classifiers that use a combination of simple speech features ( speech lengths and spoken keywords ) extracted from the participants speech in meetings . we show that this algorithm results in a role detection accuracy of 83 % on unseen test data , where the random baseline is 33.3 % . we also introduce a simple aggregation mechanism that combines evidence of the participants expertise from multiple meetings . we show that this aggregation mechanism improves the role detection accuracy from 66.7 % ( when aggregating over a single meeting ) to 83 % ( when aggregating over 5 meetings ) .

johanka spoustova miroslav spousta
as the title suggests , our paper deals with web discussion fora , whose content can be considered to be a special type of comparable corpora . we discuss the potential of this vast amount of data available now on the world wide web nearly for every language , regarding both general and common topics as well as the most obscure and specific ones . to illustrate our ideas , we propose a case study of seven wedding discussion fora in five languages .

a subcategorization acquisition system for french verbs
this paper presents a system capable of automatically acquiring subcategorization frames ( scfs ) for french verbs from the analysis of large corpora . we applied the system to a large newspaper corpus ( consisting of 10 years of the french newspaper le monde ) and acquired subcategorization information for 3267 verbs . the system learned 286 scf types for these verbs . from the analysis of 25 representative verbs , we obtained 0.82 precision , 0.59 recall and 0.69 f-measure . these results are comparable with those reported in recent related work .

an svm based voting algorithm with application to parse reranking
this paper introduces a novel support vector machines ( svms ) based voting algorithm for reranking , which provides a way to solve the sequential models indirectly . we have presented a risk formulation under the pac framework for this voting algorithm . we have applied this algorithm to the parse reranking problem , and achieved labeled recall and precision of 89.4 % /89.8 % on wsj section 23 of penn treebank .

towards the orwellian nightmare separation of business and personal emails
this paper describes the largest scale annotation project involving the enron email corpus to date . over 12,500 emails were classified , by humans , into the categories business and personal , and then subcategorised by type within these categories . the paper quantifies how well humans perform on this task ( evaluated by inter-annotator agreement ) . it presents the problems experienced with the separation of these language types . as a final section , the paper presents preliminary results using a machine to perform this classification task .

a user study : technology to increase teachers linguistic awareness to improve instructional language support for english language learners educational testing service
this paper discusses user study outcomes with teachers who used language musesm a webbased teacher professional development ( tpd ) application designed to enhance teachers linguistic awareness , and support teachers in the development of language-based instructional scaffolding ( support ) for their english language learners ( ell ) . system development was grounded in literature that supports the notion that instruction incorporating language support for ells can improve their accessibility to content-area classroom texts in terms of access to content , and improvement of language skills . measurement outcomes of user piloting with teachers in a tpd setting indicated that application use increased teachers ' linguistic knowledge and awareness , and their ability to develop appropriate language-based instruction for ells . instruction developed during the pilot was informed by the applications linguistic analysis feedback , provided by natural language processing capabilities in language muse .

four student nlp projects for low-resource languages
this paper describes a local effort to bridge the gap between computational and documentary linguistics by teaching students and young researchers in computational linguistics about doing research and developing systems for low-resource languages . we describe four student software projects developed within one semester . the projects range from a front-end for building small-vocabulary speech recognition systems , to a broad-coverage ( more than 1000 languages ) language identification system , to language-specific systems : a lemmatizer for the mayan language uspanteko and named entity recognition systems for both slovak and persian . teaching efforts such as these are an excellent way to develop not only tools for low-resource languages , but also computational linguists well-equipped to work on endangered and low-resource languages .

learning computational grammars
this paper reports on the learning computational grammars ( lcg ) project , a postdoc network devoted to studying the application of machine learning techniques to grammars suitable for computational use . we were interested in a more systematic survey to understand the relevance of many factors to the success of learning , esp . the availability of annotated data , the kind of dependencies in the data , and the availability of knowledge bases ( grammars ) . we focused on syntax , esp . noun phrase ( np ) syntax .

discriminative substring decoding for transliteration
we present a discriminative substring decoder for transliteration . this decoder extends recent approaches for discriminative character transduction by allowing for a list of known target-language words , an important resource for transliteration . our approach improves upon sherif and kondraks ( 2007b ) state-of-theart decoder , creating a 28.5 % relative improvement in transliteration accuracy on a japanese katakana-to-english task . we also conduct a controlled comparison of two feature paradigms for discriminative training : indicators and hybrid generative features . surprisingly , the generative hybrid outperforms its purely discriminative counterpart , despite losing access to rich source-context features . finally , we show that machine transliterations have a positive impact on machine translation quality , improving human judgments by 0.5 on a 4-point scale .

errgrams a way to improving asr for highly inflected dravidian languages
in this paper , we present results of our experiments with asr for a highly inflected dravidian language , telugu . first , we propose a new metric for evaluating asr performance for inflectional languages ( inflectional word error rate iwer ) which takes into account whether the incorrectly recognized word corresponds to the same lexicon lemma or not . we also present results achieved by applying a novel method errgrams to asr lattice . with respect to confidence scores , the method tries to learn typical error patterns , which are then used for lattice correction , and applied just before standard lattice rescoring . our confidence measures are based on word posteriors and were improved by applying antimodels trained on anti-examples generated by the standard n-gram language model . for telugu language , we decreased the wer from 45.2 % to 40.4 % ( by 4.8 % absolute ) , and the iwer from 41.6 % to 39.5 % ( 2.1 % absolute ) , with respect to the baseline performance . all improvements are statistically significant using all three standard nist significance tests for asr .

linguistically annotated btg for statistical machine translation human language technology
bracketing transduction grammar ( btg ) is a natural choice for effective integration of desired linguistic knowledge into statistical machine translation ( smt ) . in this paper , we propose a linguistically annotated btg ( labtg ) for smt . it conveys linguistic knowledge of source-side syntax structures to btg hierarchical structures through linguistic annotation . from the linguistically annotated data , we learn annotated btg rules and train linguistically motivated phrase translation model and reordering model . we also present an annotation algorithm that captures syntactic information for btg nodes . the experiments show that the labtg approach significantly outperforms a baseline btgbased system and a state-of-the-art phrasebased system on the nistmt-05 chineseto-english translation task . moreover , we empirically demonstrate that the proposed method achieves better translation selection and phrase reordering .

engineering linguistic creativity : bird flight and jet planes
man achieved flight by studying how birds fly , and yet the solution that engineers came up with ( jet planes ) is very different from the one birds apply . in this paper i review a number of efforts in automated story telling and poetry generation , identifying which human abilities are being modelled in each case . in an analogy to the classic example of bird-flight and jet planes , i explore how the computational models relate to ( the little we know about ) human performance , what the similarities are between the case for linguistic creativity and the case for flight , and what the analogy might have to say about artificial linguistic creativity if it were valid .

learning phrase boundaries for hierarchical phrase-based translation zhongjun he yao meng hao yu
hierarchical phrase-based models provide a powerful mechanism to capture non-local phrase reorderings for statistical machine translation ( smt ) . however , many phrase reorderings are arbitrary because the models are weak on determining phrase boundaries for patternmatching . this paper presents a novel approach to learn phrase boundaries directly from word-aligned corpus without using any syntactical information . we use phrase boundaries , which indicate the beginning/ending of phrase reordering , as soft constraints for decoding . experimental results and analysis show that the approach yields significant improvements over the baseline on large-scale chineseto-english translation .

an indexing scheme for typed feature structures
this paper describes an indexing substrate for typed feature structures ( istfs ) , which is an efficient retrieval engine for typed feature structures . given a set of typed feature structures , the istfs efficiently retrieves its subset whose elements are unifiable or in a subsumption relation with a query feature structure . the efficiency of the istfs is achieved by calculating a unifiability checking table prior to retrieval and finding the best index paths dynamically .

the relevance of collocations for parsing
although multiword expressions ( mwes ) have received an increasing amount of attention in the nlp community over the last two decades , few papers have been dedicated to the specific problem of the interaction between mwes and parsing . in this paper , we will discuss how the collocation identification task has been integrated in our rulebased parser and show how collocation knowledge has a positive impact on the parsing process . a manual evaluation has been conducted over a corpus of 4000 sentences , comparing outputs of the parser used with and without the collocation component . results of the evaluation clearly support our claim .

using tactical nlg to induce affective states : empirical investigations
this paper reports on attempts at aberdeen1 to measure the effects on readers emotions of positively and negatively slanted texts with the same basic message . the slanting methods could be implemented in an ( nlg ) system . we discuss a number of possible reasons why the studies were unable to show clear , statistically significant differences between the effects of the different texts .

using linguistically motivated features for paragraph boundary identification
in this paper we propose a machinelearning approach to paragraph boundary identification which utilizes linguistically motivated features . we investigate the relation between paragraph boundaries and discourse cues , pronominalization and information structure . we test our algorithm on german data and report improvements over three baselines including a reimplementation of sporleder & lapatas ( 2006 ) work on paragraph segmentation . an analysis of the features contribution suggests an interpretation of what paragraph boundaries indicate and what they depend on .

a new approach to automatic document summarization
in this paper we propose a new approach based on sequence segmentation models ( ssm ) to the extractive document summarization , in which summarizing is regarded as a segment labeling problem . comparing with the previous work , the difference of our approach is that the employed features are obtained not only from the sentence level , but also from the segment level . in our approach , the semi-markov crf model is employed for segment labeling . the preliminary experiments have shown that the approach does outperform all other traditional supervised and unsupervised approaches to document summarization .

a gold standard corpus of early modern german
this paper describes an annotated gold standard sample corpus of early modern german containing over 50,000 tokens of text manually annotated with pos tags , lemmas , and normalised spelling variants . the corpus is the first resource of its kind for this variant of german , and represents an ideal test bed for evaluating and adapting existing nlp tools on historical data . we describe the corpus format , annotation levels , and challenges , providing an example of the requirements and needs of smaller humanities-based corpus projects .

plug and play speech understanding & genevieve gorrell
plug and play is an increasingly important concept in system and network architectures . we introduce and describe a spoken language dialogue system architecture which supports plug and playable networks of objects in its domain . each device in the network carries the linguistic and dialogue management information which is pertinent to it and uploads it dynamically to the relevant language processing components in the spoken language interface . we describe the current state of our plug and play demonstrator and discuss theoretical issues that arise from our work . plug and play forms a central topic for the dhomme project .

automatic linguistic annotation of historical language : totrtale and xix century slovene
the paper describes a tool developed to process historical ( slovene ) text , which annotates words in a tei encoded corpus with their modern-day equivalents , morphosyntactic tags and lemmas . such a tool is useful for developing historical corpora of highly-inflecting languages , enabling full text search in digital libraries of historical texts , for modernising such texts for today 's readers and making it simpler to correct ocr transcriptions .

dynamic language models for streaming text
we present a probabilistic language model that captures temporal dynamics and conditions on arbitrary non-linguistic context features . these context features serve as important indicators of language changes that are otherwise difficult to capture using text data by itself . we learn our model in an efficient online fashion that is scalable for large , streaming data . with five streaming datasets from two different genres economics news articles and social mediawe evaluate our model on the task of sequential language modeling . our model consistently outperforms competing models .

using simple nlp tools to trace the globalization of the art world
we introduce a novel task , that of associating relative time with cities in text . we show that the task can be performed using nlp tools and techniques . the task is deployed on a large corpus of data to study a specific phenomenon , namely the temporal dimension of contemporary arts globalization over the first decade of the 21 st century .

confidence estimation for translation prediction
the purpose of this work is to investigate the use of machine learning approaches for confidence estimation within a statistical machine translation application . specifically , we attempt to learn probabilities of correctness for various model predictions , based on the native probabilites ( i.e . the probabilites given by the original model ) and on features of the current context . our experiments were conducted using three original translation models and two types of neural nets ( single-layer and multilayer perceptrons ) for the confidence estimation task .

sentimerge : combining sentiment lexicons in a bayesian framework
many approaches to sentiment analysis rely on a lexicon that labels words with a prior polarity . this is particularly true for languages other than english , where labelled training data is not easily available . existing efforts to produce such lexicons exist , and to avoid duplicated effort , a principled way to combine multiple resources is required . in this paper , we introduce a bayesian probabilistic model , which can simultaneously combine polarity scores from several data sources and estimate the quality of each source . we apply this algorithm to a set of four german sentiment lexicons , to produce the sentimerge lexicon , which we make publically available . in a simple classification task , we show that this lexicon outperforms each of the underlying resources , as well as a majority vote model .

daebak ! : peripheral diversity for multilingual word sense
we introduce peripheral diversity ( pd ) as a knowledge-based approach to achieve multilingual word sense disambiguation ( wsd ) . pd exploits the frequency and diverse use of word senses in semantic subgraphs derived from larger sense inventories such as babelnet , wikipedia , and wordnet in order to achieve wsd . pds f -measure scores for semeval 2013 task 12 outperform the most frequent sense ( mfs ) baseline for two of the five languages : english , french , german , italian , and spanish . despite pd remaining under-developed and under-explored , it demonstrates that it is robust , competitive , and encourages development .

learning to rank answer candidates for automatic resolution of crossword puzzles
in this paper , we study the impact of relational and syntactic representations for an interesting and challenging task : the automatic resolution of crossword puzzles . automatic solvers are typically based on two answer retrieval modules : ( i ) a web search engine , e.g. , google , bing , etc . and ( ii ) a database ( db ) system for accessing previously resolved crossword puzzles . we show that learning to rank models based on relational syntactic structures defined between the clues and the answer can improve both modules above . in particular , our approach accesses the db using a search engine and reranks its output by modeling paraphrasing . this improves on the mrr of previous system up to 53 % in ranking answer candidates and greatly impacts on the resolution accuracy of crossword puzzles up to 15 % .

syntactic construct : an aid for translating english nominal compound
this paper illustrates a way of using paraphrasal interpretation of english nominal compound for translating them into hindi . input nominal compound is first paraphrased automatically with the 8 prepositions as proposed by lauer ( 1995 ) for the task . english prepositions have one-to-one mapping to post-position in hindi . the english paraphrases are then translated into hindi using the mapping schema . we have got an accuracy of 71 % over a set of gold data of 250 nominal compound . the translation-strategy is motivated by the following observation : it is only 50 % of the cases that english nominal compound is translated into nominal compound in hindi . in other cases , they are translated into varied syntactic constructs . among them the most frequent construction type is modifier + postposition + head . the translation module also attempts to determine when a compound is translated using paraphrase and when it is translated into a nominal compound .

the viability of web-derived polarity lexicons leonid velikovich sasha blair-goldensohn kerry hannan ryan mcdonald
we examine the viability of building large polarity lexicons semi-automatically from the web . we begin by describing a graph propagation framework inspired by previous work on constructing polarity lexicons from lexical graphs . we then apply this technique to build an english lexicon that is significantly larger than those previously studied . crucially , this web-derived lexicon does not require wordnet , part-of-speech taggers , or other language-dependent resources typical of sentiment analysis systems . as a result , the lexicon is not limited to specific word classes e.g. , adjectives that occur in wordnet and in fact contains slang , misspellings , multiword expressions , etc . we evaluate a lexicon derived from english documents , both qualitatively and quantitatively , and show that it provides superior performance to previously studied lexicons , including one derived from wordnet .

learning to automatically solve algebra word problems
we present an approach for automatically learning to solve algebra word problems . our algorithm reasons across sentence boundaries to construct and solve a system of linear equations , while simultaneously recovering an alignment of the variables and numbers in these equations to the problem text . the learning algorithm uses varied supervision , including either full equations or just the final answers . we evaluate performance on a newly gathered corpus of algebra word problems , demonstrating that the system can correctly answer almost 70 % of the questions in the dataset . this is , to our knowledge , the first learning result for this task .

weighted alignment matrices for statistical machine translation
current statistical machine translation systems usually extract rules from bilingual corpora annotated with 1-best alignments . they are prone to learn noisy rules due to alignment mistakes . we propose a new structure called weighted alignment matrix to encode all possible alignments for a parallel text compactly . the key idea is to assign a probability to each word pair to indicate how well they are aligned . we design new algorithms for extracting phrase pairs from weighted alignment matrices and estimating their probabilities . our experiments on multiple language pairs show that using weighted matrices achieves consistent improvements over using n-best lists in significant less extraction time .

generating patient problem lists from the share corpus
an up-to-date problem list is useful for assessing a patients current clinical status . natural language processing can help maintain an accurate problem list . for instance , a patient problem list from a clinical document can be derived from individual problem mentions within the clinical document once these mentions are mapped to a standard vocabulary . in order to develop and evaluate accurate document-level inference engines for this task , a patient problem list could be generated using a standard vocabulary . adequate coverage by standard vocabularies is important for supporting a clear representation of the patient problem concepts described in the texts and for interoperability between clinical systems within and outside the care facilities . in this pilot study , we report the reliability of domain expert generation of a patient problem list from a variety of clinical texts and evaluate the coverage of annotated patient problems against snomed ct and snomed clinical observation recording and encoding ( core ) problem list . across report types , we learned that patient problems can be annotated with agreement ranging from 77.1 % to 89.6 % f1-score and mapped to the core with moderate coverage ranging from 45 % -67 % of patient problems .

towards segment-based recognition of argumentation structure in short texts applied computational linguistics
despite recent advances in discourse parsing and causality detection , the automatic recognition of argumentation structure of authentic texts is still a very challenging task . to approach this problem , we collected a small corpus of german microtexts in a text generation experiment , resulting in texts that are authentic but of controlled linguistic and rhetoric complexity . we show that trained annotators can determine the argumentation structure on these microtexts reliably . we experiment with different machine learning approaches for automatic argumentation structure recognition on various levels of granularity of the scheme . given the complex nature of such a discourse understanding tasks , the first results presented here are promising , but invite for further investigation .

dependency parsing for weibo : an efficient probabilistic logic programming approach
dependency parsing is a core task in nlp , and it is widely used by many applications such as information extraction , question answering , and machine translation . in the era of social media , a big challenge is that parsers trained on traditional newswire corpora typically suffer from the domain mismatch issue , and thus perform poorly on social media data . we present a new gfl/fudg-annotated chinese treebank with more than 18k tokens from sina weibo ( the chinese equivalent of twitter ) . we formulate the dependency parsing problem as many small and parallelizable arc prediction tasks : for each task , we use a programmable probabilistic firstorder logic to infer the dependency arc of a token in the sentence . in experiments , we show that the proposed model outperforms an off-the-shelf stanford chinese parser , as well as a strong maltparser baseline that is trained on the same in-domain data .

semantic dependency parsing using n-best semantic role sequences and roleset information
in this paper , we describe a syntactic and semantic dependency parsing system submitted to the shared task of conll 2008. the proposed system consists of five modules : syntactic dependency parser , predicate identifier , local semantic role labeler , global role sequence candidate generator , and role sequence selector . the syntactic dependency parser is based on malt parser and the sequence candidate generator is based on cky style algorithm . the remaining three modules are implemented by using maximum entropy classifiers . the proposed system achieves 76.90 of labeled f1 for the overall task , 84.82 of labeled attachment , and 68.71 of labeled f1 on the wsj+brown test set .

extracting relations with integrated information using kernel methods
entity relation detection is a form of information extraction that finds predefined relations between pairs of entities in text . this paper describes a relation detection approach that combines clues from different levels of syntactic processing using kernel methods . information from three different levels of processing is considered : tokenization , sentence parsing and deep dependency analysis . each source of information is represented by kernel functions . then composite kernels are developed to integrate and extend individual kernels so that processing errors occurring at one level can be overcome by information from other levels . we present an evaluation of these methods on the 2004 ace relation detection task , using support vector machines , and show that each level of syntactic processing contributes useful information for this task . when evaluated on the official test data , our approach produced very competitive ace value scores . we also compare the svm with knn on different kernels .

machine transliteration : leveraging on third languages
this paper presents two pivot strategies for statistical machine transliteration , namely system-based pivot strategy and model-based pivot strategy . given two independent source-pivot and pivot-target name pair corpora , the model-based strategy learns a direct sourcetarget transliteration model while the system-based strategy learns a sourcepivot model and a pivot-target model , respectively . experimental results on benchmark data show that the systembased pivot strategy is effective in reducing the high resource requirement of training corpus for low-density language pairs while the model-based pivot strategy performs worse than the system-based one .

contextual semantics for wsd
for sinequas second participation to the senseval evaluation , two systems using contextual semantic have been proposed . based on different approaches , they both share the same data preprocessing and enrichment . the first system is a combined approach using semantic classification trees and information retrieval techniques . for the second system , the words from the context are considered as clues . the final sense is determined by summing the weight assigned to each clue for a given example .

a practical solution to the problem of automatic word sense induction
recent studies in word sense induction are based on clustering global co-occurrence vectors , i.e . vectors that reflect the overall behavior of a word in a corpus . if a word is semantically ambiguous , this means that these vectors are mixtures of all its senses . inducing a words senses therefore involves the difficult problem of recovering the sense vectors from the mixtures . in this paper we argue that the demixing problem can be avoided since the contextual behavior of the senses is directly observable in the form of the local contexts of a word . from human disambiguation performance we know that the context of a word is usually sufficient to determine its sense . based on this observation we describe an algorithm that discovers the different senses of an ambiguous word by clustering its contexts . the main difficulty with this approach , namely the problem of data sparseness , could be minimized by looking at only the three main dimensions of the context matrices .

an optimal order of factors for the computational treatment of personal anaphoric devices in urdu discourse
handling of human language by computer is a very intricate and complex task . in natural languages , sentences are usually part of discourse units just as words are part of sentences . anaphora resolution plays a significant role in discourse analysis for chopping larger discourse units into smaller ones . this process is done for the purpose of better understanding and making easier the further processing of text by computer . this paper is focused on the discussion of various factors and their optimal order that play an important role in personal anaphora resolution in urdu . algorithms are developed that resolves pronominal anaphoric devices with 77 - 80 % success rate .

developing methodology for korean particle error detection
we further work on detecting errors in postpositional particle usage by learners of korean by improving the training data and developing a complete pipeline of particle selection . we improve the data by filtering non-korean data and sampling instances to better match the particle distribution . our evaluation shows that , while the data selection is effective , there is much work to be done with preprocessing and system optimization .

instance level transfer learning for cross lingual opinion analysis
this paper presents two instance-level transfer learning based algorithms for cross lingual opinion analysis by transferring useful translated opinion examples from other languages as the supplementary training data for improving the opinion classifier in target language . starting from the union of small training data in target language and large translated examples in other languages , the transfer adaboost algorithm is applied to iteratively reduce the influence of low quality translated examples . alternatively , starting only from the training data in target language , the transfer self-training algorithm is designed to iteratively select high quality translated examples to enrich the training data set . these two algorithms are applied to sentence- and document-level cross lingual opinion analysis tasks , respectively . the evaluations show that these algorithms effectively improve the opinion analysis by exploiting small target language training data and large cross lingual training data .

statistical machine translation improves question retrieval in community question answering via matrix factorization
community question answering ( cqa ) has become an increasingly popular research topic . in this paper , we focus on the problem of question retrieval . question retrieval in cqa can automatically find the most relevant and recent questions that have been solved by other users . however , the word ambiguity and word mismatch problems bring about new challenges for question retrieval in cqa . state-of-the-art approaches address these issues by implicitly expanding the queried questions with additional words or phrases using monolingual translation models . while useful , the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora ( e.g. , question-answer pairs ) in the absence of which they are troubled by noise issue . in this work , we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages . our proposed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other languages via matrix factorization . experiments conducted on a real cqa data show that our proposed approach is promising .

morphological productivity rankings of complex adjectives
this paper investigates a little-studied class of adjectives that we refer to as complex adjectives , i.e. , operationally , adjectives constituted of at least two word tokens separated by a hyphen . we study the properties of these adjectives using two very large text collections : a portion of wikipedia and a web corpus . we consider three corpus-based measures of morphological productivity , and we investigate how productivity rankings based on them correlate with each other under different conditions , thus providing different angles both on the morphological productivity of complex adjectives , and on the productivity measures themselves .

half : comparing a pure cdsm approach with a standard machine learning system for rte fabio massimo zanzotto
in this paper , we describe our submission to the shared task # 1. we tried to follow the underlying idea of the task , that is , evaluating the gap of full-fledged recognizing textual entailment systems with respect to compositional distributional semantic models ( cdsms ) applied to this task . we thus submitted two runs : 1 ) a system obtained with a machine learning approach based on the feature spaces of rules with variables and 2 ) a system completely based on a cdsm that mixes structural and syntactic information by using distributed tree kernels . our analysis shows that , under the same conditions , the fully cdsm system is still far from being competitive with more complex methods .

multilingual aligned parallel treebank corpus reflecting
this paper describes japanese-english-chinese aligned parallel treebank corpora of newspaper articles . they have been constructed by translating each sentence in the penn treebank and the kyoto university text corpus into a corresponding natural sentence in a target language . each sentence is translated so as to reflect its contextual information and is annotated with morphological and syntactic structures and phrasal alignment . this paper also describes the possible applications of the parallel corpus and proposes a new framework to aid in translation . in this framework , parallel translations whose source language sentence is similar to a given sentence can be semiautomatically generated . in this paper we show that the framework can be achieved by using our aligned parallel treebank corpus .

generating chinese couplets using a statistical mt approach
part of the unique cultural heritage of china is the game of chinese couplets ( dulin ) . one person challenges the other person with a sentence ( first sentence ) . the other person then replies with a sentence ( second sentence ) equal in length and word segmentation , in a way that corresponding words in the two sentences match each other by obeying certain constraints on semantic , syntactic , and lexical relatedness . this task is viewed as a difficult problem in ai and has not been explored in the research community . in this paper , we regard this task as a kind of machine translation process . we present a phrase-based smt approach to generate the second sentence . first , the system takes as input the first sentence , and generates as output an n-best list of proposed second sentences , using a phrase-based smt decoder . then , a set of filters is used to remove candidates violating linguistic constraints . finally , a ranking svm is applied to rerank the candidates . a comprehensive evaluation , using both human judgments and bleu scores , has been conducted , and the results demonstrate that this approach is very successful .

ezdi : a hybrid crf and svm based model for detecting and encoding disorder mentions in clinical notes
this paper describes the system used in task-7 ( analysis of clinical text ) of semeval-2014 for detecting disorder mentions and associating them with their related cui of umls 1 . for task-a , a crf based sequencing algorithm was used to find different medical entities and a binary svm classifier was used to find relationship between entities . for task-b , a dictionary look-up algorithm on a customized umls-2012 dictionary was used to find relative cui for a given disorder mention . the system achieved f-score of 0.714 for task a & accuracy of 0.599 for task b when trained only on training data set , and it achieved f-score of 0.755 for task a & accuracy of 0.646 for task b when trained on both training as well as development data set . our system was placed 3rd for both task a and b .

exploiting domain structure for named entity recognition
named entity recognition ( ner ) is a fundamental task in text mining and natural language understanding . current approaches to ner ( mostly based on supervised learning ) perform well on domains similar to the training domain , but they tend to adapt poorly to slightly different domains . we present several strategies for exploiting the domain structure in the training data to learn a more robust named entity recognizer that can perform well on a new domain . first , we propose a simple yet effective way to automatically rank features based on their generalizabilities across domains . we then train a classifier with strong emphasis on the most generalizable features . this emphasis is imposed by putting a rank-based prior on a logistic regression model . we further propose a domain-aware cross validation strategy to help choose an appropriate parameter for the rank-based prior . we evaluated the proposed method with a task of recognizing named entities ( genes ) in biology text involving three species . the experiment results show that the new domainaware approach outperforms a state-ofthe-art baseline method in adapting to new domains , especially when there is a great difference between the new domain and the training domain .

survey on parsing three dependency representations for english
in this paper we focus on practical issues of data representation for dependency parsing . we carry out an experimental comparison of ( a ) three syntactic dependency schemes ; ( b ) three data-driven dependency parsers ; and ( c ) the influence of two different approaches to lexical category disambiguation ( aka tagging ) prior to parsing . comparing parsing accuracies in various setups , we study the interactions of these three aspects and analyze which configurations are easier to learn for a dependency parser .

a tree adjoining grammar analysis of the syntax and semantics of
in this paper , we argue that in it-clefts as in it was ohno who won , the cleft pronoun ( it ) and the cleft clause ( who won ) form a discontinuous syntactic constituent , and a semantic unit as a definite description , presenting arguments from percus ( 1997 ) and hedberg ( 2000 ) . we propose a syntax of it-clefts using tree-local multicomponent tree adjoining grammar and a compositional semantics on the proposed syntax using synchronous tree adjoining grammar .

statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar
we present an application of ambiguity packing and stochastic disambiguation techniques for lexical-functional grammars ( lfg ) to the domain of sentence condensation . our system incorporates a linguistic parser/generator for lfg , a transfer component for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection . furthermore , we propose the use of standard parser evaluation methods for automatically evaluating the summarization quality of sentence condensation systems . an experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings . overall summarization quality of the proposed system is state-of-the-art , with guaranteed grammaticality of the system output due to the use of a constraint-based parser/generator .

on building a high performance gazetteer database
we define a data model for storing geographic information from multiple sources that enables the efficient production of customizable gazetteers . the gazdb separates names from features while storing the relationships between them . geographic names are stored in a variety of resolutions to allow for i18n and for multiplicity of naming . geographic features are categorized along several axes to facilitate selection and filtering .

abstractive summarization of line graphs from popular media
in popular media generally have a discourse goal that contributes to achieving the communicative intent of a multimodal document . this paper presents our work on abstractive summarization of line graphs . our methodology involves hypothesizing the intended message of a line graph and using it as the core of a summary of the graphic . this core is then augmented with salient propositions that elaborate on the intended message .

senserelate : :targetword a generalized framework for word sense disambiguation
we have previously introduced a method of word sense disambiguation that computes the intended sense of a target word , using wordnet-based measures of semantic relatedness ( patwardhan et al , 2003 ) . senserelate : :targetword is a perl package that implements this algorithm . the disambiguation process is carried out by selecting that sense of the target word which is most related to the context words . relatedness between word senses is measured using the wordnet : :similarity perl modules .

aspect-level sentiment analysis in czech ntis new technologies ntis new technologies
this paper presents a pioneering research on aspect-level sentiment analysis in czech . the main contribution of the paper is the newly created czech aspectlevel sentiment corpus , based on data from restaurant reviews . we annotated the corpus with two variants of aspect-level sentiment aspect terms and aspect categories . the corpus consists of 1,244 sentences and 1,824 annotated aspects and is freely available to the research community . furthermore , we propose a baseline system based on supervised machine learning . our system detects the aspect terms with fmeasure 68.65 % and their polarities with accuracy 66.27 % . the categories are recognized with f-measure 74.02 % and their polarities with accuracy 66.61 % .

identifying contradictory and contrastive relations between statements to outline web information on a given topic daisuke kawahara kentaro inui sadao kurohashi
we present a method for producing a birds-eye view of statements that are expressed on web pages on a given topic . this method aggregates statements that are relevant to the topic , and shows contradictory and contrastive relations among them . this view of contradictions and contrasts helps users acquire a top-down understanding of the topic . to realize this , we extract such statements and relations , including cross-document implicit contrastive relations between statements , in an unsupervised manner . our experimental results indicate the effectiveness of our approach .

enhancing cross document coreference of web documents with context similarity and very large scale text categorization lockheed martin is & gs
cross document coreference ( cdc ) is the task of constructing the coreference chain for mentions of a person across a set of documents . this work offers a holistic view of using document-level categories , sub-document level context and extracted entities and relations for the cdc task . we train a categorization component with an efficient flat algorithm using thousands of odp categories and over a million web documents . we propose to use ranked categories as coreference information , particularly suitable for web documents that are widely different in style and content . an ensemble composite coreference function , amenable to inactive features , combines these three levels of evidence for disambiguation . a thorough feature importance study is conducted to analyze how these three components contribute to the coreference results . the overall solution is evaluated using the weps benchmark data and demonstrate superior performance .

a noisy-channel model of rational human sentence comprehension
language comprehension , as with all other cases of the extraction of meaningful structure from perceptual input , takes places under noisy conditions . if human language comprehension is a rational process in the sense of making use of all available information sources , then we might expect uncertainty at the level of word-level input to affect sentence-level comprehension . however , nearly all contemporary models of sentence comprehension assume clean inputthat is , that the input to the sentence-level comprehension mechanism is a perfectly-formed , completely certain sequence of input tokens ( words ) . this article presents a simple model of rational human sentence comprehension under noisy input , and uses the model to investigate some outstanding problems in the psycholinguistic literature for theories of rational human sentence comprehension . we argue that by explicitly accounting for inputlevel noise in sentence processing , our model provides solutions for these outstanding problems and broadens the scope of theories of human sentence comprehension as rational probabilistic inference .

auditory-based acoustic distinctive features and spectral cues for robust automatic speech recognition in low-snr car environments
in this paper , a multi-stream paradigm is proposed to improve the performance of automatic speech recognition ( asr ) systems in the presence of highly interfering car noise . it was found that combining the classical mfccs with some auditory-based acoustic distinctive cues and the main formant frequencies of a speech signal using a multi-stream paradigm leads to an improvement in the recognition performance in noisy car environments .

the sentiment analysis of tweets
this paper describes state-of-the-art statistical systems for automatic sentiment analysis of tweets . in a semeval-2014 shared task ( task 9 ) , our submissions obtained highest scores in the term-level sentiment classification subtask on both the 2013 and 2014 tweets test sets . in the message-level sentiment classification task , our submissions obtained highest scores on the livejournal blog posts test set , sarcastic tweets test set , and the 2013 sms test set . these systems build on our semeval-2013 sentiment analysis systems which ranked first in both the termand message-level subtasks in 2013. key improvements over the 2013 systems are in the handling of negation . we create separate tweet-specific sentiment lexicons for terms in affirmative contexts and in negated contexts .

exploiting semantic information for manual anaphoric annotation in
this paper presents the discourse annotation followed in cast3lb , a spanish corpus annotated with several information sources ( morphological , syntactic , semantic and coreferential ) at syntactic , semantic and discourse level . 3lb annotation scheme has been developed for three languages ( spanish , catalan and basque ) . human annotators have used a set of tagging techniques and protocols . several tools have provided them with a friendly annotation scheme . at discourse level , anaphoric and coreference expressions are annotated . one of the most interesting contributions to this annotation scenario is the enriched anaphora resolution module that is based on the previously defined semantic annotation phase to expand the discourse information and use it to suggest the correct antecedent of an anaphora to the annotator . this paper describes the relevance of the semantic tags in the discourse annotation in spanish corpus cast3lb and shows both levels and tools in the mentioned discourse annotation scheme .

a low-budget tagger for old czech
the paper describes a tagger for old czech ( 1200-1500 ad ) , a fusional language with rich morphology . the practical restrictions ( no native speakers , limited corpora and lexicons , limited funding ) make old czech an ideal candidate for a resource-light crosslingual method that we have been developing ( e.g . hana et al , 2004 ; feldman and hana , 2010 ) . we use a traditional supervised tagger . however , instead of spending years of effort to create a large annotated corpus of old czech , we approximate it by a corpus of modern czech . we perform a series of simple transformations to make a modern text look more like a text in old czech and vice versa . we also use a resource-light morphological analyzer to provide candidate tags . the results are worse than the results of traditional taggers , but the amount of language-specific work needed is minimal .

learning to identify definitions using syntactic features
this paper describes an approach to learning concept definitions which operates on fully parsed text . a subcorpus of the dutch version of wikipedia was searched for sentences which have the syntactic properties of definitions . next , we experimented with various text classification techniques to distinguish actual definitions from other sentences . a maximum entropy classifier which incorporates features referring to the position of the sentence in the document as well as various syntactic features , gives the best results .

exploiting translational correspondences for pattern-independent mwe
based on a study of verb translations in the europarl corpus , we argue that a wide range of mwe patterns can be identified in translations that exhibit a correspondence between a single lexical item in the source language and a group of lexical items in the target language . we show that these correspondences can be reliably detected on dependency-parsed , word-aligned sentences . we propose an extraction method that combines word alignment with syntactic filters and is independent of the structural pattern of the translation .

a joint learning model of word segmentation , lexical acquisition , and phonetic variability
we present a cognitive model of early lexical acquisition which jointly performs word segmentation and learns an explicit model of phonetic variation . we define the model as a bayesian noisy channel ; we sample segmentations and word forms simultaneously from the posterior , using beam sampling to control the size of the search space . compared to a pipelined approach in which segmentation is performed first , our model is qualitatively more similar to human learners . on data with variable pronunciations , the pipelined approach learns to treat syllables or morphemes as words . in contrast , our joint model , like infant learners , tends to learn multiword collocations . we also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors , and relate these to developmental evidence .

using distributional semantics to trace influence and imitation in romantic orientalist poetry
in this paper , we investigate whether textual analysis can yield evidence of shared vocabulary or formal textual characteristics in the works of 19th century poets lord byron and thomas moore in the genre of romantic orientalism . in particular , we identify and trace byrons influence on moores writings to query whether moore imitated byron , as many reviewers of the time suggested . we use a distributional semantic model ( dsm ) to analyze if there is a shared vocabulary of romantic orientalism , or if it is possible to characterize a literary genre in terms of vocabulary , rather than in terms of the particular plots , characters and themes . we discuss the results that dsm models are able to provide for an abstract overview of the influence of lord byrons work on thomas moore .

domain adaptation meets active learning piyush rai , avishek saha , hal daume iii , and suresh venkatasubramanian
in this work , we show how active learning in some ( target ) domain can leverage information from a different but related ( source ) domain . we present an algorithm that harnesses the source domain data to learn the best possible initializer hypothesis for doing active learning in the target domain , resulting in improved label complexity . we also present a variant of this algorithm which additionally uses the domain divergence information to selectively query the most informative points in the target domain , leading to further reductions in label complexity . experimental results on a variety of datasets establish the efficacy of the proposed methods .

improved fully unsupervised parsing with zoomed learning
we introduce a novel training algorithm for unsupervised grammar induction , called zoomed learning . given a training set t and a test set s , the goal of our algorithm is to identify subset pairs ti , si of t and s such that when the unsupervised parser is trained on a training subset ti its results on its paired test subset si are better than when it is trained on the entire training set t . a successful application of zoomed learning improves overall performance on the full test set s. we study our algorithms effect on the leading algorithm for the task of fully unsupervised parsing ( seginer , 2007 ) in three different english domains , wsj , brown and genia , and show that it improves the parser f-score by up to 4.47 % .

can markov models over minimal translation units help phrase-based
the phrase-based and n-gram-based smt frameworks complement each other . while the former is better able to memorize , the latter provides a more principled model that captures dependencies across phrasal boundaries . some work has been done to combine insights from these two frameworks . a recent successful attempt showed the advantage of using phrasebased search on top of an n-gram-based model . we probe this question in the reverse direction by investigating whether integrating n-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption . a large scale evaluation over 8 language pairs shows that performance does significantly improve .

contextual dependencies in unsupervised word segmentation
developing better methods for segmenting continuous text into words is important for improving the processing of asian languages , and may shed light on how humans learn to segment speech . we propose two new bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively . the bigram model greatly outperforms the unigram model ( and previous probabilistic models ) , demonstrating the importance of such dependencies for word segmentation . we also show that previous probabilistic models rely crucially on suboptimal search procedures .

generating simulations of motion events from verbal descriptions
in this paper , we describe a computational model for motion events in natural language that maps from linguistic expressions , through a dynamic event interpretation , into three-dimensional temporal simulations in a model . starting with the model from ( pustejovsky and moszkowicz , 2011 ) , we analyze motion events using temporally-traced labelled transition systems . we model the distinction between path- and manner-motion in an operational semantics , and further distinguish different types of manner-of-motion verbs in terms of the mereo-topological relations that hold throughout the process of movement . from these representations , we generate minimal models , which are realized as three-dimensional simulations in software developed with the game engine , unity . the generated simulations act as a conceptual debugger for the semantics of different motion verbs : that is , by testing for consistency and informativeness in the model , simulations expose the presuppositions associated with linguistic expressions and their compositions . because the model generation component is still incomplete , this paper focuses on an implementation which maps directly from linguistic interpretations into the unity code snippets that create the simulations .

exact decoding of syntactic translation models through lagrangian relaxation
we describe an exact decoding algorithm for syntax-based statistical translation . the approach uses lagrangian relaxation to decompose the decoding problem into tractable subproblems , thereby avoiding exhaustive dynamic programming . the method recovers exact solutions , with certificates of optimality , on over 97 % of test examples ; it has comparable speed to state-of-the-art decoders .

composition of simple bags of typed terms escuela politcnica superior legans ( madrid ) spain
this paper describes a system designed to disambiguate person names in a set of web pages . in our approach web documents are represented as different sets of features or terms of different types ( bag of words , urls , names and numbers ) . we apply agglomerative vector space clustering that uses the similarity between pairs of analogous feature sets . this system achieved a value of 66 % for f=0.2 and a value of 48 % for f=0.5 in the web people search task at semeval-2007 .

mood patterns and affective lexicon access in weblogs
the emergence of social media brings chances , but also challenges , to linguistic analysis . in this paper we investigate a novel problem of discovering patterns based on emotion and the association of moods and affective lexicon usage in blogosphere , a representative for social media . we propose the use of normative emotional scores for english words in combination with a psychological model of emotion measurement and a nonparametric clustering process for inferring meaningful emotion patterns automatically from data . our results on a dataset consisting of more than 17 million mood-groundtruthed blogposts have shown interesting evidence of the emotion patterns automatically discovered that match well with the coreaffect emotion model theorized by psychologists . we then present a method based on information theory to discover the association of moods and affective lexicon usage in the new media .

tmunsw : disorder concept recognition and normalization in new south wales ,
we present our participation in task 7 of semeval shared task 2014. the goal of this particular task includes the identification of disorder named entities and the mapping of each disorder to a unique unified medical language system concept identifier , which were referred to as task a and task b respectively . we participated in both of these subtasks and used ytex as a baseline system . we further developed a supervised linear chain conditional random field model based on sets of features to predict disorder mentions . to take benefit of results from both systems we merged these results . under strict condition our best run evaluated at 0.549 f-measure for task a and an accuracy of 0.489 for task b on test dataset . based on our error analysis we conclude that recall of our system can be significantly increased by adding more features to the conditional random field model and by using another type of tag representation or frame matching algorithm to deal with the disjoint entity mentions .

segmentation of multiple objects in multi-camera video streams
in this paper , we propose a new software tool called dales to extract semantic information from multi-view videos based on the analysis of their visual content . our system is fully automatic and is well suited for multi-camera environment . once the multi-view video sequences are loaded into dales , our software performs the detection , counting , and segmentation of the visual objects evolving in the provided video streams . then , these objects of interest are processed in order to be labelled , and the related frames are thus annotated with the corresponding semantic content . moreover , a textual script is automatically generated with the video annotations . dales system shows excellent performance in terms of accuracy and computational speed and is robustly designed to ensure view synchronization .

france telecom r & d beijing word segmenter for sighan backoff 2006
this paper presents two word segmentation ( ws ) systems and a named entity recognition ( ner ) system in france telecom r & d beijing . the one system of ws is for open tracks based on ngram language model and another one is for closed tracks based on maximum entropy approach . the ner system uses a hybrid algorithm based on class-based language model and rule-based knowledge . these systems are all augmented with a set of post-processors .

sorry seems to be the hardest word allan ramsay debora field univ of manchester univ of sheffield
we are interested in the ways that language is used to achieve a variety of goals , where the same utterance may have vastly different consequences in different situations . this is closely related to the topic of creativity in language . the fact that the same utterance can be used to achieve a variety of goals opens up the possibility of using it to achieve new goals . the current paper concentrates largely on an implemented system for exploring how the effects of an utterance depend on the situation in which it is produced , but we will end with some speculations about how how utterances can come to have new kinds of uses .

improved iterative scaling can yield multiple globally optimal models with radically di ering performance levels division of informatics
log-linear models can be eciently estimated using algorithms such as improved iterative scaling ( iis ) ( la erty et al , 1997 ) . under certain conditions and for a particular class of problems , iis is guaranteed to approach both the maximum-likelihood and maximum entropy solution . this solution , in likelihood space , is unique . unfortunately , in realistic situations , multiple solutions may exist , all of which are equivalent to each other in terms of likelihood , but radically di erent from each other in terms of performance . we show that this behaviour can occur when a model contains overlapping features and the training material is sparse . experimental results , from the domain of parse selection for stochastic attribute value grammars , shows the wide variation in performance that can be found when estimating models using iis . further results show that the in uence of the initial model can be diminished by selecting either uniform weights , or else by model averaging .

the non-randomness problem in word frequency distribution modeling
frequency distribution models tuned to words and other linguistic events can predict the number of distinct types and their frequency distribution in samples of arbitrary sizes . we conduct , for the first time , a rigorous evaluation of these models based on cross-validation and separation of training and test data . our experiments reveal that the prediction accuracy of the models is marred by serious overfitting problems , due to violations of the random sampling assumption in corpus data . we then propose a simple pre-processing method to alleviate such non-randomness problems . further evaluation confirms the effectiveness of the method , which compares favourably to more complex correction techniques .

word alignment based on bilingual bracketing
in this paper , an improved word alignment based on bilingual bracketing is described . the explored approaches include using model-1 conditional probability , a boosting strategy for lexicon probabilities based on importance sampling , applying parts of speech to discriminate english words and incorporating information of english base noun phrase . the results of the shared task on french-english , romanianenglish and chinese-english word alignments are presented and discussed .

a statistical language modeling approach to lattice-based spoken document retrieval
speech recognition transcripts are far from perfect ; they are not of sufficient quality to be useful on their own for spoken document retrieval . this is especially the case for conversational speech . recent efforts have tried to overcome this issue by using statistics from speech lattices instead of only the 1best transcripts ; however , these efforts have invariably used the classical vector space retrieval model . this paper presents a novel approach to lattice-based spoken document retrieval using statistical language models : a statistical model is estimated for each document , and probabilities derived from the document models are directly used to measure relevance . experimental results show that the lattice-based language modeling method outperforms both the language modeling retrieval method using only the 1-best transcripts , as well as a recently proposed lattice-based vector space retrieval method .

statistical machine reordering
reordering is currently one of the most important problems in statistical machine translation systems . this paper presents a novel strategy for dealing with it : statistical machine reordering ( smr ) . it consists in using the powerful techniques developed for statistical machine translation ( smt ) to translate the source language ( s ) into a reordered source language ( s ) , which allows for an improved translation into the target language ( t ) . the smt task changes from s2t to s2t which leads to a monotonized word alignment and shorter translation units . in addition , the use of classes in smr helps to infer new word reorderings . experiments are reported in the esen wmt06 tasks and the zhen iwslt05 task and show significant improvement in translation quality .

an empirical model of multiword expression decomposability
this paper presents a constructioninspecific model of multiword expression decomposability based on latent semantic analysis . we use latent semantic analysis to determine the similarity between a multiword expression and its constituent words , and claim that higher similarities indicate greater decomposability . we test the model over english noun-noun compounds and verb-particles , and evaluate its correlation with similarities and hyponymy values in wordnet . based on mean hyponymy over partitions of data ranked on similarity , we furnish evidence for the calculated similarities being correlated with the semantic relational content of wordnet .

hmm based chunker for hindi
this paper presents an hmm-based chunk tagger for hindi . various tagging schemes for marking chunk boundaries are discussed along with their results . contextual information is incorporated into the chunk tags in the form of partof-speech ( pos ) information . this information is also added to the tokens themselves to achieve better precision . error analysis is carried out to reduce the number of common errors . it is found that for certain classes of words , using the pos information is more effective than using a combination of word and pos tag as the token . finally , chunk labels are also marked on the chunks .

treebank translation for cross-lingual parser induction
cross-lingual learning has become a popular approach to facilitate the development of resources and tools for low-density languages . its underlying idea is to make use of existing tools and annotations in resource-rich languages to create similar tools and resources for resource-poor languages . typically , this is achieved by either projecting annotations across parallel corpora , or by transferring models from one or more source languages to a target language . in this paper , we explore a third strategy by using machine translation to create synthetic training data from the original sourceside annotations . specifically , we apply this technique to dependency parsing , using a cross-lingually unified treebank for adequate evaluation . our approach draws on annotation projection but avoids the use of noisy source-side annotation of an unrelated parallel corpus and instead relies on manual treebank annotation in combination with statistical machine translation , which makes it possible to train fully lexicalized parsers . we show that this approach significantly outperforms delexicalized transfer parsing .

considerations on the nature of metaphorical meaning arising from a computational treatment of metaphor interpetation
this paper argues that there need not be a full correspondence between source and target domains when interpreting metaphors . instead , inference is performed in the source domain , and conclusions transferred to the target . a description of a computer system , att-meta , that partially implements these ideas is provided .

hits-based seed selection and stop list construction for bootstrapping tetsuo kiso masashi shimbo mamoru komachi yuji matsumoto
in bootstrapping ( seed set expansion ) , selecting good seeds and creating stop lists are two effective ways to reduce semantic drift , but these methods generally need human supervision . in this paper , we propose a graphbased approach to helping editors choose effective seeds and stop list instances , applicable to pantel and pennacchiottis espresso bootstrapping algorithm . the idea is to select seeds and create a stop list using the rankings of instances and patterns computed by kleinbergs hits algorithm . experimental results on a variation of the lexical sample task show the effectiveness of our method .

fast statistical parsing with parallel multiple context-free grammars
we present an algorithm for incremental statistical parsing with parallel multiple context-free grammars ( pmcfg ) . this is an extension of the algorithm by angelov ( 2009 ) to which we added statistical ranking . we show that the new algorithm is several times faster than other statistical pmcfg parsing algorithms on real-sized grammars . at the same time the algorithm is more general since it supports non-binarized and non-linear grammars . we also show that if we make the search heuristics non-admissible , the parsing speed improves even further , at the risk of returning sub-optimal solutions .

word representations : a simple and general method for semi-supervised learning
if we take an existing supervised nlp system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features . we evaluate brown clusters , collobert and weston ( 2008 ) embeddings , and hlbl ( mnih & hinton , 2009 ) embeddings of words on both ner and chunking . we use near state-of-the-art supervised baselines , and find that each of the three word representations improves the accuracy of these baselines . we find further improvements by combining different word representations . you can download our word features , for off-the-shelf use in existing nlp systems , as well as our code , here : http : //metaoptimize . com/projects/wordreprs/

unsupervised induction of contingent event pairs from film scenes
human engagement in narrative is partially driven by reasoning about discourse relations between narrative events , and the expectations about what is likely to happen next that results from such reasoning . researchers in nlp have tackled modeling such expectations from a range of perspectives , including treating it as the inference of the contingent discourse relation , or as a type of common-sense causal reasoning . our approach is to model likelihood between events by drawing on several of these lines of previous work . we implement and evaluate different unsupervised methods for learning event pairs that are likely to be contingent on one another . we refine event pairs that we learn from a corpus of film scene descriptions utilizing web search counts , and evaluate our results by collecting human judgments of contingency . our results indicate that the use of web search counts increases the average accuracy of our best method to 85.64 % over a baseline of 50 % , as compared to an average accuracy of 75.15 % without web search .

a hybrid approach to biomedical named entity recognition and richard tzong-han tsai
in this paper , we describe our hybrid approach to two key nlp technologies : biomedical named entity recognition ( bio-ner ) and ( bio-srl ) . in bio-ner , our system successfully integrates linguistic features into the crf framework . in addition , we employ web lexicons and template-based post-processing to further boost its performance . through these broad linguistic features and the nature of crf , our system outperforms state-ofthe-art machine-learning-based systems , especially in the recognition of protein names ( f=78.5 % ) . in bio-srl , first , we construct a proposition bank on top of the popular biomedical genia treebank following the propbank annotation scheme . we only annotate the predicate-argument structures ( pass ) of thirty frequently used biomedical verbs ( predicates ) and their corresponding arguments . second , we use our proposition bank to train a biomedical srl system , which uses a maximum entropy ( me ) machinelearning model . thirdly , we automatically generate argument-type templates , which can be used to improve classification of biomedical argument roles . our experimental results show that a newswire english srl system that achieves an f-score of 86.29 % in the newswire english domain can maintain an f-score of 64.64 % when ported to the biomedical domain . by using our annotated biomedical corpus , we can increase that f-score by 22.9 % .

breaking the zipfian barrier of nlp
we know that the distribution of most of the linguistic entities ( e.g . phones , words , grammar rules ) follow a power law or the zipf 's law . this makes nlp hard . interestingly , the distribution of speakers over the world , content over the web and linguistic resources available across languages also follow power law . however , the correlation between the distribution of number of speakers to that of web content and linguistic resources is rather poor , and the latter distributions are much more skewed than the former . in other words , there is a large volume of resources only for a very few languages and a large number of widely spoken languages , including all the indian languages , have little or no linguistic resource at all . this is a serious challenge for nlp in these languages , primarily because state-of-the-art techniques and tools in nlp are all data-driven . i refer to this situation as the `` zipfian barrier of nlp '' and offer a mathematical analysis of the growth dynamics of the linguistic resources and nlp research worldwide , which , afterall , is very much a socio-economic process . based on the analysis and otherwise , i propose certain technical ( e.g . unsupervised learning , wiki based approaches to gather data ) and community-wide ( e.g .

spectral clustering for example based machine translation
prior work has shown that generalization of data in an example based machine translation ( ebmt ) system , reduces the amount of pre-translated text required to achieve a certain level of accuracy ( brown , 2000 ) . several word clustering algorithms have been suggested to perform these generalizations , such as kmeans clustering or group average clustering . the hypothesis is that better contextual clustering can lead to better translation accuracy with limited training data . in this paper , we use a form of spectral clustering to cluster words , and this is shown to result in as much as 29.08 % improvement over the baseline ebmt system .

event detection and summarization in weblogs with temporal collocations
this paper deals with the relationship between weblog content and time . with the proposed temporal mutual information , we analyze the collocations in time dimension , and the interesting collocations related to special events . the temporal mutual information is employed to observe the strength of term-to-term associations over time . an event detection algorithm identifies the collocations that may cause an event in a specific timestamp . an event summarization algorithm retrieves a set of collocations which describe an event . we compare our approach with the approach without considering the time interval . the experimental results demonstrate that the temporal collocations capture the real world semantics and real world events over time .

an english-korean transliteration model using pronunciation and
there is increasing concern about english-korean ( e-k ) transliteration recently . in the previous works , direct converting methods from english alphabets to korean alphabets were a main research topic . in this paper , we present an e-k transliteration model using pronunciation and contextual rules . unlike the previous works , our method uses phonetic information such as phoneme and its context . we also use word formation information such as english words of greek origin . with them , our method shows significant performance increase about 31 % in word accuracy .

hierarchical multi-class text categorization with global margin maximization
text categorization is a crucial and wellproven method for organizing the collection of large scale documents . in this paper , we propose a hierarchical multi-class text categorization method with global margin maximization . we not only maximize the margins among leaf categories , but also maximize the margins among their ancestors . experiments show that the performance of our algorithm is competitive with the recently proposed hierarchical multi-class classification algorithms .

reducing parameter space for word alignment
this paper presents the experimental results of our attemps to reduce the size of the parameter space in word alignment algorithm . we use ibm model 4 as a baseline . in order to reduce the parameter space , we pre-processed the training corpus using a word lemmatizer and a bilingual term extraction algorithm . using these additional components , we obtained an improvement in the alignment error rate .

incrementality in syntactic processing : computational models and experimental evidence
it is a well-known intuition that human sentence understanding works in an incremental fashion , with a seemingly constant update of the interpretation through the left-to-right processing of a string . such intuitions are backed up by experimental evidence dating from at least as far back as marslen-wilson ( 1973 ) , showing that under many circumstances , interpretations are indeed updated very quickly .

ontology driven content extraction using interlingual annotation of texts in the omnia project
omnia is an on-going project that aims to retrieve images accompanied with multilingual texts . in this paper , we propose a generic method ( language and domain independent ) to extract conceptual information from such texts and spontaneous user requests . first , texts are labelled with interlingual annotation , then a generic extractor taking a domain ontology as a parameter extract relevant conceptual information . implementation is also presented with a first experiment and preliminary results .

building a discourse-tagged corpus in the framework of rhetorical structure theory mary ellen okurowski
we describe our experience in developing a discourse-annotated corpus for community-wide use . working in the framework of rhetorical structure theory , we were able to create a large annotated resource with very high consistency , using a well-defined methodology and protocol . this resource is made publicly available through the linguistic data consortium to enable researchers to develop empirically grounded , discourse-specific applications .

making sense of word sense variation
we present a pilot study of word-sense annotation using multiple annotators , relatively polysemous words , and a heterogenous corpus . annotators selected senses for words in context , using an annotation interface that presented wordnet senses . interannotator agreement ( ia ) results show that annotators agree well or not , depending primarily on the individual words and their general usage properties . our focus is on identifying systematic differences across words and annotators that can account for ia variation . we identify three lexical use factors : semantic specificity of the context , sense concreteness , and similarity of senses . we discuss systematic differences in sense selection across annotators , and present the use of association rules to mine the data for systematic differences across annotators .

expressing implicit semantic relations without supervision
we present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations . for a given input word pair yx : with some unspecified semantic relations , the corresponding output list of patterns mpp , ,1 is ranked according to how well each pattern ip expresses the relations between x and y . for example , given ostrich=x and bird=y , the two highest ranking output patterns are x is the largest y and y such as the x. the output patterns are intended to be useful for finding further pairs with the same relations , to support the construction of lexicons , ontologies , and semantic networks . the patterns are sorted by pertinence , where the pertinence of a pattern ip for a word pair yx : is the expected relational similarity between the given pair and typical pairs for ip . the algorithm is empirically evaluated on two tasks , solving multiple-choice sat word analogy questions and classifying semantic relations in noun-modifier pairs . on both tasks , the algorithm achieves stateof-the-art results , performing significantly better than several alternative pattern ranking algorithms , based on tf-idf .

creating a ccgbank and a wide-coverage ccg lexicon for german
we present an algorithm which creates a german ccgbank by translating the syntax graphs in the german tiger corpus into ccg derivation trees . the resulting corpus contains 46,628 derivations , covering 95 % of all complete sentences in tiger . lexicons extracted from this corpus contain correct lexical entries for 94 % of all known tokens in unseen text .

ucsg : a wide coverage shallow parsing system
in this paper , we propose an architecture , called ucsg shallow parsing architecture , for building wide coverage shallow parsers by using a judicious combination of linguistic and statistical techniques without need for large amount of parsed training corpus to start with . we only need a large pos tagged corpus . a parsed corpus can be developed using the architecture with minimal manual effort , and such a corpus can be used for evaluation as also for performance improvement . the ucsg architecture is designed to be extended into a full parsing system but the current work is limited to chunking and obtaining appropriate chunk sequences for a given sentence . in the ucsg architecture , a finite state grammar is designed to accept all possible chunks , referred to as word groups here . a separate statistical component , encoded in hmms ( hidden markov model ) , has been used to rate and rank the word groups so produced . note that we are not pruning , we are only rating and ranking the word groups already obtained . then we use a best first search strategy to produce parse outputs in best first order , without compromising on the ability to produce all possible parses in principle . we propose a bootstrapping strategy for improving hmm parameters and hence the performance of the parser as a whole . a wide coverage shallow parser has been implemented for english starting from the british national corpus , a nearly 100 million word pos tagged corpus .

statistical machine translation with local language models
part-of-speech language modeling is commonly used as a component in statistical machine translation systems , but there is mixed evidence that its usage leads to significant improvements . we argue that its limited effectiveness is due to the lack of lexicalization . we introduce a new approach that builds a separate local language model for each word and part-of-speech pair . the resulting models lead to more context-sensitive probability distributions and we also exploit the fact that different local models are used to estimate the language model probability of each word during decoding . our approach is evaluated for arabic- and chinese-to-english translation . we show that it leads to statistically significant improvements for multiple test sets and also across different genres , when compared against a competitive baseline and a system using a part-of-speech model .

text comparison using machine-generated nuggets
this paper describes a novel text comparison environment that facilities text comparison administered through assessing and aggregating information nuggets automatically created and extracted from the texts in question . our goal in designing such a tool is to enable and improve automatic nugget creation and present its application for evaluations of various natural language processing tasks . during our demonstration at hlt , new users will able to experience first hand text analysis can be fun , enjoyable , and interesting using system-created nuggets .

setswana tokenisation and computational verb morphology : facing the challenge of a disjunctive orthography
setswana , a bantu language in the sotho group , is one of the eleven official languages of south africa . the language is characterised by a disjunctive orthography , mainly affecting the important word category of verbs . in particular , verbal prefixal morphemes are usually written disjunctively , while suffixal morphemes follow a conjunctive writing style . therefore , setswana tokenisation can not be based solely on whitespace , as is the case in many alphabetic , segmented languages , including the conjunctively written nguni group of south african bantu languages . this paper shows how a combination of two tokeniser transducers and a finite-state ( rule-based ) morphological analyser may be combined to effectively solve the setswana tokenisation problem . the approach has the important advantage of bringing the processing of setswana beyond the morphological analysis level in line with what is appropriate for the nguni languages . this means that the challenge of the disjunctive orthography is met at the tokenisation/morphological analysis level and does not in principle propagate to subsequent levels of analysis such as pos tagging and shallow parsing , etc . indeed , the approach ensures that an aspect such as orthography does not obfuscate sound linguistics and , ultimately , proper semantic analysis , which remains the ultimate aim of linguistic analysis and therefore also computational linguistic analysis .

extracting aspects and polarity from patents
we describe an approach to terminology extraction from patent corpora that follows from a view of patents as positive reviews of inventions . as in aspect-based sentiment analysis , we focus on identifying not only the components of products but also the attributes and tasks which , in the case of patents , serve to justify an inventions utility . these semantic roles ( component , task , attribute ) can serve as a high level ontology for categorizing domain terminology , within which the positive/negative polarity of attributes serves to identify technical goals and obstacles . we show that bootstrapping using a very small set of domain-independent lexico-syntactic features may be sufficient for constructing domainspecific classifiers capable of assigning semantic roles and polarity to terms in domains as diverse as computer science and health .

a vector space model for subjectivity classification in urdu aided by co-training
the goal of this work is to produce a classifier that can distinguish subjective sentences from objective sentences for the urdu language . the amount of labeled data required for training automatic classifiers can be highly imbalanced especially in the multilingual paradigm as generating annotations is an expensive task . in this work , we propose a cotraining approach for subjectivity analysis in the urdu language that augments the positive set ( subjective set ) and generates a negative set ( objective set ) devoid of all samples close to the positive ones . using the data set thus generated for training , we conduct experiments based on svm and vsm algorithms , and show that our modified vsm based approach works remarkably well as a sentence level subjectivity classifier .

learning the meaning of scalar adjectives
texts and dialogues often express information indirectly . for instance , speakers answers to yes/no questions do not always straightforwardly convey a yes or no answer . the intended reply is clear in some cases ( was it good it was great ! ) but uncertain in others ( was it acceptable it was unprecedented . ) . in this paper , we present methods for interpreting the answers to questions like these which involve scalar modifiers . we show how to ground scalar modifier meaning based on data collected from the web . we learn scales between modifiers and infer the extent to which a given answer conveys yes or no . to evaluate the methods , we collected examples of questionanswer pairs involving scalar modifiers from cnn transcripts and the dialog act corpus and use response distributions from mechanical turk workers to assess the degree to which each answer conveys yes or no . our experimental results closely match the turkers response data , demonstrating that meanings can be learned from web data and that such meanings can drive pragmatic inference .

adding domain specificity to an mt system
in the development of a machine translation system , one important issue is being able to adapt to a specific domain without requiring timeconsuming lexical work . we have experimented with using a statistical word-alignment algorithm to derive word association pairs ( french-english ) that complement an existing multipurpose bilingual dictionary . this word association information is added to the system at the time of the automatic creation of our translation pattern database , thereby making this database more domain specific . this technique significantly improves the overall quality of translation , as measured in an independent blind evaluation .

classifying wikipedia articles into nes using svms with threshold
in this paper , a method is presented to recognize multilingual wikipedia named entity articles . this method classifies multilingual wikipedia articles using a variety of structured and unstructured features and is aided by cross-language links and features in wikipedia . adding multilingual features helps boost classification accuracy and is shown to effectively classify multilingual pages in a language independent way . classification is done using support vectors machine ( svm ) classifier at first , and then the threshold of svm is adjusted in order to improve the recall scores of classification . threshold adjustment is performed using beta-gamma threshold adjustment algorithm which is a post learning step that shifts the hyperplane of svm . this approach boosted recall with minimal effect on precision .

inclusive yet selective : supervised distributional hypernymy detection
we test the distributional inclusion hypothesis , which states that hypernyms tend to occur in a superset of contexts in which their hyponyms are found . we find that this hypothesis only holds when it is applied to relevant dimensions . we propose a robust supervised approach that achieves accuracies of .84 and .85 on two existing datasets and that can be interpreted as selecting the dimensions that are relevant for distributional inclusion .

lda based similarity modeling for question answering speech technology and
we present an exploration of generative modeling for the question answering ( qa ) task to rank candidate passages . we investigate latent dirichlet allocation ( lda ) models to obtain ranking scores based on a novel similarity measure between a natural language question posed by the user and a candidate passage . we construct two models each one introducing deeper evaluations on latent characteristics of passages together with given question . with the new representation of topical structures on qa datasets , using a limited amount of world knowledge , we show improvements on performance of a qa ranking system .

of an automatically annotated corpus
the creation of a gold standard corpus ( gsc ) is a very laborious and costly process . silver standard corpus ( ssc ) annotation is a very recent direction of corpus development which relies on multiple systems instead of human annotators . in this paper , we investigate the practical usability of an ssc when a machine learning system is trained on it and tested on an unseen benchmark gsc . the main focus of this paper is how an ssc can be maximally exploited . in this process , we inspect several hypotheses which might have influenced the idea of ssc creation . empirical results suggest that some of the hypotheses ( e.g . a positive impact of a large ssc despite of having wrong and missing annotations ) are not fully correct . we show that it is possible to automatically improve the quality and the quantity of the ssc annotations . we also observe that considering only those sentences of ssc which contain annotations rather than the full ssc results in a performance boost .

binarizing syntax trees to improve syntax-based machine translation accuracy
we show that phrase structures in penn treebank style parses are not optimal for syntaxbased machine translation . we exploit a series of binarization methods to restructure the penn treebank style trees such that syntactified phrases smaller than penn treebank constituents can be acquired and exploited in translation . we find that by employing the em algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result .

chinese native language identification
we present the first application of native language identification ( nli ) to nonenglish data . motivated by theories of language transfer , nli is the task of identifying a writers native language ( l1 ) based on their writings in a second language ( the l2 ) . an nli system was applied to chinese learner texts using topicindependent syntactic models to assess their accuracy . we find that models using part-of-speech tags , context-free grammar production rules and function words are highly effective , achieving a maximum accuracy of 71 % . interestingly , we also find that when applied to equivalent english data , the model performance is almost identical . this finding suggests a systematic pattern of cross-linguistic transfer may exist , where the degree of transfer is independent of the l1 and l2 .

bucking the trend : large-scale cost-focused active learning for statistical machine translation human language technology
we explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources . the main challenge is how to buck the trend of diminishing returns that is commonly encountered . we present an active learning-style data solicitation algorithm to meet this challenge . we test it , gathering annotations via amazon mechanical turk , and find that we get an order of magnitude increase in performance rates of improvement .

search result re-ranking by feedback control adjustment for
we propose a new method to rank a special category of time-sensitive queries that are year qualified . the method adjusts the retrieval scores of a base ranking function according to time-stamps of web documents so that the freshest documents are ranked higher . our method , which is based on feedback control theory , uses ranking errors to adjust the search engine behavior . for this purpose , we use a simple but effective method to extract year qualified queries by mining query logs and a time-stamp recognition method that considers titles and urls of web documents . our method was tested on a commercial search engine . the experiments show that our approach can significantly improve relevance ranking for year qualified queries even if all the existing methods for comparison failed .

an empirical study of translation rule extraction with multiple
translation rule extraction is an important issue in syntax-based statistical machine translation ( smt ) . recent studies show that rule coverage is one of the key factors affecting the success of syntaxbased systems . in this paper , we first present a simple and effective method to improve rule coverage by using multiple parsers in translation rule extraction , and then empirically investigate the effectiveness of our method on chineseenglish translation tasks . experimental results show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 bleu points on both nist 2004 and 2005 test corpora .

incremental predictive parsing with turboparser
most approaches to incremental parsing either incur a degradation of accuracy or they have to postpone decisions , yielding underspecified intermediate output . we present an incremental predictive dependency parser that is fast , accurate , and largely language independent . by extending a state-of-the-art dependency parser , connected analyses for sentence prefixes are obtained , which even predict properties and the structural embedding of upcoming words . in contrast to other approaches , accuracy for complete sentence analyses does not decrease .

language and task independent text categorization with simple language models
we present a simple method for language independent and task independent text categorization learning , based on character-level n-gram language models . our approach uses simple information theoretic principles and achieves effective performance across a variety of languages and tasks without requiring feature selection or extensive pre-processing . to demonstrate the language and task independence of the proposed technique , we present experimental results on several languagesgreek , english , chinese and japanesein several text categorization problemslanguage identification , authorship attribution , text genre classification , and topic detection . our experimental results show that the simple approach achieves state of the art performance in each case .

language model adaptation with map estimation and the perceptron algorithm
in this paper , we contrast two language model adaptation approaches : map estimation and the perceptron algorithm . used in isolation , we show that map estimation outperforms the latter approach , for reasons which argue for combining the two approaches . when combined , the resulting system provides a 0.7 percent absolute reduction in word error rate over map estimation alone . in addition , we demonstrate that , in a multi-pass recognition scenario , it is better to use the perceptron algorithm on early pass word lattices , since the improved error rate improves acoustic model adaptation .

a log-linear block transliteration model based on bi-stream hmms
we propose a novel hmm-based framework to accurately transliterate unseen named entities . the framework leverages features in letteralignment and letter n-gram pairs learned from available bilingual dictionaries . letter-classes , such as vowels/non-vowels , are integrated to further improve transliteration accuracy . the proposed transliteration system is applied to out-of-vocabulary named-entities in statistical machine translation ( smt ) , and a significant improvement over traditional transliteration approach is obtained . furthermore , by incorporating an automatic spell-checker based on statistics collected from web search engines , transliteration accuracy is further improved . the proposed system is implemented within our smt system and applied to a real translation scenario from arabic to english .

experiments with a higher-order projective dependency parser
we present experiments with a dependency parsing model defined on rich factors . our model represents dependency trees with factors that include three types of relations between the tokens of a dependency and their children . we extend the projective parsing algorithm of eisner ( 1996 ) for our case , and train models using the averaged perceptron . our experiments show that considering higher-order information yields significant improvements in parsing accuracy , but comes at a high cost in terms of both time and memory consumption . in the multilingual exercise of the conll-2007 shared task ( nivre et al , 2007 ) , our system obtains the best accuracy for english , and the second best accuracies for basque and czech .

pp-attachment disambiguation using large context
prepositional phrase-attachment is a common source of ambiguity in natural language . the previous approaches use limited information to solve the ambiguity four lexical heads although humans disambiguate much better when the full sentence is available . we propose to solve the pp-attachment ambiguity with a support vector machines learning model that uses complex syntactic and semantic features as well as unsupervised information obtained from the world wide web . the system was tested on several datasets obtaining an accuracy of 93.62 % on a penn treebank-ii dataset ; 91.79 % on a framenet dataset when no manuallyannotated semantic information is provided and 92.85 % when semantic information is provided .

a filter-based approach to detect end-of-utterances from prosody in
we propose an efficient method to detect end-of-utterances from prosodic information in conversational speech . our method is based on the application of a large set of binary and ramp filters to the energy and fundamental frequency signals obtained from the speech signal . these filter responses , which can be computed very efficiently , are used as input to a learning algorithm that generates the final detector . preliminary experiments using data obtained from conversations show that an accurate classifier can be trained efficiently and that good results can be obtained without requiring a speech recognition system .

mt-equal : a toolkit for human assessment of machine translation
mt-equal ( machine translation errors , quality , alignment ) is a toolkit for human assessment of machine translation ( mt ) output . mt-equal implements three different tasks in an integrated environment : annotation of translation errors , translation quality rating ( e.g . adequacy and fluency , relative ranking of alternative translations ) , and word alignment . the toolkit is webbased and multi-user , allowing large scale and remotely managed manual annotation projects . it incorporates a number of project management functions and sophisticated progress monitoring capabilities . the implemented evaluation tasks are configurable and can be adapted to several specific annotation needs . the toolkit is open source and released under apache 2.0 license .

a software tool for teaching reading based on text-to-speech
native speakers of english who are good readers can sound out words or names from printed text , even if they have never seen them before , although they may not be conscious of the strategies they use . no tools are available today that can convey that knowledge to learners , showing them the rules that apply in english text . we have adapted the letter-to-phoneme component of a text-to-speech synthesizer to a web-based software system that can teach word decoding to non-native speakers of english , english-speaking children , and adult learners .

learning to predict engagement with a spoken dialog system in open-world settings
we describe a machine learning approach that allows an open-world spoken dialog system to learn to predict engagement intentions in situ , from interaction . the proposed approach does not require any developer supervision , and leverages spatiotemporal and attentional features automatically extracted from a visual analysis of people coming into the proximity of the system to produce models that are attuned to the characteristics of the environment the system is placed in . experimental results indicate that a system using the proposed approach can learn to recognize engagement intentions at low false positive rates ( e.g . 2-4 % ) up to 3-4 seconds prior to the actual moment of engagement .

phonological constraints and morphological preprocessing for ibm deutschland entwicklung
grapheme-to-phoneme conversion ( g2p ) is a core component of any text-to-speech system . we show that adding simple syllabification and stress assignment constraints , namely one nucleus per syllable and one main stress per word , to a joint n-gram model for g2p conversion leads to a dramatic improvement in conversion accuracy . secondly , we assessed morphological preprocessing for g2p conversion . while morphological information has been incorporated in some past systems , its contribution has never been quantitatively assessed for german . we compare the relevance of morphological preprocessing with respect to the morphological segmentation method , training set size , the g2p conversion algorithm , and two languages , english and german .

assessing readability of italian texts with a view to text simplification
in this paper , we propose a new approach to readability assessment with a specific view to the task of text simplification : the intended audience includes people with low literacy skills and/or with mild cognitive impairment . readit represents the first advanced readability assessment tool for what concerns italian , which combines traditional raw text features with lexical , morpho-syntactic and syntactic information . in readit readability assessment is carried out with respect to both documents and sentences where the latter represents an important novelty of the proposed approach creating the prerequisites for aligning the readability assessment step with the text simplification process . readit shows a high accuracy in the document classification task and promising results in the sentence classification scenario .

bilingual experiments on an opinion comparable corpus
up until now most of the methods published for polarity classification are applied to english texts . however , other languages on the internet are becoming increasingly important . this paper presents a set of experiments on english and spanish product reviews . using a comparable corpus , a supervised method and two unsupervised methods have been assessed . furthermore , a list of spanish opinion words is presented as a valuable resource .

learning adaptable patterns for passage reranking
this paper proposes passage reranking models that ( i ) do not require manual feature engineering and ( ii ) greatly preserve accuracy , when changing application domain . their main characteristic is the use of relational semantic structures representing questions and their answer passages . the relations are established using information from automatic classifiers , i.e. , question category ( qc ) and focus classifiers ( fc ) and named entity recognizers ( ner ) . this way ( i ) effective structural relational patterns can be automatically learned with kernel machines ; and ( ii ) structures are more invariant w.r.t . different domains , thus fostering adaptability .

intrinsic versus extrinsic evaluations of parsing systems division of informatics
a wide range of parser and/or grammar evaluation methods have been reported in the literature . however , in most cases these evaluations take the parsers independently ( intrinsic evaluations ) , and only in a few cases has the effect of different parsers in real applications been measured ( extrinsic evaluations ) . this paper compares two evaluations of the link grammar parser and the conexor functional dependency grammar parser . the parsing systems , despite both being dependency-based , return different types of dependencies , making a direct comparison impossible . in the intrinsic evaluation , the accuracy of the parsers is compared independently by converting the dependencies into grammatical relations and using the methodology of carroll et al ( 1998 ) for parser comparison . in the extrinsic evaluation , the parsers impact in a practical application is compared within the context of answer extraction . the differences in the results are significant .

a description of tunable machine translation evaluation systems in
this paper is to describe our machine translation evaluation systems used for participation in the wmt13 shared metrics task . in the metrics task , we submitted two automatic mt evaluation systems nlepor_baseline and lepor_v3.1 . nlepor_baseline is an n-gram based language independent mt evaluation metric employing the factors of modified sentence length penalty , position difference penalty , n-gram precision and n-gram recall . nlepor_baseline measures the similarity of the system output translations and the reference translations only on word sequences . lepor_v3.1 is a new version of lepor metric using the mathematical harmonic mean to group the factors and employing some linguistic features , such as the part-of-speech information . the evaluation results of wmt13 show lepor_v3.1 yields the highest averagescore 0.86 with human judgments at systemlevel using pearson correlation criterion on english-to-other ( fr , de , es , cs , ru ) language pairs .

classifier combination for contextual idiom detection
we propose a novel unsupervised approach for distinguishing literal and non-literal use of idiomatic expressions . our model combines an unsupervised and a supervised classifier . the former bases its decision on the cohesive structure of the context and labels training data for the latter , which can then take a larger feature space into account . we show that a combination of both classifiers leads to significant improvements over using the unsupervised classifier alone .

evaluation metrics for the lexical substitution
we identify some problems of the evaluation metrics used for the english lexical substitution task of semeval-2007 , and propose alternative metrics that avoid these problems , which we hope will better guide the future development of lexical substitution systems .

extracting transfer rules for multiword expressions from parallel corpora division of linguistics and multilingual studies ,
this paper presents a procedure for extracting transfer rules for multiword expressions from parallel corpora for use in a rule based japanese-english mt system . we show that adding the multi-word rules improves translation quality and sketch ideas for learning more such rules .

an automated english scoring system
this paper explores an issue of redundant errors reported while automatically scoring english learners sentences . we use a human-computer collaboration approach to eliminate redundant errors . the first step is to automatically select candidate redundant errors using pmi and rfc . since those errors are detected with different ids although they represent the same error , the candidacy can not be confirmed automatically . the errors are then handed over to human experts to determine the candidacy . the final candidates are provided to the system and trained with a decision tree . with those redundant errors eliminated , the system accuracy has been improved .

the prosodic transcription of a corpus of hong kong english
this paper describes the prosodic transcription of a corpus of hong kong english and some preliminary findings on the communicative role of intonation in hong kong english .

deep linguistic multilingual translation and bilingual dictionaries
this paper describes the multra project , aiming at the development of an efficient multilingual translation technology based on an abstract and generic linguistic model as well as on object-oriented software design . in particular , we will address the issue of the rapid growth both of the transfer modules and of the bilingual databases . for the latter , we will show that a significant part of bilingual lexical databases can be derived automatically through transitivity , with corpus validation .

emotion analysis using latent affective folding and embedding speech & language technologies
though data-driven in nature , emotion analysis based on latent semantic analysis still relies on some measure of expert knowledge in order to isolate the emotional keywords or keysets necessary to the construction of affective categories . this makes it vulnerable to any discrepancy between the ensuing taxonomy of affective states and the underlying domain of discourse . this paper proposes a more general strategy which leverages two distincts semantic levels , one that encapsulates the foundations of the domain considered , and one that specifically accounts for the overall affective fabric of the language . exposing the emergent relationship between these two levels advantageously informs the emotion classification process . empirical evidence suggests that this is a promising solution for automatic emotion detection in text .

some statistical methods for evaluating information extraction systems in the social sciences
we present new statistical methods for evaluating information extraction systems . the methods were developed to evaluate a system used by political scientists to extract event information from news leads about international politics . the nature of this data presents two problems for evaluators : 1 ) the frequency distribution of event types in international event data is strongly skewed , so a random sample of newsleads will typically fail to contain any low frequency events . 2 ) manual information extraction necessary to create evaluation sets is costly , and most effort is wasted coding high frequency categories . we present an evaluation scheme that overcomes these problems with considerably less manual effort than traditional methods , and also allows us to interpret an information extraction system as an estimator ( in the statistical sense ) and to estimate its bias .

a hybrid hierarchical model for multi-document summarization
scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization . in this paper , we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference . we calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model . then , using these scores , we train a regression model based on the lexical and structural characteristics of the sentences , and use the model to score sentences of new documents to form a summary . our system advances current state-of-the-art improving rouge scores by 7 % . generated summaries are less redundant and more coherent based upon manual quality evaluations .

indexing student essays paragraphs using lsa over an integrated ontological
a full understanding of text is out of reach of current human language technology . however , a shallow natural language processing ( nlp ) approach can be used to provide automated help in the evaluation of essays . the main idea of this paper is that latent semantic indexing ( lsa ) can be used in conjunction with ontologies and first order logic ( fol ) to locate segments relevant to a question in a student essay . our test bed , in a first instance , is a set of ontologies such the akt reference ontology ( describing academic life ) , newspaper and a koala ontology ( concerning koalas habitat ) .

revision learning and its application to part-of-speech tagging
this paper presents a revision learning method that achieves high performance with small computational cost by combining a model with high generalization capacity and a model with small computational cost . this method uses a high capacity model to revise the output of a small cost model . we apply this method to english partof-speech tagging and japanese morphological analysis , and show that the method performs well .

finding similar sentences across multiple languages in wikipedia
we investigate whether the wikipedia corpus is amenable to multilingual analysis that aims at generating parallel corpora . we present the results of the application of two simple heuristics for the identification of similar text across multiple languages in wikipedia . despite the simplicity of the methods , evaluation carried out on a sample of wikipedia pages shows encouraging results .

towards semi-supervised classification of discourse relations using
two of the main corpora available for training discourse relation classifiers are the rst discourse treebank ( rst-dt ) and the penn discourse treebank ( pdtb ) , which are both based on the wall street journal corpus . most recent work using discourse relation classifiers have employed fully-supervised methods on these corpora . however , certain discourse relations have little labeled data , causing low classification performance for their associated classes . in this paper , we attempt to tackle this problem by employing a semi-supervised method for discourse relation classification . the proposed method is based on the analysis of feature cooccurrences in unlabeled data . this information is then used as a basis to extend the feature vectors during training . the proposed method is evaluated on both rst-dt and pdtb , where it significantly outperformed baseline classifiers . we believe that the proposed method is a first step towards improving classification performance , particularly for discourse relations lacking annotated data .

detecting health related discussions in everyday telephone conversations for studying medical events in the lives of older adults
we apply semi-supervised topic modeling techniques to detect health-related discussions in everyday telephone conversations , which has applications in large-scale epidemiological studies and for clinical interventions for older adults . the privacy requirements associated with utilizing everyday telephone conversations preclude manual annotations ; hence , we explore semi-supervised methods in this task . we adopt a semi-supervised version of latent dirichlet allocation ( lda ) to guide the learning process . within this framework , we investigate a strategy to discard irrelevant words in the topic distribution and demonstrate that this strategy improves the average f-score on the in-domain task and an out-of-domain task ( fisher corpus ) . our results show that the increase in discussion of health related conversations is statistically associated with actual medical events obtained through weekly selfreports .

ecient dialogue strategy to find users intended items from information query results
we address a dialogue framework that narrows down the users query results obtained by an information retrieval system . the followup dialogue to constrain query results is signicant especially with the speech interfaces such as telephones because a lot of query results can not be presented to the user . the proposed dialogue framework generates guiding questions based on an information theoretic criterion to eliminate retrieved candidates by a spontaneous query without assuming a semantic slot structure . we describe its concept on general information query tasks and then deal with a query task on the appliance manual , where structured task knowledge is available . a hierarchical confirmation strategy is proposed by making use of a tree structure of the manual and then three cost functions for selecting optimal question nodes are compared . experimental  evaluation demon strates that the proposed system helps users nd their intended items more eciently

the god model alfio massimiliano gliozzo
god ( general ontology discovery ) is an unsupervised system to extract semantic relations among domain specific entities and concepts from texts . operationally , it acts as a search engine returning a set of true predicates regarding the query instead of the usual ranked list of relevant documents . our approach relies on two basic assumptions : ( i ) paradigmatic relations can be established only among terms in the same semantic domain an ( ii ) they can be inferred from texts by analyzing the subject-verb-object patterns where two domain specific terms co-occur . a qualitative analysis of the system output shows that god provide true , informative and meaningful relations in a very efficient way .

browsing help for faster document retrieval
in this paper , the search engine intuition is described . it allows the user to navigate through the documents retrieved with a given query . several browse help functions are provided by the engine and described here : conceptualisation , named entities , similar documents and entity visualization . they intend to save the users time . in order to evaluate the amount of time these features can save , an evaluation was made . it involves 6 users , 18 queries and the corpus is made of 16 years of the newspaper le monde . the results show that , with the different features , a user get faster to the needed information . fewer non-relevant documents are read ( filtering ) and more relevant documents are retrieved in less time .

toward multimedia : a string pattern-based passage ranking model for video question answering
in this paper , we present a new string pattern matching-based passage ranking algorithm for extending traditional textbased qa toward videoqa . users interact with our videoqa system through natural language questions , while our system returns passage fragments with corresponding video clips as answers . we collect 75.6 hours videos and 253 chinese questions for evaluation . the experimental results showed that our method outperformed six top-performed ranking models . it is 10.16 % better than the second best method ( language model ) in relatively mrr score and 6.12 % in precision rate . besides , we also show that the use of a trained chinese word segmentation tool did decrease the overall videoqa performance where most ranking algorithms dropped at least 10 % in relatively mrr , precision , and answer pattern recall rates .

using unknown word techniques to learn known words
unknown words are a hindrance to the performance of hand-crafted computational grammars of natural language . however , words with incomplete and incorrect lexical entries pose an even bigger problem because they can be the cause of a parsing failure despite being listed in the lexicon of the grammar . such lexical entries are hard to detect and even harder to correct . we employ an error miner to pinpoint words with problematic lexical entries . an automated lexical acquisition technique is then used to learn new entries for those words which allows the grammar to parse previously uncovered sentences successfully . we test our method on a large-scale grammar of dutch and a set of sentences for which this grammar fails to produce a parse . the application of the method enables the grammar to cover 83.76 % of those sentences with an accuracy of 86.15 % .

an ensemble of grapheme and phoneme for machine transliteration
or words in one alphabetical system for the corresponding characters in another alphabetical system . there has been increasing concern on machine transliteration as an assistant of machine translation and information retrieval . three machine transliteration models , including grapheme-based model , phonemebased model , and hybrid model , have been proposed . however , there are few works trying to make use of correspondence between source grapheme and phoneme , although the correspondence plays an important role in machine transliteration . furthermore there are few works , which dynamically handle source grapheme and phoneme . in this paper , we propose a new transliteration model based on an ensemble of grapheme and phoneme . our model makes use of the correspondence and dynamically uses source grapheme and phoneme . our method shows better performance than the previous works about 15~23 % in english-to-korean transliteration and about 15~43 % in english-to-japanese transliteration .

answer validation by keyword association
answer validation is a component of question answering system , which selects reliable answer from answer candidates extracted by certain methods . in this paper , we propose an approach of answer validation based on the strengths of lexical association between the keywords extracted from a question sentence and each answer candidate . the proposed answer validation process is decomposed into two steps : the first is to extract appropriate keywords from a question sentence using word features and the strength of lexical association , while the second is to estimate the strength of the association between the keywords and an answer candidate based on the hits of search engines . in the result of experimental evaluation , we show that a good proportion ( 79 % ) of a multiple-choice quiz who wants to be a millionaire can be solved by the proposed method .

developing an interlingual translation lexicon using wordnets and grammatical framework shafqat mumtaz virk
the grammatical framework ( gf ) offers perfect translation between controlled subsets of natural languages . e.g. , an abstract syntax for a set of sentences in school mathematics is the interlingua between the corresponding sentences in english and hindi , say . gf resource grammars specify how to say something in english or hindi ; these are reused with application grammars that specify what can be said ( mathematics , tourist phrases , etc . ) . more recent robust parsing and parse-tree disambiguation allow gf to parse arbitrary english text . we report here an experiment to linearise the resulting tree directly to other languages ( e.g . hindi , german , etc . ) , i.e. , we use a languageindependent resource grammar as the interlingua . we focus particularly on the last part of the translation system , the interlingual lexicon and word sense disambiguation ( wsd ) . we improved the quality of the wide coverage interlingual translation lexicon by using the princeton and universal wordnet data . we then integrated an existing wsd tool and replaced the usual gf style lexicons , which give one target word per source word , by the wordnet based lexicons .

the relative divergence of dutch dialect pronunciations from their common source : an exploratory study
in this paper we use the reeks nederlandse dialectatlassen as a source for the reconstruction of a proto-language of dutch dialects . we used 360 dialects from locations in the netherlands , the northern part of belgium and french-flanders . the density of dialect locations is about the same everywhere . for each dialect we reconstructed 85 words . for the reconstruction of vowels we used knowledge of dutch history , and for the reconstruction of consonants we used well-known tendencies found in most textbooks about historical linguistics . we validated results by comparing the reconstructed forms with pronunciations according to a proto-germanic dictionary ( kbler , 2003 ) . for 46 % of the words we reconstructed the same vowel or the closest possible vowel when the vowel to be reconstructed was not found in the dialect material . for 52 % of the words all consonants we reconstructed were the same . for 42 % of the words , only one consonant was differently reconstructed . we measured the divergence of dutch dialects from their proto-language .

corporate language resources in multilingual content creation ,
this paper focuses on how language resources ( lr ) for translation ( hence lr4trans ) feature , and should ideally feature , within a corporate workflow of multilingual content development . the envisaged scenario will be that of a content management system that acknowledges the value of lr4trans in the organisation as a key component and corporate knowledge resource .

automatic discovery of named entity variants grammar-driven approaches to non-alphabetical transliterations
identification of transliterated names is a particularly difficult task of named entity recognition ( ner ) , especially in the chinese context . of all possible variations of transliterated named entities , the difference between prc and taiwan is the most prevalent and most challenging . in this paper , we introduce a novel approach to the automatic extraction of diverging transliterations of foreign named entities by bootstrapping cooccurrence statistics from tagged and segmented chinese corpus . preliminary experiment yields promising results and shows its potential in nlp applications .

monolingual distributional profiles for word substitution in machine
out-of-vocabulary ( oov ) words present a significant challenge for machine translation . for low-resource languages , limited training data increases the frequency of oov words and this degrades the quality of the translations . past approaches have suggested using stems or synonyms for oov words . unlike the previous methods , we show how to handle not just the oov words but rare words as well in an example-based machine translation ( ebmt ) paradigm . presence of oov words and rare words in the input sentence prevents the system from finding longer phrasal matches and produces low quality translations due to less reliable language model estimates . the proposed method requires only a monolingual corpus of the source language to find candidate replacements . a new framework is introduced to score and rank the replacements by efficiently combining features extracted for the candidate replacements . a lattice representation scheme allows the decoder to select from a beam of possible replacement candidates . the new framework gives statistically significant improvements in english-chinese and english-haitian translation systems .

timeline generation through evolutionary trans-temporal summarization
we investigate an important and challenging problem in summary generation , i.e. , evolutionary trans-temporal summarization ( etts ) , which generates news timelines from massive data on the internet . etts greatly facilitates fast news browsing and knowledge comprehension , and hence is a necessity . given the collection of time-stamped web documents related to the evolving news , etts aims to return news evolution along the timeline , consisting of individual but correlated summaries on each date . existing summarization algorithms fail to utilize trans-temporal characteristics among these component summaries . we propose to model trans-temporal correlations among component summaries for timelines , using inter-date and intra-date sentence dependencies , and present a novel combination . we develop experimental systems to compare 5 rival algorithms on 6 instinctively different datasets which amount to 10251 documents . evaluation results in rouge metrics indicate the effectiveness of the proposed approach based on trans-temporal information .

extracting clinical relationships from patient narratives
the clinical e-science framework ( clef ) project has built a system to extract clinically significant information from the textual component of medical records , for clinical research , evidence-based healthcare and genotype-meets-phenotype informatics . one part of this system is the identification of relationships between clinically important entities in the text . typical approaches to relationship extraction in this domain have used full parses , domain-specific grammars , and large knowledge bases encoding domain knowledge . in other areas of biomedical nlp , statistical machine learning approaches are now routinely applied to relationship extraction . we report on the novel application of these statistical techniques to clinical relationships . we describe a supervised machine learning system , trained with a corpus of oncology narratives hand-annotated with clinically important relationships . various shallow features are extracted from these texts , and used to train statistical classifiers . we compare the suitability of these features for clinical relationship extraction , how extraction varies between inter- and intra-sentential relationships , and examine the amount of training data needed to learn various relationships .

gender inference of twitter users in non-english contexts
while much work has considered the problem of latent attribute inference for users of social media such as twitter , little has been done on non-english-based content and users . here , we conduct the first assessment of latent attribute inference in languages beyond english , focusing on gender inference . we find that the gender inference problem in quite diverse languages can be addressed using existing machinery . further , accuracy gains can be made by taking language-specific features into account . we identify languages with complex orthography , such as japanese , as difficult for existing methods , suggesting a valuable direction for future research .

tier-based strictly local constraints for phonology
beginning with goldsmith ( 1976 ) , the phonological tier has a long history in phonological theory to describe non-local phenomena . this paper defines a class of formal languages , the tier-based strictly local languages , which begin to describe such phenomena . then this class is located within the subregular hierarchy ( mcnaughton and papert , 1971 ) . it is found that these languages contain the strictly local languages , are star-free , are incomparable with other known sub-star-free classes , and have other interesting properties .

learning to shift the polarity of words for sentiment classification daisuke ikeda hiroya takamura lev-arie ratinov manabu okumura
we propose a machine learning based method of sentiment classification of sentences using word-level polarity . the polarities of words in a sentence are not always the same as that of the sentence , because there can be polarity-shifters such as negation expressions . the proposed method models the polarity-shifters . our model can be trained in two different ways : word-wise and sentence-wise learning . in sentence-wise learning , the model can be trained so that the prediction of sentence polarities should be accurate . the model can also be combined with features used in previous work such as bag-of-words and n-grams . we empirically show that our method almost always improves the performance of sentiment classification of sentences especially when we have only small amount of training data .

part-of-speech tagging for gujarati using conditional random
this paper describes a machine learning algorithm for gujarati part of speech tagging . the machine learning part is performed using a crf model . the features given to crf are properly chosen keeping the linguistic aspect of gujarati in mind . as gujarati is currently a less privileged language in the sense of being resource poor , manually tagged data is only around 600 sentences . the tagset contains 26 different tags which is the standard indian language ( il ) tagset . both tagged ( 600 sentences ) and untagged ( 5000 sentences ) are used for learning . the algorithm has achieved an accuracy of 92 % for gujarati texts where the training corpus is of 10,000 words and the test corpus is of 5,000 words .

building emotion lexicon from weblog corpora
an emotion lexicon is an indispensable resource for emotion analysis . this paper aims to mine the relationships between words and emotions using weblog corpora . a collocation model is proposed to learn emotion lexicons from weblog articles . emotion classification at sentence level is experimented by using the mined lexicons to demonstrate their usefulness .

distortion model considering rich context for statistical machine translation isao goto , masao utiyama eiichiro sumita akihiro tamura sadao kurohashi
this paper proposes new distortion models for phrase-based smt . in decoding , a distortion model estimates the source word position to be translated next ( np ) given the last translated source word position ( cp ) . we propose a distortion model that can consider the word at the cp , a word at an np candidate , and the context of the cp and the np candidate simultaneously . moreover , we propose a further improved model that considers richer context by discriminating label sequences that specify spans from the cp to np candidates . it enables our model to learn the effect of relative word order among np candidates as well as to learn the effect of distances from the training data . in our experiments , our model improved 2.9 bleu points for japanese-english and 2.6 bleu points for chinese-english translation compared to the lexical reordering models .

greetings generation in video role playing games
we present first results of our project on the generation of contextually adequate greeting exchanges in video role playing games . to make greeting exchanges computable , an analysis of the factors influencing greeting behavior as well as the factors influencing greeting exchanges is given . based on the politeness model proposed by brown & levinson ( 1987 ) we develop a simple algorithm for the generation of greeting exchanges . an evaluation , comparing dialog from the video role playing game skyrim to dialog determined by our algorithm , shows that our algorithm is able to generate greeting exchanges that are contextually more adequate than those featured by skyrim .

minimum bayes-risk system combination alfons juan francisco casacuberta
we present minimum bayes-risk system combination , a method that integrates consensus decoding and system combination into a unified multi-system minimum bayes-risk ( mbr ) technique . unlike other mbr methods that re-rank translations of a single smt system , mbr system combination uses the mbr decision rule and a linear combination of the component systems probability distributions to search for the minimum risk translation among all the finite-length strings over the output vocabulary . we introduce expected bleu , an approximation to the bleu score that allows to efficiently apply mbr in these conditions . mbr system combination is a general method that is independent of specific smt models , enabling us to combine systems with heterogeneous structure . experiments show that our approach bring significant improvements to single-system-based mbr decoding and achieves comparable results to different state-of-the-art system combination methods .

efficient logical inference for semantic processing yusuke miyao takuya matsuzaki
dependency-based compositional semantics ( dcs ) provides a precise and expressive way to model semantics of natural language queries on relational databases , by simple dependency-like trees . recently abstract denotation is proposed to enable generic logical inference on dcs . in this paper , we discuss some other possibilities to equip dcs with logical inference , and we discuss further on how logical inference can help textual entailment recognition , or other semantic precessing tasks .

usna : a dual-classifier approach to contextual sentiment analysis
this paper describes a dual-classifier approach to contextual sentiment analysis at the semeval-2013 task 2. contextual analysis of polarity focuses on a word or phrase , rather than the broader task of identifying the sentiment of an entire text . the task 2 definition includes target word spans that range in size from a single word to entire sentences . however , the context of a single word is dependent on the words surrounding syntax , while a phrase contains most of the polarity within itself . we thus describe separate treatment with two independent classifiers , outperforming the accuracy of a single classifier . our system ranked 6th out of 19 teams on sms message classification , and 8th of 23 on twitter data . we also show a surprising result that a very small amount of word context is needed for high-performance polarity extraction .

part of speech tagging for mongolian corpus
this paper introduces the current result of a research work which aims to build a 5 million tagged word corpus for mongolian . currently , around 1 million words have been automatically tagged by developing a pos tagset and a bigram pos tagger .

conditional random fields and support vector machines for disorder named entity recognition in clinical texts
we present a comparative study between two machine learning methods , conditional random fields and support vector machines for clinical named entity recognition . we explore their applicability to clinical domain . evaluation against a set of gold standard named entities shows that crfs outperform svms . the best f-score with crfs is 0.86 and for the svms is 0.64 as compared to a baseline of 0.60 .

aggregating machine learning and rule based heuristics for named karthik gali , harshit surana , ashwini vaidya , praneeth shishtla and dipti misra sharma
this paper , submitted as an entry for the nersseal-2008 shared task , describes a system build for named entity recognition for south and south east asian languages . our paper combines machine learning techniques with language specific heuristics to model the problem of ner for indian languages . the system has been tested on five languages : telugu , hindi , bengali , urdu and oriya . it uses crf ( conditional random fields ) based machine learning , followed by post processing which involves using some heuristics or rules . the system is specifically tuned for hindi and telugu , we also report the results for the other four languages .

evangelising language technology : a practically-focussed undergraduate program
this paper describes an undergraduate program in language technology that we have developed at macquarie university . we question the industrial relevance of much that is taught in nlp courses , and emphasize the need for a practical orientation as a means to growing the size of the field . we argue that a more evangelical approach , both with regard to students and industry , is required . the paper provides an overview of the material we cover , and makes some observations for the future on the basis of our experiences so far .

semantic similarity : what for
linguistic similarity has been a prominent notion and tool in computational linguistics and related areas , as elaborated nicely in the announcement of this workshop . yet , what exactly counts as similarity , or when two linguistic concepts should be regarded as similar , often remains rather vague and ill posed , which is in fact quite typical for unsupervised notions . this talk will focus on similarity at the semantic level , and will explore the perspective that different notions of similarity may be defined relative to concrete modeling goals . in particular , i will refer to the two major goals in semantic modeling : predicting likelihood of occurrence , which is the typical goal in disambiguation and language modeling , and recognizing target meanings , which is the typical semantic goal in text understanding applications such as question answering , information extraction , summarization and information retrieval . we will discuss each goal and present corresponding semantic similarity approaches . 7

is it the right answer exploiting web redundancy for answer validation itc-irst , centro per la ricerca scientifica e tecnologica
answer validation is an emerging topic in question answering , where open domain systems are often required to rank huge amounts of candidate answers . we present a novel approach to answer validation based on the intuition that the amount of implicit knowledge which connects an answer to a question can be quantitatively estimated by exploiting the redundancy of web information . experiments carried out on the trec-2001 judged-answer collection show that the approach achieves a high level of performance ( i.e . 81 % success rate ) . the simplicity and the efficiency of this approach make it suitable to be used as a module in question answering systems .

augmenting the kappa statistic to determine interannotator reliability
this paper describes a method for evaluating interannotator reliability in an email corpus annotated for type ( e.g. , question , answer , social chat ) when annotators are allowed to assign multiple labels to a message . an augmentation is proposed to cohens kappa statistic which permits all data to be included in the reliability measure and which further permits the identification of more or less reliably annotated data points .

were not in kansas anymore : detecting domain changes in streams
domain adaptation , the problem of adapting a natural language processing system trained in one domain to perform well in a different domain , has received significant attention . this paper addresses an important problem for deployed systems that has received little attention detecting when such adaptation is needed by a system operating in the wild , i.e. , performing classification over a stream of unlabeled examples . our method uses adistance , a metric for detecting shifts in data streams , combined with classification margins to detect domain shifts . we empirically show effective domain shift detection on a variety of data sets and shift conditions .

optimizing chinese word segmentation for machine translation
previous work has shown that chinese word segmentation is useful for machine translation to english , yet the way different segmentation strategies affect mt is still poorly understood . in this paper , we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better mt performance . we find that other factors such as segmentation consistency and granularity of chinese words can be more important for machine translation . based on these findings , we implement methods inside a conditional random field segmenter that directly optimize segmentation granularity with respect to the mt task , providing an improvement of 0.73 bleu . we also show that improving segmentation consistency using external lexicon and proper noun features yields a 0.32 bleu increase .

domain adaptation with artificial data for semantic parsing of speech lonneke van der plas
we adapt a semantic role parser to the domain of goal-directed speech by creating an artificial treebank from an existing text treebank . we use a three-component model that includes distributional models from both target and source domains . we show that we improve the parsers performance on utterances collected from human-machine dialogues by training on the artificially created data without loss of performance on the text treebank .

automatically restructuring practice guidelines using the gem dtd amanda bouffier thierry poibeau
this paper describes a system capable of semi-automatically filling an xml template from free texts in the clinical domain ( practice guidelines ) . the xml template includes semantic information not explicitly encoded in the text ( pairs of conditions and actions/recommendations ) . therefore , there is a need to compute the exact scope of conditions over text sequences expressing the required actions . we present a system developed for this task . we show that it yields good performance when applied to the analysis of french practice guidelines .

question classification using hdag kernel
this paper proposes a machine learning based question classification method using a kernel function , hierarchical directed acyclic graph ( hdag ) kernel . the hdag kernel directly accepts structured natural language data , such as several levels of chunks and their relations , and computes the value of the kernel function at a practical cost and time while reflecting all of these structures . we examine the proposed method in a question classification experiment using 5011 japanese questions that are labeled by 150 question types . the results demonstrate that our proposed method improves the performance of question classification over that by conventional methods such as bag-of-words and their combinations .

zero subject detection for polish
this article reports on the first machine learning experiments on detection of null subjects in polish . it emphasizes the role of zero subject detection as the part of mention detection the initial step of endto-end coreference resolution . anaphora resolution is not studied in this article .

from temporal expressions to temporal information : semantic tagging of news messages
we present a semantic tagging system for temporal expressions and discuss how the temporal information conveyed by these expressions can be extracted . the performance of the system was evaluated wrt . a small hand-annotated corpus of news messages .

multi-level annotation in mmax
we present a light-weight tool for the annotation of linguistic data on multiple levels . it is based on the simplification of annotations to sets of markables having attributes and standing in certain relations to each other . we describe the main features of the tool , emphasizing its simplicity , customizability and versatility .

ontolexical resources for feature based opinion mining :
opinion mining is a growing research area both at the natural language processing and the information retrieval communities . companies , politicians , as well as customers need powerful tools to track opinions , sentiments , judgments and beliefs that people may express in blogs , reviews , audios and videos data regarding a product/service/person/organisation/etc . this work describes our contribution to feature based opinion mining where opinions expressed towards each feature of an object or a product are extracted and summarized . the state of the art has shown that the hierarchical organization of features is a key step . in this context , our goal is to study the role of a domain ontology to structure and extract object features as well as to produce a comprehensive summary . this paper presents the developed system and the experiments we carried out on a case study : french restaurant reviews . our results show that our approach outperforms standard baselines .

improving graph-based dependency parsing with decision history
this paper proposes an approach to improve graph-based dependency parsing by using decision history . we introduce a mechanism that considers short dependencies computed in the earlier stages of parsing to improve the accuracy of long dependencies in the later stages . this relies on the fact that short dependencies are generally more accurate than long dependencies in graph-based models and may be used as features to help parse long dependencies . the mechanism can easily be implemented by modifying a graphbased parsing model and introducing a set of new features . the experimental results show that our system achieves state-ofthe-art accuracy on the standard ptb test set for english and the standard penn chinese treebank ( ctb ) test set for chinese .

cut the noise : mutually reinforcing reordering and alignments for improved machine translation
preordering of a source language sentence to match target word order has proved to be useful for improving machine translation systems . previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation performance . in this paper , we focus on further improving the performance of the reordering model ( and thereby machine translation ) by using a larger corpus of sentence aligned data for which manual word alignments are not available but automatic machine generated alignments are available . the main challenge we tackle is to generate quality data for training the reordering model in spite of the machine alignments being noisy . to mitigate the effect of noisy machine alignments , we propose a novel approach that improves reorderings produced given noisy alignments and also improves word alignments using information from the reordering model . this approach generates alignments that are 2.6 f-measure points better than a baseline supervised aligner . the data generated allows us to train a reordering model that gives an improvement of 1.8 bleu points on the nist mt-08 urdu-english evaluation set over a reordering model that only uses manual word alignments , and a gain of 5.2 bleu points over a standard phrase-based baseline .

learning sense-specific word embeddings by exploiting
recent work has shown success in learning word embeddings with neural network language models ( nnlm ) . however , the majority of previous nnlms represent each word with a single embedding , which fails to capture polysemy . in this paper , we address this problem by representing words with multiple and sense-specific embeddings , which are learned from bilingual parallel data . we evaluate our embeddings using the word similarity measurement and show that our approach is significantly better in capturing the sense-level word similarities . we further feed our embeddings as features in chinese named entity recognition and obtain noticeable improvements against single embeddings .

improving automatic speech recognition for lectures through transformation-based rules learned from minimal data
we demonstrate that transformation-based learning can be used to correct noisy speech recognition transcripts in the lecture domain with an average word error rate reduction of 12.9 % . our method is distinguished from earlier related work by its robustness to small amounts of training data , and its resulting efficiency , in spite of its use of true word error rate computations as a rule scoring function .

predicting fine-grained social roles with selectional preferences
selectional preferences , the tendencies of predicates to select for certain semantic classes of arguments , have been successfully applied to a number of tasks in computational linguistics including word sense disambiguation , semantic role labeling , relation extraction , and textual inference . here we leverage the information encoded in selectional preferences to the task of predicting fine-grained categories of authors on the social media platform twitter . first person uses of verbs that select for a given social role as subject ( e.g . i teach ... for teacher ) are used to quickly build up binary classifiers for that role .

statistical representation of grammaticality judgements
we use a set of enriched n-gram models to track grammaticality judgements for different sorts of passive sentences in english . we construct these models by specifying scoring functions to map the log probabilities ( logprobs ) of an n-gram model for a test set of sentences onto scores which depend on properties of the string related to the parameters of the model . we test our models on classification tasks for different kinds of passive sentences . our experiments indicate that our n-gram models achieve high accuracy in identifying ill-formed passives in which ill-formedness depends on local relations within the n-gram frame , but they are far less successful in detecting non-local relations that produce unacceptability in other types of passive construction . we take these results to indicate some of the strengths and the limitations of word and lexical class n-gram models as candidate representations of speakers grammatical knowledge .

bilingual sense similarity for statistical machine translation
this paper proposes new algorithms to compute the sense similarity between two units ( words , phrases , rules , etc . ) from parallel corpora . the sense similarity scores are computed by using the vector space model . we then apply the algorithms to statistical machine translation by computing the sense similarity between the source and target side of translation rule pairs . similarity scores are used as additional features of the translation model to improve translation performance . significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system .

hultech : a general purpose system for cross-level semantic similarity based on anchor web counts
this paper describes the hultech team participation in task 3 of semeval-2014 . four different subtasks are provided to the participants , who are asked to determine the semantic similarity of cross-level test pairs : paragraphto-sentence , sentence-to-phrase , phrase-toword and word-to-sense . our system adopts a unified strategy ( general purpose system ) to calculate similarity across all subtasks based on word web frequencies . for that purpose , we define clueweb infosimba , a cross-level similarity corpus-based metric . results show that our strategy overcomes the proposed baselines and achieves adequate to moderate results when compared to other systems .

experimenting with transitive verbs in a discocat
formal and distributional semantic models offer complementary benefits in modeling meaning . the categorical compositional distributional model of meaning of coecke et al ( 2010 ) ( abbreviated to discocat in the title ) combines aspects of both to provide a general framework in which meanings of words , obtained distributionally , are composed using methods from the logical setting to form sentence meaning . concrete consequences of this general abstract setting and applications to empirical data are under active study ( grefenstette et al , 2011 ; grefenstette and sadrzadeh , 2011 ) . in this paper , we extend this study by examining transitive verbs , represented as matrices in a discocat . we discuss three ways of constructing such matrices , and evaluate each method in a disambiguation task developed by grefenstette and sadrzadeh ( 2011 ) .

exponential reservoir sampling for streaming language models
we show how rapidly changing textual streams such as twitter can be modelled in fixed space . our approach is based upon a randomised algorithm called exponential reservoir sampling , unexplored by this community until now . using language models over twitter and newswire as a testbed , our experimental results based on perplexity support the intuition that recently observed data generally outweighs that seen in the past , but that at times , the past can have valuable signals enabling better modelling of the present .

patterns of importance variation in spoken dialog
some things people say are more important , and some less so . importance varies from moment to moment in spoken dialog , and contextual prosodic features and patterns signal this . a simple linear regression model over such features gave estimates that correlated well , 0.83 , with human importance judgments .

joint training of dependency parsing filters through latent support vector machines
graph-based dependency parsing can be sped up significantly if implausible arcs are eliminated from the search-space before parsing begins . state-of-the-art methods for arc filtering use separate classifiers to make pointwise decisions about the tree ; they label tokens with roles such as root , leaf , or attaches-tothe-left , and then filter arcs accordingly . because these classifiers overlap substantially in their filtering consequences , we propose to train them jointly , so that each classifier can focus on the gaps of the others . we integrate the various pointwise decisions as latent variables in a single arc-level svm classifier . this novel framework allows us to combine nine pointwise filters , and adjust their sensitivity using a shared threshold based on arc length . our system filters 32 % more arcs than the independently-trained classifiers , without reducing filtering speed . this leads to faster parsing with no reduction in accuracy .

empirical study of utilizing morph-syntactic information in smt
morph-syntactical information to improve translation quality . with three kinds of language pairs matched according to morph-syntactical similarity or difference , we investigate the effects of various morpho-syntactical information , such as base form , part-of-speech , and the relative positional information of a word in a statistical machine translation framework . we learn not only translation models but also word-based/class-based language models by manipulating morphological and relative positional information . and we integrate the models into a log-linear model . experiments on multilingual translations showed that such morphological information as part-of-speech and base form are effective for improving performance in morphologically rich language pairs and that the relative positional features in a word group are useful for reordering the local word orders . moreover , the use of a class-based n-gram language model improves performance by alleviating the data sparseness problem in a word-based language model .

entailment : an effective metric for comparing and evaluating hierarchical and non-hierarchical annotation schemes monojit choudhury kalika bali
hierarchical or nested annotation of linguistic data often co-exists with simpler non-hierarchical or flat counterparts , a classic example being that of annotations used for parsing and chunking . in this work , we propose a general strategy for comparing across these two schemes of annotation using the concept of entailment that formalizes a correspondence between them . we use crowdsourcing to obtain query and sentence chunking and show that entailment can not only be used as an effective evaluation metric to assess the quality of annotations , but it can also be employed to filter out noisy annotations .

lightly-supervised word sense translation error detection for an interactive conversational spoken language translation system speech , language and multimedia processing unit raytheon bbn technologies
lexical ambiguity can lead to concept transfer failure in conversational spoken language translation ( cslt ) systems . this paper presents a novel , classificationbased approach to accurately detecting word sense translation errors ( wstes ) of ambiguous source words . the approach requires minimal human annotation effort , and can be easily scaled to new language pairs and domains , with only a wordaligned parallel corpus and a small set of manual translation judgments . we show that this approach is highly precise in detecting wstes , even in highly skewed data , making it practical for use in an interactive cslt system .

bestcut : a graph algorithm for coreference resolution
in this paper we describe a coreference resolution method that employs a classification and a clusterization phase . in a novel way , the clusterization is produced as a graph cutting algorithm , in which nodes of the graph correspond to the mentions of the text , whereas the edges of the graph constitute the confidences derived from the coreference classification . in experiments , the graph cutting algorithm for coreference resolution , called bestcut , achieves state-of-the-art performance .

modeling morphologically rich languages using split words and
we experiment with splitting words into their stem and suffix components for modeling morphologically rich languages . we show that using a morphological analyzer and disambiguator results in a significant perplexity reduction in turkish . we present flexible n-gram models , flexgrams , which assume that the n1 tokens that determine the probability of a given token can be chosen anywhere in the sentence rather than the preceding n1 positions . our final model achieves 27 % perplexity reduction compared to the standard n-gram model .

using two translation models
classifying research papers into patent classification systems enables an exhaustive and effective invalidity search , prior art search , and technical trend analysis . however , it is very costly to classify research papers manually . therefore , we have studied automatic classification of research papers into a patent classification system . to classify research papers into patent classification systems , the differences in terms used in research papers and patents should be taken into account . this is because the terms used in patents are often more abstract or creative than those used in research papers in order to widen the scope of the claims . it is also necessary to do exhaustive searches and analyses that focus on classification of research papers written in various languages . to solve these problems , we propose some classification methods using two machine translation models . when translating english research papers into japanese , the performance of a translation model for patents is inferior to that for research papers due to the differences in terms used in research papers and patents . however , the model for patents is thought to be useful for our task because translation results by patent translation models tend to contain more patent terms than those for research papers . to confirm the effectiveness of our methods , we conducted some experiments using the data of the patent mining task in the ntcir-7 workshop .

rare word translation extraction from aligned comparable documents
we present a first known result of high precision rare word bilingual extraction from comparable corpora , using aligned comparable documents and supervised classification . we incorporate two features , a context-vector similarity and a co-occurrence model between words in aligned documents in a machine learning approach . we test our hypothesis on different pairs of languages and corpora . we obtain very high f-measure between 80 % and 98 % for recognizing and extracting correct translations for rare terms ( from 1 to 5 occurrences ) . moreover , we show that our system can be trained on a pair of languages and test on a different pair of languages , obtaining a f-measure of 77 % for the classification of chinese-english translations using a training corpus of spanish-french . our method is therefore even potentially applicable to low resources languages without training data .

two-step translation with grammatical post-processing
this paper describes an experiment in which we try to automatically correct mistakes in grammatical agreement in english to czech mt outputs . we perform several rule-based corrections on sentences parsed to dependency trees . we prove that it is possible to improve the mt quality of majority of the systems participating in wmt shared task . we made both automatic ( bleu ) and manual evaluations .

semi-supervised recognition of sarcastic sentences
sarcasm is a form of speech act in which the speakers convey their message in an implicit way . the inherently ambiguous nature of sarcasm sometimes makes it hard even for humans to decide whether an utterance is sarcastic or not . recognition of sarcasm can benefit many sentiment analysis nlp applications , such as review summarization , dialogue systems and review ranking systems . in this paper we experiment with semisupervised sarcasm identification on two very different data sets : a collection of 5.9 million tweets collected from twitter , and a collection of 66000 product reviews from amazon . using the mechanical turk we created a gold standard sample in which each sentence was tagged by 3 annotators , obtaining f-scores of 0.78 on the product reviews dataset and 0.83 on the twitter dataset . we discuss the differences between the datasets and how the algorithm uses them ( e.g. , for the amazon dataset the algorithm makes use of structured information ) . we also discuss the utility of twitter # sarcasm hashtags for the task .

is arabic part of speech tagging feasible without word segmentation
in this paper , we compare two novel methods for part of speech tagging of arabic without the use of gold standard word segmentation but with the full pos tagset of the penn arabic treebank . the first approach uses complex tags without any word segmentation , the second approach is segmention-based , using a machine learning segmenter . surprisingly , word-based pos tagging yields the best results , with a word accuracy of 94.74 % .

using inverse and generalization to translate english
we present a system to translate natural language sentences to formulas in a formal or a knowledge representation language . our system uses two inverse -calculus operators and using them can take as input the semantic representation of some words , phrases and sentences and from that derive the semantic representation of other words and phrases . our inverse operator works on many formal languages including first order logic , database query languages and answer set programming . our system uses a syntactic combinatorial categorial parser to parse natural language sentences and also to construct the semantic meaning of the sentences as directed by their parsing . the same parser is used for both . in addition to the inverse -calculus operators , our system uses a notion of generalization to learn semantic representation of words from the semantic representation of other words that are of the same category . together with this , we use an existing statistical learning approach to assign weights to deal with multiple meanings of words . our system produces improved results on standard corpora on natural language interfaces for robot command and control and database queries .

machine translation by interaction
a machine translation model has been proposed where an input is translated through both source-language and target-language paraphrasing processes . we have implemented our prototype model for the japanese-chinese language pair . this paper describes our core idea of translation , where a source language paraphraser and a language transfer cooperates in translation by exchanging information about the source input .

bootstrapping feature-rich dependency parsers with entropic priors
one may need to build a statistical parser for a new language , using only a very small labeled treebank together with raw text . we argue that bootstrapping a parser is most promising when the model uses a rich set of redundant features , as in recent models for scoring dependency parses ( mcdonald et al , 2005 ) . drawing on abneys ( 2004 ) analysis of the yarowsky algorithm , we perform bootstrapping by entropy regularization : we maximize a linear combination of conditional likelihood on labeled data and confidence ( negative renyi entropy ) on unlabeled data . in initial experiments , this surpassed em for training a simple feature-poor generative model , and also improved the performance of a feature-rich , conditionally estimated model where em could not easily have been applied . for our models and training sets , more peaked measures of confidence , measured by renyi entropy , outperformed smoother ones . we discuss how our feature set could be extended with cross-lingual or cross-domain features , to incorporate knowledge from parallel or comparable corpora during bootstrapping .

reactive redundancy and listener comprehension in direction-giving
we explore the role of redundancy , both in anticipation of and in response to listener confusion , in task-oriented dialogue . we find that direction-givers provide redundant utterances in response to both verbal and non-verbal signals of listener confusion . we also examine the effects of prior acquaintance and visibility upon redundancy . as expected , givers use more redundant utterances overall , and more redundant utterances in response to listener questions , when communicating with strangers . we discuss our findings in relation to theories of redundancy , the balance of speaker and listener effort , and potential applications .

cross-lingual wsd for translation extraction from comparable corpora
we propose a data-driven approach to enhance translation extraction from comparable corpora . instead of resorting to an external dictionary , we translate source vector features by using a cross-lingual word sense disambiguation method . the candidate senses for a feature correspond to sense clusters of its translations in a parallel corpus and the context used for disambiguation consists of the vector that contains the feature . the translations found in the disambiguation output convey the sense of the features in the source vector , while the use of translation clusters permits to expand their translation with several variants . as a consequence , the translated vectors are less noisy and richer , and allow for the extraction of higher quality lexicons compared to simpler methods .

piggyback : using search engines for robust cross-domain named entity recognition
we use search engine results to address a particularly difficult cross-domain language processing task , the adaptation of named entity recognition ( ner ) from news text to web queries . the key novelty of the method is that we submit a token with context to a search engine and use similar contexts in the search results as additional information for correctly classifying the token . we achieve strong gains in ner performance on news , in-domain and out-of-domain , and on web queries .

an explicit statistical model of learning lexical segmentation using
this paper presents an unsupervised and incremental model of learning segmentation that combines multiple cues whose use by children and adults were attested by experimental studies . the cues we exploit in this study are predictability statistics , phonotactics , lexical stress and partial lexical information . the performance of the model presented in this paper is competitive with the state-of-the-art segmentation models in the literature , while following the child language acquisition more faithfully . besides the performance improvements over the similar models in the literature , the cues are combined in an explicit manner , allowing easier interpretation of what the model learns .

chinese sentence segmentation as comma classification
we describe a method for disambiguating chinese commas that is central to chinese sentence segmentation . chinese sentence segmentation is viewed as the detection of loosely coordinated clauses separated by commas . trained and tested on data derived from the chinese treebank , our model achieves a classification accuracy of close to 90 % overall , which translates to an f1 score of 70 % for detecting commas that signal sentence boundaries .

latent-variable synchronous cfgs for hierarchical translation
data-driven refinement of non-terminal categories has been demonstrated to be a reliable technique for improving monolingual parsing with pcfgs . in this paper , we extend these techniques to learn latent refinements of single-category synchronous grammars , so as to improve translation performance . we compare two estimators for this latent-variable model : one based on em and the other is a spectral algorithm based on the method of moments . we evaluate their performance on a chineseenglish translation task . the results indicate that we can achieve significant gains over the baseline with both approaches , but in particular the momentsbased estimator is both faster and performs better than em .

intelligent selection of language model training data
we address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation . our approach is based on comparing the cross-entropy , according to domainspecific and non-domain-specifc language models , for each sentence of the text source used to produce the latter language model . we show that this produces better language models , trained on less data , than both random data selection and two other previously proposed methods .

speech to speech machine translation : biblical chatter from finnish to english mathias creutz timo honkela
speech-to-speech machine translation is in some ways the peak of natural language processing , in that it deals directly with our original , oral mode of communication ( as opposed to derived written language ) . as such , it presents challenges that are not to be taken lightly . although existing technology covers each of the steps in the process , from speech recognition to synthesis , deriving a model of translation that is effective in the domain of spoken language is an interesting and challenging task . if we could teach our algorithms to learn as children acquire language , the result would be useful both for language technology and cognitive science . we propose several potential approaches , an implementation of a multi-path model that translates recognized morphemes alongside words , and a web-interface to test our speech translation tool as trained for finnish to english . we also discuss current approaches to machine translation and the problems they face in adapting simultaneously to morphologically rich languages and to the spoken modality .

towards a unified approach to memory- and statistical-based
we present a set of algorithms that enable us to translate natural language sentences by exploiting both a translation memory and a statistical-based translation model . our results show that an automatically derived translation memory can be used within a statistical framework to often find translations of higher probability than those found using solely a statistical model . the translations produced using both the translation memory and the statistical model are significantly better than translations produced by two commercial systems : our hybrid system translated perfectly 58 % of the 505 sentences in a test collection , while the commercial systems translated perfectly only 40-42 % of them .

enriching the output of a parser using memory-based learning
we describe a method for enriching the output of a parser with information available in a corpus . the method is based on graph rewriting using memorybased learning , applied to dependency structures . this general framework allows us to accurately recover both grammatical and semantic information as well as non-local dependencies . it also facilitates dependency-based evaluation of phrase structure parsers . our method is largely independent of the choice of parser and corpus , and shows state of the art performance .

what can syntax-based mt learn from phrase-based mt
we compare and contrast the strengths and weaknesses of a syntax-based machine translation model with a phrase-based machine translation model on several levels . we briefly describe each model , highlighting points where they differ . we include a quantitative comparison of the phrase pairs that each model has to work with , as well as the reasons why some phrase pairs are not learned by the syntax-based model . we then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured . we also compare the translation accuracy for all variations .

bayesian learning of phrasal tree-to-string templates
we examine the problem of overcoming noisy word-level alignments when learning tree-to-string translation rules . our approach introduces new rules , and reestimates rule probabilities using em . the major obstacles to this approach are the very reasons that word-alignments are used for rule extraction : the huge space of possible rules , as well as controlling overfitting . by carefully controlling which portions of the original alignments are reanalyzed , and by using bayesian inference during re-analysis , we show significant improvement over the baseline rules extracted from word-level alignments .

chinese word segmentation with maximum entropy and n-gram language model
this paper presents the chinese word segmentation systems developed by speech and hearing research group of national laboratory on machine perception ( nlmp ) at peking university , which were evaluated in the third international chinese word segmentation bakeoff held by sighan . the chinese character-based maximum entropy model , which switches the word segmentation task to a classification task , is adopted in system developing . to integrate more linguistics information , an n-gram language model as well as several post processing strategies are also employed . both the closed and open tracks regarding to all four corpora msra , upuc , cityu , ckip are involved in our systems evaluation , and good performance are achieved . especially , in the closed track on msra , our system ranks 1st .

automatic acquisition of named entity tagged corpus from world wide gary
in this paper , we present a method that automatically constructs a named entity ( ne ) tagged corpus from the web to be used for learning of named entity recognition systems . we use an ne list and an web search engine to collect web documents which contain the ne instances . the documents are refined through sentence separation and text refinement procedures and ne instances are finally tagged with the appropriate ne categories . our experiments demonstrates that the suggested method can acquire enough ne tagged corpus equally useful to the manually tagged one without any human intervention .

searching for grammar right
this paper describes our ongoing work in and thoughts on developing a grammar learning system based on a construction grammar formalism . necessary modules are presented and first results and challenges in formalizing the grammar are shown up . furthermore , we point out the major reasons why we chose construction grammar as the most fitting formalism for our purposes . then our approach and ideas of learning new linguistic phenomena , ranging from holophrastic constructions to compositional ones , is presented .

computing semantic compositionality in distributional semantics
this article introduces and evaluates an approach to semantic compositionality in computational linguistics based on the combination of distributional semantics and supervised machine learning . in brief , distributional semantic spaces containing representations for complex constructions such as adjective-noun and verb-noun pairs , as well as for their constituent parts , are built . these representations are then used as feature vectors in a supervised learning model using multivariate multiple regression . in particular , the distributional semantic representations of the constituents are used to predict those of the complex structures . this approach outperforms the rivals in a series of experiments with adjective-noun pairs extracted from the bnc . in a second experimental setting based on verb-noun pairs , a comparatively much lower performance was obtained by all the models ; however , the proposed approach gives the best results in combination with a random indexing semantic space .

incorporating coercive constructions into a verb lexicon
we take the first steps towards augmenting a lexical resource , verbnet , with probabilistic information about coercive constructions . we focus on causedmotion as an example construction occurring with verbs for which it is a typical usage or for which it must be interpreted as extending the event semantics through coercion , which occurs productively and adds substantially to the relational semantics of a verb . however , through annotation we find that verbnet fails to accurately capture all usages of the construction . we use unsupervised methods to estimate probabilistic measures from corpus data for predicting usage of the construction across verb classes in the lexicon and evaluate against verbnet . we discuss how these methods will form the basis for enhancements for verbnet supporting more accurate analysis of the relational semantics of a verb across productive usages .

improved statistical machine translation using paraphrases chris callison-burch philipp koehn miles osborne
parallel corpora are crucial for training smt systems . however , for many language pairs they are available only in very limited quantities . for these language pairs a huge portion of phrases encountered at run-time will be unknown . we show how techniques from paraphrasing can be used to deal with these otherwise unknown source language phrases . our results show that augmenting a stateof-the-art smt system with paraphrases leads to significantly improved coverage and translation quality . for a training corpus with 10,000 sentence pairs we increase the coverage of unique test set unigrams from 48 % to 90 % , with more than half of the newly covered items accurately translated , as opposed to none in current approaches .

toward a cognitive organization for electronic dictionaries , the case for
we compare a psycholinguistic approach of mental lexicon organization with a computational approach of implicit lexical organization as found in dictionaries . in this work , we associate dictionaries with small world graphs . this multidisciplinary approach aims at showing that implicit structure of dictionaries , mathematically identified , fits the way young children categorize . these dictionary graphs might therefore be considered as cognitive artifacts . this shows the importance of semantic proximity both in cognitive and computational organization of verbs lexicon .

the importance of discourse context for statistical natural language generation
surface realization in statistical natural language generation is based on the idea that when there are many ways to say the same thing , the most frequent option based on corpus counts is the best . based on data from english and finnish , we argue instead that all options are not equivalent , and the most frequent one can be incoherent in some contexts . a statistical nlg system where word order choice is based only on frequency counts of forms can not capture the contextually-appropriate use of word order . we describe an alternative method for word order selection and show how it outperforms a frequency-only approach .

a joint graph model for pinyin-to-chinese conversion with typo correction
it is very import for chinese language processing with the aid of an efficient input method engine ( ime ) , of which pinyinto-chinese ( ptc ) conversion is the core part . meanwhile , though typos are inevitable during user pinyin inputting , existing imes paid little attention to such big inconvenience . in this paper , motivated by a key equivalence of two decoding algorithms , we propose a joint graph model to globally optimize ptc and typo correction for ime . the evaluation results show that the proposed method outperforms both existing academic and commercial imes .

dcu-uvt : word-level language classification with code-mixed data and jennifer foster
this paper describes the dcu-uvt teams participation in the language identification in code-switched data shared task in the workshop on computational approaches to code switching . wordlevel classification experiments were carried out using a simple dictionary-based method , linear kernel support vector machines ( svms ) with and without contextual clues , and a k-nearest neighbour approach . based on these experiments , we select our svm-based system with contextual clues as our final system and present results for the nepali-english and spanish-english datasets .

wrapping up a summary : from representation to generation josef steinberger and marco turchi and
the main focus of this work is to investigate robust ways for generating summaries from summary representations without recurring to simple sentence extraction and aiming at more human-like summaries . this is motivated by empirical evidence from tac 2009 data showing that human summaries contain on average more and shorter sentences than the system summaries . we report encouraging preliminary results comparable to those attained by participating systems at tac 2009 .

efficient kernels for sentence pair classification fabio massimo zanzotto
in this paper , we propose a novel class of graphs , the tripartite directed acyclic graphs ( tdags ) , to model first-order rule feature spaces for sentence pair classification . we introduce a novel algorithm for computing the similarity in first-order rewrite rule feature spaces . our algorithm is extremely efficient and , as it computes the similarity of instances that can be represented in explicit feature spaces , it is a valid kernel function .

a practical qa system in restricted domains
this paper describes an on-going research for a practical question answering system for a home agent robot . because the main concern of the qa system for the home robot is the precision , rather than coverage ( no answer is better than wrong answers ) , our approach is try to achieve high accuracy in qa . we restrict the question domains and extract answers from the pre-selected , semi-structured documents on the internet . a named entity tagger and a dependency parser are used to analyze the question accurately . user profiling and inference rules are used to infer hidden information that is required for finding a precise answer . testing with a small set of queries on weather domain , the qa system showed 90.9 % of precision and 75.0 % of recall .

applying conditional random fields to japanese morphological analysis taku kudo kaoru yamamoto yuji matsumoto
this paper presents japanese morphological analysis based on conditional random fields ( crfs ) . previous work in crfs assumed that observation sequence ( word ) boundaries were fixed . however , word boundaries are not clear in japanese , and hence a straightforward application of crfs is not possible . we show how crfs can be applied to situations where word boundary ambiguity exists . crfs offer a solution to the long-standing problems in corpus-based or statistical japanese morphological analysis . first , flexible feature designs for hierarchical tagsets become possible . second , influences of label and length bias are minimized . we experiment crfs on the standard testbed corpus used for japanese morphological analysis , and evaluate our results using the same experimental dataset as the hmms and memms previously reported in this task . our results confirm that crfs not only solve the long-standing problems but also improve the performance over hmms and memms .

entering text with a fourbutton device
this paper presents the design of a textentry device that requires only four buttons . such a device is ap plicable as the text interface of portable machines and as an interface for disabled people . the textentry system is predictive ; the basis for this is an adaptive language model . our evaluation showed that the system is at least as efficient for the entry of free text as the textentry systems of currentgeneration mobile phones . the system requires fewer keystrokes than a full keyboard after adaptation one user reached a maximum speed of wpm .

bridging the gap between domain-oriented and sumire uematsu jin-dong kim junich tsujii
this paper compares domain-oriented and linguistically-oriented semantics , based on the genia event corpus and framenet . while the domain-oriented semantic structures are direct targets of text mining ( tm ) , their extraction from text is not straghtforward due to the diversity of linguistic expressions . the extraction of linguistically-oriented semactics is more straghtforward , and has been studied independentely of specific domains . in order to find a use of the domain-independent research achievements for tm , we aim at linking classes of the two types of semantics . the classes were connected by analyzing linguistically-oriented semantics of the expressions that mention one biological class . with the obtained relationship between the classes , we discuss a link between tm and linguistically-oriented semantics .

automation and evaluation of the keyword method for second language learning
in this paper , we combine existing nlp techniques with minimal supervision to build memory tips according to the keyword method , a well established mnemonic device for second language learning . we present what we believe to be the first extrinsic evaluation of a creative sentence generator on a vocabulary learning task . the results demonstrate that nlp techniques can effectively support the development of resources for second language learning .

graph-based text representation for novelty detection
we discuss several feature sets for novelty detection at the sentence level , using the data and procedure established in task 2 of the trec 2004 novelty track . in particular , we investigate feature sets derived from graph representations of sentences and sets of sentences . we show that a highly connected graph produced by using sentence-level term distances and pointwise mutual information can serve as a source to extract features for novelty detection . we compare several feature sets based on such a graph representation . these feature sets allow us to increase the accuracy of an initial novelty classifier which is based on a bagof-word representation and kl divergence . the final result ties with the best system at trec 2004 .

event detection and summarization in weblogs with temporal collocations
this paper deals with the relationship between weblog content and time . with the proposed temporal mutual information , we analyze the collocations in time dimension , and the interesting collocations related to special events . the temporal mutual information is employed to observe the strength of term-to-term associations over time . an event detection algorithm identifies the collocations that may cause an event in a specific timestamp . an event summarization algorithm retrieves a set of collocations which describe an event . we compare our approach with the approach without considering the time interval . the experimental results demonstrate that the temporal collocations capture the real world semantics and real world events over time .

a note on contextual binary feature grammars
contextual binary feature grammars were recently proposed by ( clark et al , 2008 ) as a learnable representation for richly structured context-free and context sensitive languages . in this paper we examine the representational power of the formalism , its relationship to other standard formalisms and language classes , and its appropriateness for modelling natural language .

and monolingual target-language users
machine translation ( mt ) systems have improved significantly ; however , their outputs often contain too many errors to communicate the intended meaning to their users . this paper describes a collaborative approach for mediating between an mt system and users who do not understand the source language and thus can not easily detect translation mistakes on their own . through a visualization of multiple linguistic resources , this approach enables the users to correct difficult translation errors and understand translated passages that were otherwise baffling .

predicting emotion in spoken dialogue from multiple knowledge sources
we examine the utility of multiple types of turn-level and contextual linguistic features for automatically predicting student emotions in human-human spoken tutoring dialogues . we first annotate student turns in our corpus for negative , neutral and positive emotions . we then automatically extract features representing acoustic-prosodic and other linguistic information from the speech signal and associated transcriptions . we compare the results of machine learning experiments using different feature sets to predict the annotated emotions . our best performing feature set contains both acoustic-prosodic and other types of linguistic features , extracted from both the current turn and a context of previous student turns , and yields a prediction accuracy of 84.75 % , which is a 44 % relative improvement in error reduction over a baseline . our results suggest that the intelligent tutoring spoken dialogue system we are developing can b guo e enhanced to automatically predict and adapt to student emotions .

realization of discourse relations by other means : alternative
studies of discourse relations have not , in the past , attempted to characterize what serves as evidence for them , beyond lists of frozen expressions , or markers , drawn from a few well-defined syntactic classes . in this paper , we describe how the lexicalized discourse relation annotations of the penn discourse treebank ( pdtb ) led to the discovery of a wide range of additional expressions , annotated as altlex ( alternative lexicalizations ) in the pdtb 2.0. further analysis of altlex annotation suggests that the set of markers is openended , and drawn from a wider variety of syntactic types than currently assumed . as a first attempt towards automatically identifying discourse relation markers , we propose the use of syntactic paraphrase methods .

detecting errors in discontinuous structural annotation
consistency of corpus annotation is an essential property for the many uses of annotated corpora in computational and theoretical linguistics . while some research addresses the detection of inconsistencies in positional annotation ( e.g. , partof-speech ) and continuous structural annotation ( e.g. , syntactic constituency ) , no approach has yet been developed for automatically detecting annotation errors in discontinuous structural annotation . this is significant since the annotation of potentially discontinuous stretches of material is increasingly relevant , from treebanks for free-word order languages to semantic and discourse annotation . in this paper we discuss how the variation n-gram error detection approach ( dickinson and meurers , 2003a ) can be extended to discontinuous structural annotation . we exemplify the approach by showing how it successfully detects errors in the syntactic annotation of the german tiger corpus ( brants et al , 2002 ) .

utd-srl : a pipeline architecture for extracting frame cosmin adrian bejan and chris hathaway
this paper describes our system for the task of extracting frame semantic structures in semeval2007 . the system architecture uses two types of learning models in each part of the task : support vector machines ( svm ) and maximum entropy ( me ) . designed as a pipeline of classifiers , the semantic parsing system obtained competitive precision scores on the test data .

parser from an un-annotated corpus
we propose a hybrid generative/discriminative framework for semantic parsing which combines the hidden vector state ( hvs ) model and the hidden markov support vector machines ( hmsvms ) . the hvs model is an extension of the basic discrete markov model in which context is encoded as a stack-oriented state vector . the hm-svms combine the advantages of the hidden markov models and the support vector machines . by employing a modified k-means clustering method , a small set of most representative sentences can be automatically selected from an un-annotated corpus . these sentences together with their abstract annotations are used to train an hvs model which could be subsequently applied on the whole corpus to generate semantic parsing results . the most confident semantic parsing results are selected to generate a fully-annotated corpus which is used to train the hm-svms . the proposed framework has been tested on the darpa communicator data . experimental results show that an improvement over the baseline hvs parser has been observed using the hybrid framework . when compared with the hm-svms trained from the fullyannotated corpus , the hybrid framework gave a comparable performance with only a small set of lightly annotated sentences .

uva : language modeling techniques for web people search
in this paper we describe our participation in the semeval 2007 web people search task . our main aim in participating was to adapt language modeling tools for the task , and to experiment with various document representations . our main finding is that single pass clustering , using title , snippet and body to represent documents , is the most effective setting .

aida : artificial intelligent dialogue agent
this demo paper describes our artificial intelligent dialogue agent ( aida ) , a dialogue management and orchestration platform under development at the institute for infocomm research . among other features , it integrates different human-computer interaction engines across multiple domains and communication styles such as command , question answering , task-oriented dialogue and chat-oriented dialogue . the platform accepts both speech and text as input modalities by either direct microphone/keyboard connections or by means of mobile device wireless connection . the output interface , which is supported by a talking avatar , integrates speech and text along with other visual aids .

hypertagging : supertagging for surface realization with ccg
in lexicalized grammatical formalisms , it is possible to separate lexical category assignment from the combinatory processes that make use of such categories , such as parsing and realization . we adapt techniques from supertagging a relatively recent technique that performs complex lexical tagging before full parsing ( bangalore and joshi , 1999 ; clark , 2002 ) for chart realization in openccg , an open-source nlp toolkit for ccg . we call this approach hypertagging , as it operates at a level above the syntax , tagging semantic representations with syntactic lexical categories . our results demonstrate that a hypertagger-informed chart realizer can achieve substantial improvements in realization speed ( being approximately twice as fast ) with superior realization quality .

semi-supervised feature transformation for dependency parsing
in current dependency parsing models , conventional features ( i.e . base features ) defined over surface words and part-of-speech tags in a relatively high-dimensional feature space may suffer from the data sparseness problem and thus exhibit less discriminative power on unseen data . in this paper , we propose a novel semi-supervised approach to addressing the problem by transforming the base features into high-level features ( i.e . meta features ) with the help of a large amount of automatically parsed data . the meta features are used together with base features in our final parser . our studies indicate that our proposed approach is very effective in processing unseen data and features . experiments on chinese and english data sets show that the final parser achieves the best-reported accuracy on the chinese data and comparable accuracy with the best known parsers on the english data .

quality assessment of large scale knowledge resources
this paper presents an empirical evaluation of the quality of publicly available large-scale knowledge resources . the study includes a wide range of manually and automatically derived large-scale knowledge resources . in order to establish a fair and neutral comparison , the quality of each knowledge resource is indirectly evaluated using the same method on a word sense disambiguation task . the evaluation framework selected has been the senseval-3 english lexical sample task . the study empirically demonstrates that automatically acquired knowledge resources surpass both in terms of precision and recall the knowledge resources derived manually , and that the combination of the knowledge contained in these resources is very close to the most frequent sense classifier . as far as we know , this is the first time that such a quality assessment has been performed showing a clear picture of the current state-of-the-art of publicly available wide coverage semantic resources .

chinese word sense disambiguation with pagerank and hownet and telecommunications
word sense disambiguation is a basic problem in natural language processing . this paper proposed an unsupervised word sense disambiguation method based pagerank and hownet . in the method , a free text is firstly represented as a sememe graph with sememes as vertices and relatedness of sememes as weighted edges based on hownet . then uw-pagerank is applied on the sememe graph to score the importance of sememes . score of each definition of one word can be computed from the score of sememes it contains . finally , the highest scored definition is assigned to the word . this approach is tested on senseval-3 and the experimental results prove practical and effective .

an experiment on automatic detection of named entities in bangla bidyut baran chaudhuri head- cvpr unit
several preprocessing steps are necessary in various problems of automatic natural language processing . one major step is named-entity detection , which is relatively simple in english , because such entities start with an uppercase character . for indian scripts like bangla , no such indicator exists and the problem of identification is more complex , especially for human names , which may be common nouns and adjectives as well . in this paper we have proposed a three-stage approach of namedentity detection . the stages are based on the use of named-entity ( ne ) dictionary , rules for named-entity and left-right cooccurrence statistics . experimental results obtained on anandabazar patrika ( most popular bangla newspaper ) corpus are quite encouraging .

annotating opinions in the world press intelligent systems program
in this paper we present a detailed scheme for annotating expressions of opinions , beliefs , emotions , sentiment and speculation ( private states ) in the news and other discourse . we explore inter-annotator agreement for individual private state expressions , and show that these low-level annotations are useful for producing higher-level subjective sentence annotations .

a discriminative syntactic word order model for machine translation
we present a global discriminative statistical word order model for machine translation . our model combines syntactic movement and surface movement information , and is discriminatively trained to choose among possible word orders . we show that combining discriminative training with features to detect these two different kinds of movement phenomena leads to substantial improvements in word ordering performance over strong baselines . integrating this word order model in a baseline mt system results in a 2.4 points improvement in bleu for english to japanese translation .

dynamic wordclouds and vennclouds for exploratory data analysis
the wordcloud is a ubiquitous visualization of human language , though it falls short when used for exploratory data analysis . to address some of these shortcomings , we give the viewer explicit control over the creation of the wordcloud , allowing them to interact with it in real time a dynamic wordcloud . this allows iterative adaptation of the visualization to the data and inference task at hand . we next present a principled approach to visualization which highlights the similarities and differences between two sets of documents a venncloud . we make all the visualization code ( primarily javascript ) freely available .

ngoc-quang luong laurent besacier
this paper describes our word-level qe system for wmt 2014 shared task on spanish - english pair . compared to wmt 2013 , this years task is different due to the lack of smt setting information and additional resources . we report how we overcome this challenge to retain most of the important features which performed well last year in our system . novel features related to the availability of multiple systems output ( new point of this year ) are also proposed and experimented along with baseline set . the system is optimized by several ways : tuning the classification threshold , combining with wmt 2013 data , and refining using feature selection strategy on our development set , before dealing with the test set for submission .

spoken dialogue systems
in this paper we propose a new technique to enhance emotion recognition by combining in different ways what we call emotion predictions . the technique is called f2 as the combination is based on a double fusion process . the input to the first fusion phase is the output of a number of classifiers which deal with different types of information regarding each sentence uttered by the user . the output of this process is the input to the second fusion stage , which provides as output the most likely emotional category . experiments have been carried out using a previously-developed spoken dialogue system designed for the fast food domain . results obtained considering three and two emotional categories show that our technique outperforms the standard single fusion technique by 2.25 % and 3.35 % absolute , respectively .

from electronic health records
medical coding is a process of classifying health records according to standard code sets representing procedures and diagnoses . it is an integral part of health care in the u.s. , and the high costs it incurs have prompted adoption of natural language processing techniques for automatic generation of these codes from the clinical narrative contained in electronic health records . the need for effective auto-coding methods becomes even greater with the impending adoption of icd-10 , a code inventory of greater complexity than the currently used code sets . this paper presents a system that predicts icd-10 procedure codes from the clinical narrative using several levels of abstraction . first , partial hierarchical classification is used to identify potentially relevant concepts and codes . then , for each of these concepts we estimate the confidence that it appears in a procedure code for that document . finally , confidence values for the candidate codes are estimated using features derived from concept confidence scores . the concept models can be trained on data with icd-9 codes to supplement sparse icd-10 training resources . evaluation on held-out data shows promising results .

incremental text structuring with online hierarchical ranking
many emerging applications require documents to be repeatedly updated . such documents include newsfeeds , webpages , and shared community resources such as wikipedia . in this paper we address the task of inserting new information into existing texts . in particular , we wish to determine the best location in a text for a given piece of new information . for this process to succeed , the insertion algorithm should be informed by the existing document structure . lengthy real-world texts are often hierarchically organized into chapters , sections , and paragraphs . we present an online ranking model which exploits this hierarchical structure representationally in its features and algorithmically in its learning procedure . when tested on a corpus of wikipedia articles , our hierarchically informed model predicts the correct insertion paragraph more accurately than baseline methods .

sentiment learning on product reviews via sentiment ontology tree jon atle gulla
existing works on sentiment analysis on product reviews suffer from the following limitations : ( 1 ) the knowledge of hierarchical relationships of products attributes is not fully utilized . ( 2 ) reviews or sentences mentioning several attributes associated with complicated sentiments are not dealt with very well . in this paper , we propose a novel hl-sot approach to labeling a products attributes and their associated sentiments in product reviews by a hierarchical learning ( hl ) process with a defined sentiment ontology tree ( sot ) . the empirical analysis against a humanlabeled data set demonstrates promising and reasonable performance of the proposed hl-sot approach . while this paper is mainly on sentiment analysis on reviews of one product , our proposed hlsot approach is easily generalized to labeling a mix of reviews of more than one products .

using citations to generate surveys of scientific paradigms
the number of research publications in various disciplines is growing exponentially . researchers and scientists are increasingly finding themselves in the position of having to quickly understand large amounts of technical material . in this paper we present the first steps in producing an automatically generated , readily consumable , technical survey . specifically we explore the combination of citation information and summarization techniques . even though prior work ( teufel et al. , 2006 ) argues that citation text is unsuitable for summarization , we show that in the framework of multi-document survey creation , citation texts can play a crucial role .

extracting bilingual terminologies from comparable corpora
in this paper we present a method for extracting bilingual terminologies from comparable corpora . in our approach we treat bilingual term extraction as a classification problem . for classification we use an svm binary classifier and training data taken from the eurovoc thesaurus . we test our approach on a held-out test set from eurovoc and perform precision , recall and f-measure evaluations for 20 european language pairs . the performance of our classifier reaches the 100 % precision level for many language pairs . we also perform manual evaluation on bilingual terms extracted from english-german term-tagged comparable corpora . the results of this manual evaluation showed 60-83 % of the term pairs generated are exact translations and over 90 % exact or partial translations .

optimising natural language generation decision making for situated dialogue for artificial intelligence ( dfki )
natural language generators are faced with a multitude of different decisions during their generation process . we address the joint optimisation of navigation strategies and referring expressions in a situated setting with respect to task success and human-likeness . to this end , we present a novel , comprehensive framework that combines supervised learning , hierarchical reinforcement learning and a hierarchical information state . a human evaluation shows that our learnt instructions are rated similar to human instructions , and significantly better than the supervised learning baseline .

clr : linking events and their participants in discourse using a comprehensive framenet dictionary
the cl research system for semeval-2 task 10 for linking events and their participants in discourse is an exploration of the use of a specially created framenet dictionary that captures all framenet information about frames , lexical units , and frame-to-frame relations . this system is embedded in a specially designed interface , the linguistic task analyzer . the implementation of this system was quite minimal at the time of submission , allowing only an initial completion of the role recognition and labeling task , with recall of 0.112 , precision of 0.670 , and f-score of 0.192. we describe the design of the system and the continuing efforts to determine how much of this task can be performed with the available lexical resources . changes since the official submission have improved the f-score to 0.266 .

chinese word segmentation and named entity recognition by
this paper describes our word segmentation system and named entity recognition ( ner ) system for participating in the third sighan bakeoff . both of them are based on character tagging , but use different tag sets and different features . evaluation results show that our word segmentation system achieved 93.3 % and 94.7 % f-score in upuc and msra open tests , and our ner system got 70.84 % and 81.32 % f-score in ldc and msra open tests .

the imagination of crowds : conversational aac language modeling using crowdsourcing and large data sources per ola kristensson
augmented and alternative communication ( aac ) devices enable users with certain communication disabilities to participate in everyday conversations . such devices often rely on statistical language models to improve text entry by offering word predictions . these predictions can be improved if the language model is trained on data that closely reflects the style of the users intended communications . unfortunately , there is no large dataset consisting of genuine aac messages . in this paper we demonstrate how we can crowdsource the creation of a large set of fictional aac messages . we show that these messages model conversational aac better than the currently used datasets based on telephone conversations or newswire text . we leverage our crowdsourced messages to intelligently select sentences from much larger sets of twitter , blog and usenet data . compared to a model trained only on telephone transcripts , our best performing model reduced perplexity on three test sets of aac-like communications by 60 82 % relative . this translated to a potential keystroke savings in a predictive keyboard interface of 511 % .

learning to paraphrase : an unsupervised approach using
we address the text-to-text generation problem of sentence-level paraphrasing a phenomenon distinct from and more difficult than word- or phrase-level paraphrasing . our approach applies multiple-sequence alignment to sentences gathered from unannotated comparable corpora : it learns a set of paraphrasing patterns represented by word lattice pairs and automatically determines how to apply these patterns to rewrite new sentences . the results of our evaluation experiments show that the system derives accurate paraphrases , outperforming baseline systems .

modeling wisdom of crowds using latent mixture of discriminative experts
in many computational linguistic scenarios , training labels are subjectives making it necessary to acquire the opinions of multiple annotators/experts , which is referred to as wisdom of crowds . in this paper , we propose a new approach for modeling wisdom of crowds based on the latent mixture of discriminative experts ( lmde ) model that can automatically learn the prototypical patterns and hidden dynamic among different experts . experiments show improvement over state-of-the-art approaches on the task of listener backchannel prediction in dyadic conversations .

symmetric probabilistic alignment
we recently decided to develop a new alignment algorithm for the purpose of improving our example-based machine translation ( ebmt ) systems performance , since subsentential alignment is critical in locating the correct translation for a matched fragment of the input . unlike most algorithms in the literature , this new symmetric probabilistic alignment ( spa ) algorithm treats the source and target languages in a symmetric fashion . in this short paper , we outline our basic algorithm and some extensions for using context and positional information , and compare its alignment accuracy on the romanian-english data for the shared task with ibm model 4 and the reported results from the prior workshop .

cross-lingual information retrieval system for indian
this paper describes our first participation in the indian language sub-task of the main adhoc monolingual and bilingual track in clef competition . in this track , the task is to retrieve relevant documents from an english corpus in response to a query expressed in different indian languages including hindi , tamil , telugu , bengali and marathi . groups participating in this track are required to submit a english to english monolingual run and a hindi to english bilingual run with optional runs in rest of the languages . we had submitted a monolingual english run and a hindi to english crosslingual run . we used a word alignment table that was learnt by a statistical machine translation ( smt ) system trained on aligned parallel sentences , to map a query in source language into an equivalent query in the language of the target document collection . the relevant documents are then retrieved using a language modeling based retrieval algorithm . on clef 2007 data set , our official cross-lingual performance was 54.4 % of the monolingual performance and in the post submission experiments we found that it can be significantly improved up to 73.4 % .

whats in a p-value in nlp
in nlp , we need to document that our proposed methods perform significantly better with respect to standard metrics than previous approaches , typically by reporting p-values obtained by rank- or randomization-based tests . we show that significance results following current research standards are unreliable and , in addition , very sensitive to sample size , covariates such as sentence length , as well as to the existence of multiple metrics . we estimate that under the assumption of perfect metrics and unbiased data , we need a significance cut-off at 0.0025 to reduce the risk of false positive results to < 5 % . since in practice we often have considerable selection bias and poor metrics , this , however , will not do alone .

computer science & engineering computer science & engineering
in this paper we present a chunk based keyphrase extraction method for scientific articles . different from most previous systems , supervised machine learning algorithms are not used in our system . instead , document structure information is used to remove unimportant contents ; chunk extraction and filtering is used to reduce the quantity of candidates ; keywords are used to filter the candidates before generating final keyphrases . our experimental results on test data show that the method works better than the baseline systems and is comparable with other known algorithms .

spied : stanford pattern-based information extraction and diagnostics
this paper aims to provide an effective interface for progressive refinement of pattern-based information extraction systems . pattern-based information extraction ( ie ) systems have an advantage over machine learning based systems that patterns are easy to customize to cope with errors and are interpretable by humans . building a pattern-based system is usually an iterative process of trying different parameters and thresholds to learn patterns and entities with high precision and recall . since patterns are interpretable to humans , it is possible to identify sources of errors , such as patterns responsible for extracting incorrect entities and vice-versa , and correct them . however , it involves time consuming manual inspection of the extracted output . we present a light-weight tool , spied , to aid ie system developers in learning entities using patterns with bootstrapping , and visualizing the learned entities and patterns with explanations . spied is the first publicly available tool to visualize diagnostic information of multiple pattern learning systems to the best of our knowledge .

integrated shallow and deep parsing : topp meets hpsg
we present a novel , data-driven method for integrated shallow and deep parsing . mediated by an xml-based multi-layer annotation architecture , we interleave a robust , but accurate stochastic topological field parser of german with a constraintbased hpsg parser . our annotation-based method for dovetailing shallow and deep phrasal constraints is highly flexible , allowing targeted and fine-grained guidance of constraint-based parsing . we conduct systematic experiments that demonstrate substantial performance gains.1

using topic modeling to improve prediction of neuroticism and depression mindwell psychology bethesda
we investigate the value-add of topic modeling in text analysis for depression , and for neuroticism as a strongly associated personality measure . using pennebakers linguistic inquiry and word count ( liwc ) lexicon to provide baseline features , we show that straightforward topic modeling using latent dirichlet allocation ( lda ) yields interpretable , psychologically relevant themes that add value in prediction of clinical assessments .

probabilistic context-free grammars for phonology
we present a phonological probabilistic contextfree grammar , which describes the word and syllable structure of german words . the grammar is trained on a large corpus by a simple supervised method , and evaluated on a syllabification task achieving 96.88 % word accuracy on word tokens , and 90.33 % on word types . we added rules for english phonemes to the grammar , and trained the enriched grammar on an english corpus . both grammars are evaluated qualitatively showing that probabilistic context-free grammars can contribute linguistic knowledge to phonology . our formal approach is multilingual , while the training data is language-dependent .

text simplification for reading assistance : a project note kentaro inui atsushi fujita tetsuro takahashi ryu iida
this paper describes our ongoing research project on text simplification for congenitally deaf people . text simplification we are aiming at is the task of offering a deaf reader a syntactic and lexical paraphrase of a given text for assisting her/him to understand what it means . in this paper , we discuss the issues we should address to realize text simplification and report on the present results in three different aspects of this task : readability assessment , paraphrase representation and post-transfer error detection .

a cross-task flexible transition model for arabic tokenization , affix
this paper describes cross-task flexible transition models ( ctf-tms ) and demonstrates their effectiveness for arabic natural language processing ( nlp ) . nlp pipelines often suffer from error propagation , as errors committed in lower-level tasks cascade through the remainder of the processing pipeline . by allowing a flexible order of operations across and within multiple nlp tasks , a ctf-tm can mitigate both cross-task and within-task error propagation . our arabic ctf-tm models tokenization , affix detection , affix labeling , partof-speech tagging , and dependency parsing , achieving state-of-the-art results . we present the details of our general framework , our arabic ctf-tm , and the setup and results of our experiments .

how to semantically relate dialectal dictionaries in the linked data framework
we describe on-going work towards publishing language resources included in dialectal dictionaries in the linked open data ( lod ) cloud , and so to support wider access to the diverse cultural data associated with such dictionary entries , like the various historical and geographical variations of the use of such words . beyond this , our approach allows the cross-linking of entries of dialectal dictionaries on the basis of the semantic representation of their senses , and also to link the entries of the dialectal dictionaries to lexical senses available in the lod framework . this paper focuses on the description of the steps leading to a skos-xl and lemon encoding of the entries of two austrian dialectal dictionaries , and how this work supports their cross-linking and linking to other language data in the lod .

attacking decipherment problems optimally with low-order n-gram
we introduce a method for solving substitution ciphers using low-order letter n-gram models . this method enforces global constraints using integer programming , and it guarantees that no decipherment key is overlooked . we carry out extensive empirical experiments showing how decipherment accuracy varies as a function of cipher length and n-gram order . we also make an empirical investigation of shannons ( 1949 ) theory of uncertainty in decipherment .

an empirical evaluation of a statistical dialog system in public use
this paper provides a first assessment of a statistical dialog system in public use . in our dialog system there are four main recognition tasks , or slots bus route names , bus-stop locations , dates , and times . whereas a conventional system tracks a single value for each slot i.e. , the speech recognizers top hypothesis our statistical system tracks a distribution of many possible values over each slot . past work in lab studies has showed that this distribution improves robustness to speech recognition errors ; but to our surprise , we found the distribution yielded an increase in accuracy for only two of the four slots , and actually decreased accuracy in the other two . in this paper , we identify root causes for these differences in performance , including intrinsic properties of n-best lists , parameter settings , and the quality of statistical models . we synthesize our findings into a set of guidelines which aim to assist researchers and practitioners employing statistical techniques in future dialog systems .

annotating multiparty discourse : challenges for agreement metrics
to computationally model discourse phenomena such as argumentation we need corpora with reliable annotation of the phenomena under study . annotating complex discourse phenomena poses two challenges : fuzziness of unit boundaries and the need for multiple annotators . we show that current metrics for inter-annotator agreement ( iaa ) such as p/r/f1 and krippendorffs provide inconsistent results for the same text . in addition , iaa metrics do not tell us what parts of a text are easier or harder for human judges to annotate and so do not provide sufficiently specific information for evaluating systems that automatically identify discourse units . we propose a hierarchical clustering approach that aggregates overlapping text segments of text identified by multiple annotators ; the more annotators who identify a text segment , the easier we assume that the text segment is to annotate . the clusters make it possible to quantify the extent of agreement judges show about text segments ; this information can be used to assess the output of systems that automatically identify discourse units .

how good is the crowd at real wsd
there has been a great deal of excitement recently about using the wisdom of the crowd to collect data of all kinds , quickly and cheaply . snow et al were the first to give a convincing demonstration that at least some kinds of linguistic data can be gathered from workers on the web more cheaply than and as accurately as from local experts , and there has been a steady stream of papers and workshops since then with similar results . many of the tasks which have been successfully crowdsourced involve judgments which are similar to those performed in everyday life , such as recognizing unclear writing , or , for those tasks that require considerable judgment , the responses are usually binary or from a small set of responses , such as sentiment analysis or ratings . since the framenet process is known to be relatively expensive , we were interested in whether the framenet process of fine word sense discrimination and marking of dependents with semantic roles could be performed more cheaply and equally accurately using amazons mechanical turk ( amt ) or similar resources . we report on a partial success in this respect and how it was achieved .

sequential minimal optimization isabel segura bedmar
this paper presents a method for automatic classification of semantic relations between nominals using sequential minimal optimization . we participated in the four categories of semeval task 4 ( a : no query , no wordnet ; b : wordnet , no query ; c : query , no wordnet ; d : wordnet and query ) and for all training datasets . best scores were achieved in category b using a set of feature vectors including lexical file numbers of nominals obtained from wordnet and a new feature wordnet vector designed for the task1 .

wordform- and class-based prediction of the components of german nominal compounds
in word prediction systems for augmentative and alternative communication ( aac ) , productive wordformation processes such as compounding pose a serious problem . we present a model that predicts german nominal compounds by splitting them into their modifier and head components , instead of trying to predict them as a whole . the model is improved further by the use of class-based modifierhead bigrams constructed using semantic classes automatically extracted from a corpus . the evaluation shows that the split compound model with class bigrams leads to an improvement in keystroke savings of more than 15 % over a no split compound baseline model . we also present preliminary results obtained with a word prediction model integrating compound and simple word prediction .

adapting text instead of the model : an open domain approach
natural language systems trained on labeled data from one domain do not perform well on other domains . most adaptation algorithms proposed in the literature train a new model for the new domain using unlabeled data . however , it is time consuming to retrain big models or pipeline systems . moreover , the domain of a new target sentence may not be known , and one may not have significant amount of unlabeled data for every new domain . to pursue the goal of an open domain nlp ( train once , test anywhere ) , we propose adut ( adaptation using label-preserving transformation ) , an approach that avoids the need for retraining and does not require knowledge of the new domain , or any data from it . our approach applies simple label-preserving transformations to the target text so that the transformed text is more similar to the training domain ; it then applies the existing model on the transformed sentences and combines the predictions to produce the desired prediction on the target text . we instantiate adut for the case of semantic role labeling ( srl ) and show that it compares favorably with approaches that retrain their model on the target domain . specifically , this on the fly adaptation approach yields 13 % error reduction for a single parse system when adapting from the news wire text to fiction .

analysis of titles and readers
the title of a document has two roles , to give a compact summary and to lead the reader to read the document . conventional title generation focuses on finding key expressions from the authors wording in the document to give a compact summary and pays little attention to the readers interest . to make the title play its second role properly , it is indispensable to clarify the content ( what to say ) and wording ( how to say ) of titles that are effective to attract the target readers interest . in this article , we first identify typical content and wording of titles aimed at general readers in a comparative study between titles of technical papers and headlines rewritten for newspapers . next , we describe the results of a questionnaire survey on the effects of the content and wording of titles on the readers interest . the survey of general and knowledgeable readers shows both common and different tendencies in interest .

on semi-supervised learning of gaussian mixture models for phonetic classification
this paper investigates semi-supervised learning of gaussian mixture models using an unified objective function taking both labeled and unlabeled data into account . two methods are compared in this work the hybrid discriminative/generative method and the purely generative method . they differ in the criterion type on labeled data ; the hybrid method uses the class posterior probabilities and the purely generative method uses the data likelihood . we conducted experiments on the timit database and a standard synthetic data set from uci machine learning repository . the results show that the two methods behave similarly in various conditions . for both methods , unlabeled data improve training on models of higher complexity in which the supervised method performs poorly . in addition , there is a trend that more unlabeled data results in more improvement in classification accuracy over the supervised model . we also provided experimental observations on the relative weights of labeled and unlabeled parts of the training objective and suggested a critical value which could be useful for selecting a good weighing factor .

question detection in spoken conversations using textual conversations
we investigate the use of textual internet conversations for detecting questions in spoken conversations . we compare the text-trained model with models trained on manuallylabeled , domain-matched spoken utterances with and without prosodic features . overall , the text-trained model achieves over 90 % of the performance ( measured in area under the curve ) of the domain-matched model including prosodic features , but does especially poorly on declarative questions . we describe efforts to utilize unlabeled spoken utterances and prosodic features via domain adaptation .

using paraphrases for parameter tuning in statistical machine translation
most state-of-the-art statistical machine translation systems use log-linear models , which are defined in terms of hypothesis features and weights for those features . it is standard to tune the feature weights in order to maximize a translation quality metric , using held-out test sentences and their corresponding reference translations . however , obtaining reference translations is expensive . in this paper , we introduce a new full-sentence paraphrase technique , based on english-to-english decoding with an mt system , and we demonstrate that the resulting paraphrases can be used to drastically reduce the number of human reference translations needed for parameter tuning , without a significant decrease in translation quality .

nlp applications of sinhala : tts & ocr
this paper brings together the practical applications and the evaluation of the first text-to-speech ( tts ) system for sinhala using the festival framework and an optical character recognition system for sinhala .

a relatedness benchmark to test the role of determiners in compositional distributional semantics
distributional models of semantics capture word meaning very effectively , and they have been recently extended to account for compositionally-obtained representations of phrases made of content words . we explore whether compositional distributional semantic models can also handle a construction in which grammatical terms play a crucial role , namely determiner phrases ( dps ) . we introduce a new publicly available dataset to test distributional representations of dps , and we evaluate state-of-the-art models on this set .

clustered sub-matrix singular value decomposition
this paper presents an alternative algorithm based on the singular value decomposition ( svd ) that creates vector representation for linguistic units with reduced dimensionality . the work was motivated by an application aimed to represent text segments for further processing in a multi-document summarization system . the algorithm tries to compensate for svds bias towards dominant-topic documents . our experiments on measuring document similarities have shown that the algorithm achieves higher average precision with lower number of dimensions than the baseline algorithms - the svd and the vector space model .

monads as a solution for generalized opacity
in this paper we discuss a conservative extension of the simply-typed lambda calculus in order to model a class of expressions that generalize the notion of opaque contexts . our extension is based on previous work in the semantics of programming languages aimed at providing a mathematical characterization of computations that produce some kind of side effect ( moggi , 1989 ) , and is based on the notion of monads , a construction in category theory that , intuitively , maps a collection of simple values and simple functions into a more complex value space , in a canonical way . the main advantages of our approach with respect to traditional analyses of opacity are the fact that we are able to explain in a uniform way a set of different but related phenomena , and that we do so in a principled way that has been shown to also explain other linguistic phenomena ( shan , 2001 ) .

experiments with dbpedia , wordnet and sentiwordnet as resources for sentiment analysis in micro-blogging
sentiment analysis in twitter has become an important task due to the huge user-generated content published over such media . such analysis could be useful for many domains such as marketing , finance , politics , and social . we propose to use many features in order to improve a trained classifier of twitter messages ; these features extend the feature vector of uni-gram model by the concepts extracted from dbpedia , the verb groups and the similar adjectives extracted from wordnet , the sentifeatures extracted using sentiwordnet and some useful domain specific features . we also built a dictionary for emotion icons , abbreviation and slang words in tweets which is useful before extending the tweets with different features . adding these features has improved the f-measure accuracy 2 % with svm and 4 % with naivebayes .

combining distant and partial supervision for relation extraction
broad-coverage relation extraction either requires expensive supervised training data , or suffers from drawbacks inherent to distant supervision . we present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples . we compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative . in this way , we combine the benefits of fine-grained supervision for difficult examples with the coverage of a large distantly supervised corpus . our approach gives a substantial increase of 3.9 % endto-end f 1 on the 2013 kbp slot filling evaluation , yielding a net f 1 of 37.7 % .

multi-faceted event recognition with bootstrapped dictionaries
identifying documents that describe a specific type of event is challenging due to the high complexity and variety of event descriptions . we propose a multi-faceted event recognition approach , which identifies documents about an event using event phrases as well as defining characteristics of the event . our research focuses on civil unrest events and learns civil unrest expressions as well as phrases corresponding to potential agents and reasons for civil unrest . we present a bootstrapping algorithm that automatically acquires event phrases , agent terms , and purpose ( reason ) phrases from unannotated texts . we use the bootstrapped dictionaries to identify civil unrest documents and show that multi-faceted event recognition can yield high accuracy .

a multi-pass sieve for coreference resolution
most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features . this approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones . to overcome this problem , we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision . each tier builds on the previous tiers entity cluster output . further , our model propagates global information by sharing attributes ( e.g. , gender and number ) across mentions in the same cluster . this cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time . the framework is highly modular : new coreference modules can be plugged in without any change to the other modules . in spite of its simplicity , our approach outperforms many state-of-the-art supervised and unsupervised models on several standard corpora . this suggests that sievebased approaches could be applied to other nlp tasks .

unsupervised part-of-speech tagging with bilingual graph-based projections
we describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data , but have translated text in a resource-rich language . our method does not assume any knowledge about the target language ( in particular no tagging dictionary is assumed ) , making it applicable to a wide array of resource-poor languages . we use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model ( bergkirkpatrick et al , 2010 ) . across eight european languages , our approach results in an average absolute improvement of 10.4 % over a state-of-the-art baseline , and 16.7 % over vanilla hidden markov models induced with the expectation maximization algorithm .

srcb-wsd : supervised chinese word sense disambiguation with key features
this article describes the implementation of word sense disambiguation system that participated in the semeval-2007 multilingual chinese-english lexical sample task . we adopted a supervised learning approach with maximum entropy classifier . the features used were neighboring words and their part-of-speech , as well as single words in the context , and other syntactic features based on shallow parsing . in addition , we used word category information of a chinese thesaurus as features for verb disambiguation . for the task we participated in , we obtained precision of 0.716 in micro-average , which is the best among all participated systems .

automatic keyphrase extraction : a survey of the state of the art kazi saidul hasan and vincent ng
while automatic keyphrase extraction has been examined extensively , state-of-theart performance on this task is still much lower than that on many core natural language processing tasks . we present a survey of the state of the art in automatic keyphrase extraction , examining the major sources of errors made by existing systems and discussing the challenges ahead .

discovering entailment relations using textual entailment patterns fabio massimo zanzotto maria teresa pazienza , marco pennacchiotti
in this work we investigate methods to enable the detection of a specific type of textual entailment ( strict entailment ) , starting from the preliminary assumption that these relations are often clearly expressed in texts . our method is a statistical approach based on what we call textual entailment patterns , prototypical sentences hiding entailment relations among two activities . we experimented the proposed method using the entailment relations of wordnet as test case and the web as corpus where to estimate the probabilities ; obtained results will be shown .

an evaluation exercise for word alignment
this paper presents the task definition , resources , participating systems , and comparative results for the shared task on word alignment , which was organized as part of the hlt/naacl 2003 workshop on building and using parallel texts . the shared task included romanian-english and english-french sub-tasks , and drew the participation of seven teams from around the world .

a hierarchical bayesian language model based on pitman-yor processes yee whye teh
we propose a new hierarchical bayesian n-gram model of natural languages . our model makes use of a generalization of the commonly used dirichlet distributions called pitman-yor processes which produce power-law distributions more closely resembling those in natural languages . we show that an approximation to the hierarchical pitman-yor language model recovers the exact formulation of interpolated kneser-ney , one of the best smoothing methods for n-gram language models . experiments verify that our model gives cross entropy results superior to interpolated kneser-ney and comparable to modified kneser-ney .

vector space semantics with frequency-driven motifs
traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items . in this work , we present a frequencydriven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents , or motifs . the framework subsumes issues such as differential compositional as well as noncompositional behavior of phrasal consituents , and circumvents some problems of data sparsity by design . we design a segmentation model to optimally partition a sentence into lineal constituents , which can be used to define distributional contexts that are less noisy , semantically more interpretable , and linguistically disambiguated . hellinger pca embeddings learnt using the framework show competitive results on empirical tasks .

hypotheses selection criteria in a reranking framework for spoken
reranking models have been successfully applied to many tasks of natural language processing . however , there are two aspects of this approach that need a deeper investigation : ( i ) assessment of hypotheses generated for reranking at classification phase : baseline models generate a list of hypotheses and these are used for reranking without any assessment ; ( ii ) detection of cases where reranking models provide a worst result : the best hypothesis provided by the reranking model is assumed to be always the best result . in some cases the reranking model provides an incorrect hypothesis while the baseline best hypothesis is correct , especially when baseline models are accurate . in this paper we propose solutions for these two aspects : ( i ) a semantic inconsistency metric to select possibly more correct n-best hypotheses , from a large set generated by an slu basiline model . the selected hypotheses are reranked applying a state-of-the-art model based on partial tree kernels , which encode slu hypotheses in support vector machines with complex structured features ; ( ii ) finally , we apply a decision strategy , based on confidence values , to select the final hypothesis between the first ranked hypothesis provided by the baseline slu model and the first ranked hypothesis provided by the re-ranker . we show the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate conditional random field model . we evaluate our approach on the french media corpus . the results show significant improvements with respect to current state-of-the-art and previous re-ranking models .

an analysis of the calque phenomena based on comparable corpora
in this short paper we show how comparable corpora can be constructed in order to analyze the notion of calque . we then investigate the way comparable corpora contribute to a better linguistic analysis of the calque effect and how it can help improve error correction for non-native language productions .

comparison between historical population archives and decentralized databases
differences between large-scale historical population archives and small decentralized databases can be used to improve data quality and record connectedness in both types of databases . a parser is developed to account for differences in syntax and data representation models . a matching procedure is described to discover records from different databases referring to the same historical event . the problem of verification without reliable benchmark data is addressed by matching on a subset of record attributes and measuring support for the match using a different subset of attributes . an application of the matching procedure for comparison of family trees is discussed . a visualization tool is described to present an interactive overview of comparison results .

implicit proposal filtering in multi-party consensus-building conversations
an attempt was made to statistically estimate proposals which survived the discussion to be incorporated in the final agreement in an instance of a japanese design conversation . low level speech and vision features of hearer behaviors corresponding to aiduti , noddings and gaze were found to be a positive predictor of survival . the result suggests that non-linguistic hearer responses work as implicit proposal filters in consensus building , and could provide promising candidate features for the purpose of recognition and summarization of meeting events .

intersecting hierarchical and phrase-based models of translation : marc dymetman nicola cancedda
we address the problem of constructing hybrid translation systems by intersecting a hiero-style hierarchical system with a phrase-based system and present formal techniques for doing so . we model the phrase-based component by introducing a variant of weighted finite-state automata , called -automata , provide a self-contained description of a general algorithm for intersecting weighted synchronous context-free grammars with finite-state automata , and extend these constructs to -automata . we end by briefly discussing complexity properties of the presented algorithms .

applying pairwise ranked optimisation to improve the interpolation of
in statistical machine translation we often have to combine different sources of parallel training data to build a good system . one way of doing this is to build separate translation models from each data set and linearly interpolate them , and to date the main method for optimising the interpolation weights is to minimise the model perplexity on a heldout set . in this work , rather than optimising for this indirect measure , we directly optimise for bleu on the tuning set and show improvements in average performance over two data sets and 8 language pairs .

a method for forming mutual beliefs for communication through human-robot multi-modal interaction
this paper describes a method of multimodal language processing that reflects experiences shared by people and robots . through incremental online optimization in the process of interaction , the user and the robot form mutual beliefs represented by a stochastic model . based on these mutual beliefs , the robot can interpret even fragmental and ambiguous utterances , and can act and generate utterances appropriate for a given situation .

type-aware distantly supervised relation extraction with linked arguments
distant supervision has become the leading method for training large-scale relation extractors , with nearly universal adoption in recent tac knowledge-base population competitions . however , there are still many questions about the best way to learn such extractors . in this paper we investigate four orthogonal improvements : integrating named entity linking ( nel ) and coreference resolution into argument identification for training and extraction , enforcing type constraints of linked arguments , and partitioning the model by relation type signature . we evaluate sentential extraction performance on two datasets : the popular set of ny times articles partially annotated by hoffmann et al . ( 2011 ) and a new dataset , called goreco , that is comprehensively annotated for 48 common relations . we find that using nel for argument identification boosts performance over the traditional approach ( named entity recognition with string match ) , and there is further improvement from using argument types . our best system boosts precision by 44 % and recall by 70 % .

the clarity-brevity trade-off in generating referring expressions
existing algorithms for the generation of referring expressions ( gre ) aim at generating descriptions that allow a hearer to identify its intended referent uniquely ; the length of the expression is also considered , usually as a secondary issue . we explore the possibility of making the trade-off between these two factors more explicit , via a general cost function which scores these two aspects separately . we sketch some more complex phenomena which might be amenable to this treatment .

learning to rank answers on large online qa collections
this work describes an answer ranking engine for non-factoid questions built using a large online community-generated question-answer collection ( yahoo ! answers ) . we show how such collections may be used to effectively set up large supervised learning experiments . furthermore we investigate a wide range of feature types , some exploiting nlp processors , and demonstrate that using them in combination leads to considerable improvements in accuracy .

mmr-based feature selection for text categorization
we introduce a new method of feature selection for text categorization . our mmr-based feature selection method strives to reduce redundancy between features while maintaining information gain in selecting appropriate features for text categorization . empirical results show that mmr-based feature selection is more effective than koller & sahamis method , which is one of greedy feature selection methods , and conventional information gain which is commonly used in feature selection for text categorization . moreover , mmrbased feature selection sometimes produces some improvements of conventional machine learning algorithms over svm which is known to give the best classification accuracy .

measuring the semantic similarity of texts
this paper presents a knowledge-based method for measuring the semanticsimilarity of texts . while there is a large body of previous work focused on finding the semantic similarity of concepts and words , the application of these wordoriented methods to text similarity has not been yet explored . in this paper , we introduce a method that combines wordto-word similarity metrics into a text-totext metric , and we show that this method outperforms the traditional text similarity metrics based on lexical matching .

multiple similarity measures and source-pair information in story link detection
state-of-the-art story link detection systems , that is , systems that determine whether two stories are about the same event or linked , are usually based on the cosine-similarity measured between two stories . this paper presents a method for improving the performance of a link detection system by using a variety of similarity measures and using source-pair specific statistical information . the utility of a number of different similarity measures , including cosine , hellinger , tanimoto , and clarity , both alone and in combination , was investigated . we also compared several machine learning techniques for combining the different types of information . the techniques investigated were svms , voting , and decision trees , each of which makes use of similarity and statistical information differently . our experimental results indicate that the combination of similarity measures and source-pair specific statistical information using an svm provides the largest improvement in estimating whether two stories are linked ; the resulting system was the bestperforming link detection system at tdt-2002 .

chinese word ordering errors detection and correction for non-native chinese language learners
word ordering errors ( woes ) are the most frequent type of grammatical errors at sentence level for non-native chinese language learners . learners taking chinese as a foreign language often place character ( s ) in the wrong places in sentences , and that results in wrong word ( s ) or ungrammatical sentences . besides , there are no clear word boundaries in chinese sentences . that makes woes detection and correction more challenging . in this paper , we propose methods to detect and correct woes in chinese sentences . conditional random fields ( crfs ) based woes detection models identify the sentence segments containing woes . segment point-wise mutual information ( pmi ) , inter-segment pmi difference , language model , tag of the previous segment , and crf bigram template are explored . words in the segments containing woes are reordered to generate candidates that may have correct word orderings . ranking svm based models rank the candidates and suggests the most proper corrections . training and testing sets are selected from hsk dynamic composition corpus created by beijing language and culture university .

building a hierarchically aligned chinese-english parallel treebank
we construct a hierarchically aligned chinese-english parallel treebank by manually doing word alignments and phrase alignments simultaneously on parallel phrase-based parse trees . the main innovation of our approach is that we leave words without a translation counterpart ( which are mostly language-particular function words ) unaligned on the word level , and locate and align the appropriate phrases which encapsulate them . in doing so , we harmonize word-level and phraselevel alignments . we show that this type of annotation can be performedwith high inter-annotator consistency and have both linguistic and engineering potentials .

descriptive question answering in encyclopedia
recently there is a need for a qa system to answer not only factoid questions but also descriptive questions . descriptive questions are questions which need answers that contain definitional information about the search term or describe some special events . we have proposed a new descriptive qa model and presented the result of a system which we have built to answer descriptive questions . we defined 10 descriptive answer type ( dat ) s as answer types for descriptive questions . we discussed how our proposed model was applied to the descriptive question with some experiments .

context-dependent semantic parsing for time expressions
we present an approach for learning context-dependent semantic parsers to identify and interpret time expressions . we use a combinatory categorial grammar to construct compositional meaning representations , while considering contextual cues , such as the document creation time and the tense of the governing verb , to compute the final time values . experiments on benchmark datasets show that our approach outperforms previous stateof-the-art systems , with error reductions of 13 % to 21 % in end-to-end performance .

a maximum entropy-based word sense disambiguation system armando suarez manuel palomar
in this paper , a supervised learning system of word sense disambiguation is presented . it is based on conditional maximum entropy models . this system acquires the linguistic knowledge from an annotated corpus and this knowledge is represented in the form of features . several types of features have been analyzed using the senseval-2 data for the spanish lexical sample task . such analysis shows that instead of training with the same kind of information for all words , each one is more e ectively learned using a di erent set of features . this bestfeature-selection is used to build some systems based on di erent maximum entropy classi ers , and a voting system helped by a knowledgebased method .

human evaluation of a german surface realisation ranker
in this paper we present a human-based evaluation of surface realisation alternatives . we examine the relative rankings of naturally occurring corpus sentences and automatically generated strings chosen by statistical models ( language model , loglinear model ) , as well as the naturalness of the strings chosen by the log-linear model . we also investigate to what extent preceding context has an effect on choice . we show that native speakers do accept quite some variation in word order , but there are also clearly factors that make certain realisation alternatives more natural .

filling statistics with linguistics property design for the disambiguation of german lfg parses
we present a log-linear model for the disambiguation of the analyses produced by a german broad-coverage lfg , focussing on the properties ( or features ) this model is based on . we compare this model to an initial model based only on a part of the properties provided to the final model and observe that the performance of a log-linear model for parse selection depends heavily on the types of properties that it is based on . in our case , the error reduction achieved with the log-linear model based on the extended set of properties is 51.0 % and thus compares very favorably to the error reduction of 34.5 % achieved with the initial model .

statistical machine translation system jean-baptiste fouet jean senellart
this paper describes an initial version of a general purpose french/english statistical machine translation system . the main features of this system are the open-source moses decoder , the integration of a bilingual dictionary and a continuous space target language model . we analyze the performance of this system on the test data of the wmt08 evaluation .

zones of conceptualisation in scientific papers : a window to negative and
in view of the increasing need to facilitate processing the content of scientific papers , we present an annotation scheme for annotating full papers with zones of conceptualisation , reflecting the information structure and knowledge types which constitute a scientific investigation . the latter are the core scientific concepts ( corescs ) and include hypothesis , motivation , goal , object , background , method , experiment , model , observation , result and conclusion . the coresc scheme has been used to annotate a corpus of 265 full papers in physical chemistry and biochemistry and we are currently automating the recognition of corescs in papers . we discuss how the coresc scheme relates to other views of scientific papers and indeed how the former could be used to help identify negation and speculation in scientific texts .

the extraction of enriched protein-protein interactions from biomedical
there has been much recent interest in the extraction of ppis ( protein-protein interactions ) from biomedical texts , but in order to assist with curation efforts , the ppis must be enriched with further information of biological interest . this paper describes the implementation of a system to extract and enrich ppis , developed and tested using an annotated corpus of biomedical texts , and employing both machine-learning and rulebased techniques .

japanese idiom recognition : drawing a line between literal and idiomatic meanings
recognizing idioms in a sentence is important to sentence understanding . this paper discusses the lexical knowledge of idioms for idiom recognition . the challenges are that idioms can be ambiguous between literal and idiomatic meanings , and that they can be transformed when expressed in a sentence . however , there has been little research on japanese idiom recognition with its ambiguity and transformations taken into account . we propose a set of lexical knowledge for idiom recognition . we evaluated the knowledge by measuring the performance of an idiom recognizer that exploits the knowledge . as a result , more than 90 % of the idioms in a corpus are recognized with 90 % accuracy .

phrasal cohesion and statistical machine translation
there has been much interest in using phrasal movement to improve statistical machine translation . we explore how well phrases cohere across two languages , specifically english and french , and examine the particular conditions under which they do not . we demonstrate that while there are cases where coherence is poor , there are many regularities which can be exploited by a statistical machine translation system . we also compare three variant syntactic representations to determine which one has the best properties with respect to cohesion .

towards an electronic dictionary of tamajaq language in niger direction des enseignements du cycle republique du niger
we present the tamajaq language and the dictionary we used as main linguistic resource in the two first parts . the third part details the complex morphology of this language . in the part 4 we describe the conversion of the dictionary into electronic form , the inflectional rules we wrote and their implementation in the nooj software . finally we present a plan for our future work .

learning better monolingual models with unannotated bilingual text
this work shows how to improve state-of-the-art monolingual natural language processing models using unannotated bilingual text . we build a multiview learning objective that enforces agreement between monolingual and bilingual models . in our method the first , monolingual view consists of supervised predictors learned separately for each language . the second , bilingual view consists of log-linear predictors learned over both languages on bilingual text . our training procedure estimates the parameters of the bilingual model using the output of the monolingual model , and we show how to combine the two models to account for dependence between views . for the task of named entity recognition , using bilingual predictors increases f1 by 16.1 % absolute over a supervised monolingual model , and retraining on bilingual predictions increases monolingual model f1 by 14.6 % . for syntactic parsing , our bilingual predictor increases f1 by 2.1 % absolute , and retraining a monolingual model on its output gives an improvement of 2.0 % .

translation acquisition using synonym sets
we propose a new method for translation acquisition which uses a set of synonyms to acquire translations from comparable corpora . the motivation is that , given a certain query term , it is often possible for a user to specify one or more synonyms . using the resulting set of query terms has the advantage that we can overcome the problem that a single query terms context vector does not always reliably represent a terms meaning due to the context vectors sparsity . our proposed method uses a weighted average of the synonyms context vectors , that is derived by inferring the mean vector of the von mises-fisher distribution . we evaluate our method , using the synsets from the cross-lingually aligned japanese and english wordnet . the experiments show that our proposed method significantly improves translation accuracy when compared to a previous method for smoothing context vectors .

an unsupervised approach to biography production using wikipedia
we describe an unsupervised approach to multi-document sentence-extraction based summarization for the task of producing biographies . we utilize wikipedia to automatically construct a corpus of biographical sentences and tdt4 to construct a corpus of non-biographical sentences . we build a biographical-sentence classifier from these corpora and an svm regression model for sentence ordering from the wikipedia corpus . we evaluate our work on the duc2004 evaluation data and with human judges . overall , our system significantly outperforms all systems that participated in duc2004 , according to the rouge-l metric , and is preferred by human subjects .

examining the consensus between human summaries : initial experiments with factoid analysis
we present a new approach to summary evaluation which combines two novel aspects , namely ( a ) content comparison between gold standard summary and system summary via factoids , a pseudo-semantic representation based on atomic information units which can be robustly marked in text , and ( b ) use of a gold standard consensus summary , in our case based on 50 individual summaries of one text . even though future work on more than one source text is imperative , our experiments indicate that ( 1 ) ranking with regard to a single gold standard summary is insufficient as rankings based on any two randomly chosen summaries are very dissimilar ( correlations average = 0.20 ) , ( 2 ) a stable consensus summary can only be expected if a larger number of summaries are collected ( in the range of at least 30-40 summaries ) , and ( 3 ) similarity measurement using unigrams shows a similarly low ranking correlation when compared with factoid-based ranking .

learning soft linear constraints with application to citation field
accurately segmenting a citation string into fields for authors , titles , etc . is a challenging task because the output typically obeys various global constraints . previous work has shown that modeling soft constraints , where the model is encouraged , but not require to obey the constraints , can substantially improve segmentation performance . on the other hand , for imposing hard constraints , dual decomposition is a popular technique for efficient prediction given existing algorithms for unconstrained inference . we extend dual decomposition to perform prediction subject to soft constraints . moreover , with a technique for performing inference given soft constraints , it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training . this allows us to obtain substantial gains in accuracy on a new , challenging citation extraction dataset .

a bayesian model of natural language phonology : generating alternations from underlying forms
a stochastic approach to learning phonology . the model presented captures 7-15 % more phonologically plausible underlying forms than a simple majority solution , because it prefers pure alternations . it could be useful in cases where an approximate solution is needed , or as a seed for more complex models . a similar process could be involved in some stages of child language acquisition ; in particular , early learning of phonotactics .

sarcasm as contrast between a positive sentiment and negative situation
a common form of sarcasm on twitter consists of a positive sentiment contrasted with a negative situation . for example , many sarcastic tweets include a positive sentiment , such as love or enjoy , followed by an expression that describes an undesirable activity or state ( e.g. , taking exams or being ignored ) . we have developed a sarcasm recognizer to identify this type of sarcasm in tweets . we present a novel bootstrapping algorithm that automatically learns lists of positive sentiment phrases and negative situation phrases from sarcastic tweets . we show that identifying contrasting contexts using the phrases learned through bootstrapping yields improved recall for sarcasm recognition .

a hedgehop over a max-margin framework using hedge cues
in this paper , we describe the experimental settings we adopted in the context of the 2010 conll shared task for detecting sentences containing uncertainty . the classification results reported on are obtained using discriminative learning with features essentially incorporating lexical information . hyper-parameters are tuned for each domain : using bioscope training data for the biomedical domain and wikipedia training data for the wikipedia test set . by allowing an efficient handling of combinations of large-scale input features , the discriminative approach we adopted showed highly competitive empirical results for hedge detection on the wikipedia dataset : our system is ranked as the first with an f-score of 60.17 % .

learning to simplify sentences using wikipedia
in this paper we examine the sentence simplification problem as an english-to-english translation problem , utilizing a corpus of 137k aligned sentence pairs extracted by aligning english wikipedia and simple english wikipedia . this data set contains the full range of transformation operations including rewording , reordering , insertion and deletion . we introduce a new translation model for text simplification that extends a phrasebased machine translation approach to include phrasal deletion . evaluated based on three metrics that compare against a human reference ( bleu , word-f1 and ssa ) our new approach performs significantly better than two text compression techniques ( including t3 ) and the phrase-based translation system without deletion .

adaptive chinese word segmentation with online passive-aggressive algorithm
in this paper , we describe our system1 for cips-sighan-2010 bake-off task of chinese word segmentation , which focused on the cross-domain performance of chinese word segmentation algorithms . we use the online passive-aggressive algorithm with domain invariant information for cross-domain chinese word segmentation .

word-level language identification using crf : code-switching shared task report of msr india system kalika bali monojit choudhury
we describe a crf based system for word-level language identification of code-mixed text . our method uses lexical , contextual , character n-gram , and special character features , and therefore , can easily be replicated across languages . its performance is benchmarked against the test sets provided by the shared task on code-mixing ( solorio et al. , 2014 ) for four language pairs , namely , englishspanish ( en-es ) , english-nepali ( en-ne ) , english-mandarin ( en-cn ) , and standard arabic-arabic ( ar-ar ) dialects . the experimental results show a consistent performance across the language pairs .

feature selection for fluency ranking
fluency rankers are used in modern sentence generation systems to pick sentences that are not just grammatical , but also fluent . it has been shown that feature-based models , such as maximum entropy models , work well for this task . since maximum entropy models allow for incorporation of arbitrary real-valued features , it is often attractive to create very general feature templates , that create a huge number of features . to select the most discriminative features , feature selection can be applied . in this paper we compare three feature selection methods : frequency-based selection , a generalization of maximum entropy feature selection for ranking tasks with realvalued features , and a new selection method based on feature value correlation . we show that the often-used frequency-based selection performs badly compared to maximum entropy feature selection , and that models with a few hundred well-picked features are competitive to models with no feature selection applied . in the experiments described in this paper , we compressed a model of approximately 490.000 features to 1.000 features .

efficient higher-order crfs for morphological tagging
training higher-order conditional random fields is prohibitive for huge tag sets . we present an approximated conditional random field using coarse-to-fine decoding and early updating . we show that our implementation yields fast and accurate morphological taggers across six languages with different morphological properties and that across languages higher-order models give significant improvements over 1st-order models .

word sense disambiguation using automatically translated
we present an unsupervised approach to word sense disambiguation ( wsd ) . we automatically acquire english sense examples using an english-chinese bilingual dictionary , chinese monolingual corpora and chinese-english machine translation software . we then train machine learning classifiers on these sense examples and test them on two gold standard english wsd datasets , one for binary and the other for fine-grained sense identification . on binary disambiguation , performance of our unsupervised system has approached that of the state-of-the-art supervised ones . on multi-way disambiguation , it has achieved a very good result that is competitive to other state-of-the-art unsupervised systems . given the fact that our approach does not rely on manually annotated resources , such as sense-tagged data or parallel corpora , the results are very promising .

mining linguistically interpreted texts cassiana fagundes da silva , renata vieira , fernando santos osrio pipca - unisinos vora - portugal
this paper proposes and evaluates the use of linguistic information in the pre-processing phase of text mining tasks . we present several experiments comparing our proposal for selection of terms based on linguistic knowledge with usual techniques applied in the field . the results show that part of speech information is useful for the pre-processing phase of text categorization and clustering , as an alternative for stop words and stemming .

good neighbors make good senses : exploiting distributional similarity for unsupervised wsd
we present an automatic method for senselabeling of text in an unsupervised manner . the method makes use of distributionally similar words to derive an automatically labeled training set , which is then used to train a standard supervised classifier for distinguishing word senses . experimental results on the senseval-2 and senseval-3 datasets show that our approach yields significant improvements over state-of-the-art unsupervised methods , and is competitive with supervised ones , while eliminating the annotation cost .

klue-core : a regression model of semantic textual similarity professur fr korpuslinguistik
this paper describes our system entered for the *sem 2013 shared task on semantic textual similarity ( sts ) . we focus on the core task of predicting the semantic textual similarity of sentence pairs . the current system utilizes machine learning techniques trained on semantic similarity ratings from the *sem 2012 shared task ; it achieved rank 20 out of 90 submissions from 35 different teams . given the simple nature of our approach , which uses only wordnet and unannotated corpus data as external resources , we consider this a remarkably good result , making the system an interesting tool for a wide range of practical applications .

automatic collection of related terms from the web
this paper proposes a method of collecting a dozen terms that are closely related to a given seed term . the proposed method consists of three steps . the first step , compiling corpus step , collects texts that contain the given seed term by using search engines . the second step , automatic term recognition , extracts important terms from the corpus by using nakagawas method . these extracted terms become the candidates for the final step . the final step , filtering step , removes inappropriate terms from the candidates based on search engine hits . an evaluation result shows that the precision of the method is 85 % .

incorporating topic information into sentiment analysis models
this paper reports experiments in classifying texts based upon their favorability towards the subject of the text using a feature set enriched with topic information on a small dataset of music reviews hand-annotated for topic . the results of these experiments suggest ways in which incorporating topic information into such models may yield improvement over models which do not use topic information .

design and validation of eca gestures to improve dialogue system robustness atvs , escuela politcnica superior
in this paper we present validation tests that we have carried out on gestures that we have designed for an embodied conversational agent ( ecas ) , to assess their soundness with a view to applying said gestures in a forthcoming experiment to explore the possibilities ecas can offer to overcome typical robustness problems in spoken language dialogue systems ( sldss ) . the paper is divided into two parts : first we carry our a literature review to acquire a sense of the extent to which ecas can help overcome user frustration during human-machine interaction . then we associate tentative , yet specific , eca gestural behaviour with each of the main dialogue stages , with special emphasis on problem situations . in the second part we describe the tests we have carried out to validate our ecas gestural repertoire . the results obtained show that users generally understand and naturally accept the gestures , to a reasonable degree . this encourages us to proceed with the next stage of research : evaluating the gestural strategy in real dialogue situations with the aim of learning about how to favour a more efficient and pleasant dialogue flow for the users .

context sensing using speech and common sense
we present a method of inferring aspects of a persons context by capturing conversation topics and using prior knowledge of human behavior . this paper claims that topic-spotting performance can be improved by using a large database of common sense knowledge . we describe two systems we built to infer context from noisy transcriptions of spoken conversations using common sense , and detail some preliminary results . the gister system uses omcsnet , a commonsense semantic network , to infer the most likely topics under discussion in a conversation stream . the overhear system is built on top of gister , and distinguishes between aspects of the conversation that refer to past , present , and future events by using lifenet , a probabilistic graphical model of human behavior , to help infer the events that occurred in each of those three time periods . we conclude by discussing some of the future directions we may take this work .

sequential conditional generalized iterative scaling
we describe a speedup for training conditional maximum entropy models . the algorithm is a simple variation on generalized iterative scaling , but converges roughly an order of magnitude faster , depending on the number of constraints , and the way speed is measured . rather than attempting to train all model parameters simultaneously , the algorithm trains them sequentially . the algorithm is easy to implement , typically uses only slightly more memory , and will lead to improvements for most maximum entropy problems .

manifold learning for the semi-supervised induction of framenet predicates : an empirical investigation
this work focuses on the empirical investigation of distributional models for the automatic acquisition of frame inspired predicate words . while several semantic spaces , both word-based and syntaxbased , are employed , the impact of geometric representation based on dimensionality reduction techniques is investigated . data statistics are accordingly studied along two orthogonal perspectives : latent semantic analysis exploits global properties while locality preserving projection emphasizes the role of local regularities . this latter is employed by embedding prior framenet-derived knowledge in the corresponding non-euclidean transformation . the empirical investigation here reported sheds some light on the role played by these spaces as complex kernels for supervised ( i.e . support vector machine ) algorithms : their use configures , as a novel way to semi-supervised lexical learning , a highly appealing research direction for knowledge rich scenarios like framenet-based semantic parsing .

automatic knowledge representation using a graph-based algorithm for language-independent lexical chaining
lexical chains are powerful representations of documents . in particular , they have successfully been used in the field of automatic text summarization . however , until now , lexical chaining algorithms have only been proposed for english . in this paper , we propose a greedy language-independent algorithm that automatically extracts lexical chains from texts . for that purpose , we build a hierarchical lexico-semantic knowledge base from a collection of texts by using the pole-based overlapping clustering algorithm . as a consequence , our methodology can be applied to any language and proposes a solution to languagedependent lexical chainers .

a probabilistic model for associative anaphora resolution
this paper proposes a probabilistic model for associative anaphora resolution in japanese . associative anaphora is a type of bridging anaphora , in which the anaphor and its antecedent are not coreferent . our model regards associative anaphora as a kind of zero anaphora and resolves it in the same manner as zero anaphora resolution using automatically acquired lexical knowledge . experimental results show that our model resolves associative anaphora with good performance and the performance is improved by resolving it simultaneously with zero anaphora .

the chinese persons name disambiguation evaluation : exploration of personal name disambiguation in chinese news
personal name disambiguation becomes hot as it provides a way to incorporate semantic understanding into information retrieval . in this campaign , we explore chinese personal name disambiguation in news . in order to examine how well disambiguation technologies work , we concentrate on news articles , which is well-formatted and whose genre is well-studied . we then design a diagnosis test to explore the impact of chinese word segmentation to personal name disambiguation .

lexical generalization in ccg grammar induction for semantic parsing computer science & engineering
we consider the problem of learning factored probabilistic ccg grammars for semantic parsing from data containing sentences paired with logical-form meaning representations . traditional ccg lexicons list lexical items that pair words and phrases with syntactic and semantic content . such lexicons can be inefficient when words appear repeatedly with closely related lexical content . in this paper , we introduce factored lexicons , which include both lexemes to model word meaning and templates to model systematic variation in word usage . we also present an algorithm for learning factored ccg lexicons , along with a probabilistic parse-selection model . evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers , whose generalization performance benefits greatly from the lexical factoring .

constructing information networks using one single model
in this paper , we propose a new framework that unifies the output of three information extraction ( ie ) tasks - entity mentions , relations and events as an information network representation , and extracts all of them using one single joint model based on structured prediction . this novel formulation allows different parts of the information network fully interact with each other . for example , many relations can now be considered as the resultant states of events . our approach achieves substantial improvements over traditional pipelined approaches , and significantly advances state-of-the-art end-toend event argument extraction .

varying cardinality in metonymic extensions to nouns
meaning shifting phenomena such as metonymy have recently attracted increasing interest of researchers . though these phenomena have been addressed by plenty of computational methods , the impacts of cardinalities of metonymically related items have been widely ignored in all of them . motivated by this lack of analysis , we have developed a method for representing expectations and knowledge about the cardinalities of metonymically related entities and for exploiting this information to build logical forms expressing metonymic relations , the entities related , and their cardinalities . the representation of lexically motivated knowledge is realized as an enhancement to pustejovsky 's generative lexicon , and the process of building logical forms takes into account overwriting of default information and mismatch of cardinality requirements . our method enables a precise attachment of sentence complements , and it supports reference resolution in the context of metonymic expressions .

a hybrid model for urdu hindi transliteration abbas malik laurent besacier christian boitet
we report in this paper a novel hybrid approach for urdu to hindi transliteration that combines finite-state machine ( fsm ) based techniques with statistical word language model based approach . the output from the fsm is filtered with the word language model to produce the correct hindi output . the main problem handled is the case of omission of diacritical marks from the input urdu text . our system produces the correct hindi output even when the crucial information in the form of diacritic marks is absent . the approach improves the accuracy of the transducer-only approach from 50.7 % to 79.1 % . the results reported show that performance can be improved using a word language model to disambiguate the output produced by the transducer-only approach , especially when diacritic marks are not present in the urdu input .

a current status of thai categorial grammars and
this paper presents a current status of thai resources and tools for cg development . we also proposed a thai categorial dependency grammar ( cdg ) , an extended version of cg which includes dependency analysis into cg notation . beside , an idea of how to group a word that has the same functions are presented to gain a certain type of category per word . we also discuss about a difficulty of building treebank and mention a toolkit for assisting on a thai cgs tree building and a tree format representations . in this paper , we also give a summary of applications related to thai cgs .

cross-domain dependency parsing using a deep linguistic grammar
pure statistical parsing systems achieves high in-domain accuracy but performs poorly out-domain . in this paper , we propose two different approaches to produce syntactic dependency structures using a large-scale hand-crafted hpsg grammar . the dependency backbone of an hpsg analysis is used to provide general linguistic insights which , when combined with state-of-the-art statistical dependency parsing models , achieves performance improvements on out-domain tests .

selecting an ontology for biomedical text mining
text mining for biomedicine requires a significant amount of domain knowledge . much of this information is contained in biomedical ontologies . developers of text mining applications often look for appropriate ontologies that can be integrated into their systems , rather than develop new ontologies from scratch . however , there is often a lack of documentation of the qualities of the ontologies . a number of methodologies for evaluating ontologies have been developed , but it is difficult for users by using these methods to select an ontology . in this paper , we propose a framework for selecting the most appropriate ontology for a particular text mining application . the framework comprises three components , each of which considers different aspects of requirements of text mining applications on ontologies . we also present an experiment based on the framework choosing an ontology for a gene normalization system .

multilingual summarization : dimensionality reduction and a step towards optimal term coverage
in this paper we present three term weighting approaches for multi-lingual document summarization and give results on the duc 2002 data as well as on the 2013 multilingual wikipedia feature articles data set . we introduce a new intervalbounded nonnegative matrix factorization . we use this new method , latent semantic analysis ( lsa ) , and latent dirichlet alocation ( lda ) to give three term-weighting methods for multi-document multi-lingual summarization . results on duc and tac data , as well as on the multiling 2013 data , demonstrate that these methods are very promising , since they achieve oracle coverage scores in the range of humans for 6 of the 10 test languages . finally , we present three term weighting approaches for the multiling13 single document summarization task on the wikipedia featured articles . our submissions significantly outperformed the baseline in 19 out of 41 languages .

comparing language similarity across genetic
recent studies have shown the potential benefits of leveraging resources for resource-rich languages to build tools for similar , but resource-poor languages . we examine what constitutes similarity by comparing traditional phylogenetic language groups , which are motivated largely by genetic relationships , with language groupings formed by clustering methods using typological features only . using data from the world atlas of language structures ( wals ) , our preliminary experiments show that typologically-based clusters look quite different from genetic groups , but perform as good or better when used to predict feature values of member languages .

pre-reordering for machine translation using transition-based walks on dependency parse trees antonio valerio miceli-barone dipartimento di informatica dipartimento di informatica
we propose a pre-reordering scheme to improve the quality of machine translation by permuting the words of a source sentence to a target-like order . this is accomplished as a transition-based system that walks on the dependency parse tree of the sentence and emits words in target-like order , driven by a classifier trained on a parallel corpus . our system is capable of generating arbitrary permutations up to flexible constraints determined by the choice of the classifier algorithm and input features .

an improved oracle for dependency parsing with online reordering joakim nivre marco kuhlmann johan hall
we present an improved training strategy for dependency parsers that use online reordering to handle non-projective trees . the new strategy improves both efficiency and accuracy by reducing the number of swap operations performed on non-projective trees by up to 80 % . we present state-ofthe-art results for five languages with the best ever reported results for czech .

chinese word segmentation using minimal linguistic knowledge
this paper presents a primarily data-driven chinese word segmentation system and its performances on the closed track using two corpora at the first international chinese word segmentation bakeoff . the system consists of a new words recognizer , a base segmentation algorithm , and procedures for combining single characters , suffixes , and checking segmentation consistencies .

a log-linear model for unsupervised text normalization
we present a unified unsupervised statistical model for text normalization . the relationship between standard and non-standard tokens is characterized by a log-linear model , permitting arbitrary features . the weights of these features are trained in a maximumlikelihood framework , employing a novel sequential monte carlo training algorithm to overcome the large label space , which would be impractical for traditional dynamic programming solutions . this model is implemented in a normalization system called unlol , which achieves the best known results on two normalization datasets , outperforming more complex systems . we use the output of unlol to automatically normalize a large corpus of social media text , revealing a set of coherent orthographic styles that underlie online language variation .

answering clinical questions with role identification
we describe our work in progress on natural language analysis in medical questionanswering in the context of a broader medical text-retrieval project . we analyze the limitations in the medical domain of the technologies that have been developed for general question-answering systems , and describe an alternative approach whose organizing principle is the identification of semantic roles in both question and answer texts that correspond to the fields of pico format .

training conditional random fields with multivariate evaluation
this paper proposes a framework for training conditional random fields ( crfs ) to optimize multivariate evaluation measures , including non-linear measures such as f-score . our proposed framework is derived from an error minimization approach that provides a simple solution for directly optimizing any evaluation measure . specifically focusing on sequential segmentation tasks , i.e . text chunking and named entity recognition , we introduce a loss function that closely reflects the target evaluation measure for these tasks , namely , segmentation f-score . our experiments show that our method performs better than standard crf training .

domain-specific coreference resolution with lexicalized features
most coreference resolvers rely heavily on string matching , syntactic properties , and semantic attributes of words , but they lack the ability to make decisions based on individual words . in this paper , we explore the benefits of lexicalized features in the setting of domain-specific coreference resolution . we show that adding lexicalized features to off-the-shelf coreference resolvers yields significant performance gains on four domain-specific data sets and with two types of coreference resolution architectures .

towards semantic validation of a derivational lexicon
derivationally related lemmas like friend n friendly a friendship n are derived from a common stem . frequently , their meanings are also systematically related . however , there are also many examples of derivationally related lemma pairs whose meanings differ substantially , e.g. , object n objective n . most broad-coverage derivational lexicons do not reflect this distinction , mixing up semantically related and unrelated word pairs . in this paper , we investigate strategies to recover the above distinction by recognizing semantically related lemma pairs , a process we call semantic validation . we make two main contributions : first , we perform a detailed data analysis on the basis of a large german derivational lexicon . it reveals two promising sources of information ( distributional semantics and structural information about derivational rules ) , but also systematic problems with these sources . second , we develop a classification model for the task that reflects the noisy nature of the data . it achieves an improvement of 13.6 % in precision and 5.8 % in f1-score over a strong majority class baseline . our experiments confirm that both information sources contribute to semantic validation , and that they are complementary enough that the best results are obtained from a combined model .

bipolar person name identification of topic documents using principal component analysis
in this paper , we propose an unsupervised approach for identifying bipolar person names in a set of topic documents . we employ principal component analysis ( pca ) to discover bipolar word usage patterns of person names in the documents and show that the signs of the entries in the principal eigenvector of pca partition the person names into bipolar groups spontaneously . empirical evaluations demonstrate the efficacy of the proposed approach in identifying bipolar person names of topics .

machine translation between turkic languages
we present an approach to mt between turkic languages and present results from an implementation of a mt system from turkmen to turkish . our approach relies on ambiguous lexical and morphological transfer augmented with target side rule-based repairs and rescoring with statistical language models .

generating learner-like morphological errors in russian
to speed up the process of categorizing learner errors and obtaining data for languages which lack error-annotated data , we describe a linguistically-informed method for generating learner-like morphological errors , focusing on russian . we outline a procedure to select likely errors , relying on guiding stem and suffix combinations from a segmented lexicon to match particular error categories and relying on grammatical information from the original context .

dependency-based empty category detection via phrase structure trees
we describe a novel approach to detecting empty categories ( ec ) as represented in dependency trees as well as a new metric for measuring ec detection accuracy . the new metric takes into account not only the position and type of an ec , but also the head it is a dependent of in a dependency tree . we also introduce a variety of new features that are more suited for this approach . tested on a subset of the chinese treebank , our system improved significantly over the best previously reported results even when evaluated with this more stringent metric .

usp-each frequency-based greedy attribute selection for referring expressions generation
both greedy and domain-oriented reg algorithms have significant strengths but tend to perform poorly according to humanlikeness criteria as measured by , e.g. , dice scores . in this work we describe an attempt to combine both perspectives into a single attribute selection strategy to be used as part of the dale & reiter incremental algorithm in the reg challenge 2008 , and the results in both furniture and people domains .

typesetting for improved readability using
we present results from our study of which uses syntactically and semantically motivated information to group segments of sentences into unbreakable units for the purpose of typesetting those sentences in a region of a fixed width , using an otherwise standard dynamic programming line breaking algorithm , to minimize raggedness . in addition to a rule-based baseline segmenter , we use a very modest size text , manually annotated with positions of breaks , to train a maximum entropy classifier , relying on an extensive set of lexical and syntactic features , which can then predict whether or not to break after a certain word position in a sentence . we also use a simple genetic algorithm to search for a subset of the features optimizing f1 , to arrive at a set of features that delivers 89.2 % precision , 90.2 % recall ( 89.7 % f1 ) on a test set , improving the rule-based baseline by about 11 points and the classifier trained on all features by about 1 point in f1 .

recognising the predicateargument structure of tagalog
this paper describes research on parsing tagalog text for predicateargument structure ( pas ) . we first outline the linguistic phenomenon and corpus annotation process , then detail a series of pas parsing experiments .

variation of entropy and parse trees of sentences as a function of the
in this paper we explore the variation of sentences as a function of the sentence number . we demonstrate that while the entropy of the sentence increases with the sentence number , it decreases at the paragraph boundaries in accordance with the entropy rate constancy principle ( introduced in related work ) . we also demonstrate that the principle holds for different genres and languages and explore the role of genre informativeness . we investigate potential causes of entropy variation by looking at the tree depth , the branching factor , the size of constituents , and the occurrence of gapping .

enforcing transitivity in coreference resolution
a desirable quality of a coreference resolution system is the ability to handle transitivity constraints , such that even if it places high likelihood on a particular mention being coreferent with each of two other mentions , it will also consider the likelihood of those two mentions being coreferent when making a final assignment . this is exactly the kind of constraint that integer linear programming ( ilp ) is ideal for , but , surprisingly , previous work applying ilp to coreference resolution has not encoded this type of constraint . we train a coreference classifier over pairs of mentions , and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments . we present results on two commonly used datasets which show that enforcement of transitive closure consistently improves performance , including improvements of up to 3.6 % using the b3 scorer , and up to 16.5 % using cluster f-measure .

guided learning for bidirectional sequence classification
in this paper , we propose guided learning , a new learning framework for bidirectional sequence classification . the tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single perceptron like learning algorithm . we apply this novel learning algorithm to pos tagging . it obtains an error rate of 2.67 % on the standard ptb test set , which represents 3.3 % relative error reduction over the previous best result on the same data set , while using fewer features .

text generation for brazilian portuguese : the surface realization task
despite the growing interest in nlp focused on the brazilian portuguese language in recent years , its obvious counterpart natural language generation ( nlg ) remains in that case a little-explored research field . in this paper we describe preliminary results of a first project of this kind , addressing the issue of surface realization for brazilian portuguese . our approach , which may be particularly suitable to simpler nlg applications in which a domain corpus of the most likely output sentences happens to be available , is in principle adaptable to many closely-related languages , and paves the way to further nlg research focused on romance languages in general .

using derivation trees for informative treebank inter-annotator and mohamed maamouri linguistic data consortium beatrice santorini and
this paper discusses the extension of a system developed for automatic discovery of treebank annotation inconsistencies over an entire corpus to the particular case of evaluation of inter-annotator agreement . this system makes for a more informative iaa evaluation than other systems because it pinpoints the inconsistencies and groups them by their structural types . we evaluate the system on two corpora - ( 1 ) a corpus of english web text , and ( 2 ) a corpus of modern british english .

syntactic tree-based relation extraction using a generalization of collins and duffy convolution tree kernel mahdy khayyamian seyed abolghasem
relation extraction is a challenging task in natural language processing . syntactic features are recently shown to be quite effective for relation extraction . in this paper , we generalize the state of the art syntactic convolution tree kernel introduced by collins and duffy . the proposed generalized kernel is more flexible and customizable , and can be conveniently utilized for systematic generation of more effective application specific syntactic sub-kernels . using the generalized kernel , we will also propose a number of novel syntactic sub-kernels for relation extraction . these kernels show a remarkable performance improvement over the original collins and duffy kernel in the extraction of ace-2005 relation types .

learning nonstructural distance metric by minimum cluster distortions atr spoken language translation
much natural language processing still depends on the euclidean ( cosine ) distance function between two feature vectors , but this has severe problems with regard to feature weightings and feature correlations . to answer these problems , we propose an optimal metric distance that can be used as an alternative to the cosine distance , thus accommodating the two problems at the same time . this metric is optimal in the sense of global quadratic minimization , and can be obtained from the clusters in the training data in a supervised fashion . we confirmed the effect of the proposed metric distance by a synonymous sentence retrieval task , document retrieval task and the k-means clustering of general vectorial data . the results showed constant improvement over the baseline method of euclid and tf.idf , and were especially prominent for the sentence retrieval task , showing a 33 % increase in the 11-point average precision .

combining unsupervised and supervised alignments for mt : an empirical study
word alignment plays a central role in statistical mt ( smt ) since almost all smt systems extract translation rules from word aligned parallel training data . while most smt systems use unsupervised algorithms ( e.g . giza++ ) for training word alignment , supervised methods , which exploit a small amount of human-aligned data , have become increasingly popular recently . this work empirically studies the performance of these two classes of alignment algorithms and explores strategies to combine them to improve overall system performance . we used two unsupervised aligners , giza++ and hmm , and one supervised aligner , itg , in this study . to avoid language and genre specific conclusions , we ran experiments on test sets consisting of two language pairs ( chinese-to-english and arabicto-english ) and two genres ( newswire and weblog ) . results show that the two classes of algorithms achieve the same level of mt performance . modest improvements were achieved by taking the union of the translation grammars extracted from different alignments . significant improvements ( around 1.0 in bleu ) were achieved by combining outputs of different systems trained with different alignments . the improvements are consistent across languages and genres .

the role of positive feedback in intelligent tutoring systems
the focus of this study is positive feedback in one-on-one tutoring , its computational modeling , and its application to the design of more effective intelligent tutoring systems . a data collection of tutoring sessions in the domain of basic computer science data structures has been carried out . a methodology based on multiple regression is proposed , and some preliminary results are presented . a prototype intelligent tutoring system on linked lists has been developed and deployed in a collegelevel computer science class .

detecting speech repairs incrementally using a noisy channel approach
unrehearsed spoken language often contains disfluencies . in order to correctly interpret a spoken utterance , any such disfluencies must be identified and removed or otherwise dealt with . operating on transcripts of speech which contain disfluencies , our particular focus here is the identification and correction of speech repairs using a noisy channel model . our aim is to develop a high-accuracy mechanism that can identify speech repairs in an incremental fashion , as the utterance is processed word-by-word . we also address the issue of the evaluation of such incremental systems . we propose a novel approach to evaluation , which evaluates performance in detecting and correcting disfluencies incrementally , rather than only assessing performance once the processing of an utterance is complete . this demonstrates some shortcomings in our basic incremental model , and so we then demonstrate a technique that improves performance on the detection of disfluencies as they happen .

utilizing text mining results : the pastaweb system
information extraction ( ie ) , defined as the activity to extract structured knowledge from unstructured text sources , offers new opportunities for the exploitation of biological information contained in the vast amounts of scientific literature . but while ie technology has received increasing attention in the area of molecular biology , there have not been many examples of ie systems successfully deployed in end-user applications . we describe the development of pastaweb , a wwwbased interface to the extraction output of pasta , an ie system that extracts protein structure information from medline abstracts . key characteristics of pastaweb are the seamless integration of the pasta extraction results ( templates ) with wwwbased technology , the dynamic generation of www content from static data and the fusion of information extracted from multiple documents .

generating concept map exercises from textbooks
in this paper we present a methodology for creating concept map exercises for students . concept mapping is a common pedagogical exercise in which students generate a graphical model of some domain . our method automatically extracts knowledge representations from a textbook and uses them to generate concept maps . the purpose of the study is to generate and evaluate these concept maps according to their accuracy , completeness , and pedagogy .

an expert lexicon approach to identifying english phrasal verbs
phrasal verbs are an important feature of the english language . properly identifying them provides the basis for an english parser to decode the related structures . phrasal verbs have been a challenge to natural language processing ( nlp ) because they sit at the borderline between lexicon and syntax . traditional nlp frameworks that separate the lexicon module from the parser make it difficult to handle this problem properly . this paper presents a finite state approach that integrates a phrasal verb expert lexicon between shallow parsing and deep parsing to handle morpho-syntactic interaction . with precision/recall combined performance benchmarked consistently at 95.8 % -97.5 % , the phrasal verb identification problem has basically been solved with the presented method .

improvements in phrase-based statistical machine translation chair of computer science vi
in statistical machine translation , the currently best performing systems are based in some way on phrases or word groups . we describe the baseline phrase-based translation system and various refinements . we describe a highly efficient monotone search algorithm with a complexity linear in the input sentence length . we present translation results for three tasks : verbmobil , xerox and the canadian hansards . for the xerox task , it takes less than 7 seconds to translate the whole test set consisting of more than 10k words . the translation results for the xerox and canadian hansards task are very promising . the system even outperforms the alignment template system .

toward using morphology in french-english phrase-based smt
we describe the system used in our submission to the wmt-2009 french-english translation task . we use the moses phrasebased statistical machine translation system with two simple modications of the decoding input and word-alignment strategy based on morphology , and analyze their impact on translation quality .

speech-enabled computer-aided translation : a satisfaction survey with post-editor trainees
the present study has surveyed post-editor trainees views and attitudes before and after the

exploiting n-best hypotheses for smt self-enhancement
word and n-gram posterior probabilities estimated on n-best hypotheses have been used to improve the performance of statistical machine translation ( smt ) in a rescoring framework . in this paper , we extend the idea to estimate the posterior probabilities on n-best hypotheses for translation phrase-pairs , target language n-grams , and source word reorderings . the smt system is self-enhanced with the posterior knowledge learned from nbest hypotheses in a re-decoding framework . experiments on nist chinese-to-english task show performance improvements for all the strategies . moreover , the combination of the three strategies achieves further improvements and outperforms the baseline by 0.67 bleu score on nist-2003 set , and 0.64 on nist2005 set , respectively .

discovering user interactions in ideological discussions
online discussion forums are a popular platform for people to voice their opinions on any subject matter and to discuss or debate any issue of interest . in forums where users discuss social , political , or religious issues , there are often heated debates among users or participants . existing research has studied mining of user stances or camps on certain issues , opposing perspectives , and contention points . in this paper , we focus on identifying the nature of interactions among user pairs . the central questions are : how does each pair of users interact with each other does the pair of users mostly agree or disagree what is the lexicon that people often use to express agreement and disagreement we present a topic model based approach to answer these questions . since agreement and disagreement expressions are usually multiword phrases , we propose to employ a ranking method to identify highly relevant phrases prior to topic modeling . after modeling , we use the modeling results to classify the nature of interaction of each user pair . our evaluation results using real-life discussion/debate posts demonstrate the effectiveness of the proposed techniques .

sketch techniques for scaling distributional similarity to the web amit goyal , jagadeesh jagarlamudi , hal daume iii , and suresh venkatasubramanian
in this paper , we propose a memory , space , and time efficient framework to scale distributional similarity to the web . we exploit sketch techniques , especially the count-min sketch , which approximates the frequency of an item in the corpus without explicitly storing the item itself . these methods use hashing to deal with massive amounts of the streaming text . we store all item counts computed from 90 gb of web data in just 2 billion counters ( 8 gb main memory ) of cm sketch . our method returns semantic similarity between word pairs in o ( k ) time and can compute similarity between any word pairs that are stored in the sketch . in our experiments , we show that our framework is as effective as using the exact counts .

nested named entity recognition
many named entities contain other named entities inside them . despite this fact , the field of named entity recognition has almost entirely ignored nested named entity recognition , but due to technological , rather than ideological reasons . in this paper , we present a new technique for recognizing nested named entities , by using a discriminative constituency parser . to train the model , we transform each sentence into a tree , with constituents for each named entity ( and no other syntactic structure ) . we present results on both newspaper and biomedical corpora which contain nested named entities . in three out of four sets of experiments , our model outperforms a standard semi-crf on the more traditional top-level entities . at the same time , we improve the overall f-score by up to 30 % over the flat model , which is unable to recover any nested entities .

a two-stage model for content determination
in this paper we describe a two-stage model for content determination in systems that summarise time series data . the first stage involves building a qualitative overview of the data set , and the second involves using this overview , together with the actual data , to produce summaries of the timeseries data . this model is based on our observations of how human experts summarise time-series data .

transonics : a practical speech-to-speech translator for english-farsi
we briefly describe a two-way speech-tospeech english-farsi translation system prototype developed for use in doctorpatient interactions . the overarching philosophy of the developers has been to create a system that enables effective communication , rather than focusing on maximizing component-level performance . the discussion focuses on the general approach and evaluation of the system by an independent government evaluation team .

learning dictionaries for named entity recognition using minimal
this paper describes an approach for automatic construction of dictionaries for named entity recognition ( ner ) using large amounts of unlabeled data and a few seed examples . we use canonical correlation analysis ( cca ) to obtain lower dimensional embeddings ( representations ) for candidate phrases and classify these phrases using a small number of labeled examples . our method achieves 16.5 % and 11.3 % f-1 score improvement over co-training on disease and virus ner respectively . we also show that by adding candidate phrase embeddings as features in a sequence tagger gives better performance compared to using word embeddings .

towards robust animacy classification using morphosyntactic
this paper presents results from experiments in automatic classification of animacy for norwegian nouns using decision-tree classifiers . the method makes use of relative frequency measures for linguistically motivated morphosyntactic features extracted from an automatically annotated corpus of norwegian . the classifiers are evaluated using leave-oneout training and testing and the initial results are promising ( approaching 90 % accuracy ) for high frequency nouns , however deteriorate gradually as lower frequency nouns are classified . experiments attempting to empirically locate a frequency threshold for the classification method indicate that a subset of the chosen morphosyntactic features exhibit a notable resilience to data sparseness . results will be presented which show that the classification accuracy obtained for high frequency nouns ( with absolute frequencies > 1000 ) can be maintained for nouns with considerably lower frequencies ( 50 ) by backing off to a smaller set of features at classification .

investigation of annotators behaviour using eye-tracking data ryu iida koh mitsuda takenobu tokunaga
this paper presents an analysis of an annotators behaviour during her/his annotation process for eliciting useful information for natural language processing ( nlp ) tasks . text annotation is essential for machine learning-based nlp where annotated texts are used for both training and evaluating supervised systems . since an annotators behaviour during annotation can be seen as reflecting her/his cognitive process during her/his attempt to understand the text for annotation , analysing the process of text annotation has potential to reveal useful information for nlp tasks , in particular semantic and discourse processing that require deeper language understanding . we conducted an experiment for collecting annotator actions and eye gaze during the annotation of predicate-argument relations in japanese texts . our analysis of the collected data suggests that obtained insight into human annotation behaviour is useful for exploring effective linguistic features in machine learning-based approaches .

semantic roles for string to tree machine translation
we experiment with adding semantic role information to a string-to-tree machine translation system based on the rule extraction procedure of galley et al ( 2004 ) . we compare methods based on augmenting the set of nonterminals by adding semantic role labels , and altering the rule extraction process to produce a separate set of rules for each predicate that encompass its entire predicate-argument structure . our results demonstrate that the second approach is effective in increasing the quality of translations .

company-oriented extractive summarization of financial news
the paper presents a multi-document summarization system which builds companyspecific summaries from a collection of financial news such that the extracted sentences contain novel and relevant information about the corresponding organization . the users familiarity with the companys profile is assumed . the goal of such summaries is to provide information useful for the short-term trading of the corresponding company , i.e. , to facilitate the inference from news to stock price movement in the next day . we introduce a novel query ( i.e. , company name ) expansion method and a simple unsupervized algorithm for sentence ranking . the system shows promising results in comparison with a competitive baseline .

automatic story segmentation using a bayesian decision framework for statistical models of lexical chain features wai-kit
this paper presents a bayesian decision framework that performs automatic story segmentation based on statistical modeling of one or more lexical chain features . automatic story segmentation aims to locate the instances in time where a story ends and another begins . a lexical chain is formed by linking coherent lexical items chronologically . a story boundary is often associated with a significant number of lexical chains ending before it , starting after it , as well as a low count of chains continuing through it . we devise a bayesian framework to capture such behavior , using the lexical chain features of start , continuation and end . in the scoring criteria , lexical chain starts/ends are modeled statistically with the weibull and uniform distributions at story boundaries and non-boundaries respectively . the normal distribution is used for lexical chain continuations . full combination of all lexical chain features gave the best performance ( f1=0.6356 ) . we found that modeling chain continuations contributes significantly towards segmentation performance .

enhancing automatic term recognition through recognition of variation
terminological variation is an integral part of the linguistic ability to realise a concept in many ways , but it is typically considered an obstacle to automatic term recognition ( atr ) and term management . we present a method that integrates term variation in a hybrid atr approach , in which term candidates are recognised by a set of linguistic filters and termhood assignment is based on joint frequency of occurrence of all term variants . we evaluate the effectiveness of incorporating specific types of term variation by comparing it to the performance of a baseline method that treats term variants as separate terms . we show that atr precision is enhanced by considering joint termhoods of all term variants ,

using the wordnet hierarchy for associative anaphora resolution
in this paper , we explore how the taxonomic inheritance hierarchy in a semantic net can contribute to the resolution of associative anaphoric expressions . we present the results of some preliminary experiments and discuss both their implications and the scope for improvements to the technique .

a dynamic programming approach to document length constraints keith vander linden
natural language generation ( nlg ) applications must occasionally deliver rhetorically coherent output under length constraints . for example , certain types of documents must fit on a single webpage , on a cell phone screen , or into a fixed number of printed pages . to date , applications have achieved this goal by structuring their content as a rhetorical tree and using a greedy algorithm to pick the discourse elements to include in the final document . greedy algorithms are known to pick sub-optimal solutions . this paper presents an alternate approach based on dynamic programming .

a gibbs sampler for phrasal synchronous grammar induction
we present a phrasal synchronous grammar model of translational equivalence . unlike previous approaches , we do not resort to heuristics or constraints from a word-alignment model , but instead directly induce a synchronous grammar from parallel sentence-aligned corpora . we use a hierarchical bayesian prior to bias towards compact grammars with small translation units . inference is performed using a novel gibbs sampler over synchronous derivations . this sampler side-steps the intractability issues of previous models which required inference over derivation forests . instead each sampling iteration is highly efficient , allowing the model to be applied to larger translation corpora than previous approaches .

an alternative to head-driven approaches for parsing a ( relatively ) free word-order language reut tsarfaty khalil simaan remko scha
applying statistical parsers developed for english to languages with freer wordorder has turned out to be harder than expected . this paper investigates the adequacy of different statistical parsing models for dealing with a ( relatively ) free word-order language . we show that the recently proposed relationalrealizational ( rr ) model consistently outperforms state-of-the-art head-driven ( hd ) models on the hebrew treebank . our analysis reveals a weakness of hd models : their intrinsic focus on configurational information . we conclude that the form-function separation ingrained in rr models makes them better suited for parsing nonconfigurational phenomena .

sentiment analysis of conditional sentences
this paper studies sentiment analysis of conditional sentences . the aim is to determine whether opinions expressed on different topics in a conditional sentence are positive , negative or neutral . conditional sentences are one of the commonly used language constructs in text . in a typical document , there are around 8 % of such sentences . due to the condition clause , sentiments expressed in a conditional sentence can be hard to determine . for example , in the sentence , if your nokia phone is not good , buy this great samsung phone , the author is positive about samsung phone but does not express an opinion on nokia phone ( although the owner of the nokia phone may be negative about it ) . however , if the sentence does not have if , the first clause is clearly negative . although if commonly signifies a conditional sentence , there are many other words and constructs that can express conditions . this paper first presents a linguistic analysis of such sentences , and then builds some supervised learning models to determine if sentiments expressed on different topics in a conditional sentence are positive , negative or neutral . experimental results on conditional sentences from 5 diverse domains are given to demonstrate the effectiveness of the proposed approach .

iit patna : supervised approach for sentiment analysis in twitter
in this paper we report our works for semeval-2014 sentiment analysis in twitter evaluation challenge . this is the first time we attempt for this task , and our submissions are based on supervised machine learning algorithm . we use support vector machine for both the tasks , viz . contextual polarity disambiguation and message polarity classification . we identify and implement a small set of features for each the tasks , and did not make use of any external resources and/or tools . the systems are tuned on the development sets and finally blind evaluation is performed on the respective test set , which consists of the datasets of five different domains . our submission for the first task shows the f-score values of 76.3 % , 77.04 % , 70.91 % , 72.25 % and 66.32 % for livejournal2014 , sms2013 , twitter2013 , twitter2014 and twitter2014sarcasm datasets , respectively . the system developed for the second task yields the f-score values of 54.68 % , 40.56 % , 50.32 % , 48.22 % and 36.73 % , respectively for the five different test datasets .

automatic generation of large-scale paraphrases
research on paraphrase has mostly focussed on lexical or syntactic variation within individual sentences . our concern is with larger-scale paraphrases , from multiple sentences or paragraphs to entire documents . in this paper we address the problem of generating paraphrases of large chunks of texts . we ground our discussion through a worked example of extending an existing nlg system to accept as input a source text , and to generate a range of fluent semantically-equivalent alternatives , varying not only at the lexical and syntactic levels , but also in document structure and layout .

managing dialogue interaction : a multi-layered approach
we present evidence for the importance of low-level phenomena in dialogue interaction and use this to motivate a multi-layered approach to dialogue processing . we describe an architecture that separates content-level communicative processes from interaction-level phenomena ( such as feedback , grounding , turn-management ) , and provide details of specific implementations of a number of such phenomena .

utilizing microblogs for automatic news highlights extraction
story highlights form a succinct single-document summary consisting of 3-4 highlight sentences that reflect the gist of a news article . automatically producing news highlights is very challenging . we propose a novel method to improve news highlights extraction by using microblogs . the hypothesis is that microblog posts , although noisy , are not only indicative of important pieces of information in the news story , but also inherently short and sweet resulting from the artificial compression effect due to the length limit . given a news article , we formulate the problem as two rank-then-extract tasks : ( 1 ) we find a set of indicative tweets and use them to assist the ranking of news sentences for extraction ; ( 2 ) we extract top ranked tweets as a substitute of sentence extraction . results based on our news-tweets pairing corpus indicate that the method significantly outperform some strong baselines for single-document summarization .

automatic discovery of intentions in text and its application to question
semantic relations between text concepts denote the core elements of lexical semantics . this paper presents a model for the automatic detection of intention semantic relation . our approach first identifies the syntactic patterns that encode intentions , then we select syntactic and semantic features for a svm learning classifier . in conclusion , we discuss the application of intention relations to q & a .

domain-specific image captioning
we present a data-driven framework for image caption generation which incorporates visual and textual features with varying degrees of spatial structure . we propose the task of domain-specific image captioning , where many relevant visual details can not be captured by off-the-shelf general-domain entity detectors . we extract previously-written descriptions from a database and adapt them to new query images , using a joint visual and textual bag-of-words model to determine the correctness of individual words . we implement our model using a large , unlabeled dataset of womens shoes images and natural language descriptions ( berg et al. , 2010 ) . using both automatic and human evaluations , we show that our captioning method effectively deletes inaccurate words from extracted captions while maintaining a high level of detail in the generated output .

regmt system for machine translation , system combination , and
we present the results we obtain using our regmt system , which uses transductive regression techniques to learn mappings between source and target features of given parallel corpora and use these mappings to generate machine translation outputs . our training instance selection methods perform feature decay for proper selection of training instances , which plays an important role to learn correct feature mappings . regmt uses l2 regularized regression as well as l1 regularized regression for sparse regression estimation of target features . we present translation results using our training instance selection methods , translation results using graph decoding , system combination results with regmt , and performance evaluation with the f1 measure over target features as a metric for evaluating translation quality .

discourse parsing : learning fol rules based on rich verb semantic barbara di eugenio su nam kim
we report on our work to build a discourse parser ( semdp ) that uses semantic features of sentences . we use an inductive logic programming ( ilp ) system to exploit rich verb semantics of clauses to induce rules for discourse parsing . we demonstrate that ilp can be used to learn from highly structured natural language data and that the performance of a discourse parsing model that only uses semantic information is comparable to that of the state of the art syntactic discourse parsers .

a risk minimization framework for extractive
in this paper , we formulate extractive summarization as a risk minimization problem and propose a unified probabilistic framework that naturally combines supervised and unsupervised summarization models to inherit their individual merits as

mitextexplorer : linked brushing and mutual information for exploratory text data analysis
in this paper i describe a preliminary experimental system , mitextexplorer , for textual linked brushing , which allows an analyst to interactively explore statistical relationships between ( 1 ) terms , and ( 2 ) document metadata ( covariates ) . an analyst can graphically select documents embedded in a temporal , spatial , or other continuous space , and the tool reports terms with strong statistical associations for the region . the user can then drill down to specific term and term groupings , viewing further associations , and see how terms are used in context . the goal is to rapidly compare language usage across interesting document covariates . i illustrate examples of using the tool on several datasets : geo-located twitter messages , presidential state of the union addresses , the acl anthology , and the king james bible .

mining transliterations from wikipedia using pair hmms
this paper describes the use of a pair hidden markov model ( pair hmm ) system in mining transliteration pairs from noisy wikipedia data . a pair hmm variant that uses nine transition parameters , and emission parameters associated with single character mappings between source and target language alphabets is identified and used in estimating transliteration similarity . the system resulted in a precision of 78 % and recall of 83 % when evaluated on a random selection of english-russian wikipedia topics .

semantic analysis of chinese garden-path sentences
this paper presents a semantic model for chinese garden-path sentences . based on the sentence degeneration model of hnc theory , a garden-path can arise from two types of ambiguities : sd type ambiguity and np allocated ambiguity . this paper provides an approach to process garden-paths , in which ambiguity detection and analysis take the place of revision . the performance of the approach is evaluated on a small manually annotated test set . the results show that our algorithm can analyze chinese garden-path sentences effectively .

probabilistic models of similarity in syntactic context diarmuid o seaghdha
this paper investigates novel methods for incorporating syntactic information in probabilistic latent variable models of lexical choice and contextual similarity . the resulting models capture the effects of context on the interpretation of a word and in particular its effect on the appropriateness of replacing that word with a potentially related one . evaluating our techniques on two datasets , we report performance above the prior state of the art for estimating sentence similarity and ranking lexical substitutes .

empirical lower bounds on the complexity of translational equivalence
this paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts . the study found that the complexity of these patterns in every bitext was higher than suggested in the literature . these findings shed new light on why syntactic constraints have not helped to improve statistical translation models , including finitestate phrase-based models , tree-to-string models , and tree-to-tree models . the paper also presents evidence that inversion transduction grammars can not generate some translational equivalence relations , even in relatively simple real bitexts in syntactically similar languages with rigid word order . instructions for replicating our experiments are at http : //nlp.cs.nyu.edu/genpar/acl06

icelandic data driven part of speech tagging
data driven pos tagging has achieved good performance for english , but can still lag behind linguistic rule based taggers for morphologically complex languages , such as icelandic . we extend a statistical tagger to handle fine grained tagsets and improve over the best icelandic pos tagger . additionally , we develop a case tagger for non-local case and gender decisions . an error analysis of our system suggests future directions .

unsupervised alignment of privacy policies using hidden markov models
to support empirical study of online privacy policies , as well as tools for users with privacy concerns , we consider the problem of aligning sections of a thousand policy documents , based on the issues they address . we apply an unsupervised hmm ; in two new ( and reusable ) evaluations , we find the approach more effective than clustering and topic models .

yanfa : an online automatic scoring and intelligent feedback system of student english-chinese translation
online learning calls for instant assessment and feedback . yanfa is a system developed to score online englishchinese translation exercises with intelligent feedback for chinese non-english majors . with the aid of hownet and cilinchinese synonym set ( extended version ) , the system adopts the hybrid approach to scoring student translation semantically . it compares student translation with model translation by synonym matching , sentence-pattern matching and word similarity calculating respectively . the experiment results show that the correlation ratio between the scores given by the system and by human raters is 0.58 , which indicates that the algorithm is able to fulfill the task of automated scoring . yanfa is also able to provide feedback on syntactic mistakes made by students through interacting with them . it asks students to analyze the english sentence elements . then it compares the student analyses with those of the parser and points out the parts which might lead to their wrong understanding as well as their wrong translating .

interleaved semantic interpretation in environment-based parsing
this paper extends a polynomial-time parsing algorithm that resolves structural ambiguity in input sentences by calculating and comparing the denotations of rival constituents , given some model of the application environment ( schuler , 2001 ) . the algorithm is extended to incorporate a full set of logical operators , including quanti ers and conjunctions , into this calculation without increasing the complexity of the overall algorithm beyond polynomial time , both in terms of the length of the input and the number of entities in the environment model .

language acquisition and probabilistic models : keeping it simple aline villavicencio , marco idiartrobert berwick , igor malioutov
hierarchical bayesian models ( hbms ) have been used with some success to capture empirically observed patterns of under- and overgeneralization in child language acquisition . however , as is well known , hbms are ideal learning systems , assuming access to unlimited computational resources that may not be available to child language learners . consequently , it remains crucial to carefully assess the use of hbms along with alternative , possibly simpler , candidate models . this paper presents such an evaluation for a language acquisition domain where explicit hbms have been proposed : the acquisition of english dative constructions . in particular , we present a detailed , empiricallygrounded model-selection comparison of hbms vs. a simpler alternative based on clustering along with maximum likelihood estimation that we call linear competition learning ( lcl ) . our results demonstrate that lcl can match hbm model performance without incurring on the high computational costs associated with hbms .

systematicity and the lexicon in creative metaphor
aptness is an umbrella term that covers a multitude of issues in the interpretation and generation of creative metaphor . in this paper we concentrate on one of these issues the notion of lexical systematicity and explore its role in ascertaining the coherence of creative metaphor relative to the structure of the target concept being described . we argue that all else being equal , the most apt metaphors are those that resonate most with the way the target concept is literally and metaphorically organized . as such , the lexicon plays a key role in enforcing and recognizing aptness , insofar as this existing organization will already have been lexicalized . we perform our exploration in the context of wordnet , and describe how relational structures can be automatically extracted from this lexical taxonomy to facilitate the interpretation of creative metaphors .

determining case in arabic : learning complex linguistic behavior requires complex linguistic features
this paper discusses automatic determination of case in arabic . this task is a major source of errors in full diacritization of arabic . we use a gold-standard syntactic tree , and obtain an error rate of about 4.2 % , with a machine learning based system outperforming a system using hand-written rules . a careful error analysis suggests that when we account for annotation errors in the gold standard , the error rate drops to 0.8 % , with the hand-written rules outperforming the machine learning-based system .

analysis and development of urdu pos tagged corpus
in this paper , two corpora of urdu ( with 110k and 120k words ) tagged with different pos tagsets are used to train tnt and tree taggers . error analysis of both taggers is done to identify frequent confusions in tagging . based on the analysis of tagging , and syntactic structure of urdu , a more refined tagset is derived . the existing tagged corpora are tagged with the new tagset to develop a single corpus of 230k words and the tnt tagger is retrained . the results show improvement in tagging accuracy for individual corpora to 94.2 % and also for the merged corpus to 91 % . implications of these results are discussed .

efficient and robust lfg parsing : sxlfg
in this paper , we introduce a new parser , called sxlfg , based on the lexicalfunctional grammars formalism ( lfg ) . we describe the underlying context-free parser and how functional structures are efficiently computed on top of the cfg shared forest thanks to computation sharing , lazy evaluation , and compact data representation . we then present various error recovery techniques we implemented in order to build a robust parser . finally , we offer concrete results when sxlfg is used with an existing grammar for french . we show that our parser is both efficient and robust , although the grammar is very ambiguous .

looking under the hood : tools for diagnosing your question
in this paper we analyze two question answering tasks : the trec-8 question answering task and a set of reading comprehension exams . first , we show that q/a systems perform better when there are multiple answer opportunities per question . next , we analyze common approaches to two subproblems : term overlap for answer sentence identification , and answer typing for short answer extraction . we present general tools for analyzing the strengths and limitations of techniques for these subproblems . our results quantify the limitations of both term overlap and answer typing to distinguish between competing answer candidates .

automatically learning cognitive status for multi-document summarization of newswire
machine summaries can be improved by using knowledge about the cognitive status of news article referents . in this paper , we present an approach to automatically acquiring distinctions in cognitive status using machine learning over the forms of referring expressions appearing in the input . we focus on modeling references to people , both because news often revolve around people and because existing natural language tools for named entity identification are reliable . we examine two specific distinctionswhether a person in the news can be assumed to be known to a target audience ( hearer-old vs hearer-new ) and whether a person is a major character in the news story . we report on machine learning experiments that show that these distinctions can be learned with high accuracy , and validate our approach using human subjects .

optimal search for minimum error rate training
minimum error rate training is a crucial component to many state-of-the-art nlp applications , such as machine translation and speech recognition . however , common evaluation functions such as bleu or word error rate are generally highly non-convex and thus prone to search errors . in this paper , we present lp-mert , an exact search algorithm for minimum error rate training that reaches the global optimum using a series of reductions to linear programming . given a set of n -best lists produced from s input sentences , this algorithm finds a linear model that is globally optimal with respect to this set . we find that this algorithm is polynomial in n and in the size of the model , but exponential in s. we present extensions of this work that let us scale to reasonably large tuning sets ( e.g. , one thousand sentences ) , by either searching only promising regions of the parameter space , or by using a variant of lp-mert that relies on a beam-search approximation . experimental results show improvements over the standard och algorithm .

word sense disambiguation using static and dynamic sense
it is popular in wsd to use contextual information in training sense tagged data . co-occurring words within a limited window-sized context support one sense among the semantically ambiguous ones of the word . this paper reports on word sense disambiguation of english words using static and dynamic sense vectors . first , context vectors are constructed using contextual words 1 in the training sense tagged data . then , the words in the context vector are weighted with local density . using the whole training sense tagged data , each sense of a target word2 is represented as a static sense vector in word space , which is the centroid of the context vectors . then contextual noise is removed using a automatic selective sampling . a automatic selective sampling method use information retrieval technique , so as to enhance the discriminative power . in each test case , a automatic selective sampling method retrieves n relevant training samples to reduce noise . using them , we construct another sense vectors for each sense of the target word .

analysis of semantic classes in medical text for question answering
to answer questions from clinical-evidence texts , we identify occurrences of the semantic classes disease , medication , patient outcome that are candidate elements of the answer , and the relations among them . additionally , we determine whether an outcome is positive or negative .

automatic extraction of fixed multiword
the automatic extraction of such expressions . our method involves three stages . in the first , a statistical measure is used to extract candidate bigrams . in the second , we use this list to select occurrences of candidate expressions in a corpus , together with their surrounding contexts . these examples are used as training data for supervised machine learning , resulting in a classifier which can identify target multiword expressions . the final stage is the estimation of the part of speech of each extracted expression based on its context of occurence . evaluation demonstrated that collocation measures alone are not effective in identifying target expressions . however , when trained on one million examples , the classifier identified target multiword expressions with precision greater than 90 % . part of speech estimation had precision and recall of over 95 % .

a multilingual analysis of the notion of instrumentality
instruments are expressed in language by various means : prepositions , postpositions , affixes including case marks , nonfinite verbs , etc . we consider here 12 languages from five families in order to be able to identify the different meaning components that structure instrumentality .

using natural language processing to classify suicide notes sarah arszman lavanier , jennifer combs , and robert kowatch
we hypothesize that machine-learning algorithms ( mla ) can classify completer and simulated suicide notes as well as mental health professionals ( mhp ) . five mhps classified 66 simulated or completer notes ; mlas were used for the same task . results : mhps were accurate 71 % of the time ; using the sequential minimization optimization algorithm ( smo ) mlas were accurate 78 % of the time . there was no significant difference between the mla and mph classifiers . this is an important first step in developing an evidence based suicide predictor for emergency department use .

understanding tables in context using standard nlp toolkits
tabular information in text documents contains a wealth of information , and so tables are a natural candidate for information extraction . there are many cues buried in both a table and its surrounding text that allow us to understand the meaning of the data in a table . we study how natural-language tools , such as part-of-speech tagging , dependency paths , and named-entity recognition , can be used to improve the quality of relation extraction from tables . in three domains we show that ( 1 ) a model that performs joint probabilistic inference across tabular and natural language features achieves an f1 score that is twice as high as either a puretable or pure-text system , and ( 2 ) using only shallower features or non-joint inference results in lower quality .

passage retrieval for question answering using sliding windows
the information retrieval ( ir ) community has investigated many different techniques to retrieve passages from large collections of documents for question answering ( qa ) . in this paper , we specifically examine and quantitatively compare the impact of passage retrieval for qa using sliding windows and disjoint windows . we consider two different data sets , the trec 20022003 qa data set , and 93 whyquestions against inex wikipedia . we discovered that , compared to disjoint windows , using sliding windows results in improved performance of trec-qa in terms of tdrr , and in improved performance of why-qa in terms of success @ n and mrr .

mining opinion words and opinion targets in a two-stage framework
this paper proposes a novel two-stage method for mining opinion words and opinion targets . in the first stage , we propose a sentiment graph walking algorithm , which naturally incorporates syntactic patterns in a sentiment graph to extract opinion word/target candidates . then random walking is employed to estimate confidence of candidates , which improves extraction accuracy by considering confidence of patterns . in the second stage , we adopt a self-learning strategy to refine the results from the first stage , especially for filtering out high-frequency noise terms and capturing the long-tail terms , which are not investigated by previous methods . the experimental results on three real world datasets demonstrate the effectiveness of our approach compared with stateof-the-art unsupervised methods .

extractive summarization and dialogue act modeling on email threads : an integrated probabilistic approach
in this paper , we present a novel supervised approach to the problem of summarizing email conversations and modeling dialogue acts . we assume that there is a relationship between dialogue acts and important sentences . based on this assumption , we introduce a sequential graphical model approach which simultaneously summarizes email conversation and models dialogue acts . we compare our model with sequential and non-sequential models , which independently conduct the tasks of extractive summarization and dialogue act modeling . an empirical evaluation shows that our approach significantly outperforms all baselines in classifying correct summary sentences without losing performance on dialogue act modeling task .

information density , heaps law , and perception of factiness in news
seeking information online can be an exercise in time wasted wading through repetitive , verbose text with little actual content . some documents are more densely populated with factoids ( fact-like claims ) than others . the densest documents are potentially the most efficient use of time , likely to include the most information . thus some measure of factiness might be useful to readers . based on crowdsourced ratings of the factual content of 772 online news articles , we find that after controlling for widely varying document length using heaps law , a significant positive correlation exists between perceived factual content and relative information entropy .

using english for commonsense knowledge
the work reported here arises from an attempt to provide a body of simple information about diet and its effect on various common medical conditions . expressing this knowledge in natural language has a number of advantages . it also raises a number of difficult issues . we will consider solutions , and partial solutions , to these issues below .

on maximum spanning dag algorithms for semantic dag parsing
consideration of the decoding problem in semantic parsing as finding a maximum spanning dag of a weighted directed graph carries many complexities that havent been fully addressed in the literature to date , among which are its actual appropriateness for the decoding task in semantic parsing , not to mention an explicit proof of its complexity ( and its approximability ) . in this paper , we consider the objective function for the maximum spanning dag problem , and what it means in terms of decoding for semantic parsing . in doing so , we give anecdotal evidence against its use in this task . in addition , we consider the only graph-based maximum spanning dag approximation algorithm presented in the literature ( without any approximation guarantee ) to date and finally provide an approximation guarantee for it , showing that it is an o ( 1 n ) factor approximation algorithm , where n is the size of the digraphs vertex set .

a comparison of features for automatic readability assessment
several sets of explanatory variables including shallow , language modeling , pos , syntactic , and discourse features are compared and evaluated in terms of their impact on predicting the grade level of reading material for primary school students . we find that features based on in-domain language models have the highest predictive power . entity-density ( a discourse feature ) and pos-features , in particular nouns , are individually very useful but highly correlated . average sentence length ( a shallow feature ) is more useful and less expensive to compute than individual syntactic features . a judicious combination of features examined here results in a significant improvement over the state of the art .

the role of lexical resources in cjk natural language processing
the role of lexical resources is often understated in nlp research . the complexity of chinese , japanese and korean ( cjk ) poses special challenges to developers of nlp tools , especially in the area of word segmentation ( ws ) , information retrieval ( ir ) , named entity extraction ( ner ) , and machine translation ( mt ) . these difficulties are exacerbated by the lack of comprehensive lexical resources , especially for proper nouns , and the lack of a standardized orthography , especially in japanese . this paper summarizes some of the major linguistic issues in the development nlp applications that are dependent on lexical resources , and discusses the central role such resources should play in enhancing the accuracy of nlp tools .

a corpus of italian web texts
pais ` a is a creative commons licensed , large web corpus of contemporary italian . we describe the design , harvesting , and processing steps involved in its creation .

combining deep and shallow approaches in parsing german
the paper describes two parsing schemes : a shallow approach based on machine learning and a cascaded finite-state parser with a hand-crafted grammar . it discusses several ways to combine them and presents evaluation results for the two individual approaches and their combination . an underspecification scheme for the output of the finite-state parser is introduced and shown to improve performance .

building a korean web corpus for analyzing learner language
post-positional particles are a significant source of errors for learners of korean . following methodology that has proven effective in handling english preposition errors , we are beginning the process of building a machine learner for particle error detection in l2 korean writing . as a first step , however , we must acquire data , and thus we present a methodology for constructing large-scale corpora of korean from the web , exploring the feasibility of building corpora appropriate for a given topic and grammatical construction .

arabic morphological tagging , diacritization , and lemmatization using lexeme models and feature ranking
we investigate the tasks of general morphological tagging , diacritization , and lemmatization for arabic . we show that for all tasks we consider , both modeling the lexeme explicitly , and retuning the weights of individual classifiers for the specific task , improve the performance .

training dependency parsers by jointly optimizing multiple objectives
we present an online learning algorithm for training parsers which allows for the inclusion of multiple objective functions . the primary example is the extension of a standard supervised parsing objective function with additional loss-functions , either based on intrinsic parsing quality or task-specific extrinsic measures of quality . our empirical results show how this approach performs for two dependency parsing algorithms ( graph-based and transition-based parsing ) and how it achieves increased performance on multiple target tasks including reordering for machine translation and parser adaptation .

a unified single scan algorithm for japanese base phrase chunking and dependency parsing yahoo japan corporation
we describe an algorithm for japanese analysis that does both base phrase chunking and dependency parsing simultaneously in linear-time with a single scan of a sentence . in this paper , we show a pseudo code of the algorithm and evaluate its performance empirically on the kyoto university corpus . experimental results show that the proposed algorithm with the voted perceptron yields reasonably good accuracy .

inferring semantic roles using sub-categorization frames and maximum entropy model
in this paper , we propose an approach for inferring semantic role using subcategorization frames and maximum entropy model . our approach aims to use the sub-categorization information of the verb to label the mandatory arguments of the verb in various possible ways . the ambiguity between the assignment of roles to mandatory arguments is resolved using the maximum entropy model . the unlabelled mandatory arguments and the optional arguments are labelled directly using the maximum entropy model such that their labels are not one among the frame elements of the sub-categorization frame used . maximum entropy model is preferred because of its novel approach of smoothing . using this approach , we obtained an f-measure of 68.14 % on the development set of the data provided for the conll-2005 shared task . we show that this approach performs well in comparison to an approach which uses only the maximum entropy model .

efficient unsupervised recursive word segmentation
automatic word segmentation is a basic requirement for unsupervised learning in morphological analysis . in this paper , we formulate a novel recursive method for minimum description length ( mdl ) word segmentation , whose basic operation is resegmenting the corpus on a prefix ( equivalently , a suffix ) . we derive a local expression for the change in description length under resegmentation , i.e. , one which depends only on properties of the specific prefix ( not on the rest of the corpus ) . such a formulation permits use of a new and efficient algorithm for greedy morphological segmentation of the corpus in a recursive manner . in particular , our method does not restrict words to be segmented only once , into a stem+affix form , as do many extant techniques . early results for english and turkish corpora are promising .

using parse features for preposition selection and error detection educational testing service
we evaluate the effect of adding parse features to a leading model of preposition usage . results show a significant improvement in the preposition selection task on native speaker text and a modest increment in precision and recall in an esl error detection task . analysis of the parser output indicates that it is robust enough in the face of noisy non-native writing to extract useful information .

integration of a lexical type database with a linguistically interpreted
we have constructed a large scale and detailed database of lexical types in japanese from a treebank that includes detailed linguistic information . the database helps treebank annotators and grammar developers to share precise knowledge about the grammatical status of words that constitute the treebank , allowing for consistent large scale treebanking and grammar development . in this paper , we report on the motivation and methodology of the database construction .

building lexicon for sentiment analysis from massive collection of html
recognizing polarity requires a list of polar words and phrases . for the purpose of building such lexicon automatically , a lot of studies have investigated ( semi- ) unsupervised method of learning polarity of words and phrases . in this paper , we explore to use structural clues that can extract polar sentences from japanese html documents , and build lexicon from the extracted polar sentences . the key idea is to develop the structural clues so that it achieves extremely high precision at the cost of recall . in order to compensate for the low recall , we used massive collection of html documents . thus , we could prepare enough polar sentence corpus .

semi-markov phrase-based monolingual alignment
we introduce a novel discriminative model for phrase-based monolingual alignment using a semi-markov crf . our model achieves stateof-the-art alignment accuracy on two phrasebased alignment datasets ( rte and paraphrase ) , while doing significantly better than other strong baselines in both non-identical alignment and phrase-only alignment . additional experiments highlight the potential benefit of our alignment model to rte , paraphrase identification and question answering , where even a naive application of our models alignment score approaches the state of the art .

an analysis of clarification dialogue for question answering
we examine clarification dialogue , a mechanism for refining user questions with follow-up questions , in the context of open domain question answering systems . we develop an algorithm for clarification dialogue recognition through the analysis of collected data on clarification dialogues and examine the importance of clarification dialogue recognition for question answering . the algorithm is evaluated and shown to successfully recognize the occurrence of clarification dialogue in the majority of cases and to simplify the task of answer retrieval .

a large-scale exploration of effective global features for a joint entity detection and tracking model
entity detection and tracking ( edt ) is the task of identifying textual mentions of real-world entities in documents , extending the named entity detection and coreference resolution task by considering mentions other than names ( pronouns , definite descriptions , etc . ) . like ne tagging and coreference resolution , most solutions to the edt task separate out the mention detection aspect from the coreference aspect . by doing so , these solutions are limited to using only local features for learning . in contrast , by modeling both aspects of the edt task simultaneously , we are able to learn using highly complex , non-local features . we develop a new joint edt model and explore the utility of many features , demonstrating their effectiveness on this task .

summarize what you are interested in : an optimization framework for interactive personalized summarization
most traditional summarization methods treat their outputs as static and plain texts , which fail to capture user interests during summarization because the generated summaries are the same for different users . however , users have individual preferences on a particular source document collection and obviously a universal summary for all users might not always be satisfactory . hence we investigate an important and challenging problem in summary generation , i.e. , interactive personalized summarization ( ips ) , which generates summaries in an interactive and personalized manner . given the source documents , ips captures user interests by enabling interactive clicks and incorporates personalization by modeling captured reader preference . we develop experimental systems to compare 5 rival algorithms on 4 instinctively different datasets which amount to 5197 documents . evaluation results in rouge metrics indicate the comparable performance between ips and the best competing system but ips produces summaries with much more user satisfaction according to evaluator ratings . besides , low rouge consistency among these user preferred summaries indicates the existence of personalization .

adjoining tree-to-string translation
we introduce synchronous tree adjoining grammars ( tag ) into tree-to-string translation , which converts a source tree to a target string . without reconstructing tag derivations explicitly , our rule extraction algorithm directly learns tree-to-string rules from aligned treebank-style trees . as tree-to-string translation casts decoding as a tree parsing problem rather than parsing , the decoder still runs fast when adjoining is included . less than 2 times slower , the adjoining tree-tostring system improves translation quality by +0.7 bleu over the baseline system only allowing for tree substitution on nist chineseenglish test sets .

an investigation for implicatures in chinese : implicatures in chinese and in english are similar ! intelligent systems program
implicit opinions are commonly seen in opinion-oriented documents , such as political editorials . previous work have utilized opinion inference rules to detect implicit opinions evoked by events that positively/negatively affect entities ( goodfor/badfor ) to improve sentiment analysis for english text . since people in different languages may express implicit opinions in different ways , in this work we investigate implicit opinions expressed via goodfor/badfor events in chinese . the positive results have provided evidences that such implicit opinions and inference rules are similar in chinese and in english . moreover , we have observed cases where the inferences are blocked .

two approaches to correcting homophone confusions in a hybrid machine translation system
in the context of a hybrid french-toenglish smt system for translating online forum posts , we present two methods for addressing the common problem of homophone confusions in colloquial written language . the first is based on hand-coded rules ; the second on weighted graphs derived from a large-scale pronunciation resource , with weights trained from a small bicorpus of domain language . with automatic evaluation , the weighted graph method yields an improvement of about +0.63 bleu points , while the rulebased method scores about the same as the baseline . on contrastive manual evaluation , both methods give highly significant improvements ( p < 0.0001 ) and score about equally when compared against each other .

acquiring event relation knowledge by learning cooccurrence patterns and fertilizing cooccurrence samples with verbal nouns shuya abe kentaro inui yuji matsumoto
aiming at acquiring semantic relations between events from a large corpus , this paper proposes several extensions to a state-of-theart method originally designed for entity relation extraction , reporting on the present results of our experiments on a japanese web corpus . the results show that ( a ) there are indeed specific cooccurrence patterns useful for event relation acquisition , ( b ) the use of cooccurrence samples involving verbal nouns has positive impacts on both recall and precision , and ( c ) over five thousand relation instances are acquired from a 500m-sentence web corpus with a precision of about 66 % for action-effect relations .

discriminative lexicon adaptation for improved character accuracy a new direction in chinese language modeling
while oov is always a problem for most languages in asr , in the chinese case the problem can be avoided by utilizing character n-grams and moderate performances can be obtained . however , character ngram has its own limitation and proper addition of new words can increase the asr performance . here we propose a discriminative lexicon adaptation approach for improved character accuracy , which not only adds new words but also deletes some words from the current lexicon . different from other lexicon adaptation approaches , we consider the acoustic features and make our lexicon adaptation criterion consistent with that in the decoding process . the proposed approach not only improves the asr character accuracy but also significantly enhances the performance of a characterbased spoken document retrieval system .

leveraging domain-independent information in semantic parsing
semantic parsing is a domain-dependent process by nature , as its output is defined over a set of domain symbols . motivated by the observation that interpretation can be decomposed into domain-dependent and independent components , we suggest a novel interpretation model , which augments a domain dependent model with abstract information that can be shared by multiple domains . our experiments show that this type of information is useful and can reduce the annotation effort significantly when moving between domains .

representing uncertainty about complex user goals in statistical
we point out several problems in scalingup statistical approaches to spoken dialogue systems to enable them to deal with complex but natural user goals , such as disjunctive and negated goals and preferences . in particular , we explore restrictions imposed by current independence assumptions in pomdp dialogue models . this position paper proposes the use of automatic belief compression methods to remedy these problems .

an algorithm for unsupervised transliteration mining with an application to word alignment
we propose a language-independent method for the automatic extraction of transliteration pairs from parallel corpora . in contrast to previous work , our method uses no form of supervision , and does not require linguistically informed preprocessing . we conduct experiments on data sets from the news 2010 shared task on transliteration mining and achieve an f-measure of up to 92 % , outperforming most of the semi-supervised systems that were submitted . we also apply our method to english/hindi and english/arabic parallel corpora and compare the results with manually built gold standards which mark transliterated word pairs . finally , we integrate the transliteration module into the giza++ word aligner and evaluate it on two word alignment tasks achieving improvements in both precision and recall measured against gold standard word alignments .

underspecified beta reduction
for ambiguous sentences , traditional semantics construction produces large numbers of higher-order formulas , which must then be -reduced individually . underspecified versions can produce compact descriptions of all readings , but it is not known how to perform -reduction on these descriptions . we show how to do this using -reduction constraints in the constraint language for -structures ( clls ) .

graph based semi-supervised approach for information extraction hany hassan ahmed hassan sara noeman
classification techniques deploy supervised labeled instances to train classifiers for various classification problems . however labeled instances are limited , expensive , and time consuming to obtain , due to the need of experienced human annotators . meanwhile large amount of unlabeled data is usually easy to obtain . semi-supervised learning addresses the problem of utilizing unlabeled data along with supervised labeled data , to build better classifiers . in this paper we introduce a semi-supervised approach based on mutual reinforcement in graphs to obtain more labeled data to enhance the classifier accuracy . the approach has been used to supplement a maximum entropy model for semi-supervised training of the ace relation detection and characterization ( rdc ) task . ace rdc is considered a hard task in information extraction due to lack of large amounts of training data and inconsistencies in the available data . the proposed approach provides 10 % relative improvement over the state of the art supervised baseline system .

modular and efficient top-down parsing for ambiguous left-recursive
in functional and logic programming , parsers can be built as modular executable specifications of grammars , using parser combinators and definite clause grammars respectively . these techniques are based on top-down backtracking search . commonly used implementations are inefficient for ambiguous languages , can not accommodate left-recursive grammars , and require exponential space to represent parse trees for highly ambiguous input . memoization is known to improve efficiency , and work by other researchers has had some success in accommodating left recursion . this paper combines aspects of previous approaches and presents a method by which parsers can be built as modular and efficient executable specifications of ambiguous grammars containing unconstrained left recursion .

a memorybased learning approach to event extraction in biomedical texts
in this paper we describe the memory-based machine learning system that we submitted to the bionlp shared task on event extraction . we modeled the event extraction task using an approach that has been previously applied to other natural language processing tasks like semantic role labeling or negation scope finding . the results obtained by our system ( 30.58 f-score in task 1 and 29.27 in task 2 ) suggest that the approach and the system need further adaptation to the complexity involved in extracting biomedical events .

how creative is your writing a linguistic creativity measure from computer science and cognitive psychology perspectives
we demonstrate that subjective creativity in sentence-writing can in part be predicted using computable quantities studied in computer science and cognitive psychology . we introduce a task in which a writer is asked to compose a sentence given a keyword . the sentence is then assigned a subjective creativity score by human judges . we build a linear regression model which , given the keyword and the sentence , predicts the creativity score . the model employs features on statistical language models from a large corpus , psychological word norms , and wordnet .

hal-based cascaded model for variable-length semantic pattern induction from psychiatry web resources
negative life events play an important role in triggering depressive episodes . developing psychiatric services that can automatically identify such events is beneficial for mental health care and prevention . before these services can be provided , some meaningful semantic patterns , such as < lost , parents > , have to be extracted . in this work , we present a text mining framework capable of inducing variable-length semantic patterns from unannotated psychiatry web resources . this framework integrates a cognitive motivated model , hyperspace analog to language ( hal ) , to represent words as well as combinations of words . then , a cascaded induction process ( cip ) bootstraps with a small set of seed patterns and incorporates relevance feedback to iteratively induce more relevant patterns . the experimental results show that by combining the hal model and relevance feedback , the cip can induce semantic patterns from the unannotated web corpora so as to reduce the reliance on annotated corpora .

probabilistic dialogue modelling
we show how bayesian networks and related probabilistic methods provide an efficient way of capturing the complex balancing of different factors that determine interpretation and generation in dialogue . as a case study , we show how a probabilistic approach can be used to model anaphora resolution in dialogue1 .

text modification for bulgarian sign language users slavina lozanova ivelina stoyanova svetlozara leseva svetla koeva boian savtchev
the paper discusses the main issues regarding the reading skills and comprehension proficiency in written bulgarian of people with communication difficulties , and deaf people , in particular . we consider several key components of text comprehension which pose a challenge for deaf readers and propose a rule-based system for automatic modification of bulgarian texts intended to facilitate comprehension by deaf people , to assist education , etc . in order to demonstrate the benefits of such a system and to evaluate its performance , we have carried out a study among a group of deaf people who use bulgarian sign language ( bulsl ) as their primary language ( primary bulsl users ) , which compares the comprehensibility of original texts and their modified versions . the results shows a considerable improvement in readability when using modified texts , but at the same time demonstrates that the level of comprehension is still low , and that a complex set of modifications will have to be implemented to attain satisfactory results .

reconciling ontonotes : unrestricted coreference resolution in ontonotes with reconcile
this paper describes our entry to the 2011 conll closed task ( pradhan et al , 2011 ) on modeling unrestricted coreference in ontonotes . our system is based on the reconcile coreference resolution research platform . reconcile is a general software infrastructure for the development of learning-based noun phrase ( np ) coreference resolution systems . our entry for the conll closed task is a configuration of reconcile intended to do well on ontonotes data . this paper describes our configuration of reconcile as well as the changes that we had to implement to integrate with the ontonotes task definition and data formats . we also present and discuss the performance of our system under different testing conditions on a withheld validation set .

medtag : a collection of biomedical annotations
we present a database of annotated biomedical text corpora merged into a portable data structure with uniform conventions . medtag combines three corpora , medpost , abgene and genetag , within a common relational database data model . the genetag corpus has been modified to reflect new definitions of genes and proteins . the medpost corpus has been updated to include 1,000 additional sentences from the clinical medicine domain . all data have been updated with original medline text excerpts , pubmed identifiers , and tokenization independence to facilitate data accuracy , consistency and usability . the data are available in flat files along with software to facilitate loading the data into a relational sql database from ftp : //ftp.ncbi.nlm.nih.gov/pub/lsmith /medtag/medtag.tar.gz .

