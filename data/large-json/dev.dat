{"title": "an entropy search portfolio for bayesian optimization", "abstract": "bayesian optimization is a sample-efficient method for black-box global optimization . how- ever , the performance of a bayesian optimization method very much depends on its exploration strategy , i.e . the choice of acquisition function , and it is not clear a priori which choice will result in superior performance . while portfolio methods provide an effective , principled way of combining a collection of acquisition functions , they are often based on measures of past performance which can be misleading . to address this issue , we introduce the entropy search portfolio ( esp ) : a novel approach to portfolio construction which is motivated by information theoretic considerations . we show that esp outperforms existing portfolio methods on several real and synthetic problems , including geostatistical datasets and simulated control tasks . we not only show that esp is able to offer performance as good as the best , but unknown , acquisition function , but surprisingly it often gives better performance . finally , over a wide range of conditions we find that esp is robust to the inclusion of poor acquisition functions .", "topics": ["sampling ( signal processing )", "synthetic data"]}
{"title": "safe pattern pruning : an efficient approach for predictive pattern mining", "abstract": "in this paper we study predictive pattern mining problems where the goal is to construct a predictive model based on a subset of predictive patterns in the database . our main contribution is to introduce a novel method called safe pattern pruning ( spp ) for a class of predictive pattern mining problems . the spp method allows us to efficiently find a superset of all the predictive patterns in the database that are needed for the optimal predictive model . the advantage of the spp method over existing boosting-type method is that the former can find the superset by a single search over the database , while the latter requires multiple searches . the spp method is inspired by recent development of safe feature screening . in order to extend the idea of safe feature screening into predictive pattern mining , we derive a novel pruning rule called safe pattern pruning ( spp ) rule that can be used for searching over the tree defined among patterns in the database . the spp rule has a property that , if a node corresponding to a pattern in the database is pruned out by the spp rule , then it is guaranteed that all the patterns corresponding to its descendant nodes are never needed for the optimal predictive model . we apply the spp method to graph mining and item-set mining problems , and demonstrate its computational advantage .", "topics": ["data mining"]}
{"title": "on weight initialization in deep neural networks", "abstract": "a proper initialization of the weights in a neural network is critical to its convergence . current insights into weight initialization come primarily from linear activation functions . in this paper , i develop a theory for weight initializations with non-linear activations . first , i derive a general weight initialization strategy for any neural network using activation functions differentiable at 0 . next , i derive the weight initialization strategy for the rectified linear unit ( relu ) , and provide theoretical insights into why the xavier initialization is a poor choice with relu activations . my analysis provides a clear demonstration of the role of non-linearities in determining the proper weight initializations .", "topics": ["nonlinear system"]}
{"title": "generating markov equivalent maximal ancestral graphs by single edge replacement", "abstract": "maximal ancestral graphs ( mags ) are used to encode conditional independence relations in dag models with hidden variables . different mags may represent the same set of conditional independences and are called markov equivalent . this paper considers mags without undirected edges and shows conditions under which an arrow in a mag can be reversed or interchanged with a bi-directed edge so as to yield a markov equivalent mag .", "topics": ["markov chain"]}
{"title": "dags with no tears : smooth optimization for structure learning", "abstract": "estimating the structure of directed acyclic graphs ( dags , also known as bayesian networks ) is a challenging problem since the search space of dags is combinatorial and scales superexponentially with the number of nodes . existing approaches rely on various local heuristics for enforcing the acyclicity constraint and are not well-suited to general purpose optimization packages for their solution . in this paper , we introduce a fundamentally different strategy : we formulate the structure learning problem as a smooth , constrained optimization problem over real matrices that avoids this combinatorial constraint entirely . this is achieved by a novel characterization of acyclicity that is not only smooth but also exact . the resulting nonconvex , constrained program involves smooth functions whose gradients are easy to compute and only involve elementary matrix operations . by using existing black-box optimization routines , our method uses global search to find an optimal dag and can be implemented in about 50 lines of python and outperforms existing methods without imposing any structural constraints .", "topics": ["optimization problem", "mathematical optimization"]}
{"title": "deep convolutional neural network for inverse problems in imaging", "abstract": "in this paper , we propose a novel deep convolutional neural network ( cnn ) -based algorithm for solving ill-posed inverse problems . regularized iterative algorithms have emerged as the standard approach to ill-posed inverse problems in the past few decades . these methods produce excellent results , but can be challenging to deploy in practice due to factors including the high computational cost of the forward and adjoint operators and the difficulty of hyper parameter selection . the starting point of our work is the observation that unrolled iterative methods have the form of a cnn ( filtering followed by point-wise non-linearity ) when the normal operator ( h*h , the adjoint of h times h ) of the forward model is a convolution . based on this observation , we propose using direct inversion followed by a cnn to solve normal-convolutional inverse problems . the direct inversion encapsulates the physical model of the system , but leads to artifacts when the problem is ill-posed ; the cnn combines multiresolution decomposition and residual learning in order to learn to remove these artifacts while preserving image structure . we demonstrate the performance of the proposed network in sparse-view reconstruction ( down to 50 views ) on parallel beam x-ray computed tomography in synthetic phantoms as well as in real experimental sinograms . the proposed network outperforms total variation-regularized iterative reconstruction for the more realistic phantoms and requires less than a second to reconstruct a 512 x 512 image on gpu .", "topics": ["nonlinear system", "synthetic data"]}
{"title": "dimensionality reduction by local discriminative gaussians", "abstract": "we present local discriminative gaussian ( ldg ) dimensionality reduction , a supervised dimensionality reduction technique for classification . the ldg objective function is an approximation to the leave-one-out training error of a local quadratic discriminant analysis classifier , and thus acts locally to each training point in order to find a mapping where similar data can be discriminated from dissimilar data . while other state-of-the-art linear dimensionality reduction methods require gradient descent or iterative solution approaches , ldg is solved with a single eigen-decomposition . thus , it scales better for datasets with a large number of feature dimensions or training examples . we also adapt ldg to the transfer learning setting , and show that it achieves good performance when the test data distribution differs from that of the training data .", "topics": ["test set", "loss function"]}
{"title": "dense scale selection over space , time and space-time", "abstract": "scale selection methods based on local extrema over scale of scale-normalized derivatives have been primarily developed to be applied sparsely -- - at image points where the magnitude of a scale-normalized differential expression additionally assumes local extrema over the domain where the data are defined . this paper presents a methodology for performing dense scale selection , so that hypotheses about local characteristic scales in images , temporal signals and video can be computed at every image point and every time moment . a critical problem when designing mechanisms for dense scale selection is that the scale at which scale-normalized differential entities assume local extrema over scale can be strongly dependent on the local order of the locally dominant differential structure . to address this problem , we propose a methodology where local extrema over scale are detected of a quasi quadrature measure involving scale-space derivatives up to order two and propose two independent mechanisms to reduce the phase dependency of the local scale estimates by : ( i ) introducing a second layer of post-smoothing prior to the detection of local extrema over scale and ( ii ) performing local phase compensation based on a model of the phase dependency of the local scale estimates depending on the relative strengths between first- vs. second-order differential structure . this general methodology is applied over three types of domains : ( i ) spatial images , ( ii ) temporal signals and ( iii ) spatio-temporal video . experiments show that the proposed methodology leads to intuitively reasonable results with local scale estimates that reflect variations in the characteristic scales of locally dominant structures over space and time .", "topics": ["entity"]}
{"title": "online forecasting matrix factorization", "abstract": "in this paper the problem of forecasting high dimensional time series is considered . such time series can be modeled as matrices where each column denotes a measurement . in addition , when missing values are present , low rank matrix factorization approaches are suitable for predicting future values . this paper formally defines and analyzes the forecasting problem in the online setting , i.e . where the data arrives as a stream and only a single pass is allowed . we present and analyze novel matrix factorization techniques which can learn low-dimensional embeddings effectively in an online manner . based on these embeddings a recursive minimum mean square error estimator is derived , which learns an autoregressive model on them . experiments with two real datasets with tens of millions of measurements show the benefits of the proposed approach .", "topics": ["time series"]}
{"title": "deep neural network optimized to resistive memory with nonlinear current-voltage characteristics", "abstract": "artificial neural network computation relies on intensive vector-matrix multiplications . recently , the emerging nonvolatile memory ( nvm ) crossbar array showed a feasibility of implementing such operations with high energy efficiency , thus there are many works on efficiently utilizing emerging nvm crossbar array as analog vector-matrix multiplier . however , its nonlinear i-v characteristics restrain critical design parameters , such as the read voltage and weight range , resulting in substantial accuracy loss . in this paper , instead of optimizing hardware parameters to a given neural network , we propose a methodology of reconstructing a neural network itself optimized to resistive memory crossbar arrays . to verify the validity of the proposed method , we simulated various neural network with mnist and cifar-10 dataset using two different specific resistive random access memory ( rram ) model . simulation results show that our proposed neural network produces significantly higher inference accuracies than conventional neural network when the synapse devices have nonlinear i-v characteristics .", "topics": ["nonlinear system", "simulation"]}
{"title": "predictive overlapping co-clustering", "abstract": "in the past few years co-clustering has emerged as an important data mining tool for two way data analysis . co-clustering is more advantageous over traditional one dimensional clustering in many ways such as , ability to find highly correlated sub-groups of rows and columns . however , one of the overlooked benefits of co-clustering is that , it can be used to extract meaningful knowledge for various other knowledge extraction purposes . for example , building predictive models with high dimensional data and heterogeneous population is a non-trivial task . co-clusters extracted from such data , which shows similar pattern in both the dimension , can be used for a more accurate predictive model building . several applications such as finding patient-disease cohorts in health care analysis , finding user-genre groups in recommendation systems and community detection problems can benefit from co-clustering technique that utilizes the predictive power of the data to generate co-clusters for improved data analysis . in this paper , we present the novel idea of predictive overlapping co-clustering ( pocc ) as an optimization problem for a more effective and improved predictive analysis . our algorithm generates optimal co-clusters by maximizing predictive power of the co-clusters subject to the constraints on the number of row and column clusters . in this paper precision , recall and f-measure have been used as evaluation measures of the resulting co-clusters . results of our algorithm has been compared with two other well-known techniques - k-means and spectral co-clustering , over four real data set namely , leukemia , internet-ads , ovarian cancer and movielens data set . the results demonstrate the effectiveness and utility of our algorithm pocc in practice .", "topics": ["data mining", "cluster analysis"]}
{"title": "riemann-theta boltzmann machine", "abstract": "a general boltzmann machine with continuous visible and discrete integer valued hidden states is introduced . under mild assumptions about the connection matrices , the probability density function of the visible units can be solved for analytically , yielding a novel parametric density function involving a ratio of riemann-theta functions . the conditional expectation of a hidden state for given visible states can also be calculated analytically , yielding a derivative of the logarithmic riemann-theta function . the conditional expectation can be used as activation function in a feedforward neural network , thereby increasing the modelling capacity of the network . both the boltzmann machine and the derived feedforward neural network can be successfully trained via standard gradient- and non-gradient-based optimization techniques .", "topics": ["gradient"]}
{"title": "on a cepstrum-based speech detector robust to white noise", "abstract": "we study effects of additive white noise on the cepstral representation of speech signals . distribution of each individual cepstrum coefficient of speech is shown to depend strongly on noise and to overlap significantly with the cepstrum distribution of noise . based on these studies , we suggest a scalar quantity , v , equal to the sum of weighted cepstral coefficients , which is able to classify frames containing speech against noise-like frames . the distributions of v for speech and noise frames are reasonably well separated above snr = 5 db , demonstrating the feasibility of robust speech detector based on v .", "topics": ["coefficient"]}
{"title": "continuous control with deep reinforcement learning", "abstract": "we adapt the ideas underlying the success of deep q-learning to the continuous action domain . we present an actor-critic , model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces . using the same learning algorithm , network architecture and hyper-parameters , our algorithm robustly solves more than 20 simulated physics tasks , including classic problems such as cartpole swing-up , dexterous manipulation , legged locomotion and car driving . our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives . we further demonstrate that for many of the tasks the algorithm can learn policies end-to-end : directly from raw pixel inputs .", "topics": ["reinforcement learning", "simulation"]}
{"title": "admissible and restrained revision", "abstract": "as partial justification of their framework for iterated belief revision darwiche and pearl convincingly argued against boutiliers natural revision and provided a prototypical revision operator that fits into their scheme . we show that the darwiche-pearl arguments lead naturally to the acceptance of a smaller class of operators which we refer to as admissible . admissible revision ensures that the penultimate input is not ignored completely , thereby eliminating natural revision , but includes the darwiche-pearl operator , nayaks lexicographic revision operator , and a newly introduced operator called restrained revision . we demonstrate that restrained revision is the most conservative of admissible revision operators , effecting as few changes as possible , while lexicographic revision is the least conservative , and point out that restrained revision can also be viewed as a composite operator , consisting of natural revision preceded by an application of a `` backwards revision '' operator previously studied by papini . finally , we propose the establishment of a principled approach for choosing an appropriate revision operator in different contexts and discuss future work .", "topics": ["iteration"]}
{"title": "c-mix : a high dimensional mixture model for censored durations , with applications to genetic data", "abstract": "we introduce a mixture model for censored durations ( c-mix ) , and develop maximum likelihood inference for the joint estimation of the time distributions and latent regression parameters of the model . we consider a high-dimensional setting , with datasets containing a large number of biomedical covariates . we therefore penalize the negative log-likelihood by the elastic-net , which leads to a sparse parameterization of the model . inference is achieved using an efficient quasi-newton expectation maximization ( qnem ) algorithm , for which we provide convergence properties . we then propose a score by assessing the patients risk of early adverse event . the statistical performance of the method is examined on an extensive monte carlo simulation study , and finally illustrated on three genetic datasets with high-dimensional covariates . we show that our approach outperforms the state-of-the-art , namely both the cure and cox proportional hazards models for this task , both in terms of c-index and auc ( t ) .", "topics": ["simulation", "sparse matrix"]}
{"title": "using recurrent neural network for learning expressive ontologies", "abstract": "recently , neural networks have been proven extremely effective in many natural language processing tasks such as sentiment analysis , question answering , or machine translation . aiming to exploit such advantages in the ontology learning process , in this technical report we present a detailed description of a recurrent neural network based system to be used to pursue such goal .", "topics": ["natural language processing", "recurrent neural network"]}
{"title": "apsis - framework for automated optimization of machine learning hyper parameters", "abstract": "the apsis toolkit presented in this paper provides a flexible framework for hyperparameter optimization and includes both random search and a bayesian optimizer . it is implemented in python and its architecture features adaptability to any desired machine learning code . it can easily be used with common python ml frameworks such as scikit-learn . published under the mit license other researchers are heavily encouraged to check out the code , contribute or raise any suggestions . the code can be found at github.com/frederikdiehl/apsis .", "topics": ["value ( ethics )", "optimization problem"]}
{"title": "audio adversarial examples : targeted attacks on speech-to-text", "abstract": "we construct targeted audio adversarial examples on automatic speech recognition . given any audio waveform , we can produce another that is over 99.9 % similar , but transcribes as any phrase we choose ( at a rate of up to 50 characters per second ) . we apply our iterative optimization-based attack to mozilla 's implementation deepspeech end-to-end , and show it has a 100 % success rate . the feasibility of this attack introduce a new domain to study adversarial examples .", "topics": ["speech recognition", "end-to-end principle"]}
{"title": "bayesian one-mode projection for dynamic bipartite graphs", "abstract": "we propose a bayesian methodology for one-mode projecting a bipartite network that is being observed across a series of discrete time steps . the resulting one mode network captures the uncertainty over the presence/absence of each link and provides a probability distribution over its possible weight values . additionally , the incorporation of prior knowledge over previous states makes the resulting network less sensitive to noise and missing observations that usually take place during the data collection process . the methodology consists of computationally inexpensive update rules and is scalable to large problems , via an appropriate distributed implementation .", "topics": ["value ( ethics )", "scalability"]}
{"title": "attributes as semantic units between natural language and visual recognition", "abstract": "impressive progress has been made in the fields of computer vision and natural language processing . however , it remains a challenge to find the best point of interaction for these very different modalities . in this chapter we discuss how attributes allow us to exchange information between the two modalities and in this way lead to an interaction on a semantic level . specifically we discuss how attributes allow using knowledge mined from language resources for recognizing novel visual categories , how we can generate sentence description about images and video , how we can ground natural language in visual content , and finally , how we can answer natural language questions about images .", "topics": ["natural language processing", "natural language"]}
{"title": "clustering on multiple incomplete datasets via collective kernel learning", "abstract": "multiple datasets containing different types of features may be available for a given task . for instance , users ' profiles can be used to group users for recommendation systems . in addition , a model can also use users ' historical behaviors and credit history to group users . each dataset contains different information and suffices for learning . a number of clustering algorithms on multiple datasets were proposed during the past few years . these algorithms assume that at least one dataset is complete . so far as we know , all the previous methods will not be applicable if there is no complete dataset available . however , in reality , there are many situations where no dataset is complete . as in building a recommendation system , some new users may not have a profile or historical behaviors , while some may not have a credit history . hence , no available dataset is complete . in order to solve this problem , we propose an approach called collective kernel learning to infer hidden sample similarity from multiple incomplete datasets . the idea is to collectively completes the kernel matrices of incomplete datasets by optimizing the alignment of the shared instances of the datasets . furthermore , a clustering algorithm is proposed based on the kernel matrix . the experiments on both synthetic and real datasets demonstrate the effectiveness of the proposed approach . the proposed clustering algorithm outperforms the comparison algorithms by as much as two times in normalized mutual information .", "topics": ["kernel ( operating system )", "cluster analysis"]}
{"title": "a theory for optical flow-based transport on image manifolds", "abstract": "an image articulation manifold ( iam ) is the collection of images formed when an object is articulated in front of a camera . iams arise in a variety of image processing and computer vision applications , where they provide a natural low-dimensional embedding of the collection of high-dimensional images . to date iams have been studied as embedded submanifolds of euclidean spaces . unfortunately , their promise has not been realized in practice , because real world imagery typically contains sharp edges that render an iam non-differentiable and hence non-isometric to the low-dimensional parameter space under the euclidean metric . as a result , the standard tools from differential geometry , in particular using linear tangent spaces to transport along the iam , have limited utility . in this paper , we explore a nonlinear transport operator for iams based on the optical flow between images and develop new analytical tools reminiscent of those from differential geometry using the idea of optical flow manifolds ( ofms ) . we define a new metric for iams that satisfies certain local isometry conditions , and we show how to use this metric to develop a new tools such as flow fields on iams , parallel flow fields , parallel transport , as well as a intuitive notion of curvature . the space of optical flow fields along a path of constant curvature has a natural multi-scale structure via a monoid structure on the space of all flow fields along a path . we also develop lower bounds on approximation errors while approximating non-parallel flow fields by parallel flow fields .", "topics": ["image processing", "approximation algorithm"]}
{"title": "bayesian online changepoint detection", "abstract": "changepoints are abrupt variations in the generative parameters of a data sequence . online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance , biometrics , and robotics . while frequentist methods have yielded online filtering and prediction techniques , most bayesian papers have focused on the retrospective segmentation problem . here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint . we compute the probability distribution of the length of the current `` run , '' or time since the last changepoint , using a simple message-passing algorithm . our implementation is highly modular so that the algorithm may be applied to a variety of types of data . we illustrate this modularity by demonstrating the algorithm on three different real-world data sets .", "topics": ["time series"]}
{"title": "modelling protagonist goals and desires in first-person narrative", "abstract": "many genres of natural language text are narratively structured , a testament to our predilection for organizing our experiences as narratives . there is broad consensus that understanding a narrative requires identifying and tracking the goals and desires of the characters and their narrative outcomes . however , to date , there has been limited work on computational models for this problem . we introduce a new dataset , desiredb , which includes gold-standard labels for identifying statements of desire , textual evidence for desire fulfillment , and annotations for whether the stated desire is fulfilled given the evidence in the narrative context . we report experiments on tracking desire fulfillment using different methods , and show that lstm skip-thought model achieves f-measure of 0.7 on our corpus .", "topics": ["natural language"]}
{"title": "enhanced perceptrons using contrastive biclusters", "abstract": "perceptrons are neuronal devices capable of fully discriminating linearly separable classes . although straightforward to implement and train , their applicability is usually hindered by non-trivial requirements imposed by real-world classification problems . therefore , several approaches , such as kernel perceptrons , have been conceived to counteract such difficulties . in this paper , we investigate an enhanced perceptron model based on the notion of contrastive biclusters . from this perspective , a good discriminative bicluster comprises a subset of data instances belonging to one class that show high coherence across a subset of features and high differentiation from nearest instances of the other class under the same features ( referred to as its contrastive bicluster ) . upon each local subspace associated with a pair of contrastive biclusters a perceptron is trained and the model with highest area under the receiver operating characteristic curve ( auc ) value is selected as the final classifier . experiments conducted on a range of data sets , including those related to a difficult biosignal classification problem , show that the proposed variant can be indeed very useful , prevailing in most of the cases upon standard and kernel perceptrons in terms of accuracy and auc measures .", "topics": ["kernel ( operating system )"]}
{"title": "effects of images with different levels of familiarity on eeg", "abstract": "evaluating human brain potentials during watching different images can be used for memory evaluation , information retrieving , guilty-innocent identification and examining the brain response . in this study , the effects of watching images , with different levels of familiarity , on subjects ' electroencephalogram ( eeg ) have been studied . three different groups of images with three familiarity levels of `` unfamiliar '' , `` familiar '' and `` very familiar '' have been considered for this study . eeg signals of 21 subjects ( 14 men ) were recorded . after signal acquisition , pre-processing , including noise and artifact removal , were performed on epochs of data . features , including spatial-statistical , wavelet , frequency and harmonic parameters , and also correlation between recording channels , were extracted from the data . then , we evaluated the efficiency of the extracted features by using p-value and also an orthogonal feature selection method ( combination of gram-schmitt method and fisher discriminant ratio ) for feature dimensional reduction . as the final step of feature selection , we used 'add-r take-away l ' method for choosing the most discriminative features . for data classification , including all two-class and three-class cases , we applied support vector machine ( svm ) on the extracted features . the correct classification rates ( ccr ) for `` unfamiliar-familiar '' , `` unfamiliar-very familiar '' and `` familiar-very familiar '' cases were 85.6 % , 92.6 % , and 70.6 % , respectively . the best results of classifications were obtained in pre-frontal and frontal regions of brain . also , wavelet , frequency and harmonic features were among the most discriminative features . finally , in three-class case , the best ccr was 86.8 % .", "topics": ["support vector machine", "support vector machine"]}
{"title": "bayesian image quality transfer with cnns : exploring uncertainty in dmri super-resolution", "abstract": "in this work , we investigate the value of uncertainty modeling in 3d super-resolution with convolutional neural networks ( cnns ) . deep learning has shown success in a plethora of medical image transformation problems , such as super-resolution ( sr ) and image synthesis . however , the highly ill-posed nature of such problems results in inevitable ambiguity in the learning of networks . we propose to account for intrinsic uncertainty through a per-patch heteroscedastic noise model and for parameter uncertainty through approximate bayesian inference in the form of variational dropout . we show that the combined benefits of both lead to the state-of-the-art performance sr of diffusion mr brain images in terms of errors compared to ground truth . we further show that the reduced error scores produce tangible benefits in downstream tractography . in addition , the probabilistic nature of the methods naturally confers a mechanism to quantify uncertainty over the super-resolved output . we demonstrate through experiments on both healthy and pathological brains the potential utility of such an uncertainty measure in the risk assessment of the super-resolved images for subsequent clinical use .", "topics": ["calculus of variations", "ground truth"]}
{"title": "manipulation is harder with incomplete votes", "abstract": "the coalitional manipulation ( cm ) problem has been studied extensively in the literature for many voting rules . the cm problem , however , has been studied only in the complete information setting , that is , when the manipulators know the votes of the non-manipulators . a more realistic scenario is an incomplete information setting where the manipulators do not know the exact votes of the non- manipulators but may have some partial knowledge of the votes . in this paper , we study a setting where the manipulators know a partial order for each voter that is consistent with the vote of that voter . in this setting , we introduce and study two natural computational problems - ( 1 ) weak manipulation ( wm ) problem where the manipulators wish to vote in a way that makes their preferred candidate win in at least one extension of the partial votes of the non-manipulators ; ( 2 ) strong manipulation ( sm ) problem where the manipulators wish to vote in a way that makes their preferred candidate win in all possible extensions of the partial votes of the non-manipulators . we study the computational complexity of the wm and the sm problems for commonly used voting rules such as plurality , veto , k-approval , k-veto , maximin , copeland , and bucklin . our key finding is that , barring a few exceptions , manipulation becomes a significantly harder problem in the setting of incomplete votes .", "topics": ["computational complexity theory"]}
{"title": "speckle noise reduction in medical ultrasound images", "abstract": "ultrasound imaging is an incontestable vital tool for diagnosis , it provides in non-invasive manner the internal structure of the body to detect eventually diseases or abnormalities tissues . unfortunately , the presence of speckle noise in these images affects edges and fine details which limit the contrast resolution and make diagnostic more difficult . in this paper , we propose a denoising approach which combines logarithmic transformation and a non linear diffusion tensor . since speckle noise is multiplicative and nonwhite process , the logarithmic transformation is a reasonable choice to convert signaldependent or pure multiplicative noise to an additive one . the key idea from using diffusion tensor is to adapt the flow diffusion towards the local orientation by applying anisotropic diffusion along the coherent structure direction of interesting features in the image . to illustrate the effective performance of our algorithm , we present some experimental results on synthetically and real echographic images .", "topics": ["noise reduction"]}
{"title": "a comparative analysis on the applicability of entropy in remote sensing", "abstract": "entropy is the measure of uncertainty in any data and is adopted for maximisation of mutual information in many remote sensing operations . the availability of wide entropy variations motivated us for an investigation over the suitability preference of these versions to specific operations . methodologies were implemented in matlab and were enhanced with entropy variations . evaluation of various implementations was based on different statistical parameters with reference to the study area the popular available versions like tsalli 's , shanon 's , and renyi 's entropies were analysed in context of various remote sensing operations namely thresholding , clustering and registration .", "topics": ["cluster analysis"]}
{"title": "financial portfolio optimization : computationally guided agents to investigate , analyse and invest ! ?", "abstract": "financial portfolio optimization is a widely studied problem in mathematics , statistics , financial and computational literature . it adheres to determining an optimal combination of weights associated with financial assets held in a portfolio . in practice , it faces challenges by virtue of varying math . formulations , parameters , business constraints and complex financial instruments . empirical nature of data is no longer one-sided ; thereby reflecting upside and downside trends with repeated yet unidentifiable cyclic behaviours potentially caused due to high frequency volatile movements in asset trades . portfolio optimization under such circumstances is theoretically and computationally challenging . this work presents a novel mechanism to reach an optimal solution by encoding a variety of optimal solutions in a solution bank to guide the search process for the global investment objective formulation . it conceptualizes the role of individual solver agents that contribute optimal solutions to a bank of solutions , a super-agent solver that learns from the solution bank , and , thus reflects a knowledge-based computationally guided agents approach to investigate , analyse and reach to optimal solution for informed investment decisions . conceptual understanding of classes of solver agents that represent varying problem formulations and , mathematically oriented deterministic solvers along with stochastic-search driven evolutionary and swarm-intelligence based techniques for optimal weights are discussed . algorithmic implementation is presented by an enhanced neighbourhood generation mechanism in simulated annealing algorithm . a framework for inclusion of heuristic knowledge and human expertise from financial literature related to investment decision making process is reflected via introduction of controlled perturbation strategies using a decision matrix for neighbourhood generation .", "topics": ["optimization problem", "heuristic"]}
{"title": "discriminative transformation learning for fuzzy sparse subspace clustering", "abstract": "this paper develops a novel iterative framework for subspace clustering in a learned discriminative feature domain . this framework consists of two modules of fuzzy sparse subspace clustering and discriminative transformation learning . in the first module , fuzzy latent labels containing discriminative information and latent representations capturing the subspace structure will be simultaneously evaluated in a feature domain . then the linear transforming operator with respect to the feature domain will be successively updated in the second module with the advantages of more discrimination , subspace structure preservation and robustness to outliers . these two modules will be alternatively carried out and both theoretical analysis and empirical evaluations will demonstrate its effectiveness and superiorities . in particular , experimental results on three benchmark databases for subspace clustering clearly illustrate that the proposed framework can achieve significant improvements than other state-of-the-art approaches in terms of clustering accuracy .", "topics": ["cluster analysis", "sparse matrix"]}
{"title": "development of statewide aadt estimation model from short-term counts : a comparative study for south carolina", "abstract": "annual average daily traffic ( aadt ) is an important parameter used in traffic engineering analysis . departments of transportation ( dots ) continually collect traffic count using both permanent count stations ( i.e . , automatic traffic recorders or atrs ) and temporary short-term count stations . in south carolina , 87 % of the atrs are located on interstates and arterial highways . for most secondary highways ( i.e . , collectors and local roads ) , aadt is estimated based on short-term counts . this paper develops aadt estimation models for different roadway functional classes with two machine learning techniques : artificial neural network ( ann ) and support vector regression ( svr ) . the models aim to predict aadt from short-term counts . the results are first compared against each other to identify the best model . then , the results of the best model are compared against a regression method and factor-based method . the comparison reveals the superiority of svr for aadt estimation for different roadway functional classes over all other methods . among all developed models for different functional roadway classes , the svr-based model shows a minimum root mean square error ( rmse ) of 0.22 and a mean absolute percentage error ( mape ) of 11.3 % for the interstate/expressway functional class . this model also shows a higher r-squared value compared to the traditional factor-based model and regression model . svr models are validated for each roadway functional class using the 2016 atr data and selected short-term count data collected by the south carolina department of transportation ( scdot ) . the validation results show that the svr-based aadt estimation models can be used by the scdot as a reliable option to predict aadt from the short-term counts .", "topics": ["support vector machine"]}
{"title": "deep stochastic configuration networks with universal approximation property", "abstract": "this paper develops a randomized approach for incrementally building deep neural networks , where a supervisory mechanism is proposed to constrain the random assignment of the weights and biases , and all the hidden layers have direct links to the output layer . a fundamental result on the universal approximation property is established for such a class of randomized leaner models , namely deep stochastic configuration networks ( deepscns ) . a learning algorithm is presented to implement deepscns with either specific architecture or self-organization . the read-out weights attached with all direct links from each hidden layer to the output layer are evaluated by the least squares method . given a set of training examples , deepscns can speedily produce a learning representation , that is , a collection of random basis functions with the cascaded inputs together with the read-out weights . an empirical study on a function approximation is carried out to demonstrate some properties of the proposed deep learner model .", "topics": ["feature learning", "simulation"]}
{"title": "recurrent slice networks for 3d segmentation on point clouds", "abstract": "in this paper , we present a conceptually simple and powerful framework , recurrent slice network ( rsnet ) , for 3d semantic segmentation on point clouds . performing 3d segmentation on point clouds is computationally efficient . and it is free of the quantitation artifact problems which exists in other 3d data formats such as voxelized volumes and multi view renderings . however , existing point clouds based methods either do not model local dependencies or rely on heavy extra computations . in contrast , our rsnet is equipped with a lightweight local dependency module , which is a combination of a novel slice pooling layer , recurrent neural network ( rnn ) layers , and a slice unpooling layer . the slice pooling layer is designed to project features of unordered points into an ordered sequence of feature vectors . then , rnns are applied to model dependencies for the sequence . we validate the importance of local contexts and the effectiveness of our rsnet on the s3dis , scannet , and shapenet dataset . without bells and whistles , rsnet surpasses all previous state-of-the-art methods on these benchmarks . moreover , additional computation analysis demonstrates the efficiency of rsnet .", "topics": ["feature vector", "recurrent neural network"]}
{"title": "a cognitive architecture based on a learning classifier system with spiking classifiers", "abstract": "learning classifier systems ( lcs ) are population-based reinforcement learners that were originally designed to model various cognitive phenomena . this paper presents an explicitly cognitive lcs by using spiking neural networks as classifiers , providing each classifier with a measure of temporal dynamism . we employ a constructivist model of growth of both neurons and synaptic connections , which permits a genetic algorithm ( ga ) to automatically evolve sufficiently-complex neural structures . the spiking classifiers are coupled with a temporally-sensitive reinforcement learning algorithm , which allows the system to perform temporal state decomposition by appropriately rewarding `` macro-actions , '' created by chaining together multiple atomic actions . the combination of temporal reinforcement learning and neural information processing is shown to outperform benchmark neural classifier systems , and successfully solve a robotic navigation task .", "topics": ["reinforcement learning", "robot"]}
{"title": "on the complexity of piecewise affine system identification", "abstract": "the paper provides results regarding the computational complexity of hybrid system identification . more precisely , we focus on the estimation of piecewise affine ( pwa ) maps from input-output data and analyze the complexity of computing a global minimizer of the error . previous work showed that a global solution could be obtained for continuous pwa maps with a worst-case complexity exponential in the number of data . in this paper , we show how global optimality can be reached for a slightly more general class of possibly discontinuous pwa maps with a complexity only polynomial in the number of data , however with an exponential complexity with respect to the data dimension . this result is obtained via an analysis of the intrinsic classification subproblem of associating the data points to the different modes . in addition , we prove that the problem is np-hard , and thus that the exponential complexity in the dimension is a natural expectation for any exact algorithm .", "topics": ["computational complexity theory", "time complexity"]}
{"title": "an extensive technique to detect and analyze melanoma : a challenge at the international symposium on biomedical imaging ( isbi ) 2017", "abstract": "an automated method to detect and analyze the melanoma is presented to improve diagnosis which will leads to the exact treatment . image processing techniques such as segmentation , feature descriptors and classification models are involved in this method . in the first phase the lesion region is segmented using cielab color space based segmentation . then feature descriptors such as shape , color and texture are extracted . finally , in the third phase lesion region is classified as melanoma , seborrheic keratosis or nevus using multi class o-a svm model . experiment with isic 2017 archive skin image database has been done and analyzed the results .", "topics": ["image processing"]}
{"title": "technical report : a tool for measuring prosodic accommodation", "abstract": "this article has been withdrawn by arxiv administrators because the submitter did not have the legal authority to grant the license applied to the work .", "topics": ["feature extraction", "ground truth"]}
{"title": "natural language processing - a survey", "abstract": "the utility and power of natural language processing ( nlp ) seems destined to change our technological society in profound and fundamental ways . however there are , to date , few accessible descriptions of the science of nlp that have been written for a popular audience , or even for an audience of intelligent , but uninitiated scientists . this paper aims to provide just such an overview . in short , the objective of this article is to describe the purpose , procedures and practical applications of nlp in a clear , balanced , and readable way . we will examine the most recent literature describing the methods and processes of nlp , analyze some of the challenges that researchers are faced with , and briefly survey some of the current and future applications of this science to it research in general .", "topics": ["natural language processing"]}
{"title": "least square ellipsoid fitting using iterative orthogonal transformations", "abstract": "we describe a generalised method for ellipsoid fitting against a minimum set of data points . the proposed method is numerically stable and applies to a wide range of ellipsoidal shapes , including highly elongated and arbitrarily oriented ellipsoids . this new method also provides for the retrieval of rotational angle and length of semi-axes of the fitted ellipsoids accurately . we demonstrate the efficacy of this algorithm on simulated data sets and also indicate its potential use in gravitational wave data analysis .", "topics": ["simulation"]}
{"title": "impacts of dirty data : and experimental evaluation", "abstract": "data quality issues have attracted widespread attention due to the negative impacts of dirty data on data mining and machine learning results . the relationship between data quality and the accuracy of results could be applied on the selection of the appropriate algorithm with the consideration of data quality and the determination of the data share to clean . however , rare research has focused on exploring such relationship . motivated by this , this paper conducts an experimental comparison for the effects of missing , inconsistent and conflicting data on classification , clustering , and regression algorithms . based on the experimental findings , we provide guidelines for algorithm selection and data cleaning .", "topics": ["data mining", "statistical classification"]}
{"title": "fixation prediction with a combined model of bottom-up saliency and vanishing point", "abstract": "by predicting where humans look in natural scenes , we can understand how they perceive complex natural scenes and prioritize information for further high-level visual processing . several models have been proposed for this purpose , yet there is a gap between best existing saliency models and human performance . while many researchers have developed purely computational models for fixation prediction , less attempts have been made to discover cognitive factors that guide gaze . here , we study the effect of a particular type of scene structural information , known as the vanishing point , and show that human gaze is attracted to the vanishing point regions . we record eye movements of 10 observers over 532 images , out of which 319 have vanishing points . we then construct a combined model of traditional saliency and a vanishing point channel and show that our model outperforms state of the art saliency models using three scores on our dataset .", "topics": ["high- and low-level"]}
{"title": "learning why things change : the difference-based causality learner", "abstract": "in this paper , we present the difference- based causality learner ( dbcl ) , an algorithm for learning a class of discrete-time dynamic models that represents all causation across time by means of difference equations driving change in a system . we motivate this representation with real-world mechanical systems and prove dbcl 's correctness for learning structure from time series data , an endeavour that is complicated by the existence of latent derivatives that have to be detected . we also prove that , under common assumptions for causal discovery , dbcl will identify the presence or absence of feedback loops , making the model more useful for predicting the effects of manipulating variables when the system is in equilibrium . we argue analytically and show empirically the advantages of dbcl over vector autoregression ( var ) and granger causality models as well as modified forms of bayesian and constraintbased structure discovery algorithms . finally , we show that our algorithm can discover causal directions of alpha rhythms in human brains from eeg data .", "topics": ["time series", "causality"]}
{"title": "neural aggregation network for video face recognition", "abstract": "this paper presents a neural aggregation network ( nan ) for video face recognition . the network takes a face video or face image set of a person with a variable number of face images as its input , and produces a compact , fixed-dimension feature representation for recognition . the whole network is composed of two modules . the feature embedding module is a deep convolutional neural network ( cnn ) which maps each face image to a feature vector . the aggregation module consists of two attention blocks which adaptively aggregate the feature vectors to form a single feature inside the convex hull spanned by them . due to the attention mechanism , the aggregation is invariant to the image order . our nan is trained with a standard classification or verification loss without any extra supervision signal , and we found that it automatically learns to advocate high-quality face images while repelling low-quality ones such as blurred , occluded and improperly exposed faces . the experiments on ijb-a , youtube face , celebrity-1000 video face recognition benchmarks show that it consistently outperforms naive aggregation methods and achieves the state-of-the-art accuracy .", "topics": ["feature vector"]}
{"title": "kernel coding : general formulation and special cases", "abstract": "representing images by compact codes has proven beneficial for many visual recognition tasks . most existing techniques , however , perform this coding step directly in image feature space , where the distributions of the different classes are typically entangled . in contrast , here , we study the problem of performing coding in a high-dimensional hilbert space , where the classes are expected to be more easily separable . to this end , we introduce a general coding formulation that englobes the most popular techniques , such as bag of words , sparse coding and locality-based coding , and show how this formulation and its special cases can be kernelized . importantly , we address several aspects of learning in our general formulation , such as kernel learning , dictionary learning and supervised kernel coding . our experimental evaluation on several visual recognition tasks demonstrates the benefits of performing coding in hilbert space , and in particular of jointly learning the kernel , the dictionary and the classifier .", "topics": ["kernel ( operating system )", "feature vector"]}
{"title": "co2 forest : improved random forest by continuous optimization of oblique splits", "abstract": "we propose a novel algorithm for optimizing multivariate linear threshold functions as split functions of decision trees to create improved random forest classifiers . standard tree induction methods resort to sampling and exhaustive search to find good univariate split functions . in contrast , our method computes a linear combination of the features at each node , and optimizes the parameters of the linear combination ( oblique ) split functions by adopting a variant of latent variable svm formulation . we develop a convex-concave upper bound on the classification loss for a one-level decision tree , and optimize the bound by stochastic gradient descent at each internal node of the tree . forests of up to 1000 continuously optimized oblique ( co2 ) decision trees are created , which significantly outperform random forest with univariate splits and previous techniques for constructing oblique trees . experimental results are reported on multi-class classification benchmarks and on labeled faces in the wild ( lfw ) dataset .", "topics": ["support vector machine", "gradient descent"]}
{"title": "recalling holistic information for semantic segmentation", "abstract": "semantic segmentation requires a detailed labeling of image pixels by object category . information derived from local image patches is necessary to describe the detailed shape of individual objects . however , this information is ambiguous and can result in noisy labels . global inference of image content can instead capture the general semantic concepts present . we advocate that high-recall holistic inference of image concepts provides valuable information for detailed pixel labeling . we build a two-stream neural network architecture that facilitates information flow from holistic information to local pixels , while keeping common image features shared among the low-level layers of both the holistic analysis and segmentation branches . we empirically evaluate our network on four standard semantic segmentation datasets . our network obtains state-of-the-art performance on pascal-context and nyudv2 , and ablation studies verify its effectiveness on ade20k and sift-flow .", "topics": ["image segmentation", "high- and low-level"]}
{"title": "hidden integrality of sdp relaxation for sub-gaussian mixture models", "abstract": "we consider the problem of estimating the discrete clustering structures under sub-gaussian mixture models . our main results establish a hidden integrality property of a semidefinite programming ( sdp ) relaxation for this problem : while the optimal solutions to the sdp are not integer-valued in general , their estimation errors can be upper bounded in terms of the error of an idealized integer program . the error of the integer program , and hence that of the sdp , are further shown to decay exponentially in the signal-to-noise ratio . to the best of our knowledge , this is the first exponentially decaying error bound for convex relaxations of mixture models , and our results reveal the `` global-to-local '' mechanism that drives the performance of the sdp relaxation . a corollary of our results shows that in certain regimes the sdp solutions are in fact integral and exact , improving on existing exact recovery results for convex relaxations . more generally , our results establish sufficient conditions for the sdp to correctly recover the cluster memberships of $ ( 1-\\delta ) $ fraction of the points for any $ \\delta\\in ( 0,1 ) $ . as a special case , we show that under the $ d $ -dimensional stochastic ball model , sdp achieves non-trivial ( sometimes exact ) recovery when the center separation is as small as $ \\sqrt { 1/d } $ , which complements previous exact recovery results that require constant separation .", "topics": ["cluster analysis"]}
{"title": "an olac extension for dravidian languages", "abstract": "olac was founded in 2000 for creating online databases of language resources . this paper intends to review the bottom-up distributed character of the project and proposes an extension of the architecture for dravidian languages . an ontological structure is considered for effective natural language processing ( nlp ) and its advantages over statistical methods are reviewed", "topics": ["natural language processing", "natural language"]}
{"title": "alternating direction method of multipliers for penalized zero-variance discriminant analysis", "abstract": "we consider the task of classification in the high dimensional setting where the number of features of the given data is significantly greater than the number of observations . to accomplish this task , we propose a heuristic , called sparse zero-variance discriminant analysis ( szvd ) , for simultaneously performing linear discriminant analysis and feature selection on high dimensional data . this method combines classical zero-variance discriminant analysis , where discriminant vectors are identified in the null space of the sample within-class covariance matrix , with penalization applied to induce sparse structures in the resulting vectors . to approximately solve the resulting nonconvex problem , we develop a simple algorithm based on the alternating direction method of multipliers . further , we show that this algorithm is applicable to a larger class of penalized generalized eigenvalue problems , including a particular relaxation of the sparse principal component analysis problem . finally , we establish theoretical guarantees for convergence of our algorithm to stationary points of the original nonconvex problem , and empirically demonstrate the effectiveness of our heuristic for classifying simulated data and data drawn from applications in time-series classification .", "topics": ["optimization problem", "time series"]}
{"title": "efficient per-example gradient computations", "abstract": "this technical report describes an efficient technique for computing the norm of the gradient of the loss function for a neural network with respect to its parameters . this gradient norm can be computed efficiently for every example .", "topics": ["loss function", "gradient"]}
{"title": "essp : an efficient approach to minimizing dense and nonsubmodular energy functions", "abstract": "many recent advances in computer vision have demonstrated the impressive power of dense and nonsubmodular energy functions in solving visual labeling problems . however , minimizing such energies is challenging . none of existing techniques ( such as s-t graph cut , qpbo , bp and trw-s ) can individually do this well . in this paper , we present an efficient method , namely essp , to optimize binary mrfs with arbitrary pairwise potentials , which could be nonsubmodular and with dense connectivity . we also provide a comparative study of our approach and several recent promising methods . from our study , we make some reasonable recommendations of combining existing methods that perform the best in different situations for this challenging problem . experimental results validate that for dense and nonsubmodular energy functions , the proposed approach can usually obtain lower energies than the best combination of other techniques using comparably reasonable time .", "topics": ["computer vision"]}
{"title": "teaching compositionality to cnns", "abstract": "convolutional neural networks ( cnns ) have shown great success in computer vision , approaching human-level performance when trained for specific tasks via application-specific loss functions . in this paper , we propose a method for augmenting and training cnns so that their learned features are compositional . it encourages networks to form representations that disentangle objects from their surroundings and from each other , thereby promoting better generalization . our method is agnostic to the specific details of the underlying cnn to which it is applied and can in principle be used with any cnn . as we show in our experiments , the learned representations lead to feature activations that are more localized and improve performance over non-compositional baselines in object recognition tasks .", "topics": ["baseline ( configuration management )", "computer vision"]}
{"title": "maximum margin vector correlation filter", "abstract": "correlation filters ( cfs ) are a class of classifiers which are designed for accurate pattern localization . traditionally cfs have been used with scalar features only , which limits their ability to be used with vector feature representations like gabor filter banks , sift , hog , etc . in this paper we present a new cf named maximum margin vector correlation filter ( mmvcf ) which extends the traditional cf designs to vector features . mmvcf further combines the generalization capability of large margin based classifiers like support vector machines ( svms ) and the localization properties of cfs for better robustness to outliers . we demonstrate the efficacy of mmvcf for object detection and landmark localization on a variety of databases and demonstrate that mmvcf consistently shows improved pattern localization capability in comparison to svms .", "topics": ["support vector machine", "object detection"]}
{"title": "robust optimization for tree-structured stochastic network design", "abstract": "stochastic network design is a general framework for optimizing network connectivity . it has several applications in computational sustainability including spatial conservation planning , pre-disaster network preparation , and river network optimization . a common assumption in previous work has been made that network parameters ( e.g . , probability of species colonization ) are precisely known , which is unrealistic in real- world settings . we therefore address the robust river network design problem where the goal is to optimize river connectivity for fish movement by removing barriers . we assume that fish passability probabilities are known only imprecisely , but are within some interval bounds . we then develop a planning approach that computes the policies with either high robust ratio or low regret . empirically , our approach scales well to large river networks . we also provide insights into the solutions generated by our robust approach , which has significantly higher robust ratio than the baseline solution with mean parameter estimates .", "topics": ["baseline ( configuration management )", "regret ( decision theory )"]}
{"title": "a study of the effect of jpg compression on adversarial images", "abstract": "neural network image classifiers are known to be vulnerable to adversarial images , i.e . , natural images which have been modified by an adversarial perturbation specifically designed to be imperceptible to humans yet fool the classifier . not only can adversarial images be generated easily , but these images will often be adversarial for networks trained on disjoint subsets of data or with different architectures . adversarial images represent a potential security risk as well as a serious machine learning challenge -- -it is clear that vulnerable neural networks perceive images very differently from humans . noting that virtually every image classification data set is composed of jpg images , we evaluate the effect of jpg compression on the classification of adversarial images . for fast-gradient-sign perturbations of small magnitude , we found that jpg compression often reverses the drop in classification accuracy to a large extent , but not always . as the magnitude of the perturbations increases , jpg recompression alone is insufficient to reverse the effect .", "topics": ["computer vision", "gradient"]}
{"title": "using sentence plausibility to learn the semantics of transitive verbs", "abstract": "the functional approach to compositional distributional semantics considers transitive verbs to be linear maps that transform the distributional vectors representing nouns into a vector representing a sentence . we conduct an initial investigation that uses a matrix consisting of the parameters of a logistic regression classifier trained on a plausibility task as a transitive verb function . we compare our method to a commonly used corpus-based method for constructing a verb matrix and find that the plausibility training may be more effective for disambiguation tasks .", "topics": ["map"]}
{"title": "towards robust neural networks via random self-ensemble", "abstract": "recent studies have revealed the vulnerability of deep neural networks - a small adversarial perturbation that is imperceptible to human can easily make a well-trained deep neural network mis-classify . this makes it unsafe to apply neural networks in security-critical applications . in this paper , we propose a new defensive algorithm called random self-ensemble ( rse ) by combining two important concepts : $ { \\bf randomness } $ and $ { \\bf ensemble } $ . to protect a targeted model , rse adds random noise layers to the neural network to prevent from state-of-the-art gradient-based attacks , and ensembles the prediction over random noises to stabilize the performance . we show that our algorithm is equivalent to ensemble an infinite number of noisy models $ f_\\epsilon $ without any additional memory overhead , and the proposed training procedure based on noisy stochastic gradient descent can ensure the ensemble model has good predictive capability . our algorithm significantly outperforms previous defense techniques on real datasets . for instance , on cifar-10 with vgg network ( which has $ 92\\ % $ accuracy without any attack ) , under the state-of-the-art c & w attack within a certain distortion tolerance , the accuracy of unprotected model drops to less than $ 10\\ % $ , the best previous defense technique has $ 48\\ % $ accuracy , while our method still has $ 86\\ % $ prediction accuracy under the same level of attack . finally , our method is simple and easy to integrate into any neural network .", "topics": ["neural networks", "gradient descent"]}
{"title": "recurrent neural network postfilters for statistical parametric speech synthesis", "abstract": "in the last two years , there have been numerous papers that have looked into using deep neural networks to replace the acoustic model in traditional statistical parametric speech synthesis . however , far less attention has been paid to approaches like dnn-based postfiltering where dnns work in conjunction with traditional acoustic models . in this paper , we investigate the use of recurrent neural networks as a potential postfilter for synthesis . we explore the possibility of replacing existing postfilters , as well as highlight the ease with which arbitrary new features can be added as input to the postfilter . we also tried a novel approach of jointly training the classification and regression tree and the postfilter , rather than the traditional approach of training them independently .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "excess risk bounds for multitask learning with trace norm regularization", "abstract": "trace norm regularization is a popular method of multitask learning . we give excess risk bounds with explicit dependence on the number of tasks , the number of examples per task and properties of the data distribution . the bounds are independent of the dimension of the input space , which may be infinite as in the case of reproducing kernel hilbert spaces . a byproduct of the proof are bounds on the expected norm of sums of random positive semidefinite matrices with subexponential moments .", "topics": ["matrix regularization"]}
{"title": "opinion dynamics with varying susceptibility to persuasion", "abstract": "a long line of work in social psychology has studied variations in people 's susceptibility to persuasion -- the extent to which they are willing to modify their opinions on a topic . this body of literature suggests an interesting perspective on theoretical models of opinion formation by interacting parties in a network : in addition to considering interventions that directly modify people 's intrinsic opinions , it is also natural to consider interventions that modify people 's susceptibility to persuasion . in this work , we adopt a popular model for social opinion dynamics , and we formalize the opinion maximization and minimization problems where interventions happen at the level of susceptibility . we show that modeling interventions at the level of susceptibility lead to an interesting family of new questions in network opinion dynamics . we find that the questions are quite different depending on whether there is an overall budget constraining the number of agents we can target or not . we give a polynomial-time algorithm for finding the optimal target-set to optimize the sum of opinions when there are no budget constraints on the size of the target-set . we show that this problem is np-hard when there is a budget , and that the objective function is neither submodular nor supermodular . finally , we propose a heuristic for the budgeted opinion optimization and show its efficacy at finding target-sets that optimize the sum of opinions compared on real world networks , including a twitter network with real opinion estimates .", "topics": ["baseline ( configuration management )", "time complexity"]}
{"title": "reinforcement learning with unsupervised auxiliary tasks", "abstract": "deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward . however , environments contain a much wider variety of possible training signals . in this paper , we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning . all of these tasks share a common representation that , like unsupervised learning , continues to develop in the absence of extrinsic rewards . we also introduce a novel mechanism for focusing this representation upon extrinsic rewards , so that learning can rapidly adapt to the most relevant aspects of the actual task . our agent significantly outperforms the previous state-of-the-art on atari , averaging 880\\ % expert human performance , and a challenging suite of first-person , three-dimensional \\emph { labyrinth } tasks leading to a mean speedup in learning of 10 $ \\times $ and averaging 87\\ % expert human performance on labyrinth .", "topics": ["unsupervised learning", "approximation algorithm"]}
{"title": "flow-guided feature aggregation for video object detection", "abstract": "extending state-of-the-art object detectors from image to video is challenging . the accuracy of detection suffers from degenerated object appearances in videos , e.g . , motion blur , video defocus , rare poses , etc . existing work attempts to exploit temporal information on box level , but such methods are not trained end-to-end . we present flow-guided feature aggregation , an accurate and end-to-end learning framework for video object detection . it leverages temporal coherence on feature level instead . it improves the per-frame features by aggregation of nearby features along the motion paths , and thus improves the video recognition accuracy . our method significantly improves upon strong single-frame baselines in imagenet vid , especially for more challenging fast moving objects . our framework is principled , and on par with the best engineered systems winning the imagenet vid challenges 2016 , without additional bells-and-whistles . the proposed method , together with deep feature flow , powered the winning entry of imagenet vid challenges 2017 . the code is available at https : //github.com/msracver/flow-guided-feature-aggregation .", "topics": ["object detection", "end-to-end principle"]}
{"title": "learning to parse and translate improves neural machine translation", "abstract": "there has been relatively little attention to incorporating linguistic prior to neural machine translation . much of the previous work was further constrained to considering linguistic prior on the source side . in this paper , we propose a hybrid model , called nmt+rnng , that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation . our approach encourages the neural machine translation model to incorporate linguistic prior during training , and lets it translate on its own afterward . extensive experiments with four language pairs show the effectiveness of the proposed nmt+rnng .", "topics": ["recurrent neural network", "machine translation"]}
{"title": "measuring the state of the art of automated pathway curation using graph algorithms - a case study of the mtor pathway", "abstract": "this paper evaluates the difference between human pathway curation and current nlp systems . we propose graph analysis methods for quantifying the gap between human curated pathway maps and the output of state-of-the-art automatic nlp systems . evaluation is performed on the popular mtor pathway . based on analyzing where current systems perform well and where they fail , we identify possible avenues for progress .", "topics": ["natural language processing", "map"]}
{"title": "3d regression neural network for the quantification of enlarged perivascular spaces in brain mri", "abstract": "enlarged perivascular spaces ( epvs ) in the brain are an emerging imaging marker for cerebral small vessel disease , and have been shown to be related to increased risk of various neurological diseases , including stroke and dementia . automatic quantification of epvs would greatly help to advance research into its etiology and its potential as a risk indicator of disease . we propose a convolutional network regression method to quantify the extent of epvs in the basal ganglia from 3d brain mri . we first segment the basal ganglia and subsequently apply a 3d convolutional regression network designed for small object detection within this region of interest . the network takes an image as input , and outputs a quantification score of epvs . the network has significantly more convolution operations than pooling ones and no final activation , allowing it to span the space of real numbers . we validated our approach using a dataset of 2000 brain mri scans scored visually . experiments with varying sizes of training and test sets showed that a good performance can be achieved with a training set of only 200 scans . with a training set of 1000 scans , the intraclass correlation coefficient ( icc ) between our scoring method and the expert 's visual score was 0.74 . our method outperforms by a large margin - more than 0.10 - four more conventional automated approaches based on intensities , scale-invariant feature transform , and random forest . we show that the network learns the structures of interest and investigate the influence of hyper-parameters on the performance . we also evaluate the reproducibility of our network using a set of 60 subjects scanned twice ( scan-rescan reproducibility ) . on this set our network achieves an icc of 0.93 , while the intrarater agreement reaches 0.80 . furthermore , the automatic epvs scoring correlates similarly to age as visual scoring .", "topics": ["test set", "object detection"]}
{"title": "application of multifractal analysis to segmentation of water bodies in optical and synthetic aperture radar satellite images", "abstract": "a method for segmenting water bodies in optical and synthetic aperture radar ( sar ) satellite images is proposed . it makes use of the textural features of the different regions in the image for segmentation . the method consists in a multiscale analysis of the images , which allows us to study the images regularity both , locally and globally . as results of the analysis , coarse multifractal spectra of studied images and a group of images that associates each position ( pixel ) with its corresponding value of local regularity ( or singularity ) spectrum are obtained . thresholds are then applied to the multifractal spectra of the images for the classification . these thresholds are selected after studying the characteristics of the spectra under the assumption that water bodies have larger local regularity than other soil types . classifications obtained by the multifractal method are compared quantitatively with those obtained by neural networks trained to classify the pixels of the images in covered against uncovered by water . in optical images , the classifications are also compared with those derived using the so-called normalized differential water index ( ndwi ) .", "topics": ["synthetic data", "pixel"]}
{"title": "spatial random sampling : a structure-preserving data sketching tool", "abstract": "random column sampling is not guaranteed to yield data sketches that preserve the underlying structures of the data and may not sample sufficiently from less-populated data clusters . also , adaptive sampling can often provide accurate low rank approximations , yet may fall short of producing descriptive data sketches , especially when the cluster centers are linearly dependent . motivated by that , this paper introduces a novel randomized column sampling tool dubbed spatial random sampling ( srs ) , in which data points are sampled based on their proximity to randomly sampled points on the unit sphere . the most compelling feature of srs is that the corresponding probability of sampling from a given data cluster is proportional to the surface area the cluster occupies on the unit sphere , independently from the size of the cluster population . although it is fully randomized , srs is shown to provide descriptive and balanced data representations . the proposed idea addresses a pressing need in data science and holds potential to inspire many novel approaches for analysis of big data .", "topics": ["sampling ( signal processing )", "approximation"]}
{"title": "a hierarchical spectral method for extreme classification", "abstract": "extreme classification problems are multiclass and multilabel classification problems where the number of outputs is so large that straightforward strategies are neither statistically nor computationally viable . one strategy for dealing with the computational burden is via a tree decomposition of the output space . while this typically leads to training and inference that scales sublinearly with the number of outputs , it also results in reduced statistical performance . in this work , we identify two shortcomings of tree decomposition methods , and describe two heuristic mitigations . we compose these with an eigenvalue technique for constructing the tree . the end result is a computationally efficient algorithm that provides good statistical performance on several extreme data sets .", "topics": ["computational complexity theory", "heuristic"]}
{"title": "predicting process behaviour using deep learning", "abstract": "predicting business process behaviour is an important aspect of business process management . motivated by research in natural language processing , this paper describes an application of deep learning with recurrent neural networks to the problem of predicting the next event in a business process . this is both a novel method in process prediction , which has largely relied on explicit process models , and also a novel application of deep learning methods . the approach is evaluated on two real datasets and our results surpass the state-of-the-art in prediction precision .", "topics": ["natural language processing", "recurrent neural network"]}
{"title": "image-text multi-modal representation learning by adversarial backpropagation", "abstract": "we present novel method for image-text multi-modal representation learning . in our knowledge , this work is the first approach of applying adversarial learning concept to multi-modal learning and not exploiting image-text pair information to learn multi-modal feature . we only use category information in contrast with most previous methods using image-text pair information for multi-modal embedding . in this paper , we show that multi-modal feature can be achieved without image-text pair information and our method makes more similar distribution with image and text in multi-modal feature space than other methods which use image-text pair information . and we show our multi-modal feature has universal semantic information , even though it was trained for category prediction . our model is end-to-end backpropagation , intuitive and easily extended to other multi-modal learning work .", "topics": ["feature learning", "feature vector"]}
{"title": "a syntax-based part-of-speech analyser", "abstract": "there are two main methodologies for constructing the knowledge base of a natural language analyser : the linguistic and the data-driven . recent state-of-the-art part-of-speech taggers are based on the data-driven approach . because of the known feasibility of the linguistic rule-based approach at related levels of description , the success of the data-driven approach in part-of-speech analysis may appear surprising . in this paper , a case is made for the syntactic nature of part-of-speech tagging . a new tagger of english that uses only linguistic distributional rules is outlined and empirically evaluated . tested against a benchmark corpus of 38,000 words of previously unseen text , this syntax-based system reaches an accuracy of above 99 % . compared to the 95-97 % accuracy of its best competitors , this result suggests the feasibility of the linguistic approach also in part-of-speech analysis .", "topics": ["natural language"]}
{"title": "feature descriptors for tracking by detection : a benchmark", "abstract": "in this paper , we provide an extensive evaluation of the performance of local descriptors for tracking applications . many different descriptors have been proposed in the literature for a wide range of application in computer vision such as object recognition and 3d reconstruction . more recently , due to fast key-point detectors , local image features can be used in online tracking frameworks . however , while much effort has been spent on evaluating their performance in terms of distinctiveness and robustness to image transformations , very little has been done in the contest of tracking . our evaluation is performed in terms of distinctiveness , tracking precision and tracking speed . our results show that binary descriptors like orb or brisk have comparable results to sift or akaze due to a higher number of key-points .", "topics": ["computer vision"]}
{"title": "degrees of freedom in deep neural networks", "abstract": "in this paper , we explore degrees of freedom in deep sigmoidal neural networks . we show that the degrees of freedom in these models is related to the expected optimism , which is the expected difference between test error and training error . we provide an efficient monte-carlo method to estimate the degrees of freedom for multi-class classification methods . we show degrees of freedom are lower than the parameter count in a simple xor network . we extend these results to neural nets trained on synthetic and real data , and investigate impact of network 's architecture and different regularization choices . the degrees of freedom in deep networks are dramatically smaller than the number of parameters , in some real datasets several orders of magnitude . further , we observe that for fixed number of parameters , deeper networks have less degrees of freedom exhibiting a regularization-by-depth .", "topics": ["neural networks", "synthetic data"]}
{"title": "firefly algorithm : recent advances and applications", "abstract": "nature-inspired metaheuristic algorithms , especially those based on swarm intelligence , have attracted much attention in the last ten years . firefly algorithm appeared in about five years ago , its literature has expanded dramatically with diverse applications . in this paper , we will briefly review the fundamentals of firefly algorithm together with a selection of recent publications . then , we discuss the optimality associated with balancing exploration and exploitation , which is essential for all metaheuristic algorithms . by comparing with intermittent search strategy , we conclude that metaheuristics such as firefly algorithm are better than the optimal intermittent search strategy . we also analyse algorithms and their implications for higher-dimensional optimization problems .", "topics": ["mathematical optimization"]}
{"title": "square deal : lower bounds and improved relaxations for tensor recovery", "abstract": "recovering a low-rank tensor from incomplete information is a recurring problem in signal processing and machine learning . the most popular convex relaxation of this problem minimizes the sum of the nuclear norms of the unfoldings of the tensor . we show that this approach can be substantially suboptimal : reliably recovering a $ k $ -way tensor of length $ n $ and tucker rank $ r $ from gaussian measurements requires $ \\omega ( r n^ { k-1 } ) $ observations . in contrast , a certain ( intractable ) nonconvex formulation needs only $ o ( r^k + nrk ) $ observations . we introduce a very simple , new convex relaxation , which partially bridges this gap . our new formulation succeeds with $ o ( r^ { \\lfloor k/2 \\rfloor } n^ { \\lceil k/2 \\rceil } ) $ observations . while these results pertain to gaussian measurements , simulations strongly suggest that the new norm also outperforms the sum of nuclear norms for tensor completion from a random subset of entries . our lower bound for the sum-of-nuclear-norms model follows from a new result on recovering signals with multiple sparse structures ( e.g . sparse , low rank ) , which perhaps surprisingly demonstrates the significant suboptimality of the commonly used recovery approach via minimizing the sum of individual sparsity inducing norms ( e.g . $ l_1 $ , nuclear norm ) . our new formulation for low-rank tensor recovery however opens the possibility in reducing the sample complexity by exploiting several structures jointly .", "topics": ["sparse matrix", "simulation"]}
{"title": "using more data to speed-up training time", "abstract": "in many recent applications , data is plentiful . by now , we have a rather clear understanding of how more data can be used to improve the accuracy of learning algorithms . recently , there has been a growing interest in understanding how more data can be leveraged to reduce the required training runtime . in this paper , we study the runtime of learning as a function of the number of available training examples , and underscore the main high-level techniques . we provide some initial positive results showing that the runtime can decrease exponentially while only requiring a polynomial growth of the number of examples , and spell-out several interesting open problems .", "topics": ["synthetic data", "polynomial"]}
{"title": "dispersion and line formation in artificial swarm intelligence", "abstract": "one of the major motifs in collective or swarm intelligence is that , even though individuals follow simple rules , the resulting global behavior can be complex and intelligent . in artificial swarm systems , such as swarm robots , the goal is to use systems that are as simple and cheap as possible , deploy many of them , and coordinate them to conduct complex tasks that each individual can not accomplish . shape formation in artificial intelligence systems is usually required for specific task-oriented performance , including 1 ) forming sensing grids , 2 ) exploring and mapping in space , underwater , or hazardous environments , and 3 ) forming a barricade for surveillance or protecting an area or a person . this paper presents a dynamic model of an artificial swarm system based on a virtual spring damper model and algorithms for dispersion without a leader and line formation with an interim leader using only the distance estimation among the neighbors .", "topics": ["artificial intelligence", "robot"]}
{"title": "learning filter bank sparsifying transforms", "abstract": "data is said to follow the transform ( or analysis ) sparsity model if it becomes sparse when acted on by a linear operator called a sparsifying transform . several algorithms have been designed to learn such a transform directly from data , and data-adaptive sparsifying transforms have demonstrated excellent performance in signal restoration tasks . sparsifying transforms are typically learned using small sub-regions of data called patches , but these algorithms often ignore redundant information shared between neighboring patches . we show that many existing transform and analysis sparse representations can be viewed as filter banks , thus linking the local properties of patch-based model to the global properties of a convolutional model . we propose a new transform learning framework where the sparsifying transform is an undecimated perfect reconstruction filter bank . unlike previous transform learning algorithms , the filter length can be chosen independently of the number of filter bank channels . numerical results indicate filter bank sparsifying transforms outperform existing patch-based transform learning for image denoising while benefiting from additional flexibility in the design process .", "topics": ["noise reduction", "sparse matrix"]}
{"title": "linear centralization classifier", "abstract": "a classification algorithm , called the linear centralization classifier ( lcc ) , is introduced . the algorithm seeks to find a transformation that best maps instances from the feature space to a space where they concentrate towards the center of their own classes , while maximimizing the distance between class centers . we formulate the classifier as a quadratic program with quadratic constraints . we then simplify this formulation to a linear program that can be solved effectively using a linear programming solver ( e.g . , simplex-dual ) . we extend the formulation for lcc to enable the use of kernel functions for non-linear classification applications . we compare our method with two standard classification methods ( support vector machine and linear discriminant analysis ) and four state-of-the-art classification methods when they are applied to eight standard classification datasets . our experimental results show that lcc is able to classify instances more accurately ( based on the area under the receiver operating characteristic ) in comparison to other tested methods on the chosen datasets . we also report the results for lcc with a particular kernel to solve for synthetic non-linear classification problems .", "topics": ["feature vector", "support vector machine"]}
{"title": "towards an automated image de-fencing algorithm using sparsity", "abstract": "conventional approaches to image de-fencing suffer from non-robust fence detection and are limited to processing images of static scenes . in this position paper , we propose an automatic de-fencing algorithm for images of dynamic scenes . we divide the problem of image de-fencing into the tasks of automated fence detection , motion estimation and fusion of data from multiple frames of a captured video of the dynamic scene . fences are detected automatically using two approaches , namely , employing gabor filter and a machine learning method . we cast the fence removal problem in an optimization framework , by modeling the formation of the degraded observations . the inverse problem is solved using split bregman technique assuming total variation of the de-fenced image as the regularization constraint .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "an exact and two heuristic strategies for truthful bidding in combinatorial transport auctions", "abstract": "to support a freight carrier in a combinatorial transport auction , we proposes an exact and two heuristic strategies for bidding on subsets of requests . the exact bidding strategy is based on the concept of elementary request combinations . we show that it is sufficient and necessary for a carrier to bid on each elementary request combination in order to guarantee the same result as bidding on each element of the powerset of the set of tendered requests . both heuristic bidding strategies identify promising request combinations . for this , pairwise synergies based on saving values as well as the capacitated p-median problem are used . the bidding strategies are evaluated by a computational study that simulates an auction . it is based on 174 benchmark instances and therefore easily extendable by other researchers . on average , the two heuristic strategies achieve 91 percent and 81 percent of the available sales potential while generating 36 and only 4 percent of the bundle bids of the exact strategy . therefore , the proposed bidding strategies help a carrier to increase her chance to win and at the same time reduce the computational burden to participate in a combinatorial transport auction .", "topics": ["heuristic"]}
{"title": "a visibility graph averaging aggregation operator", "abstract": "the problem of aggregation is considerable importance in many disciplines . in this paper , a new type of operator called visibility graph averaging ( vga ) aggregation operator is proposed . this proposed operator is based on the visibility graph which can convert a time series into a graph . the weights are obtained according to the importance of the data in the visibility graph . finally , the vga operator is used in the analysis of the taiex database to illustrate that it is practical and compared with the classic aggregation operators , it shows its advantage that it not only implements the aggregation of the data purely , but also conserves the time information , and meanwhile , the determination of the weights is more reasonable .", "topics": ["time series"]}
{"title": "nonnegative matrix factorization for semi-supervised dimensionality reduction", "abstract": "we show how to incorporate information from labeled examples into nonnegative matrix factorization ( nmf ) , a popular unsupervised learning algorithm for dimensionality reduction . in addition to mapping the data into a space of lower dimensionality , our approach aims to preserve the nonnegative components of the data that are important for classification . we identify these components from the support vectors of large-margin classifiers and derive iterative updates to preserve them in a semi-supervised version of nmf . these updates have a simple multiplicative form like their unsupervised counterparts ; they are also guaranteed at each iteration to decrease their loss function -- -a weighted sum of i-divergences that captures the trade-off between unsupervised and supervised learning . we evaluate these updates for dimensionality reduction when they are used as a precursor to linear classification . in this role , we find that they yield much better performance than their unsupervised counterparts . we also find one unexpected benefit of the low dimensional representations discovered by our approach : often they yield more accurate classifiers than both ordinary and transductive svms trained in the original input space .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "transduction on directed graphs via absorbing random walks", "abstract": "in this paper we consider the problem of graph-based transductive classification , and we are particularly interested in the directed graph scenario which is a natural form for many real world applications . different from existing research efforts that either only deal with undirected graphs or circumvent directionality by means of symmetrization , we propose a novel random walk approach on directed graphs using absorbing markov chains , which can be regarded as maximizing the accumulated expected number of visits from the unlabeled transient states . our algorithm is simple , easy to implement , and works with large-scale graphs . in particular , it is capable of preserving the graph structure even when the input graph is sparse and changes over time , as well as retaining weak signals presented in the directed edges . we present its intimate connections to a number of existing methods , including graph kernels , graph laplacian based methods , and interestingly , spanning forest of graphs . its computational complexity and the generalization error are also studied . empirically our algorithm is systematically evaluated on a wide range of applications , where it has shown to perform competitively comparing to a suite of state-of-the-art methods .", "topics": ["computational complexity theory", "sparse matrix"]}
{"title": "fast forward feature selection for the nonlinear classification of hyperspectral images", "abstract": "a fast forward feature selection algorithm is presented in this paper . it is based on a gaussian mixture model ( gmm ) classifier . gmm are used for classifying hyperspectral images . the algorithm selects iteratively spectral features that maximizes an estimation of the classification rate . the estimation is done using the k-fold cross validation . in order to perform fast in terms of computing time , an efficient implementation is proposed . first , the gmm can be updated when the estimation of the classification rate is computed , rather than re-estimate the full model . secondly , using marginalization of the gmm , sub models can be directly obtained from the full model learned with all the spectral features . experimental results for two real hyperspectral data sets show that the method performs very well in terms of classification accuracy and processing time . furthermore , the extracted model contains very few spectral channels .", "topics": ["nonlinear system"]}
{"title": "accelerated reinforcement learning", "abstract": "policy gradient methods are widely used in reinforcement learning algorithms to search for better policies in the parameterized policy space . they do gradient search in the policy space and are known to converge very slowly . nesterov developed an accelerated gradient search algorithm for convex optimization problems . this has been recently extended for non-convex and also stochastic optimization . we use nesterov 's acceleration for policy gradient search in the well-known actor-critic algorithm and show the convergence using ode method . we tested this algorithm on a scheduling problem . here an incoming job is scheduled into one of the four queues based on the queue lengths . we see from experimental results that algorithm using nesterov 's acceleration has significantly better performance compared to algorithm which do not use acceleration . to the best of our knowledge this is the first time nesterov 's acceleration has been used with actor-critic algorithm .", "topics": ["reinforcement learning", "gradient"]}
{"title": "optimal transport maps for distribution preserving operations on latent spaces of generative models", "abstract": "generative models such as variational auto encoders ( vaes ) and generative adversarial networks ( gans ) are typically trained for a fixed prior distribution in the latent space , such as uniform or gaussian . after a trained model is obtained , one can sample the generator in various forms for exploration and understanding , such as interpolating between two samples , sampling in the vicinity of a sample or exploring differences between a pair of samples applied to a third sample . in this paper , we show that the latent space operations used in the literature so far induce a distribution mismatch between the resulting outputs and the prior distribution the model was trained on . to address this , we propose to use distribution matching transport maps to ensure that such latent space operations preserve the prior distribution , while minimally modifying the original operation . our experimental results validate that the proposed operations give higher quality samples compared to the original operations .", "topics": ["sampling ( signal processing )", "map"]}
{"title": "discovering underlying plans based on distributed representations of actions", "abstract": "plan recognition aims to discover target plans ( i.e . , sequences of actions ) behind observed actions , with history plan libraries or domain models in hand . previous approaches either discover plans by maximally `` matching '' observed actions to plan libraries , assuming target plans are from plan libraries , or infer plans by executing domain models to best explain the observed actions , assuming complete domain models are available . in real world applications , however , target plans are often not from plan libraries and complete domain models are often not available , since building complete sets of plans and complete domain models are often difficult or expensive . in this paper we view plan libraries as corpora and learn vector representations of actions using the corpora ; we then discover target plans based on the vector representations . our approach is capable of discovering underlying plans that are not from plan libraries , without requiring domain models provided . we empirically demonstrate the effectiveness of our approach by comparing its performance to traditional plan recognition approaches in three planning domains .", "topics": ["text corpus"]}
{"title": "deep within-class covariance analysis for acoustic scene classification", "abstract": "within-class covariance normalization ( wccn ) is a powerful post-processing method for normalizing the within-class covariance of a set of data points . wccn projects the observations into a linear sub-space where the within-class variability is reduced . this property has proven to be beneficial in subsequent recognition tasks . the central idea of this paper is to reformulate the classic wccn as a deep neural network ( dnn ) compatible version . we propose the deep withinclass covariance analysis ( dwcca ) which can be incorporated in a dnn architecture . this formulation enables us to exploit the beneficial properties of wccn , and still allows for training with stochastic gradient descent ( sgd ) in an end-to-end fashion . we investigate the advantages of dwcca on deep neural networks with convolutional layers for supervised learning . our results on acoustic scene classification show that via dwcca we can achieves equal or superior performance in a vgg-style deep neural network .", "topics": ["supervised learning", "gradient descent"]}
{"title": "anaphora resolution in japanese sentences using surface expressions and examples", "abstract": "anaphora resolution is one of the major problems in natural language processing . it is also one of the important tasks in machine translation and man/machine dialogue . we solve the problem by using surface expressions and examples . surface expressions are the words in sentences which provide clues for anaphora resolution . examples are linguistic data which are actually used in conversations and texts . the method using surface expressions and examples is a practical method . this thesis handles almost all kinds of anaphora : i . the referential property and number of a noun phrase ii . noun phrase direct anaphora iii . noun phrase indirect anaphora iv . pronoun anaphora v. verb phrase ellipsis", "topics": ["natural language processing", "machine translation"]}
{"title": "notes about a more aware dependency parser", "abstract": "in this paper i explain the reasons that led me to research and conceive a novel technology for dependency parsing , mixing together the strengths of data-driven transition-based and constraint-based approaches . in particular i highlight the problem to infer the reliability of the results of a data-driven transition-based parser , which is extremely important for high-level processes that expect to use correct parsing results . i then briefly introduce a number of notes about a new parser model i 'm working on , capable to proceed with the analysis in a `` more aware '' way , with a more `` robust '' concept of robustness .", "topics": ["high- and low-level", "parsing"]}
{"title": "projection onto the probability simplex : an efficient algorithm with a simple proof , and an application", "abstract": "we provide an elementary proof of a simple , efficient algorithm for computing the euclidean projection of a point onto the probability simplex . we also show an application in laplacian k-modes clustering .", "topics": ["cluster analysis", "optimization problem"]}
{"title": "bayesian forecasting of www traffic on the time varying poisson model", "abstract": "traffic forecasting from past observed traffic data with small calculation complexity is one of important problems for planning of servers and networks . focusing on world wide web ( www ) traffic as fundamental investigation , this paper would deal with bayesian forecasting of network traffic on the time varying poisson model from a viewpoint from statistical decision theory . under this model , we would show that the estimated forecasting value is obtained by simple arithmetic calculation and expresses real www traffic well from both theoretical and empirical points of view .", "topics": ["computational complexity theory"]}
{"title": "dockerface : an easy to install and use faster r-cnn face detector in a docker container", "abstract": "face detection is a very important task and a necessary pre-processing step for many applications such as facial landmark detection , pose estimation , sentiment analysis and face recognition . not only is face detection an important pre-processing step in computer vision applications but also in computational psychology , behavioral imaging and other fields where researchers might not be initiated in computer vision frameworks and state-of-the-art detection applications . a large part of existing research that includes face detection as a pre-processing step uses existing out-of-the-box detectors such as dlib and the opencv haar face detector which no longer state-of-the-art - they are primarily used because of their ease of use . we introduce dockerface , a very accurate faster r-cnn face detector in a docker container which requires no training and is easy to install and use .", "topics": ["computer vision"]}
{"title": "fast generation for convolutional autoregressive models", "abstract": "convolutional autoregressive models have recently demonstrated state-of-the-art performance on a number of generation tasks . while fast , parallel training methods have been crucial for their success , generation is typically implemented in a na\\ '' { i } ve fashion where redundant computations are unnecessarily repeated . this results in slow generation , making such models infeasible for production environments . in this work , we describe a method to speed up generation in convolutional autoregressive models . the key idea is to cache hidden states to avoid redundant computation . we apply our fast generation method to the wavenet and pixelcnn++ models and achieve up to $ 21\\times $ and $ 183\\times $ speedups respectively .", "topics": ["computation"]}
{"title": "probabilistic similarity logic", "abstract": "many machine learning applications require the ability to learn from and reason about noisy multi-relational data . to address this , several effective representations have been developed that provide both a language for expressing the structural regularities of a domain , and principled support for probabilistic inference . in addition to these two aspects , however , many applications also involve a third aspect-the need to reason about similarities-which has not been directly supported in existing frameworks . this paper introduces probabilistic similarity logic ( psl ) , a general-purpose framework for joint reasoning about similarity in relational domains that incorporates probabilistic reasoning about similarities and relational structure in a principled way . psl can integrate any existing domain-specific similarity measures and also supports reasoning about similarities between sets of entities . we provide efficient inference and learning techniques for psl and demonstrate its effectiveness both in common relational tasks and in settings that require reasoning about similarity .", "topics": ["entity"]}
{"title": "high level pattern classification via tourist walks in networks", "abstract": "complex networks refer to large-scale graphs with nontrivial connection patterns . the salient and interesting features that the complex network study offer in comparison to graph theory are the emphasis on the dynamical properties of the networks and the ability of inherently uncovering pattern formation of the vertices . in this paper , we present a hybrid data classification technique combining a low level and a high level classifier . the low level term can be equipped with any traditional classification techniques , which realize the classification task considering only physical features ( e.g . , geometrical or statistical features ) of the input data . on the other hand , the high level term has the ability of detecting data patterns with semantic meanings . in this way , the classification is realized by means of the extraction of the underlying network 's features constructed from the input data . as a result , the high level classification process measures the compliance of the test instances with the pattern formation of the training data . out of various high level perspectives that can be utilized to capture semantic meaning , we utilize the dynamical features that are generated from a tourist walker in a networked environment . specifically , a weighted combination of transient and cycle lengths generated by the tourist walk is employed for that end . interestingly , our study shows that the proposed technique is able to further improve the already optimized performance of traditional classification techniques .", "topics": ["test set", "high- and low-level"]}
{"title": "neural turing machines", "abstract": "we extend the capabilities of neural networks by coupling them to external memory resources , which they can interact with by attentional processes . the combined system is analogous to a turing machine or von neumann architecture but is differentiable end-to-end , allowing it to be efficiently trained with gradient descent . preliminary results demonstrate that neural turing machines can infer simple algorithms such as copying , sorting , and associative recall from input and output examples .", "topics": ["gradient descent", "gradient"]}
{"title": "policy iteration for decentralized control of markov decision processes", "abstract": "coordination of distributed agents is required for problems arising in many areas , including multi-robot systems , networking and e-commerce . as a formal framework for such problems , we use the decentralized partially observable markov decision process ( dec-pomdp ) . though much work has been done on optimal dynamic programming algorithms for the single-agent version of the problem , optimal algorithms for the multiagent case have been elusive . the main contribution of this paper is an optimal policy iteration algorithm for solving dec-pomdps . the algorithm uses stochastic finite-state controllers to represent policies . the solution can include a correlation device , which allows agents to correlate their actions without communicating . this approach alternates between expanding the controller and performing value-preserving transformations , which modify the controller without sacrificing value . we present two efficient value-preserving transformations : one can reduce the size of the controller and the other can improve its value while keeping the size fixed . empirical results demonstrate the usefulness of value-preserving transformations in increasing value while keeping controller size to a minimum . to broaden the applicability of the approach , we also present a heuristic version of the policy iteration algorithm , which sacrifices convergence to optimality . this algorithm further reduces the size of the controllers at each step by assuming that probability distributions over the other agents actions are known . while this assumption may not hold in general , it helps produce higher quality solutions in our test problems .", "topics": ["iteration", "heuristic"]}
{"title": "on the computation of paracoherent answer sets", "abstract": "answer set programming ( asp ) is a well-established formalism for nonmonotonic reasoning . an asp program can have no answer set due to cyclic default negation . in this case , it is not possible to draw any conclusion , even if this is not intended . recently , several paracoherent semantics have been proposed that address this issue , and several potential applications for these semantics have been identified . however , paracoherent semantics have essentially been inapplicable in practice , due to the lack of efficient algorithms and implementations . in this paper , this lack is addressed , and several different algorithms to compute semi-stable and semi-equilibrium models are proposed and implemented into an answer set solving framework . an empirical performance comparison among the new algorithms on benchmarks from asp competitions is given as well .", "topics": ["computation"]}
{"title": "a novel directional weighted minimum deviation ( dwmd ) based filter for removal of random valued impulse noise", "abstract": "the most median-based de noising methods works fine for restoring the images corrupted by randomn valued impulse noise with low noise level but very poor with highly corrupted images . in this paper a directional weighted minimum deviation ( dwmd ) based filter has been proposed for removal of high random valued impulse noise ( rvin ) . the proposed approach based on standard deviation ( sd ) works in two phases . the first phase detects the contaminated pixels by differencing between the test pixel and its neighbor pixels aligned with four main directions . the second phase filters only those pixels keeping others intact . the filtering scheme is based on minimum standard deviation of the four directional pixels . extensive simulations show that the proposed filter not only provide better performance of de noising rvin but can preserve more details features even thin lines or dots . this technique shows better performance in terms of psnr , image fidelity and computational cost compared to the existing filters .", "topics": ["simulation", "pixel"]}
{"title": "group-sparse signal denoising : non-convex regularization , convex optimization", "abstract": "convex optimization with sparsity-promoting convex regularization is a standard approach for estimating sparse signals in noise . in order to promote sparsity more strongly than convex regularization , it is also standard practice to employ non-convex optimization . in this paper , we take a third approach . we utilize a non-convex regularization term chosen such that the total cost function ( consisting of data consistency and regularization terms ) is convex . therefore , sparsity is more strongly promoted than in the standard convex formulation , but without sacrificing the attractive aspects of convex optimization ( unique minimum , robust algorithms , etc . ) . we use this idea to improve the recently developed 'overlapping group shrinkage ' ( ogs ) algorithm for the denoising of group-sparse signals . the algorithm is applied to the problem of speech enhancement with favorable results in terms of both snr and perceptual quality .", "topics": ["noise reduction", "matrix regularization"]}
{"title": "a direct method for estimating a causal ordering in a linear non-gaussian acyclic model", "abstract": "structural equation models and bayesian networks have been widely used to analyze causal relations between continuous variables . in such frameworks , linear acyclic models are typically used to model the datagenerating process of variables . recently , it was shown that use of non-gaussianity identifies a causal ordering of variables in a linear acyclic model without using any prior knowledge on the network structure , which is not the case with conventional methods . however , existing estimation methods are based on iterative search algorithms and may not converge to a correct solution in a finite number of steps . in this paper , we propose a new direct method to estimate a causal ordering based on non-gaussianity . in contrast to the previous methods , our algorithm requires no algorithmic parameters and is guaranteed to converge to the right solution within a small fixed number of steps if the data strictly follows the model .", "topics": ["bayesian network", "causality"]}
{"title": "english character recognition using artificial neural network", "abstract": "this work focuses on development of a offline hand written english character recognition algorithm based on artificial neural network ( ann ) . the ann implemented in this work has single output neuron which shows whether the tested character belongs to a particular cluster or not . the implementation is carried out completely in 'c ' language . ten sets of english alphabets ( small-26 , capital-26 ) were used to train the ann and 5 sets of english alphabets were used to test the network . the characters were collected from different persons over duration of about 25 days . the algorithm was tested with 5 capital letters and 5 small letter sets . however , the result showed that the algorithm recognized english alphabet patterns with maximum accuracy of 92.59 % and false rejection rate ( frr ) of 0 % .", "topics": ["cluster analysis"]}
{"title": "f-gans in an information geometric nutshell", "abstract": "nowozin \\textit { et al } showed last year how to extend the gan \\textit { principle } to all $ f $ -divergences . the approach is elegant but falls short of a full description of the supervised game , and says little about the key player , the generator : for example , what does the generator actually converge to if solving the gan game means convergence in some space of parameters ? how does that provide hints on the generator 's design and compare to the flourishing but almost exclusively experimental literature on the subject ? in this paper , we unveil a broad class of distributions for which such convergence happens -- - namely , deformed exponential families , a wide superset of exponential families -- - and show tight connections with the three other key gan parameters : loss , game and architecture . in particular , we show that current deep architectures are able to factorize a very large number of such densities using an especially compact design , hence displaying the power of deep architectures and their concinnity in the $ f $ -gan game . this result holds given a sufficient condition on \\textit { activation functions } -- - which turns out to be satisfied by popular choices . the key to our results is a variational generalization of an old theorem that relates the kl divergence between regular exponential families and divergences between their natural parameters . we complete this picture with additional results and experimental insights on how these results may be used to ground further improvements of gan architectures , via ( i ) a principled design of the activation functions in the generator and ( ii ) an explicit integration of proper composite losses ' link function in the discriminator .", "topics": ["calculus of variations", "time complexity"]}
{"title": "fast approximate nearest neighbor search with the navigating spreading-out graph", "abstract": "the approximate nearest neighbor search ( anns ) is a fundamental problem in machine learning and data mining . an anns algorithm is required to be efficient on both memory use and search performance . recently , graph-based methods have achieved revolutionary performance on public datasets . the search algorithm on a graph is a greedy algorithm . it can be applied to various graph structures and shows promising performance . recent improvements of the graph-based methods mainly focus on two aspects : ( 1 ) some provide better initial search position to prevent the search from being stuck in some local optima , which is far away from the correct answers . ( 2 ) others try to construct better graphs for faster traversing and neighbor locating . in our observation , an ideal graph for search should consider four aspects , ( 1 ) lowering the average out-degree of the graph for fast traversing ; ( 2 ) ensuring the connectivity of the graph ; ( 3 ) avoiding detours ; and ( 4 ) avoiding additional index structures to reduce the index size . none of the previous methods take all these four aspects into consideration simultaneously , which prevents them from achieving better performance . in this paper , we present a novel graph structure named navigating spreading-out graph ( nsg ) to take the four aspects into account simultaneously . extensive experiments show that our algorithm outperforms all the existing algorithms significantly . what 's more , our algorithm outperforms the existing approach of taobao ( alibaba group ) and has been integrated into their search engine for billion-scale search .", "topics": ["data mining"]}
{"title": "how many dissimilarity/kernel self organizing map variants do we need ?", "abstract": "in numerous applicative contexts , data are too rich and too complex to be represented by numerical vectors . a general approach to extend machine learning and data mining techniques to such data is to really on a dissimilarity or on a kernel that measures how different or similar two objects are . this approach has been used to define several variants of the self organizing map ( som ) . this paper reviews those variants in using a common set of notations in order to outline differences and similarities between them . it discusses the advantages and drawbacks of the variants , as well as the actual relevance of the dissimilarity/kernel som for practical applications .", "topics": ["kernel ( operating system )", "data mining"]}
{"title": "optimization of test case generation using genetic algorithm ( ga )", "abstract": "testing provides means pertaining to assuring software performance . the total aim of software industry is actually to make a certain start associated with high quality software for the end user . however , associated with software testing has quite a few underlying concerns , which are very important and need to pay attention on these issues . these issues are effectively generating , prioritization of test cases , etc . these issues can be overcome by paying attention and focus . solitary of the greatest problems in the software testing area is usually how to acquire a great proper set associated with cases to confirm software . some other strategies and also methodologies are proposed pertaining to shipping care of most of these issues . genetic algorithm ( ga ) belongs to evolutionary algorithms . evolutionary algorithms have a significant role in the automatic test generation and many researchers are focusing on it . in this study explored software testing related issues by using the ga approach . in addition to right after applying some analysis , better solution produced , that is feasible and reliable . the particular research presents the implementation of gas because of its generation of optimized test cases . along these lines , this paper gives proficient system for the optimization of test case generation using genetic algorithm .", "topics": ["mathematical optimization"]}
{"title": "gibbs-ringing artifact removal based on local subvoxel-shifts", "abstract": "gibbs-ringing is a well known artifact which manifests itself as spurious oscillations in the vicinity of sharp image transients , e.g . at tissue boundaries . the origin can be seen in the truncation of k-space during mri data-acquisition . consequently , correction techniques like gegenbauer reconstruction or extrapolation methods aim at recovering these missing data . here , we present a simple and robust method which exploits a different view on the gibbs-phenomena . the truncation in k-space can be interpreted as a convolution with a sinc-function in image space . hence , the severity of the artifacts depends on how the sinc-function is sampled . we propose to re-interpolate the image based on local , subvoxel shifts to sample the ringing pattern at the zero-crossings of the oscillating sinc-function . with this , the artifact can effectively and robustly be removed with a minimal amount of smoothing .", "topics": ["image processing", "convolution"]}
{"title": "enhancing underwater imagery using generative adversarial networks", "abstract": "autonomous underwater vehicles ( auvs ) rely on a variety of sensors - acoustic , inertial and visual - for intelligent decision making . due to its non-intrusive , passive nature , and high information content , vision is an attractive sensing modality , particularly at shallower depths . however , factors such as light refraction and absorption , suspended particles in the water , and color distortion affect the quality of visual data , resulting in noisy and distorted images . auvs that rely on visual sensing thus face difficult challenges , and consequently exhibit poor performance on vision-driven tasks . this paper proposes a method to improve the quality of visual underwater scenes using generative adversarial networks ( gans ) , with the goal of improving input to vision-driven behaviors further down the autonomy pipeline . furthermore , we show how recently proposed methods are able to generate a dataset for the purpose of such underwater image restoration . for any visually-guided underwater robots , this improvement can result in increased safety and reliability through robust visual perception . to that effect , we present quantitative and qualitative data which demonstrates that images corrected through the proposed approach generate more visually appealing images , and also provide increased accuracy for a diver tracking algorithm .", "topics": ["sensor", "autonomous car"]}
{"title": "stochastic primal dual coordinate method with non-uniform sampling based on optimality violations", "abstract": "we study primal-dual type stochastic optimization algorithms with non-uniform sampling . our main theoretical contribution in this paper is to present a convergence analysis of stochastic primal dual coordinate ( spdc ) method with arbitrary sampling . based on this theoretical framework , we propose optimality violation-based sampling spdc ( ovsspdc ) , a non-uniform sampling method based on optimality violation . we also propose two efficient heuristic variants of ovsspdc called ovssdpc+ and ovssdpc++ . through intensive numerical experiments , we demonstrate that the proposed method and its variants are faster than other state-of-the-art primal-dual type stochastic optimization methods .", "topics": ["sampling ( signal processing )", "numerical analysis"]}
{"title": "similarity of objects and the meaning of words", "abstract": "we survey the emerging area of compression-based , parameter-free , similarity distance measures useful in data-mining , pattern recognition , learning and automatic semantics extraction . given a family of distances on a set of objects , a distance is universal up to a certain precision for that family if it minorizes every distance in the family between every two objects in the set , up to the stated precision ( we do not require the universal distance to be an element of the family ) . we consider similarity distances for two types of objects : literal objects that as such contain all of their meaning , like genomes or books , and names for objects . the latter may have literal embodyments like the first type , but may also be abstract like `` red '' or `` christianity . '' for the first type we consider a family of computable distance measures corresponding to parameters expressing similarity according to particular featuresdistances generated by web users corresponding to particular semantic relations between the ( names for ) the designated objects . for both families we give universal similarity distance measures , incorporating all particular distance measures in the family . in the first case the universal distance is based on compression and in the second case it is based on google page counts related to search terms . in both cases experiments on a massive scale give evidence of the viability of the approaches . between pairs of literal objects . for the second type we consider similarity", "topics": ["data mining"]}
{"title": "infynlp at smm4h task 2 : stacked ensemble of shallow convolutional neural networks for identifying personal medication intake from twitter", "abstract": "this paper describes infosys 's participation in the `` 2nd social media mining for health applications shared task at amia , 2017 , task 2 '' . mining social media messages for health and drug related information has received significant interest in pharmacovigilance research . this task targets at developing automated classification models for identifying tweets containing descriptions of personal intake of medicines . towards this objective we train a stacked ensemble of shallow convolutional neural network ( cnn ) models on an annotated dataset provided by the organizers . we use random search for tuning the hyper-parameters of the cnn and submit an ensemble of best models for the prediction task . our system secured first place among 9 teams , with a micro-averaged f-score of 0.693 .", "topics": ["data mining", "test set"]}
{"title": "action recognition with visual attention on skeleton images", "abstract": "action recognition with 3d skeleton sequences is becoming popular due to its speed and robustness . the recently proposed convolutional neural networks ( cnn ) based methods have shown good performance in learning spatio-temporal representations for skeleton sequences . despite the good recognition accuracy achieved by previous cnn based methods , there exist two problems that potentially limit the performance . first , previous skeleton representations are generated by chaining joints with a fixed order . the corresponding semantic meaning is unclear and the structural information among the joints is lost . second , previous models do not have an ability to focus on informative joints . the attention mechanism is important for skeleton based action recognition because there exist spatio-temporal key stages and the joint predictions can be inaccurate . to solve the two problems , we propose a novel cnn based method for skeleton based action recognition . we first redesign the skeleton representations with a depth-first tree traversal order , which enhances the semantic meaning of skeleton images and better preserves the structural information . we then propose the idea of a two-branch attention architecture that focuses on spatio-temporal key stages and filters out unreliable joint predictions . a base attention model with the simplest structure is first introduced to illustrate the two-branch attention architecture . by improving the structures in both branches , we further propose a global long-sequence attention network ( glan ) . experiment results on the ntu rgb+d dataset and the sbu kinetic interaction dataset show that our proposed approach outperforms the state-of-the-art , as well as the effectiveness of each component .", "topics": ["neural networks"]}
{"title": "multi-timescale memory dynamics in a reinforcement learning network with attention-gated memory", "abstract": "learning and memory are intertwined in our brain and their relationship is at the core of several recent neural network models . in particular , the attention-gated memory tagging model ( augment ) is a reinforcement learning network with an emphasis on biological plausibility of memory dynamics and learning . we find that the augment network does not solve some hierarchical tasks , where higher-level stimuli have to be maintained over a long time , while lower-level stimuli need to be remembered and forgotten over a shorter timescale . to overcome this limitation , we introduce hybrid augment , with leaky or short-timescale and non-leaky or long-timescale units in memory , that allow to exchange lower-level information while maintaining higher-level one , thus solving both hierarchical and distractor tasks .", "topics": ["reinforcement learning"]}
{"title": "object detection using image processing", "abstract": "an unmanned ariel vehicle ( uav ) has greater importance in the army for border security . the main objective of this article is to develop an opencv-python code using haar cascade algorithm for object and face detection . currently , uavs are used for detecting and attacking the infiltrated ground targets . the main drawback for this type of uavs is that sometimes the object are not properly detected , which thereby causes the object to hit the uav . this project aims to avoid such unwanted collisions and damages of uav . uav is also used for surveillance that uses voila-jones algorithm to detect and track humans . this algorithm uses cascade object detector function and vision . train function to train the algorithm . the main advantage of this code is the reduced processing time . the python code was tested with the help of available database of video and image , the output was verified .", "topics": ["image processing", "object detection"]}
{"title": "binary decision diagrams for affine approximation", "abstract": "selman and kautz 's work on `` knowledge compilation '' established how approximation ( strengthening and/or weakening ) of a propositional knowledge-base can be used to speed up query processing , at the expense of completeness . in this classical approach , querying uses horn over- and under-approximations of a given knowledge-base , which is represented as a propositional formula in conjunctive normal form ( cnf ) . along with the class of horn functions , one could imagine other boolean function classes that might serve the same purpose , owing to attractive deduction-computational properties similar to those of the horn functions . indeed , zanuttini has suggested that the class of affine boolean functions could be useful in knowledge compilation and has presented an affine approximation algorithm . since cnf is awkward for presenting affine functions , zanuttini considers both a sets-of-models representation and the use of modulo 2 congruence equations . in this paper , we propose an algorithm based on reduced ordered binary decision diagrams ( robdds ) . this leads to a representation which is more compact than the sets of models and , once we have established some useful properties of affine boolean functions , a more efficient algorithm .", "topics": ["approximation algorithm", "approximation"]}
{"title": "attended end-to-end architecture for age estimation from facial expression videos", "abstract": "the main challenges of age estimation from facial expression videos lie not only in the modeling of the static facial appearance , but also in the capturing of the temporal facial dynamics . traditional techniques to this problem focus on constructing handcrafted features to explore the discriminative information contained in facial appearance and dynamics separately . this relies on sophisticated feature-refinement and framework-design . in this paper , we present an end-to-end architecture for age estimation which is able to simultaneously learn both the appearance and dynamics of age from raw videos of facial expressions . specifically , we employ convolutional neural networks to extract effective latent appearance representations and feed them into recurrent networks to model the temporal dynamics . more importantly , we propose to leverage attention models for salience detection in both the spatial domain for each single image and the temporal domain for the whole video as well . we design a specific spatially-indexed attention mechanism among the convolutional layers to extract the salient facial regions in each individual image , and a temporal attention layer to assign attention weights to each frame . this two-pronged approach not only improves the performance by allowing the model to focus on informative frames and facial areas , but it also offers an interpretable correspondence between the spatial facial regions as well as temporal frames , and the task of age estimation . we demonstrate the strong performance of our model in experiments on a large , gender-balanced database with 400 subjects with ages spanning from 8 to 76 years . experiments reveal that our model exhibits significant superiority over the state-of-the-art methods given sufficient training data .", "topics": ["test set", "end-to-end principle"]}
{"title": "a generic tool to generate a lexicon for nlp from lexicon-grammar tables", "abstract": "lexicon-grammar tables constitute a large-coverage syntactic lexicon but they can not be directly used in natural language processing ( nlp ) applications because they sometimes rely on implicit information . in this paper , we introduce lgextract , a generic tool for generating a syntactic lexicon for nlp from the lexicon-grammar tables . it is based on a global table that contains undefined information and on a unique extraction script including all operations to be performed for all tables . we also present an experiment that has been conducted to generate a new lexicon of french verbs and predicative nouns .", "topics": ["natural language processing", "natural language"]}
{"title": "exploiting spatio-temporal structure with recurrent winner-take-all networks", "abstract": "we propose a convolutional recurrent neural network , with winner-take-all dropout for high dimensional unsupervised feature learning in multi-dimensional time series . we apply the proposedmethod for object recognition with temporal context in videos and obtain better results than comparable methods in the literature , including the deep predictive coding networks previously proposed by chalasani and principe.our contributions can be summarized as a scalable reinterpretation of the deep predictive coding networks trained end-to-end with backpropagation through time , an extension of the previously proposed winner-take-all autoencoders to sequences in time , and a new technique for initializing and regularizing convolutional-recurrent neural networks .", "topics": ["feature learning", "recurrent neural network"]}
{"title": "impala : scalable distributed deep-rl with importance weighted actor-learner architectures", "abstract": "in this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters . a key challenge is to handle the increased amount of data and extended training time . we have developed a new distributed agent impala ( importance weighted actor-learner architecture ) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation . we achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called v-trace . we demonstrate the effectiveness of impala for multi-task reinforcement learning on dmlab-30 ( a set of 30 tasks from the deepmind lab environment ( beattie et al . , 2016 ) ) and atari-57 ( all available atari games in arcade learning environment ( bellemare et al . , 2013a ) ) . our results show that impala is able to achieve better performance than previous agents with less data , and crucially exhibits positive transfer between tasks as a result of its multi-task approach .", "topics": ["reinforcement learning"]}
{"title": "factored contextual policy search with bayesian optimization", "abstract": "scarce data is a major challenge to scaling robot learning to truly complex tasks , as we need to generalize locally learned policies over different `` contexts '' . bayesian optimization approaches to contextual policy search ( cps ) offer data-efficient policy learning that generalize over a context space . we propose to improve data- efficiency by factoring typically considered contexts into two components : target- type contexts that correspond to a desired outcome of the learned behavior , e.g . target position for throwing a ball ; and environment type contexts that correspond to some state of the environment , e.g . initial ball position or wind speed . our key observation is that experience can be directly generalized over target-type contexts . based on that we introduce factored contextual policy search with bayesian optimization for both passive and active learning settings . preliminary results show faster policy generalization on a simulated toy problem .", "topics": ["simulation"]}
{"title": "localized partial evaluation of belief networks", "abstract": "most algorithms for propagating evidence through belief networks have been exact and exhaustive : they produce an exact ( point-valued ) marginal probability for every node in the network . often , however , an application will not need information about every n ode in the network nor will it need exact probabilities . we present the localized partial evaluation ( lpe ) propagation algorithm , which computes interval bounds on the marginal probability of a specified query node by examining a subset of the nodes in the entire network . conceptually , lpe ignores parts of the network that are `` too far away '' from the queried node to have much impact on its value . lpe has the `` anytime '' property of being able to produce better solutions ( tighter intervals ) given more time to consider more of the network .", "topics": ["bayesian network"]}
{"title": "3d display calibration by visual pattern analysis", "abstract": "nearly all 3d displays need calibration for correct rendering . more often than not , the optical elements in a 3d display are misaligned from the designed parameter setting . as a result , 3d magic does not perform well as intended . the observed images tend to get distorted . in this paper , we propose a novel display calibration method to fix the situation . in our method , a pattern image is displayed on the panel and a camera takes its pictures twice at different positions . then , based on a quantitative model , we extract all display parameters ( i.e . , pitch , slanted angle , gap or thickness , offset ) from the observed patterns in the captured images . for high accuracy and robustness , our method analyzes the patterns mostly in frequency domain . we conduct two types of experiments for validation ; one with optical simulation for quantitative results and the other with real-life displays for qualitative assessment . experimental results demonstrate that our method is quite accurate , about a half order of magnitude higher than prior work ; is efficient , spending less than 2 s for computation ; and is robust to noise , working well in the snr regime as low as 6 db .", "topics": ["simulation", "computation"]}
{"title": "parallel attention : a unified framework for visual object discovery through dialogs and queries", "abstract": "recognising objects according to a pre-defined fixed set of class labels has been well studied in the computer vision . there are a great many practical applications where the subjects that may be of interest are not known beforehand , or so easily delineated , however . in many of these cases natural language dialog is a natural way to specify the subject of interest , and the task achieving this capability ( a.k.a , referring expression comprehension ) has recently attracted attention . to this end we propose a unified framework , the parallel attention ( plan ) network , to discover the object in an image that is being referred to in variable length natural expression descriptions , from short phrases query to long multi-round dialogs . the plan network has two attention mechanisms that relate parts of the expressions to both the global visual content and also directly to object candidates . furthermore , the attention mechanisms are recurrent , making the referring process visualizable and explainable . the attended information from these dual sources are combined to reason about the referred object . these two attention mechanisms can be trained in parallel and we find the combined system outperforms the state-of-art on several benchmarked datasets with different length language input , such as refcoco , refcoco+ and guesswhat ? ! .", "topics": ["recurrent neural network", "computer vision"]}
{"title": "computational capabilities of random automata networks for reservoir computing", "abstract": "this paper underscores the conjecture that intrinsic computation is maximal in systems at the `` edge of chaos . '' we study the relationship between dynamics and computational capability in random boolean networks ( rbn ) for reservoir computing ( rc ) . rc is a computational paradigm in which a trained readout layer interprets the dynamics of an excitable component ( called the reservoir ) that is perturbed by external input . the reservoir is often implemented as a homogeneous recurrent neural network , but there has been little investigation into the properties of reservoirs that are discrete and heterogeneous . random boolean networks are generic and heterogeneous dynamical systems and here we use them as the reservoir . an rbn is typically a closed system ; to use it as a reservoir we extend it with an input layer . as a consequence of perturbation , the rbn does not necessarily fall into an attractor . computational capability in rc arises from a trade-off between separability and fading memory of inputs . we find the balance of these properties predictive of classification power and optimal at critical connectivity . these results are relevant to the construction of devices which exploit the intrinsic dynamics of complex heterogeneous systems , such as biomolecular substrates .", "topics": ["recurrent neural network", "computation"]}
{"title": "improved distributed principal component analysis", "abstract": "we study the distributed computing setting in which there are multiple servers , each holding a set of points , who wish to compute functions on the union of their point sets . a key task in this setting is principal component analysis ( pca ) , in which the servers would like to compute a low dimensional subspace capturing as much of the variance of the union of their point sets as possible . given a procedure for approximate pca , one can use it to approximately solve $ \\ell_2 $ -error fitting problems such as $ k $ -means clustering and subspace clustering . the essential properties of an approximate distributed pca algorithm are its communication cost and computational efficiency for a given desired accuracy in downstream applications . we give new algorithms and analyses for distributed pca which lead to improved communication and computational costs for $ k $ -means clustering and related problems . our empirical study on real world data shows a speedup of orders of magnitude , preserving communication with only a negligible degradation in solution quality . some of these techniques we develop , such as a general transformation from a constant success probability subspace embedding to a high success probability subspace embedding with a dimension and sparsity independent of the success probability , may be of independent interest .", "topics": ["cluster analysis", "approximation algorithm"]}
{"title": "easy monotonic policy iteration", "abstract": "a key problem in reinforcement learning for control with general function approximators ( such as deep neural networks and other nonlinear functions ) is that , for many algorithms employed in practice , updates to the policy or $ q $ -function may fail to improve performance -- -or worse , actually cause the policy performance to degrade . prior work has addressed this for policy iteration by deriving tight policy improvement bounds ; by optimizing the lower bound on policy improvement , a better policy is guaranteed . however , existing approaches suffer from bounds that are hard to optimize in practice because they include sup norm terms which can not be efficiently estimated or differentiated . in this work , we derive a better policy improvement bound where the sup norm of the policy divergence has been replaced with an average divergence ; this leads to an algorithm , easy monotonic policy iteration , that generates sequences of policies with guaranteed non-decreasing returns and is easy to implement in a sample-based framework .", "topics": ["nonlinear system", "reinforcement learning"]}
{"title": "perception-based energy functions in seam-cutting", "abstract": "image stitching is challenging in consumer-level photography , due to alignment difficulties in unconstrained shooting environment . recent studies show that seam-cutting approaches can effectively relieve artifacts generated by local misalignment . normally , seam-cutting is described in terms of energy minimization , however , few of existing methods consider human perception in their energy functions , which sometimes causes that a seam with minimum energy is not most invisible in the overlapping region . in this paper , we propose a novel perception-based energy function in the seam-cutting framework , which considers the nonlinearity and the nonuniformity of human perception in energy minimization . our perception-based approach adopts a sigmoid metric to characterize the perception of color discrimination , and a saliency weight to simulate that human eyes incline to pay more attention to salient objects . in addition , our seam-cutting composition can be easily implemented into other stitching pipelines . experiments show that our method outperforms the seam-cutting method of the normal energy function , and a user study demonstrates that our composed results are more consistent with human perception .", "topics": ["mathematical optimization", "nonlinear system"]}
{"title": "multilingual part-of-speech tagging : two unsupervised approaches", "abstract": "we demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging . the central assumption of our work is that by combining cues from multiple languages , the structure of each becomes more apparent . we consider two ways of applying this intuition to the problem of unsupervised part-of-speech tagging : a model that directly merges tag structures for a pair of languages into a single sequence and a second model which instead incorporates multilingual context using latent variables . both approaches are formulated as hierarchical bayesian models , using markov chain monte carlo sampling techniques for inference . our results demonstrate that by incorporating multilingual evidence we can achieve impressive performance gains across a range of scenarios . we also found that performance improves steadily as the number of available languages increases .", "topics": ["sampling ( signal processing )", "unsupervised learning"]}
{"title": "demystifying resnet", "abstract": "the residual network ( resnet ) , proposed in he et al . ( 2015 ) , utilized shortcut connections to significantly reduce the difficulty of training , which resulted in great performance boosts in terms of both training and generalization error . it was empirically observed in he et al . ( 2015 ) that stacking more layers of residual blocks with shortcut 2 results in smaller training error , while it is not true for shortcut of length 1 or 3 . we provide a theoretical explanation for the uniqueness of shortcut 2 . we show that with or without nonlinearities , by adding shortcuts that have depth two , the condition number of the hessian of the loss function at the zero initial point is depth-invariant , which makes training very deep models no more difficult than shallow ones . shortcuts of higher depth result in an extremely flat ( high-order ) stationary point initially , from which the optimization algorithm is hard to escape . the shortcut 1 , however , is essentially equivalent to no shortcuts , which has a condition number exploding to infinity as the number of layers grows . we further argue that as the number of layers tends to infinity , it suffices to only look at the loss function at the zero initial point . extensive experiments are provided accompanying our theoretical results . we show that initializing the network to small weights with shortcut 2 achieves significantly better results than random gaussian ( xavier ) initialization , orthogonal initialization , and shortcuts of deeper depth , from various perspectives ranging from final loss , learning dynamics and stability , to the behavior of the hessian along the learning process .", "topics": ["loss function"]}
{"title": "ran4iqa : restorative adversarial nets for no-reference image quality assessment", "abstract": "inspired by the free-energy brain theory , which implies that human visual system ( hvs ) tends to reduce uncertainty and restore perceptual details upon seeing a distorted image , we propose restorative adversarial net ( ran ) , a gan-based model for no-reference image quality assessment ( nr-iqa ) . ran , which mimics the process of hvs , consists of three components : a restorator , a discriminator and an evaluator . the restorator restores and reconstructs input distorted image patches , while the discriminator distinguishes the reconstructed patches from the pristine distortion-free patches . after restoration , we observe that the perceptual distance between the restored and the distorted patches is monotonic with respect to the distortion level . we further define gain of restoration ( gor ) based on this phenomenon . the evaluator predicts perceptual score by extracting feature representations from the distorted and restored patches to measure gor . eventually , the quality score of an input image is estimated by weighted sum of the patch scores . experimental results on waterloo exploration , live and tid2013 show the effectiveness and generalization ability of ran compared to the state-of-the-art nr-iqa models .", "topics": ["noise reduction"]}
{"title": "weakly- and semi-supervised object detection with expectation-maximization algorithm", "abstract": "object detection when provided image-level labels instead of instance-level labels ( i.e . , bounding boxes ) during training is an important problem in computer vision , since large scale image datasets with instance-level labels are extremely costly to obtain . in this paper , we address this challenging problem by developing an expectation-maximization ( em ) based object detection method using deep convolutional neural networks ( cnns ) . our method is applicable to both the weakly-supervised and semi-supervised settings . extensive experiments on pascal voc 2007 benchmark show that ( 1 ) in the weakly supervised setting , our method provides significant detection performance improvement over current state-of-the-art methods , ( 2 ) having access to a small number of strongly ( instance-level ) annotated images , our method can almost match the performace of the fully supervised fast rcnn . we share our source code at https : //github.com/ziangyan/em-wsd .", "topics": ["object detection", "computer vision"]}
{"title": "srl4orl : improving opinion role labelling using multi-task learning with semantic role labeling", "abstract": "for over 12 years , machine learning is used to extract opinion-holder-target structures from text to answer the question : who expressed what kind of sentiment towards what ? . however , recent neural approaches do not outperform the state-of-the-art feature-based model for opinion role labelling ( orl ) . we suspect this is due to the scarcity of labelled training data and address this issue using different multi-task learning techniques with a related task which has substantially more data , i.e . semantic role labelling ( srl ) . despite difficulties of the benchmark mpqa corpus , we show that indeed the orl model benefits from srl knowledge .", "topics": ["test set"]}
{"title": "a market-oriented programming environment and its application to distributed multicommodity flow problems", "abstract": "market price systems constitute a well-understood class of mechanisms that under certain conditions provide effective decentralization of decision making with minimal communication overhead . in a market-oriented programming approach to distributed problem solving , we derive the activities and resource allocations for a set of computational agents by computing the competitive equilibrium of an artificial economy . walras provides basic constructs for defining computational market structures , and protocols for deriving their corresponding price equilibria . in a particular realization of this approach for a form of multicommodity flow problem , we see that careful construction of the decision process according to economic principles can lead to efficient distributed resource allocation , and that the behavior of the system can be meaningfully analyzed in economic terms .", "topics": ["interaction"]}
{"title": "rethinking collapsed variational bayes inference for lda", "abstract": "we propose a novel interpretation of the collapsed variational bayes inference with a zero-order taylor expansion approximation , called cvb0 inference , for latent dirichlet allocation ( lda ) . we clarify the properties of the cvb0 inference by using the alpha-divergence . we show that the cvb0 inference is composed of two different divergence projections : alpha=1 and -1 . this interpretation will help shed light on cvb0 works .", "topics": ["calculus of variations"]}
{"title": "reactive reasoning with the event calculus", "abstract": "systems for symbolic event recognition accept as input a stream of time-stamped events from sensors and other computational devices , and seek to identify high-level composite events , collections of events that satisfy some pattern . rtec is an event calculus dialect with novel implementation and 'windowing ' techniques that allow for efficient event recognition , scalable to large data streams . rtec can deal with applications where event data arrive with a ( variable ) delay from , and are revised by , the underlying sources . rtec can update already recognised events and recognise new events when data arrive with a delay or following data revision . our evaluation shows that rtec can support real-time event recognition and is capable of meeting the performance requirements identified in a recent survey of event processing use cases .", "topics": ["high- and low-level", "scalability"]}
{"title": "refining geometry from depth sensors using ir shading images", "abstract": "we propose a method to refine geometry of 3d meshes from a consumer level depth camera , e.g . kinect , by exploiting shading cues captured from an infrared ( ir ) camera . a major benefit to using an ir camera instead of an rgb camera is that the ir images captured are narrow band images that filter out most undesired ambient light , which makes our system robust against natural indoor illumination . moreover , for many natural objects with colorful textures in the visible spectrum , the subjects appear to have a uniform albedo in the ir spectrum . based on our analyses on the ir projector light of the kinect , we define a near light source ir shading model that describes the captured intensity as a function of surface normals , albedo , lighting direction , and distance between light source and surface points . to resolve the ambiguity in our model between the normals and distances , we utilize an initial 3d mesh from the kinect fusion and multi-view information to reliably estimate surface details that were not captured and reconstructed by the kinect fusion . our approach directly operates on the mesh model for geometry refinement . we ran experiments on our algorithm for geometries captured by both the kinect i and kinect ii , as the depth acquisition in kinect i is based on a structured-light technique and that of the kinect ii is based on a time-of-flight ( tof ) technology . the effectiveness of our approach is demonstrated through several challenging real-world examples . we have also performed a user study to evaluate the quality of the mesh models before and after our refinements .", "topics": ["sensor"]}
{"title": "the study of cuckoo optimization algorithm for production planning problem", "abstract": "constrained nonlinear programming problems are hard problems , and one of the most widely used and common problems for production planning problem to optimize . in this study , one of the mathematical models of production planning is survey and the problem solved by cuckoo algorithm . cuckoo algorithm is efficient method to solve continues non linear problem . moreover , mentioned models of production planning solved with genetic algorithm and lingo software and the results will compared . the cuckoo algorithm is suitable choice for optimization in convergence of solution", "topics": ["mathematical optimization"]}
{"title": "aggregating deep convolutional features for image retrieval", "abstract": "several recent works have shown that image descriptors produced by deep convolutional neural networks provide state-of-the-art performance for image classification and retrieval problems . it has also been shown that the activations from the convolutional layers can be interpreted as local features describing particular image regions . these local features can be aggregated using aggregation approaches developed for local features ( e.g . fisher vectors ) , thus providing new powerful global descriptors . in this paper we investigate possible ways to aggregate local deep features to produce compact global descriptors for image retrieval . first , we show that deep features and traditional hand-engineered features have quite different distributions of pairwise similarities , hence existing aggregation methods have to be carefully re-evaluated . such re-evaluation reveals that in contrast to shallow features , the simple aggregation method based on sum pooling provides arguably the best performance for deep convolutional features . this method is efficient , has few parameters , and bears little risk of overfitting when e.g . learning the pca matrix . overall , the new compact global descriptor improves the state-of-the-art on four common benchmarks considerably .", "topics": ["computer vision"]}
{"title": "residual belief propagation for topic modeling", "abstract": "fast convergence speed is a desired property for training latent dirichlet allocation ( lda ) , especially in online and parallel topic modeling for massive data sets . this paper presents a novel residual belief propagation ( rbp ) algorithm to accelerate the convergence speed for training lda . the proposed rbp uses an informed scheduling scheme for asynchronous message passing , which passes fast-convergent messages with a higher priority to influence those slow-convergent messages at each learning iteration . extensive empirical studies confirm that rbp significantly reduces the training time until convergence while achieves a much lower predictive perplexity than other state-of-the-art training algorithms for lda , including variational bayes ( vb ) , collapsed gibbs sampling ( gs ) , loopy belief propagation ( bp ) , and residual vb ( rvb ) .", "topics": ["calculus of variations", "iteration"]}
{"title": "accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "abstract": "we introduce a proximal version of the stochastic dual coordinate ascent method and show how to accelerate the method using an inner-outer iteration procedure . we analyze the runtime of the framework and obtain rates that improve state-of-the-art results for various key machine learning optimization problems including svm , logistic regression , ridge regression , lasso , and multiclass svm . experiments validate our theoretical findings .", "topics": ["support vector machine", "iteration"]}
{"title": "regularization for multiple kernel learning via sum-product networks", "abstract": "in this paper , we are interested in constructing general graph-based regularizers for multiple kernel learning ( mkl ) given a structure which is used to describe the way of combining basis kernels . such structures are represented by sum-product networks ( spns ) in our method . accordingly we propose a new convex regularization method for mlk based on a path-dependent kernel weighting function which encodes the entire spn structure in our method . under certain conditions and from the view of probability , this function can be considered to follow multinomial distributions over the weights associated with product nodes in spns . we also analyze the convexity of our regularizer and the complexity of our induced classifiers , and further propose an efficient wrapper algorithm to optimize our formulation . in our experiments , we apply our method to ... ...", "topics": ["kernel ( operating system )", "matrix regularization"]}
{"title": "abductive equivalential translation and its application to natural language database interfacing", "abstract": "the thesis describes a logical formalization of natural-language database interfacing . we assume the existence of a `` natural language engine '' capable of mediating between surface linguistic string and their representations as `` literal '' logical forms : the focus of interest will be the question of relating `` literal '' logical forms to representations in terms of primitives meaningful to the underlying database engine . we begin by describing the nature of the problem , and show how a variety of interface functionalities can be considered as instances of a type of formal inference task which we call `` abductive equivalential translation '' ( aet ) ; functionalities which can be reduced to this form include answering questions , responding to commands , reasoning about the completeness of answers , answering meta-questions of type `` do you know ... '' , and generating assertions and questions . in each case , a `` linguistic domain theory '' ( ldt ) $ \\gamma $ and an input formula $ f $ are given , and the goal is to construct a formula with certain properties which is equivalent to $ f $ , given $ \\gamma $ and a set of permitted assumptions . if the ldt is of a certain specified type , whose formulas are either conditional equivalences or horn-clauses , we show that the aet problem can be reduced to a goal-directed inference method . we present an abstract description of this method , and sketch its realization in prolog . the relationship between aet and several problems previously discussed in the literature is discussed . in particular , we show how aet can provide a simple and elegant solution to the so-called `` doctor on board '' problem , and in effect allows a `` relativization '' of the closed world assumption . the ideas in the thesis have all been implemented concretely within the sri clare project , using a real projects and payments database . the ldt for the example database is described in detail , and examples of the types of functionality that can be achieved within the example domain are presented .", "topics": ["natural language", "database"]}
{"title": "online segment to segment neural transduction", "abstract": "we introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read . by independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginalization of the latent segmentation during training , and during decoding beam search is employed to find the best alignment path together with the predicted output sequence . our model tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their fixed-length hidden states before producing any output . it is different from previous attentive models in that , instead of treating the attention weights as output of a deterministic function , our model assigns attention weights to a sequential latent variable which can be marginalized out and permits online generation . experiments on abstractive sentence summarization and morphological inflection show significant performance gains over the baseline encoder-decoders .", "topics": ["baseline ( configuration management )", "encoder"]}
{"title": "a novel hybrid cnn-ais visual pattern recognition engine", "abstract": "machine learning methods are used today for most recognition problems . convolutional neural networks ( cnn ) have time and again proved successful for many image processing tasks primarily for their architecture . in this paper we propose to apply cnn to small data sets like for example , personal albums or other similar environs where the size of training dataset is a limitation , within the framework of a proposed hybrid cnn-ais model . we use artificial immune system principles to enhance small size of training data set . a layer of clonal selection is added to the local filtering and max pooling of cnn architecture . the proposed architecture is evaluated using the standard mnist dataset by limiting the data size and also with a small personal data sample belonging to two different classes . experimental results show that the proposed hybrid cnn-ais based recognition engine works well when the size of training data is limited in size", "topics": ["test set", "image processing"]}
{"title": "deep online convex optimization by putting forecaster to sleep", "abstract": "methods from convex optimization such as accelerated gradient descent are widely used as building blocks for deep learning algorithms . however , the reasons for their empirical success are unclear , since neural networks are not convex and standard guarantees do not apply . this paper develops the first rigorous link between online convex optimization and error backpropagation on convolutional networks . the first step is to introduce circadian games , a mild generalization of convex games with similar convergence properties . the main result is that error backpropagation on a convolutional network is equivalent to playing out a circadian game . it follows immediately that the waking-regret of players in the game ( the units in the neural network ) controls the overall rate of convergence of the network . finally , we explore some implications of the results : ( i ) we describe the representations learned by a neural network game-theoretically , ( ii ) propose a learning setting at the level of individual units that can be plugged into deep architectures , and ( iii ) propose a new approach to adaptive model selection by applying bandit algorithms to choose which players to wake on each round .", "topics": ["regret ( decision theory )", "gradient descent"]}
{"title": "a complete solver for constraint games", "abstract": "game theory studies situations in which multiple agents having conflicting objectives have to reach a collective decision . the question of a compact representation language for agents utility function is of crucial importance since the classical representation of a $ n $ -players game is given by a $ n $ -dimensional matrix of exponential size for each player . in this paper we use the framework of constraint games in which csp are used to represent utilities . constraint programming -- including global constraints -- allows to easily give a compact and elegant model to many useful games . constraint games come in two flavors : constraint satisfaction games and constraint optimization games , the first one using satisfaction to define boolean utilities . in addition to multimatrix games , it is also possible to model more complex games where hard constraints forbid certain situations . in this paper we study complete search techniques and show that our solver using the compact representation of constraint games is faster than the classical game solver gambit by one to two orders of magnitude .", "topics": ["time complexity"]}
{"title": "bayesian learning of clique tree structure", "abstract": "the problem of categorical data analysis in high dimensions is considered . a discussion of the fundamental difficulties of probability modeling is provided , and a solution to the derivation of high dimensional probability distributions based on bayesian learning of clique tree decomposition is presented . the main contributions of this paper are an automated determination of the optimal clique tree structure for probability modeling , the resulting derived probability distribution , and a corresponding unified approach to clustering and anomaly detection based on the probability distribution .", "topics": ["cluster analysis"]}
{"title": "small drone field experiment : data collection & processing", "abstract": "following an initiative formalized in april 2016 formally known as arl west between the u.s. army research laboratory ( arl ) and university of southern california 's institute for creative technologies ( usc ict ) , a field experiment was coordinated and executed in the summer of 2016 by arl , usc ict , and headwall photonics . the purpose was to image part of the usc main campus in los angeles , usa , using two portable cots ( commercial off the shelf ) aerial drone solutions for data acquisition , for photogrammetry ( 3d reconstruction from images ) , and fusion of hyperspectral data with the recovered set of 3d point clouds representing the target area . the research aims for determining the viability of having a machine capable of segmenting the target area into key material classes ( e.g . , manmade structures , live vegetation , water ) for use in multiple purposes , to include providing the user with a more accurate scene understanding and enabling the unsupervised automatic sampling of meaningful material classes from the target area for adaptive semi-supervised machine learning . in the latter , a target set library may be used for automatic machine training with data of local material classes , as an example , to increase the prediction chances of machines recognizing targets . the field experiment and associated data post processing approach to correct for reflectance , geo-rectify , recover the area 's dense point clouds from images , register spectral with elevation properties of scene surfaces from the independently collected datasets , and generate the desired scene segmented maps are discussed . lessons learned from the experience are also highlighted throughout the paper .", "topics": ["sampling ( signal processing )", "map"]}
{"title": "improvements and experiments of a compact statistical background model", "abstract": "change detection plays an important role in most video-based applications . the first stage is to build appropriate background model , which is now becoming increasingly complex as more sophisticated statistical approaches are introduced to cover challenging situations and provide reliable detection . this paper reports a simple and intuitive statistical model based on deeper learning spatial correlation among pixels : for each observed pixel , we select a group of supporting pixels with high correlation , and then use a single gaussian to model the intensity deviations between the observed pixel and the supporting ones . in addition , a multi-channel model updating is integrated on-line and a temporal intensity constraint for each pixel is defined . although this method is mainly designed for coping with sudden illumination changes , experimental results using all the video sequences provided on changedetection.net validate it is comparable with other recent methods under various situations .", "topics": ["pixel"]}
{"title": "kernels on sample sets via nonparametric divergence estimates", "abstract": "most machine learning algorithms , such as classification or regression , treat the individual data point as the object of interest . here we consider extending machine learning algorithms to operate on groups of data points . we suggest treating a group of data points as an i.i.d . sample set from an underlying feature distribution for that group . our approach employs kernel machines with a kernel on i.i.d . sample sets of vectors . we define certain kernel functions on pairs of distributions , and then use a nonparametric estimator to consistently estimate those functions based on sample sets . the projection of the estimated gram matrix to the cone of symmetric positive semi-definite matrices enables us to use kernel machines for classification , regression , anomaly detection , and low-dimensional embedding in the space of distributions . we present several numerical experiments both on real and simulated datasets to demonstrate the advantages of our new approach .", "topics": ["statistical classification", "numerical analysis"]}
{"title": "convergence rate of frank-wolfe for non-convex objectives", "abstract": "we give a simple proof that the frank-wolfe algorithm obtains a stationary point at a rate of $ o ( 1/\\sqrt { t } ) $ on non-convex objectives with a lipschitz continuous gradient . our analysis is affine invariant and is the first , to the best of our knowledge , giving a similar rate to what was already proven for projected gradient methods ( though on slightly different measures of stationarity ) .", "topics": ["gradient"]}
{"title": "non-stationary gaussian process regression with hamiltonian monte carlo", "abstract": "we present a novel approach for fully non-stationary gaussian process regression ( gpr ) , where all three key parameters -- noise variance , signal variance and lengthscale -- can be simultaneously input-dependent . we develop gradient-based inference methods to learn the unknown function and the non-stationary model parameters , without requiring any model approximations . we propose to infer full parameter posterior with hamiltonian monte carlo ( hmc ) , which conveniently extends the analytical gradient-based gpr learning by guiding the sampling with model gradients . we also learn the map solution from the posterior by gradient ascent . in experiments on several synthetic datasets and in modelling of temporal gene expression , the nonstationary gpr is shown to be necessary for modeling realistic input-dependent dynamics , while it performs comparably to conventional stationary or previous non-stationary gpr models otherwise .", "topics": ["sampling ( signal processing )", "synthetic data"]}
{"title": "memory networks", "abstract": "we describe a new class of learning models called memory networks . memory networks reason with inference components combined with a long-term memory component ; they learn how to use these jointly . the long-term memory can be read and written to , with the goal of using it for prediction . we investigate these models in the context of question answering ( qa ) where the long-term memory effectively acts as a ( dynamic ) knowledge base , and the output is a textual response . we evaluate them on a large-scale qa task , and a smaller , but more complex , toy task generated from a simulated world . in the latter , we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs .", "topics": ["simulation"]}
{"title": "hybrid data clustering approach using k-means and flower pollination algorithm", "abstract": "data clustering is a technique for clustering set of objects into known number of groups . several approaches are widely applied to data clustering so that objects within the clusters are similar and objects in different clusters are far away from each other . k-means , is one of the familiar center based clustering algorithms since implementation is very easy and fast convergence . however , k-means algorithm suffers from initialization , hence trapped in local optima . flower pollination algorithm ( fpa ) is the global optimization technique , which avoids trapping in local optimum solution . in this paper , a novel hybrid data clustering approach using flower pollination algorithm and k-means ( fpakm ) is proposed . the proposed algorithm results are compared with k-means and fpa on eight datasets . from the experimental results , fpakm is better than fpa and k-means .", "topics": ["cluster analysis"]}
{"title": "idk cascades : fast deep learning by learning not to overthink", "abstract": "advances in deep learning have led to substantial increases in prediction accuracy but have been accompanied by increases in the cost of rendering predictions . we conjecture that for a majority of real-world inputs , the recent advances in deep learning have created models that effectively `` over-think '' on simple inputs . in this paper we revisit the question of how to effectively build model cascades to reduce prediction costs . while classic cascade techniques primarily leverage class asymmetry to reduce cost , we extend this approach to arbitrary multi-class prediction tasks . we introduce the `` i do n't know '' ( idk ) prediction cascades framework , a general framework for composing a set of pre-trained models to accelerate inference without a loss in prediction accuracy . we propose two search based methods for constructing cascades as well as a new cost-aware objective within this framework . we evaluate these techniques on a range of both benchmark and real-world datasets and demonstrate that prediction cascades can reduce computation by 37 % , resulting in up to 1.6x speedups in image classification tasks over state-of-the-art models without a loss in accuracy . furthermore , on a driving motion prediction task evaluated on a large scale autonomous driving dataset , prediction cascades achieved 95 % accuracy when combined with human experts , while requiring human intervention on less than 30 % of the queries .", "topics": ["computer vision", "computation"]}
{"title": "modeling photographic composition via triangles", "abstract": "the capacity of automatically modeling photographic composition is valuable for many real-world machine vision applications such as digital photography , image retrieval , image understanding , and image aesthetics assessment . the triangle technique is among those indispensable composition methods on which professional photographers often rely . this paper proposes a system that can identify prominent triangle arrangements in two major categories of photographs : natural or urban scenes , and portraits . for the natural or urban scene pictures , the focus is on the effect of linear perspective . for portraits , we carefully examine the positioning of human subjects in a photo . we show that line analysis is highly advantageous for modeling composition in both categories . based on the detected triangles , new mathematical descriptors for composition are formulated and used to retrieve similar images . leveraging the rich source of high aesthetics photos online , similar approaches can potentially be incorporated in future smart cameras to enhance a person 's photo composition skills .", "topics": ["computer vision"]}
{"title": "bounding the test log-likelihood of generative models", "abstract": "several interesting generative learning algorithms involve a complex probability distribution over many random variables , involving intractable normalization constants or latent variable normalization . some of them may even not have an analytic expression for the unnormalized probability function and no tractable approximation . this makes it difficult to estimate the quality of these models , once they have been trained , or to monitor their quality ( e.g . for early stopping ) while training . a previously proposed method is based on constructing a non-parametric density estimator of the model 's probability function from samples generated by the model . we revisit this idea , propose a more efficient estimator , and prove that it provides a lower bound on the true test log-likelihood , and an unbiased estimator as the number of generated samples goes to infinity , although one that incorporates the effect of poor mixing . we further propose a biased variant of the estimator that can be used reliably with a finite number of samples for the purpose of model comparison .", "topics": ["sampling ( signal processing )", "generative model"]}
{"title": "rule generalisation in intrusion detection systems using snort", "abstract": "intrusion detection systems ( ids ) provide an important layer of security for computer systems and networks , and are becoming more and more necessary as reliance on internet services increases and systems with sensitive data are more commonly open to internet access . an ids responsibility is to detect suspicious or unacceptable system and network activity and to alert a systems administrator to this activity . the majority of ids use a set of signatures that define what suspicious traffic is , and snort is one popular and actively developing open-source ids that uses such a set of signatures known as snort rules . our aim is to identify a way in which snort could be developed further by generalising rules to identify novel attacks . in particular , we attempted to relax and vary the conditions and parameters of current snort rules , using a similar approach to classic rule learning operators such as generalisation and specialisation . we demonstrate the effectiveness of our approach through experiments with standard datasets and show that we are able to detect previously undeleted variants of various attacks . we conclude by discussing the general effectiveness and appropriateness of generalisation in snort based ids rule processing .", "topics": ["mathematical optimization", "heuristic"]}
{"title": "amortised map inference for image super-resolution", "abstract": "image super-resolution ( sr ) is an underdetermined inverse problem , where a large number of plausible high-resolution images can explain the same downsampled image . most current single image sr methods use empirical risk minimisation , often with a pixel-wise mean squared error ( mse ) loss . however , the outputs from such methods tend to be blurry , over-smoothed and generally appear implausible . a more desirable approach would employ maximum a posteriori ( map ) inference , preferring solutions that always have a high probability under the image prior , and thus appear more plausible . direct map estimation for sr is non-trivial , as it requires us to build a model for the image prior from samples . furthermore , map inference is often performed via optimisation-based iterative algorithms which do n't compare well with the efficiency of neural-network-based alternatives . here we introduce new methods for amortised map inference whereby we calculate the map estimate directly using a convolutional neural network . we first introduce a novel neural network architecture that performs a projection to the affine subspace of valid sr solutions ensuring that the high resolution output of the network is always consistent with the low resolution input . we show that , using this architecture , the amortised map inference problem reduces to minimising the cross-entropy between two distributions , similar to training generative models . we propose three methods to solve this optimisation problem : ( 1 ) generative adversarial networks ( gan ) ( 2 ) denoiser-guided sr which backpropagates gradient-estimates from denoising to train the network , and ( 3 ) a baseline method using a maximum-likelihood-trained image prior . our experiments show that the gan based approach performs best on real image data . lastly , we establish a connection between gans and amortised variational inference as in e.g . variational autoencoders .", "topics": ["baseline ( configuration management )", "generative model"]}
{"title": "multivariate industrial time series with cyber-attack simulation : fault detection using an lstm-based predictive data model", "abstract": "we adopted an approach based on an lstm neural network to monitor and detect faults in industrial multivariate time series data . to validate the approach we created a modelica model of part of a real gasoil plant . by introducing hacks into the logic of the modelica model , we were able to generate both the roots and causes of fault behavior in the plant . having a self-consistent data set with labeled faults , we used an lstm architecture with a forecasting error threshold to obtain precision and recall quality metrics . the dependency of the quality metric on the threshold level is considered . an appropriate mechanism such as `` one handle '' was introduced for filtering faults that are outside of the plant operator field of interest .", "topics": ["time series", "simulation"]}
{"title": "fraud/uncollectible debt detection using a bayesian network based learning system : a rare binary outcome with mixed data structures", "abstract": "the fraud/uncollectible debt problem in the telecommunications industry presents two technical challenges : the detection and the treatment of the account given the detection . in this paper , we focus on the first problem of detection using bayesian network models , and we briefly discuss the application of a normative expert system for the treatment at the end . we apply bayesian network models to the problem of fraud/uncollectible debt detection for telecommunication services . in addition to being quite successful at predicting rare event outcomes , it is able to handle a mixture of categorical and continuous data . we present a performance comparison using linear and non-linear discriminant analysis , classification and regression trees , and bayesian network models", "topics": ["statistical classification", "bayesian network"]}
{"title": "a multiple instance learning approach for sequence data with across bag dependencies", "abstract": "in multiple instance learning ( mil ) problem for sequence data , the learning data consist of a set of bags where each bag contains a set of instances/sequences . in many real world applications such as bioinformatics , web mining , and text mining , comparing a random couple of sequences makes no sense . in fact , each instance of each bag may have structural and/or temporal relation with other instances in other bags . thus , the classification task should take into account the relation between semantically related instances across bags . in this paper , we present two novel mil approaches for sequence data classification : ( 1 ) abclass and ( 2 ) absim . in abclass , each sequence is represented by one vector of attributes . for each sequence of the unknown bag , a discriminative classifier is applied in order to compute a partial classification result . then , an aggregation method is applied to these partial results in order to generate the final result . in absim , we use a similarity measure between each sequence of the unknown bag and the corresponding sequences in the learning bags . an unknown bag is labeled with the bag that presents more similar sequences . we applied both approaches to the problem of bacterial ionizing radiation resistance ( irr ) prediction . we evaluated and discussed the proposed approaches on well known ionizing radiation resistance bacteria ( irrb ) and ionizing radiation sensitive bacteria ( irsb ) represented by primary structure of basal dna repair proteins . the experimental results show that both abclass and absim approaches are efficient .", "topics": ["statistical classification"]}
{"title": "deep convolutional neural networks for pairwise causality", "abstract": "discovering causal models from observational and interventional data is an important first step preceding what-if analysis or counterfactual reasoning . as has been shown before , the direction of pairwise causal relations can , under certain conditions , be inferred from observational data via standard gradient-boosted classifiers ( gbc ) using carefully engineered statistical features . in this paper we apply deep convolutional neural networks ( cnns ) to this problem by plotting attribute pairs as 2-d scatter plots that are fed to the cnn as images . we evaluate our approach on the 'cause- effect pairs ' nips 2013 data challenge . we observe that a weighted ensemble of cnn with the earlier gbc approach yields significant improvement . further , we observe that when less training data is available , our approach performs better than the gbc based approach suggesting that cnn models pre-trained to determine the direction of pairwise causal direction could have wider applicability in causal discovery and enabling what-if or counterfactual analysis .", "topics": ["test set", "gradient descent"]}
{"title": "modelling contextuality by probabilistic programs with hypergraph semantics", "abstract": "models of a phenomenon are often developed by examining it under different experimental conditions , or measurement contexts . the resultant probabilistic models assume that the underlying random variables , which define a measurable set of outcomes , can be defined independent of the measurement context . the phenomenon is deemed contextual when this assumption fails . contextuality is an important issue in quantum physics . however , there has been growing speculation that it manifests outside the quantum realm with human cognition being a particularly prominent area of investigation . this article contributes the foundations of a probabilistic programming language that allows convenient exploration of contextuality in wide range of applications relevant to cognitive science and artificial intelligence . specific syntax is proposed to allow the specification of `` measurement contexts '' . each such context delivers a partial model of the phenomenon based on the associated experimental condition described by the measurement context . the probabilistic program is translated into a hypergraph in a modular way . recent theoretical results from the field of quantum physics show that contextuality can be equated with the possibility of constructing a probabilistic model on the resulting hypergraph . the use of hypergraphs opens the door for a theoretically succinct and efficient computational semantics sensitive to modelling both contextual and non-contextual phenomena . finally , this article raises awareness of contextuality beyond quantum physics and to contribute formal methods to detect its presence by means of hypergraph semantics .", "topics": ["artificial intelligence"]}
{"title": "efficient globally optimal 2d-to-3d deformable shape matching", "abstract": "we propose the first algorithm for non-rigid 2d-to-3d shape matching , where the input is a 2d shape represented as a planar curve and a 3d shape represented as a surface ; the output is a continuous curve on the surface . we cast the problem as finding the shortest circular path on the prod- uct 3-manifold of the surface and the curve . we prove that the optimal matching can be computed in polynomial time with a ( worst-case ) complexity of $ o ( mn^2\\log ( n ) ) $ , where $ m $ and $ n $ denote the number of vertices on the template curve and the 3d shape respectively . we also demonstrate that in practice the runtime is essentially linear in $ m\\ ! \\cdot\\ ! n $ making it an efficient method for shape analysis and shape retrieval . quantitative evaluation confirms that the method provides excellent results for sketch-based deformable 3d shape re- trieval .", "topics": ["time complexity", "polynomial"]}
{"title": "predictive user modeling with actionable attributes", "abstract": "different machine learning techniques have been proposed and used for modeling individual and group user needs , interests and preferences . in the traditional predictive modeling instances are described by observable variables , called attributes . the goal is to learn a model for predicting the target variable for unseen instances . for example , for marketing purposes a company consider profiling a new user based on her observed web browsing behavior , referral keywords or other relevant information . in many real world applications the values of some attributes are not only observable , but can be actively decided by a decision maker . furthermore , in some of such applications the decision maker is interested not only to generate accurate predictions , but to maximize the probability of the desired outcome . for example , a direct marketing manager can choose which type of a special offer to send to a client ( actionable attribute ) , hoping that the right choice will result in a positive response with a higher probability . we study how to learn to choose the value of an actionable attribute in order to maximize the probability of a desired outcome in predictive modeling . we emphasize that not all instances are equally sensitive to changes in actions . accurate choice of an action is critical for those instances , which are on the borderline ( e.g . users who do not have a strong opinion one way or the other ) . we formulate three supervised learning approaches for learning to select the value of an actionable attribute at an instance level . we also introduce a focused training procedure which puts more emphasis on the situations where varying the action is the most likely to take the effect . the proof of concept experimental validation on two real-world case studies in web analytics and e-learning domains highlights the potential of the proposed approaches .", "topics": ["value ( ethics )", "supervised learning"]}
{"title": "recurrent relational networks for complex relational reasoning", "abstract": "humans possess an ability to abstractly reason about objects and their interactions , an ability not shared with state-of-the-art deep learning models . relational networks , introduced by santoro et al . ( 2017 ) , add the capacity for relational reasoning to deep neural networks , but are limited in the complexity of the reasoning tasks they can address . we introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning . we use recurrent relational networks to solve sudoku puzzles and achieve state-of-the-art results by solving 96.6 % of the hardest sudoku puzzles , where relational networks fail to solve any . we also apply our model to the babi textual qa dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers . the recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning .", "topics": ["interaction", "sparse matrix"]}
{"title": "skill analysis with time series image data", "abstract": "we present a skill analysis with time series image data using data mining methods , focused on table tennis . we do not use body model , but use only hi-speed movies , from which time series data are obtained and analyzed using data mining methods such as c4.5 and so on . we identify internal models for technical skills as evaluation skillfulness for the forehand stroke of table tennis , and discuss mono and meta-functional skills for improving skills .", "topics": ["data mining", "time series"]}
{"title": "intelligent personal assistant with knowledge navigation", "abstract": "an intelligent personal agent ( ipa ) is an agent that has the purpose of helping the user to gain information through reliable resources with the help of knowledge navigation techniques and saving time to search the best content . the agent is also responsible for responding to the chat-based queries with the help of conversation corpus . we will be testing different methods for optimal query generation . to felicitate the ease of usage of the application , the agent will be able to accept the input through text ( keyboard ) , voice ( speech recognition ) and server ( facebook ) and output responses using the same method . existing chat bots reply by making changes in the input , but we will give responses based on multiple srt files . the model will learn using the human dialogs dataset and will be able respond human-like . responses to queries about famous things ( places , people , and words ) can be provided using web scraping which will enable the bot to have knowledge navigation features . the agent will even learn from its past experiences supporting semi-supervised learning .", "topics": ["supervised learning", "speech recognition"]}
{"title": "modifying bayesian networks by probability constraints", "abstract": "this paper deals with the following problem : modify a bayesian network to satisfy a given set of probability constraints by only change its conditional probability tables , and the probability distribution of the resulting network should be as close as possible to that of the original network . we propose to solve this problem by extending ipfp ( iterative proportional fitting procedure ) to probability distributions represented by bayesian networks . the resulting algorithm e-ipfp is further developed to d-ipfp , which reduces the computational cost by decomposing a global eipfp into a set of smaller local e-ipfp problems . limited analysis is provided , including the convergence proofs of the two algorithms . computer experiments were conducted to validate the algorithms . the results are consistent with the theoretical analysis .", "topics": ["bayesian network"]}
{"title": "synthesizing normalized faces from facial identity features", "abstract": "we present a method for synthesizing a frontal , neutral-expression image of a person 's face given an input face photograph . this is achieved by learning to generate facial landmarks and textures from features extracted from a facial-recognition network . unlike previous approaches , our encoding feature vector is largely invariant to lighting , pose , and facial expression . exploiting this invariance , we train our decoder network using only frontal , neutral-expression photographs . since these photographs are well aligned , we can decompose them into a sparse set of landmark points and aligned texture maps . the decoder then predicts landmarks and textures independently and combines them using a differentiable image warping operation . the resulting images can be used for a number of applications , such as analyzing facial attributes , exposure and white balance adjustment , or creating a 3-d avatar .", "topics": ["feature vector", "map"]}
{"title": "learning tensors in reproducing kernel hilbert spaces with multilinear spectral penalties", "abstract": "we present a general framework to learn functions in tensor product reproducing kernel hilbert spaces ( tp-rkhss ) . the methodology is based on a novel representer theorem suitable for existing as well as new spectral penalties for tensors . when the functions in the tp-rkhs are defined on the cartesian product of finite discrete sets , in particular , our main problem formulation admits as a special case existing tensor completion problems . other special cases include transfer learning with multimodal side information and multilinear multitask learning . for the latter case , our kernel-based view is instrumental to derive nonlinear extensions of existing model classes . we give a novel algorithm and show in experiments the usefulness of the proposed extensions .", "topics": ["nonlinear system", "reinforcement learning"]}
{"title": "recurrent fully convolutional neural networks for multi-slice mri cardiac segmentation", "abstract": "in cardiac magnetic resonance imaging , fully-automatic segmentation of the heart enables precise structural and functional measurements to be taken , e.g . from short-axis mr images of the left-ventricle . in this work we propose a recurrent fully-convolutional network ( rfcn ) that learns image representations from the full stack of 2d slices and has the ability to leverage inter-slice spatial dependences through internal memory units . rfcn combines anatomical detection and segmentation into a single architecture that is trained end-to-end thus significantly reducing computational time , simplifying the segmentation pipeline , and potentially enabling real-time applications . we report on an investigation of rfcn using two datasets , including the publicly available miccai 2009 challenge dataset . comparisons have been carried out between fully convolutional networks and deep restricted boltzmann machines , including a recurrent version that leverages inter-slice spatial correlation . our studies suggest that rfcn produces state-of-the-art results and can substantially improve the delineation of contours near the apex of the heart .", "topics": ["time complexity", "end-to-end principle"]}
{"title": "verifying controllers against adversarial examples with bayesian optimization", "abstract": "recent successes in reinforcement learning have lead to the development of complex controllers for real-world robots . as these robots are deployed in safety-critical applications and interact with humans , it becomes critical to ensure safety in order to avoid causing harm . a first step in this direction is to test the controllers in simulation . to be able to do this , we need to capture what we mean by safety and then efficiently search the space of all behaviors to see if they are safe . in this paper , we present an active-testing framework based on bayesian optimization . we specify safety constraints using logic and exploit structure in the problem in order to test the system for adversarial counter examples that violate the safety specifications . these specifications are defined as complex boolean combinations of smooth functions on the trajectories and , unlike reward functions in reinforcement learning , are expressive and impose hard constraints on the system . in our framework , we exploit regularity assumptions on individual functions in form of a gaussian process ( gp ) prior . we combine these into a coherent optimization framework using problem structure . the resulting algorithm is able to provably verify complex safety specifications or alternatively find counter examples . experimental results show that the proposed method is able to find adversarial examples quickly .", "topics": ["reinforcement learning", "simulation"]}
{"title": "idel : in-database entity linking with neural embeddings", "abstract": "we present a novel architecture , in-database entity linking ( idel ) , in which we integrate the analytics-optimized rdbms monetdb with neural text mining abilities . our system design abstracts core tasks of most neural entity linking systems for monetdb . to the best of our knowledge , this is the first defacto implemented system integrating entity-linking in a database . we leverage the ability of monetdb to support in-database-analytics with user defined functions ( udfs ) implemented in python . these functions call machine learning libraries for neural text mining , such as tensorflow . the system achieves zero cost for data shipping and transformation by utilizing monetdb 's ability to embed python processes in the database kernel and exchange data in numpy arrays . idel represents text and relational data in a joint vector space with neural embeddings and can compensate errors with ambiguous entity representations . for detecting matching entities , we propose a novel similarity function based on joint neural embeddings which are learned via minimizing pairwise contrastive ranking loss . this function utilizes a high dimensional index structures for fast retrieval of matching entities . our first implementation and experiments using the webnlg corpus show the effectiveness and the potentials of idel .", "topics": ["database"]}
{"title": "spatio-temporal deep de-aliasing for prospective assessment of real-time ventricular volumes", "abstract": "purpose : real-time assessment of ventricular volumes requires high acceleration factors . residual convolutional neural networks ( cnn ) have shown potential for removing artifacts caused by data undersampling . in this study we investigated the effect of different radial sampling patterns on the accuracy of a cnn . we also acquired actual real-time undersampled radial data in patients with congenital heart disease ( chd ) , and compare cnn reconstruction to compressed sensing ( cs ) . methods : a 3d ( 2d plus time ) cnn architecture was developed , and trained using 2276 gold-standard paired 3d data sets , with 14x radial undersampling . four sampling schemes were tested , using 169 previously unseen 3d 'synthetic ' test data sets . actual real-time tiny golden angle ( tga ) radial ssfp data was acquired in 10 new patients ( 122 3d data sets ) , and reconstructed using the 3d cnn as well as a cs algorithm ; grasp . results : sampling pattern was shown to be important for image quality , and accurate visualisation of cardiac structures . for actual real-time data , overall reconstruction time with cnn ( including creation of aliased images ) was shown to be more than 5x faster than grasp . additionally , cnn image quality and accuracy of biventricular volumes was observed to be superior to grasp for the same raw data . conclusion : this paper has demonstrated the potential for the use of a 3d cnn for deep de-aliasing of real-time radial data , within the clinical setting . clinical measures of ventricular volumes using real-time data with cnn reconstruction are not statistically significantly different from the gold-standard , cardiac gated , bh techniques .", "topics": ["sampling ( signal processing )"]}
{"title": "improving accuracy and scalability of the pc algorithm by maximizing p-value", "abstract": "a number of attempts have been made to improve accuracy and/or scalability of the pc ( peter and clark ) algorithm , some well known ( buhlmann , et al . , 2010 ; kalisch and buhlmann , 2007 ; 2008 ; zhang , 2012 , to give some examples ) . we add here one more tool to the toolbox : the simple observation that if one is forced to choose between a variety of possible conditioning sets for a pair of variables , one should choose the one with the highest p-value . one can use the cpc ( conservative pc , ramsey et al . , 2012 ) algorithm as a guide to possible sepsets for a pair of variables . however , whereas cpc uses a voting rule to classify colliders versus noncolliders , our proposed algorithm , pc-max , picks the conditioning set with the highest p-value , so that there are no ambiguities . we combine this with two other optimizations : ( a ) avoiding bidirected edges in the orientation of colliders , and ( b ) parallelization . for ( b ) we borrow ideas from the pc-stable algorithm ( colombo and maathuis , 2014 ) . the result is an algorithm that scales quite well both in terms of accuracy and time , with no risk of bidirected edges .", "topics": ["scalability"]}
{"title": "nested invariance pooling and rbm hashing for image instance retrieval", "abstract": "the goal of this work is the computation of very compact binary hashes for image instance retrieval . our approach has two novel contributions . the first one is nested invariance pooling ( nip ) , a method inspired from i-theory , a mathematical theory for computing group invariant transformations with feed-forward neural networks . nip is able to produce compact and well-performing descriptors with visual representations extracted from convolutional neural networks . we specifically incorporate scale , translation and rotation invariances but the scheme can be extended to any arbitrary sets of transformations . we also show that using moments of increasing order throughout nesting is important . the nip descriptors are then hashed to the target code size ( 32-256 bits ) with a restricted boltzmann machine with a novel batch-level regularization scheme specifically designed for the purpose of hashing ( rbmh ) . a thorough empirical evaluation with state-of-the-art shows that the results obtained both with the nip descriptors and the nip+rbmh hashes are consistently outstanding across a wide range of datasets .", "topics": ["matrix regularization", "computation"]}
{"title": "reconstructive sparse code transfer for contour detection and semantic labeling", "abstract": "we frame the task of predicting a semantic labeling as a sparse reconstruction procedure that applies a target-specific learned transfer function to a generic deep sparse code representation of an image . this strategy partitions training into two distinct stages . first , in an unsupervised manner , we learn a set of generic dictionaries optimized for sparse coding of image patches . we train a multilayer representation via recursive sparse dictionary learning on pooled codes output by earlier layers . second , we encode all training images with the generic dictionaries and learn a transfer function that optimizes reconstruction of patches extracted from annotated ground-truth given the sparse codes of their corresponding image patches . at test time , we encode a novel image using the generic dictionaries and then reconstruct using the transfer function . the output reconstruction is a semantic labeling of the test image . applying this strategy to the task of contour detection , we demonstrate performance competitive with state-of-the-art systems . unlike almost all prior work , our approach obviates the need for any form of hand-designed features or filters . to illustrate general applicability , we also show initial results on semantic part labeling of human faces . the effectiveness of our approach opens new avenues for research on deep sparse representations . our classifiers utilize this representation in a novel manner . rather than acting on nodes in the deepest layer , they attach to nodes along a slice through multiple layers of the network in order to make predictions about local patches . our flexible combination of a generatively learned sparse representation with discriminatively trained transfer classifiers extends the notion of sparse reconstruction to encompass arbitrary semantic labeling tasks .", "topics": ["unsupervised learning", "sparse matrix"]}
{"title": "statistically motivated second order pooling", "abstract": "second-order pooling , a.k.a . bilinear pooling , has proven effective for visual recognition . the recent progress in this area has focused on either designing normalization techniques for second-order models , or compressing the second-order representations . however , these two directions have typically been followed separately , and without any clear statistical motivation . here , by contrast , we introduce a statistically-motivated framework that jointly tackles normalization and compression of second-order representations . to this end , we design a parametric vectorization layer , which maps a covariance matrix , known to follow a wishart distribution , to a vector whose elements can be shown to follow a chi-square distribution . we then propose to make use of a square-root normalization , which makes the distribution of the resulting representation converge to a gaussian , thus complying with the standard machine learning assumption . as evidenced by our experiments , this lets us outperform the state-of-the-art second-order models on several benchmark recognition datasets .", "topics": ["map"]}
{"title": "( blue ) taxi destination and trip time prediction from partial trajectories", "abstract": "real-time estimation of destination and travel time for taxis is of great importance for existing electronic dispatch systems . we present an approach based on trip matching and ensemble learning , in which we leverage the patterns observed in a dataset of roughly 1.7 million taxi journeys to predict the corresponding final destination and travel time for ongoing taxi trips , as a solution for the ecml/pkdd discovery challenge 2015 competition . the results of our empirical evaluation show that our approach is effective and very robust , which led our team -- bluetaxi -- to the 3rd and 7th position of the final rankings for the trip time and destination prediction tasks , respectively . given the fact that the final rankings were computed using a very small test set ( with only 320 trips ) we believe that our approach is one of the most robust solutions for the challenge based on the consistency of our good results across the test sets .", "topics": ["test set"]}
{"title": "towards predicting first daily departure times : a gaussian modeling approach for load shift forecasting", "abstract": "this work provides two statistical gaussian forecasting methods for predicting first daily departure times ( fddts ) of everyday use electric vehicles . this is important in smart grid applications to understand disconnection times of such mobile storage units , for instance to forecast storage of non dispatchable loads ( e.g . wind and solar power ) . we provide a review of the relevant state-of-the-art driving behavior features towards fddt prediction , to then propose an approximated gaussian method which qualitatively forecasts how many vehicles will depart within a given time frame , by assuming that departure times follow a normal distribution . this method considers sampling sessions as poisson distributions which are superimposed to obtain a single approximated gaussian model . given the gaussian distribution assumption of the departure times , we also model the problem with gaussian mixture models ( gmm ) , in which the priorly set number of clusters represents the desired time granularity . evaluation has proven that for the dataset tested , low error and high confidence ( $ \\approx 95\\ % $ ) is possible for 15 and 10 minute intervals , and that gmm outperforms traditional modeling but is less generalizable across datasets , as it is a closer fit to the sampling data . conclusively we discuss future possibilities and practical applications of the discussed model .", "topics": ["sampling ( signal processing )"]}
{"title": "a framework for on-line devanagari handwritten character recognition", "abstract": "the main challenge in on-line handwritten character recognition in indian lan- guage is the large size of the character set , larger similarity between different characters in the script and the huge variation in writing style . in this paper we propose a framework for on-line handwitten script recognition taking cues from speech signal processing literature . the framework is based on identify- ing strokes , which in turn lead to recognition of handwritten on-line characters rather that the conventional character identification . though the framework is described for devanagari script , the framework is general and can be applied to any language . the proposed platform consists of pre-processing , feature extraction , recog- nition and post processing like the conventional character recognition but ap- plied to strokes . the on-line devanagari character recognition reduces to one of recognizing one of 69 primitives and recognition of a character is performed by recognizing a sequence of such primitives . we further show the impact of noise removal on on-line raw data which is usually noisy . the use of fuzzy direc- tional features to enhance the accuracy of stroke recognition is also described . the recognition results are compared with commonly used directional features in literature using several classifiers .", "topics": ["feature extraction"]}
{"title": "enhancing evolutionary optimization in uncertain environments by allocating evaluations via multi-armed bandit algorithms", "abstract": "optimization problems with uncertain fitness functions are common in the real world , and present unique challenges for evolutionary optimization approaches . existing issues include excessively expensive evaluation , lack of solution reliability , and incapability in maintaining high overall fitness during optimization . using conversion rate optimization as an example , this paper proposes a series of new techniques for addressing these issues . the main innovation is to augment evolutionary algorithms by allocating evaluation budget through multi-armed bandit algorithms . experimental results demonstrate that multi-armed bandit algorithms can be used to allocate evaluations efficiently , select the winning solution reliably and increase overall fitness during exploration . the proposed methods can be generalized to any optimization problems with noisy fitness functions .", "topics": ["optimization problem", "mathematical optimization"]}
{"title": "accelerating reinforcement learning by composing solutions of automatically identified subtasks", "abstract": "this paper discusses a system that accelerates reinforcement learning by using transfer from related tasks . without such transfer , even if two tasks are very similar at some abstract level , an extensive re-learning effort is required . the system achieves much of its power by transferring parts of previously learned solutions rather than a single complete solution . the system exploits strong features in the multi-dimensional function produced by reinforcement learning in solving a particular task . these features are stable and easy to recognize early in the learning process . they generate a partitioning of the state space and thus the function . the partition is represented as a graph . this is used to index and compose functions stored in a case base to form a close approximation to the solution of the new task . experiments demonstrate that function composition often produces more than an order of magnitude increase in learning rate compared to a basic reinforcement learning algorithm .", "topics": ["reinforcement learning"]}
{"title": "regularization techniques for fine-tuning in neural machine translation", "abstract": "we investigate techniques for supervised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset . in this scenario , overfitting is a major challenge . we investigate a number of techniques to reduce overfitting and improve transfer learning , including regularization techniques such as dropout and l2-regularization towards an out-of-domain prior . in addition , we introduce tuneout , a novel regularization technique inspired by dropout . we apply these techniques , alone and in combination , to neural machine translation , obtaining improvements on iwslt datasets for english- > german and english- > russian . we also investigate the amounts of in-domain training data needed for domain adaptation in nmt , and find a logarithmic relationship between the amount of training data and gain in bleu score .", "topics": ["test set", "machine translation"]}
{"title": "learning heterogeneous similarity measures for hybrid-recommendations in meta-mining", "abstract": "the notion of meta-mining has appeared recently and extends the traditional meta-learning in two ways . first it does not learn meta-models that provide support only for the learning algorithm selection task but ones that support the whole data-mining process . in addition it abandons the so called black-box approach to algorithm description followed in meta-learning . now in addition to the datasets , algorithms also have descriptors , workflows as well . for the latter two these descriptions are semantic , describing properties of the algorithms . with the availability of descriptors both for datasets and data mining workflows the traditional modelling techniques followed in meta-learning , typically based on classification and regression algorithms , are no longer appropriate . instead we are faced with a problem the nature of which is much more similar to the problems that appear in recommendation systems . the most important meta-mining requirements are that suggestions should use only datasets and workflows descriptors and the cold-start problem , e.g . providing workflow suggestions for new datasets . in this paper we take a different view on the meta-mining modelling problem and treat it as a recommender problem . in order to account for the meta-mining specificities we derive a novel metric-based-learning recommender approach . our method learns two homogeneous metrics , one in the dataset and one in the workflow space , and a heterogeneous one in the dataset-workflow space . all learned metrics reflect similarities established from the dataset-workflow preference matrix . we demonstrate our method on meta-mining over biological ( microarray datasets ) problems . the application of our method is not limited to the meta-mining problem , its formulations is general enough so that it can be applied on problems with similar requirements .", "topics": ["data mining"]}
{"title": "sparse deep stacking network for image classification", "abstract": "sparse coding can learn good robust representation to noise and model more higher-order representation for image classification . however , the inference algorithm is computationally expensive even though the supervised signals are used to learn compact and discriminative dictionaries in sparse coding techniques . luckily , a simplified neural network module ( snnm ) has been proposed to directly learn the discriminative dictionaries for avoiding the expensive inference . but the snnm module ignores the sparse representations . therefore , we propose a sparse snnm module by adding the mixed-norm regularization ( l1/l2 norm ) . the sparse snnm modules are further stacked to build a sparse deep stacking network ( s-dsn ) . in the experiments , we evaluate s-dsn with four databases , including extended yaleb , ar , 15 scene and caltech101 . experimental results show that our model outperforms related classification methods with only a linear classifier . it is worth noting that we reach 98.8 % recognition accuracy on 15 scene .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "laf-fabric : a data analysis tool for linguistic annotation framework with an application to the hebrew bible", "abstract": "the linguistic annotation framework ( laf ) provides a general , extensible stand-off markup system for corpora . this paper discusses laf-fabric , a new tool to analyse laf resources in general with an extension to process the hebrew bible in particular . we first walk through the history of the hebrew bible as text database in decennium-wide steps . then we describe how laf-fabric may serve as an analysis tool for this corpus . finally , we describe three analytic projects/workflows that benefit from the new laf representation : 1 ) the study of linguistic variation : extract cooccurrence data of common nouns between the books of the bible ( martijn naaijer ) ; 2 ) the study of the grammar of hebrew poetry in the psalms : extract clause typology ( gino kalkman ) ; 3 ) construction of a parser of classical hebrew by data oriented parsing : generate tree structures from the database ( andreas van cranenburgh ) .", "topics": ["text corpus", "parsing"]}
{"title": "unsupervised depth estimation , 3d face rotation and replacement", "abstract": "we present an unsupervised approach for learning to estimate three dimensional ( 3d ) facial structure from a single image while also predicting 3d viewpoint transformations that match a desired pose and facial geometry . we achieve this by inferring the depth of facial key-points in an input image in an unsupervised way . we show how it is possible to use these depths as intermediate computations within a new backpropable loss to predict the parameters of a 3d affine transformation matrix that maps inferred 3d key-points of an input face to corresponding 2d key-points on a desired target facial geometry or pose . our resulting approach can therefore be used to infer plausible 3d transformations from one face pose to another , allowing faces to be frontalized , trans- formed into 3d models or even warped to another pose and facial geometry . lastly , we identify certain shortcomings with our formulation , and explore adversarial image translation techniques as a post-processing step . correspondingly , we explore several adversarial image transformation methods which allow us to re-synthesize complete head shots for faces re-targeted to different poses as well as repair images resulting from face replacements across identities .", "topics": ["unsupervised learning", "map"]}
{"title": "a nonlinear kernel support matrix machine for matrix learning", "abstract": "in many problems of supervised tensor learning ( stl ) , real world data such as face images or mri scans are naturally represented as matrices , which are also called as second order tensors . most existing classifiers based on tensor representation , such as support tensor machine ( stm ) need to solve iteratively which occupy much time and may suffer from local minima . in this paper , we present a kernel support matrix machine ( ksmm ) to perform supervised learning when data are represented as matrices . ksmm is a general framework for the construction of matrix-based hyperplane to exploit structural information . we analyze a unifying optimization problem for which we propose an asymptotically convergent algorithm . theoretical analysis for the generalization bounds is derived based on rademacher complexity with respect to a probability distribution . we demonstrate the merits of the proposed method by exhaustive experiments on both simulation study and a number of real-word datasets from a variety of application domains .", "topics": ["kernel ( operating system )", "supervised learning"]}
{"title": "object recognition based on amounts of unlabeled data", "abstract": "this paper proposes a novel semi-supervised method on object recognition . first , based on boost picking , a universal algorithm , boost picking teaching ( bpt ) , is proposed to train an effective binary-classifier just using a few labeled data and amounts of unlabeled data . then , an ensemble strategy is detailed to synthesize multiple bpt-trained binary-classifiers to be a high-performance multi-classifier . the rationality of the strategy is also analyzed in theory . finally , the proposed method is tested on two databases , cifar-10 and cifar-100 . using 2 % labeled data and 98 % unlabeled data , the accuracies of the proposed method on the two data sets are 78.39 % and 50.77 % respectively .", "topics": ["database"]}
{"title": "multi-level error-resilient neural networks with learning", "abstract": "the problem of neural network association is to retrieve a previously memorized pattern from its noisy version using a network of neurons . an ideal neural network should include three components simultaneously : a learning algorithm , a large pattern retrieval capacity and resilience against noise . prior works in this area usually improve one or two aspects at the cost of the third . our work takes a step forward in closing this gap . more specifically , we show that by forcing natural constraints on the set of learning patterns , we can drastically improve the retrieval capacity of our neural network . moreover , we devise a learning algorithm whose role is to learn those patterns satisfying the above mentioned constraints . finally we show that our neural network can cope with a fair amount of noise .", "topics": ["neural networks"]}
{"title": "structboost : boosting methods for predicting structured output variables", "abstract": "boosting is a method for learning a single accurate predictor by linearly combining a set of less accurate weak learners . recently , structured learning has found many applications in computer vision . inspired by structured support vector machines ( ssvm ) , here we propose a new boosting algorithm for structured output prediction , which we refer to as structboost . structboost supports nonlinear structured learning by combining a set of weak structured learners . as ssvm generalizes svm , our structboost generalizes standard boosting approaches such as adaboost , or lpboost to structured learning . the resulting optimization problem of structboost is more challenging than ssvm in the sense that it may involve exponentially many variables and constraints . in contrast , for ssvm one usually has an exponential number of constraints and a cutting-plane method is used . in order to efficiently solve structboost , we formulate an equivalent $ 1 $ -slack formulation and solve it using a combination of cutting planes and column generation . we show the versatility and usefulness of structboost on a range of problems such as optimizing the tree loss for hierarchical multi-class classification , optimizing the pascal overlap criterion for robust visual tracking and learning conditional random field parameters for image segmentation .", "topics": ["optimization problem", "support vector machine"]}
{"title": "towards optimally decentralized multi-robot collision avoidance via deep reinforcement learning", "abstract": "developing a safe and efficient collision avoidance policy for multiple robots is challenging in the decentralized scenarios where each robot generate its paths without observing other robots ' states and intents . while other distributed multi-robot collision avoidance systems exist , they often require extracting agent-level features to plan a local collision-free action , which can be computationally prohibitive and not robust . more importantly , in practice the performance of these methods are much lower than their centralized counterparts . we present a decentralized sensor-level collision avoidance policy for multi-robot systems , which directly maps raw sensor measurements to an agent 's steering commands in terms of movement velocity . as a first step toward reducing the performance gap between decentralized and centralized methods , we present a multi-scenario multi-stage training framework to find an optimal policy which is trained over a large number of robots on rich , complex environments simultaneously using a policy gradient based reinforcement learning algorithm . we validate the learned sensor-level collision avoidance policy in a variety of simulated scenarios with thorough performance evaluations and show that the final learned policy is able to find time efficient , collision-free paths for a large-scale robot system . we also demonstrate that the learned policy can be well generalized to new scenarios that do not appear in the entire training period , including navigating a heterogeneous group of robots and a large-scale scenario with 100 robots . videos are available at https : //sites.google.com/view/drlmaca", "topics": ["reinforcement learning", "simulation"]}
{"title": "energy clustering", "abstract": "energy statistics was proposed by sz\\ ' { e } kely in the 80 's inspired by the newtonian gravitational potential from classical mechanics , and it provides a hypothesis test for equality of distributions . it was further generalized from euclidean spaces to metric spaces of strong negative type , and more recently , a connection with reproducing kernel hilbert spaces ( rkhs ) was established . here we consider the clustering problem from an energy statistics theory perspective , providing a precise mathematical formulation yielding a quadratically constrained quadratic program ( qcqp ) in the associated rkhs , thus establishing the connection with kernel methods . we show that this qcqp is equivalent to kernel $ k $ -means optimization problem once the kernel is fixed . these results imply a first principles derivation of kernel $ k $ -means from energy statistics . however , energy statistics fixes a family of standard kernels . furthermore , we also consider a weighted version of energy statistics , making connection to graph partitioning problems . to find local optimizers of such qcqp we propose an iterative algorithm based on hartigan 's method , which in this case has the same computational cost as kernel $ k $ -means algorithm , based on lloyd 's heuristic , but usually with better clustering quality . we provide carefully designed numerical experiments showing the superiority of the proposed method compared to kernel $ k $ -means , spectral clustering , standard $ k $ -means , and gaussian mixture models in a variety of settings .", "topics": ["kernel ( operating system )", "cluster analysis"]}
{"title": "challenges in kurdish text processing", "abstract": "despite having a large number of speakers , the kurdish language is among the less-resourced languages . in this work we highlight the challenges and problems in providing the required tools and techniques for processing texts written in kurdish . from a high-level perspective , the main challenges are : the inherent diversity of the language , standardization and segmentation issues , and the lack of language resources .", "topics": ["high- and low-level", "image segmentation"]}
{"title": "a robust particle detection algorithm based on symmetry", "abstract": "particle tracking is common in many biophysical , ecological , and micro-fluidic applications . reliable tracking information is heavily dependent on of the system under study and algorithms that correctly determines particle position between images . however , in a real environmental context with the presence of noise including particular or dissolved matter in water , and low and fluctuating light conditions , many algorithms fail to obtain reliable information . we propose a new algorithm , the circular symmetry algorithm ( c-sym ) , for detecting the position of a circular particle with high accuracy and precision in noisy conditions . the algorithm takes advantage of the spatial symmetry of the particle allowing for subpixel accuracy . we compare the proposed algorithm with four different methods using both synthetic and experimental datasets . the results show that c-sym is the most accurate and precise algorithm when tracking micro-particles in all tested conditions and it has the potential for use in applications including tracking biota in their environment .", "topics": ["synthetic data", "pixel"]}
{"title": "fast mcmc sampling for markov jump processes and continuous time bayesian networks", "abstract": "markov jump processes and continuous time bayesian networks are important classes of continuous time dynamical systems . in this paper , we tackle the problem of inferring unobserved paths in these models by introducing a fast auxiliary variable gibbs sampler . our approach is based on the idea of uniformization , and sets up a markov chain over paths by sampling a finite set of virtual jump times and then running a standard hidden markov model forward filtering-backward sampling algorithm over states at the set of extant and virtual jump times . we demonstrate significant computational benefits over a state-of-the-art gibbs sampler on a number of continuous time bayesian networks .", "topics": ["sampling ( signal processing )", "bayesian network"]}
{"title": "emotional metaheuristics for in-situ foraging using sensor constrained robot swarms", "abstract": "we present a new social animal inspired emotional swarm intelligence technique . this technique is used to solve a variant of the popular collective robots problem called foraging . we show with a simulation study how simple interaction rules based on sensations like hunger and loneliness can lead to globally coherent emergent behavior which allows sensor constrained robots to solve the given problem", "topics": ["simulation", "robot"]}
{"title": "private empirical risk minimization beyond the worst case : the effect of the constraint set geometry", "abstract": "empirical risk minimization ( erm ) is a standard technique in machine learning , where a model is selected by minimizing a loss function over constraint set . when the training dataset consists of private information , it is natural to use a differentially private erm algorithm , and this problem has been the subject of a long line of work started with chaudhuri and monteleoni 2008 . a private erm algorithm outputs an approximate minimizer of the loss function and its error can be measured as the difference from the optimal value of the loss function . when the constraint set is arbitrary , the required error bounds are fairly well understood \\cite { bassilyst14 } . in this work , we show that the geometric properties of the constraint set can be used to derive significantly better results . specifically , we show that a differentially private version of mirror descent leads to error bounds of the form $ \\tilde { o } ( g_ { \\mathcal { c } } /n ) $ for a lipschitz loss function , improving on the $ \\tilde { o } ( \\sqrt { p } /n ) $ bounds in bassily , smith and thakurta 2014 . here $ p $ is the dimensionality of the problem , $ n $ is the number of data points in the training set , and $ g_ { \\mathcal { c } } $ denotes the gaussian width of the constraint set that we optimize over . we show similar improvements for strongly convex functions , and for smooth functions . in addition , we show that when the loss function is lipschitz with respect to the $ \\ell_1 $ norm and $ \\mathcal { c } $ is $ \\ell_1 $ -bounded , a differentially private version of the frank-wolfe algorithm gives error bounds of the form $ \\tilde { o } ( n^ { -2/3 } ) $ . this captures the important and common case of sparse linear regression ( lasso ) , when the data $ x_i $ satisfies $ |x_i|_ { \\infty } \\leq 1 $ and we optimize over the $ \\ell_1 $ ball . we show new lower bounds for this setting , that together with known bounds , imply that all our upper bounds are tight .", "topics": ["approximation algorithm", "loss function"]}
{"title": "satellite image classification and segmentation using non-additive entropy", "abstract": "here we compare the boltzmann-gibbs-shannon ( standard ) with the tsallis entropy on the pattern recognition and segmentation of coloured images obtained by satellites , via `` google earth '' . by segmentation we mean split an image to locate regions of interest . here , we discriminate and define an image partition classes according to a training basis . this training basis consists of three pattern classes : aquatic , urban and vegetation regions . our numerical experiments demonstrate that the tsallis entropy , used as a feature vector composed of distinct entropic indexes $ q $ outperforms the standard entropy . there are several applications of our proposed methodology , once satellite images can be used to monitor migration form rural to urban regions , agricultural activities , oil spreading on the ocean etc .", "topics": ["computer vision"]}
{"title": "planecell : representing the 3d space with planes", "abstract": "reconstruction based on the stereo camera has received considerable attention recently , but two particular challenges still remain . the first concerns the need to aggregate similar pixels in an effective approach , and the second is to maintain as much of the available information as possible while ensuring sufficient accuracy . to overcome these issues , we propose a new 3d representation method , namely , planecell , that extracts planarity from the depth-assisted image segmentation and then projects these depth planes into the 3d world . an energy function formulated from conditional random field that generalizes the planar relationships is maximized to merge coplanar segments . we evaluate our method with a variety of reconstruction baselines on both kitti and middlebury datasets , and the results indicate the superiorities compared to other 3d space representation methods in accuracy , memory requirements and further applications .", "topics": ["baseline ( configuration management )", "image segmentation"]}
{"title": "suboptimal solution path algorithm for support vector machine", "abstract": "we consider a suboptimal solution path algorithm for the support vector machine . the solution path algorithm is an effective tool for solving a sequence of a parametrized optimization problems in machine learning . the path of the solutions provided by this algorithm are very accurate and they satisfy the optimality conditions more strictly than other svm optimization algorithms . in many machine learning application , however , this strict optimality is often unnecessary , and it adversely affects the computational efficiency . our algorithm can generate the path of suboptimal solutions within an arbitrary user-specified tolerance level . it allows us to control the trade-off between the accuracy of the solution and the computational cost . moreover , we also show that our suboptimal solutions can be interpreted as the solution of a \\emph { perturbed optimization problem } from the original one . we provide some theoretical analyses of our algorithm based on this novel interpretation . the experimental results also demonstrate the effectiveness of our algorithm .", "topics": ["optimization problem", "support vector machine"]}
{"title": "fast predictive image registration", "abstract": "we present a method to predict image deformations based on patch-wise image appearance . specifically , we design a patch-based deep encoder-decoder network which learns the pixel/voxel-wise mapping between image appearance and registration parameters . our approach can predict general deformation parameterizations , however , we focus on the large deformation diffeomorphic metric mapping ( lddmm ) registration model . by predicting the lddmm momentum-parameterization we retain the desirable theoretical properties of lddmm , while reducing computation time by orders of magnitude : combined with patch pruning , we achieve a 1500x/66x speed up compared to gpu-based optimization for 2d/3d image registration . our approach has better prediction accuracy than predicting deformation or velocity fields and results in diffeomorphic transformations . additionally , we create a bayesian probabilistic version of our network , which allows evaluation of deformation field uncertainty through monte carlo sampling using dropout at test time . we show that deformation uncertainty highlights areas of ambiguous deformations . we test our method on the oasis brain image dataset in 2d and 3d .", "topics": ["sampling ( signal processing )", "time complexity"]}
{"title": "deep temporal-recurrent-replicated-softmax for topical trends over time", "abstract": "dynamic topic modeling facilitates the identification of topical trends over time in temporal collections of unstructured documents . we introduce a novel unsupervised neural dynamic topic model known as recurrent neural network-replicated softmax model ( rnnrsm ) , where the discovered topics at each time influence the topic discovery in the subsequent time steps . we account for the temporal ordering of documents by explicitly modeling a joint distribution of latent topical dependencies over time , using distributional estimators with temporal recurrent connections . applying rnn-rsm to 19 years of articles on nlp research , we demonstrate that compared to state-of-the art topic models , rnn-rsm shows better generalization , topic interpretation , evolution and trends . we also propose to quantify the capability of dynamic topic model to capture word evolution in topics over time .", "topics": ["natural language processing", "recurrent neural network"]}
{"title": "multi-task gloh feature selection for human age estimation", "abstract": "in this paper , we propose a novel age estimation method based on gloh feature descriptor and multi-task learning ( mtl ) . the gloh feature descriptor , one of the state-of-the-art feature descriptor , is used to capture the age-related local and spatial information of face image . as the exacted gloh features are often redundant , mtl is designed to select the most informative feature bins for age estimation problem , while the corresponding weights are determined by ridge regression . this approach largely reduces the dimensions of feature , which can not only improve performance but also decrease the computational burden . experiments on the public available fg-net database show that the proposed method can achieve comparable performance over previous approaches while using much fewer features .", "topics": ["gradient"]}
{"title": "document clustering evaluation : divergence from a random baseline", "abstract": "divergence from a random baseline is a technique for the evaluation of document clustering . it ensures cluster quality measures are performing work that prevents ineffective clusterings from giving high scores to clusterings that provide no useful result . these concepts are defined and analysed using intrinsic and extrinsic approaches to the evaluation of document cluster quality . this includes the classical clusters to categories approach and a novel approach that uses ad hoc information retrieval . the divergence from a random baseline approach is able to differentiate ineffective clusterings encountered in the inex xml mining track . it also appears to perform a normalisation similar to the normalised mutual information ( nmi ) measure but it can be applied to any measure of cluster quality . when it is applied to the intrinsic measure of distortion as measured by rmse , subtraction from a random baseline provides a clear optimum that is not apparent otherwise . this approach can be applied to any clustering evaluation . this paper describes its use in the context of document clustering evaluation .", "topics": ["baseline ( configuration management )", "cluster analysis"]}
{"title": "punny captions : witty wordplay in image descriptions", "abstract": "wit is a quintessential form of rich inter-human interaction , and is often grounded in a specific situation ( e.g . , a comment in response to an event ) . in this work , we attempt to build computational models that can produce witty descriptions for a given image . inspired by a cognitive account of humor appreciation , we employ linguistic wordplay , specifically puns . we compare our approach against meaningful baseline approaches via human studies . in a turing test style evaluation , people find our model 's description for an image to be wittier than a human 's witty description 55 % of the time !", "topics": ["baseline ( configuration management )"]}
{"title": "deepstyle : multimodal search engine for fashion and interior design", "abstract": "in this paper , we propose a multimodal search engine that combines visual and textual cues to retrieve items from a multimedia database aesthetically similar to the query . the goal of our engine is to enable intuitive retrieval of fashion merchandise such as clothes or furniture . existing search engines treat textual input only as an additional source of information about the query image and do not correspond to the real-life scenario where the user looks for 'the same shirt but of denim ' . our novel method , dubbed deepstyle , mitigates those shortcomings by using a joint neural network architecture to model contextual dependencies between features of different modalities . we prove the robustness of this approach on two different challenging datasets of fashion items and furniture where our deepstyle engine outperforms baseline methods by 18-21 % on the tested datasets . our search engine is commercially deployed and available through a web-based application .", "topics": ["baseline ( configuration management )"]}
{"title": "heuristic feature selection for clickbait detection", "abstract": "we study feature selection as a means to optimize the baseline clickbait detector employed at the clickbait challenge 2017 . the challenge 's task is to score the `` clickbaitiness '' of a given twitter tweet on a scale from 0 ( no clickbait ) to 1 ( strong clickbait ) . unlike most other approaches submitted to the challenge , the baseline approach is based on manual feature engineering and does not compete out of the box with many of the deep learning-based approaches . we show that scaling up feature selection efforts to heuristically identify better-performing feature subsets catapults the performance of the baseline classifier to second rank overall , beating 12 other competing approaches and improving over the baseline performance by 20 % . this demonstrates that traditional classification approaches can still keep up with deep learning on this task .", "topics": ["baseline ( configuration management )", "heuristic"]}
{"title": "deep linear discriminant analysis", "abstract": "we introduce deep linear discriminant analysis ( deeplda ) which learns linearly separable latent representations in an end-to-end fashion . classic lda extracts features which preserve class separability and is used for dimensionality reduction for many classification problems . the central idea of this paper is to put lda on top of a deep neural network . this can be seen as a non-linear extension of classic lda . instead of maximizing the likelihood of target labels for individual samples , we propose an objective function that pushes the network to produce feature distributions which : ( a ) have low variance within the same class and ( b ) high variance between different classes . our objective is derived from the general lda eigenvalue problem and still allows to train with stochastic gradient descent and back-propagation . for evaluation we test our approach on three different benchmark datasets ( mnist , cifar-10 and stl-10 ) . deeplda produces competitive results on mnist and cifar-10 and outperforms a network trained with categorical cross entropy ( same architecture ) on a supervised setting of stl-10 .", "topics": ["optimization problem", "nonlinear system"]}
{"title": "deep action- and context-aware sequence learning for activity recognition and anticipation", "abstract": "action recognition and anticipation are key to the success of many computer vision applications . existing methods can roughly be grouped into those that extract global , context-aware representations of the entire image or sequence , and those that aim at focusing on the regions where the action occurs . while the former may suffer from the fact that context is not always reliable , the latter completely ignore this source of information , which can nonetheless be helpful in many situations . in this paper , we aim at making the best of both worlds by developing an approach that leverages both context-aware and action-aware features . at the core of our method lies a novel multi-stage recurrent architecture that allows us to effectively combine these two sources of information throughout a video . this architecture first exploits the global , context-aware features , and merges the resulting representation with the localized , action-aware ones . our experiments on standard datasets evidence the benefits of our approach over methods that use each information type separately . we outperform the state-of-the-art methods that , as us , rely only on rgb frames as input for both action recognition and anticipation .", "topics": ["computer vision"]}
{"title": "a two-stage method for text line detection in historical documents", "abstract": "this work presents a two-stage text line detection method for historical documents . in a first stage , a deep neural network called aru-net labels pixels to belong to one of the three classes : baseline , separator or other . the separator class marks beginning and end of each text line . the aru-net is trainable from scratch with manageably few manually annotated example images ( less than 50 ) . this is achieved by utilizing data augmentation strategies . the network predictions are used as input for the second stage which performs a bottom-up clustering to build baselines . the developed method is capable of handling complex layouts as well as curved and arbitrarily oriented text lines . it substantially outperforms current state-of-the-art approaches . for example , for the complex track of the cbad : icdar2017 competiton on baseline detection the f-value is increased from 0.859 to 0.922 . the framework to train and run the aru-net is open source .", "topics": ["baseline ( configuration management )", "cluster analysis"]}
{"title": "area protection in adversarial path-finding scenarios with multiple mobile agents on graphs : a theoretical and experimental study of target-allocation strategies for defense coordination", "abstract": "we address a problem of area protection in graph-based scenarios with multiple agents . the problem consists of two adversarial teams of agents that move in an undirected graph shared by both teams . agents are placed in vertices of the graph ; at most one agent can occupy a vertex ; and they can move into adjacent vertices in a conflict free way . teams have asymmetric goals : the aim of one team - attackers - is to invade into given area while the aim of the opponent team - defenders - is to protect the area from being entered by attackers by occupying selected vertices . we study strategies for allocating vertices to be occupied by the team of defenders to block attacking agents . we show that the decision version of the problem of area protection is pspace-hard under the assumption that agents can allocate their target vertices multiple times . further we develop various on-line vertex-allocation strategies for the defender team in a simplified variant of the problem with single stage vertex allocation and evaluated their performance in multiple benchmarks . the success of a strategy is heavily dependent on the type of the instance , and so one of the contributions of this work is that we identify suitable vertex-allocation strategies for diverse instance types . in particular , we introduce a simulation-based method that identifies and tries to capture bottlenecks in the graph , that are frequently used by the attackers . our experimental evaluation suggests that this method often allows a successful defense even in instances where the attackers significantly outnumber the defenders .", "topics": ["simulation"]}
{"title": "the generalization ability of online algorithms for dependent data", "abstract": "we study the generalization performance of online learning algorithms trained on samples coming from a dependent source of data . we show that the generalization error of any stable online algorithm concentrates around its regret -- an easily computable statistic of the online performance of the algorithm -- when the underlying ergodic process is $ \\beta $ - or $ \\phi $ -mixing . we show high probability error bounds assuming the loss function is convex , and we also establish sharp convergence rates and deviation bounds for strongly convex losses and several linear prediction problems such as linear and logistic regression , least-squares svm , and boosting on dependent data . in addition , our results have straightforward applications to stochastic optimization with dependent data , and our analysis requires only martingale convergence arguments ; we need not rely on more powerful statistical tools such as empirical process theory .", "topics": ["regret ( decision theory )", "loss function"]}
{"title": "end-to-end semantic face segmentation with conditional random fields as convolutional , recurrent and adversarial networks", "abstract": "recent years have seen a sharp increase in the number of related yet distinct advances in semantic segmentation . here , we tackle this problem by leveraging the respective strengths of these advances . that is , we formulate a conditional random field over a four-connected graph as end-to-end trainable convolutional and recurrent networks , and estimate them via an adversarial process . importantly , our model learns not only unary potentials but also pairwise potentials , while aggregating multi-scale contexts and controlling higher-order inconsistencies . we evaluate our model on two standard benchmark datasets for semantic face segmentation , achieving state-of-the-art results on both of them .", "topics": ["end-to-end principle"]}
{"title": "towards lightweight convolutional neural networks for object detection", "abstract": "we propose model with larger spatial size of feature maps and evaluate it on object detection task . with the goal to choose the best feature extraction network for our model we compare several popular lightweight networks . after that we conduct a set of experiments with channels reduction algorithms in order to accelerate execution . our vehicle detection models are accurate , fast and therefore suit for embedded visual applications . with only 1.5 gflops our best model gives 93.39 ap on validation subset of challenging detrac dataset . the smallest of our models is the first to achieve real-time inference speed on cpu with reasonable accuracy drop to 91.43 ap .", "topics": ["feature extraction", "object detection"]}
{"title": "move evaluation in go using deep convolutional neural networks", "abstract": "the game of go is more challenging than other board games , due to the difficulty of constructing a position or move evaluation function . in this paper we investigate whether deep convolutional networks can be used to directly represent and learn this knowledge . we train a large 12-layer convolutional neural network by supervised learning from a database of human professional games . the network correctly predicts the expert move in 55 % of positions , equalling the accuracy of a 6 dan human player . when the trained convolutional network was used directly to play games of go , without any search , it beat the traditional search program gnugo in 97 % of games , and matched the performance of a state-of-the-art monte-carlo tree search that simulates a million positions per move .", "topics": ["supervised learning"]}
{"title": "superpixels based segmentation and svm based classification method to distinguish five diseases from normal regions in wireless capsule endoscopy", "abstract": "wireless capsule endoscopy ( wce ) is relatively a new technology to examine the entire gi trace . during an examination , it captures more than 55,000 frames . reviewing all these images is time-consuming and prone to human error . it has been a challenge to develop intelligent methods assisting physicians to review the frames . the wce frames are captured in 8-bit color depths which provides enough a color range to detect abnormalities . here , superpixel based methods are proposed to segment five diseases including : bleeding , crohn 's disease , lymphangiectasia , xanthoma , and lymphoid hyperplasia . two superpixels methods are compared to provide semantic segmentation of these prolific diseases : simple linear iterative clustering ( slic ) and quick shift ( qs ) . the segmented superpixels were classified into two classes ( normal and abnormal ) by support vector machine ( svm ) using texture and color features . for both superpixel methods , the accuracy , specificity , sensitivity , and precision ( slic , qs ) were around 92 % , 93 % , 93 % , and 88 % , respectively . however , slic was dramatically faster than qs .", "topics": ["cluster analysis", "support vector machine"]}
{"title": "the tensor memory hypothesis", "abstract": "we discuss memory models which are based on tensor decompositions using latent representations of entities and events . we show how episodic memory and semantic memory can be realized and discuss how new memory traces can be generated from sensory input : existing memories are the basis for perception and new memories are generated via perception . we relate our mathematical approach to the hippocampal memory indexing theory . we describe the first detailed mathematical models for the complete processing pipeline from sensory input and its semantic decoding , i.e . , perception , to the formation of episodic and semantic memories and their declarative semantic decodings . our main hypothesis is that perception includes an active semantic decoding process , which relies on latent representations of entities and predicates , and that episodic and semantic memories depend on the same decoding process . we contribute to the debate between the leading memory consolidation theories , i.e . , the standard consolidation theory ( sct ) and the multiple trace theory ( mtt ) . the latter is closely related to the complementary learning systems ( cls ) framework . in particular , we show explicitly how episodic memory can teach the neocortex to form a semantic memory , which is a core issue in mtt and cls .", "topics": ["entity"]}
{"title": "noun-phrase analysis in unrestricted text for information retrieval", "abstract": "information retrieval is an important application area of natural-language processing where one encounters the genuine challenge of processing large quantities of unrestricted natural-language text . this paper reports on the application of a few simple , yet robust and efficient noun-phrase analysis techniques to create better indexing phrases for information retrieval . in particular , we describe a hybrid approach to the extraction of meaningful ( continuous or discontinuous ) subcompounds from complex noun phrases using both corpus statistics and linguistic heuristics . results of experiments show that indexing based on such extracted subcompounds improves both recall and precision in an information retrieval system . the noun-phrase analysis techniques are also potentially useful for book indexing and automatic thesaurus extraction .", "topics": ["natural language processing", "natural language"]}
{"title": "text as statistical mechanics object", "abstract": "in this article we present a model of human written text based on statistical mechanics approach by deriving the potential energy for different parts of the text using large text corpus . we have checked the results numerically and found that the specific heat parameter effectively separates the closed class words from the specific terms used in the text .", "topics": ["numerical analysis", "text corpus"]}
{"title": "optimizing f-measure : a tale of two approaches", "abstract": "f-measures are popular performance metrics , particularly for tasks with imbalanced data sets . algorithms for learning to maximize f-measures follow two approaches : the empirical utility maximization ( eum ) approach learns a classifier having optimal performance on training data , while the decision-theoretic approach learns a probabilistic model and then predicts labels with maximum expected f-measure . in this paper , we investigate the theoretical justifications and connections for these two approaches , and we study the conditions under which one approach is preferable to the other using synthetic and real datasets . given accurate models , our results suggest that the two approaches are asymptotically equivalent given large training and test sets . nevertheless , empirically , the eum approach appears to be more robust against model misspecification , and given a good model , the decision-theoretic approach appears to be better for handling rare classes and a common domain adaptation scenario .", "topics": ["test set", "synthetic data"]}
{"title": "large deviation methods for approximate probabilistic inference", "abstract": "we study two-layer belief networks of binary random variables in which the conditional probabilities pr [ childlparents ] depend monotonically on weighted sums of the parents . in large networks where exact probabilistic inference is intractable , we show how to compute upper and lower bounds on many probabilities of interest . in particular , using methods from large deviation theory , we derive rigorous bounds on marginal probabilities such as pr [ children ] and prove rates of convergence for the accuracy of our bounds as a function of network size . our results apply to networks with generic transfer function parameterizations of the conditional probability tables , such as sigmoid and noisy-or . they also explicitly illustrate the types of averaging behavior that can simplify the problem of inference in large networks .", "topics": ["bayesian network"]}
{"title": "sparse inertial poser : automatic 3d human pose estimation from sparse imus", "abstract": "we address the problem of making human motion capture in the wild more practical by using a small set of inertial sensors attached to the body . since the problem is heavily under-constrained , previous methods either use a large number of sensors , which is intrusive , or they require additional video input . we take a different approach and constrain the problem by : ( i ) making use of a realistic statistical body model that includes anthropometric constraints and ( ii ) using a joint optimization framework to fit the model to orientation and acceleration measurements over multiple frames . the resulting tracker sparse inertial poser ( sip ) enables 3d human pose estimation using only 6 sensors ( attached to the wrists , lower legs , back and head ) and works for arbitrary human motions . experiments on the recently released tnt15 dataset show that , using the same number of sensors , sip achieves higher accuracy than the dataset baseline without using any video data . we further demonstrate the effectiveness of sip on newly recorded challenging motions in outdoor scenarios such as climbing or jumping over a wall .", "topics": ["baseline ( configuration management )", "sparse matrix"]}
{"title": "data-driven online decision making with costly information acquisition", "abstract": "in most real-world settings such as recommender systems , finance , and healthcare , collecting useful information is costly and requires an active choice on the part of the decision maker . the decision-maker needs to learn simultaneously what observations to make and what actions to take . this paper incorporates the information acquisition decision into an online learning framework . we propose two different algorithms for this dual learning problem : sim-oos and seq-oos where observations are made simultaneously and sequentially , respectively . we prove that both algorithms achieve a regret that is sublinear in time . the developed framework and algorithms can be used in many applications including medical informatics , recommender systems and actionable intelligence in transportation , finance , cyber-security etc . , in which collecting information prior to making decisions is costly . we validate our algorithms in a breast cancer example setting in which we show substantial performance gains for our proposed algorithms .", "topics": ["regret ( decision theory )"]}
{"title": "optimal time-series motifs", "abstract": "motifs are the most repetitive/frequent patterns of a time-series . the discovery of motifs is crucial for practitioners in order to understand and interpret the phenomena occurring in sequential data . currently , motifs are searched among series sub-sequences , aiming at selecting the most frequently occurring ones . search-based methods , which try out series sub-sequence as motif candidates , are currently believed to be the best methods in finding the most frequent patterns . however , this paper proposes an entirely new perspective in finding motifs . we demonstrate that searching is non-optimal since the domain of motifs is restricted , and instead we propose a principled optimization approach able to find optimal motifs . we treat the occurrence frequency as a function and time-series motifs as its parameters , therefore we \\textit { learn } the optimal motifs that maximize the frequency function . in contrast to searching , our method is able to discover the most repetitive patterns ( hence optimal ) , even in cases where they do not explicitly occur as sub-sequences . experiments on several real-life time-series datasets show that the motifs found by our method are highly more frequent than the ones found through searching , for exactly the same distance threshold .", "topics": ["time series"]}
{"title": "context-sensitive measurement of word distance by adaptive scaling of a semantic space", "abstract": "the paper proposes a computationally feasible method for measuring context-sensitive semantic distance between words . the distance is computed by adaptive scaling of a semantic space . in the semantic space , each word in the vocabulary v is represented by a multi-dimensional vector which is obtained from an english dictionary through a principal component analysis . given a word set c which specifies a context for measuring word distance , each dimension of the semantic space is scaled up or down according to the distribution of c in the semantic space . in the space thus transformed , distance between words in v becomes dependent on the context c. an evaluation through a word prediction task shows that the proposed measurement successfully extracts the context of a text .", "topics": ["scalability"]}
{"title": "reactive reinforcement learning in asynchronous environments", "abstract": "the relationship between a reinforcement learning ( rl ) agent and an asynchronous environment is often ignored . frequently used models of the interaction between an agent and its environment , such as markov decision processes ( mdp ) or semi-markov decision processes ( smdp ) , do not capture the fact that , in an asynchronous environment , the state of the environment may change during computation performed by the agent . in an asynchronous environment , minimizing reaction time -- -the time it takes for an agent to react to an observation -- -also minimizes the time in which the state of the environment may change following observation . in many environments , the reaction time of an agent directly impacts task performance by permitting the environment to transition into either an undesirable terminal state or a state where performing the chosen action is inappropriate . we propose a class of reactive reinforcement learning algorithms that address this problem of asynchronous environments by immediately acting after observing new state information . we compare a reactive sarsa learning algorithm with the conventional sarsa learning algorithm on two asynchronous robotic tasks ( emergency stopping and impact prevention ) , and show that the reactive rl algorithm reduces the reaction time of the agent by approximately the duration of the algorithm 's learning update . this new class of reactive algorithms may facilitate safer control and faster decision making without any change to standard learning guarantees .", "topics": ["reinforcement learning", "computation"]}
{"title": "sharesnet : reducing residual network parameter number by sharing weights", "abstract": "deep residual networks have reached the state of the art in many image processing tasks such image classification . however , the cost for a gain in accuracy in terms of depth and memory is prohibitive as it requires a higher number of residual blocks , up to double the initial value . to tackle this problem , we propose in this paper a way to reduce the redundant information of the networks . we share the weights of convolutional layers between residual blocks operating at the same spatial scale . the signal flows multiple times in the same convolutional layer . the resulting architecture , called sharesnet , contains block specific layers and shared layers . these sharesnet are trained exactly in the same fashion as the commonly used residual networks . we show , on the one hand , that they are almost as efficient as their sequential counterparts while involving less parameters , and on the other hand that they are more efficient than a residual network with the same number of parameters . for example , a 152-layer-deep residual network can be reduced to 106 convolutional layers , i.e . a parameter gain of 39\\ % , while loosing less than 0.2\\ % accuracy on imagenet .", "topics": ["image processing", "computer vision"]}
{"title": "operator-valued bochner theorem , fourier feature maps for operator-valued kernels , and vector-valued learning", "abstract": "this paper presents a framework for computing random operator-valued feature maps for operator-valued positive definite kernels . this is a generalization of the random fourier features for scalar-valued kernels to the operator-valued case . our general setting is that of operator-valued kernels corresponding to rkhs of functions with values in a hilbert space . we show that in general , for a given kernel , there are potentially infinitely many random feature maps , which can be bounded or unbounded . most importantly , given a kernel , we present a general , closed form formula for computing a corresponding probability measure , which is required for the construction of the fourier features , and which , unlike the scalar case , is not uniquely and automatically determined by the kernel . we also show that , under appropriate conditions , random bounded feature maps can always be computed . furthermore , we show the uniform convergence , under the hilbert-schmidt norm , of the resulting approximate kernel to the exact kernel on any compact subset of euclidean space . our convergence requires differentiable kernels , an improvement over the twice-differentiability requirement in previous work in the scalar setting . we then show how operator-valued feature maps and their approximations can be employed in a general vector-valued learning framework . the mathematical formulation is illustrated by numerical examples on matrix-valued kernels .", "topics": ["kernel ( operating system )", "approximation algorithm"]}
{"title": "recent technological advances in natural language processing and artificial intelligence", "abstract": "a recent advance in computer technology has permitted scientists to implement and test algorithms that were known from quite some time ( or not ) but which were computationally expensive . two such projects are ibm 's jeopardy as a part of its deepqa project [ 1 ] and wolfram 's wolframalpha [ 2 ] . both these methods implement natural language processing ( another goal of ai scientists ) and try to answer questions as asked by the user . though the goal of the two projects is similar , both of them have a different procedure at it 's core . in the following sections , the mechanism and history of ibm 's jeopardy and wolfram alpha has been explained followed by the implications of these projects in realizing ray kurzweil 's [ 3 ] dream of passing the turing test by 2029 . a recipe of taking the above projects to a new level is also explained .", "topics": ["natural language processing", "artificial intelligence"]}
{"title": "a tale of two draggns : a hybrid approach for interpreting action-oriented and goal-oriented instructions", "abstract": "robots operating alongside humans in diverse , stochastic environments must be able to accurately interpret natural language commands . these instructions often fall into one of two categories : those that specify a goal condition or target state , and those that specify explicit actions , or how to perform a given task . recent approaches have used reward functions as a semantic representation of goal-based commands , which allows for the use of a state-of-the-art planner to find a policy for the given task . however , these reward functions can not be directly used to represent action-oriented commands . we introduce a new hybrid approach , the deep recurrent action-goal grounding network ( draggn ) , for task grounding and execution that handles natural language from either category as input , and generalizes to unseen environments . our robot-simulation results demonstrate that a system successfully interpreting both goal-oriented and action-oriented task specifications brings us closer to robust natural language understanding for human-robot interaction .", "topics": ["natural language", "robot"]}
{"title": "spherical cnns", "abstract": "convolutional neural networks ( cnns ) have become the method of choice for learning problems involving 2d planar images . however , a number of problems of recent interest have created a demand for models that can analyze spherical images . examples include omnidirectional vision for drones , robots , and autonomous cars , molecular regression problems , and global weather and climate modelling . a naive application of convolutional networks to a planar projection of the spherical signal is destined to fail , because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective . in this paper we introduce the building blocks for constructing spherical cnns . we propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant . the spherical correlation satisfies a generalized fourier theorem , which allows us to compute it efficiently using a generalized ( non-commutative ) fast fourier transform ( fft ) algorithm . we demonstrate the computational efficiency , numerical accuracy , and effectiveness of spherical cnns applied to 3d model recognition and atomization energy regression .", "topics": ["numerical analysis", "autonomous car"]}
{"title": "towards deep learning in hindi ner : an approach to tackle the labelled data scarcity", "abstract": "in this paper we describe an end to end neural model for named entity recognition ner ) which is based on bi-directional rnn-lstm . almost all ner systems for hindi use language specific features and handcrafted rules with gazetteers . our model is language independent and uses no domain specific features or any handcrafted rules . our models rely on semantic information in the form of word vectors which are learnt by an unsupervised learning algorithm on an unannotated corpus . our model attained state of the art performance in both english and hindi without the use of any morphological analysis or without using gazetteers of any sort .", "topics": ["unsupervised learning"]}
{"title": "bioinformatics and classical literary study", "abstract": "this paper describes the quantitative criticism lab , a collaborative initiative between classicists , quantitative biologists , and computer scientists to apply ideas and methods drawn from the sciences to the study of literature . a core goal of the project is the use of computational biology , natural language processing , and machine learning techniques to investigate authorial style , intertextuality , and related phenomena of literary significance . as a case study in our approach , here we review the use of sequence alignment , a common technique in genomics and computational linguistics , to detect intertextuality in latin literature . sequence alignment is distinguished by its ability to find inexact verbal similarities , which makes it ideal for identifying phonetic echoes in large corpora of latin texts . although especially suited to latin , sequence alignment in principle can be extended to many other languages .", "topics": ["natural language processing", "text corpus"]}
{"title": "semi-supervised active learning for support vector machines : a novel approach that exploits structure information in data", "abstract": "in our today 's information society more and more data emerges , e.g.~in social networks , technical applications , or business applications . companies try to commercialize these data using data mining or machine learning methods . for this purpose , the data are categorized or classified , but often at high ( monetary or temporal ) costs . an effective approach to reduce these costs is to apply any kind of active learning ( al ) methods , as al controls the training process of a classifier by specific querying individual data points ( samples ) , which are then labeled ( e.g . , provided with class memberships ) by a domain expert . however , an analysis of current al research shows that al still has some shortcomings . in particular , the structure information given by the spatial pattern of the ( un ) labeled data in the input space of a classification model ( e.g . , ~cluster information ) , is used in an insufficient way . in addition , many existing al techniques pay too little attention to their practical applicability . to meet these challenges , this article presents several techniques that together build a new approach for combining al and semi-supervised learning ( ssl ) for support vector machines ( svm ) in classification tasks . structure information is captured by means of probabilistic models that are iteratively improved at runtime when label information becomes available . the probabilistic models are considered in a selection strategy based on distance , density , diversity , and distribution ( 4ds strategy ) information for al and in a kernel function ( responsibility weighted mahalanobis kernel ) for svm . the approach fuses generative and discriminative modeling techniques . with 20 benchmark data sets and with the mnist data set it is shown that our new solution yields significantly better results than state-of-the-art methods .", "topics": ["data mining", "supervised learning"]}
{"title": "deep learning microscopy", "abstract": "we demonstrate that a deep neural network can significantly improve optical microscopy , enhancing its spatial resolution over a large field-of-view and depth-of-field . after its training , the only input to this network is an image acquired using a regular optical microscope , without any changes to its design . we blindly tested this deep learning approach using various tissue samples that are imaged with low-resolution and wide-field systems , where the network rapidly outputs an image with remarkably better resolution , matching the performance of higher numerical aperture lenses , also significantly surpassing their limited field-of-view and depth-of-field . these results are transformative for various fields that use microscopy tools , including e.g . , life sciences , where optical microscopy is considered as one of the most widely used and deployed techniques . beyond such applications , our presented approach is broadly applicable to other imaging modalities , also spanning different parts of the electromagnetic spectrum , and can be used to design computational imagers that get better and better as they continue to image specimen and establish new transformations among different modes of imaging .", "topics": ["numerical analysis"]}
{"title": "elliptical slice sampling", "abstract": "many probabilistic models introduce strong dependencies between variables using a latent multivariate gaussian distribution or a gaussian process . we present a new markov chain monte carlo algorithm for performing inference in models with multivariate gaussian priors . its key properties are : 1 ) it has simple , generic code applicable to many models , 2 ) it has no free parameters , 3 ) it works well for a variety of gaussian process based models . these properties make our method ideal for use while model building , removing the need to spend time deriving and tuning updates for more complex algorithms .", "topics": ["markov chain"]}
{"title": "augmentative message passing for traveling salesman problem and graph partitioning", "abstract": "the cutting plane method is an augmentative constrained optimization procedure that is often used with continuous-domain optimization techniques such as linear and convex programs . we investigate the viability of a similar idea within message passing -- which produces integral solutions -- in the context of two combinatorial problems : 1 ) for traveling salesman problem ( tsp ) , we propose a factor-graph based on held-karp formulation , with an exponential number of constraint factors , each of which has an exponential but sparse tabular form . 2 ) for graph-partitioning ( a.k.a . , community mining ) using modularity optimization , we introduce a binary variable model with a large number of constraints that enforce formation of cliques . in both cases we are able to derive surprisingly simple message updates that lead to competitive solutions on benchmark instances . in particular for tsp we are able to find near-optimal solutions in the time that empirically grows with n^3 , demonstrating that augmentation is practical and efficient .", "topics": ["time complexity", "sparse matrix"]}
{"title": "batch-normalized maxout network in network", "abstract": "this paper reports a novel deep architecture referred to as maxout network in network ( min ) , which can enhance model discriminability and facilitate the process of information abstraction within the receptive field . the proposed network adopts the framework of the recently developed network in network structure , which slides a universal approximator , multilayer perceptron ( mlp ) with rectifier units , to exact features . instead of mlp , we employ maxout mlp to learn a variety of piecewise linear activation functions and to mediate the problem of vanishing gradients that can occur when using rectifier units . moreover , batch normalization is applied to reduce the saturation of maxout units by pre-conditioning the model and dropout is applied to prevent overfitting . finally , average pooling is used in all pooling layers to regularize maxout mlp in order to facilitate information abstraction in every receptive field while tolerating the change of object position . because average pooling preserves all features in the local patch , the proposed min model can enforce the suppression of irrelevant information during training . our experiments demonstrated the state-of-the-art classification performance when the min model was applied to mnist , cifar-10 , and cifar-100 datasets and comparable performance for svhn dataset .", "topics": ["relevance", "mnist database"]}
{"title": "the in-town monitoring system for ambulance dispatch centre", "abstract": "the paper presents the vehicles integrated monitoring system giving priorities for emergency vehicles . the described system exploits the data gathered by : geographical positioning systems and geographical information systems . the digital maps and roadside cameras provide the dispatchers with aims for in town ambulances traffic management . the method of vehicles positioning in the city network and algorithms for ambulances recognition by image processing techniques have been discussed in the paper . these priorities are needed for an efficient life-saving actions that require the real-time controlling strategies .", "topics": ["image processing", "map"]}
{"title": "deep value networks learn to evaluate and iteratively refine structured outputs", "abstract": "we approach structured output prediction by optimizing a deep value network ( dvn ) to precisely estimate the task loss on different output configurations for a given input . once the model is trained , we perform inference by gradient descent on the continuous relaxations of the output variables to find outputs with promising scores from the value network . when applied to image segmentation , the value network takes an image and a segmentation mask as inputs and predicts a scalar estimating the intersection over union between the input and ground truth masks . for multi-label classification , the dvn 's objective is to correctly predict the f1 score for any potential label configuration . the dvn framework achieves the state-of-the-art results on multi-label prediction and image segmentation benchmarks .", "topics": ["image segmentation", "gradient descent"]}
{"title": "investigation on the use of hidden-markov models in automatic transcription of music", "abstract": "hidden markov models ( hmms ) are a ubiquitous tool to model time series data , and have been widely used in two main tasks of automatic music transcription ( amt ) : note segmentation , i.e . identifying the played notes after a multi-pitch estimation , and sequential post-processing , i.e . correcting note segmentation using training data . in this paper , we employ the multi-pitch estimation method called probabilistic latent component analysis ( plca ) , and develop amt systems by integrating different hmm-based modules in this framework . for note segmentation , we use two different twostate on/o ? hmms , including a higher-order one for duration modeling . for sequential post-processing , we focused on a musicological modeling of polyphonic harmonic transitions , using a first- and second-order hmms whose states are defined through candidate note mixtures . these different plca plus hmm systems have been evaluated comparatively on two different instrument repertoires , namely the piano ( using the maps database ) and the marovany zither . our results show that the use of hmms could bring noticeable improvements to transcription results , depending on the instrument repertoire .", "topics": ["test set", "time series"]}
{"title": "a frame tracking model for memory-enhanced dialogue systems", "abstract": "recently , resources and tasks were proposed to go beyond state tracking in dialogue systems . an example is the frame tracking task , which requires recording multiple frames , one for each user goal set during the dialogue . this allows a user , for instance , to compare items corresponding to different goals . this paper proposes a model which takes as input the list of frames created so far during the dialogue , the current user utterance as well as the dialogue acts , slot types , and slot values associated with this utterance . the model then outputs the frame being referenced by each triple of dialogue act , slot type , and slot value . we show that on the recently published frames dataset , this model significantly outperforms a previously proposed rule-based baseline . in addition , we propose an extensive analysis of the frame tracking task by dividing it into sub-tasks and assessing their difficulty with respect to our model .", "topics": ["baseline ( configuration management )"]}
{"title": "a weighting strategy for active shape models", "abstract": "active shape models ( asm ) are an iterative segmentation technique to find a landmark-based contour of an object . in each iteration , a least-squares fit of a plausible shape to some detected target landmarks is determined . finding these targets is a critical step : some landmarks are more reliably detected than others , and some landmarks may not be within the field of view of their detectors . to add robustness while preserving simplicity at the same time , a generalized least-squares approach can be used , where a weighting matrix incorporates reliability information about the landmarks . we propose a strategy to choose this matrix , based on the covariance of empirically determined residuals of the fit . we perform a further step to determine whether the target landmarks are within the range of their detectors . we evaluate our strategy on fluoroscopic x-ray images to segment the femur . we show that our technique outperforms the standard asm as well as other more heuristic weighted least-squares strategies .", "topics": ["iteration", "heuristic"]}
{"title": "3d hand pose detection in egocentric rgb-d images", "abstract": "we focus on the task of everyday hand pose estimation from egocentric viewpoints . for this task , we show that depth sensors are particularly informative for extracting near-field interactions of the camera wearer with his/her environment . despite the recent advances in full-body pose estimation using kinect-like sensors , reliable monocular hand pose estimation in rgb-d images is still an unsolved problem . the problem is considerably exacerbated when analyzing hands performing daily activities from a first-person viewpoint , due to severe occlusions arising from object manipulations and a limited field-of-view . our system addresses these difficulties by exploiting strong priors over viewpoint and pose in a discriminative tracking-by-detection framework . our priors are operationalized through a photorealistic synthetic model of egocentric scenes , which is used to generate training data for learning depth-based pose classifiers . we evaluate our approach on an annotated dataset of real egocentric object manipulation scenes and compare to both commercial and academic approaches . our method provides state-of-the-art performance for both hand detection and pose estimation in egocentric rgb-d images .", "topics": ["test set", "synthetic data"]}
{"title": "flattened convolutional neural networks for feedforward acceleration", "abstract": "we present flattened convolutional neural networks that are designed for fast feedforward execution . the redundancy of the parameters , especially weights of the convolutional filters in convolutional neural networks has been extensively studied and different heuristics have been proposed to construct a low rank basis of the filters after training . in this work , we train flattened networks that consist of consecutive sequence of one-dimensional filters across all directions in 3d space to obtain comparable performance as conventional convolutional networks . we tested flattened model on different datasets and found that the flattened layer can effectively substitute for the 3d filters without loss of accuracy . the flattened convolution pipelines provide around two times speed-up during feedforward pass compared to the baseline model due to the significant reduction of learning parameters . furthermore , the proposed method does not require efforts in manual tuning or post processing once the model is trained .", "topics": ["baseline ( configuration management )", "neural networks"]}
{"title": "cvae-gan : fine-grained image generation through asymmetric training", "abstract": "we present variational generative adversarial networks , a general learning framework that combines a variational auto-encoder with a generative adversarial network , for synthesizing images in fine-grained categories , such as faces of a specific person or objects in a category . our approach models an image as a composition of label and latent attributes in a probabilistic model . by varying the fine-grained category label fed into the resulting generative model , we can generate images in a specific category with randomly drawn values on a latent attribute vector . our approach has two novel aspects . first , we adopt a cross entropy loss for the discriminative and classifier network , but a mean discrepancy objective for the generative network . this kind of asymmetric loss function makes the gan training more stable . second , we adopt an encoder network to learn the relationship between the latent space and the real image space , and use pairwise feature matching to keep the structure of generated images . we experiment with natural images of faces , flowers , and birds , and demonstrate that the proposed models are capable of generating realistic and diverse samples with fine-grained category labels . we further show that our models can be applied to other tasks , such as image inpainting , super-resolution , and data augmentation for training better face recognition models .", "topics": ["generative model", "calculus of variations"]}
{"title": "simple regret for infinitely many armed bandits", "abstract": "we consider a stochastic bandit problem with infinitely many arms . in this setting , the learner has no chance of trying all the arms even once and has to dedicate its limited number of samples only to a certain number of arms . all previous algorithms for this setting were designed for minimizing the cumulative regret of the learner . in this paper , we propose an algorithm aiming at minimizing the simple regret . as in the cumulative regret setting of infinitely many armed bandits , the rate of the simple regret will depend on a parameter $ \\beta $ characterizing the distribution of the near-optimal arms . we prove that depending on $ \\beta $ , our algorithm is minimax optimal either up to a multiplicative constant or up to a $ \\log ( n ) $ factor . we also provide extensions to several important cases : when $ \\beta $ is unknown , in a natural setting where the near-optimal arms have a small variance , and in the case of unknown time horizon .", "topics": ["regret ( decision theory )"]}
{"title": "training process reduction based on potential weights linear analysis to accelarate back propagation network", "abstract": "learning is the important property of back propagation network ( bpn ) and finding the suitable weights and thresholds during training in order to improve training time as well as achieve high accuracy . currently , data pre-processing such as dimension reduction input values and pre-training are the contributing factors in developing efficient techniques for reducing training time with high accuracy and initialization of the weights is the important issue which is random and creates paradox , and leads to low accuracy with high training time . one good data preprocessing technique for accelerating bpn classification is dimension reduction technique but it has problem of missing data . in this paper , we study current pre-training techniques and new preprocessing technique called potential weight linear analysis ( pwla ) which combines normalization , dimension reduction input values and pre-training . in pwla , the first data preprocessing is performed for generating normalized input values and then applying them by pre-training technique in order to obtain the potential weights . after these phases , dimension of input values matrix will be reduced by using real potential weights . for experiment results xor problem and three datasets , which are spect heart , spectf heart and liver disorders ( bupa ) will be evaluated . our results , however , will show that the new technique of pwla will change bpn to new supervised multi layer feed forward neural network ( smffnn ) model with high accuracy in one epoch without training cycle . also pwla will be able to have power of non linear supervised and unsupervised dimension reduction property for applying by other supervised multi layer feed forward neural network model in future work .", "topics": ["statistical classification"]}
{"title": "distilling word embeddings : an encoding approach", "abstract": "distilling knowledge from a well-trained cumbersome network to a small one has recently become a new research topic , as lightweight neural networks with high performance are particularly in need in various resource-restricted systems . this paper addresses the problem of distilling word embeddings for nlp tasks . we propose an encoding approach to distill task-specific knowledge from a set of high-dimensional embeddings , which can reduce model complexity by a large margin as well as retain high accuracy , showing a good compromise between efficiency and performance . experiments in two tasks reveal the phenomenon that distilling knowledge from cumbersome embeddings is better than directly training neural networks with small embeddings .", "topics": ["natural language processing"]}
{"title": "inference of fine-grained attributes of bengali corpus for stylometry detection", "abstract": "stylometry , the science of inferring characteristics of the author from the characteristics of documents written by that author , is a problem with a long history and belongs to the core task of text categorization that involves authorship identification , plagiarism detection , forensic investigation , computer security , copyright and estate disputes etc . in this work , we present a strategy for stylometry detection of documents written in bengali . we adopt a set of fine-grained attribute features with a set of lexical markers for the analysis of the text and use three semi-supervised measures for making decisions . finally , a majority voting approach has been taken for final classification . the system is fully automatic and language-independent . evaluation results of our attempt for bengali author 's stylometry detection show reasonably promising accuracy in comparison to the baseline model .", "topics": ["baseline ( configuration management )"]}
{"title": "learning word representations from relational graphs", "abstract": "attributes of words and relations between two words are central to numerous tasks in artificial intelligence such as knowledge representation , similarity measurement , and analogy detection . often when two words share one or more attributes in common , they are connected by some semantic relations . on the other hand , if there are numerous semantic relations between two words , we can expect some of the attributes of one of the words to be inherited by the other . motivated by this close connection between attributes and relations , given a relational graph in which words are inter- connected via numerous semantic relations , we propose a method to learn a latent representation for the individual words . the proposed method considers not only the co-occurrences of words as done by existing approaches for word representation learning , but also the semantic relations in which two words co-occur . to evaluate the accuracy of the word representations learnt using the proposed method , we use the learnt word representations to solve semantic word analogy problems . our experimental results show that it is possible to learn better word representations by using semantic semantics between words .", "topics": ["feature learning", "artificial intelligence"]}
{"title": "accelerating optimization via adaptive prediction", "abstract": "we present a powerful general framework for designing data-dependent optimization algorithms , building upon and unifying recent techniques in adaptive regularization , optimistic gradient predictions , and problem-dependent randomization . we first present a series of new regret guarantees that hold at any time and under very minimal assumptions , and then show how different relaxations recover existing algorithms , both basic as well as more recent sophisticated ones . finally , we show how combining adaptivity , optimism , and problem-dependent randomization can guide the design of algorithms that benefit from more favorable guarantees than recent state-of-the-art methods .", "topics": ["regret ( decision theory )", "mathematical optimization"]}
{"title": "online deforestation detection", "abstract": "deforestation detection using satellite images can make an important contribution to forest management . current approaches can be broadly divided into those that compare two images taken at similar periods of the year and those that monitor changes by using multiple images taken during the growing season . the cmfda algorithm described in zhu et al . ( 2012 ) is an algorithm that builds on the latter category by implementing a year-long , continuous , time-series based approach to monitoring images . this algorithm was developed for 30m resolution , 16-day frequency reflectance data from the landsat satellite . in this work we adapt the algorithm to 1km , 16-day frequency reflectance data from the modis sensor aboard the terra satellite . the cmfda algorithm is composed of two submodels which are fitted on a pixel-by-pixel basis . the first estimates the amount of surface reflectance as a function of the day of the year . the second estimates the occurrence of a deforestation event by comparing the last few predicted and real reflectance values . for this comparison , the reflectance observations for six different bands are first combined into a forest index . real and predicted values of the forest index are then compared and high absolute differences for consecutive observation dates are flagged as deforestation events . our adapted algorithm also uses the two model framework . however , since the modis 13a2 dataset used , includes reflectance data for different spectral bands than those included in the landsat dataset , we can not construct the forest index . instead we propose two contrasting approaches : a multivariate and an index approach similar to that of cmfda .", "topics": ["time series", "ground truth"]}
{"title": "implementation of deep convolutional neural network in multi-class categorical image classification", "abstract": "convolutional neural networks has been implemented in many complex machine learning takes such as image classification , object identification , autonomous vehicle and robotic vision tasks . however , convnet architecture efficiency and accuracy depend on a large number of fac- tors . also , the complex architecture requires a significant amount of data to train and involves with a large number of hyperparameters that increases the computational expenses and difficul- ties . hence , it is necessary to address the limitations and techniques to overcome the barriers to ensure that the architecture performs well in complex visual tasks . this article is intended to develop an efficient convnet architecture for multi-class image categorical classification applica- tion . in the development of the architecture , large pool of grey scale images are taken as input information images and split into training and test datasets . the numerously available technique is implemented to reduce the overfitting and poor generalization of the network . the hyperpa- rameters of determined by bayesian optimization with gaussian process prior algorithm . relu non-linear activation function is implemented after the convolutional layers . max pooling op- eration is carried out to downsampling the data points in pooling layers . cross-entropy loss function is used to measure the performance of the architecture where the softmax is used in the classification layer . mini-batch gradient descent with adam optimizer algorithm is used for backpropagation . developed architecture is validated with confusion matrix and classification report .", "topics": ["neural networks", "nonlinear system"]}
{"title": "vision-and-language navigation : interpreting visually-grounded navigation instructions in real environments", "abstract": "a robot that can carry out a natural-language instruction has been a dream since before the jetsons cartoon series imagined a life of leisure mediated by a fleet of attentive robot helpers . it is a dream that remains stubbornly distant . however , recent advances in vision and language methods have made incredible progress in closely related areas . this is significant because a robot interpreting a natural-language navigation instruction on the basis of what it sees is carrying out a vision and language process that is similar to visual question answering . both tasks can be interpreted as visually grounded sequence-to-sequence translation problems , and many of the same methods are applicable . to enable and encourage the application of vision and language methods to the problem of interpreting visually-grounded navigation instructions , we present the matterport3d simulator -- a large-scale reinforcement learning environment based on real imagery . using this simulator , which can in future support a range of embodied vision and language tasks , we provide the first benchmark dataset for visually-grounded natural language navigation in real buildings -- the room-to-room ( r2r ) dataset .", "topics": ["natural language", "reinforcement learning"]}
{"title": "supervised and unsupervised tumor characterization in the deep learning era", "abstract": "computer aided diagnosis ( cad ) tools are often needed for fast and accurate detection , characterization , and risk assessment of different tumors from radiology images . any improvement in robust and accurate image-based tumor characterization can assist in determining non-invasive cancer stage , prognosis , and personalized treatment planning as a part of precision medicine . in this study , we propose both supervised and unsupervised machine learning strategies to improve tumor characterization . our first approach is based on supervised learning for which we demonstrate significant gains in deep learning algorithms , particularly convolutional neural network ( cnn ) , by utilizing completely 3d approach and transfer learning to address the requirements of volumetric and large amount of training data , respectively . motivated by the radiologists ' interpretations of the scans , we then show how to incorporate task dependent feature representations into a cad system via a graph regularized sparse multi-task learning ( mtl ) framework . in the second approach , we explore an unsupervised scheme in order to address the limited availability of labeled training data , a common problem in medical imaging applications . inspired by learning from label proportion ( llp ) approaches , we propose a new algorithm , proportion-svm , to characterize tumor types . in this second approach , we also seek the answer to the fundamental question about the goodness of `` deep features '' for unsupervised tumor classification . finally , we study the effect of unsupervised representation learning using generative adversarial networks ( gan ) on classification performance . we evaluate our proposed approaches ( both supervised and unsupervised ) on two different tumor diagnosis challenges : lung and pancreas with 1018 ct and 171 mri scans respectively .", "topics": ["feature learning", "supervised learning"]}
{"title": "structured-based curriculum learning for end-to-end english-japanese speech translation", "abstract": "sequence-to-sequence attentional-based neural network architectures have been shown to provide a powerful model for machine translation and speech recognition . recently , several works have attempted to extend the models for end-to-end speech translation task . however , the usefulness of these models were only investigated on language pairs with similar syntax and word order ( e.g . , english-french or english-spanish ) . in this work , we focus on end-to-end speech translation tasks on syntactically distant language pairs ( e.g . , english-japanese ) that require distant word reordering . to guide the encoder-decoder attentional model to learn this difficult problem , we propose a structured-based curriculum learning strategy . unlike conventional curriculum learning that gradually emphasizes difficult data examples , we formalize learning strategies from easier network structures to more difficult network structures . here , we start the training with end-to-end encoder-decoder for speech recognition or text-based machine translation task then gradually move to end-to-end speech translation task . the experiment results show that the proposed approach could provide significant improvements in comparison with the one without curriculum learning .", "topics": ["machine translation", "speech recognition"]}
{"title": "edge preserving and multi-scale contextual neural network for salient object detection", "abstract": "in this paper , we propose a novel edge preserving and multi-scale contextual neural network for salient object detection . the proposed framework is aiming to address two limits of the existing cnn based methods . first , region-based cnn methods lack sufficient context to accurately locate salient object since they deal with each region independently . second , pixel-based cnn methods suffer from blurry boundaries due to the presence of convolutional and pooling layers . motivated by these , we first propose an end-to-end edge-preserved neural network based on fast r-cnn framework ( named regionnet ) to efficiently generate saliency map with sharp object boundaries . later , to further improve it , multi-scale spatial context is attached to regionnet to consider the relationship between regions and the global scenes . furthermore , our method can be generally applied to rgb-d saliency detection by depth refinement . the proposed framework achieves both clear detection boundary and multi-scale contextual robustness simultaneously for the first time , and thus achieves an optimized performance . experiments on six rgb and two rgb-d benchmark datasets demonstrate that the proposed method achieves state-of-the-art performance .", "topics": ["object detection", "end-to-end principle"]}
{"title": "unified neural architecture for drug , disease and clinical entity recognition", "abstract": "most existing methods for biomedical entity recognition task rely on explicit feature engineering where many features either are specific to a particular task or depends on output of other existing nlp tools . neural architectures have been shown across various domains that efforts for explicit feature design can be reduced . in this work we propose an unified framework using bi-directional long short term memory network ( blstm ) for named entity recognition ( ner ) tasks in biomedical and clinical domains . three important characteristics of the framework are as follows - ( 1 ) model learns contextual as well as morphological features using two different blstm in hierarchy , ( 2 ) model uses first order linear conditional random field ( crf ) in its output layer in cascade of blstm to infer label or tag sequence , ( 3 ) model does not use any domain specific features or dictionary , i.e . , in another words , same set of features are used in the three ner tasks , namely , disease name recognition ( disease ner ) , drug name recognition ( drug ner ) and clinical entity recognition ( clinical ner ) . we compare performance of the proposed model with existing state-of-the-art models on the standard benchmark datasets of the three tasks . we show empirically that the proposed framework outperforms all existing models . further our analysis of crf layer and word-embedding obtained using character based embedding show their importance .", "topics": ["natural language processing", "dictionary"]}
{"title": "a unified multiscale framework for discrete energy minimization", "abstract": "discrete energy minimization is a ubiquitous task in computer vision , yet is np-hard in most cases . in this work we propose a multiscale framework for coping with the np-hardness of discrete optimization . our approach utilizes algebraic multiscale principles to efficiently explore the discrete solution space , yielding improved results on challenging , non-submodular energies for which current methods provide unsatisfactory approximations . in contrast to popular multiscale methods in computer vision , that builds an image pyramid , our framework acts directly on the energy to construct an energy pyramid . deriving a multiscale scheme from the energy itself makes our framework application independent and widely applicable . our framework gives rise to two complementary energy coarsening strategies : one in which coarser scales involve fewer variables , and a more revolutionary one in which the coarser scales involve fewer discrete labels . we empirically evaluated our unified framework on a variety of both non-submodular and submodular energies , including energies from middlebury benchmark .", "topics": ["computer vision", "approximation"]}
{"title": "lift : learned invariant feature transform", "abstract": "we introduce a novel deep network architecture that implements the full feature point handling pipeline , that is , detection , orientation estimation , and feature description . while previous works have successfully tackled each one of these problems individually , we show how to learn to do all three in a unified manner while preserving end-to-end differentiability . we then demonstrate that our deep pipeline outperforms state-of-the-art methods on a number of benchmark datasets , without the need of retraining .", "topics": ["end-to-end principle"]}
{"title": "optimal stochastic strongly convex optimization with a logarithmic number of projections", "abstract": "we consider stochastic strongly convex optimization with a complex inequality constraint . this complex inequality constraint may lead to computationally expensive projections in algorithmic iterations of the stochastic gradient descent~ ( sgd ) methods . to reduce the computation costs pertaining to the projections , we propose an epoch-projection stochastic gradient descent~ ( epro-sgd ) method . the proposed epro-sgd method consists of a sequence of epochs ; it applies sgd to an augmented objective function at each iteration within the epoch , and then performs a projection at the end of each epoch . given a strongly convex optimization and for a total number of $ t $ iterations , epro-sgd requires only $ \\log ( t ) $ projections , and meanwhile attains an optimal convergence rate of $ o ( 1/t ) $ , both in expectation and with a high probability . to exploit the structure of the optimization problem , we propose a proximal variant of epro-sgd , namely epro-orda , based on the optimal regularized dual averaging method . we apply the proposed methods on real-world applications ; the empirical results demonstrate the effectiveness of our methods .", "topics": ["optimization problem", "loss function"]}
{"title": "large scale distributed distance metric learning", "abstract": "in large scale machine learning and data mining problems with high feature dimensionality , the euclidean distance between data points can be uninformative , and distance metric learning ( dml ) is often desired to learn a proper similarity measure ( using side information such as example data pairs being similar or dissimilar ) . however , high dimensionality and large volume of pairwise constraints in modern big data can lead to prohibitive computational cost for both the original dml formulation in xing et al . ( 2002 ) and later extensions . in this paper , we present a distributed algorithm for dml , and a large-scale implementation on a parameter server architecture . our approach builds on a parallelizable reformulation of xing et al . ( 2002 ) , and an asynchronous stochastic gradient descent optimization procedure . to our knowledge , this is the first distributed solution to dml , and we show that , on a system with 256 cpu cores , our program is able to complete a dml task on a dataset with 1 million data points , 22-thousand features , and 200 million labeled data pairs , in 15 hours ; and the learned metric shows great effectiveness in properly measuring distances .", "topics": ["data mining", "gradient descent"]}
{"title": "from-below approximations in boolean matrix factorization : geometry and new algorithm", "abstract": "we present new results on boolean matrix factorization and a new algorithm based on these results . the results emphasize the significance of factorizations that provide from-below approximations of the input matrix . while the previously proposed algorithms do not consider the possibly different significance of different matrix entries , our results help measure such significance and suggest where to focus when computing factors . an experimental evaluation of the new algorithm on both synthetic and real data demonstrates its good performance in terms of good coverage by the first k factors as well as a small number of factors needed for exact decomposition and indicates that the algorithm outperforms the available ones in these terms . we also propose future research topics .", "topics": ["synthetic data", "approximation"]}
{"title": "optimization of distributions differences for classification", "abstract": "in this paper we introduce a new classification algorithm called optimization of distributions differences ( odd ) . the algorithm aims to find a transformation from the feature space to a new space where the instances in the same class are as close as possible to one another while the gravity centers of these classes are as far as possible from one another . this aim is formulated as a multiobjective optimization problem that is solved by a hybrid of an evolutionary strategy and the quasi-newton method . the choice of the transformation function is flexible and could be any continuous space function . we experiment with a linear and a non-linear transformation in this paper . we show that the algorithm can outperform 6 other state-of-the-art classification methods , namely naive bayes , support vector machines , linear discriminant analysis , multi-layer perceptrons , decision trees , and k-nearest neighbors , in 12 standard classification datasets . our results show that the method is less sensitive to the imbalanced number of instances comparing to these methods . we also show that odd maintains its performance better than other classification methods in these datasets , hence , offers a better generalization ability .", "topics": ["feature vector", "support vector machine"]}
{"title": "a framework for sequential planning in multi-agent settings", "abstract": "this paper extends the framework of partially observable markov decision processes ( pomdps ) to multi-agent settings by incorporating the notion of agent models into the state space . agents maintain beliefs over physical states of the environment and over models of other agents , and they use bayesian updates to maintain their beliefs over time . the solutions map belief states to actions . models of other agents may include their belief states and are related to agent types considered in games of incomplete information . we express the agents autonomy by postulating that their models are not directly manipulable or observable by other agents . we show that important properties of pomdps , such as convergence of value iteration , the rate of convergence , and piece-wise linearity and convexity of the value functions carry over to our framework . our approach complements a more traditional approach to interactive settings which uses nash equilibria as a solution paradigm . we seek to avoid some of the drawbacks of equilibria which may be non-unique and do not capture off-equilibrium behaviors . we do so at the cost of having to represent , process and continuously revise models of other agents . since the agents beliefs may be arbitrarily nested , the optimal solutions to decision making problems are only asymptotically computable . however , approximate belief updates and approximately optimal plans are computable . we illustrate our framework using a simple application domain , and we show examples of belief updates and value functions .", "topics": ["approximation algorithm", "iteration"]}
{"title": "learning and visualizing localized geometric features using 3d-cnn : an application to manufacturability analysis of drilled holes", "abstract": "3d convolutional neural networks ( 3d-cnn ) have been used for object recognition based on the voxelized shape of an object . however , interpreting the decision making process of these 3d-cnns is still an infeasible task . in this paper , we present a unique 3d-cnn based gradient-weighted class activation mapping method ( 3d-gradcam ) for visual explanations of the distinct local geometric features of interest within an object . to enable efficient learning of 3d geometries , we augment the voxel data with surface normals of the object boundary . we then train a 3d-cnn with this augmented data and identify the local features critical for decision-making using 3d gradcam . an application of this feature identification framework is to recognize difficult-to-manufacture drilled hole features in a complex cad geometry . the framework can be extended to identify difficult-to-manufacture features at multiple spatial scales leading to a real-time design for manufacturability decision support system .", "topics": ["gradient"]}
{"title": "replica exchange using q-gaussian swarm quantum particle intelligence method", "abstract": "we present a newly developed replica exchange algorithm using q -gaussian swarm quantum particle optimization ( rex @ q-gsqpo ) method for solving the problem of finding the global optimum . the basis of the algorithm is to run multiple copies of independent swarms at different values of q parameter . based on an energy criterion , chosen to satisfy the detailed balance , we are swapping the particle coordinates of neighboring swarms at regular iteration intervals . the swarm replicas with high q values are characterized by high diversity of particles allowing escaping local minima faster , while the low q replicas , characterized by low diversity of particles , are used to sample more efficiently the local basins . we compare the new algorithm with the standard gaussian swarm quantum particle optimization ( gsqpo ) and q-gaussian swarm quantum particle optimization ( q-gsqpo ) algorithms , and we found that the new algorithm is more robust in terms of the number of fitness function calls , and more efficient in terms ability convergence to the global minimum . in additional , we also provide a method of optimally allocating the swarm replicas among different q values . our algorithm is tested for three benchmark functions , which are known to be multimodal problems , at different dimensionalities . in addition , we considered a polyalanine peptide of 12 residues modeled using a g\\=o coarse-graining potential energy function .", "topics": ["mathematical optimization", "iteration"]}
{"title": "probabilistic default reasoning with conditional constraints", "abstract": "we propose a combination of probabilistic reasoning from conditional constraints with approaches to default reasoning from conditional knowledge bases . in detail , we generalize the notions of pearl 's entailment in system z , lehmann 's lexicographic entailment , and geffner 's conditional entailment to conditional constraints . we give some examples that show that the new notions of z- , lexicographic , and conditional entailment have similar properties like their classical counterparts . moreover , we show that the new notions of z- , lexicographic , and conditional entailment are proper generalizations of both their classical counterparts and the classical notion of logical entailment for conditional constraints .", "topics": ["computational complexity theory", "relevance"]}
{"title": "automatic 3d liver segmentation using sparse representation of global and local image information via level set formulation", "abstract": "in this paper , a novel framework for automated liver segmentation via a level set formulation is presented . a sparse representation of both global ( region-based ) and local ( voxel-wise ) image information is embedded in a level set formulation to innovate a new cost function . two dictionaries are build : a region-based feature dictionary and a voxel-wise dictionary . these dictionaries are learned , using the k-svd method , from a public database of liver segmentation challenge ( miccai-sliver07 ) . the learned dictionaries provide prior knowledge to the level set formulation . for the quantitative evaluation , the proposed method is evaluated using the testing data of miccai-sliver07 database . the results are evaluated using different metric scores computed by the challenge organizers . the experimental results demonstrate the superiority of the proposed framework by achieving the highest segmentation accuracy ( 79.6\\ % ) in comparison to the state-of-the-art methods .", "topics": ["loss function", "sparse matrix"]}
{"title": "a statistical perspective on algorithmic leveraging", "abstract": "one popular method for dealing with large-scale data sets is sampling . for example , by using the empirical statistical leverage scores as an importance sampling distribution , the method of algorithmic leveraging samples and rescales rows/columns of data matrices to reduce the data size before performing computations on the subproblem . this method has been successful in improving computational efficiency of algorithms for matrix problems such as least-squares approximation , least absolute deviations approximation , and low-rank matrix approximation . existing work has focused on algorithmic issues such as worst-case running times and numerical issues associated with providing high-quality implementations , but none of it addresses statistical aspects of this method . in this paper , we provide a simple yet effective framework to evaluate the statistical properties of algorithmic leveraging in the context of estimating parameters in a linear regression model with a fixed number of predictors . we show that from the statistical perspective of bias and variance , neither leverage-based sampling nor uniform sampling dominates the other . this result is particularly striking , given the well-known result that , from the algorithmic perspective of worst-case analysis , leverage-based sampling provides uniformly superior worst-case algorithmic results , when compared with uniform sampling . based on these theoretical results , we propose and analyze two new leveraging algorithms . a detailed empirical evaluation of existing leverage-based methods as well as these two new methods is carried out on both synthetic and real data sets . the empirical results indicate that our theory is a good predictor of practical performance of existing and new leverage-based algorithms and that the new algorithms achieve improved performance .", "topics": ["sampling ( signal processing )", "numerical analysis"]}
{"title": "modular continual learning in a unified visual environment", "abstract": "a core aspect of human intelligence is the ability to learn new tasks quickly and switch between them flexibly . here , we describe a modular continual reinforcement learning paradigm inspired by these abilities . we first introduce a visual interaction environment that allows many types of tasks to be unified in a single framework . we then describe a reward map prediction scheme that learns new tasks robustly in the very large state and action spaces required by such an environment . we investigate how properties of module architecture influence efficiency of task learning , showing that a module motif incorporating specific design principles ( e.g . early bottlenecks , low-order polynomial nonlinearities , and symmetry ) significantly outperforms more standard neural network motifs , needing fewer training examples and fewer neurons to achieve high levels of performance . finally , we present a meta-controller architecture for task switching based on a dynamic neural voting scheme , which allows new modules to use information learned from previously-seen tasks to substantially improve their own learning efficiency .", "topics": ["reinforcement learning", "polynomial"]}
{"title": "efficient estimation of word representations in vector space", "abstract": "we propose two novel model architectures for computing continuous vector representations of words from very large data sets . the quality of these representations is measured in a word similarity task , and the results are compared to the previously best performing techniques based on different types of neural networks . we observe large improvements in accuracy at much lower computational cost , i.e . it takes less than a day to learn high quality word vectors from a 1.6 billion words data set . furthermore , we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities .", "topics": ["test set"]}
{"title": "contrastive-center loss for deep neural networks", "abstract": "the deep convolutional neural network ( cnn ) has significantly raised the performance of image classification and face recognition . softmax is usually used as supervision , but it only penalizes the classification loss . in this paper , we propose a novel auxiliary supervision signal called contrastivecenter loss , which can further enhance the discriminative power of the features , for it learns a class center for each class . the proposed contrastive-center loss simultaneously considers intra-class compactness and inter-class separability , by penalizing the contrastive values between : ( 1 ) the distances of training samples to their corresponding class centers , and ( 2 ) the sum of the distances of training samples to their non-corresponding class centers . experiments on different datasets demonstrate the effectiveness of contrastive-center loss .", "topics": ["computer vision"]}
{"title": "normalized online learning", "abstract": "we introduce online learning algorithms which are independent of feature scales , proving regret bounds dependent on the ratio of scales existent in the data rather than the absolute scale . this has several useful effects : there is no need to pre-normalize data , the test-time and test-space complexity are reduced , and the algorithms are more robust .", "topics": ["regret ( decision theory )"]}
{"title": "sqlnet : generating structured queries from natural language without reinforcement learning", "abstract": "synthesizing sql queries from natural language is a long-standing open problem and has been attracting considerable interest recently . toward solving the problem , the de facto approach is to employ a sequence-to-sequence-style model . such an approach will necessarily require the sql queries to be serialized . since the same sql query may have multiple equivalent serializations , training a sequence-to-sequence-style model is sensitive to the choice from one of them . this phenomenon is documented as the `` order-matters '' problem . existing state-of-the-art approaches rely on reinforcement learning to reward the decoder when it generates any of the equivalent serializations . however , we observe that the improvement from reinforcement learning is limited . in this paper , we propose a novel approach , i.e . , sqlnet , to fundamentally solve this problem by avoiding the sequence-to-sequence structure when the order does not matter . in particular , we employ a sketch-based approach where the sketch contains a dependency graph so that one prediction can be done by taking into consideration only the previous predictions that it depends on . in addition , we propose a sequence-to-set model as well as the column attention mechanism to synthesize the query based on the sketch . by combining all these novel techniques , we show that sqlnet can outperform the prior art by 9 % to 13 % on the wikisql task .", "topics": ["reinforcement learning", "natural language"]}
{"title": "string and membrane gaussian processes", "abstract": "in this paper we introduce a novel framework for making exact nonparametric bayesian inference on latent functions , that is particularly suitable for big data tasks . firstly , we introduce a class of stochastic processes we refer to as string gaussian processes ( string gps ) , which are not to be mistaken for gaussian processes operating on text . we construct string gps so that their finite-dimensional marginals exhibit suitable local conditional independence structures , which allow for scalable , distributed , and flexible nonparametric bayesian inference , without resorting to approximations , and while ensuring some mild global regularity constraints . furthermore , string gp priors naturally cope with heterogeneous input data , and the gradient of the learned latent function is readily available for explanatory analysis . secondly , we provide some theoretical results relating our approach to the standard gp paradigm . in particular , we prove that some string gps are gaussian processes , which provides a complementary global perspective on our framework . finally , we derive a scalable and distributed mcmc scheme for supervised learning tasks under string gp priors . the proposed mcmc scheme has computational time complexity $ \\mathcal { o } ( n ) $ and memory requirement $ \\mathcal { o } ( dn ) $ , where $ n $ is the data size and $ d $ the dimension of the input space . we illustrate the efficacy of the proposed approach on several synthetic and real-world datasets , including a dataset with $ 6 $ millions input points and $ 8 $ attributes .", "topics": ["time complexity", "supervised learning"]}
{"title": "bayesian discovery of multiple bayesian networks via transfer learning", "abstract": "bayesian network structure learning algorithms with limited data are being used in domains such as systems biology and neuroscience to gain insight into the underlying processes that produce observed data . learning reliable networks from limited data is difficult , therefore transfer learning can improve the robustness of learned networks by leveraging data from related tasks . existing transfer learning algorithms for bayesian network structure learning give a single maximum a posteriori estimate of network models . yet , many other models may be equally likely , and so a more informative result is provided by bayesian structure discovery . bayesian structure discovery algorithms estimate posterior probabilities of structural features , such as edges . we present transfer learning for bayesian structure discovery which allows us to explore the shared and unique structural features among related tasks . efficient computation requires that our transfer learning objective factors into local calculations , which we prove is given by a broad class of transfer biases . theoretically , we show the efficiency of our approach . empirically , we show that compared to single task learning , transfer learning is better able to positively identify true edges . we apply the method to whole-brain neuroimaging data .", "topics": ["bayesian network", "computation"]}
{"title": "on some provably correct cases of variational inference for topic models", "abstract": "variational inference is a very efficient and popular heuristic used in various forms in the context of latent variable models . it 's closely related to expectation maximization ( em ) , and is applied when exact em is computationally infeasible . despite being immensely popular , current theoretical understanding of the effectiveness of variaitonal inference based algorithms is very limited . in this work we provide the first analysis of instances where variational inference algorithms converge to the global optimum , in the setting of topic models . more specifically , we show that variational inference provably learns the optimal parameters of a topic model under natural assumptions on the topic-word matrix and the topic priors . the properties that the topic word matrix must satisfy in our setting are related to the topic expansion assumption introduced in ( anandkumar et al . , 2013 ) , as well as the anchor words assumption in ( arora et al . , 2012c ) . the assumptions on the topic priors are related to the well known dirichlet prior , introduced to the area of topic modeling by ( blei et al . , 2003 ) . it is well known that initialization plays a crucial role in how well variational based algorithms perform in practice . the initializations that we use are fairly natural . one of them is similar to what is currently used in lda-c , the most popular implementation of variational inference for topic models . the other one is an overlapping clustering algorithm , inspired by a work by ( arora et al . , 2014 ) on dictionary learning , which is very simple and efficient . while our primary goal is to provide insights into when variational inference might work in practice , the multiplicative , rather than the additive nature of the variational inference updates forces us to use fairly non-standard proof arguments , which we believe will be of general interest .", "topics": ["calculus of variations", "cluster analysis"]}
{"title": "eigenvectors for clustering : unipartite , bipartite , and directed graph cases", "abstract": "this paper presents a concise tutorial on spectral clustering for broad spectrum graphs which include unipartite ( undirected ) graph , bipartite graph , and directed graph . we show how to transform bipartite graph and directed graph into corresponding unipartite graph , therefore allowing a unified treatment to all cases . in bipartite graph , we show that the relaxed solution to the $ k $ -way co-clustering can be found by computing the left and right eigenvectors of the data matrix . this gives a theoretical basis for $ k $ -way spectral co-clustering algorithms proposed in the literatures . we also show that solving row and column co-clustering is equivalent to solving row and column clustering separately , thus giving a theoretical support for the claim : `` column clustering implies row clustering and vice versa '' . and in the last part , we generalize the ky fan theorem -- -which is the central theorem for explaining spectral clustering -- -to rectangular complex matrix motivated by the results from bipartite graph analysis .", "topics": ["cluster analysis"]}
{"title": "feature based framework to detect diseases , tumor , and bleeding in wireless capsule endoscopy", "abstract": "studying animal locomotion improves our understanding of motor control and aids in the treatment of motor impairment . mice are a premier model of human disease and are the model system of choice for much of basic neuroscience . high frame rates ( 250 hz ) are needed to quantify the kinematics of these running rodents . manual tracking , especially for multiple markers , becomes time-consuming and impossible . therefore , an automated method is necessary . we propose a method to track the paws of the animal in the following manner : first , segmenting all the possible paws based on color ; second , classifying the segmented objects using a support vector machine ( svm ) and neural network ( nn ) ; third , classifying the objects using the kinematic features of the running animal , coupled with texture features from earlier frames ; and finally , detecting and handling collisions to assure the correctness of labelled paws . the proposed method is validated in sixty 1,000 frame video sequences ( 4 seconds ) captured by four cameras from five mice . the total sensitivity for tracking of the front and hind paw is 99.70 % using the svm classifier and 99.76 % using the nn classifier . in addition , we show the feasibility of 3d reconstruction using the four camera system .", "topics": ["support vector machine"]}
{"title": "influence of the learning method in the performance of feedforward neural networks when the activity of neurons is modified", "abstract": "a method that allows us to give a different treatment to any neuron inside feedforward neural networks is presented . the algorithm has been implemented with two very different learning methods : a standard back-propagation ( bp ) procedure and an evolutionary algorithm . first , we have demonstrated that the ea training method converges faster and gives more accurate results than bp . then we have made a full analysis of the effects of turning off different combinations of neurons after the training phase . we demonstrate that ea is much more robust than bp for all the cases under study . even in the case when two hidden neurons are lost , ea training is still able to give good average results . this difference implies that we must be very careful when pruning or redundancy effects are being studied since the network performance when losing neurons strongly depends on the training method . moreover , the influence of the individual inputs will also depend on the training algorithm . since ea keeps a good classification performance when units are lost , this method could be a good way to simulate biological learning systems since they must be robust against deficient neuron performance . although biological systems are much more complex than the simulations shown in this article , we propose that a smart training strategy such as the one shown here could be considered as a first protection against the losing of a certain number of neurons .", "topics": ["simulation"]}
{"title": "improving implicit semantic role labeling by predicting semantic frame arguments", "abstract": "implicit semantic role labeling ( isrl ) is the task of predicting the semantic roles of a predicate that do not appear as explicit arguments , but rather regard common sense knowledge or are mentioned earlier in the discourse . we introduce an approach to isrl based on a predictive recurrent neural semantic frame model ( prnsfm ) that uses a large unannotated corpus to learn the probability of a sequence of semantic arguments given a predicate . we leverage the sequence probabilities predicted by the prnsfm to estimate selectional preferences for predicates and their arguments . on the nombank isrl test set , our approach improves state-of-the-art performance on implicit semantic role labeling with less reliance than prior work on manually constructed language resources .", "topics": ["test set"]}
{"title": "improving statistical machine translation for a resource-poor language using related resource-rich languages", "abstract": "we propose a novel language-independent approach for improving machine translation for resource-poor languages by exploiting their similarity to resource-rich ones . more precisely , we improve the translation from a resource-poor source language x_1 into a resource-rich language y given a bi-text containing a limited number of parallel sentences for x_1-y and a larger bi-text for x_2-y for some resource-rich language x_2 that is closely related to x_1 . this is achieved by taking advantage of the opportunities that vocabulary overlap and similarities between the languages x_1 and x_2 in spelling , word order , and syntax offer : ( 1 ) we improve the word alignments for the resource-poor language , ( 2 ) we further augment it with additional translation options , and ( 3 ) we take care of potential spelling differences through appropriate transliteration . the evaluation for indonesian- > english using malay and for spanish - > english using portuguese and pretending spanish is resource-poor shows an absolute gain of up to 1.35 and 3.37 bleu points , respectively , which is an improvement over the best rivaling approaches , while using much less additional data . overall , our method cuts the amount of necessary `` real training data by a factor of 2 -- 5 .", "topics": ["test set", "machine translation"]}
{"title": "what uncertainties do we need in bayesian deep learning for computer vision ?", "abstract": "there are two major types of uncertainty one can model . aleatoric uncertainty captures noise inherent in the observations . on the other hand , epistemic uncertainty accounts for uncertainty in the model -- uncertainty which can be explained away given enough data . traditionally it has been difficult to model epistemic uncertainty in computer vision , but with new bayesian deep learning tools this is now possible . we study the benefits of modeling epistemic vs . aleatoric uncertainty in bayesian deep learning models for vision tasks . for this we present a bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty . we study models under the framework with per-pixel semantic segmentation and depth regression tasks . further , our explicit uncertainty formulation leads to new loss functions for these tasks , which can be interpreted as learned attenuation . this makes the loss more robust to noisy data , also giving new state-of-the-art results on segmentation and depth regression benchmarks .", "topics": ["computer vision", "loss function"]}
{"title": "a study for the effect of the emphaticness and language and dialect for voice onset time ( vot ) in modern standard arabic ( msa )", "abstract": "the signal sound contains many different features , including voice onset time ( vot ) , which is a very important feature of stop sounds in many languages . the only application of vot values is stopping phoneme subsets . this subset of consonant sounds is stop phonemes exist in the arabic language , and in fact , all languages . the pronunciation of these sounds is hard and unique especially for less-educated arabs and non-native arabic speakers . vot can be utilized by the human auditory system to distinguish between voiced and unvoiced stops such as /p/ and /b/ in english.this search focuses on computing and analyzing vot of modern standard arabic ( msa ) , within the arabic language , for all pairs of non-emphatic ( namely , /d/ and /t/ ) and emphatic pairs ( namely , /d ? / and /t ? / ) depending on carrier words . this research uses a database built by ourselves , and uses the carrier words syllable structure : cv-cv-cv . one of the main outcomes always found is the emphatic sounds ( /d ? / , /t ? / ) are less than 50 % of non-emphatic ( counter-part ) sounds ( /d/ , /t/ ) .also , vot can be used to classify or detect for a dialect ina language .", "topics": ["database"]}
{"title": "performance analysis of neuro genetic algorithm applied on detecting proportion of components in manhole gas mixture", "abstract": "the article presents performance analysis of a real valued neuro genetic algorithm applied for the detection of proportion of the gases found in manhole gas mixture . the neural network ( nn ) trained using genetic algorithm ( ga ) leads to concept of neuro genetic algorithm , which is used for implementing an intelligent sensory system for the detection of component gases present in manhole gas mixture usually a manhole gas mixture contains several toxic gases like hydrogen sulfide , ammonia , methane , carbon dioxide , nitrogen oxide , and carbon monoxide . a semiconductor based gas sensor array used for sensing manhole gas components is an integral part of the proposed intelligent system . it consists of many sensor elements , where each sensor element is responsible for sensing particular gas component . multiple sensors of different gases used for detecting gas mixture of multiple gases , results in cross-sensitivity . the cross-sensitivity is a major issue and the problem is viewed as pattern recognition problem . the objective of this article is to present performance analysis of the real valued neuro genetic algorithm which is applied for multiple gas detection .", "topics": ["sensor", "artificial intelligence"]}
{"title": "identifying the relevant nodes without learning the model", "abstract": "we propose a method to identify all the nodes that are relevant to compute all the conditional probability distributions for a given set of nodes . our method is simple , effcient , consistent , and does not require learning a bayesian network first . therefore , our method can be applied to high-dimensional databases , e.g . gene expression databases .", "topics": ["bayesian network", "database"]}
{"title": "autonomous 3d reconstruction using a mav", "abstract": "an approach is proposed for high resolution 3d reconstruction of an object using a micro air vehicle ( mav ) . a system is described which autonomously captures images and performs a dense 3d reconstruction via structure from motion with no prior knowledge of the environment . only the mavs own sensors , the front facing camera and the inertial measurement unit ( imu ) are utilized . precision agriculture is considered as an example application for the system .", "topics": ["sensor", "autonomous car"]}
{"title": "modelling of directional data using kent distributions", "abstract": "the modelling of data on a spherical surface requires the consideration of directional probability distributions . to model asymmetrically distributed data on a three-dimensional sphere , kent distributions are often used . the moment estimates of the parameters are typically used in modelling tasks involving kent distributions . however , these lack a rigorous statistical treatment . the focus of the paper is to introduce a bayesian estimation of the parameters of the kent distribution which has not been carried out in the literature , partly because of its complex mathematical form . we employ the bayesian information-theoretic paradigm of minimum message length ( mml ) to bridge this gap and derive reliable estimators . the inferred parameters are subsequently used in mixture modelling of kent distributions . the problem of inferring the suitable number of mixture components is also addressed using the mml criterion . we demonstrate the superior performance of the derived mml-based parameter estimates against the traditional estimators . we apply the mml principle to infer mixtures of kent distributions to model empirical data corresponding to protein conformations . we demonstrate the effectiveness of kent models to act as improved descriptors of protein structural data as compared to commonly used von mises-fisher distributions .", "topics": ["bayesian network", "eisenstein 's criterion"]}
{"title": "a divide-and-conquer strategy for parsing", "abstract": "in this paper , we propose a novel strategy which is designed to enhance the accuracy of the parser by simplifying complex sentences before parsing . this approach involves the separate parsing of the constituent sub-sentences within a complex sentence . to achieve that , the divide-and-conquer strategy first disambiguates the roles of the link words in the sentence and segments the sentence based on these roles . the separate parse trees of the segmented sub-sentences and the noun phrases within them are then synthesized to form the final parse . to evaluate the effects of this strategy on parsing , we compare the original performance of a dependency parser with the performance when it is enhanced with the divide-and-conquer strategy . when tested on 600 sentences of the ipsm'95 data sets , the enhanced parser saw a considerable error reduction of 21.2 % in its accuracy .", "topics": ["parsing"]}
{"title": "generalized quantum reinforcement learning with quantum technologies", "abstract": "we propose a protocol to perform generalized quantum reinforcement learning with quantum technologies . at variance with recent results on quantum reinforcement learning with superconducting circuits [ l. lamata , sci . rep. 7 , 1609 ( 2017 ) ] , in our current protocol coherent feedback during the learning process is not required , enabling its implementation in a wide variety of quantum systems . we consider diverse possible scenarios for an agent , an environment , and a register that connects them , involving multiqubit and multilevel systems , as well as open-system dynamics . we finally propose possible implementations of this protocol in trapped ions and superconducting circuits . the field of quantum reinforcement learning with quantum technologies will enable enhanced quantum control , as well as more efficient machine learning calculations .", "topics": ["reinforcement learning"]}
{"title": "alpha-divergences in variational dropout", "abstract": "we investigate the use of alternative divergences to kullback-leibler ( kl ) in variational inference ( vi ) , based on the variational dropout \\cite { kingma2015 } . stochastic gradient variational bayes ( sgvb ) \\cite { aevb } is a general framework for estimating the evidence lower bound ( elbo ) in variational bayes . in this work , we extend the sgvb estimator with using alpha-divergences , which are alternative to divergences to vi ' kl objective . the gaussian dropout can be seen as a local reparametrization trick of the sgvb objective . we extend the variational dropout to use alpha divergences for variational inference . our results compare $ \\alpha $ -divergence variational dropout with standard variational dropout with correlated and uncorrelated weight noise . we show that the $ \\alpha $ -divergence with $ \\alpha \\rightarrow 1 $ ( or kl divergence ) is still a good measure for use in variational inference , in spite of the efficient use of alpha-divergences for dropout vi \\cite { li17 } . $ \\alpha \\rightarrow 1 $ can yield the lowest training error , and optimizes a good lower bound for the evidence lower bound ( elbo ) among all values of the parameter $ \\alpha \\in [ 0 , \\infty ) $ .", "topics": ["calculus of variations", "gradient"]}
{"title": "learning structured sparsity in deep neural networks", "abstract": "high demand for computation resources severely hinders deployment of large-scale deep neural networks ( dnn ) in resource constrained devices . in this work , we propose a structured sparsity learning ( ssl ) method to regularize the structures ( i.e . , filters , channels , filter shapes , and layer depth ) of dnns . ssl can : ( 1 ) learn a compact structure from a bigger dnn to reduce computation cost ; ( 2 ) obtain a hardware-friendly structured sparsity of dnn to efficiently accelerate the dnns evaluation . experimental results show that ssl achieves on average 5.1x and 3.1x speedups of convolutional layer computation of alexnet against cpu and gpu , respectively , with off-the-shelf libraries . these speedups are about twice speedups of non-structured sparsity ; ( 3 ) regularize the dnn structure to improve classification accuracy . the results show that for cifar-10 , regularization on layer depth can reduce 20 layers of a deep residual network ( resnet ) to 18 layers while improve the accuracy from 91.25 % to 92.60 % , which is still slightly higher than that of original resnet with 32 layers . for alexnet , structure regularization by ssl also reduces the error by around ~1 % . open source code is in https : //github.com/wenwei202/caffe/tree/scnn", "topics": ["neural networks", "matrix regularization"]}
{"title": "a fresnelet-based encryption of medical images using arnold transform", "abstract": "medical images are commonly stored in digital media and transmitted via internet for certain uses . if a medical information image alters , this can lead to a wrong diagnosis which may create a serious health problem . moreover , medical images in digital form can easily be modified by wiping off or adding small pieces of information intentionally for certain illegal purposes . hence , the reliability of medical images is an important criterion in a hospital information system . in this paper , fresnelet transform is employed along with appropriate handling of the arnold transform and the discrete cosine transform to provide secure distribution of medical images . this method presents a new data hiding system in which steganography and cryptography are used to prevent unauthorized data access . the experimental results exhibit high imperceptibility for embedded images and significant encryption of information images .", "topics": ["eisenstein 's criterion"]}
{"title": "private posterior distributions from variational approximations", "abstract": "privacy preserving mechanisms such as differential privacy inject additional randomness in the form of noise in the data , beyond the sampling mechanism . ignoring this additional noise can lead to inaccurate and invalid inferences . in this paper , we incorporate the privacy mechanism explicitly into the likelihood function by treating the original data as missing , with an end goal of estimating posterior distributions over model parameters . this leads to a principled way of performing valid statistical inference using private data , however , the corresponding likelihoods are intractable . in this paper , we derive fast and accurate variational approximations to tackle such intractable likelihoods that arise due to privacy . we focus on estimating posterior distributions of parameters of the naive bayes log-linear model , where the sufficient statistics of this model are shared using a differentially private interface . using a simulation study , we show that the posterior approximations outperform the naive method of ignoring the noise addition mechanism .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "lidar-camera calibration using 3d-3d point correspondences", "abstract": "with the advent of autonomous vehicles , lidar and cameras have become an indispensable combination of sensors . they both provide rich and complementary data which can be used by various algorithms and machine learning to sense and make vital inferences about the surroundings . we propose a novel pipeline and experimental setup to find accurate rigid-body transformation for extrinsically calibrating a lidar and a camera . the pipeling uses 3d-3d point correspondences in lidar and camera frame and gives a closed form solution . we further show the accuracy of the estimate by fusing point clouds from two stereo cameras which align perfectly with the rotation and translation estimated by our method , confirming the accuracy of our method 's estimates both mathematically and visually . taking our idea of extrinsic lidar-camera calibration forward , we demonstrate how two cameras with no overlapping field-of-view can also be calibrated extrinsically using 3d point correspondences . the code has been made available as open-source software in the form of a ros package , more information about which can be sought here : https : //github.com/ankitdhall/lidar_camera_calibration .", "topics": ["sensor", "autonomous car"]}
{"title": "the relation between acausality and interference in quantum-like bayesian networks", "abstract": "we analyse a quantum-like bayesian network that puts together cause/effect relationships and semantic similarities between events . these semantic similarities constitute acausal connections according to the synchronicity principle and provide new relationships to quantum like probabilistic graphical models . as a consequence , beliefs ( or any other event ) can be represented in vector spaces , in which quantum parameters are determined by the similarities that these vectors share between them . events attached by a semantic meaning do not need to have an explanation in terms of cause and effect .", "topics": ["graphical model", "bayesian network"]}
{"title": "datasheets for datasets", "abstract": "currently there is no standard way to identify how a dataset was created , and what characteristics , motivations , and potential skews it represents . to begin to address this issue , we propose the concept of a datasheet for datasets , a short document to accompany public datasets , commercial apis , and pretrained models . the goal of this proposal is to enable better communication between dataset creators and users , and help the ai community move toward greater transparency and accountability . by analogy , in computer hardware , it has become industry standard to accompany everything from the simplest components ( e.g . , resistors ) , to the most complex microprocessor chips , with datasheets detailing standard operating characteristics , test results , recommended usage , and other information . we outline some of the questions a datasheet for datasets should answer . these questions focus on when , where , and how the training data was gathered , its recommended use cases , and , in the case of human-centric datasets , information regarding the subjects ' demographics and consent as applicable . we develop prototypes of datasheets for two well-known datasets : labeled faces in the wild~\\cite { lfw } and the pang \\ & lee polarity dataset~\\cite { polarity } .", "topics": ["test set"]}
{"title": "hierarchical policy search via return-weighted density estimation", "abstract": "learning an optimal policy from a multi-modal reward function is a challenging problem in reinforcement learning ( rl ) . hierarchical rl ( hrl ) tackles this problem by learning a hierarchical policy , where multiple option policies are in charge of different strategies corresponding to modes of a reward function and a gating policy selects the best option for a given context . although hrl has been demonstrated to be promising , current state-of-the-art methods can not still perform well in complex real-world problems due to the difficulty of identifying modes of the reward function . in this paper , we propose a novel method called hierarchical policy search via return-weighted density estimation ( hpsde ) , which can efficiently identify the modes through density estimation with return-weighted importance sampling . our proposed method finds option policies corresponding to the modes of the return function and automatically determines the number and the location of option policies , which significantly reduces the burden of hyper-parameters tuning . through experiments , we demonstrate that the proposed hpsde successfully learns option policies corresponding to modes of the return function and that it can be successfully applied to a challenging motion planning problem of a redundant robotic manipulator .", "topics": ["sampling ( signal processing )", "mathematical optimization"]}
{"title": "reasoning about actions with temporal answer sets", "abstract": "in this paper we combine answer set programming ( asp ) with dynamic linear time temporal logic ( dltl ) to define a temporal logic programming language for reasoning about complex actions and infinite computations . dltl extends propositional temporal logic of linear time with regular programs of propositional dynamic logic , which are used for indexing temporal modalities . the action language allows general dltl formulas to be included in domain descriptions to constrain the space of possible extensions . we introduce a notion of temporal answer set for domain descriptions , based on the usual notion of answer set . also , we provide a translation of domain descriptions into standard asp and we use bounded model checking techniques for the verification of dltl constraints .", "topics": ["time complexity"]}
{"title": "concatenated $ p $ -mean word embeddings as universal cross-lingual sentence representations", "abstract": "average word embeddings are a common baseline for more sophisticated sentence embedding techniques . an important advantage of average word embeddings is their computational and conceptual simplicity . however , they typically fall short of the performances of more complex models such as infersent . here , we generalize the concept of average word embeddings to $ p $ -mean word embeddings , which are ( almost ) as efficiently computable . we show that the concatenation of different types of $ p $ -mean word embeddings considerably closes the gap to state-of-the-art methods such as infersent monolingually and substantially outperforms these more complex techniques cross-lingually . in addition , our proposed method outperforms different recently proposed baselines such as sif and sent2vec by a solid margin , thus constituting a much harder-to-beat monolingual baseline for a wide variety of transfer tasks . our data and code are publicly available .", "topics": ["baseline ( configuration management )"]}
{"title": "tgif-qa : toward spatio-temporal reasoning in visual question answering", "abstract": "vision and language understanding has emerged as a subject undergoing intense study in artificial intelligence . among many tasks in this line of research , visual question answering ( vqa ) has been one of the most successful ones , where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form . despite the rapid progress in the past few years , most existing work in vqa have focused primarily on images . in this paper , we focus on extending vqa to the video domain and contribute to the literature in three important ways . first , we propose three new tasks designed specifically for video vqa , which require spatio-temporal reasoning from videos to answer questions correctly . next , we introduce a new large-scale dataset for video vqa named tgif-qa that extends existing vqa work with our new tasks . finally , we propose a dual-lstm based approach with both spatial and temporal attention , and show its effectiveness over conventional vqa techniques through empirical evaluations .", "topics": ["natural language", "artificial intelligence"]}
{"title": "natural language inference for arabic using extended tree edit distance with subtrees", "abstract": "many natural language processing ( nlp ) applications require the computation of similarities between pairs of syntactic or semantic trees . many researchers have used tree edit distance for this task , but this technique suffers from the drawback that it deals with single node operations only . we have extended the standard tree edit distance algorithm to deal with subtree transformation operations as well as single nodes . the extended algorithm with subtree operations , ted+st , is more effective and flexible than the standard algorithm , especially for applications that pay attention to relations among nodes ( e.g . in linguistic trees , deleting a modifier subtree should be cheaper than the sum of deleting its components individually ) . we describe the use of ted+st for checking entailment between two arabic text snippets . the preliminary results of using ted+st were encouraging when compared with two string-based approaches and with the standard algorithm .", "topics": ["natural language processing", "natural language"]}
{"title": "genetic programming , validation sets , and parsimony pressure", "abstract": "fitness functions based on test cases are very common in genetic programming ( gp ) . this process can be assimilated to a learning task , with the inference of models from a limited number of samples . this paper is an investigation on two methods to improve generalization in gp-based learning : 1 ) the selection of the best-of-run individuals using a three data sets methodology , and 2 ) the application of parsimony pressure in order to reduce the complexity of the solutions . results using gp in a binary classification setup show that while the accuracy on the test sets is preserved , with less variances compared to baseline results , the mean tree size obtained with the tested methods is significantly reduced .", "topics": ["baseline ( configuration management )", "supervised learning"]}
{"title": "sub-dividing genetic method for optimization problems", "abstract": "nowadays , optimization problem have more application in all major but they have problem in computation . computation global point in continuous functions have high calculation and this became clearer in large space .in this paper , we proposed sub- dividing genetic method ( sgm ) that have less computation than other method for achieving global points . this method userotation mutation and crossover based sub-division method that sub diving method is used for minimize search space and rotation mutation with crossover is used for finding global optimal points . in experimental , sgm algorithm is implemented on de jong function . the numerical examples show that sgm is performed more optimal than other methods such as grefensstette , random value , and png .", "topics": ["optimization problem", "numerical analysis"]}
{"title": "combinatorial multi-armed bandits for real-time strategy games", "abstract": "games with large branching factors pose a significant challenge for game tree search algorithms . in this paper , we address this problem with a sampling strategy for monte carlo tree search ( mcts ) algorithms called { \\em na\\ '' { i } ve sampling } , based on a variant of the multi-armed bandit problem called { \\em combinatorial multi-armed bandits } ( cmab ) . we analyze the theoretical properties of several variants of { \\em na\\ '' { i } ve sampling } , and empirically compare it against the other existing strategies in the literature for cmabs . we then evaluate these strategies in the context of real-time strategy ( rts ) games , a genre of computer games characterized by their very large branching factors . our results show that as the branching factor grows , { \\em na\\ '' { i } ve sampling } outperforms the other sampling strategies .", "topics": ["sampling ( signal processing )"]}
{"title": "a framework for improving the performance of verification algorithms with a low false positive rate requirement and limited training data", "abstract": "in this paper we address the problem of matching patterns in the so-called verification setting in which a novel , query pattern is verified against a single training pattern : the decision sought is whether the two match ( i.e . belong to the same class ) or not . unlike previous work which has universally focused on the development of more discriminative distance functions between patterns , here we consider the equally important and pervasive task of selecting a distance threshold which fits a particular operational requirement - specifically , the target false positive rate ( fpr ) . first , we argue on theoretical grounds that a data-driven approach is inherently ill-conditioned when the desired fpr is low , because by the very nature of the challenge only a small portion of training data affects or is affected by the desired threshold . this leads us to propose a general , statistical model-based method instead . our approach is based on the interpretation of an inter-pattern distance as implicitly defining a pattern embedding which approximately distributes patterns according to an isotropic multi-variate normal distribution in some space . this interpretation is then used to show that the distribution of training inter-pattern distances is the non-central chi2 distribution , differently parameterized for each class . thus , to make the class-specific threshold choice we propose a novel analysis-by-synthesis iterative algorithm which estimates the three free parameters of the model ( for each class ) using task-specific constraints . the validity of the premises of our work and the effectiveness of the proposed method are demonstrated by applying the method to the task of set-based face verification on a large database of pseudo-random head motion videos .", "topics": ["test set", "database"]}
{"title": "a triclustering approach for time evolving graphs", "abstract": "this paper introduces a novel technique to track structures in time evolving graphs . the method is based on a parameter free approach for three-dimensional co-clustering of the source vertices , the target vertices and the time . all these features are simultaneously segmented in order to build time segments and clusters of vertices whose edge distributions are similar and evolve in the same way over the time segments . the main novelty of this approach lies in that the time segments are directly inferred from the evolution of the edge distribution between the vertices , thus not requiring the user to make an a priori discretization . experiments conducted on a synthetic dataset illustrate the good behaviour of the technique , and a study of a real-life dataset shows the potential of the proposed approach for exploratory data analysis .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "mining compatible/incompatible entities from question and answering via yes/no answer classification using distant label expansion", "abstract": "product community question answering ( pcqa ) provides useful information about products and their features ( aspects ) that may not be well addressed by product descriptions and reviews . we observe that a product 's compatibility issues with other products are frequently discussed in pcqa and such issues are more frequently addressed in accessories , i.e . , via a yes/no question `` does this mouse work with windows 10 ? '' . in this paper , we address the problem of extracting compatible and incompatible products from yes/no questions in pcqa . this problem can naturally have a two-stage framework : first , we perform complementary entity ( product ) recognition ( cer ) on yes/no questions ; second , we identify the polarities of yes/no answers to assign the complementary entities a compatibility label ( compatible , incompatible or unknown ) . we leverage an existing unsupervised method for the first stage and a 3-class classifier by combining a distant pu-learning method ( learning from positive and unlabeled examples ) together with a binary classifier for the second stage . the benefit of using distant pu-learning is that it can help to expand more implicit yes/no answers without using any human annotated data . we conduct experiments on 4 products to show that the proposed method is effective .", "topics": ["unsupervised learning", "entity"]}
{"title": "lamarckism and mechanism synthesis : approaching constrained optimization with ideas from biology", "abstract": "nonlinear constrained optimization problems are encountered in many scientific fields . to utilize the huge calculation power of current computers , many mathematic models are also rebuilt as optimization problems . most of them have constrained conditions which need to be handled . borrowing biological concepts , a study is accomplished for dealing with the constraints in the synthesis of a four-bar mechanism . biologically regarding the constrained condition as a form of selection for characteristics of a population , four new algorithms are proposed , and a new explanation is given for the penalty method . using these algorithms , three cases are tested in differential-evolution based programs . better , or comparable , results show that the presented algorithms and methodology may become common means for constraint handling in optimization problems .", "topics": ["mathematical optimization", "optimization problem"]}
{"title": "improving the csiec project and adapting it to the english teaching and learning in china", "abstract": "in this paper after short review of the csiec project initialized by us in 2003 we present the continuing development and improvement of the csiec project in details , including the design of five new microsoft agent characters representing different virtual chatting partners and the limitation of simulated dialogs in specific practical scenarios like graduate job application interview , then briefly analyze the actual conditions and features of its application field : web-based english education in china . finally we introduce our efforts to adapt this system to the requirements of english teaching and learning in china and point out the work next to do .", "topics": ["simulation"]}
{"title": "peyma : a tagged corpus for persian named entities", "abstract": "the goal in the ner task is to classify proper nouns of a text into classes such as person , location , and organization . this is an important preprocessing step in many nlp tasks such as question-answering and summarization . although many research studies have been conducted in this area in english and the state-of-the-art ner systems have reached performances of higher than 90 percent in terms of f1 measure , there are very few research studies for this task in persian . one of the main important causes of this may be the lack of a standard persian ner dataset to train and test ner systems . in this research we create a standard , big-enough tagged persian ner dataset which will be distributed for free for research purposes . in order to construct such a standard dataset , we studied standard ner datasets which are constructed for english researches and found out that almost all of these datasets are constructed using news texts . so we collected documents from ten news websites . later , in order to provide annotators with some guidelines to tag these documents , after studying guidelines used for constructing conll and muc standard english datasets , we set our own guidelines considering the persian linguistic rules .", "topics": ["natural language processing", "natural language"]}
{"title": "tagger evaluation given hierarchical tag sets", "abstract": "we present methods for evaluating human and automatic taggers that extend current practice in three ways . first , we show how to evaluate taggers that assign multiple tags to each test instance , even if they do not assign probabilities . second , we show how to accommodate a common property of manually constructed `` gold standards '' that are typically used for objective evaluation , namely that there is often more than one correct answer . third , we show how to measure performance when the set of possible tags is tree-structured in an is-a hierarchy . to illustrate how our methods can be used to measure inter-annotator agreement , we show how to compute the kappa coefficient over hierarchical tag sets .", "topics": ["coefficient"]}
{"title": "on the influence of dice loss function in multi-class organ segmentation of abdominal ct using 3d fully convolutional networks", "abstract": "deep learning-based methods achieved impressive results for the segmentation of medical images . with the development of 3d fully convolutional networks ( fcns ) , it has become feasible to produce improved results for multi-organ segmentation of 3d computed tomography ( ct ) images . the results of multi-organ segmentation using deep learning-based methods not only depend on the choice of networks architecture , but also strongly rely on the choice of loss function . in this paper , we present a discussion on the influence of dice-based loss functions for multi-class organ segmentation using a dataset of abdominal ct volumes . we investigated three different types of weighting the dice loss functions based on class label frequencies ( uniform , simple and square ) and evaluate their influence on segmentation accuracies . furthermore , we compared the influence of different initial learning rates . we achieved average dice scores of 81.3 % , 59.5 % and 31.7 % for uniform , simple and square types of weighting when the learning rate is 0.001 , and 78.2 % , 81.0 % and 58.5 % for each weighting when the learning rate is 0.01 . our experiments indicated a strong relationship between class balancing weights and initial learning rate in training .", "topics": ["loss function"]}
{"title": "global deconvolutional networks for semantic segmentation", "abstract": "semantic image segmentation is a principal problem in computer vision , where the aim is to correctly classify each individual pixel of an image into a semantic label . its widespread use in many areas , including medical imaging and autonomous driving , has fostered extensive research in recent years . empirical improvements in tackling this task have primarily been motivated by successful exploitation of convolutional neural networks ( cnns ) pre-trained for image classification and object recognition . however , the pixel-wise labelling with cnns has its own unique challenges : ( 1 ) an accurate deconvolution , or upsampling , of low-resolution output into a higher-resolution segmentation mask and ( 2 ) an inclusion of global information , or context , within locally extracted features . to address these issues , we propose a novel architecture to conduct the equivalent of the deconvolution operation globally and acquire dense predictions . we demonstrate that it leads to improved performance of state-of-the-art semantic segmentation models on the pascal voc 2012 benchmark , reaching 74.0 % mean iu accuracy on the test set .", "topics": ["image segmentation", "graphical model"]}
{"title": "accelerated gradient temporal difference learning", "abstract": "the family of temporal difference ( td ) methods span a spectrum from computationally frugal linear methods like td ( { \\lambda } ) to data efficient least squares methods . least square methods make the best use of available data directly computing the td solution and thus do not require tuning a typically highly sensitive learning rate parameter , but require quadratic computation and storage . recent algorithmic developments have yielded several sub-quadratic methods that use an approximation to the least squares td solution , but incur bias . in this paper , we propose a new family of accelerated gradient td ( atd ) methods that ( 1 ) provide similar data efficiency benefits to least-squares methods , at a fraction of the computation and storage ( 2 ) significantly reduce parameter sensitivity compared to linear td methods , and ( 3 ) are asymptotically unbiased . we illustrate these claims with a proof of convergence in expectation and experiments on several benchmark domains and a large-scale industrial energy allocation domain .", "topics": ["computation", "gradient"]}
{"title": "short-term solar irradiance and irradiation forecasts via different time series techniques : a preliminary study", "abstract": "this communication is devoted to solar irradiance and irradiation short-term forecasts , which are useful for electricity production . several different time series approaches are employed . our results and the corresponding numerical simulations show that techniques which do not need a large amount of historical data behave better than those which need them , especially when those data are quite noisy .", "topics": ["time series", "numerical analysis"]}
{"title": "cubic range error model for stereo vision with illuminators", "abstract": "use of low-cost depth sensors , such as a stereo camera setup with illuminators , is of particular interest for numerous applications ranging from robotics and transportation to mixed and augmented reality . the ability to quantify noise is crucial for these applications , e.g . , when the sensor is used for map generation or to develop a sensor scheduling policy in a multi-sensor setup . range error models provide uncertainty estimates and help weigh the data correctly in instances where range measurements are taken from different vantage points or with different sensors . the weighing is important to fuse range data into a map in a meaningful way , i.e . , the high confidence data is relied on most heavily . such a model is derived in this work . we show that the range error for stereo systems with integrated illuminators is cubic and validate the proposed model experimentally with an off-the-shelf structured light stereo system . the experiments confirm the validity of the model and simplify the application of this type of sensor in robotics . the proposed error model is relevant to any stereo system with low ambient light where the main light source is located at the camera system . among others , this is the case for structured light stereo systems and night stereo systems with headlights . in this work , we propose that the range error is cubic in range for stereo systems with integrated illuminators . experimental validation with an off-the-shelf structured light stereo system shows that the exponent is between 2.4 and 2.6 . the deviation is attributed to our model considering only shot noise .", "topics": ["sensor"]}
{"title": "san francisco crime classification", "abstract": "san francisco crime classification is an online competition administered by kaggle inc . the competition aims at predicting the future crimes based on a given set of geographical and time-based features . in this paper , i achieved a an accuracy that ranks at top % 18 , as of may 19th , 2016 . i will explore the data , and explain in details the tools i used to achieve that result .", "topics": ["gradient"]}
{"title": "a deep convolutional neural network for background subtraction", "abstract": "in this work , we present a novel background subtraction system that uses a deep convolutional neural network ( cnn ) to perform the segmentation . with this approach , feature engineering and parameter tuning become unnecessary since the network parameters can be learned from data by training a single cnn that can handle various video scenes . additionally , we propose a new approach to estimate background model from video . for the training of the cnn , we employed randomly 5 percent video frames and their ground truth segmentations taken from the change detection challenge 2014 ( cdnet 2014 ) . we also utilized spatial-median filtering as the post-processing of the network outputs . our method is evaluated with different data-sets , and the network outperforms the existing algorithms with respect to the average ranking over different evaluation metrics . furthermore , due to the network architecture , our cnn is capable of real time processing .", "topics": ["ground truth"]}
{"title": "summarizing decisions in spoken meetings", "abstract": "this paper addresses the problem of summarizing decisions in spoken meetings : our goal is to produce a concise { \\it decision abstract } for each meeting decision . we explore and compare token-level and dialogue act-level automatic summarization methods using both unsupervised and supervised learning frameworks . in the supervised summarization setting , and given true clusterings of decision-related utterances , we find that token-level summaries that employ discourse context can approach an upper bound for decision abstracts derived directly from dialogue acts . in the unsupervised summarization setting , we find that summaries based on unsupervised partitioning of decision-related utterances perform comparably to those based on partitions generated using supervised techniques ( 0.22 rouge-f1 using lda-based topic models vs. 0.23 using svms ) .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "data collection for interactive learning through the dialog", "abstract": "this paper presents a dataset collected from natural dialogs which enables to test the ability of dialog systems to learn new facts from user utterances throughout the dialog . this interactive learning will help with one of the most prevailing problems of open domain dialog system , which is the sparsity of facts a dialog system can reason about . the proposed dataset , consisting of 1900 collected dialogs , allows simulation of an interactive gaining of denotations and questions explanations from users which can be used for the interactive learning .", "topics": ["simulation", "sparse matrix"]}
{"title": "a bayesian model for recognizing handwritten mathematical expressions", "abstract": "recognizing handwritten mathematics is a challenging classification problem , requiring simultaneous identification of all the symbols comprising an input as well as the complex two-dimensional relationships between symbols and subexpressions . because of the ambiguity present in handwritten input , it is often unrealistic to hope for consistently perfect recognition accuracy . we present a system which captures all recognizable interpretations of the input and organizes them in a parse forest from which individual parse trees may be extracted and reported . if the top-ranked interpretation is incorrect , the user may request alternates and select the recognition result they desire . the tree extraction step uses a novel probabilistic tree scoring strategy in which a bayesian network is constructed based on the structure of the input , and each joint variable assignment corresponds to a different parse tree . parse trees are then reported in order of decreasing probability . two accuracy evaluations demonstrate that the resulting recognition system is more accurate than previous versions ( which used non-probabilistic methods ) and other academic math recognizers .", "topics": ["parsing", "bayesian network"]}
{"title": "supervised metric learning with generalization guarantees", "abstract": "the crucial importance of metrics in machine learning algorithms has led to an increasing interest in optimizing distance and similarity functions , an area of research known as metric learning . when data consist of feature vectors , a large body of work has focused on learning a mahalanobis distance . less work has been devoted to metric learning from structured objects ( such as strings or trees ) , most of it focusing on optimizing a notion of edit distance . we identify two important limitations of current metric learning approaches . first , they allow to improve the performance of local algorithms such as k-nearest neighbors , but metric learning for global algorithms ( such as linear classifiers ) has not been studied so far . second , the question of the generalization ability of metric learning methods has been largely ignored . in this thesis , we propose theoretical and algorithmic contributions that address these limitations . our first contribution is the derivation of a new kernel function built from learned edit probabilities . our second contribution is a novel framework for learning string and tree edit similarities inspired by the recent theory of ( e , g , t ) -good similarity functions . using uniform stability arguments , we establish theoretical guarantees for the learned similarity that give a bound on the generalization error of a linear classifier built from that similarity . in our third contribution , we extend these ideas to metric learning from feature vectors by proposing a bilinear similarity learning method that efficiently optimizes the ( e , g , t ) -goodness . generalization guarantees are derived for our approach , highlighting that our method minimizes a tighter bound on the generalization error of the classifier . our last contribution is a framework for establishing generalization bounds for a large class of existing metric learning algorithms based on a notion of algorithmic robustness .", "topics": ["test set", "cluster analysis"]}
{"title": "improving human action recognition by non-action classification", "abstract": "in this paper we consider the task of recognizing human actions in realistic video where human actions are dominated by irrelevant factors . we first study the benefits of removing non-action video segments , which are the ones that do not portray any human action . we then learn a non-action classifier and use it to down-weight irrelevant video segments . the non-action classifier is trained using actionthread , a dataset with shot-level annotation for the occurrence or absence of a human action . the non-action classifier can be used to identify non-action shots with high precision and subsequently used to improve the performance of action recognition systems .", "topics": ["relevance"]}
{"title": "probabilistic inference from arbitrary uncertainty using mixtures of factorized generalized gaussians", "abstract": "this paper presents a general and efficient framework for probabilistic inference and learning from arbitrary uncertain information . it exploits the calculation properties of finite mixture models , conjugate families and factorization . both the joint probability density of the variables and the likelihood function of the ( objective or subjective ) observation are approximated by a special mixture model , in such a way that any desired conditional distribution can be directly obtained without numerical integration . we have developed an extended version of the expectation maximization ( em ) algorithm to estimate the parameters of mixture models from uncertain training examples ( indirect observations ) . as a consequence , any piece of exact or uncertain information about both input and output values is consistently handled in the inference and learning stages . this ability , extremely useful in certain situations , is not found in most alternative methods . the proposed framework is formally justified from standard probabilistic principles and illustrative examples are provided in the fields of nonparametric pattern classification , nonlinear regression and pattern completion . finally , experiments on a real application and comparative results over standard databases provide empirical evidence of the utility of the method in a wide range of applications .", "topics": ["statistical classification", "numerical analysis"]}
{"title": "learning planar ising models", "abstract": "inference and learning of graphical models are both well-studied problems in statistics and machine learning that have found many applications in science and engineering . however , exact inference is intractable in general graphical models , which suggests the problem of seeking the best approximation to a collection of random variables within some tractable family of graphical models . in this paper , we focus on the class of planar ising models , for which exact inference is tractable using techniques of statistical physics . based on these techniques and recent methods for planarity testing and planar embedding , we propose a simple greedy algorithm for learning the best planar ising model to approximate an arbitrary collection of binary random variables ( possibly from sample data ) . given the set of all pairwise correlations among variables , we select a planar graph and optimal planar ising model defined on this graph to best approximate that set of correlations . we demonstrate our method in simulations and for the application of modeling senate voting records .", "topics": ["graphical model", "approximation algorithm"]}
{"title": "real time speckle image de-noising", "abstract": "the paper presents real time speckle de-noising based on activity computation algorithm and wavelet transform . speckles arise in an image when laser light is reflected from an illuminated surface . the process involves detection of speckles in an image by obtaining a number of frames of the same object under different illumination or angle and comparing the frames for the granular computation and de-noising the same on presence of greater activity index . the project can be implemented in fpga ( field programmable gate array ) technology . the results can be shown that the used activity computation algorithm and wavelet transform has better accuracy in the process of speckle detection and de-noising .", "topics": ["noise reduction", "computation"]}
{"title": "a methodology for customizing clinical tests for esophageal cancer based on patient preferences", "abstract": "tests for esophageal cancer can be expensive , uncomfortable and can have side effects . for many patients , we can predict non-existence of disease with 100 % certainty , just using demographics , lifestyle , and medical history information . our objective is to devise a general methodology for customizing tests using user preferences so that expensive or uncomfortable tests can be avoided . we propose to use classifiers trained from electronic health records ( ehr ) for selection of tests . the key idea is to design classifiers with 100 % false normal rates , possibly at the cost higher false abnormals . we compare naive bayes classification ( nb ) , random forests ( rf ) , support vector machines ( svm ) and logistic regression ( lr ) , and find kernel logistic regression to be most suitable for the task . we propose an algorithm for finding the best probability threshold for kernel lr , based on test set accuracy . using the proposed algorithm , we describe schemes for selecting tests , which appear as features in the automatic classification algorithm , using preferences on costs and discomfort of the users . we test our methodology with ehrs collected for more than 3000 patients , as a part of project carried out by a reputed hospital in mumbai , india . kernel svm and kernel lr with a polynomial kernel of degree 3 , yields an accuracy of 99.8 % and sensitivity 100 % , without the mp features , i.e . using only clinical tests . we demonstrate our test selection algorithm using two case studies , one using cost of clinical tests , and other using `` discomfort '' values for clinical tests . we compute the test sets corresponding to the lowest false abnormals for each criterion described above , using exhaustive enumeration of 15 clinical tests . the sets turn out to different , substantiating our claim that one can customize test sets based on user preferences .", "topics": ["test set", "support vector machine"]}
{"title": "d-separation : from theorems to algorithms", "abstract": "an efficient algorithm is developed that identifies all independencies implied by the topology of a bayesian network . its correctness and maximality stems from the soundness and completeness of d-separation with respect to probability theory . the algorithm runs in time o ( l e l ) where e is the number of edges in the network .", "topics": ["bayesian network"]}
{"title": "non-sparse regularization for multiple kernel learning", "abstract": "learning linear combinations of multiple kernels is an appealing strategy when the right choice of features is unknown . previous approaches to multiple kernel learning ( mkl ) promote sparse kernel combinations to support interpretability and scalability . unfortunately , this 1-norm mkl is rarely observed to outperform trivial baselines in practical applications . to allow for robust kernel mixtures , we generalize mkl to arbitrary norms . we devise new insights on the connection between several existing mkl formulations and develop two efficient interleaved optimization strategies for arbitrary norms , like p-norms with p > 1 . empirically , we demonstrate that the interleaved optimization strategies are much faster compared to the commonly used wrapper approaches . a theoretical analysis and an experiment on controlled artificial data experiment sheds light on the appropriateness of sparse , non-sparse and $ \\ell_\\infty $ -norm mkl in various scenarios . empirical applications of p-norm mkl to three real-world problems from computational biology show that non-sparse mkl achieves accuracies that go beyond the state-of-the-art .", "topics": ["kernel ( operating system )", "baseline ( configuration management )"]}
{"title": "decoupled neural interfaces using synthetic gradients", "abstract": "training directed neural networks typically requires forward-propagating data through a computation graph , followed by backpropagating error signal , to produce weight updates . all layers , or more generally , modules , of the network are therefore locked , in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated . in this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph . these models predict what the result of the modelled subgraph will produce using only local information . in particular we focus on modelling error gradients : by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs , and can update them independently and asynchronously i.e . we realise decoupled neural interfaces . we show results for feed-forward models , where every layer is trained asynchronously , recurrent neural networks ( rnns ) where predicting one 's future gradient extends the time over which the rnn can effectively model , and also a hierarchical rnn system with ticking at different timescales . finally , we demonstrate that in addition to predicting gradients , the same framework can be used to predict inputs , resulting in models which are decoupled in both the forward and backwards pass -- amounting to independent networks which co-learn such that they can be composed into a single functioning corporation .", "topics": ["recurrent neural network", "synthetic data"]}
{"title": "adaptive multi-penalty regularization based on a generalized lasso path", "abstract": "for many algorithms , parameter tuning remains a challenging and critical task , which becomes tedious and infeasible in a multi-parameter setting . multi-penalty regularization , successfully used for solving undetermined sparse regression of problems of unmixing type where signal and noise are additively mixed , is one of such examples . in this paper , we propose a novel algorithmic framework for an adaptive parameter choice in multi-penalty regularization with a focus on the correct support recovery . building upon the theory of regularization paths and algorithms for single-penalty functionals , we extend these ideas to a multi-penalty framework by providing an efficient procedure for the construction of regions containing structurally similar solutions , i.e . , solutions with the same sparsity and sign pattern , over the whole range of parameters . combining this with a model selection criterion , we can choose regularization parameters in a data-adaptive manner . another advantage of our algorithm is that it provides an overview on the solution stability over the whole range of parameters . this can be further exploited to obtain additional insights into the problem of interest . we provide a numerical analysis of our method and compare it to the state-of-the-art single-penalty algorithms for compressed sensing problems in order to demonstrate the robustness and power of the proposed algorithm .", "topics": ["computational complexity theory", "numerical analysis"]}
{"title": "challenging images for minds and machines", "abstract": "there is no denying the tremendous leap in the performance of machine learning methods in the past half-decade . some might even say that specific sub-fields in pattern recognition , such as machine-vision , are as good as solved , reaching human and super-human levels . arguably , lack of training data and computation power are all that stand between us and solving the remaining ones . in this position paper we underline cases in vision which are challenging to machines and even to human observers . this is to show limitations of contemporary models that are hard to ameliorate by following the current trend to increase training data , network capacity or computational power . moreover , we claim that attempting to do so is in principle a suboptimal approach . we provide a taster of such examples in hope to encourage and challenge the machine learning community to develop new directions to solve the said difficulties .", "topics": ["test set", "computation"]}
{"title": "neural vector spaces for unsupervised information retrieval", "abstract": "we propose the neural vector space model ( nvsm ) , a method that learns representations of documents in an unsupervised manner for news article retrieval . in the nvsm paradigm , we learn low-dimensional representations of words and documents from scratch using gradient descent and rank documents according to their similarity with query representations that are composed from word representations . we show that nvsm performs better at document ranking than existing latent semantic vector space methods . the addition of nvsm to a mixture of lexical language models and a state-of-the-art baseline vector space model yields a statistically significant increase in retrieval effectiveness . consequently , nvsm adds a complementary relevance signal . next to semantic matching , we find that nvsm performs well in cases where lexical matching is needed . nvsm learns a notion of term specificity directly from the document collection without feature engineering . we also show that nvsm learns regularities related to luhn significance . finally , we give advice on how to deploy nvsm in situations where model selection ( e.g . , cross-validation ) is infeasible . we find that an unsupervised ensemble of multiple models trained with different hyperparameter values performs better than a single cross-validated model . therefore , nvsm can safely be used for ranking documents without supervised relevance judgments .", "topics": ["baseline ( configuration management )", "unsupervised learning"]}
{"title": "relation classification via recurrent neural network", "abstract": "deep learning has gained much success in sentence-level relation classification . for example , convolutional neural networks ( cnn ) have delivered competitive performance without much effort on feature engineering as the conventional pattern-based methods . thus a lot of works have been produced based on cnn structures . however , a key issue that has not been well addressed by the cnn-based method is the lack of capability to learn temporal features , especially long-distance dependency between nominal pairs . in this paper , we propose a simple framework based on recurrent neural networks ( rnn ) and compare it with cnn-based model . to show the limitation of popular used semeval-2010 task 8 dataset , we introduce another dataset refined from mimlre ( angeli et al . , 2014 ) . experiments on two different datasets strongly indicates that the rnn-based model can deliver better performance on relation classification , and it is particularly capable of learning long-distance relation patterns . this makes it suitable for real-world applications where complicated expressions are often involved .", "topics": ["recurrent neural network"]}
{"title": "structured inference networks for nonlinear state space models", "abstract": "gaussian state space models have been used for decades as generative models of sequential data . they admit an intuitive probabilistic interpretation , have a simple functional form , and enjoy widespread adoption . we introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models , including variants where the emission and transition distributions are modeled by deep neural networks . our learning algorithm simultaneously learns a compiled inference network and the generative model , leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution . we apply the learning algorithm to both synthetic and real-world datasets , demonstrating its scalability and versatility . we find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood .", "topics": ["generative model", "calculus of variations"]}
{"title": "self-adaptation of mutation rates in non-elitist populations", "abstract": "the runtime of evolutionary algorithms ( eas ) depends critically on their parameter settings , which are often problem-specific . automated schemes for parameter tuning have been developed to alleviate the high costs of manual parameter tuning . experimental results indicate that self-adaptation , where parameter settings are encoded in the genomes of individuals , can be effective in continuous optimisation . however , results in discrete optimisation have been less conclusive . furthermore , a rigorous runtime analysis that explains how self-adaptation can lead to asymptotic speedups has been missing . this paper provides the first such analysis for discrete , population-based eas . we apply level-based analysis to show how a self-adaptive ea is capable of fine-tuning its mutation rate , leading to exponential speedups over eas using fixed mutation rates .", "topics": ["mathematical optimization", "time complexity"]}
{"title": "learning to perform physics experiments via deep reinforcement learning", "abstract": "when encountering novel objects , humans are able to infer a wide range of physical properties such as mass , friction and deformability by interacting with them in a goal driven way . this process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts . recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in go , atari , natural language processing , and complex control problems ; however , it is not clear that these systems can rival the scientific intuition of even a young child . in this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences . we found that state of art deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties . by systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments , we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations .", "topics": ["baseline ( configuration management )", "natural language processing"]}
{"title": "automatic construction and natural-language description of nonparametric regression models", "abstract": "this paper presents the beginnings of an automatic statistician , focusing on regression problems . our system explores an open-ended space of statistical models to discover a good explanation of a data set , and then produces a detailed report with figures and natural-language text . our approach treats unknown regression functions nonparametrically using gaussian processes , which has two important consequences . first , gaussian processes can model functions in terms of high-level properties ( e.g . smoothness , trends , periodicity , changepoints ) . taken together with the compositional structure of our language of models this allows us to automatically describe functions in simple terms . second , the use of flexible nonparametric models and a rich language for composing them in an open-ended manner also results in state-of-the-art extrapolation performance evaluated over 13 real time series data sets from various domains .", "topics": ["time series", "high- and low-level"]}
{"title": "land cover classification from multi-temporal , multi-spectral remotely sensed imagery using patch-based recurrent neural networks", "abstract": "sustainability of the global environment is dependent on the accurate land cover information over large areas . even with the increased number of satellite systems and sensors acquiring data with improved spectral , spatial , radiometric and temporal characteristics and the new data distribution policy , most existing land cover datasets were derived from a pixel-based single-date multi-spectral remotely sensed image with low accuracy . to improve the accuracy , the bottleneck is how to develop an accurate and effective image classification technique . by incorporating and utilizing the complete multi-spectral , multi-temporal and spatial information in remote sensing images and considering their inherit spatial and sequential interdependence , we propose a new patch-based rnn ( pb-rnn ) system tailored for multi-temporal remote sensing data . the system is designed by incorporating distinctive characteristics in multi-temporal remote sensing data . in particular , it uses multi-temporal-spectral-spatial samples and deals with pixels contaminated by clouds/shadow present in the multi-temporal data series . using a florida everglades ecosystem study site covering an area of 771 square kilo-meters , the proposed pb-rnn system has achieved a significant improvement in the classification accuracy over pixel-based rnn system , pixel-based single-imagery nn system , pixel-based multi-images nn system , patch-based single-imagery nn system and patch-based multi-images nn system . for example , the proposed system achieves 97.21 % classification accuracy while a pixel-based single-imagery nn system achieves 64.74 % . by utilizing methods like the proposed pb-rnn one , we believe that much more accurate land cover datasets can be produced over large areas efficiently .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "generating contradictory , neutral , and entailing sentences", "abstract": "learning distributed sentence representations remains an interesting problem in the field of natural language processing ( nlp ) . we want to learn a model that approximates the conditional latent space over the representations of a logical antecedent of the given statement . in our paper , we propose an approach to generating sentences , conditioned on an input sentence and a logical inference label . we do this by modeling the different possibilities for the output sentence as a distribution over the latent representation , which we train using an adversarial objective . we evaluate the model using two state-of-the-art models for the recognizing textual entailment ( rte ) task , and measure the bleu scores against the actual sentences as a probe for the diversity of sentences produced by our model . the experiment results show that , given our framework , we have clear ways to improve the quality and diversity of generated sentences .", "topics": ["natural language processing", "natural language"]}
{"title": "non-monotonic reasoning and the reversibility of belief change", "abstract": "traditional approaches to non-monotonic reasoning fail to satisfy a number of plausible axioms for belief revision and suffer from conceptual difficulties as well . recent work on ranked preferential models ( rpms ) promises to overcome some of these difficulties . here we show that rpms are not adequate to handle iterated belief change . specifically , we show that rpms do not always allow for the reversibility of belief change . this result indicates the need for numerical strengths of belief .", "topics": ["numerical analysis", "iteration"]}
{"title": "cascade ranking for operational e-commerce search", "abstract": "in the 'big data ' era , many real-world applications like search involve the ranking problem for a large number of items . it is important to obtain effective ranking results and at the same time obtain the results efficiently in a timely manner for providing good user experience and saving computational costs . valuable prior research has been conducted for learning to efficiently rank like the cascade ranking ( learning ) model , which uses a sequence of ranking functions to progressively filter some items and rank the remaining items . however , most existing research of learning to efficiently rank in search is studied in a relatively small computing environments with simulated user queries . this paper presents novel research and thorough study of designing and deploying a cascade model in a large-scale operational e-commerce search application ( cloes ) , which deals with hundreds of millions of user queries per day with hundreds of servers . the challenge of the real-world application provides new insights for research : 1 ) . real-world search applications often involve multiple factors of preferences or constraints with respect to user experience and computational costs such as search accuracy , search latency , size of search results and total cpu cost , while most existing search solutions only address one or two factors ; 2 ) . effectiveness of e-commerce search involves multiple types of user behaviors such as click and purchase , while most existing cascade ranking in search only models the click behavior . based on these observations , a novel cascade ranking model is designed and deployed in an operational e-commerce search application . an extensive set of experiments demonstrate the advantage of the proposed work to address multiple factors of effectiveness , efficiency and user experience in the real-world application .", "topics": ["simulation"]}
{"title": "dynamic capacity estimation in hopfield networks", "abstract": "understanding the memory capacity of neural networks remains a challenging problem in implementing artificial intelligence systems . in this paper , we address the notion of capacity with respect to hopfield networks and propose a dynamic approach to monitoring a network 's capacity . we define our understanding of capacity as the maximum number of stored patterns which can be retrieved when probed by the stored patterns . prior work in this area has presented static expressions dependent on neuron count $ n $ , forcing network designers to assume worst-case input characteristics for bias and correlation when setting the capacity of the network . instead , our model operates simultaneously with the learning hopfield network and concludes on a capacity estimate based on the patterns which were stored . by continuously updating the crosstalk associated with the stored patterns , our model guards the network from overwriting its memory traces and exceeding its capacity . we simulate our model using artificially generated random patterns , which can be set to a desired bias and correlation , and observe capacity estimates between 93 % and 97 % accurate . as a result , our model doubles the memory efficiency of hopfield networks in comparison to the static and worst-case capacity estimate while minimizing the risk of lost patterns .", "topics": ["artificial intelligence"]}
{"title": "application and verification of algorithm learning based neural network", "abstract": "this paper has been withdrawn by the author due to a crucial accuracy error in fig . 5 . for precise performance of albnn please refer to yoon et al . 's work in the following article . yoon , h. , park , c. s. , kim , j. s. , & baek , j. g . ( 2013 ) . algorithm learning based neural network integrating feature selection and classification . expert systems with applications , 40 ( 1 ) , 231-241 . http : //www.sciencedirect.com/science/article/pii/s0957417412008731", "topics": ["statistical classification"]}
{"title": "ibmms decision support tool for management of bank telemarketing campaigns", "abstract": "although direct marketing is a good method for banks to utilize in the face of global competition and the financial crisis , it has been shown to exhibit poor performance . however , there are some drawbacks to direct campaigns , such as those related to improving the negative attributes that customers ascribe to banks . to overcome these problems , attractive long-term deposit campaigns should be organized and managed more effectively . the aim of this study is to develop an intelligent bank market management system ( ibmms ) for bank managers who want to manage efficient marketing campaigns . ibmms is the first system developed by combining the power of data mining with the capabilities of expert systems in this area . moreover , ibmms includes important features that enable it to be intelligent : a knowledge base , an inference engine and an advisor . using this system , a manager can successfully direct marketing campaigns and follow the decision schemas of customers both as individuals and as a group ; moreover , a manager can make decisions that lead to the desired response by customers .", "topics": ["data mining"]}
{"title": "non-convex robust pca", "abstract": "we propose a new method for robust pca -- the task of recovering a low-rank matrix from sparse corruptions that are of unknown value and support . our method involves alternating between projecting appropriate residuals onto the set of low-rank matrices , and the set of sparse matrices ; each projection is { \\em non-convex } but easy to compute . in spite of this non-convexity , we establish exact recovery of the low-rank matrix , under the same conditions that are required by existing methods ( which are based on convex optimization ) . for an $ m \\times n $ input matrix ( $ m \\leq n ) $ , our method has a running time of $ o ( r^2mn ) $ per iteration , and needs $ o ( \\log ( 1/\\epsilon ) ) $ iterations to reach an accuracy of $ \\epsilon $ . this is close to the running time of simple pca via the power method , which requires $ o ( rmn ) $ per iteration , and $ o ( \\log ( 1/\\epsilon ) ) $ iterations . in contrast , existing methods for robust pca , which are based on convex optimization , have $ o ( m^2n ) $ complexity per iteration , and take $ o ( 1/\\epsilon ) $ iterations , i.e . , exponentially more iterations for the same accuracy . experiments on both synthetic and real data establishes the improved speed and accuracy of our method over existing convex implementations .", "topics": ["time complexity", "synthetic data"]}
{"title": "disentangling motion , foreground and background features in videos", "abstract": "this paper introduces an unsupervised framework to extract semantically rich features for video representation . inspired by how the human visual system groups objects based on motion cues , we propose a deep convolutional neural network that disentangles motion , foreground and background information . the proposed architecture consists of a 3d convolutional feature encoder for blocks of 16 frames , which is trained for reconstruction tasks over the first and last frames of the sequence . a preliminary supervised experiment was conducted to verify the feasibility of proposed method by training the model with a fraction of videos from the ucf-101 dataset taking as ground truth the bounding boxes around the activity regions . qualitative results indicate that the network can successfully segment foreground and background in videos as well as update the foreground appearance based on disentangled motion features . the benefits of these learned features are shown in a discriminative classification task , where initializing the network with the proposed pretraining method outperforms both random initialization and autoencoder pretraining . our model and source code are publicly available at https : //imatge-upc.github.io/unsupervised-2017-cvprw/ .", "topics": ["unsupervised learning", "encoder"]}
{"title": "revisiting video saliency : a large-scale benchmark and a new model", "abstract": "in this work , we contribute to video saliency research in two ways . first , we introduce a new benchmark for predicting human eye movements during dynamic scene free-viewing , which is long-time urged in this field . our dataset , named dhf1k ( dynamic human fixation ) , consists of 1k high-quality , elaborately selected video sequences spanning a large range of scenes , motions , object types and background complexity . existing video saliency datasets lack variety and generality of common dynamic scenes and fall short in covering challenging situations in unconstrained environments . in contrast , dhf1k makes a significant leap in terms of scalability , diversity and difficulty , and is expected to boost video saliency modeling . second , we propose a novel video saliency model that augments the cnn-lstm network architecture with an attention mechanism to enable fast , end-to-end saliency learning . the attention mechanism explicitly encodes static saliency information , thus allowing lstm to focus on learning more flexible temporal saliency representation across successive frames . such a design fully leverages existing large-scale static fixation datasets , avoids overfitting , and significantly improves training efficiency and testing performance . we thoroughly examine the performance of our model , with respect to state-of-the-art saliency models , on three large-scale datasets ( i.e . , dhf1k , hollywood2 , ucf sports ) . experimental results over more than 1.2k testing videos containing 400k frames demonstrate that our model outperforms other competitors .", "topics": ["end-to-end principle", "scalability"]}
{"title": "robust mixture of experts modeling using the skew $ t $ distribution", "abstract": "mixture of experts ( moe ) is a popular framework in the fields of statistics and machine learning for modeling heterogeneity in data for regression , classification and clustering . moe for continuous data are usually based on the normal distribution . however , it is known that for data with asymmetric behavior , heavy tails and atypical observations , the use of the normal distribution is unsuitable . we introduce a new robust non-normal mixture of experts modeling using the skew $ t $ distribution . the proposed skew $ t $ mixture of experts , named stmoe , handles these issues of the normal mixtures experts regarding possibly skewed , heavy-tailed and noisy data . we develop a dedicated expectation conditional maximization ( ecm ) algorithm to estimate the model parameters by monotonically maximizing the observed data log-likelihood . we describe how the presented model can be used in prediction and in model-based clustering of regression data . numerical experiments carried out on simulated data show the effectiveness and the robustness of the proposed model in fitting non-linear regression functions as well as in model-based clustering . then , the proposed model is applied to the real-world data of tone perception for musical data analysis , and the one of temperature anomalies for the analysis of climate change data . the obtained results confirm the usefulness of the model for practical data analysis applications .", "topics": ["cluster analysis", "nonlinear system"]}
{"title": "self-supervised learning for stereo matching with self-improving ability", "abstract": "exiting deep-learning based dense stereo matching methods often rely on ground-truth disparity maps as the training signals , which are however not always available in many situations . in this paper , we design a simple convolutional neural network architecture that is able to learn to compute dense disparity maps directly from the stereo inputs . training is performed in an end-to-end fashion without the need of ground-truth disparity maps . the idea is to use image warping error ( instead of disparity-map residuals ) as the loss function to drive the learning process , aiming to find a depth-map that minimizes the warping error . while this is a simple concept well-known in stereo matching , to make it work in a deep-learning framework , many non-trivial challenges must be overcome , and in this work we provide effective solutions . our network is self-adaptive to different unseen imageries as well as to different camera settings . experiments on kitti and middlebury stereo benchmark datasets show that our method outperforms many state-of-the-art stereo matching methods with a margin , and at the same time significantly faster .", "topics": ["supervised learning", "loss function"]}
{"title": "porcellio scaber algorithm ( psa ) for solving constrained optimization problems", "abstract": "in this paper , we extend a bio-inspired algorithm called the porcellio scaber algorithm ( psa ) to solve constrained optimization problems , including a constrained mixed discrete-continuous nonlinear optimization problem . our extensive experiment results based on benchmark optimization problems show that the psa has a better performance than many existing methods or algorithms . the results indicate that the psa is a promising algorithm for constrained optimization .", "topics": ["optimization problem", "nonlinear system"]}
{"title": "charged point normalization : an efficient solution to the saddle point problem", "abstract": "recently , the problem of local minima in very high dimensional non-convex optimization has been challenged and the problem of saddle points has been introduced . this paper introduces a dynamic type of normalization that forces the system to escape saddle points . unlike other saddle point escaping algorithms , second order information is not utilized , and the system can be trained with an arbitrary gradient descent learner . the system drastically improves learning in a range of deep neural networks on various data-sets in comparison to non-cpn neural networks .", "topics": ["gradient descent", "gradient"]}
{"title": "pose2instance : harnessing keypoints for person instance segmentation", "abstract": "human keypoints are a well-studied representation of people.we explore how to use keypoint models to improve instance-level person segmentation . the main idea is to harness the notion of a distance transform of oracle provided keypoints or estimated keypoint heatmaps as a prior for person instance segmentation task within a deep neural network . for training and evaluation , we consider all those images from coco where both instance segmentation and human keypoints annotations are available . we first show how oracle keypoints can boost the performance of existing human segmentation model during inference without any training . next , we propose a framework to directly learn a deep instance segmentation model conditioned on human pose . experimental results show that at various intersection over union ( iou ) thresholds , in a constrained environment with oracle keypoints , the instance segmentation accuracy achieves 10 % to 12 % relative improvements over a strong baseline of oracle bounding boxes . in a more realistic environment , without the oracle keypoints , the proposed deep person instance segmentation model conditioned on human pose achieves 3.8 % to 10.5 % relative improvements comparing with its strongest baseline of a deep network trained only for segmentation .", "topics": ["baseline ( configuration management )"]}
{"title": "issues in stacked generalization", "abstract": "stacked generalization is a general method of using a high-level model to combine lower-level models to achieve greater predictive accuracy . in this paper we address two crucial issues which have been considered to be a `black art ' in classification tasks ever since the introduction of stacked generalization in 1992 by wolpert : the type of generalizer that is suitable to derive the higher-level model , and the kind of attributes that should be used as its input . we find that best results are obtained when the higher-level model combines the confidence ( and not just the predictions ) of the lower-level ones . we demonstrate the effectiveness of stacked generalization for combining three different types of learning algorithms for classification tasks . we also compare the performance of stacked generalization with majority vote and published results of arcing and bagging .", "topics": ["high- and low-level"]}
{"title": "localized lrr on grassmann manifolds : an extrinsic view", "abstract": "subspace data representation has recently become a common practice in many computer vision tasks . it demands generalizing classical machine learning algorithms for subspace data . low-rank representation ( lrr ) is one of the most successful models for clustering vectorial data according to their subspace structures . this paper explores the possibility of extending lrr for subspace data on grassmann manifolds . rather than directly embedding the grassmann manifolds into the symmetric matrix space , an extrinsic view is taken to build the lrr self-representation in the local area of the tangent space at each grassmannian point , resulting in a localized lrr method on grassmann manifolds . a novel algorithm for solving the proposed model is investigated and implemented . the performance of the new clustering algorithm is assessed through experiments on several real-world datasets including mnist handwritten digits , ballet video clips , skig action clips , dyntex++ dataset and highway traffic video clips . the experimental results show the new method outperforms a number of state-of-the-art clustering methods", "topics": ["cluster analysis", "computer vision"]}
{"title": "deep multi-spectral registration using invariant descriptor learning", "abstract": "in this paper , we introduce a novel deep-learning method to align cross-spectral images . our approach relies on a learned descriptor which is invariant to different spectra . multi-modal images of the same scene capture different signals and therefore their registration is challenging and it is not solved by classic approaches . to that end , we developed a feature-based approach that solves the visible ( vis ) to near-infra-red ( nir ) registration problem . our algorithm detects corners by harris and matches them by a patch-metric learned on top of cifar-10 network descriptor . as our experiments demonstrate we achieve a high-quality alignment of cross-spectral images with a sub-pixel accuracy . comparing to other existing methods , our approach is more accurate in the task of vis to nir registration .", "topics": ["pixel"]}
{"title": "3d hand pose tracking and estimation using stereo matching", "abstract": "3d hand pose tracking/estimation will be very important in the next generation of human-computer interaction . most of the currently available algorithms rely on low-cost active depth sensors . however , these sensors can be easily interfered by other active sources and require relatively high power consumption . as a result , they are currently not suitable for outdoor environments and mobile devices . this paper aims at tracking/estimating hand poses using passive stereo which avoids these limitations . a benchmark with 18,000 stereo image pairs and 18,000 depth images captured from different scenarios and the ground-truth 3d positions of palm and finger joints ( obtained from the manual label ) is thus proposed . this paper demonstrates that the performance of the state-of-the art tracking/estimation algorithms can be maintained with most stereo matching algorithms on the proposed benchmark , as long as the hand segmentation is correct . as a result , a novel stereo-based hand segmentation algorithm specially designed for hand tracking/estimation is proposed . the quantitative evaluation demonstrates that the proposed algorithm is suitable for the state-of-the-art hand pose tracking/estimation algorithms and the tracking quality is comparable to the use of active depth sensors under different challenging scenarios .", "topics": ["sensor"]}
{"title": "minimal perceptrons for memorizing complex patterns", "abstract": "feedforward neural networks have been investigated to understand learning and memory , as well as applied to numerous practical problems in pattern classification . it is a rule of thumb that more complex tasks require larger networks . however , the design of optimal network architectures for specific tasks is still an unsolved fundamental problem . in this study , we consider three-layered neural networks for memorizing binary patterns . we developed a new complexity measure of binary patterns , and estimated the minimal network size for memorizing them as a function of their complexity . we formulated the minimal network size for regular , random , and complex patterns . in particular , the minimal size for complex patterns , which are neither ordered nor disordered , was predicted by measuring their hamming distances from known ordered patterns . our predictions agreed with simulations based on the back-propagation algorithm .", "topics": ["statistical classification", "simulation"]}
{"title": "cobb angle measurement of scoliosis with reduced variability", "abstract": "cobb angle , which is a measure of spinal curvature is the standard method for quantifying the magnitude of scoliosis related to spinal deformity in orthopedics . determining the cobb angle through manual process is subject to human errors . in this work , we propose a methodology to measure the magnitude of cobb angle , which appreciably reduces the variability related to its measurement compared to the related works . the proposed methodology is facilitated by using a suitable new improved version of non-local means for image denoisation and otsus automatic threshold selection for canny edge detection . we have selected nlm for preprocessing of the image as it is one of the fine states of art for image denoisation and helps in retaining the image quality . trimmedmean , median are more robust to outliners than mean and following this concept we observed that nlm denoising quality performance can be enhanced by using euclidean trimmed-mean replacing the mean . to prove the better performance of the non-local euclidean trimmed-mean denoising filter , we have provided some comparative study results of the proposed denoising technique with traditional nlm and nonlocal euclidean medians . the experimental results for cobb angle measurement over intra observer and inter observer experimental data reveals the better performance and superiority of the proposed approach compared to the related works . matlab2009b image processing toolbox was used for the purpose of simulation and verification of the proposed methodology .", "topics": ["image processing", "noise reduction"]}
{"title": "multi-view face detection using deep convolutional neural networks", "abstract": "in this paper we consider the problem of multi-view face detection . while there has been significant research on this problem , current state-of-the-art approaches for this task require annotation of facial landmarks , e.g . tsm [ 25 ] , or annotation of face poses [ 28 , 22 ] . they also require training dozens of models to fully capture faces in all orientations , e.g . 22 models in headhunter method [ 22 ] . in this paper we propose deep dense face detector ( ddfd ) , a method that does not require pose/landmark annotation and is able to detect faces in a wide range of orientations using a single model based on deep convolutional neural networks . the proposed method has minimal complexity ; unlike other recent deep learning object detection methods [ 9 ] , it does not require additional components such as segmentation , bounding-box regression , or svm classifiers . furthermore , we analyzed scores of the proposed face detector for faces in different orientations and found that 1 ) the proposed method is able to detect faces from different angles and can handle occlusion to some extent , 2 ) there seems to be a correlation between dis- tribution of positive examples in the training set and scores of the proposed face detector . the latter suggests that the proposed methods performance can be further improved by using better sampling strategies and more sophisticated data augmentation techniques . evaluations on popular face detection benchmark datasets show that our single-model face detector algorithm has similar or better performance compared to the previous methods , which are more complex and require annotations of either different poses or facial landmarks .", "topics": ["sampling ( signal processing )", "object detection"]}
{"title": "towards accurate word segmentation for chinese patents", "abstract": "a patent is a property right for an invention granted by the government to the inventor . an invention is a solution to a specific technological problem . so patents often have a high concentration of scientific and technical terms that are rare in everyday language . the chinese word segmentation model trained on currently available everyday language data sets performs poorly because it can not effectively recognize these scientific and technical terms . in this paper we describe a pragmatic approach to chinese word segmentation on patents where we train a character-based semi-supervised sequence labeling model by extracting features from a manually segmented corpus of 142 patents , enhanced with information extracted from the chinese treebank . experiments show that the accuracy of our model reached 95.08 % ( f1 score ) on a held-out test set and 96.59 % on development set , compared with an f1 score of 91.48 % on development set if the model is trained on the chinese treebank . we also experimented with some existing domain adaptation techniques , the results show that the amount of target domain data and the selected features impact the performance of the domain adaptation techniques .", "topics": ["test set"]}
{"title": "towards adapting imagenet to reality : scalable domain adaptation with implicit low-rank transformations", "abstract": "images seen during test time are often not from the same distribution as images used for learning . this problem , known as domain shift , occurs when training classifiers from object-centric internet image databases and trying to apply them directly to scene understanding tasks . the consequence is often severe performance degradation and is one of the major barriers for the application of classifiers in real-world systems . in this paper , we show how to learn transform-based domain adaptation classifiers in a scalable manner . the key idea is to exploit an implicit rank constraint , originated from a max-margin domain adaptation formulation , to make optimization tractable . experiments show that the transformation between domains can be very efficiently learned from data and easily applied to new categories . this begins to bridge the gap between large-scale internet image collections and object images captured in everyday life environments .", "topics": ["scalability", "database"]}
{"title": "using bayesian networks to identify the causal effect of speeding in individual vehicle/pedestrian collisions", "abstract": "on roads showing significant violations of posted speed limits , one measure of the safety effect of speeding is the difference between the road 's actual accident count and the count that would have occurred if the posted speed limit had been strictly obeyed . an estimate of this accident reduction can be had by computing the probability that speeding was a necessary condition for each of set of accidents . this is an instance of assessing individual probabilities of causation , which is generally not possible absent prior knowledge of causal structure . for traffic accidents such prior knowledge is often available and this paper illustrates how , for a commonly occurring class of vehicle/pedestrian accidents , approaches to uncertainty and causal analyses appearing in the accident reconstruction literature can be unified using bayesian networks . measured skidmarks , pedestrian throw distances , and pedestrian injury severity are treated as evidence , and using the gibbs sampling routine bugs , the posterior probability distribution over exogenous variables , such as the vehicle 's initial speed , location , and driver reaction time , is computed . this posterior distribution is then used to compute the `` probability of necessity '' for speeding .", "topics": ["sampling ( signal processing )", "computation"]}
{"title": "total variation and euler 's elastica for supervised learning", "abstract": "in recent years , total variation ( tv ) and euler 's elastica ( ee ) have been successfully applied to image processing tasks such as denoising and inpainting . this paper investigates how to extend tv and ee to the supervised learning settings on high dimensional data . the supervised learning problem can be formulated as an energy functional minimization under tikhonov regularization scheme , where the energy is composed of a squared loss and a total variation smoothing ( or euler 's elastica smoothing ) . its solution via variational principles leads to an euler-lagrange pde . however , the pde is always high-dimensional and can not be directly solved by common methods . instead , radial basis functions are utilized to approximate the target function , reducing the problem to finding the linear coefficients of basis functions . we apply the proposed methods to supervised learning tasks ( including binary classification , multi-class classification , and regression ) on benchmark data sets . extensive experiments have demonstrated promising results of the proposed methods .", "topics": ["calculus of variations", "approximation algorithm"]}
{"title": "barrier frank-wolfe for marginal inference", "abstract": "we introduce a globally-convergent algorithm for optimizing the tree-reweighted ( trw ) variational objective over the marginal polytope . the algorithm is based on the conditional gradient method ( frank-wolfe ) and moves pseudomarginals within the marginal polytope through repeated maximum a posteriori ( map ) calls . this modular structure enables us to leverage black-box map solvers ( both exact and approximate ) for variational inference , and obtains more accurate results than tree-reweighted algorithms that optimize over the local consistency relaxation . theoretically , we bound the sub-optimality for the proposed algorithm despite the trw objective having unbounded gradients at the boundary of the marginal polytope . empirically , we demonstrate the increased quality of results found by tightening the relaxation over the marginal polytope as well as the spanning tree polytope on synthetic and real-world instances .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "analysing congestion problems in multi-agent reinforcement learning", "abstract": "congestion problems are omnipresent in today 's complex networks and represent a challenge in many research domains . in the context of multi-agent reinforcement learning ( marl ) , approaches like difference rewards and resource abstraction have shown promising results in tackling such problems . resource abstraction was shown to be an ideal candidate for solving large-scale resource allocation problems in a fully decentralized manner . however , its performance and applicability strongly depends on some , until now , undocumented assumptions . two of the main congestion benchmark problems considered in the literature are : the beach problem domain and the traffic lane domain . in both settings the highest system utility is achieved when overcrowding one resource and keeping the rest at optimum capacity . we analyse how abstract grouping can promote this behaviour and how feasible it is to apply this approach in a real-world domain ( i.e . , what assumptions need to be satisfied and what knowledge is necessary ) . we introduce a new test problem , the road network domain ( rnd ) , where the resources are no longer independent , but rather part of a network ( e.g . , road network ) , thus choosing one path will also impact the load on other paths having common road segments . we demonstrate the application of state-of-the-art marl methods for this new congestion model and analyse their performance . rnd allows us to highlight an important limitation of resource abstraction and show that the difference rewards approach manages to better capture and inform the agents about the dynamics of the environment .", "topics": ["reinforcement learning"]}
{"title": "fast subspace clustering based on the kronecker product", "abstract": "subspace clustering is a useful technique for many computer vision applications in which the intrinsic dimension of high-dimensional data is often smaller than the ambient dimension . spectral clustering , as one of the main approaches to subspace clustering , often takes on a sparse representation or a low-rank representation to learn a block diagonal self-representation matrix for subspace generation . however , existing methods require solving a large scale convex optimization problem with a large set of data , with computational complexity reaches o ( n^3 ) for n data points . therefore , the efficiency and scalability of traditional spectral clustering methods can not be guaranteed for large scale datasets . in this paper , we propose a subspace clustering model based on the kronecker product . due to the property that the kronecker product of a block diagonal matrix with any other matrix is still a block diagonal matrix , we can efficiently learn the representation matrix which is formed by the kronecker product of k smaller matrices . by doing so , our model significantly reduces the computational complexity to o ( kn^ { 3/k } ) . furthermore , our model is general in nature , and can be adapted to different regularization based subspace clustering methods . experimental results on two public datasets show that our model significantly improves the efficiency compared with several state-of-the-art methods . moreover , we have conducted experiments on synthetic data to verify the scalability of our model for large scale datasets .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers", "abstract": "model pruning has become a useful technique that improves the computational efficiency of deep learning , making it possible to deploy solutions in resource-limited scenarios . a widely-used practice in relevant work assumes that a smaller-norm parameter or feature plays a less informative role at the inference time . in this paper , we propose a channel pruning technique for accelerating the computations of deep convolutional neural networks ( cnns ) that does not critically rely on this assumption . instead , it focuses on direct simplification of the channel-to-channel computation graph of a cnn without the need of performing a computationally difficult and not-always-useful task of making high-dimensional tensors of cnn structured sparse . our approach takes two stages : first to adopt an end-to- end stochastic training method that eventually forces the outputs of some channels to be constant , and then to prune those constant channels from the original neural network by adjusting the biases of their impacting layers such that the resulting compact model can be quickly fine-tuned . our approach is mathematically appealing from an optimization perspective and easy to reproduce . we experimented our approach through several image learning benchmarks and demonstrate its interesting aspects and competitive performance .", "topics": ["computation", "sparse matrix"]}
{"title": "robust associative memories naturally occuring from recurrent hebbian networks under noise", "abstract": "the brain is a noisy system subject to energy constraints . these facts are rarely taken into account when modelling artificial neural networks . in this paper , we are interested in demonstrating that those factors can actually lead to the appearance of robust associative memories . we first propose a simplified model of noise in the brain , taking into account synaptic noise and interference from neurons external to the network . when coarsely quantized , we show that this noise can be reduced to insertions and erasures . we take a neural network with recurrent modifiable connections , and subject it to noisy external inputs . we introduce an energy usage limitation principle in the network as well as consolidated hebbian learning , resulting in an incremental processing of inputs . we show that the connections naturally formed correspond to state-of-the-art binary sparse associative memories .", "topics": ["recurrent neural network", "sparse matrix"]}
{"title": "from image to text classification : a novel approach based on clustering word embeddings", "abstract": "in this paper , we propose a novel approach for text classification based on clustering word embeddings , inspired by the bag of visual words model , which is widely used in computer vision . after each word in a collection of documents is represented as word vector using a pre-trained word embeddings model , a k-means algorithm is applied on the word vectors in order to obtain a fixed-size set of clusters . the centroid of each cluster is interpreted as a super word embedding that embodies all the semantically related word vectors in a certain region of the embedding space . every embedded word in the collection of documents is then assigned to the nearest cluster centroid . in the end , each document is represented as a bag of super word embeddings by computing the frequency of each super word embedding in the respective document . we also diverge from the idea of building a single vocabulary for the entire collection of documents , and propose to build class-specific vocabularies for better performance . using this kind of representation , we report results on two text mining tasks , namely text categorization by topic and polarity classification . on both tasks , our model yields better performance than the standard bag of words .", "topics": ["cluster analysis", "computer vision"]}
{"title": "image classification by feature dimension reduction and graph based ranking", "abstract": "dimensionality reduction ( dr ) of image features plays an important role in image retrieval and classification tasks . recently , two types of methods have been proposed to improve the both the accuracy and efficiency for the dimensionality reduction problem . one uses non-negative matrix factorization ( nmf ) to describe the image distribution on the space of base matrix . another one for dimension reduction trains a subspace projection matrix to project original data space into some low-dimensional subspaces which have deep architecture , so that the low-dimensional codes would be learned . at the same time , the graph based similarity learning algorithm which tries to exploit contextual information for improving the effectiveness of image rankings is also proposed for image class and retrieval problem . in this paper , after above two methods mentioned are utilized to reduce the high-dimensional features of images respectively , we learn the graph based similarity for the image classification problem . this paper compares the proposed approach with other approaches on an image database .", "topics": ["computer vision"]}
{"title": "personality traits and echo chambers on facebook", "abstract": "in online social networks , users tend to select information that adhere to their system of beliefs and to form polarized groups of like minded people . polarization as well as its effects on online social interactions have been extensively investigated . still , the relation between group formation and personality traits remains unclear . a better understanding of the cognitive and psychological determinants of online social dynamics might help to design more efficient communication strategies and to challenge the digital misinformation threat . in this work , we focus on users commenting posts published by us facebook pages supporting scientific and conspiracy-like narratives , and we classify the personality traits of those users according to their online behavior . we show that different and conflicting communities are populated by users showing similar psychological profiles , and that the dominant personality model is the same in both scientific and conspiracy echo chambers . moreover , we observe that the permanence within echo chambers slightly shapes users ' psychological profiles . our results suggest that the presence of specific personality traits in individuals lead to their considerable involvement in supporting narratives inside virtual echo chambers .", "topics": ["interaction"]}
{"title": "a novel technique for grading of dates using shape and texture features", "abstract": "this paper presents a novel method to grade the date fruits based on the combination of shape and texture features . the method begins with reducing the specular reflection and small noise using a bilateral filter . threshold based segmentation is performed for background removal and fruit part selection from the given image . shape features is extracted using the contour of the date fruit and texture features are extracted using curvelet transform and local binary pattern ( lbp ) from the selected date fruit region . finally , combinations of shape and texture features are fused to grade the dates into six grades . k-nearest neighbour ( k-nn ) classifier yields the best grading rate compared to other two classifiers such as support vector machine ( svm ) and linear discriminant ( lda ) classifiers . the experiment result shows that our technique achieves highest accuracy .", "topics": ["support vector machine"]}
{"title": "a learning framework for winner-take-all networks with stochastic synapses", "abstract": "many recent generative models make use of neural networks to transform the probability distribution of a simple low-dimensional noise process into the complex distribution of the data . this raises the question of whether biological networks operate along similar principles to implement a probabilistic model of the environment through transformations of intrinsic noise processes . the intrinsic neural and synaptic noise processes in biological networks , however , are quite different from the noise processes used in current abstract generative networks . this , together with the discrete nature of spikes and local circuit interactions among the neurons , raises several difficulties when using recent generative modeling frameworks to train biologically motivated models . in this paper , we show that a biologically motivated model based on multi-layer winner-take-all ( wta ) circuits and stochastic synapses admits an approximate analytical description . this allows us to use the proposed networks in a variational learning setting where stochastic backpropagation is used to optimize a lower bound on the data log likelihood , thereby learning a generative model of the data . we illustrate the generality of the proposed networks and learning technique by using them in a structured output prediction task , and in a semi-supervised learning task . our results extend the domain of application of modern stochastic network architectures to networks where synaptic transmission failure is the principal noise mechanism .", "topics": ["generative model", "calculus of variations"]}
{"title": "dolphin - dictionary learning for phase retrieval", "abstract": "we propose a new algorithm to learn a dictionary for reconstructing and sparsely encoding signals from measurements without phase . specifically , we consider the task of estimating a two-dimensional image from squared-magnitude measurements of a complex-valued linear transformation of the original image . several recent phase retrieval algorithms exploit underlying sparsity of the unknown signal in order to improve recovery performance . in this work , we consider such a sparse signal prior in the context of phase retrieval , when the sparsifying dictionary is not known in advance . our algorithm jointly reconstructs the unknown signal - possibly corrupted by noise - and learns a dictionary such that each patch of the estimated image can be sparsely represented . numerical experiments demonstrate that our approach can obtain significantly better reconstructions for phase retrieval problems with noise than methods that can not exploit such `` hidden '' sparsity . moreover , on the theoretical side , we provide a convergence result for our method .", "topics": ["numerical analysis", "sparse matrix"]}
{"title": "a general algorithm for deciding transportability of experimental results", "abstract": "generalizing empirical findings to new environments , settings , or populations is essential in most scientific explorations . this article treats a particular problem of generalizability , called `` transportability '' , defined as a license to transfer information learned in experimental studies to a different population , on which only observational studies can be conducted . given a set of assumptions concerning commonalities and differences between the two populations , pearl and bareinboim ( 2011 ) derived sufficient conditions that permit such transfer to take place . this article summarizes their findings and supplements them with an effective procedure for deciding when and how transportability is feasible . it establishes a necessary and sufficient condition for deciding when causal effects in the target population are estimable from both the statistical information available and the causal information transferred from the experiments . the article further provides a complete algorithm for computing the transport formula , that is , a way of combining observational and experimental information to synthesize bias-free estimate of the desired causal relation . finally , the article examines the differences between transportability and other variants of generalizability .", "topics": ["causality"]}
{"title": "large-scale object discovery and detector adaptation from unlabeled video", "abstract": "we explore object discovery and detector adaptation based on unlabeled video sequences captured from a mobile platform . we propose a fully automatic approach for object mining from video which builds upon a generic object tracking approach . by applying this method to three large video datasets from autonomous driving and mobile robotics scenarios , we demonstrate its robustness and generality . based on the object mining results , we propose a novel approach for unsupervised object discovery by appearance-based clustering . we show that this approach successfully discovers interesting objects relevant to driving scenarios . in addition , we perform self-supervised detector adaptation in order to improve detection performance on the kitti dataset for existing categories . our approach has direct relevance for enabling large-scale object learning for autonomous driving .", "topics": ["cluster analysis", "relevance"]}
{"title": "on usage of autoencoders and siamese networks for online handwritten signature verification", "abstract": "in this paper , we propose a novel writer-independent global feature extraction framework for the task of automatic signature verification which aims to make robust systems for automatically distinguishing negative and positive samples . our method consists of an autoencoder for modeling the sample space into a fixed length latent space and a siamese network for classifying the fixed-length samples obtained from the autoencoder based on the reference samples of a subject as being `` genuine '' or `` forged . '' during our experiments , usage of attention mechanism and applying downsampling significantly improved the accuracy of the proposed framework . we evaluated our proposed framework using sigwicomp2013 japanese and gpdssyntheticonlineofflinesignature datasets . on the sigwicomp2013 japanese dataset , we achieved 8.65 % eer that means 1.2 % relative improvement compared to the best-reported result . furthermore , on the gpdssyntheticonlineofflinesignature dataset , we achieved average eers of 0.13 % , 0.12 % , 0.21 % and 0.25 % respectively for 150 , 300 , 1000 and 2000 test subjects which indicates improvement of relative eer on the best-reported result by 95.67 % , 95.26 % , 92.9 % and 91.52 % respectively . apart from the accuracy gain , because of the nature of our proposed framework which is based on neural networks and consequently is as simple as some consecutive matrix multiplications , it has less computational cost than conventional methods such as dtw and could be used concurrently on devices such as gpu , tpu , etc .", "topics": ["feature extraction", "autoencoder"]}
{"title": "training wide residual networks for deployment using a single bit for each weight", "abstract": "for fast and energy-efficient deployment of trained deep neural networks on resource-constrained embedded hardware , each learned weight parameter should ideally be represented and stored using a single bit . error-rates usually increase when this requirement is imposed . here , we report large improvements in error rates on multiple datasets , for deep convolutional neural networks deployed with 1-bit-per-weight . using wide residual networks as our main baseline , our approach simplifies existing methods that binarize weights by applying the sign function in training ; we apply scaling factors for each layer with constant unlearned values equal to the layer-specific standard deviations used for initialization . for cifar-10 , cifar-100 and imagenet , and models with 1-bit-per-weight requiring less than 10 mb of parameter memory , we achieve error rates of 3.9 % , 18.5 % and 26.0 % / 8.5 % ( top-1 / top-5 ) respectively . we also considered mnist , svhn and imagenet32 , achieving 1-bit-per-weight test results of 0.27 % , 1.9 % , and 41.3 % / 19.1 % respectively . for cifar , our error rates halve previously reported values , and are within about 1 % of our error-rates for the same network with full-precision weights . for networks that overfit , we also show significant improvements in error rate by not learning batch normalization scale and offset parameters . this applies to both full precision and 1-bit-per-weight networks . using a warm-restart learning-rate schedule , we found that training for 1-bit-per-weight is just as fast as full-precision networks , with better accuracy than standard schedules , and achieved about 98 % -99 % of peak performance in just 62 training epochs for cifar-10/100 . for full training code and trained models in matlab , keras and pytorch see https : //github.com/mcdonnell-lab/1-bit-per-weight/ .", "topics": ["baseline ( configuration management )", "mnist database"]}
{"title": "ideas by statistical mechanics ( ism )", "abstract": "ideas by statistical mechanics ( ism ) is a generic program to model evolution and propagation of ideas/patterns throughout populations subjected to endogenous and exogenous interactions . the program is based on the author 's work in statistical mechanics of neocortical interactions ( smni ) , and uses the author 's adaptive simulated annealing ( asa ) code for optimizations of training sets , as well as for importance-sampling to apply the author 's copula financial risk-management codes , trading in risk dimensions ( trd ) , for assessments of risk and uncertainty . this product can be used for decision support for projects ranging from diplomatic , information , military , and economic ( dime ) factors of propagation/evolution of ideas , to commercial sales , trading indicators across sectors of financial markets , advertising and political campaigns , etc . a statistical mechanical model of neocortical interactions , developed by the author and tested successfully in describing short-term memory and eeg indicators , is the proposed model . parameters with a given subset of macrocolumns will be fit using asa to patterns representing ideas . parameters of external and inter-regional interactions will be determined that promote or inhibit the spread of these ideas . tools of financial risk management , developed by the author to process correlated multivariate systems with differing non-gaussian distributions using modern copula analysis , importance-sampled using asa , will enable bona fide correlations and uncertainties of success and failure to be calculated . marginal distributions will be evolved to determine their expected duration and stability using algorithms developed by the author , i.e . , pathtree and pathint codes .", "topics": ["sampling ( signal processing )", "interaction"]}
{"title": "thompson sampling for learning parameterized markov decision processes", "abstract": "we consider reinforcement learning in parameterized markov decision processes ( mdps ) , where the parameterization may induce correlation across transition probabilities or rewards . consequently , observing a particular state transition might yield useful information about other , unobserved , parts of the mdp . we present a version of thompson sampling for parameterized reinforcement learning problems , and derive a frequentist regret bound for priors over general parameter spaces . the result shows that the number of instants where suboptimal actions are chosen scales logarithmically with time , with high probability . it holds for prior distributions that put significant probability near the true model , without any additional , specific closed-form structure such as conjugate or product-form priors . the constant factor in the logarithmic scaling encodes the information complexity of learning the mdp in terms of the kullback-leibler geometry of the parameter space .", "topics": ["regret ( decision theory )", "reinforcement learning"]}
{"title": "two-stream convolutional networks for action recognition in videos", "abstract": "we investigate architectures of discriminatively trained deep convolutional networks ( convnets ) for action recognition in video . the challenge is to capture the complementary information on appearance from still frames and motion between frames . we also aim to generalise the best performing hand-crafted features within a data-driven learning framework . our contribution is three-fold . first , we propose a two-stream convnet architecture which incorporates spatial and temporal networks . second , we demonstrate that a convnet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data . finally , we show that multi-task learning , applied to two different action classification datasets , can be used to increase the amount of training data and improve the performance on both . our architecture is trained and evaluated on the standard video actions benchmarks of ucf-101 and hmdb-51 , where it is competitive with the state of the art . it also exceeds by a large margin previous attempts to use deep nets for video classification .", "topics": ["test set"]}
{"title": "cnn-based projected gradient descent for consistent image reconstruction", "abstract": "we present a new method for image reconstruction which replaces the projector in a projected gradient descent ( pgd ) with a convolutional neural network ( cnn ) . cnns trained as high-dimensional ( image-to-image ) regressors have recently been used to efficiently solve inverse problems in imaging . however , these approaches lack a feedback mechanism to enforce that the reconstructed image is consistent with the measurements . this is crucial for inverse problems , and more so in biomedical imaging , where the reconstructions are used for diagnosis . in our scheme , the gradient descent enforces measurement consistency , while the cnn recursively projects the solution closer to the space of desired reconstruction images . we provide a formal framework to ensure that the classical pgd converges to a local minimizer of a non-convex constrained least-squares problem . when the projector is replaced with a cnn , we propose a relaxed pgd , which always converges . finally , we propose a simple scheme to train a cnn to act like a projector . our experiments on sparse view computed tomography ( ct ) reconstruction for both noiseless and noisy measurements show an improvement over the total-variation ( tv ) method and a recent cnn-based technique .", "topics": ["gradient descent", "sparse matrix"]}
{"title": "when do numbers really matter ?", "abstract": "common wisdom has it that small distinctions in the probabilities ( parameters ) quantifying a belief network do not matter much for the results of probabilistic queries . yet , one can develop realistic scenarios under which small variations in network parameters can lead to significant changes in computed queries . a pending theoretical question is then to analytically characterize parameter changes that do or do not matter . in this paper , we study the sensitivity of probabilistic queries to changes in network parameters and prove some tight bounds on the impact that such parameters can have on queries . our analytic results pinpoint some interesting situations under which parameter changes do or do not matter . these results are important for knowledge engineers as they help them identify influential network parameters . they also help explain some of the previous experimental results and observations with regards to network robustness against parameter changes .", "topics": ["bayesian network"]}
{"title": "open data platform for knowledge access in plant health domain : vespa mining", "abstract": "important data are locked in ancient literature . it would be uneconomic to produce these data again and today or to extract them without the help of text mining technologies . vespa is a text mining project whose aim is to extract data on pest and crops interactions , to model and predict attacks on crops , and to reduce the use of pesticides . a few attempts proposed an agricultural information access . another originality of our work is to parse documents with a dependency of the document architecture .", "topics": ["interaction", "entity"]}
{"title": "kernel machines with two layers and multiple kernel learning", "abstract": "in this paper , the framework of kernel machines with two layers is introduced , generalizing classical kernel methods . the new learning methodology provide a formal connection between computational architectures with multiple layers and the theme of kernel learning in standard regularization methods . first , a representer theorem for two-layer networks is presented , showing that finite linear combinations of kernels on each layer are optimal architectures whenever the corresponding functions solve suitable variational problems in reproducing kernel hilbert spaces ( rkhs ) . the input-output map expressed by these architectures turns out to be equivalent to a suitable single-layer kernel machines in which the kernel function is also learned from the data . recently , the so-called multiple kernel learning methods have attracted considerable attention in the machine learning literature . in this paper , multiple kernel learning methods are shown to be specific cases of kernel machines with two layers in which the second layer is linear . finally , a simple and effective multiple kernel learning method called rls2 ( regularized least squares with two layers ) is introduced , and his performances on several learning problems are extensively analyzed . an open source matlab toolbox to train and validate rls2 models with a graphic user interface is available .", "topics": ["kernel ( operating system )", "calculus of variations"]}
{"title": "class specific feature selection for interval valued data through interval k-means clustering", "abstract": "in this paper , a novel feature selection approach for supervised interval valued features is proposed . the proposed approach takes care of selecting the class specific features through interval k-means clustering . the kernel of k-means clustering algorithm is modified to adapt interval valued data . during training , a set of samples corresponding to a class is fed into the interval k-means clustering algorithm , which clusters features into k distinct clusters . hence , there are k number of features corresponding to each class . subsequently , corresponding to each class , the cluster representatives are chosen . this procedure is repeated for all the samples of remaining classes . during testing the feature indices correspond to each class are used for validating the given dataset through classification using suitable symbolic classifiers . for experimentation , four standard supervised interval datasets are used . the results show the superiority of the proposed model when compared with the other existing state-of-the-art feature selection methods .", "topics": ["cluster analysis"]}
{"title": "delay and cooperation in nonstochastic bandits", "abstract": "we study networks of communicating learning agents that cooperate to solve a common nonstochastic bandit problem . agents use an underlying communication network to get messages about actions selected by other agents , and drop messages that took more than $ d $ hops to arrive , where $ d $ is a delay parameter . we introduce \\textsc { exp3-coop } , a cooperative version of the { \\sc exp3 } algorithm and prove that with $ k $ actions and $ n $ agents the average per-agent regret after $ t $ rounds is at most of order $ \\sqrt { \\bigl ( d+1 + \\tfrac { k } { n } \\alpha_ { \\le d } \\bigr ) ( t\\ln k ) } $ , where $ \\alpha_ { \\le d } $ is the independence number of the $ d $ -th power of the connected communication graph $ g $ . we then show that for any connected graph , for $ d=\\sqrt { k } $ the regret bound is $ k^ { 1/4 } \\sqrt { t } $ , strictly better than the minimax regret $ \\sqrt { kt } $ for noncooperating agents . more informed choices of $ d $ lead to bounds which are arbitrarily close to the full information minimax regret $ \\sqrt { t\\ln k } $ when $ g $ is dense . when $ g $ has sparse components , we show that a variant of \\textsc { exp3-coop } , allowing agents to choose their parameters according to their centrality in $ g $ , strictly improves the regret . finally , as a by-product of our analysis , we provide the first characterization of the minimax regret for bandit learning with delay .", "topics": ["regret ( decision theory )", "sparse matrix"]}
{"title": "analyzing covert social network foundation behind terrorism disaster", "abstract": "this paper addresses a method to analyze the covert social network foundation hidden behind the terrorism disaster . it is to solve a node discovery problem , which means to discover a node , which functions relevantly in a social network , but escaped from monitoring on the presence and mutual relationship of nodes . the method aims at integrating the expert investigator 's prior understanding , insight on the terrorists ' social network nature derived from the complex graph theory , and computational data processing . the social network responsible for the 9/11 attack in 2001 is used to execute simulation experiment to evaluate the performance of the method .", "topics": ["simulation"]}
{"title": "natural language multitasking : analyzing and improving syntactic saliency of hidden representations", "abstract": "we train multi-task autoencoders on linguistic tasks and analyze the learned hidden sentence representations . the representations change significantly when translation and part-of-speech decoders are added . the more decoders a model employs , the better it clusters sentences according to their syntactic similarity , as the representation space becomes less entangled . we explore the structure of the representation space by interpolating between sentences , which yields interesting pseudo-english sentences , many of which have recognizable syntactic structure . lastly , we point out an interesting property of our models : the difference-vector between two sentences can be added to change a third sentence with similar features in a meaningful way .", "topics": ["natural language"]}
{"title": "generating high quality visible images from sar images using cnns", "abstract": "we propose a novel approach for generating high quality visible-like images from synthetic aperture radar ( sar ) images using deep convolutional generative adversarial network ( gan ) architectures . the proposed approach is based on a cascaded network of convolutional neural nets ( cnns ) for despeckling and image colorization . the cascaded structure results in faster convergence during training and produces high quality visible images from the corresponding sar images . experimental results on both simulated and real sar images show that the proposed method can produce visible-like images better compared to the recent state-of-the-art deep learning-based methods .", "topics": ["synthetic data", "noise reduction"]}
{"title": "a multidimensional cascade neuro-fuzzy system with neuron pool optimization in each cascade", "abstract": "a new architecture and learning algorithms for the multidimensional hybrid cascade neural network with neuron pool optimization in each cascade are proposed in this paper . the proposed system differs from the well-known cascade systems in its capability to process multidimensional time series in an online mode , which makes it possible to process non-stationary stochastic and chaotic signals with the required accuracy . compared to conventional analogs , the proposed system provides computational simplicity and possesses both tracking and filtering capabilities .", "topics": ["time series", "mathematical optimization"]}
{"title": "pivo : probabilistic inertial-visual odometry for occlusion-robust navigation", "abstract": "this paper presents a novel method for visual-inertial odometry . the method is based on an information fusion framework employing low-cost imu sensors and the monocular camera in a standard smartphone . we formulate a sequential inference scheme , where the imu drives the dynamical model and the camera frames are used in coupling trailing sequences of augmented poses . the novelty in the model is in taking into account all the cross-terms in the updates , thus propagating the inter-connected uncertainties throughout the model . stronger coupling between the inertial and visual data sources leads to robustness against occlusion and feature-poor environments . we demonstrate results on data collected with an iphone and provide comparisons against the tango device and using the euroc data set .", "topics": ["sensor"]}
{"title": "optimization under uncertainty using the generalized inverse distribution function", "abstract": "a framework for robust optimization under uncertainty based on the use of the generalized inverse distribution function ( gidf ) , also called quantile function , is here proposed . compared to more classical approaches that rely on the usage of statistical moments as deterministic attributes that define the objectives of the optimization process , the inverse cumulative distribution function allows for the use of all the possible information available in the probabilistic domain . furthermore , the use of a quantile based approach leads naturally to a multi-objective methodology which allows an a-posteriori selection of the candidate design based on risk/opportunity criteria defined by the designer . finally , the error on the estimation of the objectives due to the resolution of the gidf will be proven to be quantifiable", "topics": ["mathematical optimization"]}
{"title": "where do goals come from ? a generic approach to autonomous goal-system development", "abstract": "goals express agents ' intentions and allow them to organize their behavior based on low-dimensional abstractions of high-dimensional world states . how can agents develop such goals autonomously ? this paper proposes a detailed conceptual and computational account to this longstanding problem . we argue to consider goals as high-level abstractions of lower-level intention mechanisms such as rewards and values , and point out that goals need to be considered alongside with a detection of the own actions ' effects . we propose latent goal analysis as a computational learning formulation thereof , and show constructively that any reward or value function can by explained by goals and such self-detection as latent mechanisms . we first show that learned goals provide a highly effective dimensionality reduction in a practical reinforcement learning problem . then , we investigate a developmental scenario in which entirely task-unspecific rewards induced by visual saliency lead to self and goal representations that constitute goal-directed reaching .", "topics": ["value ( ethics )", "high- and low-level"]}
{"title": "knowledge discovery of hydrocyclone s circuit based on sonfis and sorst", "abstract": "this study describes application of some approximate reasoning methods to analysis of hydrocyclone performance . in this manner , using a combining of self organizing map ( som ) , neuro-fuzzy inference system ( nfis ) -sonfis- and rough set theory ( rst ) -sorst-crisp and fuzzy granules are obtained . balancing of crisp granules and non-crisp granules can be implemented in close-open iteration . using different criteria and based on granulation level balance point ( interval ) or a pseudo-balance point is estimated . validation of the proposed methods , on the data set of the hydrocyclone is rendered .", "topics": ["iteration"]}
{"title": "online bayesian passive-aggressive learning", "abstract": "online passive-aggressive ( pa ) learning is an effective framework for performing max-margin online learning . but the deterministic formulation and estimated single large-margin model could limit its capability in discovering descriptive structures underlying complex data . this pa- per presents online bayesian passive-aggressive ( bayespa ) learning , which subsumes the online pa and extends naturally to incorporate latent variables and perform nonparametric bayesian inference , thus providing great flexibility for explorative analysis . we apply bayespa to topic modeling and derive efficient online learning algorithms for max-margin topic models . we further develop nonparametric methods to resolve the number of topics . experimental results on real datasets show that our approaches significantly improve time efficiency while maintaining comparable results with the batch counterparts .", "topics": ["bayesian network"]}
{"title": "segmentation of natural images by texture and boundary compression", "abstract": "we present a novel algorithm for segmentation of natural images that harnesses the principle of minimum description length ( mdl ) . our method is based on observations that a homogeneously textured region of a natural image can be well modeled by a gaussian distribution and the region boundary can be effectively coded by an adaptive chain code . the optimal segmentation of an image is the one that gives the shortest coding length for encoding all textures and boundaries in the image , and is obtained via an agglomerative clustering process applied to a hierarchy of decreasing window sizes as multi-scale texture features . the optimal segmentation also provides an accurate estimate of the overall coding length and hence the true entropy of the image . we test our algorithm on the publicly available berkeley segmentation dataset . it achieves state-of-the-art segmentation results compared to other existing methods .", "topics": ["cluster analysis"]}
{"title": "face recognition machine vision system using eigenfaces", "abstract": "face recognition is a common problem in machine learning . this technology has already been widely used in our lives . for example , facebook can automatically tag people 's faces in images , and also some mobile devices use face recognition to protect private security . face images comes with different background , variant illumination , different facial expression and occlusion . there are a large number of approaches for the face recognition . different approaches for face recognition have been experimented with specific databases which consist of single type , format and composition of image . doing so , these approaches do n't suit with different face databases . one of the basic face recognition techniques is eigenface which is quite simple , efficient , and yields generally good results in controlled circumstances . so , this paper presents an experimental performance comparison of face recognition using principal component analysis ( pca ) and normalized principal component analysis ( npca ) . the experiments are carried out on the orl ( att ) and indian face database ( ifd ) which contain variability in expression , pose , and facial details . the results obtained for the two methods have been compared by varying the number of training images . matlab is used for implementing algorithms also .", "topics": ["database"]}
{"title": "convex optimization for non-convex problems via column generation", "abstract": "we apply column generation to approximating complex structured objects via a set of primitive structured objects under either the cross entropy or l2 loss . we use l1 regularization to encourage the use of few structured primitive objects . we attack approximation using convex optimization over an infinite number of variables each corresponding to a primitive structured object that are generated on demand by easy inference in the lagrangian dual . we apply our approach to producing low rank approximations to large 3-way tensors .", "topics": ["approximation algorithm", "matrix regularization"]}
{"title": "a comparative study of transportation problem under probabilistic and fuzzy uncertainties", "abstract": "transportation problem is an important aspect which has been widely studied in operations research domain . it has been studied to simulate different real life problems . in particular , application of this problem in np- hard problems has a remarkable significance . in this paper , we present a comparative study of transportation problem through probabilistic and fuzzy uncertainties . fuzzy logic is a computational paradigm that generalizes classical two-valued logic for reasoning under uncertainty . in order to achieve this , the notation of membership in a set needs to become a matter of degree . by doing this we accomplish two things viz . , ( i ) ease of describing human knowledge involving vague concepts and ( ii ) enhanced ability to develop cost-effective solution to real-world problem . the multi-valued nature of fuzzy sets allows handling uncertain and vague information . it is a model-less approach and a clever disguise of probability theory . we give comparative simulation results of both approaches and discuss the computational complexity . to the best of our knowledge , this is the first work on comparative study of transportation problem using probabilistic and fuzzy uncertainties .", "topics": ["computational complexity theory", "simulation"]}
{"title": "efficient estimation of generalization error and bias-variance components of ensembles", "abstract": "for many applications , an ensemble of base classifiers is an effective solution . the tuning of its parameters ( number of classes , amount of data on which each classifier is to be trained on , etc . ) requires g , the generalization error of a given ensemble . the efficient estimation of g is the focus of this paper . the key idea is to approximate the variance of the class scores/probabilities of the base classifiers over the randomness imposed by the training subset by normal/beta distribution at each point x in the input feature space . we estimate the parameters of the distribution using a small set of randomly chosen base classifiers and use those parameters to give efficient estimation schemes for g. we give empirical evidence for the quality of the various estimators . we also demonstrate their usefulness in making design choices such as the number of classifiers in the ensemble and the size of a subset of data used for training that is needed to achieve a certain value of generalization error . our approach also has great potential for designing distributed ensemble classifiers .", "topics": ["approximation algorithm", "feature vector"]}
{"title": "compilig at semeval-2017 task 1 : cross-language plagiarism detection methods for semantic textual similarity", "abstract": "we present our submitted systems for semantic textual similarity ( sts ) track 4 at semeval-2017 . given a pair of spanish-english sentences , each system must estimate their semantic similarity by a score between 0 and 5 . in our submission , we use syntax-based , dictionary-based , context-based , and mt-based methods . we also combine these methods in unsupervised and supervised way . our best run ranked 1st on track 4a with a correlation of 83.02 % with human annotations .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "a graph-based semi-supervised k nearest-neighbor method for nonlinear manifold distributed data classification", "abstract": "$ k $ nearest neighbors ( $ k $ nn ) is one of the most widely used supervised learning algorithms to classify gaussian distributed data , but it does not achieve good results when it is applied to nonlinear manifold distributed data , especially when a very limited amount of labeled samples are available . in this paper , we propose a new graph-based $ k $ nn algorithm which can effectively handle both gaussian distributed data and nonlinear manifold distributed data . to achieve this goal , we first propose a constrained tired random walk ( trw ) by constructing an $ r $ -level nearest-neighbor strengthened tree over the graph , and then compute a trw matrix for similarity measurement purposes . after this , the nearest neighbors are identified according to the trw matrix and the class label of a query point is determined by the sum of all the trw weights of its nearest neighbors . to deal with online situations , we also propose a new algorithm to handle sequential samples based a local neighborhood reconstruction . comparison experiments are conducted on both synthetic data sets and real-world data sets to demonstrate the validity of the proposed new $ k $ nn algorithm and its improvements to other version of $ k $ nn algorithms . given the widespread appearance of manifold structures in real-world problems and the popularity of the traditional $ k $ nn algorithm , the proposed manifold version $ k $ nn shows promising potential for classifying manifold-distributed data .", "topics": ["supervised learning", "synthetic data"]}
{"title": "a sentence compression based framework to query-focused multi-document summarization", "abstract": "we consider the problem of using sentence compression techniques to facilitate query-focused multi-document summarization . we present a sentence-compression-based framework for the task , and design a series of learning-based compression models built on parse trees . an innovative beam search decoder is proposed to efficiently find highly probable compressions . under this framework , we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function . our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics ( e.g . 8.0 % and 5.4 % improvements in rouge-2 respectively ) for the duc 2006 and 2007 summarization task .", "topics": ["relevance"]}
{"title": "real-valued ( medical ) time series generation with recurrent conditional gans", "abstract": "generative adversarial networks ( gans ) have shown remarkable success as a framework for training models to produce realistic-looking data . in this work , we propose a recurrent gan ( rgan ) and recurrent conditional gan ( rcgan ) to produce realistic real-valued multi-dimensional time series , with an emphasis on their application to medical data . rgans make use of recurrent neural networks in the generator and the discriminator . in the case of rcgans , both of these rnns are conditioned on auxiliary information . we demonstrate our models in a set of toy datasets , where we show visually and quantitatively ( using sample likelihood and maximum mean discrepancy ) that they can successfully generate realistic time-series . we also describe novel evaluation methods for gans , where we generate a synthetic labelled training dataset , and evaluate on a real test set the performance of a model trained on the synthetic data , and vice-versa . we illustrate with these metrics that rcgans can generate time-series data useful for supervised training , with only minor degradation in performance on real test data . this is demonstrated on digit classification from 'serialised ' mnist and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit . we further discuss and analyse the privacy concerns that may arise when using rcgans to generate realistic synthetic medical time series data .", "topics": ["test set", "time series"]}
{"title": "recent progress of face image synthesis", "abstract": "face synthesis has been a fascinating yet challenging problem in computer vision and machine learning . its main research effort is to design algorithms to generate photo-realistic face images via given semantic domain . it has been a crucial prepossessing step of main-stream face recognition approaches and an excellent test of ai ability to use complicated probability distributions . in this paper , we provide a comprehensive review of typical face synthesis works that involve traditional methods as well as advanced deep learning approaches . particularly , generative adversarial net ( gan ) is highlighted to generate photo-realistic and identity preserving results . furthermore , the public available databases and evaluation metrics are introduced in details . we end the review with discussing unsolved difficulties and promising directions for future research .", "topics": ["computer vision", "database"]}
{"title": "probabilistic models for unified collaborative and content-based recommendation in sparse-data environments", "abstract": "recommender systems leverage product and community information to target products to consumers . researchers have developed collaborative recommenders , content-based recommenders , and ( largely ad-hoc ) hybrid systems . we propose a unified probabilistic framework for merging collaborative and content-based recommendations . we extend hofmann 's [ 1999 ] aspect model to incorporate three-way co-occurrence data among users , items , and item content . the relative influence of collaboration data versus content data is not imposed as an exogenous parameter , but rather emerges naturally from the given data sources . global probabilistic models coupled with standard expectation maximization ( em ) learning algorithms tend to drastically overfit in sparse-data situations , as is typical in recommendation applications . we show that secondary content information can often be used to overcome sparsity . experiments on data from the researchindex library of computer science publications show that appropriate mixture models incorporating secondary data produce significantly better quality recommenders than k-nearest neighbors ( k-nn ) . global probabilistic models also allow more general inferences than local methods like k-nn .", "topics": ["sparse matrix"]}
{"title": "fusing deep learned and hand-crafted features of appearance , shape , and dynamics for automatic pain estimation", "abstract": "automatic continuous time , continuous value assessment of a patient 's pain from face video is highly sought after by the medical profession . despite the recent advances in deep learning that attain impressive results in many domains , pain estimation risks not being able to benefit from this due to the difficulty in obtaining data sets of considerable size . in this work we propose a combination of hand-crafted and deep-learned features that makes the most of deep learning techniques in small sample settings . encoding shape , appearance , and dynamics , our method significantly outperforms the current state of the art , attaining a rmse error of less than 1 point on a 16-level pain scale , whilst simultaneously scoring a 67.3 % pearson correlation coefficient between our predicted pain level time series and the ground truth .", "topics": ["time series", "ground truth"]}
{"title": "bias-variance techniques for monte carlo optimization : cross-validation for the ce method", "abstract": "in this paper , we examine the ce method in the broad context of monte carlo optimization ( mco ) and parametric learning ( pl ) , a type of machine learning . a well-known overarching principle used to improve the performance of many pl algorithms is the bias-variance tradeoff . this tradeoff has been used to improve pl algorithms ranging from monte carlo estimation of integrals , to linear estimation , to general statistical estimation . moreover , as described by , mco is very closely related to pl . owing to this similarity , the bias-variance tradeoff affects mco performance , just as it does pl performance . in this article , we exploit the bias-variance tradeoff to enhance the performance of mco algorithms . we use the technique of cross-validation , a technique based on the bias-variance tradeoff , to significantly improve the performance of the cross entropy ( ce ) method , which is an mco algorithm . in previous work we have confirmed that other pl techniques improve the perfomance of other mco algorithms . we conclude that the many techniques pioneered in pl could be investigated as ways to improve mco algorithms in general , and the ce method in particular .", "topics": ["supervised learning"]}
{"title": "learning measures of semi-additive behaviour", "abstract": "in business analytics , measure values , such as sales numbers or volumes of cargo transported , are often summed along values of one or more corresponding categories , such as time or shipping container . however , not every measure should be added by default ( e.g . , one might more typically want a mean over the heights of a set of people ) ; similarly , some measures should only be summed within certain constraints ( e.g . , population measures need not be summed over years ) . in systems such as watson analytics , the exact additive behaviour of a measure is often determined by a human expert . in this work , we propose a small set of features for this issue . we use these features in a case-based reasoning approach , where the system suggests an aggregation behaviour , with 86 % accuracy in our collected dataset .", "topics": ["value ( ethics )"]}
{"title": "convergence of gradient descent on separable data", "abstract": "the implicit bias of gradient descent is not fully understood even in simple linear classification tasks ( e.g . , logistic regression ) . soudry et al . ( 2018 ) studied this bias on separable data , where there are multiple solutions that correctly classify the data . it was found that , when optimizing monotonically decreasing loss functions with exponential tails using gradient descent , the linear classifier specified by the gradient descent iterates converge to the $ l_2 $ max margin separator . however , the convergence rate to the maximum margin solution with fixed step size was found to be extremely slow : $ 1/\\log ( t ) $ . here we examine how the convergence is influenced by using different loss functions and by using variable step sizes . first , we calculate the convergence rate for loss functions with poly-exponential tails near $ \\exp ( -u^ { \\nu } ) $ . we prove that $ \\nu=1 $ yields the optimal convergence rate in the range $ \\nu > 0.25 $ . based on further analysis we conjecture that this remains the optimal rate for $ \\nu \\leq 0.25 $ , and even for sub-poly-exponential tails -- - until loss functions with polynomial tails no longer converge to the max margin . second , we prove the convergence rate could be improved to $ ( \\log t ) /\\sqrt { t } $ for the exponential loss , by using aggressive step sizes which compensate for the rapidly vanishing gradients .", "topics": ["time complexity", "loss function"]}
{"title": "hessian-based analysis of large batch training and robustness to adversaries", "abstract": "large batch size training of neural networks has been shown to incur accuracy loss when trained with the current methods . the precise underlying reasons for this are still not completely understood . here , we study large batch size training through the lens of the hessian operator and robust optimization . in particular , we perform a hessian based study to analyze how the landscape of the loss functional is different for large batch size training . we compute the true hessian spectrum , without approximation , by back-propagating the second derivative . our results on multiple networks show that , when training at large batch sizes , one tends to stop at points in the parameter space with noticeably higher/larger hessian spectrum , i.e . , where the eigenvalues of the hessian are much larger . we then study how batch size affects robustness of the model in the face of adversarial attacks . all the results show that models trained with large batches are more susceptible to adversarial attacks , as compared to models trained with small batch sizes . furthermore , we prove a theoretical result which shows that the problem of finding an adversarial perturbation is a saddle-free optimization problem . finally , we show empirical results that demonstrate that adversarial training leads to areas with smaller hessian spectrum . we present detailed experiments with five different network architectures tested on mnist , cifar-10 , and cifar-100 datasets .", "topics": ["optimization problem", "mnist database"]}
{"title": "a note on the uniqueness of models in social abstract argumentation", "abstract": "social abstract argumentation is a principled way to assign values to conflicting ( weighted ) arguments . in this note we discuss the important property of the uniqueness of the model .", "topics": ["value ( ethics )"]}
{"title": "a constraint programming approach for solving a queueing control problem", "abstract": "in a facility with front room and back room operations , it is useful to switch workers between the rooms in order to cope with changing customer demand . assuming stochastic customer arrival and service times , we seek a policy for switching workers such that the expected customer waiting time is minimized while the expected back room staffing is sufficient to perform all work . three novel constraint programming models and several shaving procedures for these models are presented . experimental results show that a model based on closed-form expressions together with a combination of shaving procedures is the most efficient . this model is able to find and prove optimal solutions for many problem instances within a reasonable run-time . previously , the only available approach was a heuristic algorithm . furthermore , a hybrid method combining the heuristic and the best constraint programming method is shown to perform as well as the heuristic in terms of solution quality over time , while achieving the same performance in terms of proving optimality as the pure constraint programming model . this is the first work of which we are aware that solves such queueing-based problems with constraint programming .", "topics": ["heuristic"]}
{"title": "psa : a novel optimization algorithm based on survival rules of porcellio scaber", "abstract": "bio-inspired algorithms have received a significant amount of attention in both academic and engineering societies . in this paper , based on the observation of two major survival rules of a species of woodlice , i.e . , porcellio scaber , we design and propose an algorithm called the porcellio scaber algorithm ( psa ) for solving optimization problems , including differentiable and non-differential ones as well as the case with local optimums . numerical results based on benchmark problems are presented to validate the efficacy of psa .", "topics": ["mathematical optimization", "numerical analysis"]}
{"title": "modelling local and global phenomena with sparse gaussian processes", "abstract": "much recent work has concerned sparse approximations to speed up the gaussian process regression from the unfavorable o ( n3 ) scaling in computational time to o ( nm2 ) . thus far , work has concentrated on models with one covariance function . however , in many practical situations additive models with multiple covariance functions may perform better , since the data may contain both long and short length-scale phenomena . the long length-scales can be captured with global sparse approximations , such as fully independent conditional ( fic ) , and the short length-scales can be modeled naturally by covariance functions with compact support ( cs ) . cs covariance functions lead to naturally sparse covariance matrices , which are computationally cheaper to handle than full covariance matrices . in this paper , we propose a new sparse gaussian process model with two additive components : fic for the long length-scales and cs covariance function for the short length-scales . we give theoretical and experimental results and show that under certain conditions the proposed model has the same computational complexity as fic . we also compare the model performance of the proposed model to additive models approximated by fully and partially independent conditional ( pic ) . we use real data sets and show that our model outperforms fic and pic approximations for data sets with two additive phenomena .", "topics": ["computational complexity theory", "sparse matrix"]}
{"title": "asymptotic behavior of mean partitions in consensus clustering", "abstract": "although consistency is a minimum requirement of any estimator , little is known about consistency of the mean partition approach in consensus clustering . this contribution studies the asymptotic behavior of mean partitions . we show that under normal assumptions , the mean partition approach is consistent and asymptotic normal . to derive both results , we represent partitions as points of some geometric space , called orbit space . then we draw on results from the theory of fr\\'echet means and stochastic programming . the asymptotic properties hold for continuous extensions of standard cluster criteria ( indices ) . the results justify consensus clustering using finite but sufficiently large sample sizes . furthermore , the orbit space framework provides a mathematical foundation for studying further statistical , geometrical , and analytical properties of sets of partitions .", "topics": ["cluster analysis"]}
{"title": "hand tracking based on hierarchical clustering of range data", "abstract": "fast and robust hand segmentation and tracking is an essential basis for gesture recognition and thus an important component for contact-less human-computer interaction ( hci ) . hand gesture recognition based on 2d video data has been intensively investigated . however , in practical scenarios purely intensity based approaches suffer from uncontrollable environmental conditions like cluttered background colors . in this paper we present a real-time hand segmentation and tracking algorithm using time-of-flight ( tof ) range cameras and intensity data . the intensity and range information is fused into one pixel value , representing its combined intensity-depth homogeneity . the scene is hierarchically clustered using a gpu based parallel merging algorithm , allowing a robust identification of both hands even for inhomogeneous backgrounds . after the detection , both hands are tracked on the cpu . our tracking algorithm can cope with the situation that one hand is temporarily covered by the other hand .", "topics": ["pixel"]}
{"title": "non-minimal triangulations for mixed stochastic/deterministic graphical models", "abstract": "we observe that certain large-clique graph triangulations can be useful to reduce both computational and space requirements when making queries on mixed stochastic/deterministic graphical models . we demonstrate that many of these large-clique triangulations are non-minimal and are thus unattainable via the variable elimination algorithm . we introduce ancestral pairs as the basis for novel triangulation heuristics and prove that no more than the addition of edges between ancestral pairs need be considered when searching for state space optimal triangulations in such graphs . empirical results on random and real world graphs show that the resulting triangulations that yield significant speedups are almost always non-minimal . we also give an algorithm and correctness proof for determining if a triangulation can be obtained via elimination , and we show that the decision problem associated with finding optimal state space triangulations in this mixed stochastic/deterministic setting is np-complete .", "topics": ["graphical model", "heuristic"]}
{"title": "introducing geometry in active learning for image segmentation", "abstract": "we propose an active learning approach to training a segmentation classifier that exploits geometric priors to streamline the annotation process in 3d image volumes . to this end , we use these priors not only to select voxels most in need of annotation but to guarantee that they lie on 2d planar patch , which makes it much easier to annotate than if they were randomly distributed in the volume . a simplified version of this approach is effective in natural 2d images . we evaluated our approach on electron microscopy and magnetic resonance image volumes , as well as on natural images . comparing our approach against several accepted baselines demonstrates a marked performance increase .", "topics": ["image segmentation"]}
{"title": "a novel approach for effective learning in low resourced scenarios", "abstract": "deep learning based discriminative methods , being the state-of-the-art machine learning techniques , are ill-suited for learning from lower amounts of data . in this paper , we propose a novel framework , called simultaneous two sample learning ( s2sl ) , to effectively learn the class discriminative characteristics , even from very low amount of data . in s2sl , more than one sample ( here , two samples ) are simultaneously considered to both , train and test the classifier . we demonstrate our approach for speech/music discrimination and emotion classification through experiments . further , we also show the effectiveness of s2sl approach for classification in low-resource scenario , and for imbalanced data .", "topics": ["statistical classification"]}
{"title": "to prune , or not to prune : exploring the efficacy of pruning for model compression", "abstract": "model pruning seeks to induce sparsity in a deep neural network 's various connection matrices , thereby reducing the number of nonzero-valued parameters in the model . recent reports ( han et al . , 2015 ; narang et al . , 2017 ) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size . this hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model 's dense connection structure , exposing a similar trade-off in model size and accuracy . we investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process . we compare the accuracy of large , but pruned models ( large-sparse ) and their smaller , but dense ( small-dense ) counterparts with identical memory footprint . across a broad range of neural network architectures ( deep cnns , stacked lstm , and seq2seq lstm models ) , we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy .", "topics": ["baseline ( configuration management )", "sparse matrix"]}
{"title": "a better alternative to piecewise linear time series segmentation", "abstract": "time series are difficult to monitor , summarize and predict . segmentation organizes time series into few intervals having uniform characteristics ( flatness , linearity , modality , monotonicity and so on ) . for scalability , we require fast linear time algorithms . the popular piecewise linear model can determine where the data goes up or down and at what rate . unfortunately , when the data does not follow a linear model , the computation of the local slope creates overfitting . we propose an adaptive time series model where the polynomial degree of each interval vary ( constant , linear and so on ) . given a number of regressors , the cost of each interval is its polynomial degree : constant intervals cost 1 regressor , linear intervals cost 2 regressors , and so on . our goal is to minimize the euclidean ( l_2 ) error for a given model complexity . experimentally , we investigate the model where intervals can be either constant or linear . over synthetic random walks , historical stock market prices , and electrocardiograms , the adaptive model provides a more accurate segmentation than the piecewise linear model without increasing the cross-validation error or the running time , while providing a richer vocabulary to applications . implementation issues , such as numerical stability and real-world performance , are discussed .", "topics": ["time series", "time complexity"]}
{"title": "denoising adversarial autoencoders", "abstract": "unsupervised learning is of growing interest because it unlocks the potential held in vast amounts of unlabelled data to learn useful representations for inference . autoencoders , a form of generative model , may be trained by learning to reconstruct unlabelled input data from a latent representation space . more robust representations may be produced by an autoencoder if it learns to recover clean input samples from corrupted ones . representations may be further improved by introducing regularisation during training to shape the distribution of the encoded data in latent space . we suggest denoising adversarial autoencoders , which combine denoising and regularisation , shaping the distribution of latent space using adversarial training . we introduce a novel analysis that shows how denoising may be incorporated into the training and sampling of adversarial autoencoders . experiments are performed to assess the contributions that denoising makes to the learning of representations for classification and sample synthesis . our results suggest that autoencoders trained using a denoising criterion achieve higher classification performance , and can synthesise samples that are more consistent with the input data than those trained without a corruption process .", "topics": ["sampling ( signal processing )", "generative model"]}
{"title": "near-lossless binarization of word embeddings", "abstract": "is it possible to learn binary word embeddings of arbitrary size from their real-value counterparts with ( almost ) no loss in task performance ? if so , inferences performed in downstream nlp applications would benefit a massive speed-up brought by binary representations . in this paper , we derive an autoencoder architecture to learn semantic preserving binary embeddings from existing real-value ones . a binary encoder requires an element-wise function that outputs a bit given a real value - it is unfortunately highly non-differentiable . we propose to use the same parameters matrix for the encoder and the decoder so that we are able to learn weights using the decoder . we also show that it is possible and desirable to minimize the correlation between the different binary features at training time through ad hoc regularization . the learned binary codes can be of arbitrary sizes , and the binary representations yield ( almost ) the same performances than their real-value counterparts . finally , we show that we are able to recon- struct semantic-preserving real-value embeddings from the binary embeddings - the cherry on top .", "topics": ["natural language processing", "matrix regularization"]}
{"title": "smash : one-shot model architecture search through hypernetworks", "abstract": "designing architectures for deep neural networks requires expert knowledge and substantial computation time . we propose a technique to accelerate architecture selection by learning an auxiliary hypernet that generates the weights of a main model conditioned on that model 's architecture . by comparing the relative validation performance of networks with hypernet-generated weights , we can effectively search over a wide range of architectures at the cost of a single training run . to facilitate this search , we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns , with resnet , densenet , and fractalnet blocks as special cases . we validate our method ( smash ) on cifar-10 and cifar-100 , stl-10 , modelnet10 , and imagenet32x32 , achieving competitive performance with similarly-sized hand-designed networks . our code is available at https : //github.com/ajbrock/smash", "topics": ["time complexity", "computation"]}
{"title": "oil price trackers inspired by immune memory", "abstract": "we outline initial concepts for an immune inspired algorithm to evaluate and predict oil price time series data . the proposed solution evolves a short term pool of trackers dynamically , with each member attempting to map trends and anticipate future price movements . successful trackers feed into a long term memory pool that can generalise across repeating trend patterns . the resulting sequence of trackers , ordered in time , can be used as a forecasting tool . examination of the pool of evolving trackers also provides valuable insight into the properties of the crude oil market .", "topics": ["time series"]}
{"title": "what will i do next ? the intention from motion experiment", "abstract": "in computer vision , video-based approaches have been widely explored for the early classification and the prediction of actions or activities . however , it remains unclear whether this modality ( as compared to 3d kinematics ) can still be reliable for the prediction of human intentions , defined as the overarching goal embedded in an action sequence . since the same action can be performed with different intentions , this problem is more challenging but yet affordable as proved by quantitative cognitive studies which exploit the 3d kinematics acquired through motion capture systems . in this paper , we bridge cognitive and computer vision studies , by demonstrating the effectiveness of video-based approaches for the prediction of human intentions . precisely , we propose intention from motion , a new paradigm where , without using any contextual information , we consider instantaneous grasping motor acts involving a bottle in order to forecast why the bottle itself has been reached ( to pass it or to place in a box , or to pour or to drink the liquid inside ) . we process only the grasping onsets casting intention prediction as a classification framework . leveraging on our multimodal acquisition ( 3d motion capture data and 2d optical videos ) , we compare the most commonly used 3d descriptors from cognitive studies with state-of-the-art video-based techniques . since the two analyses achieve an equivalent performance , we demonstrate that computer vision tools are effective in capturing the kinematics and facing the cognitive problem of human intention prediction .", "topics": ["computer vision"]}
{"title": "investigation of a chaotic spiking neuron model", "abstract": "chaos provides many interesting properties that can be used to achieve computational tasks . such properties are sensitivity to initial conditions , space filling , control and synchronization . chaotic neural models have been devised to exploit such properties . in this paper , a chaotic spiking neuron model is investigated experimentally . this investigation is performed to understand the dynamic behaviours of the model . the aim of this research is to investigate the dynamics of the nonlinear dynamic state neuron ( nds ) experimentally . the experimental approach has revealed some quantitative and qualitative properties of the nds model such as the control mechanism , the reset mechanism , and the way the model may exhibit dynamic behaviours in phase space . it is shown experimentally in this paper that both the reset mechanism and the self-feed back control mechanism are important for the nds model to work and to stabilise to one of the large number of available unstable periodic orbits ( upos ) that are embedded in its attractor . the experimental investigation suggests that the internal dynamics of the nds neuron provide a rich set of dynamic behaviours that can be controlled and stabilised . these wide range of dynamic behaviours may be exploited to carry out information processing tasks .", "topics": ["nonlinear system"]}
{"title": "localization under topological uncertainty for lane identification of autonomous vehicles", "abstract": "autonomous vehicles ( avs ) require accurate metric and topological location estimates for safe , effective navigation and decision-making . although many high-definition ( hd ) roadmaps exist , they are not always accurate since public roads are dynamic , shaped unpredictably by both human activity and nature . thus , avs must be able to handle situations in which the topology specified by the map does not agree with reality . we present the variable structure multiple hidden markov model ( vsm-hmm ) as a framework for localizing in the presence of topological uncertainty , and demonstrate its effectiveness on an av where lane membership is modeled as a topological localization process . vsm-hmms use a dynamic set of hmms to simultaneously reason about location within a set of most likely current topologies and therefore may also be applied to topological structure estimation as well as av lane estimation . in addition , we present an extension to the earth mover 's distance which allows uncertainty to be taken into account when computing the distance between belief distributions on simplices of arbitrary relative sizes .", "topics": ["autonomous car"]}
{"title": "risk-sensitive and robust decision-making : a cvar optimization approach", "abstract": "in this paper we address the problem of decision making within a markov decision process ( mdp ) framework where risk and modeling errors are taken into account . our approach is to minimize a risk-sensitive conditional-value-at-risk ( cvar ) objective , as opposed to a standard risk-neutral expectation . we refer to such problem as cvar mdp . our first contribution is to show that a cvar objective , besides capturing risk sensitivity , has an alternative interpretation as expected cost under worst-case modeling errors , for a given error budget . this result , which is of independent interest , motivates cvar mdps as a unifying framework for risk-sensitive and robust decision making . our second contribution is to present an approximate value-iteration algorithm for cvar mdps and analyze its convergence rate . to our knowledge , this is the first solution algorithm for cvar mdps that enjoys error guarantees . finally , we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach .", "topics": ["approximation algorithm", "numerical analysis"]}
{"title": "global optimality of local search for low rank matrix recovery", "abstract": "we show that there are no spurious local minima in the non-convex factorized parametrization of low-rank matrix recovery from incoherent linear measurements . with noisy measurements we show all local minima are very close to a global optimum . together with a curvature bound at saddle points , this yields a polynomial time global convergence guarantee for stochastic gradient descent { \\em from random initialization } .", "topics": ["time complexity", "gradient descent"]}
{"title": "unsupervised learning of visual representations using videos", "abstract": "is strong supervision necessary for learning a good visual representation ? do we really need millions of semantically-labeled images to train a convolutional neural network ( cnn ) ? in this paper , we present a simple yet surprisingly powerful approach for unsupervised learning of cnn . specifically , we use hundreds of thousands of unlabeled videos from the web to learn visual representations . our key idea is that visual tracking provides the supervision . that is , two patches connected by a track should have similar visual representation in deep feature space since they probably belong to the same object or object part . we design a siamese-triplet network with a ranking loss function to train this cnn representation . without using a single image from imagenet , just using 100k unlabeled videos and the voc 2012 dataset , we train an ensemble of unsupervised networks that achieves 52 % map ( no bounding box regression ) . this performance comes tantalizingly close to its imagenet-supervised counterpart , an ensemble which achieves a map of 54.4 % . we also show that our unsupervised network can perform competitively in other tasks such as surface-normal estimation .", "topics": ["unsupervised learning"]}
{"title": "real time facial expression recognition using a novel method", "abstract": "this paper discusses a novel method for facial expression recognition system which performs facial expression analysis in a near real time from a live web cam feed . primary objectives were to get results in a near real time with light invariant , person independent and pose invariant way . the system is composed of two different entities trainer and evaluator . each frame of video feed is passed through a series of steps including haar classifiers , skin detection , feature extraction , feature points tracking , creating a learned support vector machine model to classify emotions to achieve a tradeoff between accuracy and result rate . a processing time of 100-120 ms per 10 frames was achieved with accuracy of around 60 % . we measure our accuracy in terms of variety of interaction and classification scenarios . we conclude by discussing relevance of our work to human computer interaction and exploring further measures that can be taken .", "topics": ["statistical classification", "support vector machine"]}
{"title": "counterfactual multi-agent policy gradients", "abstract": "cooperative multi-agent systems can be naturally used to model many real world problems , such as network packet routing and the coordination of autonomous vehicles . there is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems . to this end , we propose a new multi-agent actor-critic method called counterfactual multi-agent ( coma ) policy gradients . coma uses a centralised critic to estimate the q-function and decentralised actors to optimise the agents ' policies . in addition , to address the challenges of multi-agent credit assignment , it uses a counterfactual baseline that marginalises out a single agent 's action , while keeping the other agents ' actions fixed . coma also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass . we evaluate coma in the testbed of starcraft unit micromanagement , using a decentralised variant with significant partial observability . coma significantly improves average performance over other multi-agent actor-critic methods in this setting , and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state .", "topics": ["baseline ( configuration management )", "reinforcement learning"]}
{"title": "multi-view metric learning for multi-instance image classification", "abstract": "it is critical and meaningful to make image classification since it can help human in image retrieval and recognition , object detection , etc . in this paper , three-sides efforts are made to accomplish the task . first , visual features with bag-of-words representation , not single vector , are extracted to characterize the image . to improve the performance , the idea of multi-view learning is implemented and three kinds of features are provided , each one corresponds to a single view . the information from three views is complementary to each other , which can be unified together . then a new distance function is designed for bags by computing the weighted sum of the distances between instances . the technique of metric learning is explored to construct a data-dependent distance metric to measure the relationships between instances , meanwhile between bags and images , more accurately . last , a novel approach , called mvml , is proposed , which optimizes the joint probability that every image is similar with its nearest image . mvml learns multiple distance metrics , each one models a single view , to unifies the information from multiple views . the method can be solved by alternate optimization iteratively . gradient ascent and positive semi-definite projection are utilized in the iterations . distance comparisons verified that the new bag distance function is prior to previous functions . in model evaluation , numerical experiments show that mvml with multiple views performs better than single view condition , which demonstrates that our model can assemble the complementary information efficiently and measure the distance between images more precisely . experiments on influence of parameters and instance number validate the consistency of the method .", "topics": ["object detection", "numerical analysis"]}
{"title": "yedda : a lightweight collaborative text span annotation tool", "abstract": "in this paper , we introduce yedda , a lightweight but efficient and comprehensive open-source tool for text span annotation . yedda provides a systematic solution for text span annotation , ranging from collaborative user annotation to administrator evaluation and analysis . it overcomes the low efficiency of traditional text annotation tools by annotating entities through both command line and shortcut keys , which are configurable with custom labels . yedda also gives intelligent recommendations by learning the up-to-date annotated text . an administrator client is developed to evaluate annotation quality of multiple annotators and generate detailed comparison report for each annotator pair . experiments show that the proposed system can reduce the annotation time by half compared with existing annotation tools . and the annotation time can be further compressed by 16.47\\ % through intelligent recommendation .", "topics": ["entity"]}
{"title": "an attention-based word-level interaction model : relation detection for knowledge base question answering", "abstract": "relation detection plays a crucial role in knowledge base question answering ( kbqa ) because of the high variance of relation expression in the question . traditional deep learning methods follow an encoding-comparing paradigm , where the question and the candidate relation are represented as vectors to compare their semantic similarity . max- or average- pooling operation , which compresses the sequence of words into fixed-dimensional vectors , becomes the bottleneck of information . in this paper , we propose to learn attention-based word-level interactions between questions and relations to alleviate the bottleneck issue . similar to the traditional models , the question and relation are firstly represented as sequences of vectors . then , instead of merging the sequence into a single vector with pooling operation , soft alignments between words from the question and the relation are learned . the aligned words are subsequently compared with the convolutional neural network ( cnn ) and the comparison results are merged finally . through performing the comparison on low-level representations , the attention-based word-level interaction model ( abwim ) relieves the information loss issue caused by merging the sequence into a fixed-dimensional vector before the comparison . the experimental results of relation detection on both simplequestions and webquestions datasets show that abwim achieves state-of-the-art accuracy , demonstrating its effectiveness .", "topics": ["high- and low-level", "interaction"]}
{"title": "efficient iterative policy optimization", "abstract": "we tackle the issue of finding a good policy when the number of policy updates is limited . this is done by approximating the expected policy reward as a sequence of concave lower bounds which can be efficiently maximized , drastically reducing the number of policy updates required to achieve good performance . we also extend existing methods to negative rewards , enabling the use of control variates .", "topics": ["approximation algorithm", "optimization problem"]}
{"title": "multi-layer representation learning for medical concepts", "abstract": "learning efficient representations for concepts has been proven to be an important basis for many applications such as machine translation or document classification . proper representations of medical concepts such as diagnosis , medication , procedure codes and visits will have broad applications in healthcare analytics . however , in electronic health records ( ehr ) the visit sequences of patients include multiple concepts ( diagnosis , procedure , and medication codes ) per visit . this structure provides two types of relational information , namely sequential order of visits and co-occurrence of the codes within each visit . in this work , we propose med2vec , which not only learns distributed representations for both medical codes and visits from a large ehr dataset with over 3 million visits , but also allows us to interpret the learned representations confirmed positively by clinical experts . in the experiments , med2vec displays significant improvement in key medical applications compared to popular baselines such as skip-gram , glove and stacked autoencoder , while providing clinically meaningful interpretation .", "topics": ["autoencoder"]}
{"title": "structured attentions for visual question answering", "abstract": "visual attention , which assigns weights to image regions according to their relevance to a question , is considered as an indispensable part by most visual question answering models . although the questions may involve complex relations among multiple regions , few attention models can effectively encode such cross-region relations . in this paper , we demonstrate the importance of encoding such relations by showing the limited effective receptive field of resnet on two datasets , and propose to model the visual attention as a multivariate distribution over a grid-structured conditional random field on image regions . we demonstrate how to convert the iterative inference algorithms , mean field and loopy belief propagation , as recurrent layers of an end-to-end neural network . we empirically evaluated our model on 3 datasets , in which it surpasses the best baseline model of the newly released clevr dataset by 9.5 % , and the best published model on the vqa dataset by 1.25 % . source code is available at https : //github.com/zhuchen03/vqa-sva .", "topics": ["baseline ( configuration management )", "relevance"]}
{"title": "compression of deep convolutional neural networks for fast and low power mobile applications", "abstract": "although the latest high-end smartphone has powerful cpu and gpu , running deeper convolutional neural networks ( cnns ) for complex tasks such as imagenet classification on mobile devices is challenging . to deploy deep cnns on mobile devices , we present a simple and effective scheme to compress the entire cnn , which we call one-shot whole network compression . the proposed scheme consists of three steps : ( 1 ) rank selection with variational bayesian matrix factorization , ( 2 ) tucker decomposition on kernel tensor , and ( 3 ) fine-tuning to recover accumulated loss of accuracy , and each step can be easily implemented using publicly available tools . we demonstrate the effectiveness of the proposed scheme by testing the performance of various compressed cnns ( alexnet , vggs , googlenet , and vgg-16 ) on the smartphone . significant reductions in model size , runtime , and energy consumption are obtained , at the cost of small loss in accuracy . in addition , we address the important implementation level issue on 1 ? 1 convolution , which is a key operation of inception module of googlenet as well as cnns compressed by our proposed scheme .", "topics": ["calculus of variations", "convolution"]}
{"title": "robust statistical ranking : theory and algorithms", "abstract": "deeply rooted in classical social choice and voting theory , statistical ranking with paired comparison data experienced its renaissance with the wide spread of crowdsourcing technique . as the data quality might be significantly damaged in an uncontrolled crowdsourcing environment , outlier detection and robust ranking have become a hot topic in such data analysis . in this paper , we propose a robust ranking framework based on the principle of huber 's robust statistics , which formulates outlier detection as a lasso problem to find sparse approximations of the cyclic ranking projection in hodge decomposition . moreover , simple yet scalable algorithms are developed based on linearized bregman iteration to achieve an even less biased estimator than lasso . statistical consistency of outlier detection is established in both cases which states that when the outliers are strong enough and in erdos-renyi random graph sampling settings , outliers can be faithfully detected . our studies are supported by experiments with both simulated examples and real-world data . the proposed framework provides us a promising tool for robust ranking with large scale crowdsourcing data arising from computer vision , multimedia , machine learning , sociology , etc .", "topics": ["computer vision", "simulation"]}
{"title": "sample-efficient algorithms for recovering structured signals from magnitude-only measurements", "abstract": "we consider the problem of recovering a signal $ \\mathbf { x } ^* \\in \\mathbf { r } ^n $ , from magnitude-only measurements $ y_i = |\\left\\langle\\mathbf { a } _i , \\mathbf { x } ^*\\right\\rangle| $ for $ i= [ m ] $ . also called the phase retrieval , this is a fundamental challenge in bio- , astronomical imaging and speech processing . the problem above is ill-posed ; additional assumptions on the signal and/or the measurements are necessary . in this paper we first study the case where the signal $ \\mathbf { x } ^* $ is $ s $ -sparse . we develop a novel algorithm that we call compressive phase retrieval with alternating minimization , or copram . our algorithm is simple ; it combines the classical alternating minimization approach for phase retrieval with the cosamp algorithm for sparse recovery . despite its simplicity , we prove that copram achieves a sample complexity of $ o ( s^2\\log n ) $ with gaussian measurements $ \\mathbf { a } _i $ , matching the best known existing results ; moreover , it demonstrates linear convergence in theory and practice . additionally , it requires no extra tuning parameters other than signal sparsity $ s $ and is robust to noise . when the sorted coefficients of the sparse signal exhibit a power law decay , we show that copram achieves a sample complexity of $ o ( s\\log n ) $ , which is close to the information-theoretic limit . we also consider the case where the signal $ \\mathbf { x } ^* $ arises from structured sparsity models . we specifically examine the case of block-sparse signals with uniform block size of $ b $ and block sparsity $ k=s/b $ . for this problem , we design a recovery algorithm block copram that further reduces the sample complexity to $ o ( ks\\log n ) $ . for sufficiently large block lengths of $ b=\\theta ( s ) $ , this bound equates to $ o ( s\\log n ) $ . to our knowledge , this constitutes the first end-to-end algorithm for phase retrieval where the gaussian sample complexity has a sub-quadratic dependence on the signal sparsity level .", "topics": ["sparse matrix", "coefficient"]}
{"title": "emergent translation in multi-agent communication", "abstract": "while most machine translation systems to date are trained on large parallel corpora , humans learn language in a different way : by being grounded in an environment and interacting with other humans . in this work , we propose a communication game where two agents , native speakers of their own respective languages , jointly learn to solve a visual referential task . we find that the ability to understand and translate a foreign language emerges as a means to achieve shared goals . the emergent translation is interactive and multimodal , and crucially does not require parallel corpora , but only monolingual , independent text and corresponding images . our proposed translation model achieves this by grounding the source and target languages into a shared visual modality , and outperforms several baselines on both word-level and sentence-level translation tasks . furthermore , we show that agents in a multilingual community learn to translate better and faster than in a bilingual communication setting .", "topics": ["baseline ( configuration management )", "machine translation"]}
{"title": "applications of probabilistic programming ( master 's thesis , 2015 )", "abstract": "this thesis describes work on two applications of probabilistic programming : the learning of probabilistic program code given specifications , in particular program code of one-dimensional samplers ; and the facilitation of sequential monte carlo inference with help of data-driven proposals . the latter is presented with experimental results on a linear gaussian model and a non-parametric dependent dirichlet process mixture of objects model for object recognition and tracking . in chapter 1 we provide a brief introduction to probabilistic programming . in chapter 3 we present an approach to automatic discovery of samplers in the form of probabilistic programs . we formulate a bayesian approach to this problem by specifying a grammar-based prior over probabilistic program code . we use an approximate bayesian computation method to learn the programs , whose executions generate samples that statistically match observed data or analytical characteristics of distributions of interest . in our experiments we leverage different probabilistic programming systems to perform markov chain monte carlo sampling over the space of programs . experimental results have demonstrated that , using the proposed methodology , we can learn approximate and even some exact samplers . finally , we show that our results are competitive with regard to genetic programming methods . in chapter 3 , we describe a way to facilitate sequential monte carlo inference in probabilistic programming using data-driven proposals . in particular , we develop a distance-based proposal for the non-parametric dependent dirichlet process mixture of objects model . we implement this approach in the probabilistic programming system anglican , and show that for that model data-driven proposals provide significant performance improvements . we also explore the possibility of using neural networks to improve data-driven proposals .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "alternative technique to asymmetry analysis-based overlapping for foot ulcer examination : scalable scanning", "abstract": "asymmetry analysis based on the overlapping of thermal images proved able to detect inflammation and , predict foot ulceration . this technique involves three main steps : segmentation , geometric transformation , and overlapping . however , the overlapping technique , which consists of subtracting the intensity levels of the right foot from those of the left foot , can also detect false abnormal areas if the projections of the left and right feet are not the same . in this paper , we present an alternative technique to asymmetry analysis-based overlapping . the proposed technique , scalable scanning , allows for an effective comparison even if the shapes and sizes of the feet projections appear differently in the image . the tested results show that asymmetry analysis- based scalable scanning provides fewer false abnormal areas than does asymmetry analysis -based overlapping .", "topics": ["image segmentation", "scalability"]}
{"title": "employing weak annotations for medical image analysis problems", "abstract": "to efficiently establish training databases for machine learning methods , collaborative and crowdsourcing platforms have been investigated to collectively tackle the annotation effort . however , when this concept is ported to the medical imaging domain , reading expertise will have a direct impact on the annotation accuracy . in this study , we examine the impact of expertise and the amount of available annotations on the accuracy outcome of a liver segmentation problem in an abdominal computed tomography ( ct ) image database . in controlled experiments , we study this impact for different types of weak annotations . to address the decrease in accuracy associated with lower expertise , we propose a method for outlier correction making use of a weakly labelled atlas . using this approach , we demonstrate that weak annotations subject to high error rates can achieve a similarly high accuracy as state-of-the-art multi-atlas segmentation approaches relying on a large amount of expert manual segmentations . annotations of this nature can realistically be obtained from a non-expert crowd and can potentially enable crowdsourcing of weak annotation tasks for medical image analysis .", "topics": ["database"]}
{"title": "interpolated policy gradient : merging on-policy and off-policy gradient estimation for deep reinforcement learning", "abstract": "off-policy model-free deep reinforcement learning methods using previously collected data can improve sample efficiency over on-policy policy gradient techniques . on the other hand , on-policy algorithms are often more stable and easier to use . this paper examines , both theoretically and empirically , approaches to merging on- and off-policy updates for deep reinforcement learning . theoretical results show that off-policy updates with a value function estimator can be interpolated with on-policy policy gradient updates whilst still satisfying performance bounds . our analysis uses control variate methods to produce a family of policy gradient algorithms , with several recently proposed algorithms being special cases of this family . we then provide an empirical comparison of these techniques with the remaining algorithmic details fixed , and show how different mixing of off-policy gradient estimates with on-policy samples contribute to improvements in empirical performance . the final algorithm provides a generalization and unification of existing deep policy gradient techniques , has theoretical guarantees on the bias introduced by off-policy updates , and improves on the state-of-the-art model-free deep rl methods on a number of openai gym continuous control benchmarks .", "topics": ["reinforcement learning", "gradient"]}
{"title": "bridge correlational neural networks for multilingual multimodal representation learning", "abstract": "recently there has been a lot of interest in learning common representations for multiple views of data . typically , such common representations are learned using a parallel corpus between the two views ( say , 1m images and their english captions ) . in this work , we address a real-world scenario where no direct parallel data is available between two views of interest ( say , $ v_1 $ and $ v_2 $ ) but parallel data is available between each of these views and a pivot view ( $ v_3 $ ) . we propose a model for learning a common representation for $ v_1 $ , $ v_2 $ and $ v_3 $ using only the parallel data available between $ v_1v_3 $ and $ v_2v_3 $ . the proposed model is generic and even works when there are $ n $ views of interest and only one pivot view which acts as a bridge between them . there are two specific downstream applications that we focus on ( i ) transfer learning between languages $ l_1 $ , $ l_2 $ , ... , $ l_n $ using a pivot language $ l $ and ( ii ) cross modal access between images and a language $ l_1 $ using a pivot language $ l_2 $ . our model achieves state-of-the-art performance in multilingual document classification on the publicly available multilingual ted corpus and promising results in multilingual multimodal retrieval on a new dataset created and released as a part of this work .", "topics": ["neural networks"]}
{"title": "graph construction with label information for semi-supervised learning", "abstract": "in the literature , most existing graph-based semi-supervised learning ( ssl ) methods only use the label information of observed samples in the label propagation stage , while ignoring such valuable information when learning the graph . in this paper , we argue that it is beneficial to consider the label information in the graph learning stage . specifically , by enforcing the weight of edges between labeled samples of different classes to be zero , we explicitly incorporate the label information into the state-of-the-art graph learning methods , such as the low-rank representation ( lrr ) , and propose a novel semi-supervised graph learning method called semi-supervised low-rank representation ( sslrr ) . this results in a convex optimization problem with linear constraints , which can be solved by the linearized alternating direction method . though we take lrr as an example , our proposed method is in fact very general and can be applied to any self-representation graph learning methods . experiment results on both synthetic and real datasets demonstrate that the proposed graph learning method can better capture the global geometric structure of the data , and therefore is more effective for semi-supervised learning tasks .", "topics": ["supervised learning", "optimization problem"]}
{"title": "online stochastic linear optimization under one-bit feedback", "abstract": "in this paper , we study a special bandit setting of online stochastic linear optimization , where only one-bit of information is revealed to the learner at each round . this problem has found many applications including online advertisement and online recommendation . we assume the binary feedback is a random variable generated from the logit model , and aim to minimize the regret defined by the unknown linear function . although the existing method for generalized linear bandit can be applied to our problem , the high computational cost makes it impractical for real-world problems . to address this challenge , we develop an efficient online learning algorithm by exploiting particular structures of the observation model . specifically , we adopt online newton step to estimate the unknown parameter and derive a tight confidence region based on the exponential concavity of the logistic loss . our analysis shows that the proposed algorithm achieves a regret bound of $ o ( d\\sqrt { t } ) $ , which matches the optimal result of stochastic linear bandits .", "topics": ["regret ( decision theory )", "time complexity"]}
{"title": "q-learning with nearest neighbors", "abstract": "we consider the problem of model-free reinforcement learning for infinite-horizon discounted markov decision processes ( mdps ) with a continuous state space and unknown transition kernels , when only a single sample path of the system is available . we focus on the classical approach of q-learning where the goal is to learn the optimal q-function . we propose the nearest neighbor q-learning approach that utilizes nearest neighbor regression method to learn the q function . we provide finite sample analysis of the convergence rate using this method . in particular , we establish that the algorithm is guaranteed to output an $ \\epsilon $ -accurate estimate of the optimal q-function with high probability using a number of observations that depends polynomially on $ \\epsilon $ and the model parameters . to establish our results , we develop a robust version of stochastic approximation results ; this may be of interest in its own right .", "topics": ["reinforcement learning", "markov chain"]}
{"title": "predicting face recognition performance using image quality", "abstract": "this paper proposes a data driven model to predict the performance of a face recognition system based on image quality features . we model the relationship between image quality features ( e.g . pose , illumination , etc . ) and recognition performance measures using a probability density function . to address the issue of limited nature of practical training data inherent in most data driven models , we have developed a bayesian approach to model the distribution of recognition performance measures in small regions of the quality space . since the model is based solely on image quality features , it can predict performance even before the actual recognition has taken place . we evaluate the performance predictive capabilities of the proposed model for six face recognition systems ( two commercial and four open source ) operating on three independent data sets : multipie , frgc and cas-peal . our results show that the proposed model can accurately predict performance using an accurate and unbiased image quality assessor ( iqa ) . furthermore , our experiments highlight the impact of the unaccounted quality space -- the image quality features not considered by iqa -- in contributing to performance prediction errors .", "topics": ["test set"]}
{"title": "recognizing gender from human facial regions using genetic algorithm", "abstract": "recently , recognition of gender from facial images has gained a lot of importance . there exist a handful of research work that focus on feature extraction to obtain gender specific information from facial images . however , analyzing different facial regions and their fusion help in deciding the gender of a person from facial images . in this paper , we propose a new approach to identify gender from frontal facial images that is robust to background , illumination , intensity , and facial expression . in our framework , first the frontal face image is divided into a number of distinct regions based on facial landmark points that are obtained by the chehra model proposed by asthana et al . the model provides 49 facial landmark points covering different regions of the face , e.g . forehead , left eye , right eye , lips . next , a face image is segmented into facial regions using landmark points and features are extracted from each region . the compass lbp feature , a variant of lbp feature , has been used in our framework to obtain discriminative gender-specific information . following this , a support vector machine based classifier has been used to compute the probability scores from each facial region . finally , the classification scores obtained from individual regions are combined with a genetic algorithm based learning to improve the overall classification accuracy . the experiments have been performed on popular face image datasets such as adience , cferet ( color feret ) , lfw and two sketch datasets , namely cufs and cufsf . through experiments , we have observed that , the proposed method outperforms existing approaches .", "topics": ["feature extraction", "support vector machine"]}
{"title": "variational dropout sparsifies deep neural networks", "abstract": "we explore a recently proposed variational dropout technique that provided an elegant bayesian interpretation to gaussian dropout . we extend variational dropout to the case when dropout rates are unbounded , propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight . interestingly , it leads to extremely sparse solutions both in fully-connected and convolutional layers . this effect is similar to automatic relevance determination effect in empirical bayes but has a number of advantages . we reduce the number of parameters up to 280 times on lenet architectures and up to 68 times on vgg-like networks with a negligible decrease of accuracy .", "topics": ["calculus of variations", "sparse matrix"]}
{"title": "visualization regularizers for neural network based image recognition", "abstract": "the success of deep neural networks is mostly due their ability to learn meaningful features from the data . features learned in the hidden layers of deep neural networks trained in computer vision tasks have been shown to be similar to mid-level vision features . we leverage this fact in this work and propose the visualization regularizer for image tasks . the proposed regularization technique enforces smoothness of the features learned by hidden nodes and turns out to be a special case of tikhonov regularization . we achieve higher classification accuracy as compared to existing regularizers such as the l2 norm regularizer and dropout , on benchmark datasets without changing the training computational complexity .", "topics": ["computational complexity theory", "computer vision"]}
{"title": "intelligent parameter tuning in optimization-based iterative ct reconstruction via deep reinforcement learning", "abstract": "a number of image-processing problems can be formulated as optimization problems . the objective function typically contains several terms specifically designed for different purposes . parameters in front of these terms are used to control the relative weights among them . it is of critical importance to tune these parameters , as quality of the solution depends on their values . tuning parameter is a relatively straightforward task for a human , as one can intelligently determine the direction of parameter adjustment based on the solution quality . yet manual parameter tuning is not only tedious in many cases , but becomes impractical when a number of parameters exist in a problem . aiming at solving this problem , this paper proposes an approach that employs deep reinforcement learning to train a system that can automatically adjust parameters in a human-like manner . we demonstrate our idea in an example problem of optimization-based iterative ct reconstruction with a pixel-wise total-variation regularization term . we set up a parameter tuning policy network ( ptpn ) , which maps an ct image patch to an output that specifies the direction and amplitude by which the parameter at the patch center is adjusted . we train the ptpn via an end-to-end reinforcement learning procedure . we demonstrate that under the guidance of the trained ptpn for parameter tuning at each pixel , reconstructed ct images attain quality similar or better than in those reconstructed with manually tuned parameters .", "topics": ["image processing", "optimization problem"]}
{"title": "scalable kernel k-means clustering with nystrom approximation : relative-error bounds", "abstract": "kernel $ k $ -means clustering can correctly identify and extract a far more varied collection of cluster structures than the linear $ k $ -means clustering algorithm . however , kernel $ k $ -means clustering is computationally expensive when the non-linear feature map is high-dimensional and there are many input points . kernel approximation , e.g . , the nystr\\ '' om method , has been applied in previous works to approximately solve kernel learning problems when both of the above conditions are present . this work analyzes the application of this paradigm to kernel $ k $ -means clustering , and shows that applying the linear $ k $ -means clustering algorithm to $ \\frac { k } { \\epsilon } ( 1 + o ( 1 ) ) $ features constructed using a so-called rank-restricted nystr\\ '' om approximation results in cluster assignments that satisfy a $ 1 + \\epsilon $ approximation ratio in terms of the kernel $ k $ -means cost function , relative to the guarantee provided by the same algorithm without the use of the nystr\\ '' om method . as part of the analysis , this work establishes a novel $ 1 + \\epsilon $ relative-error trace norm guarantee for low-rank approximation using the rank-restricted nystr\\ '' om approximation . empirical evaluations on the $ 8.1 $ million instance mnist8m dataset demonstrate the scalability and usefulness of kernel $ k $ -means clustering with nystr\\ '' om approximation . this work argues that spectral clustering using nystr\\ '' om approximation -- -a popular and computationally efficient , but theoretically unsound approach to non-linear clustering -- -should be replaced with the efficient and theoretically sound combination of kernel $ k $ -means clustering with nystr\\ '' om approximation . the superior performance of the latter approach is empirically verified .", "topics": ["kernel ( operating system )", "approximation algorithm"]}
{"title": "uncovering group level insights with accordant clustering", "abstract": "clustering is a widely-used data mining tool , which aims to discover partitions of similar items in data . we introduce a new clustering paradigm , \\emph { accordant clustering } , which enables the discovery of ( predefined ) group level insights . unlike previous clustering paradigms that aim to understand relationships amongst the individual members , the goal of accordant clustering is to uncover insights at the group level through the analysis of their members . group level insight can often support a call to action that can not be informed through previous clustering techniques . we propose the first accordant clustering algorithm , and prove that it finds near-optimal solutions when data possesses inherent cluster structure . the insights revealed by accordant clusterings enabled experts in the field of medicine to isolate successful treatments for a neurodegenerative disease , and those in finance to discover patterns of unnecessary spending .", "topics": ["cluster analysis", "data mining"]}
{"title": "challenges of computational processing of code-switching", "abstract": "this paper addresses challenges of natural language processing ( nlp ) on non-canonical multilingual data in which two or more languages are mixed . it refers to code-switching which has become more popular in our daily life and therefore obtains an increasing amount of attention from the research community . we report our experience that cov- ers not only core nlp tasks such as normalisation , language identification , language modelling , part-of-speech tagging and dependency parsing but also more downstream ones such as machine translation and automatic speech recognition . we highlight and discuss the key problems for each of the tasks with supporting examples from different language pairs and relevant previous work .", "topics": ["natural language processing", "machine translation"]}
{"title": "multi-label image recognition by recurrently discovering attentional regions", "abstract": "this paper proposes a novel deep architecture to address multi-label image recognition , a fundamental and practical task towards general visual understanding . current solutions for this task usually rely on an extra step of extracting hypothesis regions ( i.e . , region proposals ) , resulting in redundant computation and sub-optimal performance . in this work , we achieve the interpretable and contextualized multi-label image classification by developing a recurrent memorized-attention module . this module consists of two alternately performed components : i ) a spatial transformer layer to locate attentional regions from the convolutional feature maps in a region-proposal-free way and ii ) an lstm ( long-short term memory ) sub-network to sequentially predict semantic labeling scores on the located regions while capturing the global dependencies of these regions . the lstm also output the parameters for computing the spatial transformer . on large-scale benchmarks of multi-label image classification ( e.g . , ms-coco and pascal voc 07 ) , our approach demonstrates superior performances over other existing state-of-the-arts in both accuracy and efficiency .", "topics": ["computer vision", "map"]}
{"title": "coarse-to-fine classification via parametric and nonparametric models for computer-aided diagnosis", "abstract": "classification is one of the core problems in computer-aided diagnosis ( cad ) , targeting for early cancer detection using 3d medical imaging interpretation . high detection sensitivity with desirably low false positive ( fp ) rate is critical for a cad system to be accepted as a valuable or even indispensable tool in radiologists ' workflow . given various spurious imagery noises which cause observation uncertainties , this remains a very challenging task . in this paper , we propose a novel , two-tiered coarse-to-fine ( ctf ) classification cascade framework to tackle this problem . we first obtain classification-critical data samples ( e.g . , samples on the decision boundary ) extracted from the holistic data distributions using a robust parametric model ( e.g . , \\cite { raykar08 } ) ; then we build a graph-embedding based nonparametric classifier on sampled data , which can more accurately preserve or formulate the complex classification boundary . these two steps can also be considered as effective `` sample pruning '' and `` feature pursuing + $ k $ nn/template matching '' , respectively . our approach is validated comprehensively in colorectal polyp detection and lung nodule detection cad systems , as the top two deadly cancers , using hospital scale , multi-site clinical datasets . the results show that our method achieves overall better classification/detection performance than existing state-of-the-art algorithms using single-layer classifiers , such as the support vector machine variants \\cite { wang08 } , boosting \\cite { slabaugh10 } , logistic regression \\cite { ravesteijn10 } , relevance vector machine \\cite { raykar08 } , $ k $ -nearest neighbor \\cite { murphy09 } or spectral projections on graph \\cite { cai08 } .", "topics": ["support vector machine", "support vector machine"]}
{"title": "feature based task recommendation in crowdsourcing with implicit observations", "abstract": "existing research in crowdsourcing has investigated how to recommend tasks to workers based on which task the workers have already completed , referred to as { \\em implicit feedback } . we , on the other hand , investigate the task recommendation problem , where we leverage both implicit feedback and explicit features of the task . we assume that we are given a set of workers , a set of tasks , interactions ( such as the number of times a worker has completed a particular task ) , and the presence of explicit features of each task ( such as , task location ) . we intend to recommend tasks to the workers by exploiting the implicit interactions , and the presence or absence of explicit features in the tasks . we formalize the problem as an optimization problem , propose two alternative problem formulations and respective solutions that exploit implicit feedback , explicit features , as well as similarity between the tasks . we compare the efficacy of our proposed solutions against multiple state-of-the-art techniques using two large scale real world datasets .", "topics": ["optimization problem", "interaction"]}
{"title": "inaccuracy minimization by partioning fuzzy data sets - validation of analystical methodology", "abstract": "in the last two decades , a number of methods have been proposed for forecasting based on fuzzy time series . most of the fuzzy time series methods are presented for forecasting of car road accidents . however , the forecasting accuracy rates of the existing methods are not good enough . in this paper , we compared our proposed new method of fuzzy time series forecasting with existing methods . our method is based on means based partitioning of the historical data of car road accidents . the proposed method belongs to the kth order and time-variant methods . the proposed method can get the best forecasting accuracy rate for forecasting the car road accidents than the existing methods .", "topics": ["time series"]}
{"title": "simple open stance classification for rumour analysis", "abstract": "stance classification determines the attitude , or stance , in a ( typically short ) text . the task has powerful applications , such as the detection of fake news or the automatic extraction of attitudes toward entities or events in the media . this paper describes a surprisingly simple and efficient classification approach to open stance classification in twitter , for rumour and veracity classification . the approach profits from a novel set of automatically identifiable problem-specific features , which significantly boost classifier accuracy and achieve above state-of-the-art results on recent benchmark datasets . this calls into question the value of using complex sophisticated models for stance classification without first doing informed feature extraction .", "topics": ["feature extraction", "entity"]}
{"title": "a novel approach to artistic textual visualization via gan", "abstract": "while the visualization of statistical data tends to a mature technology , the visualization of textual data is still in its infancy , especially for the artistic text . due to the fact that visualization of artistic text is valuable and attractive in both art and information science , we attempt to realize this tentative idea in this article . we propose the generative adversarial network based artistic textual visualization ( gan-atv ) which can create paintings after analyzing the semantic content of existing poems . our gan-atv consists of two main sections : natural language analysis section and visual information synthesis section . in natural language analysis section , we use bag-of-word ( bow ) feature descriptors and a two-layer network to mine and analyze the high-level semantic information from poems . in visual information synthesis section , we design a cross-modal semantic understanding module and integrate it with generative adversarial network ( gan ) to create paintings , whose content are corresponding to the original poems . moreover , in order to train our gan-atv and verify its performance , we establish a cross-modal artistic dataset named `` cross-art '' . in the cross-art dataset , there are six topics and each topic has their corresponding paintings and poems . the experimental results on cross-art dataset are shown in this article .", "topics": ["high- and low-level", "natural language"]}
{"title": "on deducing conditional independence from d-separation in causal graphs with feedback ( research note )", "abstract": "pearl and dechter ( 1996 ) claimed that the d-separation criterion for conditional independence in acyclic causal networks also applies to networks of discrete variables that have feedback cycles , provided that the variables of the system are uniquely determined by the random disturbances . i show by example that this is not true in general . some condition stronger than uniqueness is needed , such as the existence of a causal dynamics guaranteed to lead to the unique solution .", "topics": ["value ( ethics )", "numerical analysis"]}
{"title": "deformable distributed multiple detector fusion for multi-person tracking", "abstract": "this paper addresses fully automated multi-person tracking in complex environments with challenging occlusion and extensive pose variations . our solution combines multiple detectors for a set of different regions of interest ( e.g . , full-body and head ) for multi-person tracking . the use of multiple detectors leads to fewer miss detections as it is able to exploit the complementary strengths of the individual detectors . while the number of false positives may increase with the increased number of bounding boxes detected from multiple detectors , we propose to group the detection outputs by bounding box location and depth information . for robustness to significant pose variations , deformable spatial relationship between detectors are learnt in our multi-person tracking system . on rgbd data from a live intensive care unit ( icu ) , we show that the proposed method significantly improves multi-person tracking performance over state-of-the-art methods .", "topics": ["sensor"]}
{"title": "sparse vs. non-sparse : which one is better for practical visual tracking ?", "abstract": "recently , sparse representation based visual tracking methods have attracted increasing attention in the computer vision community . although achieve superior performance to traditional tracking methods , however , a basic problem has not been answered yet -- - that whether the sparsity constrain is really needed for visual tracking ? to answer this question , in this paper , we first propose a robust non-sparse representation based tracker and then conduct extensive experiments to compare it against several state-of-the-art sparse representation based trackers . our experiment results and analysis indicate that the proposed non-sparse tracker achieved competitive tracking accuracy with sparse trackers while having faster running speed , which support our non-sparse tracker to be used in practical applications .", "topics": ["computer vision", "sparse matrix"]}
{"title": "icr : iterative convex refinement for sparse signal recovery using spike and slab priors", "abstract": "in this letter , we address sparse signal recovery using spike and slab priors . in particular , we focus on a bayesian framework where sparsity is enforced on reconstruction coefficients via probabilistic priors . the optimization resulting from spike and slab prior maximization is known to be a hard non-convex problem , and existing solutions involve simplifying assumptions and/or relaxations . we propose an approach called iterative convex refinement ( icr ) that aims to solve the aforementioned optimization problem directly allowing for greater generality in the sparse structure . essentially , icr solves a sequence of convex optimization problems such that sequence of solutions converges to a sub-optimal solution of the original hard optimization problem . we propose two versions of our algorithm : a . ) an unconstrained version , and b . ) with a non-negativity constraint on sparse coefficients , which may be required in some real-world problems . experimental validation is performed on both synthetic data and for a real-world image recovery problem , which illustrates merits of icr over state of the art alternatives .", "topics": ["sparse matrix"]}
{"title": "neural machine translation leveraging phrase-based models in a hybrid search", "abstract": "in this paper , we introduce a hybrid search for attention-based neural machine translation ( nmt ) . a target phrase learned with statistical mt models extends a hypothesis in the nmt beam search when the attention of the nmt model focuses on the source words translated by this phrase . phrases added in this way are scored with the nmt model , but also with smt features including phrase-level translation probabilities and a target language model . experimental results on german- > english news domain and english- > russian e-commerce domain translation tasks show that using phrase-based models in nmt search improves mt quality by up to 2.3 % bleu absolute as compared to a strong nmt baseline .", "topics": ["machine translation"]}
{"title": "ultradense word embeddings by orthogonal transformation", "abstract": "embeddings are generic representations that are useful for many nlp tasks . in this paper , we introduce densifier , a method that learns an orthogonal transformation of the embedding space that focuses the information relevant for a task in an ultradense subspace of a dimensionality that is smaller by a factor of 100 than the original space . we show that ultradense embeddings generated by densifier reach state of the art on a lexicon creation task in which words are annotated with three types of lexical information - sentiment , concreteness and frequency . on the semeval2015 10b sentiment analysis task we show that no information is lost when the ultradense subspace is used , but training is an order of magnitude more efficient due to the compactness of the ultradense space .", "topics": ["natural language processing"]}
{"title": "image denoising and restoration with cnn-lstm encoder decoder with direct attention", "abstract": "image denoising is always a challenging task in the field of computer vision and image processing . in this paper , we have proposed an encoder-decoder model with direct attention , which is capable of denoising and reconstruct highly corrupted images . our model consists of an encoder and a decoder , where the encoder is a convolutional neural network and decoder is a multilayer long short-term memory network . in the proposed model , the encoder reads an image and catches the abstraction of that image in a vector , where decoder takes that vector as well as the corrupted image to reconstruct a clean image . we have trained our model on mnist handwritten digit database after making lower half of every image as black as well as adding noise top of that . after a massive destruction of the images where it is hard for a human to understand the content of those images , our model can retrieve that image with minimal error . our proposed model has been compared with convolutional encoder-decoder , where our model has performed better at generating missing part of the images than convolutional autoencoder .", "topics": ["image processing", "noise reduction"]}
{"title": "revisiting stochastic off-policy action-value gradients", "abstract": "off-policy stochastic actor-critic methods rely on approximating the stochastic policy gradient in order to derive an optimal policy . one may also derive the optimal policy by approximating the action-value gradient . the use of action-value gradients is desirable as policy improvement occurs along the direction of steepest ascent . this has been studied extensively within the context of natural gradient actor-critic algorithms and more recently within the context of deterministic policy gradients . in this paper we briefly discuss the off-policy stochastic counterpart to deterministic action-value gradients , as well as an incremental approach for following the policy gradient in lieu of the natural gradient .", "topics": ["approximation algorithm", "value ( ethics )"]}
{"title": "kernel methods on approximate infinite-dimensional covariance operators for image classification", "abstract": "this paper presents a novel framework for visual object recognition using infinite-dimensional covariance operators of input features in the paradigm of kernel methods on infinite-dimensional riemannian manifolds . our formulation provides in particular a rich representation of image features by exploiting their non-linear correlations . theoretically , we provide a finite-dimensional approximation of the log-hilbert-schmidt ( log-hs ) distance between covariance operators that is scalable to large datasets , while maintaining an effective discriminating capability . this allows us to efficiently approximate any continuous shift-invariant kernel defined using the log-hs distance . at the same time , we prove that the log-hs inner product between covariance operators is only approximable by its finite-dimensional counterpart in a very limited scenario . consequently , kernels defined using the log-hs inner product , such as polynomial kernels , are not scalable in the same way as shift-invariant kernels . computationally , we apply the approximate log-hs distance formulation to covariance operators of both handcrafted and convolutional features , exploiting both the expressiveness of these features and the power of the covariance representation . empirically , we tested our framework on the task of image classification on twelve challenging datasets . in almost all cases , the results obtained outperform other state of the art methods , demonstrating the competitiveness and potential of our framework .", "topics": ["kernel ( operating system )", "approximation algorithm"]}
{"title": "mazebase : a sandbox for learning from games", "abstract": "this paper introduces mazebase : an environment for simple 2d games , designed as a sandbox for machine learning approaches to reasoning and planning . within it , we create 10 simple games embodying a range of algorithmic tasks ( e.g . if-then statements or set negation ) . a variety of neural models ( fully connected , convolutional network , memory network ) are deployed via reinforcement learning on these games , with and without a procedurally generated curriculum . despite the tasks ' simplicity , the performance of the models is far from optimal , suggesting directions for future development . we also demonstrate the versatility of mazebase by using it to emulate small combat scenarios from starcraft . models trained on the mazebase version can be directly applied to starcraft , where they consistently beat the in-game ai .", "topics": ["reinforcement learning"]}
{"title": "identification of non-linear behavior models with restricted or redundant data", "abstract": "this study presents a new strategy for the identification of material parameters in the case of restricted or redundant data , based on a hybrid approach combining a genetic algorithm and the levenberg-marquardt method . the proposed methodology consists essentially in a statistically based topological analysis of the search domain , after this one has been reduced by the analysis of the parameters ranges . this is used to identify the parameters of a model representing the behavior of damaged elastic , visco-elastic , plastic and visco-plastic composite laminates . optimization of the experimental tests on tubular samples leads to the selective identification of these parameters .", "topics": ["nonlinear system"]}
{"title": "bayesian representation learning with oracle constraints", "abstract": "representation learning systems typically rely on massive amounts of labeled data in order to be trained to high accuracy . recently , high-dimensional parametric models like neural networks have succeeded in building rich representations using either compressive , reconstructive or supervised criteria . however , the semantic structure inherent in observations is oftentimes lost in the process . human perception excels at understanding semantics but can not always be expressed in terms of labels . thus , \\emph { oracles } or \\emph { human-in-the-loop systems } , for example crowdsourcing , are often employed to generate similarity constraints using an implicit similarity function encoded in human perception . in this work we propose to combine \\emph { generative unsupervised feature learning } with a \\emph { probabilistic treatment of oracle information like triplets } in order to transfer implicit privileged oracle knowledge into explicit nonlinear bayesian latent factor models of the observations . we use a fast variational algorithm to learn the joint model and demonstrate applicability to a well-known image dataset . we show how implicit triplet information can provide rich information to learn representations that outperform previous metric learning approaches as well as generative models without this side-information in a variety of predictive tasks . in addition , we illustrate that the proposed approach compartmentalizes the latent spaces semantically which allows interpretation of the latent variables .", "topics": ["generative model", "feature learning"]}
{"title": "event-based , 6-dof camera tracking from photometric depth maps", "abstract": "event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames . these cameras do not suffer from motion blur and have a very high dynamic range , which enables them to provide reliable visual information during high-speed motions or in scenes characterized by high dynamic range . these features , along with a very low power consumption , make event cameras an ideal complement to standard cameras for vr/ar and video game applications . with these applications in mind , this paper tackles the problem of accurate , low-latency tracking of an event camera from an existing photometric depth map ( i.e . , intensity plus depth information ) built via classic dense reconstruction pipelines . our approach tracks the 6-dof pose of the event camera upon the arrival of each event , thus virtually eliminating latency . we successfully evaluate the method in both indoor and outdoor scenes and show that -- -because of the technological advantages of the event camera -- -our pipeline works in scenes characterized by high-speed motion , which are still unaccessible to standard cameras .", "topics": ["sensor", "pixel"]}
{"title": "sensitivities : an alternative to conditional probabilities for bayesian belief networks", "abstract": "we show an alternative way of representing a bayesian belief network by sensitivities and probability distributions . this representation is equivalent to the traditional representation by conditional probabilities , but makes dependencies between nodes apparent and intuitively easy to understand . we also propose a qr matrix representation for the sensitivities and/or conditional probabilities which is more efficient , in both memory requirements and computational speed , than the traditional representation for computer-based implementations of probabilistic inference . we use sensitivities to show that for a certain class of binary networks , the computation time for approximate probabilistic inference with any positive upper bound on the error of the result is independent of the size of the network . finally , as an alternative to traditional algorithms that use conditional probabilities , we describe an exact algorithm for probabilistic inference that uses the qr-representation for sensitivities and updates probability distributions of nodes in a network according to messages from the neighbors .", "topics": ["approximation algorithm", "bayesian network"]}
{"title": "lazily adapted constant kinky inference for nonparametric regression and model-reference adaptive control", "abstract": "techniques known as nonlinear set membership prediction , lipschitz interpolation or kinky inference are approaches to machine learning that utilise presupposed lipschitz properties to compute inferences over unobserved function values . provided a bound on the true best lipschitz constant of the target function is known a priori they offer convergence guarantees as well as bounds around the predictions . considering a more general setting that builds on hoelder continuity relative to pseudo-metrics , we propose an online method for estimating the hoelder constant online from function value observations that possibly are corrupted by bounded observational errors . utilising this to compute adaptive parameters within a kinky inference rule gives rise to a nonparametric machine learning method , for which we establish strong universal approximation guarantees . that is , we show that our prediction rule can learn any continuous function in the limit of increasingly dense data to within a worst-case error bound that depends on the level of observational uncertainty . we apply our method in the context of nonparametric model-reference adaptive control ( mrac ) . across a range of simulated aircraft roll-dynamics and performance metrics our approach outperforms recently proposed alternatives that were based on gaussian processes and rbf-neural networks . for discrete-time systems , we provide stability guarantees for our learning-based controllers both for the batch and the online learning setting .", "topics": ["simulation"]}
{"title": "spatial semantic regularisation for large scale object detection", "abstract": "large scale object detection with thousands of classes introduces the problem of many contradicting false positive detections , which have to be suppressed . class-independent non-maximum suppression has traditionally been used for this step , but it does not scale well as the number of classes grows . traditional non-maximum suppression does not consider label- and instance-level relationships nor does it allow an exploitation of the spatial layout of detection proposals . we propose a new multi-class spatial semantic regularisation method based on affinity propagation clustering , which simultaneously optimises across all categories and all proposed locations in the image , to improve both the localisation and categorisation of selected detection proposals . constraints are shared across the labels through the semantic wordnet hierarchy . our approach proves to be especially useful in large scale settings with thousands of classes , where spatial and semantic interactions are very frequent and only weakly supervised detectors can be built due to a lack of bounding box annotations . detection experiments are conducted on the imagenet and coco dataset , and in settings with thousands of detected categories . our method provides a significant precision improvement by reducing false positives , while simultaneously improving the recall .", "topics": ["object detection", "cluster analysis"]}
{"title": "lifted variable elimination for probabilistic logic programming", "abstract": "lifted inference has been proposed for various probabilistic logical frameworks in order to compute the probability of queries in a time that depends on the size of the domains of the random variables rather than the number of instances . even if various authors have underlined its importance for probabilistic logic programming ( plp ) , lifted inference has been applied up to now only to relational languages outside of logic programming . in this paper we adapt generalized counting first order variable elimination ( gc-fove ) to the problem of computing the probability of queries to probabilistic logic programs under the distribution semantics . in particular , we extend the prolog factor language ( pfl ) to include two new types of factors that are needed for representing problog programs . these factors take into account the existing causal independence relationships among random variables and are managed by the extension to variable elimination proposed by zhang and poole for dealing with convergent variables and heterogeneous factors . two new operators are added to gc-fove for treating heterogeneous factors . the resulting algorithm , called lp $ ^2 $ for lifted probabilistic logic programming , has been implemented by modifying the pfl implementation of gc-fove and tested on three benchmarks for lifted inference . a comparison with pita and problog2 shows the potential of the approach .", "topics": ["causality"]}
{"title": "deep triphone embedding improves phoneme recognition", "abstract": "in this paper , we present a novel deep triphone embedding ( dte ) representation derived from deep neural network ( dnn ) to encapsulate the discriminative information present in the adjoining speech frames . dtes are generated using a four hidden layer dnn with 3000 nodes in each hidden layer at the first-stage . this dnn is trained with the tied-triphone classification accuracy as an optimization criterion . thereafter , we retain the activation vectors ( 3000 ) of the last hidden layer , for each speech mfcc frame , and perform dimension reduction to further obtain a 300 dimensional representation , which we termed as dte . dtes along with mfcc features are fed into a second-stage four hidden layer dnn , which is subsequently trained for the task of tied-triphone classification . both dnns are trained using tri-phone labels generated from a tied-state triphone hmm-gmm system , by performing a forced-alignment between the transcriptions and mfcc feature frames . we conduct the experiments on publicly available ted-lium speech corpus . the results show that the proposed dte method provides an improvement of absolute 2.11 % in phoneme recognition , when compared with a competitive hybrid tied-state triphone hmm-dnn system .", "topics": ["eisenstein 's criterion"]}
{"title": "predicting rapid fire growth ( flashover ) using conditional generative adversarial networks", "abstract": "a flashover occurs when a fire spreads very rapidly through crevices due to intense heat . flashovers present one of the most frightening and challenging fire phenomena to those who regularly encounter them : firefighters . firefighters ' safety and lives often depend on their ability to predict flashovers before they occur . typical pre-flashover fire characteristics include dark smoke , high heat , and rollover ( `` angel fingers '' ) and can be quantified by color , size , and shape . using a color video stream from a firefighter 's body camera , we applied generative adversarial neural networks for image enhancement . the neural networks were trained to enhance very dark fire and smoke patterns in videos and monitor dynamic changes in smoke and fire areas . preliminary tests with limited flashover training videos showed that we predicted a flashover as early as 55 seconds before it occurred .", "topics": ["image processing"]}
{"title": "value alignment , fair play , and the rights of service robots", "abstract": "ethics and safety research in artificial intelligence is increasingly framed in terms of `` alignment '' with human values and interests . i argue that turing 's call for `` fair play for machines '' is an early and often overlooked contribution to the alignment literature . turing 's appeal to fair play suggests a need to correct human behavior to accommodate our machines , a surprising inversion of how value alignment is treated today . reflections on `` fair play '' motivate a novel interpretation of turing 's notorious `` imitation game '' as a condition not of intelligence but instead of value alignment : a machine demonstrates a minimal degree of alignment ( with the norms of conversation , for instance ) when it can go undetected when interrogated by a human . i carefully distinguish this interpretation from the moral turing test , which is not motivated by a principle of fair play , but instead depends on imitation of human moral behavior . finally , i consider how the framework of fair play can be used to situate the debate over robot rights within the alignment literature . i argue that extending rights to service robots operating in public spaces is `` fair '' in precisely the sense that it encourages an alignment of interests between humans and machines .", "topics": ["value ( ethics )", "test set"]}
{"title": "mvg mechanism : differential privacy under matrix-valued query", "abstract": "differential privacy mechanism design has traditionally been tailored for a scalar-valued query function . although many mechanisms such as the laplace and gaussian mechanisms can be extended to a matrix-valued query function by adding i.i.d . noise to each element of the matrix , this method is often suboptimal as it forfeits an opportunity to exploit the structural characteristics typically associated with matrix analysis . to address this challenge , we propose a novel differential privacy mechanism called the matrix-variate gaussian ( mvg ) mechanism , which adds a matrix-valued noise drawn from a matrix-variate gaussian distribution , and we rigorously prove that the mvg mechanism preserves $ ( \\epsilon , \\delta ) $ -differential privacy . furthermore , we introduce the concept of directional noise made possible by the design of the mvg mechanism . directional noise allows the impact of the noise on the utility of the matrix-valued query function to be moderated . finally , we experimentally demonstrate the performance of our mechanism using three matrix-valued queries on three privacy-sensitive datasets . we find that the mvg mechanism notably outperforms four previous state-of-the-art approaches , and provides comparable utility to the non-private baseline .", "topics": ["baseline ( configuration management )"]}
{"title": "not all neural embeddings are born equal", "abstract": "neural language models learn word representations that capture rich linguistic and conceptual information . here we investigate the embeddings learned by neural machine translation models . we show that translation-based embeddings outperform those learned by cutting-edge monolingual models at single-language tasks requiring knowledge of conceptual similarity and/or syntactic role . the findings suggest that , while monolingual models learn information about how concepts are related , neural-translation models better capture their true ontological status .", "topics": ["natural language processing", "high- and low-level"]}
{"title": "toward fuzzy block theory", "abstract": "this study , fundamentals of fuzzy block theory , and its application in assessment of stability in underground openings , has surveyed . using fuzzy topics and inserting them in to key block theory , in two ways , fundamentals of fuzzy block theory has been presented . in indirect combining , by coupling of adaptive neuro fuzzy inference system ( nfis ) and classic block theory , we could extract possible damage parts around a tunnel . in direct solution , some principles of block theory , by means of different fuzzy facets theory , were rewritten .", "topics": ["value ( ethics )", "approximation algorithm"]}
{"title": "a comprehensive study of sparse codes on abnormality detection", "abstract": "sparse representation has been applied successfully in abnormal event detection , in which the baseline is to learn a dictionary accompanied by sparse codes . while much emphasis is put on discriminative dictionary construction , there are no comparative studies of sparse codes regarding abnormality detection . we comprehensively study two types of sparse codes solutions - greedy algorithms and convex l1-norm solutions - and their impact on abnormality detection performance . we also propose our framework of combining sparse codes with different detection methods . our comparative experiments are carried out from various angles to better understand the applicability of sparse codes , including computation time , reconstruction error , sparsity , detection accuracy , and their performance combining various detection methods . experiments show that combining omp codes with maximum coordinate detection could achieve state-of-the-art performance on the ucsd dataset [ 14 ] .", "topics": ["baseline ( configuration management )", "time complexity"]}
{"title": "a multivariate biomarker for parkinson 's disease", "abstract": "in this study , we executed a genomic analysis with the objective of selecting a set of genes ( possibly small ) that would help in the detection and classification of samples from patients affected by parkinson disease . we performed a complete data analysis and during the exploratory phase , we selected a list of differentially expressed genes . despite their association with the diseased state , we could not use them as a biomarker tool . therefore , our research was extended to include a multivariate analysis approach resulting in the identification and selection of a group of 20 genes that showed a clear potential in detecting and correctly classify parkinson disease samples even in the presence of other neurodegenerative disorders .", "topics": ["data mining"]}
{"title": "local component analysis", "abstract": "kernel density estimation , a.k.a . parzen windows , is a popular density estimation method , which can be used for outlier detection or clustering . with multivariate data , its performance is heavily reliant on the metric used within the kernel . most earlier work has focused on learning only the bandwidth of the kernel ( i.e . , a scalar multiplicative factor ) . in this paper , we propose to learn a full euclidean metric through an expectation-minimization ( em ) procedure , which can be seen as an unsupervised counterpart to neighbourhood component analysis ( nca ) . in order to avoid overfitting with a fully nonparametric density estimator in high dimensions , we also consider a semi-parametric gaussian-parzen density model , where some of the variables are modelled through a jointly gaussian density , while others are modelled through parzen windows . for these two models , em leads to simple closed-form updates based on matrix inversions and eigenvalue decompositions . we show empirically that our method leads to density estimators with higher test-likelihoods than natural competing methods , and that the metrics may be used within most unsupervised learning techniques that rely on such metrics , such as spectral clustering or manifold learning methods . finally , we present a stochastic approximation scheme which allows for the use of this method in a large-scale setting .", "topics": ["kernel ( operating system )", "cluster analysis"]}
{"title": "capsule network performance on complex data", "abstract": "in recent years , convolutional neural networks ( cnn ) have played an important role in the field of deep learning . variants of cnn 's have proven to be very successful in classification tasks across different domains . however , there are two big drawbacks to cnn 's : their failure to take into account of important spatial hierarchies between features , and their lack of rotational invariance . as long as certain key features of an object are present in the test data , cnn 's classify the test data as the object , disregarding features ' relative spatial orientation to each other . this causes false positives . the lack of rotational invariance in cnn 's would cause the network to incorrectly assign the object another label , causing false negatives . to address this concern , hinton et al . propose a novel type of neural network using the concept of capsules in a recent paper . with the use of dynamic routing and reconstruction regularization , the capsule network model would be both rotation invariant and spatially aware . the capsule network has shown its potential by achieving a state-of-the-art result of 0.25 % test error on mnist without data augmentation such as rotation and scaling , better than the previous baseline of 0.39 % . to further test out the application of capsule networks on data with higher dimensionality , we attempt to find the best set of configurations that yield the optimal test error on cifar10 dataset .", "topics": ["matrix regularization"]}
{"title": "stochastic gradient mcmc methods for hidden markov models", "abstract": "stochastic gradient mcmc ( sg-mcmc ) algorithms have proven useful in scaling bayesian inference to large datasets under an assumption of i.i.d data . we instead develop an sg-mcmc algorithm to learn the parameters of hidden markov models ( hmms ) for time-dependent data . there are two challenges to applying sg-mcmc in this setting : the latent discrete states , and needing to break dependencies when considering minibatches . we consider a marginal likelihood representation of the hmm and propose an algorithm that harnesses the inherent memory decay of the process . we demonstrate the effectiveness of our algorithm on synthetic experiments and an ion channel recording data , with runtimes significantly outperforming batch mcmc .", "topics": ["synthetic data", "gradient"]}
{"title": "interstitial content detection", "abstract": "interstitial content is online content which grays out , or otherwise obscures the main page content . in this technical report , we discuss exploratory research into detecting the presence of interstitial content in web pages . we discuss the use of computer vision techniques to detect interstitials , and the potential use of these techniques to provide a labelled dataset for machine learning .", "topics": ["computer vision"]}
{"title": "unsupervised part-of-speech induction", "abstract": "part-of-speech ( pos ) tagging is an old and fundamental task in natural language processing . while supervised pos taggers have shown promising accuracy , it is not always feasible to use supervised methods due to lack of labeled data . in this project , we attempt to unsurprisingly induce pos tags by iteratively looking for a recurring pattern of words through a hierarchical agglomerative clustering process . our approach shows promising results when compared to the tagging results of the state-of-the-art unsupervised pos taggers .", "topics": ["cluster analysis", "supervised learning"]}
{"title": "cycada : cycle-consistent adversarial domain adaptation", "abstract": "domain adaptation is critical for success in new , unseen environments . adversarial adaptation models applied in feature spaces discover domain invariant representations , but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts . recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at mapping images between domains , even without the use of aligned image pairs . we propose a novel discriminatively-trained cycle-consistent adversarial domain adaptation model . cycada adapts representations at both the pixel-level and feature-level , enforces cycle-consistency while leveraging a task loss , and does not require aligned pairs . our model can be applied in a variety of visual recognition and prediction settings . we show new state-of-the-art results across multiple adaptation tasks , including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains .", "topics": ["high- and low-level", "synthetic data"]}
{"title": "new fuzzy lbp features for face recognition", "abstract": "there are many local texture features each very in way they implement and each of the algorithm trying improve the performance . an attempt is made in this paper to represent a theoretically very simple and computationally effective approach for face recognition . in our implementation the face image is divided into 3x3 sub-regions from which the features are extracted using the local binary pattern ( lbp ) over a window , fuzzy membership function and at the central pixel . the lbp features possess the texture discriminative property and their computational cost is very low . by utilising the information from lbp , membership function , and central pixel , the limitations of traditional lbp is eliminated . the bench mark database like orl and sheffield databases are used for the evaluation of proposed features with svm classifier . for the proposed approach k-fold and roc curves are obtained and results are compared .", "topics": ["pixel"]}
{"title": "query-by-example search with discriminative neural acoustic word embeddings", "abstract": "query-by-example search often uses dynamic time warping ( dtw ) for comparing queries and proposed matching segments . recent work has shown that comparing speech segments by representing them as fixed-dimensional vectors -- - acoustic word embeddings -- - and measuring their vector distance ( e.g . , cosine distance ) can discriminate between words more accurately than dtw-based approaches . we consider an approach to query-by-example search that embeds both the query and database segments according to a neural model , followed by nearest-neighbor search to find the matching segments . earlier work on embedding-based query-by-example , using template-based acoustic word embeddings , achieved competitive performance . we find that our embeddings , based on recurrent neural networks trained to optimize word discrimination , achieve substantial improvements in performance and run-time efficiency over the previous approaches .", "topics": ["recurrent neural network"]}
{"title": "a tractable inference algorithm for diagnosing multiple diseases", "abstract": "we examine a probabilistic model for the diagnosis of multiple diseases . in the model , diseases and findings are represented as binary variables . also , diseases are marginally independent , features are conditionally independent given disease instances , and diseases interact to produce findings via a noisy or-gate . an algorithm for computing the posterior probability of each disease , given a set of observed findings , called quickscore , is presented . the time complexity of the algorithm is o ( nm-2m+ ) , where n is the number of diseases , m+ is the number of positive findings and m- is the number of negative findings . although the time complexity of quickscore i5 exponential in the number of positive findings , the algorithm is useful in practice because the number of observed positive findings is usually far less than the number of diseases under consideration . performance results for quickscore applied to a probabilistic version of quick medical reference ( qmr ) are provided .", "topics": ["time complexity"]}
{"title": "a study on the behavior of a neural network for grouping the data", "abstract": "one of the frequently stated advantages of neural networks is that they can work effectively with non-normally distributed data . but optimal results are possible with normalized data.in this paper , how normality of the input affects the behaviour of a k-means fast learning artificial neural network ( kflann ) for grouping the data is presented . basically , the grouping of high dimensional input data is controlled by additional neural network input parameters namely vigilance and tolerance.neural networks learn faster and give better performance if the input variables are pre-processed before being fed to the input units of the neural network . a common way of dealing with data that is not normally distributed is to perform some form of mathematical transformation on the data that shifts it towards a normal distribution.in a neural network , data preprocessing transforms the data into a format that will be more easily and effectively processed for the purpose of the user . among various methods , normalization is one which organizes data for more efficient access . experimental results on several artificial and synthetic data sets indicate that the groups formed in the data vary with non-normally distributed data and normalized data and also depends on the normalization method used .", "topics": ["synthetic data", "neural networks"]}
{"title": "nisp : pruning networks using neuron importance score propagation", "abstract": "to reduce the significant redundancy in deep convolutional neural networks ( cnns ) , most existing methods prune neurons by only considering statistics of an individual layer or two consecutive layers ( e.g . , prune one layer to minimize the reconstruction error of the next layer ) , ignoring the effect of error propagation in deep networks . in contrast , we argue that it is essential to prune neurons in the entire neuron network jointly based on a unified goal : minimizing the reconstruction error of important responses in the `` final response layer '' ( frl ) , which is the second-to-last layer before classification , for a pruned network to retrain its predictive power . specifically , we apply feature ranking techniques to measure the importance of each neuron in the frl , and formulate network pruning as a binary integer optimization problem and derive a closed-form solution to it for pruning neurons in earlier layers . based on our theoretical analysis , we propose the neuron importance score propagation ( nisp ) algorithm to propagate the importance scores of final responses to every neuron in the network . the cnn is pruned by removing neurons with least importance , and then fine-tuned to retain its predictive power . nisp is evaluated on several datasets with multiple cnn models and demonstrated to achieve significant acceleration and compression with negligible accuracy loss .", "topics": ["optimization problem"]}
{"title": "fast feature fool : a data independent approach to universal adversarial perturbations", "abstract": "state-of-the-art object recognition convolutional neural networks ( cnns ) are shown to be fooled by image agnostic perturbations , called universal adversarial perturbations . it is also observed that these perturbations generalize across multiple networks trained on the same target data . however , these algorithms require training data on which the cnns were trained and compute adversarial perturbations via complex optimization . the fooling performance of these approaches is directly proportional to the amount of available training data . this makes them unsuitable for practical attacks since its unreasonable for an attacker to have access to the training data . in this paper , for the first time , we propose a novel data independent approach to generate image agnostic perturbations for a range of cnns trained for object recognition . we further show that these perturbations are transferable across multiple network architectures trained either on same or different data . in the absence of data , our method generates universal adversarial perturbations efficiently via fooling the features learned at multiple layers thereby causing cnns to misclassify . experiments demonstrate impressive fooling rates and surprising transferability for the proposed universal perturbations generated without any training data .", "topics": ["test set"]}
{"title": "learning to answer questions", "abstract": "we present an open-domain question-answering system that learns to answer questions based on successful past interactions . we follow a pattern-based approach to answer-extraction , where ( lexico-syntactic ) patterns that relate a question to its answer are automatically learned and used to answer future questions . results show that our approach contributes to the system 's best performance when it is conjugated with typical answer-extraction strategies . moreover , it allows the system to learn with the answered questions and to rectify wrong or unsolved past questions .", "topics": ["interaction"]}
{"title": "nonparametric bayesian label prediction on a graph", "abstract": "an implementation of a nonparametric bayesian approach to solving binary classification problems on graphs is described . a hierarchical bayesian approach with a randomly scaled gaussian prior is considered . the prior uses the graph laplacian to take into account the underlying geometry of the graph . a method based on a theoretically optimal prior and a more flexible variant using partial conjugacy are proposed . two simulated data examples and two examples using real data are used in order to illustrate the proposed methods .", "topics": ["simulation", "bayesian network"]}
{"title": "improving the accuracy of stereo visual odometry using visual illumination estimation", "abstract": "in the absence of reliable and accurate gps , visual odometry ( vo ) has emerged as an effective means of estimating the egomotion of robotic vehicles . like any dead-reckoning technique , vo suffers from unbounded accumulation of drift error over time , but this accumulation can be limited by incorporating absolute orientation information from , for example , a sun sensor . in this paper , we leverage recent work on visual outdoor illumination estimation to show that estimation error in a stereo vo pipeline can be reduced by inferring the sun position from the same image stream used to compute vo , thereby gaining the benefits of sun sensing without requiring a dedicated sun sensor or the sun to be visible to the camera . we compare sun estimation methods based on hand-crafted visual cues and convolutional neural networks ( cnns ) and demonstrate our approach on a combined 7.8 km of urban driving from the popular kitti dataset , achieving up to a 43 % reduction in translational average root mean squared error ( armse ) and a 59 % reduction in final translational drift error compared to pure vo alone .", "topics": ["robot"]}
{"title": "automatic subspace learning via principal coefficients embedding", "abstract": "in this paper , we address two challenging problems in unsupervised subspace learning : 1 ) how to automatically identify the feature dimension of the learned subspace ( i.e . , automatic subspace learning ) , and 2 ) how to learn the underlying subspace in the presence of gaussian noise ( i.e . , robust subspace learning ) . we show that these two problems can be simultaneously solved by proposing a new method ( called principal coefficients embedding , pce ) . for a given data set $ \\mathbf { d } \\in \\mathds { r } ^ { m\\times n } $ , pce recovers a clean data set $ \\mathbf { d } _ { 0 } \\in \\mathds { r } ^ { m\\times n } $ from $ \\mathbf { d } $ and simultaneously learns a global reconstruction relation $ \\mathbf { c } \\in \\mathbf { r } ^ { n\\times n } $ of $ \\mathbf { d } _ { 0 } $ . by preserving $ \\mathbf { c } $ into an $ m^ { \\prime } $ -dimensional space , the proposed method obtains a projection matrix that can capture the latent manifold structure of $ \\mathbf { d } _ { 0 } $ , where $ m^ { \\prime } \\ll m $ is automatically determined by the rank of $ \\mathbf { c } $ with theoretical guarantees . pce has three advantages : 1 ) it can automatically determine the feature dimension even though data are sampled from a union of multiple linear subspaces in presence of the gaussian noise , 2 ) although the objective function of pce only considers the gaussian noise , experimental results show that it is robust to the non-gaussian noise ( \\textit { e.g . } , random pixel corruption ) and real disguises , 3 ) our method has a closed-form solution and can be calculated very fast . extensive experimental results show the superiority of pce on a range of databases with respect to the classification accuracy , robustness and efficiency .", "topics": ["loss function", "unsupervised learning"]}
{"title": "contextual lstm ( clstm ) models for large scale nlp tasks", "abstract": "documents exhibit sequential structure at multiple levels of abstraction ( e.g . , sentences , paragraphs , sections ) . these abstractions constitute a natural hierarchy for representing the context in which to infer the meaning of words and larger fragments of text . in this paper , we present clstm ( contextual lstm ) , an extension of the recurrent neural network lstm ( long-short term memory ) model , where we incorporate contextual features ( e.g . , topics ) into the model . we evaluate clstm on three specific nlp tasks : word prediction , next sentence selection , and sentence topic prediction . results from experiments run on two corpora , english documents in wikipedia and a subset of articles from a recent snapshot of english google news , indicate that using both words and topics as features improves performance of the clstm models over baseline lstm models for these tasks . for example on the next sentence selection task , we get relative accuracy improvements of 21 % for the wikipedia dataset and 18 % for the google news dataset . this clearly demonstrates the significant benefit of using context appropriately in natural language ( nl ) tasks . this has implications for a wide variety of nl applications like question answering , sentence completion , paraphrase generation , and next utterance prediction in dialog systems .", "topics": ["baseline ( configuration management )", "natural language processing"]}
{"title": "learning dense facial correspondences in unconstrained images", "abstract": "we present a minimalistic but effective neural network that computes dense facial correspondences in highly unconstrained rgb images . our network learns a per-pixel flow and a matchability mask between 2d input photographs of a person and the projection of a textured 3d face model . to train such a network , we generate a massive dataset of synthetic faces with dense labels using renderings of a morphable face model with variations in pose , expressions , lighting , and occlusions . we found that a training refinement using real photographs is required to drastically improve the ability to handle real images . when combined with a facial detection and 3d face fitting step , we show that our approach outperforms the state-of-the-art face alignment methods in terms of accuracy and speed . by directly estimating dense correspondences , we do not rely on the full visibility of sparse facial landmarks and are not limited to the model space of regression-based approaches . we also assess our method on video frames and demonstrate successful per-frame processing under extreme pose variations , occlusions , and lighting conditions . compared to existing 3d facial tracking techniques , our fitting does not rely on previous frames or frontal facial initialization and is robust to imperfect face detections .", "topics": ["synthetic data", "sparse matrix"]}
{"title": "low-dose ct with a residual encoder-decoder convolutional neural network ( red-cnn )", "abstract": "given the potential x-ray radiation risk to the patient , low-dose ct has attracted a considerable interest in the medical imaging field . the current main stream low-dose ct methods include vendor-specific sinogram domain filtration and iterative reconstruction , but they need to access original raw data whose formats are not transparent to most users . due to the difficulty of modeling the statistical characteristics in the image domain , the existing methods for directly processing reconstructed images can not eliminate image noise very well while keeping structural details . inspired by the idea of deep learning , here we combine the autoencoder , the deconvolution network , and shortcut connections into the residual encoder-decoder convolutional neural network ( red-cnn ) for low-dose ct imaging . after patch-based training , the proposed red-cnn achieves a competitive performance relative to the-state-of-art methods in both simulated and clinical cases . especially , our method has been favorably evaluated in terms of noise suppression , structural preservation and lesion detection .", "topics": ["encoder"]}
{"title": "a split-merge framework for comparing clusterings", "abstract": "clustering evaluation measures are frequently used to evaluate the performance of algorithms . however , most measures are not properly normalized and ignore some information in the inherent structure of clusterings . we model the relation between two clusterings as a bipartite graph and propose a general component-based decomposition formula based on the components of the graph . most existing measures are examples of this formula . in order to satisfy consistency in the component , we further propose a split-merge framework for comparing clusterings of different data sets . our framework gives measures that are conditionally normalized , and it can make use of data point information , such as feature vectors and pairwise distances . we use an entropy-based instance of the framework and a coreference resolution data set to demonstrate empirically the utility of our framework over other measures .", "topics": ["cluster analysis", "feature vector"]}
{"title": "scaling up inductive logic programming by learning from interpretations", "abstract": "when comparing inductive logic programming ( ilp ) and attribute-value learning techniques , there is a trade-off between expressive power and efficiency . inductive logic programming techniques are typically more expressive but also less efficient . therefore , the data sets handled by current inductive logic programming systems are small according to general standards within the data mining community . the main source of inefficiency lies in the assumption that several examples may be related to each other , so they can not be handled independently . within the learning from interpretations framework for inductive logic programming this assumption is unnecessary , which allows to scale up existing ilp algorithms . in this paper we explain this learning setting in the context of relational databases . we relate the setting to propositional data mining and to the classical ilp setting , and show that learning from interpretations corresponds to learning from multiple relations and thus extends the expressiveness of propositional learning , while maintaining its efficiency to a large extent ( which is not the case in the classical ilp setting ) . as a case study , we present two alternative implementations of the ilp system tilde ( top-down induction of logical decision trees ) : tilde-classic , which loads all data in main memory , and tilde-lds , which loads the examples one by one . we experimentally compare the implementations , showing tilde-lds can handle large data sets ( in the order of 100,000 examples or 100 mb ) and indeed scales up linearly in the number of examples .", "topics": ["data mining", "database"]}
{"title": "detecting offensive language in tweets using deep learning", "abstract": "this paper addresses the important problem of discerning hateful content in social media . we propose a detection scheme that is an ensemble of recurrent neural network ( rnn ) classifiers , and it incorporates various features associated with user-related information , such as the users ' tendency towards racism or sexism . these data are fed as input to the above classifiers along with the word frequency vectors derived from the textual content . our approach has been evaluated on a publicly available corpus of 16k tweets , and the results demonstrate its effectiveness in comparison to existing state of the art solutions . more specifically , our scheme can successfully distinguish racism and sexism messages from normal text , and achieve higher classification quality than current state-of-the-art algorithms .", "topics": ["recurrent neural network", "text corpus"]}
{"title": "hardening deep neural networks via adversarial model cascades", "abstract": "deep neural networks ( dnns ) have been shown to be vulnerable to adversarial examples - malicious inputs which are crafted by the adversary to induce the trained model to produce erroneous outputs . this vulnerability has inspired a lot of research on how to secure neural networks against these kinds of attacks . although existing techniques increase the robustness of the models against white-box attacks , they are ineffective against black-box attacks . to address the challenge of black-box adversarial attacks , we propose adversarial model cascades ( amc ) ; a framework that performs better than existing state-of-the-art defenses , in both black-box and white-box settings and is easy to integrate into existing set-ups . our approach trains a cascade of models by injecting images crafted from an already defended proxy model , to improve the robustness of the target models against adversarial attacks . amc provides an increase in robustness of 8.175 % & 7.115 % for white-box attacks and 30.218 % & 4.717 % for black-box , in comparison to defensive distillation and adversarial hardening . to the best of our knowledge , ours is the first work that aims to provide a defense mechanism that can improve robustness against multiple adversarial attacks simultaneously .", "topics": ["neural networks"]}
{"title": "task-driven adaptive statistical compressive sensing of gaussian mixture models", "abstract": "a framework for adaptive and non-adaptive statistical compressive sensing is developed , where a statistical model replaces the standard sparsity model of classical compressive sensing . we propose within this framework optimal task-specific sensing protocols specifically and jointly designed for classification and reconstruction . a two-step adaptive sensing paradigm is developed , where online sensing is applied to detect the signal class in the first step , followed by a reconstruction step adapted to the detected class and the observed samples . the approach is based on information theory , here tailored for gaussian mixture models ( gmms ) , where an information-theoretic objective relationship between the sensed signals and a representation of the specific task of interest is maximized . experimental results using synthetic signals , landsat satellite attributes , and natural images of different sizes and with different noise levels show the improvements achieved using the proposed framework when compared to more standard sensing protocols . the underlying formulation can be applied beyond gmms , at the price of higher mathematical and computational complexity .", "topics": ["computational complexity theory", "synthetic data"]}
{"title": "intelligent indoor mobile robot navigation using stereo vision", "abstract": "majority of the existing robot navigation systems , which facilitate the use of laser range finders , sonar sensors or artificial landmarks , has the ability to locate itself in an unknown environment and then build a map of the corresponding environment . stereo vision , while still being a rapidly developing technique in the field of autonomous mobile robots , are currently less preferable due to its high implementation cost . this paper aims at describing an experimental approach for the building of a stereo vision system that helps the robots to avoid obstacles and navigate through indoor environments and at the same time remaining very much cost effective . this paper discusses the fusion techniques of stereo vision and ultrasound sensors which helps in the successful navigation through different types of complex environments . the data from the sensor enables the robot to create the two dimensional topological map of unknown environments and stereo vision systems models the three dimension model of the same environment .", "topics": ["sensor", "autonomous car"]}
{"title": "encoding sentences with graph convolutional networks for semantic role labeling", "abstract": "semantic role labeling ( srl ) is the task of identifying the predicate-argument structure of a sentence . it is typically regarded as an important step in the standard nlp pipeline . as the semantic representations are closely related to syntactic ones , we exploit syntactic information in our model . we propose a version of graph convolutional networks ( gcns ) , a recent class of neural networks operating on graphs , suited to model syntactic dependency graphs . gcns over syntactic dependency trees are used as sentence encoders , producing latent feature representations of words in a sentence . we observe that gcn layers are complementary to lstm ones : when we stack both gcn and lstm layers , we obtain a substantial improvement over an already state-of-the-art lstm srl model , resulting in the best reported scores on the standard benchmark ( conll-2009 ) both for chinese and english .", "topics": ["natural language processing", "encoder"]}
{"title": "a syntactic neural model for general-purpose code generation", "abstract": "we consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like python . existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language . informed by previous work in semantic parsing , in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge . experiments find this an effective way to scale up to generation of complex programs from natural language descriptions , achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches .", "topics": ["natural language", "parsing"]}
{"title": "eigendecompositions of transfer operators in reproducing kernel hilbert spaces", "abstract": "transfer operators such as the perron-frobenius or koopman operator play an important role in the global analysis of complex dynamical systems . the eigenfunctions of these operators can be used to detect metastable sets , to project the dynamics onto the dominant slow processes , or to separate superimposed signals . we extend transfer operator theory to reproducing kernel hilbert spaces and show that these operators are related to hilbert space representations of conditional distributions , known as conditional mean embeddings in the machine learning community . moreover , numerical methods to compute empirical estimates of these embeddings are akin to data-driven methods for the approximation of transfer operators such as extended dynamic mode decomposition and its variants . in fact , most of the existing methods can be derived from our framework , providing a unifying view on the approximation of transfer operators . one main benefit of the presented kernel-based approaches is that these methods can be applied to any domain where a similarity measure given by a kernel is available . we illustrate the results with the aid of guiding examples and highlight potential applications in molecular dynamics as well as video and text data analysis .", "topics": ["kernel ( operating system )", "text corpus"]}
{"title": "scene invariant crowd segmentation and counting using scale-normalized histogram of moving gradients ( homg )", "abstract": "the problem of automated crowd segmentation and counting has garnered significant interest in the field of video surveillance . this paper proposes a novel scene invariant crowd segmentation and counting algorithm designed with high accuracy yet low computational complexity in mind , which is key for widespread industrial adoption . a novel low-complexity , scale-normalized feature called histogram of moving gradients ( homg ) is introduced for highly effective spatiotemporal representation of individuals and crowds within a video . real-time crowd segmentation is achieved via boosted cascade of weak classifiers based on sliding-window homg features , while linear svm regression of crowd-region homg features is employed for real-time crowd counting . experimental results using multi-camera crowd datasets show that the proposed algorithm significantly outperform state-of-the-art crowd counting algorithms , as well as achieve very promising crowd segmentation results , thus demonstrating the efficacy of the proposed method for highly-accurate , real-time video-driven crowd analysis .", "topics": ["computational complexity theory"]}
{"title": "disturblabel : regularizing cnn on the loss layer", "abstract": "during a long period of time we are combating over-fitting in the cnn training process with model regularization , including weight decay , model averaging , data augmentation , etc . in this paper , we present disturblabel , an extremely simple algorithm which randomly replaces a part of labels as incorrect values in each iteration . although it seems weird to intentionally generate incorrect training labels , we show that disturblabel prevents the network training from over-fitting by implicitly averaging over exponentially many networks which are trained with different label sets . to the best of our knowledge , disturblabel serves as the first work which adds noises on the loss layer . meanwhile , disturblabel cooperates well with dropout to provide complementary regularization functions . experiments demonstrate competitive recognition results on several popular image recognition datasets .", "topics": ["value ( ethics )", "matrix regularization"]}
{"title": "multi-step reinforcement learning : a unifying algorithm", "abstract": "unifying seemingly disparate algorithmic ideas to produce better performing algorithms has been a longstanding goal in reinforcement learning . as a primary example , td ( $ \\lambda $ ) elegantly unifies one-step td prediction with monte carlo methods through the use of eligibility traces and the trace-decay parameter $ \\lambda $ . currently , there are a multitude of algorithms that can be used to perform td control , including sarsa , $ q $ -learning , and expected sarsa . these methods are often studied in the one-step case , but they can be extended across multiple time steps to achieve better performance . each of these algorithms is seemingly distinct , and no one dominates the others for all problems . in this paper , we study a new multi-step action-value algorithm called $ q ( \\sigma ) $ which unifies and generalizes these existing algorithms , while subsuming them as special cases . a new parameter , $ \\sigma $ , is introduced to allow the degree of sampling performed by the algorithm at each step during its backup to be continuously varied , with sarsa existing at one extreme ( full sampling ) , and expected sarsa existing at the other ( pure expectation ) . $ q ( \\sigma ) $ is generally applicable to both on- and off-policy learning , but in this work we focus on experiments in the on-policy case . our results show that an intermediate value of $ \\sigma $ , which results in a mixture of the existing algorithms , performs better than either extreme . the mixture can also be varied dynamically which can result in even greater performance .", "topics": ["sampling ( signal processing )", "value ( ethics )"]}
{"title": "learning piece-wise linear models from large scale data for ad click prediction", "abstract": "ctr prediction in real-world business is a difficult machine learning problem with large scale nonlinear sparse data . in this paper , we introduce an industrial strength solution with model named large scale piece-wise linear model ( ls-plm ) . we formulate the learning problem with $ l_1 $ and $ l_ { 2,1 } $ regularizers , leading to a non-convex and non-smooth optimization problem . then , we propose a novel algorithm to solve it efficiently , based on directional derivatives and quasi-newton method . in addition , we design a distributed system which can run on hundreds of machines parallel and provides us with the industrial scalability . ls-plm model can capture nonlinear patterns from massive sparse data , saving us from heavy feature engineering jobs . since 2012 , ls-plm has become the main ctr prediction model in alibaba 's online display advertising system , serving hundreds of millions users every day .", "topics": ["optimization problem", "nonlinear system"]}
{"title": "recent advances in recurrent neural networks", "abstract": "recurrent neural networks ( rnns ) are capable of learning features and long term dependencies from sequential and time-series data . the rnns have a stack of non-linear units where at least one connection between units forms a directed cycle . a well-trained rnn can model any dynamical system ; however , training rnns is mostly plagued by issues in learning long-term dependencies . in this paper , we present a survey on rnns and several new advances for newcomers and professionals in the field . the fundamentals and recent advances are explained and the research challenges are introduced .", "topics": ["recurrent neural network", "time series"]}
{"title": "distant supervision for entity linking", "abstract": "entity linking is an indispensable operation of populating knowledge repositories for information extraction . it studies on aligning a textual entity mention to its corresponding disambiguated entry in a knowledge repository . in this paper , we propose a new paradigm named distantly supervised entity linking ( dsel ) , in the sense that the disambiguated entities that belong to a huge knowledge repository ( freebase ) are automatically aligned to the corresponding descriptive webpages ( wiki pages ) . in this way , a large scale of weakly labeled data can be generated without manual annotation and fed to a classifier for linking more newly discovered entities . compared with traditional paradigms based on solo knowledge base , dsel benefits more via jointly leveraging the respective advantages of freebase and wikipedia . specifically , the proposed paradigm facilitates bridging the disambiguated labels ( freebase ) of entities and their textual descriptions ( wikipedia ) for web-scale entities . experiments conducted on a dataset of 140,000 items and 60,000 features achieve a baseline f1-measure of 0.517 . furthermore , we analyze the feature performance and improve the f1-measure to 0.545 .", "topics": ["baseline ( configuration management )", "statistical classification"]}
{"title": "am i a baller ? basketball performance assessment from first-person videos", "abstract": "this paper presents a method to assess a basketball player 's performance from his/her first-person video . a key challenge lies in the fact that the evaluation metric is highly subjective and specific to a particular evaluator . we leverage the first-person camera to address this challenge . the spatiotemporal visual semantics provided by a first-person view allows us to reason about the camera wearer 's actions while he/she is participating in an unscripted basketball game . our method takes a player 's first-person video and provides a player 's performance measure that is specific to an evaluator 's preference . to achieve this goal , we first use a convolutional lstm network to detect atomic basketball events from first-person videos . our network 's ability to zoom-in to the salient regions addresses the issue of a severe camera wearer 's head movement in first-person videos . the detected atomic events are then passed through the gaussian mixtures to construct a highly non-linear visual spatiotemporal basketball assessment feature . finally , we use this feature to learn a basketball assessment model from pairs of labeled first-person basketball videos , for which a basketball expert indicates , which of the two players is better . we demonstrate that despite not knowing the basketball evaluator 's criterion , our model learns to accurately assess the players in real-world games . furthermore , our model can also discover basketball events that contribute positively and negatively to a player 's performance .", "topics": ["nonlinear system", "eisenstein 's criterion"]}
{"title": "multiview rgb-d dataset for object instance detection", "abstract": "this paper presents a new multi-view rgb-d dataset of nine kitchen scenes , each containing several objects in realistic cluttered environments including a subset of objects from the bigbird dataset . the viewpoints of the scenes are densely sampled and objects in the scenes are annotated with bounding boxes and in the 3d point cloud . also , an approach for detection and recognition is presented , which is comprised of two parts : i ) a new multi-view 3d proposal generation method and ii ) the development of several recognition baselines using alexnet to score our proposals , which is trained either on crops of the dataset or on synthetically composited training images . finally , we compare the performance of the object proposals and a detection baseline to the washington rgb-d scenes ( wrgb-d ) dataset and demonstrate that our kitchen scenes dataset is more challenging for object detection and recognition . the dataset is available at : http : //cs.gmu.edu/~robot/gmu-kitchens.html .", "topics": ["baseline ( configuration management )", "object detection"]}
{"title": "fuzzy least squares twin support vector machines", "abstract": "least squares twin support vector machine ( lstsvm ) is an extremely efficient and fast version of svm algorithm for binary classification . lstsvm combines the idea of least squares svm and twin svm in which two non-parallel hyperplanes are found by solving two systems of linear equations . although the algorithm is very fast and efficient in many classification tasks , it is unable to cope with two features of real-world problems . first , in many real-world classification problems , it is almost impossible to assign data points to a single class . second , data points in real-world problems may have different importance . in this study , we propose a novel version of lstsvm based on fuzzy concepts to deal with these two characteristics of real-world data . the algorithm is called fuzzy lstsvm ( flstsvm ) which provides more flexibility than the binary classification of lstsvm . two models are proposed for the algorithm . in the first model , a fuzzy membership value is assigned to each data point and the hyperplanes are optimized based on these fuzzy samples . in the second model we construct fuzzy hyperplanes to classify data . finally , we apply our proposed flstsvm to an artificial as well as three real-world datasets . results demonstrate that flstsvm obtains better performance than svm and lstsvm .", "topics": ["support vector machine"]}
{"title": "beyond normality : learning sparse probabilistic graphical models in the non-gaussian setting", "abstract": "we present an algorithm to identify sparse dependence structure in continuous and non-gaussian probability distributions , given a corresponding set of data . the conditional independence structure of an arbitrary distribution can be represented as an undirected graph ( or markov random field ) , but most algorithms for learning this structure are restricted to the discrete or gaussian cases . our new approach allows for more realistic and accurate descriptions of the distribution in question , and in turn better estimates of its sparse markov structure . sparsity in the graph is of interest as it can accelerate inference , improve sampling methods , and reveal important dependencies between variables . the algorithm relies on exploiting the connection between the sparsity of the graph and the sparsity of transport maps , which deterministically couple one probability measure to another .", "topics": ["sampling ( signal processing )", "graphical model"]}
{"title": "flaw selection strategies for partial-order planning", "abstract": "several recent studies have compared the relative efficiency of alternative flaw selection strategies for partial-order causal link ( pocl ) planning . we review this literature , and present new experimental results that generalize the earlier work and explain some of the discrepancies in it . in particular , we describe the least-cost flaw repair ( lcfr ) strategy developed and analyzed by joslin and pollack ( 1994 ) , and compare it with other strategies , including gerevini and schubert 's ( 1996 ) zlifo strategy . lcfr and zlifo make very different , and apparently conflicting claims about the most effective way to reduce search-space size in pocl planning . we resolve this conflict , arguing that much of the benefit that gerevini and schubert ascribe to the lifo component of their zlifo strategy is better attributed to other causes . we show that for many problems , a strategy that combines least-cost flaw selection with the delay of separable threats will be effective in reducing search-space size , and will do so without excessive computational overhead . although such a strategy thus provides a good default , we also show that certain domain characteristics may reduce its effectiveness .", "topics": ["causality"]}
{"title": "projective simulation applied to the grid-world and the mountain-car problem", "abstract": "we study the model of projective simulation ( ps ) which is a novel approach to artificial intelligence ( ai ) . recently it was shown that the ps agent performs well in a number of simple task environments , also when compared to standard models of reinforcement learning ( rl ) . in this paper we study the performance of the ps agent further in more complicated scenarios . to that end we chose two well-studied benchmarking problems , namely the `` grid-world '' and the `` mountain-car '' problem , which challenge the model with large and continuous input space . we compare the performance of the ps agent model with those of existing models and show that the ps agent exhibits competitive performance also in such scenarios .", "topics": ["reinforcement learning", "simulation"]}
{"title": "mbt : a memory-based part of speech tagger-generator", "abstract": "we introduce a memory-based approach to part of speech tagging . memory-based learning is a form of supervised learning based on similarity-based reasoning . the part of speech tag of a word in a particular context is extrapolated from the most similar cases held in memory . supervised learning approaches are useful when a tagged corpus is available as an example of the desired output of the tagger . based on such a corpus , the tagger-generator automatically builds a tagger which is able to tag new text the same way , diminishing development time for the construction of a tagger considerably . memory-based tagging shares this advantage with other statistical or machine learning approaches . additional advantages specific to a memory-based approach include ( i ) the relatively small tagged corpus size sufficient for training , ( ii ) incremental learning , ( iii ) explanation capabilities , ( iv ) flexible integration of information in case representations , ( v ) its non-parametric nature , ( vi ) reasonably good results on unknown words without morphological analysis , and ( vii ) fast learning and tagging . in this paper we show that a large-scale application of the memory-based approach is feasible : we obtain a tagging accuracy that is on a par with that of known statistical approaches , and with attractive space and time complexity properties when using { \\em igtree } , a tree-based formalism for indexing and searching huge case bases . } the use of igtree has as additional advantage that optimal context size for disambiguation is dynamically computed .", "topics": ["supervised learning", "time complexity"]}
{"title": "ks_ju @ dpil-fire2016 : detecting paraphrases in indian languages using multinomial logistic regression model", "abstract": "in this work , we describe a system that detects paraphrases in indian languages as part of our participation in the shared task on detecting paraphrases in indian languages ( dpil ) organized by forum for information retrieval evaluation ( fire ) in 2016 . our paraphrase detection method uses a multinomial logistic regression model trained with a variety of features which are basically lexical and semantic level similarities between two sentences in a pair . the performance of the system has been evaluated against the test set released for the fire 2016 shared task on dpil . our system achieves the highest f-measure of 0.95 on task1 in punjabi language.the performance of our system on task1 in hindi language is f-measure of 0.90 . out of 11 teams participated in the shared task , only four teams participated in all four languages , hindi , punjabi , malayalam and tamil , but the remaining 7 teams participated in one of the four languages . we also participated in task1 and task2 both for all four indian languages . the overall average performance of our system including task1 and task2 overall four languages is f1-score of 0.81 which is the second highest score among the four systems that participated in all four languages .", "topics": ["test set"]}
{"title": "on anomaly ranking and excess-mass curves", "abstract": "learning how to rank multivariate unlabeled observations depending on their degree of abnormality/novelty is a crucial problem in a wide range of applications . in practice , it generally consists in building a real valued `` scoring '' function on the feature space so as to quantify to which extent observations should be considered as abnormal . in the 1-d situation , measurements are generally considered as `` abnormal '' when they are remote from central measures such as the mean or the median . anomaly detection then relies on tail analysis of the variable of interest . extensions to the multivariate setting are far from straightforward and it is precisely the main purpose of this paper to introduce a novel and convenient ( functional ) criterion for measuring the performance of a scoring function regarding the anomaly ranking task , referred to as the excess-mass curve ( em curve ) . in addition , an adaptive algorithm for building a scoring function based on unlabeled data x1 , . . . , xn with a nearly optimal em is proposed and is analyzed from a statistical perspective .", "topics": ["feature vector", "eisenstein 's criterion"]}
{"title": "introduction to tensor decompositions and their applications in machine learning", "abstract": "tensors are multidimensional arrays of numerical values and therefore generalize matrices to multiple dimensions . while tensors first emerged in the psychometrics community in the $ 20^ { \\text { th } } $ century , they have since then spread to numerous other disciplines , including machine learning . tensors and their decompositions are especially beneficial in unsupervised learning settings , but are gaining popularity in other sub-disciplines like temporal and multi-relational data analysis , too . the scope of this paper is to give a broad overview of tensors , their decompositions , and how they are used in machine learning . as part of this , we are going to introduce basic tensor concepts , discuss why tensors can be considered more rigid than matrices with respect to the uniqueness of their decomposition , explain the most important factorization algorithms and their properties , provide concrete examples of tensor decomposition applications in machine learning , conduct a case study on tensor-based estimation of mixture models , talk about the current state of research , and provide references to available software libraries .", "topics": ["unsupervised learning", "numerical analysis"]}
{"title": "a keygraph classification framework for real-time object detection", "abstract": "in this paper , we propose a new approach for keypoint-based object detection . traditional keypoint-based methods consist in classifying individual points and using pose estimation to discard misclassifications . since a single point carries no relational features , such methods inherently restrict the usage of structural information to the pose estimation phase . therefore , the classifier considers purely appearance-based feature vectors , thus requiring computationally expensive feature extraction or complex probabilistic modelling to achieve satisfactory robustness . in contrast , our approach consists in classifying graphs of keypoints , which incorporates structural information during the classification phase and allows the extraction of simpler feature vectors that are naturally robust . in the present work , 3-vertices graphs have been considered , though the methodology is general and larger order graphs may be adopted . successful experimental results obtained for real-time object detection in video sequences are reported .", "topics": ["feature extraction", "feature vector"]}
{"title": "marta gans : unsupervised representation learning for remote sensing image classification", "abstract": "with the development of deep learning , supervised learning has frequently been adopted to classify remotely sensed images using convolutional networks ( cnns ) . however , due to the limited amount of labeled data available , supervised learning is often difficult to carry out . therefore , we proposed an unsupervised model called multiple-layer feature-matching generative adversarial networks ( marta gans ) to learn a representation using only unlabeled data . marta gans consists of both a generative model $ g $ and a discriminative model $ d $ . we treat $ d $ as a feature extractor . to fit the complex properties of remote sensing data , we use a fusion layer to merge the mid-level and global features . $ g $ can produce numerous images that are similar to the training data ; therefore , $ d $ can learn better representations of remotely sensed images using the training data provided by $ g $ . the classification results on two widely used remote sensing image databases show that the proposed method significantly improves the classification performance compared with other state-of-the-art methods .", "topics": ["generative model", "test set"]}
{"title": "a parallel way to select the parameters of svm based on the ant optimization algorithm", "abstract": "a large number of experimental data shows that support vector machine ( svm ) algorithm has obvious advantages in text classification , handwriting recognition , image classification , bioinformatics , and some other fields . to some degree , the optimization of svm depends on its kernel function and slack variable , the determinant of which is its parameters $ \\delta $ and c in the classification function . that is to say , to optimize the svm algorithm , the optimization of the two parameters play a huge role . ant colony optimization ( aco ) is optimization algorithm which simulate ants to find the optimal path.in the available literature , we mix the aco algorithm and parallel algorithm together to find a well parameters .", "topics": ["support vector machine", "mathematical optimization"]}
{"title": "stock prediction : a method based on extraction of news features and recurrent neural networks", "abstract": "this paper proposed a method for stock prediction . in terms of feature extraction , we extract the features of stock-related news besides stock prices . we first select some seed words based on experience which are the symbols of good news and bad news . then we propose an optimization method and calculate the positive polar of all words . after that , we construct the features of news based on the positive polar of their words . in consideration of sequential stock prices and continuous news effects , we propose a recurrent neural network model to help predict stock prices . compared to svm classifier with price features , we find our proposed method has an over 5 % improvement on stock prediction accuracy in experiments .", "topics": ["feature extraction", "recurrent neural network"]}
{"title": "the effects of hyperparameters on sgd training of neural networks", "abstract": "the performance of neural network classifiers is determined by a number of hyperparameters , including learning rate , batch size , and depth . a number of attempts have been made to explore these parameters in the literature , and at times , to develop methods for optimizing them . however , exploration of parameter spaces has often been limited . in this note , i report the results of large scale experiments exploring these different parameters and their interactions .", "topics": ["test set", "feature vector"]}
{"title": "when to reset your keys : optimal timing of security updates via learning", "abstract": "cybersecurity is increasingly threatened by advanced and persistent attacks . as these attacks are often designed to disable a system ( or a critical resource , e.g . , a user account ) repeatedly , it is crucial for the defender to keep updating its security measures to strike a balance between the risk of being compromised and the cost of security updates . moreover , these decisions often need to be made with limited and delayed feedback due to the stealthy nature of advanced attacks . in addition to targeted attacks , such an optimal timing policy under incomplete information has broad applications in cybersecurity . examples include key rotation , password change , application of patches , and virtual machine refreshing . however , rigorous studies of optimal timing are rare . further , existing solutions typically rely on a pre-defined attack model that is known to the defender , which is often not the case in practice . in this work , we make an initial effort towards achieving optimal timing of security updates in the face of unknown stealthy attacks . we consider a variant of the influential flipit game model with asymmetric feedback and unknown attack time distribution , which provides a general model to consecutive security updates . the defender 's problem is then modeled as a time associative bandit problem with dependent arms . we derive upper confidence bound based learning policies that achieve low regret compared with optimal periodic defense strategies that can only be derived when attack time distributions are known .", "topics": ["regret ( decision theory )", "value ( ethics )"]}
{"title": "a generative model of words and relationships from multiple sources", "abstract": "neural language models are a powerful tool to embed words into semantic vector spaces . however , learning such models generally relies on the availability of abundant and diverse training examples . in highly specialised domains this requirement may not be met due to difficulties in obtaining a large corpus , or the limited range of expression in average use . such domains may encode prior knowledge about entities in a knowledge base or ontology . we propose a generative model which integrates evidence from diverse data sources , enabling the sharing of semantic information . we achieve this by generalising the concept of co-occurrence from distributional semantics to include other relationships between entities or words , which we model as affine transformations on the embedding space . we demonstrate the effectiveness of this approach by outperforming recent models on a link prediction task and demonstrating its ability to profit from partially or fully unobserved data training labels . we further demonstrate the usefulness of learning from different data sources with overlapping vocabularies .", "topics": ["generative model", "entity"]}
{"title": "free energy-based reinforcement learning using a quantum processor", "abstract": "recent theoretical and experimental results suggest the possibility of using current and near-future quantum hardware in challenging sampling tasks . in this paper , we introduce free energy-based reinforcement learning ( ferl ) as an application of quantum hardware . we propose a method for processing a quantum annealer 's measured qubit spin configurations in approximating the free energy of a quantum boltzmann machine ( qbm ) . we then apply this method to perform reinforcement learning on the grid-world problem using the d-wave 2000q quantum annealer . the experimental results show that our technique is a promising method for harnessing the power of quantum sampling in reinforcement learning tasks .", "topics": ["reinforcement learning"]}
{"title": "a paradigm shift : detecting human rights violations through web images", "abstract": "the growing presence of devices carrying digital cameras , such as mobile phones and tablets , combined with ever improving internet networks have enabled ordinary citizens , victims of human rights abuse , and participants in armed conflicts , protests , and disaster situations to capture and share via social media networks images and videos of specific events . this paper discusses the potential of images in human rights context including the opportunities and challenges they present . this study demonstrates that real-world images have the capacity to contribute complementary data to operational human rights monitoring efforts when combined with novel computer vision approaches . the analysis is concluded by arguing that if images are to be used effectively to detect and identify human rights violations by rights advocates , greater attention to gathering task-specific visual concepts from large-scale web images is required .", "topics": ["computer vision"]}
{"title": "efficient sum of outer products dictionary learning ( soup-dil ) and its application to inverse problems", "abstract": "the sparsity of signals in a transform domain or dictionary has been exploited in applications such as compression , denoising and inverse problems . more recently , data-driven adaptation of synthesis dictionaries has shown promise compared to analytical dictionary models . however , dictionary learning problems are typically non-convex and np-hard , and the usual alternating minimization approaches for these problems are often computationally expensive , with the computations dominated by the np-hard synthesis sparse coding step . this paper exploits the ideas that drive algorithms such as k-svd , and investigates in detail efficient methods for aggregate sparsity penalized dictionary learning by first approximating the data with a sum of sparse rank-one matrices ( outer products ) and then using a block coordinate descent approach to estimate the unknowns . the resulting block coordinate descent algorithms involve efficient closed-form solutions . furthermore , we consider the problem of dictionary-blind image reconstruction , and propose novel and efficient algorithms for adaptive image reconstruction using block coordinate descent and sum of outer products methodologies . we provide a convergence study of the algorithms for dictionary learning and dictionary-blind image reconstruction . our numerical experiments show the promising performance and speed-ups provided by the proposed methods over previous schemes in sparse data representation and compressed sensing-based image reconstruction .", "topics": ["approximation algorithm", "numerical analysis"]}
{"title": "an ontology construction approach for the domain of poultry science using protege", "abstract": "the information retrieval systems that are present nowadays are mainly based on full text matching of keywords or topic based classification . this matching of keywords often returns a large number of irrelevant information and this does not meet the users query requirement . in order to solve this problem and to enhance the search using semantic environment , a technique named ontology is implemented for the field of poultry in this paper . ontology is an emerging technique in the current field of research in semantic environment . this paper constructs ontology using the tool named protege version 4.0 and this also generates resource description framework schema and xml scripts for using poultry ontology in web .", "topics": ["relevance"]}
{"title": "understanding deep convolutional networks", "abstract": "deep convolutional networks provide state of the art classifications and regressions results over many high-dimensional problems . we review their architecture , which scatters data with a cascade of linear filter weights and non-linearities . a mathematical framework is introduced to analyze their properties . computations of invariants involve multiscale contractions , the linearization of hierarchical symmetries , and sparse separations . applications are discussed .", "topics": ["sparse matrix"]}
{"title": "a smartphone application to measure the quality of pest control spraying machines via image analysis", "abstract": "the need for higher agricultural productivity has demanded the intensive use of pesticides . however , their correct use depends on assessment methods that can accurately predict how well the pesticides ' spraying covered the intended crop region . some methods have been proposed in the literature , but their high cost and low portability harm their widespread use . this paper proposes and experimentally evaluates a new methodology based on the use of a smartphone-based mobile application , named dropleaf . experiments performed using dropleaf showed that , in addition to its versatility , it can predict with high accuracy the pesticide spraying . dropleaf is a five-fold image-processing methodology based on : ( i ) color space conversion , ( ii ) threshold noise removal , ( iii ) convolutional operations of dilation and erosion , ( iv ) detection of contour markers in the water-sensitive card , and , ( v ) identification of droplets via the marker-controlled watershed transformation . the authors performed successful experiments over two case studies , the first using a set of synthetic cards and the second using a real-world crop . the proposed tool can be broadly used by farmers equipped with conventional mobile phones , improving the use of pesticides with health , environmental and financial benefits .", "topics": ["image segmentation", "synthetic data"]}
{"title": "an analysis of scale invariance in object detection - snip", "abstract": "an analysis of different techniques for recognizing and detecting objects under extreme scale variation is presented . scale specific and scale invariant design of detectors are compared by training them with different configurations of input data . to examine if upsampling images is necessary for detecting small objects , we evaluate the performance of different network architectures for classifying small objects on imagenet . based on this analysis , we propose a deep end-to-end trainable image pyramid network for object detection which operates on the same image scales during training and inference . since small and large objects are difficult to recognize at smaller and larger scales respectively , we present a novel training scheme called scale normalization for image pyramids ( snip ) which selectively back-propagates the gradients of object instances of different sizes as a function of the image scale . on the coco dataset , our single model performance is 45.7 % and an ensemble of 3 networks obtains an map of 48.3 % . we use imagenet-1000 pre-trained models and only train with bounding box supervision . our submission won the best student entry in the coco 2017 challenge . code will be made available at http : //bit.ly/2yxvg4c .", "topics": ["object detection", "end-to-end principle"]}
{"title": "which learning algorithms can generalize identity-based rules to novel inputs ?", "abstract": "we propose a novel framework for the analysis of learning algorithms that allows us to say when such algorithms can and can not generalize certain patterns from training data to test data . in particular we focus on situations where the rule that must be learned concerns two components of a stimulus being identical . we call such a basis for discrimination an identity-based rule . identity-based rules have proven to be difficult or impossible for certain types of learning algorithms to acquire from limited datasets . this is in contrast to human behaviour on similar tasks . here we provide a framework for rigorously establishing which learning algorithms will fail at generalizing identity-based rules to novel stimuli . we use this framework to show that such algorithms are unable to generalize identity-based rules to novel inputs unless trained on virtually all possible inputs . we demonstrate these results computationally with a multilayer feedforward neural network .", "topics": ["test set"]}
{"title": "low rank variation dictionary and inverse projection group sparse representation model for breast tumor classification", "abstract": "sparse representation classification achieves good results by addressing recognition problem with sufficient training samples per subject . however , src performs not very well for small sample data . in this paper , an inverse-projection group sparse representation model is presented for breast tumor classification , which is based on constructing low-rank variation dictionary . the proposed low-rank variation dictionary tackles tumor recognition problem from the viewpoint of detecting and using variations in gene expression profiles of normal and patients , rather than directly using these samples . the inverse projection group sparsity representation model is constructed based on taking full using of exist samples and group effect of microarray gene data . extensive experiments on public breast tumor microarray gene expression datasets demonstrate the proposed technique is competitive with state-of-the-art methods . the results of breast-1 , breast-2 and breast-3 databases are 80.81 % , 89.10 % and 100 % respectively , which are better than the latest literature .", "topics": ["sparse matrix", "database"]}
{"title": "apply local clustering method to improve the running speed of ant colony optimization", "abstract": "ant colony optimization ( aco ) has time complexity o ( t*m*n*n ) , and its typical application is to solve traveling salesman problem ( tsp ) , where t , m , and n denotes the iteration number , number of ants , number of cities respectively . cutting down running time is one of study focuses , and one way is to decrease parameter t and n , especially n. for this focus , the following method is presented in this paper . firstly , design a novel clustering algorithm named special local clustering algorithm ( slc ) , then apply it to classify all cities into compact classes , where compact class is the class that all cities in this class cluster tightly in a small region . secondly , let aco act on every class to get a local tsp route . thirdly , all local tsp routes are jointed to form solution . fourthly , the inaccuracy of solution caused by clustering is eliminated . simulation shows that the presented method improves the running speed of aco by 200 factors at least . and this high speed is benefit from two factors . one is that class has small size and parameter n is cut down . the route length at every iteration step is convergent when aco acts on compact class . the other factor is that , using the convergence of route length as termination criterion of aco and parameter t is cut down .", "topics": ["cluster analysis", "time complexity"]}
{"title": "sparql as a foreign language", "abstract": "in the last years , the linked data cloud has achieved a size of more than 100 billion facts pertaining to a multitude of domains . however , accessing this information has been significantly challenging for lay users . approaches to problems such as question answering on linked data and link discovery have notably played a role in increasing information access . these approaches are often based on handcrafted and/or statistical models derived from data observation . recently , deep learning architectures based on neural networks called seq2seq have shown to achieve state-of-the-art results at translating sequences into sequences . in this direction , we propose neural sparql machines , end-to-end deep architectures to translate any natural language expression into sentences encoding sparql queries . our preliminary results , restricted on selected dbpedia classes , show that neural sparql machines are a promising approach for question answering on linked data , as they can deal with known problems such as vocabulary mismatch and perform graph pattern composition .", "topics": ["neural networks", "natural language"]}
{"title": "stardata : a starcraft ai research dataset", "abstract": "we release a dataset of 65646 starcraft replays that contains 1535 million frames and 496 million player actions . we provide full game state data along with the original replays that can be viewed in starcraft . the game state data was recorded every 3 frames which ensures suitability for a wide variety of machine learning tasks such as strategy classification , inverse reinforcement learning , imitation learning , forward modeling , partial information extraction , and others . we use torchcraft to extract and store the data , which standardizes the data format for both reading from replays and reading directly from the game . furthermore , the data can be used on different operating systems and platforms . the dataset contains valid , non-corrupted replays only and its quality and diversity was ensured by a number of heuristics . we illustrate the diversity of the data with various statistics and provide examples of tasks that benefit from the dataset . we make the dataset available at https : //github.com/torchcraft/stardata . en taro adun !", "topics": ["reinforcement learning", "heuristic"]}
{"title": "accelerated stochastic power iteration", "abstract": "principal component analysis ( pca ) is one of the most powerful tools in machine learning . the simplest method for pca , the power iteration , requires $ \\mathcal o ( 1/\\delta ) $ full-data passes to recover the principal component of a matrix with eigen-gap $ \\delta $ . lanczos , a significantly more complex method , achieves an accelerated rate of $ \\mathcal o ( 1/\\sqrt { \\delta } ) $ passes . modern applications , however , motivate methods that only ingest a subset of available data , known as the stochastic setting . in the online stochastic setting , simple algorithms like oja 's iteration achieve the optimal sample complexity $ \\mathcal o ( \\sigma^2/\\delta^2 ) $ . unfortunately , they are fully sequential , and also require $ \\mathcal o ( \\sigma^2/\\delta^2 ) $ iterations , far from the $ \\mathcal o ( 1/\\sqrt { \\delta } ) $ rate of lanczos . we propose a simple variant of the power iteration with an added momentum term , that achieves both the optimal sample and iteration complexity . in the full-pass setting , standard analysis shows that momentum achieves the accelerated rate , $ \\mathcal o ( 1/\\sqrt { \\delta } ) $ . we demonstrate empirically that naively applying momentum to a stochastic method , does not result in acceleration . we perform a novel , tight variance analysis that reveals the `` breaking-point variance '' beyond which this acceleration does not occur . by combining this insight with modern variance reduction techniques , we construct stochastic pca algorithms , for the online and offline setting , that achieve an accelerated iteration complexity $ \\mathcal o ( 1/\\sqrt { \\delta } ) $ . due to the embarassingly parallel nature of our methods , this acceleration translates directly to wall-clock time if deployed in a parallel environment . our approach is very general , and applies to many non-convex optimization problems that can now be accelerated using the same technique .", "topics": ["iteration"]}
{"title": "exact sampling from determinantal point processes", "abstract": "determinantal point processes ( dpps ) are an important concept in random matrix theory and combinatorics . they have also recently attracted interest in the study of numerical methods for machine learning , as they offer an elegant `` missing link '' between independent monte carlo sampling and deterministic evaluation on regular grids , applicable to a general set of spaces . this is helpful whenever an algorithm *explores* to reduce uncertainty , such as in active learning , bayesian optimization , reinforcement learning , and marginalization in graphical models . to draw samples from a dpp in practice , existing literature focuses on approximate schemes of low cost , or comparably inefficient exact algorithms like rejection sampling . we point out that , for many settings of relevance to machine learning , it is also possible to draw *exact* samples from dpps on continuous domains . we start from an intuitive example on the real line , which is then generalized to multivariate real vector spaces . we also compare to previously studied approximations , showing that exact sampling , despite higher cost , can be preferable where precision is needed .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "conditional accelerated lazy stochastic gradient descent", "abstract": "in this work we introduce a conditional accelerated lazy stochastic gradient descent algorithm with optimal number of calls to a stochastic first-order oracle and convergence rate $ o\\left ( \\frac { 1 } { \\varepsilon^2 } \\right ) $ improving over the projection-free , online frank-wolfe based stochastic gradient descent of hazan and kale [ 2012 ] with convergence rate $ o\\left ( \\frac { 1 } { \\varepsilon^4 } \\right ) $ .", "topics": ["gradient descent", "gradient"]}
{"title": "learning macro-actions for state-space planning", "abstract": "planning has achieved significant progress in recent years . among the various approaches to scale up plan synthesis , the use of macro-actions has been widely explored . as a first stage towards the development of a solution to learn on-line macro-actions , we propose an algorithm to identify useful macro-actions based on data mining techniques . the integration in the planning search of these learned macro-actions shows significant improvements over four classical planning benchmarks .", "topics": ["data mining"]}
{"title": "outlier detection for text data : an extended version", "abstract": "the problem of outlier detection is extremely challenging in many domains such as text , in which the attribute values are typically non-negative , and most values are zero . in such cases , it often becomes difficult to separate the outliers from the natural variations in the patterns in the underlying data . in this paper , we present a matrix factorization method , which is naturally able to distinguish the anomalies with the use of low rank approximations of the underlying data . our iterative algorithm tonmf is based on block coordinate descent ( bcd ) framework . we define blocks over the term-document matrix such that the function becomes solvable . given most recently updated values of other matrix blocks , we always update one block at a time to its optimal . our approach has significant advantages over traditional methods for text outlier detection . finally , we present experimental results illustrating the effectiveness of our method over competing methods .", "topics": ["value ( ethics )", "approximation"]}
{"title": "geometric implications of the naive bayes assumption", "abstract": "a naive ( or idiot ) bayes network is a network with a single hypothesis node and several observations that are conditionally independent given the hypothesis . we recently surveyed a number of members of the uai community and discovered a general lack of understanding of the implications of the naive bayes assumption on the kinds of problems that can be solved by these networks . it has long been recognized [ minsky 61 ] that if observations are binary , the decision surfaces in these networks are hyperplanes . we extend this result ( hyperplane separability ) to naive bayes networks with m-ary observations . in addition , we illustrate the effect of observation-observation dependencies on decision surfaces . finally , we discuss the implications of these results on knowledge acquisition and research in learning .", "topics": ["bayesian network"]}
{"title": "iterative exact global histogram specification and ssim gradient ascent : a proof of convergence , step size and parameter selection", "abstract": "the ssim-optimized exact global histogram specification ( eghs ) is shown to converge in the sense that the first order approximation of the result 's quality ( i.e . , its structural similarity with input ) does not decrease in an iteration , when the step size is small . each iteration is composed of ssim gradient ascent and basic eghs with the specified target histogram . selection of step size and other parameters is also discussed .", "topics": ["iteration", "gradient descent"]}
{"title": "towards grounding conceptual spaces in neural representations", "abstract": "the highly influential framework of conceptual spaces provides a geometric way of representing knowledge . it aims at bridging the gap between symbolic and subsymbolic processing . instances are represented by points in a high-dimensional space and concepts are represented by convex regions in this space . in this paper , we present our approach towards grounding the dimensions of a conceptual space in latent spaces learned by an infogan from unlabeled data .", "topics": ["artificial intelligence"]}
{"title": "praaline : integrating tools for speech corpus research", "abstract": "this paper presents praaline , an open-source software system for managing , annotating , analysing and visualising speech corpora . researchers working with speech corpora are often faced with multiple tools and formats , and they need to work with ever-increasing amounts of data in a collaborative way . praaline integrates and extends existing time-proven tools for spoken corpora analysis ( praat , sonic visualiser and a bridge to the r statistical package ) in a modular system , facilitating automation and reuse . users are exposed to an integrated , user-friendly interface from which to access multiple tools . corpus metadata and annotations may be stored in a database , locally or remotely , and users can define the metadata and annotation structure . users may run a customisable cascade of analysis steps , based on plug-ins and scripts , and update the database with the results . the corpus database may be queried , to produce aggregated data-sets . praaline is extensible using python or c++ plug-ins , while praat and r scripts may be executed against the corpus data . a series of visualisations , editors and plug-ins are provided . praaline is free software , released under the gpl license .", "topics": ["text corpus", "speech recognition"]}
{"title": "a fusion algorithm for solving bayesian decision problems", "abstract": "this paper proposes a new method for solving bayesian decision problems . the method consists of representing a bayesian decision problem as a valuation-based system and applying a fusion algorithm for solving it . the fusion algorithm is a hybrid of local computational methods for computation of marginals of joint probability distributions and the local computational methods for discrete optimization problems .", "topics": ["value ( ethics )", "computation"]}
{"title": "efficient computation of mean truncated hitting times on very large graphs", "abstract": "previous work has shown the effectiveness of random walk hitting times as a measure of dissimilarity in a variety of graph-based learning problems such as collaborative filtering , query suggestion or finding paraphrases . however , application of hitting times has been limited to small datasets because of computational restrictions . this paper develops a new approximation algorithm with which hitting times can be computed on very large , disk-resident graphs , making their application possible to problems which were previously out of reach . this will potentially benefit a range of large-scale problems .", "topics": ["approximation algorithm", "computation"]}
{"title": "addressing the data sparsity issue in neural amr parsing", "abstract": "neural attention models have achieved great success in different nlp tasks . how- ever , they have not fulfilled their promise on the amr parsing task due to the data sparsity issue . in this paper , we de- scribe a sequence-to-sequence model for amr parsing and present different ways to tackle the data sparsity problem . we show that our methods achieve significant improvement over a baseline neural atten- tion model and our results are also compet- itive against state-of-the-art systems that do not use extra linguistic resources .", "topics": ["baseline ( configuration management )", "natural language processing"]}
{"title": "tensorial mixture models", "abstract": "casting neural networks in generative frameworks is a highly sought-after endeavor these days . contemporary methods , such as generative adversarial networks , capture some of the generative capabilities , but not all . in particular , they lack the ability of tractable marginalization , and thus are not suitable for many tasks . other methods , based on arithmetic circuits and sum-product networks , do allow tractable marginalization , but their performance is challenged by the need to learn the structure of a circuit . building on the tractability of arithmetic circuits , we leverage concepts from tensor analysis , and derive a family of generative models we call tensorial mixture models ( tmms ) . tmms assume a simple convolutional network structure , and in addition , lend themselves to theoretical analyses that allow comprehensive understanding of the relation between their structure and their expressive properties . we thus obtain a generative model that is tractable on one hand , and on the other hand , allows effective representation of rich distributions in an easily controlled manner . these two capabilities are brought together in the task of classification under missing data , where tmms deliver state of the art accuracies with seamless implementation and design .", "topics": ["generative model", "time complexity"]}
{"title": "situated structure learning of a bayesian logic network for commonsense reasoning", "abstract": "this paper details the implementation of an algorithm for automatically generating a high-level knowledge network to perform commonsense reasoning , specifically with the application of robotic task repair . the network is represented using a bayesian logic network ( bln ) ( jain , waldherr , and beetz 2009 ) , which combines a set of directed relations between abstract concepts , including isa , atlocation , hasproperty , and usedfor , with a corresponding probability distribution that models the uncertainty inherent in these relations . inference over this network enables reasoning over the abstract concepts in order to perform appropriate object substitution or to locate missing objects in the robot 's environment . the structure of the network is generated by combining information from two existing knowledge sources : conceptnet ( speer and havasi 2012 ) , and wordnet ( miller 1995 ) . this is done in a `` situated '' manner by only including information relevant a given context . results show that the generated network is able to accurately predict object categories , locations , properties , and affordances in three different household scenarios .", "topics": ["high- and low-level"]}
{"title": "building an interpretable recommender via loss-preserving transformation", "abstract": "we propose a method for building an interpretable recommender system for personalizing online content and promotions . historical data available for the system consists of customer features , provided content ( promotions ) , and user responses . unlike in a standard multi-class classification setting , misclassification costs depend on both recommended actions and customers . our method transforms such a data set to a new set which can be used with standard interpretable multi-class classification algorithms . the transformation has the desirable property that minimizing the standard misclassification penalty in this new space is equivalent to minimizing the custom cost function .", "topics": ["loss function"]}
{"title": "detecting adversarial perturbations with saliency", "abstract": "in this paper we propose a novel method for detecting adversarial examples by training a binary classifier with both origin data and saliency data . in the case of image classification model , saliency simply explain how the model make decisions by identifying significant pixels for prediction . a model shows wrong classification output always learns wrong features and shows wrong saliency as well . our approach shows good performance on detecting adversarial perturbations . we quantitatively evaluate generalization ability of the detector , showing that detectors trained with strong adversaries perform well on weak adversaries .", "topics": ["computer vision", "pixel"]}
{"title": "boosting the kernelized shapelets : theory and algorithms for local features", "abstract": "we consider binary classification problems using local features of objects . one of motivating applications is time-series classification , where features reflecting some local closeness measure between a time series and a pattern sequence called shapelet are useful . despite the empirical success of such approaches using local features , the generalization ability of resulting hypotheses is not fully understood and previous work relies on a bunch of heuristics . in this paper , we formulate a class of hypotheses using local features , where the richness of features is controlled by kernels . we derive generalization bounds of sparse ensembles over the class which is exponentially better than a standard analysis in terms of the number of possible local features . the resulting optimization problem is well suited to the boosting approach and the weak learning problem is formulated as a dc program , for which practical algorithms exist . in preliminary experiments on time-series data sets , our method achieves competitive accuracy with the state-of-the-art algorithms with small parameter-tuning cost .", "topics": ["time series", "optimization problem"]}
{"title": "spotlight the negatives : a generalized discriminative latent model", "abstract": "discriminative latent variable models ( lvm ) are frequently applied to various visual recognition tasks . in these systems the latent ( hidden ) variables provide a formalism for modeling structured variation of visual features . conventionally , latent variables are de- fined on the variation of the foreground ( positive ) class . in this work we augment lvms to include negative latent variables corresponding to the background class . we formalize the scoring function of such a generalized lvm ( glvm ) . then we discuss a framework for learning a model based on the glvm scoring function . we theoretically showcase how some of the current visual recognition methods can benefit from this generalization . finally , we experiment on a generalized form of deformable part models with negative latent variables and show significant improvements on two different detection tasks .", "topics": ["computer vision"]}
{"title": "character-based neural machine translation", "abstract": "neural machine translation ( mt ) has reached state-of-the-art results . however , one of the main challenges that neural mt still faces is dealing with very large vocabularies and morphologically rich languages . in this paper , we propose a neural mt system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations . the resulting unlimited-vocabulary and affix-aware source word embeddings are tested in a state-of-the-art neural mt based on an attention-based bidirectional recurrent neural network . the proposed mt scheme provides improved results even when the source language is not morphologically rich . improvements up to 3 bleu points are obtained in the german-english wmt task .", "topics": ["machine translation"]}
{"title": "transferring autonomous driving knowledge on simulated and real intersections", "abstract": "we view intersection handling on autonomous vehicles as a reinforcement learning problem , and study its behavior in a transfer learning setting . we show that a network trained on one type of intersection generally is not able to generalize to other intersections . however , a network that is pre-trained on one intersection and fine-tuned on another performs better on the new task compared to training in isolation . this network also retains knowledge of the prior task , even though some forgetting occurs . finally , we show that the benefits of fine-tuning hold when transferring simulated intersection handling knowledge to a real autonomous vehicle .", "topics": ["reinforcement learning", "simulation"]}
{"title": "the exact schema theorem", "abstract": "a schema is a naturally defined subset of the space of fixed-length binary strings . the holland schema theorem gives a lower bound on the expected fraction of a population in a schema after one generation of a simple genetic algorithm . this paper gives formulas for the exact expected fraction of a population in a schema after one generation of the simple genetic algorithm . holland 's schema theorem has three parts , one for selection , one for crossover , and one for mutation . the selection part is exact , whereas the crossover and mutation parts are approximations . this paper shows how the crossover and mutation parts can be made exact . holland 's schema theorem follows naturally as a corollary . there is a close relationship between schemata and the representation of the population in the walsh basis . this relationship is used in the derivation of the results , and can also make computation of the schema averages more efficient . this paper gives a version of the vose infinite population model where crossover and mutation are separated into two functions rather than a single `` mixing '' function .", "topics": ["approximation", "computation"]}
{"title": "the power of arc consistency for csps defined by partially-ordered forbidden patterns", "abstract": "characterising tractable fragments of the constraint satisfaction problem ( csp ) is an important challenge in theoretical computer science and artificial intelligence . forbidding patterns ( generic sub-instances ) provides a means of defining csp fragments which are neither exclusively language-based nor exclusively structure-based . it is known that the class of binary csp instances in which the broken-triangle pattern ( btp ) does not occur , a class which includes all tree-structured instances , are decided by arc consistency ( ac ) , a ubiquitous reduction operation in constraint solvers . we provide a characterisation of simple partially-ordered forbidden patterns which have this ac-solvability property . it turns out that btp is just one of five such ac-solvable patterns . the four other patterns allow us to exhibit new tractable classes .", "topics": ["artificial intelligence"]}
{"title": "segmentation of facial expressions using semi-definite programming and generalized principal component analysis", "abstract": "in this paper , we use semi-definite programming and generalized principal component analysis ( gpca ) to distinguish between two or more different facial expressions . in the first step , semi-definite programming is used to reduce the dimension of the image data and `` unfold '' the manifold which the data points ( corresponding to facial expressions ) reside on . next , gpca is used to fit a series of subspaces to the data points and associate each data point with a subspace . data points that belong to the same subspace are claimed to belong to the same facial expression category . an example is provided .", "topics": ["polynomial"]}
{"title": "fuzzy bayesian learning", "abstract": "in this paper we propose a novel approach for learning from data using rule based fuzzy inference systems where the model parameters are estimated using bayesian inference and markov chain monte carlo ( mcmc ) techniques . we show the applicability of the method for regression and classification tasks using synthetic data-sets and also a real world example in the financial services industry . then we demonstrate how the method can be extended for knowledge extraction to select the individual rules in a bayesian way which best explains the given data . finally we discuss the advantages and pitfalls of using this method over state-of-the-art techniques and highlight the specific class of problems where this would be useful .", "topics": ["synthetic data", "markov chain"]}
{"title": "stochastic attribute-value grammars", "abstract": "probabilistic analogues of regular and context-free grammars are well-known in computational linguistics , and currently the subject of intensive research . to date , however , no satisfactory probabilistic analogue of attribute-value grammars has been proposed : previous attempts have failed to define a correct parameter-estimation algorithm . in the present paper , i define stochastic attribute-value grammars and give a correct algorithm for estimating their parameters . the estimation algorithm is adapted from della pietra , della pietra , and lafferty ( 1995 ) . to estimate model parameters , it is necessary to compute the expectations of certain functions under random fields . in the application discussed by della pietra , della pietra , and lafferty ( representing english orthographic constraints ) , gibbs sampling can be used to estimate the needed expectations . the fact that attribute-value grammars generate constrained languages makes gibbs sampling inapplicable , but i show how a variant of gibbs sampling , the metropolis-hastings algorithm , can be used instead .", "topics": ["sampling ( signal processing )", "value ( ethics )"]}
{"title": "a kernel test for three-variable interactions with random processes", "abstract": "we apply a wild bootstrap method to the lancaster three-variable interaction measure in order to detect factorisation of the joint distribution on three variables forming a stationary random process , for which the existing permutation bootstrap method fails . as in the i.i.d . case , the lancaster test is found to outperform existing tests in cases for which two independent variables individually have a weak influence on a third , but that when considered jointly the influence is strong . the main contributions of this paper are twofold : first , we prove that the lancaster statistic satisfies the conditions required to estimate the quantiles of the null distribution using the wild bootstrap ; second , the manner in which this is proved is novel , simpler than existing methods , and can further be applied to other statistics .", "topics": ["kernel ( operating system )"]}
{"title": "women also snowboard : overcoming bias in captioning models", "abstract": "most machine learning methods are known to capture and exploit biases of the training data . while some biases are beneficial for learning , others are harmful . specifically , image captioning models tend to exaggerate biases present in training data ( e.g . , if a word is present in 60 % of training sentences , it might be predicted in 70 % of sentences at test time ) . this can lead to incorrect captions in domains where unbiased captions are desired , or required , due to over-reliance on the learned prior and image context . in this work we investigate generation of gender-specific caption words ( e.g . man , woman ) based on the person 's appearance or the image context . we introduce a new equalizer model that ensures equal gender probability when gender evidence is occluded in a scene and confident predictions when gender evidence is present . the resulting model is forced to look at a person rather than use contextual cues to make a gender-specific predictions . the losses that comprise our model , the appearance confusion loss and the confident loss , are general , and can be added to any description model in order to mitigate impacts of unwanted bias in a description dataset . our proposed model has lower error than prior work when describing images with people and mentioning their gender and more closely matches the ground truth ratio of sentences including women to sentences including men . we also show that unlike other approaches , our model is indeed more often looking at people when predicting their gender .", "topics": ["test set", "ground truth"]}
{"title": "cnndroid : gpu-accelerated execution of trained deep convolutional neural networks on android", "abstract": "many mobile applications running on smartphones and wearable devices would potentially benefit from the accuracy and scalability of deep cnn-based machine learning algorithms . however , performance and energy consumption limitations make the execution of such computationally intensive algorithms on mobile devices prohibitive . we present a gpu-accelerated library , dubbed cnndroid , for execution of trained deep cnns on android-based mobile devices . empirical evaluations show that cnndroid achieves up to 60x speedup and 130x energy saving on current mobile devices . the cnndroid open source library is available for download at https : //github.com/encp/cnndroid", "topics": ["scalability"]}
{"title": "star-rt : visual attention for real-time video game playing", "abstract": "in this paper we present star-rt - the first working prototype of selective tuning attention reference ( star ) model and cognitive programs ( cps ) . the selective tuning ( st ) model received substantial support through psychological and neurophysiological experiments . the star framework expands st and applies it to practical visual tasks . in order to do so , similarly to many cognitive architectures , star combines the visual hierarchy ( based on st ) with the executive controller , working and short-term memory components and fixation controller . cps in turn enable the communication among all these elements for visual task execution . to test the relevance of the system in a realistic context , we implemented the necessary components of star and designed cps for playing two closed-source video games - canabaltand robot unicorn attack . since both games run in a browser window , our algorithm has the same amount of information and the same amount of time to react to the events on the screen as a human player would . star-rt plays both games in real time using only visual input and achieves scores comparable to human expert players . it thus provides an existence proof for the utility of the particular cp structure and primitives used and the potential for continued experimentation and verification of their utility in broader scenarios .", "topics": ["relevance"]}
{"title": "x.ent : r package for entities and relations extraction based on unsupervised learning and document structure", "abstract": "relation extraction with accurate precision is still a challenge when processing full text databases . we propose an approach based on cooccurrence analysis in each document for which we used document organization to improve accuracy of relation extraction . this approach is implemented in a r package called \\emph { x.ent } . another facet of extraction relies on use of extracted relation into a querying system for expert end-users . two datasets had been used . one of them gets interest from specialists of epidemiology in plant health . for this dataset usage is dedicated to plant-disease exploration through agricultural information news . an open-data platform exploits exports from \\emph { x.ent } and is publicly available .", "topics": ["unsupervised learning", "database"]}
{"title": "geometrical insights for implicit generative modeling", "abstract": "learning algorithms for implicit generative models can optimize a variety of criteria that measure how the data distribution differs from the implicit model distribution , including the wasserstein distance , the energy distance , and the maximum mean discrepancy criterion . a careful look at the geometries induced by these distances on the space of probability measures reveals interesting differences . in particular , we can establish surprising approximate global convergence guarantees for the $ 1 $ -wasserstein distance , even when the parametric generator has a nonconvex parametrization .", "topics": ["approximation algorithm", "eisenstein 's criterion"]}
{"title": "resource modalities in game semantics", "abstract": "the description of resources in game semantics has never achieved the simplicity and precision of linear logic , because of a misleading conception : the belief that linear logic is more primitive than game semantics . we advocate instead the contrary : that game semantics is conceptually more primitive than linear logic . starting from this revised point of view , we design a categorical model of resources in game semantics , and construct an arena game model where the usual notion of bracketing is extended to multi- bracketing in order to capture various resource policies : linear , af & # 64257 ; ne and exponential .", "topics": ["time complexity"]}
{"title": "sparse approximation of a kernel mean", "abstract": "kernel means are frequently used to represent probability distributions in machine learning problems . in particular , the well known kernel density estimator and the kernel mean embedding both have the form of a kernel mean . unfortunately , kernel means are faced with scalability issues . a single point evaluation of the kernel density estimator , for example , requires a computation time linear in the training sample size . to address this challenge , we present a method to efficiently construct a sparse approximation of a kernel mean . we do so by first establishing an incoherence-based bound on the approximation error , and then noticing that , for the case of radial kernels , the bound can be minimized by solving the $ k $ -center problem . the outcome is a linear time construction of a sparse kernel mean , which also lends itself naturally to an automatic sparsity selection scheme . we show the computational gains of our method by looking at three problems involving kernel means : euclidean embedding of distributions , class proportion estimation , and clustering using the mean-shift algorithm .", "topics": ["kernel ( operating system )", "cluster analysis"]}
{"title": "improving neural networks with bunches of neurons modeled by kumaraswamy units : preliminary study", "abstract": "deep neural networks have recently achieved state-of-the-art results in many machine learning problems , e.g . , speech recognition or object recognition . hitherto , work on rectified linear units ( relu ) provides empirical and theoretical evidence on performance increase of neural networks comparing to typically used sigmoid activation function . in this paper , we investigate a new manner of improving neural networks by introducing a bunch of copies of the same neuron modeled by the generalized kumaraswamy distribution . as a result , we propose novel non-linear activation function which we refer to as kumaraswamy unit which is closely related to relu . in the experimental study with mnist image corpora we evaluate the kumaraswamy unit applied to single-layer ( shallow ) neural network and report a significant drop in test classification error and test cross-entropy in comparison to sigmoid unit , relu and noisy relu .", "topics": ["nonlinear system", "speech recognition"]}
{"title": "dart : dropouts meet multiple additive regression trees", "abstract": "multiple additive regression trees ( mart ) , an ensemble model of boosted regression trees , is known to deliver high prediction accuracy for diverse tasks , and it is widely used in practice . however , it suffers an issue which we call over-specialization , wherein trees added at later iterations tend to impact the prediction of only a few instances , and make negligible contribution towards the remaining instances . this negatively affects the performance of the model on unseen data , and also makes the model over-sensitive to the contributions of the few , initially added tress . we show that the commonly used tool to address this issue , that of shrinkage , alleviates the problem only to a certain extent and the fundamental issue of over-specialization still remains . in this work , we explore a different approach to address the problem that of employing dropouts , a tool that has been recently proposed in the context of learning deep neural networks . we propose a novel way of employing dropouts in mart , resulting in the dart algorithm . we evaluate dart on ranking , regression and classification tasks , using large scale , publicly available datasets , and show that dart outperforms mart in each of the tasks , with a significant margin . we also show that dart overcomes the issue of over-specialization to a considerable extent .", "topics": ["statistical classification", "iteration"]}
{"title": "deep watershed transform for instance segmentation", "abstract": "most contemporary approaches to instance segmentation use complex pipelines involving conditional random fields , recurrent neural networks , object proposals , or template matching schemes . in our paper , we present a simple yet powerful end-to-end convolutional neural network to tackle this task . our approach combines intuitions from the classical watershed transform and modern deep learning to produce an energy map of the image where object instances are unambiguously represented as basins in the energy map . we then perform a cut at a single energy level to directly yield connected components corresponding to object instances . our model more than doubles the performance of the state-of-the-art on the challenging cityscapes instance level segmentation task .", "topics": ["recurrent neural network", "end-to-end principle"]}
{"title": "bandit algorithms for tree search", "abstract": "bandit based methods for tree search have recently gained popularity when applied to huge trees , e.g . in the game of go ( gelly et al . , 2006 ) . the uct algorithm ( kocsis and szepesvari , 2006 ) , a tree search method based on upper confidence bounds ( ucb ) ( auer et al . , 2002 ) , is believed to adapt locally to the effective smoothness of the tree . however , we show that uct is too `` optimistic '' in some cases , leading to a regret o ( exp ( exp ( d ) ) ) where d is the depth of the tree . we propose alternative bandit algorithms for tree search . first , a modification of uct using a confidence sequence that scales exponentially with the horizon depth is proven to have a regret o ( 2^d \\sqrt { n } ) , but does not adapt to possible smoothness in the tree . we then analyze flat-ucb performed on the leaves and provide a finite regret bound with high probability . then , we introduce a ucb-based bandit algorithm for smooth trees which takes into account actual smoothness of the rewards for performing efficient `` cuts '' of sub-optimal branches with high confidence . finally , we present an incremental tree search version which applies when the full tree is too big ( possibly infinite ) to be entirely represented and show that with high probability , essentially only the optimal branches is indefinitely developed . we illustrate these methods on a global optimization problem of a lipschitz function , given noisy data .", "topics": ["regret ( decision theory )", "optimization problem"]}
{"title": "no spurious local minima in nonconvex low rank problems : a unified geometric analysis", "abstract": "in this paper we develop a new framework that captures the common landscape underlying the common non-convex low-rank matrix problems including matrix sensing , matrix completion and robust pca . in particular , we show for all above problems ( including asymmetric cases ) : 1 ) all local minima are also globally optimal ; 2 ) no high-order saddle points exists . these results explain why simple algorithms such as stochastic gradient descent have global converge , and efficiently optimize these non-convex objective functions in practice . our framework connects and simplifies the existing analyses on optimization landscapes for matrix sensing and symmetric matrix completion . the framework naturally leads to new results for asymmetric matrix completion and robust pca .", "topics": ["gradient descent", "gradient"]}
{"title": "compact shape trees : a contribution to the forest of shape correspondences and matching methods", "abstract": "we propose a novel technique , termed compact shape trees , for computing correspondences of single-boundary 2-d shapes in o ( n2 ) time . together with zero or more features defined at each of n sample points on the shape 's boundary , the compact shape tree of a shape comprises the o ( n ) collection of vectors emanating from any of the sample points on the shape 's boundary to the rest of the sample points on the boundary . as it turns out , compact shape trees have a number of elegant properties both in the spatial and frequency domains . in particular , via a simple vector-algebraic argument , we show that the o ( n ) collection of vectors in a compact shape tree possesses at least the same discriminatory power as the o ( n2 ) collection of lines emanating from each sample point to every other sample point on a shape 's boundary . in addition , we describe neat approaches for achieving scale and rotation invariance with compact shape trees in the spatial domain ; by viewing compact shape trees as aperiodic discrete signals , we also prove scale and rotation invariance properties for them in the fourier domain . towards these , along the way , using concepts from differential geometry and the calculus , we propose a novel theory for sampling 2-d shape boundaries in a scale and rotation invariant manner . finally , we propose a number of shape recognition experiments to test the efficacy of our concept .", "topics": ["sampling ( signal processing )"]}
{"title": "nested hierarchical dirichlet processes for multi-level non-parametric admixture modeling", "abstract": "dirichlet process ( dp ) is a bayesian non-parametric prior for infinite mixture modeling , where the number of mixture components grows with the number of data items . the hierarchical dirichlet process ( hdp ) , is an extension of dp for grouped data , often used for non-parametric topic modeling , where each group is a mixture over shared mixture densities . the nested dirichlet process ( ndp ) , on the other hand , is an extension of the dp for learning group level distributions from data , simultaneously clustering the groups . it allows group level distributions to be shared across groups in a non-parametric setting , leading to a non-parametric mixture of mixtures . the ncrf extends the ndp for multilevel non-parametric mixture modeling , enabling modeling topic hierarchies . however , the ndp and ncrf do not allow sharing of distributions as required in many applications , motivating the need for multi-level non-parametric admixture modeling . we address this gap by proposing multi-level nested hdps ( nhdp ) where the base distribution of the hdp is itself a hdp at each level thereby leading to admixtures of admixtures at each level . because of couplings between various hdp levels , scaling up is naturally a challenge during inference . we propose a multi-level nested chinese restaurant franchise ( ncrf ) representation for the nested hdp , with which we outline an inference algorithm based on gibbs sampling . we evaluate our model with the two level nhdp for non-parametric entity topic modeling where an inner hdp creates a countably infinite set of topic mixtures and associates them with author entities , while an outer hdp associates documents with these author entities . in our experiments on two real world research corpora , the nhdp is able to generalize significantly better than existing models and detect missing author entities with a reasonable level of accuracy .", "topics": ["cluster analysis", "entity"]}
{"title": "learning representations of affect from speech", "abstract": "there has been a lot of prior work on representation learning for speech recognition applications , but not much emphasis has been given to an investigation of effective representations of affect from speech , where the paralinguistic elements of speech are separated out from the verbal content . in this paper , we explore denoising autoencoders for learning paralinguistic attributes i.e . categorical and dimensional affective traits from speech . we show that the representations learnt by the bottleneck layer of the autoencoder are highly discriminative of activation intensity and at separating out negative valence ( sadness and anger ) from positive valence ( happiness ) . we experiment with different input speech features ( such as fft and log-mel spectrograms with temporal context windows ) , and different autoencoder architectures ( such as stacked and deep autoencoders ) . we also learn utterance specific representations by a combination of denoising autoencoders and blstm based recurrent autoencoders . emotion classification is performed with the learnt temporal/dynamic representations to evaluate the quality of the representations . experiments on a well-established real-life speech dataset ( iemocap ) show that the learnt representations are comparable to state of the art feature extractors ( such as voice quality features and mfccs ) and are competitive with state-of-the-art approaches at emotion and dimensional affect recognition .", "topics": ["feature learning", "noise reduction"]}
{"title": "approximate maximum a posteriori inference with entropic priors", "abstract": "in certain applications it is useful to fit multinomial distributions to observed data with a penalty term that encourages sparsity . for example , in probabilistic latent audio source decomposition one may wish to encode the assumption that only a few latent sources are active at any given time . the standard heuristic of applying an l1 penalty is not an option when fitting the parameters to a multinomial distribution , which are constrained to sum to 1 . an alternative is to use a penalty term that encourages low-entropy solutions , which corresponds to maximum a posteriori ( map ) parameter estimation with an entropic prior . the lack of conjugacy between the entropic prior and the multinomial distribution complicates this approach . in this report i propose a simple iterative algorithm for map estimation of multinomial distributions with sparsity-inducing entropic priors .", "topics": ["sparse matrix", "heuristic"]}
{"title": "temporally coherent bayesian models for entity discovery in videos by tracklet clustering", "abstract": "a video can be represented as a sequence of tracklets , each spanning 10-20 frames , and associated with one entity ( eg . a person ) . the task of \\emph { entity discovery } in videos can be naturally posed as tracklet clustering . we approach this task by leveraging \\emph { temporal coherence } ( tc ) : the fundamental property of videos that each tracklet is likely to be associated with the same entity as its temporal neighbors . our major contributions are the first bayesian nonparametric models for tc at tracklet-level . we extend chinese restaurant process ( crp ) to propose tc-crp , and further to temporally coherent chinese restaurant franchise ( tc-crf ) to jointly model short temporal segments . on the task of discovering persons in tv serial videos without meta-data like scripts , these methods show considerable improvement in cluster purity and person coverage compared to state-of-the-art approaches to tracklet clustering . we represent entities with mixture components , and tracklets with vectors of very generic features , which can work for any type of entity ( not necessarily person ) . the proposed methods can perform online tracklet clustering on streaming videos with little performance deterioration unlike existing approaches , and can automatically reject tracklets resulting from false detections . finally we discuss entity-driven video summarization- where some temporal segments of the video are selected automatically based on the discovered entities .", "topics": ["cluster analysis", "computer vision"]}
{"title": "persistent evidence of local image properties in generic convnets", "abstract": "supervised training of a convolutional network for object classification should make explicit any information related to the class of objects and disregard any auxiliary information associated with the capture of the image or the variation within the object class . does this happen in practice ? although this seems to pertain to the very final layers in the network , if we look at earlier layers we find that this is not the case . surprisingly , strong spatial information is implicit . this paper addresses this , in particular , exploiting the image representation at the first fully connected layer , i.e . the global image descriptor which has been recently shown to be most effective in a range of visual recognition tasks . we empirically demonstrate evidences for the finding in the contexts of four different tasks : 2d landmark detection , 2d object keypoints prediction , estimation of the rgb values of input image , and recovery of semantic label of each pixel . we base our investigation on a simple framework with ridge rigression commonly across these tasks , and show results which all support our insight . such spatial information can be used for computing correspondence of landmarks to a good accuracy , but should potentially be useful for improving the training of the convolutional nets for classification purposes .", "topics": ["pixel"]}
{"title": "subspace approximation for approximate nearest neighbor search in nlp", "abstract": "most natural language processing tasks can be formulated as the approximated nearest neighbor search problem , such as word analogy , document similarity , machine translation . take the question-answering task as an example , given a question as the query , the goal is to search its nearest neighbor in the training dataset as the answer . however , existing methods for approximate nearest neighbor search problem may not perform well owing to the following practical challenges : 1 ) there are noise in the data ; 2 ) the large scale dataset yields a huge retrieval space and high search time complexity . in order to solve these problems , we propose a novel approximate nearest neighbor search framework which i ) projects the data to a subspace based spectral analysis which eliminates the influence of noise ; ii ) partitions the training dataset to different groups in order to reduce the search space . specifically , the retrieval space is reduced from $ o ( n ) $ to $ o ( \\log n ) $ ( where $ n $ is the number of data points in the training dataset ) . we prove that the retrieved nearest neighbor in the projected subspace is the same as the one in the original feature space . we demonstrate the outstanding performance of our framework on real-world natural language processing tasks .", "topics": ["approximation algorithm", "feature vector"]}
{"title": "utility decomposition with deep corrections for scalable planning under uncertainty", "abstract": "decomposition methods have been proposed in the past to approximate solutions to large sequential decision making problems . in contexts where an agent interacts with multiple entities , utility decomposition can be used where each individual entity is considered independently . the individual utility functions are then combined in real time to solve the global problem . although these techniques can perform well empirically , they sacrifice optimality . this paper proposes an approach inspired from multi-fidelity optimization to learn a correction term with a neural network representation . learning this correction can significantly improve performance . we demonstrate this approach on a pedestrian avoidance problem for autonomous driving . by leveraging strategies to avoid a single pedestrian , the decomposition method can scale to avoid multiple pedestrians . we verify empirically that the proposed correction method leads to a significant improvement over the decomposition method alone and outperforms a policy trained on the full scale problem without utility decomposition .", "topics": ["approximation algorithm", "entity"]}
{"title": "learning mixtures of product distributions via higher multilinear moments", "abstract": "learning mixtures of $ k $ binary product distributions is a central problem in computational learning theory , but one where there are wide gaps between the best known algorithms and lower bounds ( even for restricted families of algorithms ) . we narrow many of these gaps by developing novel insights about how to reason about higher order multilinear moments . our results include : 1 ) an $ n^ { o ( k^2 ) } $ time algorithm for learning mixtures of binary product distributions , giving the first improvement on the $ n^ { o ( k^3 ) } $ time algorithm of feldman , o'donnell and servedio 2 ) an $ n^ { \\omega ( \\sqrt { k } ) } $ statistical query lower bound , improving on the $ n^ { \\omega ( \\log k ) } $ lower bound that is based on connections to sparse parity with noise 3 ) an $ n^ { o ( \\log k ) } $ time algorithm for learning mixtures of $ k $ subcubes . this special case can still simulate many other hard learning problems , but is much richer than any of them alone . as a corollary , we obtain more flexible algorithms for learning decision trees under the uniform distribution , that work with stochastic transitions , when we are only given positive examples and with a polylogarithmic number of samples for any fixed $ k $ . our algorithms are based on a win-win analysis where we either build a basis for the moments or locate a degeneracy that can be used to simplify the problem , which we believe will have applications to other learning problems over discrete domains .", "topics": ["sparse matrix", "simulation"]}
{"title": "`` why should i trust you ? `` : explaining the predictions of any classifier", "abstract": "despite widespread adoption , machine learning models remain mostly black boxes . understanding the reasons behind predictions is , however , quite important in assessing trust , which is fundamental if one plans to take action based on a prediction , or when choosing whether to deploy a new model . such understanding also provides insights into the model , which can be used to transform an untrustworthy model or prediction into a trustworthy one . in this work , we propose lime , a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner , by learning an interpretable model locally around the prediction . we also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way , framing the task as a submodular optimization problem . we demonstrate the flexibility of these methods by explaining different models for text ( e.g . random forests ) and image classification ( e.g . neural networks ) . we show the utility of explanations via novel experiments , both simulated and with human subjects , on various scenarios that require trust : deciding if one should trust a prediction , choosing between models , improving an untrustworthy classifier , and identifying why a classifier should not be trusted .", "topics": ["optimization problem", "computer vision"]}
{"title": "synthesized classifiers for zero-shot learning", "abstract": "given semantic descriptions of object classes , zero-shot learning aims to accurately recognize objects of the unseen classes , from which no examples are available at the training stage , by associating them to the seen classes , from which labeled examples are provided . we propose to tackle this problem from the perspective of manifold learning . our main idea is to align the semantic space that is derived from external information to the model space that concerns itself with recognizing visual features . to this end , we introduce a set of `` phantom '' object classes whose coordinates live in both the semantic space and the model space . serving as bases in a dictionary , they can be optimized from labeled data such that the synthesized real object classifiers achieve optimal discriminative performance . we demonstrate superior accuracy of our approach over the state of the art on four benchmark datasets for zero-shot learning , including the full imagenet fall 2011 dataset with more than 20,000 unseen classes .", "topics": ["dictionary"]}
{"title": "study of the influence of the number normalization scheme used in two chaotic pseudo random number generators used as the source of randomness in differential evolution", "abstract": "in many publications , authors showed that chaotic pseudo random number generators ( prngs ) may improve performance of the evolutionary algorithms . in this paper , we use two chaotic maps gingerbread man and tinkerbell as the chaotic prngs instead of the classical prng in the differential evolution . numbers generated by this maps are normalized to the unit interval by three different methods -- operation modulo , straightforward number normalization where we know minimal and maximal generated number and arctangent of the two variables $ x $ and $ y $ , where numbers $ x $ and $ y $ are generated by the gingerbread man map and tinkerbell map . the first goal of this paper is to show whether the differential evolution convergence speed might be affected by the way how we normalize number generated by the chaotic map . the second goal is to find out the influence of the probability distribution function of the selected chaotic prngs . the results mentioned below showed that the selected normalization method may improve differential evolution convergence speed , especially in the case of arctangent and straightforward number normalization , where we know the minimal and maximal generated numbers .", "topics": ["map"]}
{"title": "variational deep q network", "abstract": "we propose a framework that directly tackles the probability distribution of the value function parameters in deep q network ( dqn ) , with powerful variational inference subroutines to approximate the posterior of the parameters . we will establish the equivalence between our proposed surrogate objective and variational inference loss . our new algorithm achieves efficient exploration and performs well on large scale chain markov decision process ( mdp ) .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "adversarially learned inference", "abstract": "we introduce the adversarially learned inference ( ali ) model , which jointly learns a generation network and an inference network using an adversarial process . the generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables . an adversarial game is cast between these two networks and a discriminative network is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network . we illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with state-of-the-art on the semi-supervised svhn and cifar10 tasks .", "topics": ["bayesian network", "map"]}
{"title": "adaptive texture energy measure method", "abstract": "recent developments in image quality , data storage , and computational capacity have heightened the need for texture analysis in image process . to date various methods have been developed and introduced for assessing textures in images . one of the most popular texture analysis methods is the texture energy measure ( tem ) and it has been used for detecting edges , levels , waves , spots and ripples by employing predefined tem masks to images . despite several success- ful studies , tem has a number of serious weaknesses in use . the major drawback is ; the masks are predefined therefore they can not be adapted to image . a new method , adaptive texture energy measure method ( atem ) , was offered to over- come this disadvantage of tem by using adaptive masks by adjusting the contrast , sharpening and orientation angle of the mask . to assess the applicability of atem , it is compared with tem . the accuracy of the classification of butterfly , flower seed and brodatz datasets are 0.08 , 0.3292 and 0.3343 , respectively by tem and 0.0053 , 0.2417 and 0.3153 , respectively by atem . the results of this study indicate that atem is a successful method for texture analysis .", "topics": ["numerical analysis"]}
{"title": "latent bayesian melding for integrating individual and population models", "abstract": "in many statistical problems , a more coarse-grained model may be suitable for population-level behaviour , whereas a more detailed model is appropriate for accurate modelling of individual behaviour . this raises the question of how to integrate both types of models . methods such as posterior regularization follow the idea of generalized moment matching , in that they allow matching expectations between two models , but sometimes both models are most conveniently expressed as latent variable models . we propose latent bayesian melding , which is motivated by averaging the distributions over populations statistics of both the individual-level and the population-level models under a logarithmic opinion pool framework . in a case study on electricity disaggregation , which is a type of single-channel blind source separation problem , we show that latent bayesian melding leads to significantly more accurate predictions than an approach based solely on generalized moment matching .", "topics": ["matrix regularization"]}
{"title": "retrieval and clustering from a 3d human database based on body and head shape", "abstract": "in this paper , we describe a framework for similarity based retrieval and clustering from a 3d human database . our technique is based on both body and head shape representation and the retrieval is based on similarity of both of them . the 3d human database used in our study is the caesar anthropometric database which contains approximately 5000 bodies . we have developed a web-based interface for specifying the queries to interact with the retrieval system . our approach performs the similarity based retrieval in a reasonable amount of time and is a practical approach .", "topics": ["cluster analysis"]}
{"title": "image segmentation by iterative inference from conditional score estimation", "abstract": "inspired by the combination of feedforward and iterative computations in the virtual cortex , and taking advantage of the ability of denoising autoencoders to estimate the score of a joint distribution , we propose a novel approach to iterative inference for capturing and exploiting the complex joint distribution of output variables conditioned on some input variables . this approach is applied to image pixel-wise segmentation , with the estimated conditional score used to perform gradient ascent towards a mode of the estimated conditional distribution . this extends previous work on score estimation by denoising autoencoders to the case of a conditional distribution , with a novel use of a corrupted feedforward predictor replacing gaussian corruption . an advantage of this approach over more classical ways to perform iterative inference for structured outputs , like conditional random fields ( crfs ) , is that it is not any more necessary to define an explicit energy function linking the output variables . to keep computations tractable , such energy function parametrizations are typically fairly constrained , involving only a few neighbors of each of the output variables in each clique . we experimentally find that the proposed iterative inference from conditional score estimation by conditional denoising autoencoders performs better than comparable models based on crfs or those not using any explicit modeling of the conditional joint distribution of outputs .", "topics": ["mathematical optimization", "image segmentation"]}
{"title": "phase transition of tractability in constraint satisfaction and bayesian network inference", "abstract": "there has been great interest in identifying tractable subclasses of np complete problems and designing efficient algorithms for these tractable classes . constraint satisfaction and bayesian network inference are two examples of such problems that are of great importance in ai and algorithms . in this paper we study , under the frameworks of random constraint satisfaction problems and random bayesian networks , a typical tractable subclass characterized by the treewidth of the problems . we show that the property of having a bounded treewidth for csps and bayesian network inference problem has a phase transition that occurs while the underlying structures of problems are still sparse . this implies that algorithms making use of treewidth based structural knowledge only work efficiently in a limited range of random instance .", "topics": ["bayesian network", "sparse matrix"]}
{"title": "synthetic and natural noise both break neural machine translation", "abstract": "character-based neural machine translation ( nmt ) models alleviate out-of-vocabulary issues , learn morphology , and move us closer to completely end-to-end translation systems . unfortunately , they are also very brittle and easily falter when presented with noisy data . in this paper , we confront nmt models with synthetic and natural sources of noise . we find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending . we explore two approaches to increase model robustness : structure-invariant word representations and robust training on noisy texts . we find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise .", "topics": ["machine translation"]}
{"title": "performance analysis of aim-k-means & k-means in quality cluster generation", "abstract": "among all the partition based clustering algorithms k-means is the most popular and well known method . it generally shows impressive results even in considerably large data sets . the computational complexity of k-means does not suffer from the size of the data set . the main disadvantage faced in performing this clustering is that the selection of initial means . if the user does not have adequate knowledge about the data set , it may lead to erroneous results . the algorithm automatic initialization of means ( aim ) , which is an extension to k-means , has been proposed to overcome the problem of initial mean generation . in this paper an attempt has been made to compare the performance of the algorithms through implementation", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "poisoning attacks against support vector machines", "abstract": "we investigate a family of poisoning attacks against support vector machines ( svm ) . such attacks inject specially crafted training data that increases the svm 's test error . central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution . however , this assumption does not generally hold in security-sensitive settings . as we demonstrate , an intelligent adversary can , to some extent , predict the change of the svm 's decision function due to malicious input and use this ability to construct malicious data . the proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the svm 's optimal solution . this method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels . we experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface , which significantly increases the classifier 's test error .", "topics": ["test set", "support vector machine"]}
{"title": "required sample size for learning sparse bayesian networks with many variables", "abstract": "learning joint probability distributions on n random variables requires exponential sample size in the generic case . here we consider the case that a temporal ( or causal ) order of the variables is known and that the ( unknown ) graph of causal dependencies has bounded in-degree delta . then the joint measure is uniquely determined by the probabilities of all ( 2 delta+1 ) -tuples . upper bounds on the sample size required for estimating their probabilities can be given in terms of the vc-dimension of the set of corresponding cylinder sets . the sample size grows less than linearly with n .", "topics": ["time complexity", "sparse matrix"]}
{"title": "bayesian models of data streams with hierarchical power priors", "abstract": "making inferences from data streams is a pervasive problem in many modern data analysis applications . but it requires to address the problem of continuous model updating and adapt to changes or drifts in the underlying data generating distribution . in this paper , we approach these problems from a bayesian perspective covering general conjugate exponential models . our proposal makes use of non-conjugate hierarchical priors to explicitly model temporal changes of the model parameters . we also derive a novel variational inference scheme which overcomes the use of non-conjugate priors while maintaining the computational efficiency of variational methods over conjugate models . the approach is validated on three real data sets over three latent variable models .", "topics": ["calculus of variations"]}
{"title": "a computational model of syntactic processing : ambiguity resolution from interpretation", "abstract": "syntactic ambiguity abounds in natural language , yet humans have no difficulty coping with it . in fact , the process of ambiguity resolution is almost always unconscious . but it is not infallible , however , as example 1 demonstrates . 1 . the horse raced past the barn fell . this sentence is perfectly grammatical , as is evident when it appears in the following context : 2 . two horses were being shown off to a prospective buyer . one was raced past a meadow . and the other was raced past a barn . ... grammatical yet unprocessable sentences such as 1 are called `garden-path sentences . ' their existence provides an opportunity to investigate the human sentence processing mechanism by studying how and when it fails . the aim of this thesis is to construct a computational model of language understanding which can predict processing difficulty . the data to be modeled are known examples of garden path and non-garden path sentences , and other results from psycholinguistics . it is widely believed that there are two distinct loci of computation in sentence processing : syntactic parsing and semantic interpretation . one longstanding controversy is which of these two modules bears responsibility for the immediate resolution of ambiguity . my claim is that it is the latter , and that the syntactic processing module is a very simple device which blindly and faithfully constructs all possible analyses for the sentence up to the current point of processing . the interpretive module serves as a filter , occasionally discarding certain of these analyses which it deems less appropriate for the ongoing discourse than their competitors . this document is divided into three parts . the first is introductory , and reviews a selection of proposals from the sentence processing literature . the second part explores a body of data which has been adduced in support of a theory of structural preferences -- - one that is inconsistent with the present claim . i show how the current proposal can be specified to account for the available data , and moreover to predict where structural preference theories will go wrong . the third part is a theoretical investigation of how well the proposed architecture can be realized using current conceptions of linguistic competence . in it , i present a parsing algorithm and a meaning-based ambiguity resolution method .", "topics": ["natural language", "parsing"]}
{"title": "image denoising via cnns : an adversarial approach", "abstract": "is it possible to recover an image from its noisy version using convolutional neural networks ? this is an interesting problem as convolutional layers are generally used as feature detectors for tasks like classification , segmentation and object detection . we present a new cnn architecture for blind image denoising which synergically combines three architecture components , a multi-scale feature extraction layer which helps in reducing the effect of noise on feature maps , an l_p regularizer which helps in selecting only the appropriate feature maps for the task of reconstruction , and finally a three step training approach which leverages adversarial training to give the final performance boost to the model . the proposed model shows competitive denoising performance when compared to the state-of-the-art approaches .", "topics": ["feature extraction", "statistical classification"]}
{"title": "collaborative learning for weakly supervised object detection", "abstract": "weakly supervised object detection has recently received much attention , since it only requires image-level labels instead of the bounding-box labels consumed in strongly supervised learning . nevertheless , the save in labeling expense is usually at the cost of model accuracy . in this paper , we propose a simple but effective weakly supervised collaborative learning framework to resolve this problem , which trains a weakly supervised learner and a strongly supervised learner jointly by enforcing partial feature sharing and prediction consistency . for object detection , taking wsddn-like architecture as weakly supervised detector sub-network and faster-rcnn-like architecture as strongly supervised detector sub-network , we propose an end-to-end weakly supervised collaborative detection network . as there is no strong supervision available to train the faster-rcnn-like sub-network , a new prediction consistency loss is defined to enforce consistency of predictions between the two sub-networks as well as within the faster-rcnn-like sub-networks . at the same time , the two detectors are designed to partially share features to further guarantee the model consistency at perceptual level . extensive experiments on pascal voc 2007 and 2012 data sets have demonstrated the effectiveness of the proposed framework .", "topics": ["supervised learning", "object detection"]}
{"title": "large pose 3d face reconstruction from a single image via direct volumetric cnn regression", "abstract": "3d face reconstruction is a fundamental computer vision problem of extraordinary difficulty . current systems often assume the availability of multiple facial images ( sometimes from the same subject ) as input , and must address a number of methodological challenges such as establishing dense correspondences across large facial poses , expressions , and non-uniform illumination . in general these methods require complex and inefficient pipelines for model building and fitting . in this work , we propose to address many of these limitations by training a convolutional neural network ( cnn ) on an appropriate dataset consisting of 2d images and 3d facial models or scans . our cnn works with just a single 2d facial image , does not require accurate alignment nor establishes dense correspondence between images , works for arbitrary facial poses and expressions , and can be used to reconstruct the whole 3d facial geometry ( including the non-visible parts of the face ) bypassing the construction ( during training ) and fitting ( during testing ) of a 3d morphable model . we achieve this via a simple cnn architecture that performs direct regression of a volumetric representation of the 3d facial geometry from a single 2d image . we also demonstrate how the related task of facial landmark localization can be incorporated into the proposed framework and help improve reconstruction quality , especially for the cases of large poses and facial expressions . testing code will be made available online , along with pre-trained models http : //aaronsplace.co.uk/papers/jackson2017recon", "topics": ["computer vision"]}
{"title": "deep learning with nonparametric clustering", "abstract": "clustering is an essential problem in machine learning and data mining . one vital factor that impacts clustering performance is how to learn or design the data representation ( or features ) . fortunately , recent advances in deep learning can learn unsupervised features effectively , and have yielded state of the art performance in many classification problems , such as character recognition , object recognition and document categorization . however , little attention has been paid to the potential of deep learning for unsupervised clustering problems . in this paper , we propose a deep belief network with nonparametric clustering . as an unsupervised method , our model first leverages the advantages of deep learning for feature representation and dimension reduction . then , it performs nonparametric clustering under a maximum margin framework -- a discriminative clustering model and can be trained online efficiently in the code space . lastly model parameters are refined in the deep belief network . thus , this model can learn features for clustering and infer model complexity in an unified framework . the experimental results show the advantage of our approach over competitive baselines .", "topics": ["baseline ( configuration management )", "data mining"]}
{"title": "interactive generative adversarial networks for facial expression generation in dyadic interactions", "abstract": "a social interaction is a social exchange between two or more individuals , where individuals modify and adjust their behaviors in response to their interaction partners . our social interactions are one of most fundamental aspects of our lives and can profoundly affect our mood , both positively and negatively . with growing interest in virtual reality and avatar-mediated interactions , it is desirable to make these interactions natural and human like to promote positive effect in the interactions and applications such as intelligent tutoring systems , automated interview systems and e-learning . in this paper , we propose a method to generate facial behaviors for an agent . these behaviors include facial expressions and head pose and they are generated considering the users affective state . our models learn semantically meaningful representations of the face and generate appropriate and temporally smooth facial behaviors in dyadic interactions .", "topics": ["interaction"]}
{"title": "hybrid bayesian networks with linear deterministic variables", "abstract": "when a hybrid bayesian network has conditionally deterministic variables with continuous parents , the joint density function for the continuous variables does not exist . conditional linear gaussian distributions can handle such cases when the continuous variables have a multi-variate normal distribution and the discrete variables do not have continuous parents . in this paper , operations required for performing inference with conditionally deterministic variables in hybrid bayesian networks are developed . these methods allow inference in networks with deterministic variables where continuous variables may be non-gaussian , and their density functions can be approximated by mixtures of truncated exponentials . there are no constraints on the placement of continuous and discrete nodes in the network .", "topics": ["bayesian network"]}
{"title": "dynanewton - accelerating newton 's method for machine learning", "abstract": "newton 's method is a fundamental technique in optimization with quadratic convergence within a neighborhood around the optimum . however reaching this neighborhood is often slow and dominates the computational costs . we exploit two properties specific to empirical risk minimization problems to accelerate newton 's method , namely , subsampling training data and increasing strong convexity through regularization . we propose a novel continuation method , where we define a family of objectives over increasing sample sizes and with decreasing regularization strength . solutions on this path are tracked such that the minimizer of the previous objective is guaranteed to be within the quadratic convergence region of the next objective to be optimized . thereby every newton iteration is guaranteed to achieve super-linear contractions with regard to the chosen objective , which becomes a moving target . we provide a theoretical analysis that motivates our algorithm , called dynanewton , and characterizes its speed of convergence . experiments on a wide range of data sets and problems consistently confirm the predicted computational savings .", "topics": ["test set", "mathematical optimization"]}
{"title": "penalized k-nearest-neighbor-graph based metrics for clustering", "abstract": "a difficult problem in clustering is how to handle data with a manifold structure , i.e . data that is not shaped in the form of compact clouds of points , forming arbitrary shapes or paths embedded in a high-dimensional space . in this work we introduce the penalized k-nearest-neighbor-graph ( pknng ) based metric , a new tool for evaluating distances in such cases . the new metric can be used in combination with most clustering algorithms . the pknng metric is based on a two-step procedure : first it constructs the k-nearest-neighbor-graph of the dataset of interest using a low k-value and then it adds edges with an exponentially penalized weight for connecting the sub-graphs produced by the first step . we discuss several possible schemes for connecting the different sub-graphs . we use three artificial datasets in four different embedding situations to evaluate the behavior of the new metric , including a comparison among different clustering methods . we also evaluate the new metric in a real world application , clustering the mnist digits dataset . in all cases the pknng metric shows promising clustering results .", "topics": ["cluster analysis", "mnist database"]}
{"title": "improving speech related facial action unit recognition by audiovisual information fusion", "abstract": "it is challenging to recognize facial action unit ( au ) from spontaneous facial displays , especially when they are accompanied by speech . the major reason is that the information is extracted from a single source , i.e . , the visual channel , in the current practice . however , facial activity is highly correlated with voice in natural human communications . instead of solely improving visual observations , this paper presents a novel audiovisual fusion framework , which makes the best use of visual and acoustic cues in recognizing speech-related facial aus . in particular , a dynamic bayesian network ( dbn ) is employed to explicitly model the semantic and dynamic physiological relationships between aus and phonemes as well as measurement uncertainty . a pilot audiovisual au-coded database has been collected to evaluate the proposed framework , which consists of a `` clean '' subset containing frontal faces under well controlled circumstances and a challenging subset with large head movements and occlusions . experiments on this database have demonstrated that the proposed framework yields significant improvement in recognizing speech-related aus compared to the state-of-the-art visual-based methods especially for those aus whose visual observations are impaired during speech , and more importantly also outperforms feature-level fusion methods by explicitly modeling and exploiting physiological relationships between aus and phonemes .", "topics": ["bayesian network"]}
{"title": "multi-label mrf optimization via least squares s-t cuts", "abstract": "there are many applications of graph cuts in computer vision , e.g . segmentation . we present a novel method to reformulate the np-hard , k-way graph partitioning problem as an approximate minimal s-t graph cut problem , for which a globally optimal solution is found in polynomial time . each non-terminal vertex in the original graph is replaced by a set of ceil ( log_2 ( k ) ) new vertices . the original graph edges are replaced by new edges connecting the new vertices to each other and to only two , source s and sink t , terminal nodes . the weights of the new edges are obtained using a novel least squares solution approximating the constraints of the initial k-way setup . the minimal s-t cut labels each new vertex with a binary ( s vs t ) `` gray '' encoding , which is then decoded into a decimal label number that assigns each of the original vertices to one of k classes . we analyze the properties of the approximation and present quantitative as well as qualitative segmentation results .", "topics": ["approximation algorithm", "optimization problem"]}
{"title": "amidst : a java toolbox for scalable probabilistic machine learning", "abstract": "the amidst toolbox is a software for scalable probabilistic machine learning with a spe- cial focus on ( massive ) streaming data . the toolbox supports a flexible modeling language based on probabilistic graphical models with latent variables and temporal dependencies . the specified models can be learnt from large data sets using parallel or distributed implementa- tions of bayesian learning algorithms for either streaming or batch data . these algorithms are based on a flexible variational message passing scheme , which supports discrete and continu- ous variables from a wide range of probability distributions . amidst also leverages existing functionality and algorithms by interfacing to software tools such as flink , spark , moa , weka , r and hugin . amidst is an open source toolbox written in java and available at http : //www.amidsttoolbox.com under the apache software license version 2.0 .", "topics": ["graphical model", "calculus of variations"]}
{"title": "training spiking neural networks for cognitive tasks : a versatile framework compatible to various temporal codes", "abstract": "conventional modeling approaches have found limitations in matching the increasingly detailed neural network structures and dynamics recorded in experiments to the diverse brain functionalities . on another approach , studies have demonstrated to train spiking neural networks for simple functions using supervised learning . here , we introduce a modified spikeprop learning algorithm , which achieved better learning stability in different activity states . in addition , we show biological realistic features such as lateral connections and sparse activities can be included in the network . we demonstrate the versatility of this framework by implementing three well-known temporal codes for different types of cognitive tasks , which are mnist digits recognition , spatial coordinate transformation , and motor sequence generation . moreover , we find several characteristic features have evolved alongside the task training , such as selective activity , excitatory-inhibitory balance , and weak pair-wise correlation . the coincidence between the self-evolved and experimentally observed features indicates their importance on the brain functionality . our results suggest a unified setting in which diverse cognitive computations and mechanisms can be studied .", "topics": ["supervised learning", "computation"]}
{"title": "mining heterogeneous multivariate time-series for learning meaningful patterns : application to home health telecare", "abstract": "for the last years , time-series mining has become a challenging issue for researchers . an important application lies in most monitoring purposes , which require analyzing large sets of time-series for learning usual patterns . any deviation from this learned profile is then considered as an unexpected situation . moreover , complex applications may involve the temporal study of several heterogeneous parameters . in that paper , we propose a method for mining heterogeneous multivariate time-series for learning meaningful patterns . the proposed approach allows for mixed time-series -- containing both pattern and non-pattern data -- such as for imprecise matches , outliers , stretching and global translating of patterns instances in time . we present the early results of our approach in the context of monitoring the health status of a person at home . the purpose is to build a behavioral profile of a person by analyzing the time variations of several quantitative or qualitative parameters recorded through a provision of sensors installed in the home .", "topics": ["time series", "sensor"]}
{"title": "arco1 : an application of belief networks to the oil market", "abstract": "belief networks are a new , potentially important , class of knowledge-based models . arco1 , currently under development at the atlantic richfield company ( arco ) and the university of southern california ( usc ) , is the most advanced reported implementation of these models in a financial forecasting setting . arco1 's underlying belief network models the variables believed to have an impact on the crude oil market . a pictorial market model-developed on a mac ii- facilitates consensus among the members of the forecasting team . the system forecasts crude oil prices via monte carlo analyses of the network . several different models of the oil market have been developed ; the system 's ability to be updated quickly highlights its flexibility .", "topics": ["bayesian network"]}
{"title": "deep compositional captioning : describing novel object categories without paired training data", "abstract": "while recent deep neural network models have achieved promising results on the image captioning task , they rely largely on the availability of corpora with paired image and sentence captions to describe objects in context . in this work , we propose the deep compositional captioner ( dcc ) to address the task of generating descriptions of novel objects which are not present in paired image-sentence datasets . our method achieves this by leveraging large object recognition datasets and external text corpora and by transferring knowledge between semantically similar concepts . current deep caption models can only describe objects contained in paired image-sentence corpora , despite the fact that they are pre-trained with large object recognition datasets , namely imagenet . in contrast , our model can compose sentences that describe novel objects and their interactions with other objects . we demonstrate our model 's ability to describe novel concepts by empirically evaluating its performance on mscoco and show qualitative results on imagenet images of objects for which no paired image-caption data exist . further , we extend our approach to generate descriptions of objects in video clips . our results show that dcc has distinct advantages over existing image and video captioning approaches for generating descriptions of new objects in context .", "topics": ["interaction", "text corpus"]}
{"title": "over-representation of extreme events in decision-making : a rational metacognitive account", "abstract": "the availability bias , manifested in the over-representation of extreme eventualities in decision-making , is a well-known cognitive bias , and is generally taken as evidence of human irrationality . in this work , we present the first rational , metacognitive account of the availability bias , formally articulated at marr 's algorithmic level of analysis . concretely , we present a normative , metacognitive model of how a cognitive system should over-represent extreme eventualities , depending on the amount of time available at its disposal for decision-making . our model also accounts for two well-known framing effects in human decision-making under risk -- -the fourfold pattern of risk preferences in outcome probability ( tversky & kahneman , 1992 ) and in outcome magnitude ( markovitz , 1952 ) -- -thereby providing the first metacognitively-rational basis for those effects . empirical evidence , furthermore , confirms an important prediction of our model . surprisingly , our model is unimaginably robust with respect to its focal parameter . we discuss the implications of our work for studies on human decision-making , and conclude by presenting a counterintuitive prediction of our model , which , if confirmed , would have intriguing implications for human decision-making under risk . to our knowledge , our model is the first metacognitive , resource-rational process model of cognitive biases in decision-making .", "topics": ["artificial intelligence"]}
{"title": "towards common-sense reasoning via conditional simulation : legacies of turing in artificial intelligence", "abstract": "the problem of replicating the flexibility of human common-sense reasoning has captured the imagination of computer scientists since the early days of alan turing 's foundational work on computation and the philosophy of artificial intelligence . in the intervening years , the idea of cognition as computation has emerged as a fundamental tenet of artificial intelligence ( ai ) and cognitive science . but what kind of computation is cognition ? we describe a computational formalism centered around a probabilistic turing machine called query , which captures the operation of probabilistic conditioning via conditional simulation . through several examples and analyses , we demonstrate how the query abstraction can be used to cast common-sense reasoning as probabilistic inference in a statistical model of our observations and the uncertain structure of the world that generated that experience . this formulation is a recent synthesis of several research programs in ai and cognitive science , but it also represents a surprising convergence of several of turing 's pioneering insights in ai , the foundations of computation , and statistics .", "topics": ["simulation", "computation"]}
{"title": "target curricula via selection of minimum feature sets : a case study in boolean networks", "abstract": "we consider the effect of introducing a curriculum of targets when training boolean models on supervised multi label classification ( mlc ) problems . in particular , we consider how to order targets in the absence of prior knowledge , and how such a curriculum may be enforced when using meta-heuristics to train discrete non-linear models . we show that hierarchical dependencies between targets can be exploited by enforcing an appropriate curriculum using hierarchical loss functions . on several multi output circuit-inference problems with known target difficulties , feedforward boolean networks ( fbns ) trained with such a loss function achieve significantly lower out-of-sample error , up to $ 10\\ % $ in some cases . this improvement increases as the loss places more emphasis on target order and is strongly correlated with an easy-to-hard curricula . we also demonstrate the same improvements on three real-world models and two gene regulatory network ( grn ) inference problems . we posit a simple a-priori method for identifying an appropriate target order and estimating the strength of target relationships in boolean mlcs . these methods use intrinsic dimension as a proxy for target difficulty , which is estimated using optimal solutions to a combinatorial optimisation problem known as the minimum-feature-set ( minfs ) problem . we also demonstrate that the same generalisation gains can be achieved without providing any knowledge of target difficulty .", "topics": ["mathematical optimization", "nonlinear system"]}
{"title": "s-mart : novel tree-based structured learning algorithms applied to tweet entity linking", "abstract": "non-linear models recently receive a lot of attention as people are starting to discover the power of statistical and embedding features . however , tree-based models are seldom studied in the context of structured learning despite their recent success on various classification and ranking tasks . in this paper , we propose s-mart , a tree-based structured learning framework based on multiple additive regression trees . s-mart is especially suitable for handling tasks with dense features , and can be used to learn many different structures under various loss functions . we apply s-mart to the task of tweet entity linking -- - a core component of tweet information extraction , which aims to identify and link name mentions to entities in a knowledge base . a novel inference algorithm is proposed to handle the special structure of the task . the experimental results show that s-mart significantly outperforms state-of-the-art tweet entity linking systems .", "topics": ["entity", "loss function"]}
{"title": "sketch recognition using domain classification", "abstract": "conceptualizing away the sketch processing details in a user interface will enable general users and domain experts to create more complex sketches . there are many domains for which sketch recognition systems are being developed . but they entail image-processing skill if they are to handle the details of each domain , and also they are lengthy to build . the implemented system goal is to enable user interface designers and domain experts who may not have proficiency in sketch recognition to be able to construct these sketch systems . this sketch recognition system takes in rough sketches from user drawn with the help of mouse as its input . it then recognizes the sketch using segmentation and domain classification , the properties of the user drawn sketch and segments are searched heuristically in the domains and each figures of each domain , and finally it shows its domain , the figure name and properties . it also draws the sketch smoothly . the work is resulted through extensive research and study of many existing image processing and pattern matching algorithms .", "topics": ["image processing", "image segmentation"]}
{"title": "managing change in graph-structured data using description logics ( long version with appendix )", "abstract": "in this paper , we consider the setting of graph-structured data that evolves as a result of operations carried out by users or applications . we study different reasoning problems , which range from ensuring the satisfaction of a given set of integrity constraints after a given sequence of updates , to deciding the ( non- ) existence of a sequence of actions that would take the data to an ( un ) desirable state , starting either from a specific data instance or from an incomplete description of it . we consider an action language in which actions are finite sequences of conditional insertions and deletions of nodes and labels , and use description logics for describing integrity constraints and ( partial ) states of the data . we then formalize the above data management problems as a static verification problem and several planning problems . we provide algorithms and tight complexity bounds for the formalized problems , both for an expressive dl and for a variant of dl-lite .", "topics": ["computational complexity theory"]}
{"title": "teaching machines to describe images via natural language feedback", "abstract": "robots will eventually be part of every household . it is thus critical to enable algorithms to learn from and be guided by non-expert users . in this paper , we bring a human in the loop , and enable a human teacher to give feedback to a learning agent in the form of natural language . we argue that a descriptive sentence can provide a much stronger learning signal than a numeric reward in that it can easily point to where the mistakes are and how to correct them . we focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts . we propose a hierarchical phrase-based captioning model trained with policy gradients , and design a feedback network that provides reward to the learner by conditioning on the human-provided feedback . we show that by exploiting descriptive feedback our model learns to perform better than when given independently written human captions .", "topics": ["natural language", "robot"]}
{"title": "identifying harm events in clinical care through medical narratives", "abstract": "preventable medical errors are estimated to be among the leading causes of injury and death in the united states . to prevent such errors , healthcare systems have implemented patient safety and incident reporting systems . these systems enable clinicians to report unsafe conditions and cases where patients have been harmed due to errors in medical care . these reports are narratives in natural language and while they provide detailed information about the situation , it is non-trivial to perform large scale analysis for identifying common causes of errors and harm to the patients . in this work , we present a method based on attentive convolutional and recurrent networks for identifying harm events in patient care and categorize the harm based on its severity level . we demonstrate that our methods can significantly improve the performance over existing methods in identifying harm in clinical care .", "topics": ["natural language"]}
{"title": "partial face detection in the mobile domain", "abstract": "generic face detection algorithms do not perform well in the mobile domain due to significant presence of occluded and partially visible faces . one promising technique to handle the challenge of partial faces is to design face detectors based on facial segments . in this paper two different approaches of facial segment-based face detection are discussed , namely , proposal-based detection and detection by end-to-end regression . methods that follow the first approach rely on generating face proposals that contain facial segment information . the three detectors following this approach , namely facial segment-based face detector ( fsfd ) , segface and deepsegface , discussed in this paper , perform binary classification on each proposal based on features learned from facial segments . the process of proposal generation , however , needs to be handled separately , which can be very time consuming , and is not truly necessary given the nature of the active authentication problem . hence a novel algorithm , deep regression-based user image detector ( druid ) is proposed , which shifts from the classification to the regression paradigm , thus obviating the need for proposal generation . druid has an unique network architecture with customized loss functions , is trained using a relatively small amount of data by utilizing a novel data augmentation scheme and is fast since it outputs the bounding boxes of a face and its segments in a single pass . being robust to occlusion by design , the facial segment-based face detection methods , especially druid show superior performance over other state-of-the-art face detectors in terms of precision-recall and roc curve on two mobile face datasets .", "topics": ["loss function", "end-to-end principle"]}
{"title": "full-frame scene coordinate regression for image-based localization", "abstract": "image-based localization , or camera relocalization , is a fundamental problem in computer vision and robotics , and it refers to estimating camera pose from an image . recent state-of-the-art approaches use learning based methods , such as random forests ( rfs ) and convolutional neural networks ( cnns ) , to regress for each pixel in the image its corresponding position in the scene 's world coordinate frame , and solve the final pose via a ransac-based optimization scheme using the predicted correspondences . in this paper , instead of in a patch-based manner , we propose to perform the scene coordinate regression in a full-frame manner to make the computation efficient at test time and , more importantly , to add more global context to the regression process to improve the robustness . to do so , we adopt a fully convolutional encoder-decoder neural network architecture which accepts a whole image as input and produces scene coordinate predictions for all pixels in the image . however , using more global context is prone to overfitting . to alleviate this issue , we propose to use data augmentation to generate more data for training . in addition to the data augmentation in 2d image space , we also augment the data in 3d space . we evaluate our approach on the publicly available 7-scenes dataset , and experiments show that it has better scene coordinate predictions and achieves state-of-the-art results in localization with improved robustness on the hardest frames ( e.g . , frames with repeated structures ) .", "topics": ["computer vision", "computation"]}
{"title": "perception lie paradox : mathematically proved uncertainty about humans perception similarity", "abstract": "agents ' judgment depends on perception and previous knowledge . assuming that previous knowledge depends on perception , we can say that judgment depends on perception . so , if judgment depends on perception , can agents judge that they have the same perception ? in few words , this is the addressed paradox through this document . while illustrating on the paradox , it 's found that to reach agreement in communication , it 's not necessary for parties to have the same perception however the necessity is to have perception correspondence . the attempted solution to this paradox reveals a potential uncertainty in judging the matter thus supporting the skeptical view of the problem . moreover , relating perception to intelligence , the same uncertainty is inherited by judging the level of intelligence of an agent compared to others not necessarily from the same kind ( e.g . machine intelligence compared to human intelligence ) . using a proposed simple mathematical model for perception and action , a tool is developed to construct scenarios , and the problem is addressed mathematically such that conclusions are drawn systematically based on mathematically defined properties . when it comes to formalization , philosophical arguments and views become more visible and explicit .", "topics": ["artificial intelligence"]}
{"title": "spectral video construction from rgb video : application to image guided neurosurgery", "abstract": "spectral imaging has received enormous interest in the field of medical imaging modalities . it provides a powerful tool for the analysis of different organs and non-invasive tissues . therefore , significant amount of research has been conducted to explore the possibility of using spectral imaging in biomedical applications . to observe spectral image information in real time during surgery and monitor the temporal changes in the organs and tissues is a demanding task . available spectral imaging devices are not sufficient to accomplish this task with an acceptable spatial and spectral resolution . a solution to this problem is to estimate the spectral video from rgb video and perform visualization with the most prominent spectral bands . in this research , we propose a framework to generate neurosurgery spectral video from rgb video . a spectral estimation technique is applied on each rgb video frames . the rgb video is captured using a digital camera connected with an operational microscope dedicated to neurosurgery . a database of neurosurgery spectral images is used to collect training data and evaluate the estimation accuracy . a searching technique is used to identify the best training set . five different spectrum estimation techniques are experimented to indentify the best method . although this framework is established for neurosurgery spectral video generation , however , the methodology outlined here would also be applicable to other similar research .", "topics": ["test set", "database"]}
{"title": "dragnn : a transition-based framework for dynamically connected neural networks", "abstract": "in this work , we present a compact , modular framework for constructing novel recurrent neural architectures . our basic module is a new generic unit , the transition based recurrent unit ( tbru ) . in addition to hidden layer activations , tbrus have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations . by connecting multiple tbrus , we can extend and combine commonly used architectures such as sequence-to-sequence , attention mechanisms , and re-cursive tree-structured models . a tbru can also serve as both an encoder for downstream tasks and as a decoder for its own task simultaneously , resulting in more accurate multi-task learning . we call our approach dynamic recurrent acyclic graphical neural networks , or dragnn . we show that dragnn is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks .", "topics": ["neural networks", "parsing"]}
{"title": "self-delimiting neural networks", "abstract": "self-delimiting ( slim ) programs are a central concept of theoretical computer science , particularly algorithmic information & probability theory , and asymptotically optimal program search ( aops ) . to apply aops to ( possibly recurrent ) neural networks ( nns ) , i introduce slim nns . neurons of a typical slim nn have threshold activation functions . during a computational episode , activations are spreading from input neurons through the slim nn until the computation activates a special halt neuron . weights of the nn 's used connections define its program . halting programs form a prefix code . the reset of the initial nn state does not cost more than the latest program execution . since prefixes of slim programs influence their suffixes ( weight changes occurring early in an episode influence which weights are considered later ) , slim nn learning algorithms ( las ) should execute weight changes online during activation spreading . this can be achieved by applying aops to growing slim nns . to efficiently teach a slim nn to solve many tasks , such as correctly classifying many different patterns , or solving many different robot control tasks , each connection keeps a list of tasks it is used for . the lists may be efficiently updated during training . to evaluate the overall effect of currently tested weight changes , a slim nn la needs to re-test performance only on the efficiently computable union of tasks potentially affected by the current weight changes . future slim nns will be implemented on 3-dimensional brain-like multi-processor hardware . their las will minimize task-specific total wire length of used connections , to encourage efficient solutions of subtasks by subsets of neurons that are physically close . the novel class of slim nn las is currently being probed in ongoing experiments to be reported in separate papers .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "practice in synonym extraction at large scale", "abstract": "synonym extraction is an important task in natural language processing and often used as a submodule in query expansion , question answering and other applications . automatic synonym extractor is highly preferred for large scale applications . previous studies in synonym extraction are most limited to small scale datasets . in this paper , we build a large dataset with 3.4 million synonym/non-synonym pairs to capture the challenges in real world scenarios . we proposed ( 1 ) a new cost function to accommodate the unbalanced learning problem , and ( 2 ) a feature learning based deep neural network to model the complicated relationships in synonym pairs . we compare several different approaches based on svms and neural networks , and find out a novel feature learning based neural network outperforms the methods with hand-assigned features . specifically , the best performance of our model surpasses the svm baseline with a significant 97\\ % relative improvement .", "topics": ["baseline ( configuration management )", "feature learning"]}
{"title": "a parallel and efficient algorithm for learning to match", "abstract": "many tasks in data mining and related fields can be formalized as matching between objects in two heterogeneous domains , including collaborative filtering , link prediction , image tagging , and web search . machine learning techniques , referred to as learning-to-match in this paper , have been successfully applied to the problems . among them , a class of state-of-the-art methods , named feature-based matrix factorization , formalize the task as an extension to matrix factorization by incorporating auxiliary features into the model . unfortunately , making those algorithms scale to real world problems is challenging , and simple parallelization strategies fail due to the complex cross talking patterns between sub-tasks . in this paper , we tackle this challenge with a novel parallel and efficient algorithm for feature-based matrix factorization . our algorithm , based on coordinate descent , can easily handle hundreds of millions of instances and features on a single machine . the key recipe of this algorithm is an iterative relaxation of the objective to facilitate parallel updates of parameters , with guaranteed convergence on minimizing the original objective function . experimental results demonstrate that the proposed method is effective on a wide range of matching problems , with efficiency significantly improved upon the baselines while accuracy retained unchanged .", "topics": ["data mining", "loss function"]}
{"title": "learning balanced mixtures of discrete distributions with small sample", "abstract": "we study the problem of partitioning a small sample of $ n $ individuals from a mixture of $ k $ product distributions over a boolean cube $ \\ { 0 , 1\\ } ^k $ according to their distributions . each distribution is described by a vector of allele frequencies in $ \\r^k $ . given two distributions , we use $ \\gamma $ to denote the average $ \\ell_2^2 $ distance in frequencies across $ k $ dimensions , which measures the statistical divergence between them . we study the case assuming that bits are independently distributed across $ k $ dimensions . this work demonstrates that , for a balanced input instance for $ k = 2 $ , a certain graph-based optimization function returns the correct partition with high probability , where a weighted graph $ g $ is formed over $ n $ individuals , whose pairwise hamming distances between their corresponding bit vectors define the edge weights , so long as $ k = \\omega ( \\ln n/\\gamma ) $ and $ kn = \\tilde\\omega ( \\ln n/\\gamma^2 ) $ . the function computes a maximum-weight balanced cut of $ g $ , where the weight of a cut is the sum of the weights across all edges in the cut . this result demonstrates a nice property in the high-dimensional feature space : one can trade off the number of features that are required with the size of the sample to accomplish certain tasks like clustering .", "topics": ["feature vector", "cluster analysis"]}
{"title": "rotation invariance neural network", "abstract": "rotation invariance and translation invariance have great values in image recognition tasks . in this paper , we bring a new architecture in convolutional neural network ( cnn ) named cyclic convolutional layer to achieve rotation invariance in 2-d symbol recognition . we can also get the position and orientation of the 2-d symbol by the network to achieve detection purpose for multiple non-overlap target . last but not least , this architecture can achieve one-shot learning in some cases using those invariance .", "topics": ["object detection", "computer vision"]}
{"title": "investigating the effects of dynamic precision scaling on neural network training", "abstract": "training neural networks is a time- and compute-intensive operation . this is mainly due to the large amount of floating point tensor operations that are required during training . these constraints limit the scope of design space explorations ( in terms of hyperparameter search ) for data scientists and researchers . recent work has explored the possibility of reducing the numerical precision used to represent parameters , activations , and gradients during neural network training as a way to reduce the computational cost of training ( and thus reducing training time ) . in this paper we develop a novel dynamic precision scaling scheme and evaluate its performance , comparing it to previous works . using stochastic fixed-point rounding , a quantization-error based scaling scheme , and dynamic bit-widths during training , we achieve 98.8 % test accuracy on the mnist dataset using an average bit-width of just 16 bits for weights and 14 bits for activations . this beats the previous state-of-the-art dynamic bit-width precision scaling algorithm .", "topics": ["numerical analysis", "mnist database"]}
{"title": "rough set model for discovering hybrid association rules", "abstract": "in this paper , the mining of hybrid association rules with rough set approach is investigated as the algorithm rshar.the rshar algorithm is constituted of two steps mainly . at first , to join the participant tables into a general table to generate the rules which is expressing the relationship between two or more domains that belong to several different tables in a database . then we apply the mapping code on selected dimension , which can be added directly into the information system as one certain attribute . to find the association rules , frequent itemsets are generated in second step where candidate itemsets are generated through equivalence classes and also transforming the mapping code in to real dimensions . the searching method for candidate itemset is similar to apriori algorithm . the analysis of the performance of algorithm has been carried out .", "topics": ["database"]}
{"title": "streaming binary sketching based on subspace tracking and diagonal uniformization", "abstract": "in this paper , we address the problem of learning compact similarity-preserving embeddings for massive high-dimensional streams of data in order to perform efficient similarity search . we present a new online method for computing binary compressed representations -sketches- of high-dimensional real feature vectors . given an expected code length $ c $ and high-dimensional input data points , our algorithm provides a $ c $ -bits binary code for preserving the distance between the points from the original high-dimensional space . our algorithm does not require neither the storage of the whole dataset nor a chunk , thus it is fully adaptable to the streaming setting . it also provides low time complexity and convergence guarantees . we demonstrate the quality of our binary sketches through experiments on real data for the nearest neighbors search task in the online setting .", "topics": ["time complexity", "computation"]}
{"title": "a machine learning framework for register placement optimization in digital circuit design", "abstract": "in modern digital circuit back-end design , designers heavily rely on electronic-design-automoation ( eda ) tool to close timing . however , the heuristic algorithms used in the place and route tool usually does not result in optimal solution . thus , significant design effort is used to tune parameters or provide user constraints or guidelines to improve the tool performance . in this paper , we targeted at those optimization space left behind by the eda tools and propose a machine learning framework that helps to define what are the guidelines and constraints for registers placement , which can yield better performance and quality for back-end design . in other words , the framework is trying to learn what are the flaws of the existing eda tools and tries to optimize it by providing additional information . we discuss what is the proper input feature vector to be extracted , and what is metric to be used for reference output . we also develop a scheme to generate perturbed training samples using existing design based on gaussian randomization . by applying our methodology , we are able to improve the design runtime by up to 36 % and timing quality by up to 23 % .", "topics": ["mathematical optimization", "optimization problem"]}
{"title": "multi-observation elicitation", "abstract": "we study loss functions that measure the accuracy of a prediction based on multiple data points simultaneously . to our knowledge , such loss functions have not been studied before in the area of property elicitation or in machine learning more broadly . as compared to traditional loss functions that take only a single data point , these multi-observation loss functions can in some cases drastically reduce the dimensionality of the hypothesis required . in elicitation , this corresponds to requiring many fewer reports ; in empirical risk minimization , it corresponds to algorithms on a hypothesis space of much smaller dimension . we explore some examples of the tradeoff between dimensionality and number of observations , give some geometric characterizations and intuition for relating loss functions and the properties that they elicit , and discuss some implications for both elicitation and machine-learning contexts .", "topics": ["loss function"]}
{"title": "a foundation to perception computing , logic and automata", "abstract": "in this report , a novel approach to intelligence and learning is introduced , this approach is based on what we call 'perception logic ' . based on this logic , a computing mechanism and automata are introduced . multi-resolution analysis of perceptual information is given , in which learning is accomplished in at most o ( log ( n ) ) epochs , where n is the number of samples , and the convergence is guarnteed . this approach combines the favors of computational modeles in the sense that they are structured and mathematically well-defined , and the adaptivity of soft computing approaches , in addition to the continuity and real-time response of dynamical systems .", "topics": ["heuristic"]}
{"title": "robust processing of natural language", "abstract": "previous approaches to robustness in natural language processing usually treat deviant input by relaxing grammatical constraints whenever a successful analysis can not be provided by `` normal '' means . this schema implies , that error detection always comes prior to error handling , a behaviour which hardly can compete with its human model , where many erroneous situations are treated without even noticing them . the paper analyses the necessary preconditions for achieving a higher degree of robustness in natural language processing and suggests a quite different approach based on a procedure for structural disambiguation . it not only offers the possibility to cope with robustness issues in a more natural way but eventually might be suited to accommodate quite different aspects of robust behaviour within a single framework .", "topics": ["test set", "natural language processing"]}
{"title": "concrete dropout", "abstract": "dropout is used as a practical tool to obtain uncertainty estimates in large vision models and reinforcement learning ( rl ) tasks . but to obtain well-calibrated uncertainty estimates , a grid-search over the dropout probabilities is necessary - a prohibitive operation with large models , and an impossible one with rl . we propose a new dropout variant which gives improved performance and better calibrated uncertainties . relying on recent developments in bayesian deep learning , we use a continuous relaxation of dropout 's discrete masks . together with a principled optimisation objective , this allows for automatic tuning of the dropout probability in large models , and as a result faster experimentation cycles . in rl this allows the agent to adapt its uncertainty dynamically as more data is observed . we analyse the proposed variant extensively on a range of tasks , and give insights into common practice in the field where larger dropout probabilities are often used in deeper model layers .", "topics": ["calculus of variations"]}
{"title": "efficient algorithms for non-convex isotonic regression through submodular optimization", "abstract": "we consider the minimization of submodular functions subject to ordering constraints . we show that this optimization problem can be cast as a convex optimization problem on a space of uni-dimensional measures , with ordering constraints corresponding to first-order stochastic dominance . we propose new discretization schemes that lead to simple and efficient algorithms based on zero-th , first , or higher order oracles ; these algorithms also lead to improvements without isotonic constraints . finally , our experiments show that non-convex loss functions can be much more robust to outliers for isotonic regression , while still leading to an efficient optimization problem .", "topics": ["optimization problem", "loss function"]}
{"title": "music genre classification using masked conditional neural networks", "abstract": "the conditional neural networks ( clnn ) and the masked conditional neural networks ( mclnn ) exploit the nature of multi-dimensional temporal signals . the clnn captures the conditional temporal influence between the frames in a window and the mask in the mclnn enforces a systematic sparseness that follows a filterbank-like pattern over the network links . the mask induces the network to learn about time-frequency representations in bands , allowing the network to sustain frequency shifts . additionally , the mask in the mclnn automates the exploration of a range of feature combinations , usually done through an exhaustive manual search . we have evaluated the mclnn performance using the ballroom and homburg datasets of music genres . mclnn has achieved accuracies that are competitive to state-of-the-art handcrafted attempts in addition to models based on convolutional neural networks .", "topics": ["neural networks"]}
{"title": "ensemble of neural classifiers for scoring knowledge base triples", "abstract": "this paper describes our approach for the triple scoring task at the wsdm cup 2017 . the task required participants to assign a relevance score for each pair of entities and their types in a knowledge base in order to enhance the ranking results in entity retrieval tasks . we propose an approach wherein the outputs of multiple neural network classifiers are combined using a supervised machine learning model . the experimental results showed that our proposed method achieved the best performance in one out of three measures ( i.e . , kendall 's tau ) , and performed competitively in the other two measures ( i.e . , accuracy and average score difference ) .", "topics": ["supervised learning", "entity"]}
{"title": "multitask learning for fine-grained twitter sentiment analysis", "abstract": "traditional sentiment analysis approaches tackle problems like ternary ( 3-category ) and fine-grained ( 5-category ) classification by learning the tasks separately . we argue that such classification tasks are correlated and we propose a multitask approach based on a recurrent neural network that benefits by jointly learning them . our study demonstrates the potential of multitask models on this type of problems and improves the state-of-the-art results in the fine-grained sentiment classification problem .", "topics": ["recurrent neural network"]}
{"title": "entropy rate estimation for markov chains with large state space", "abstract": "estimating the entropy based on data is one of the prototypical problems in distribution property testing and estimation . for estimating the shannon entropy of a distribution on $ s $ elements with independent samples , [ paninski2004 ] showed that the sample complexity is sublinear in $ s $ , and [ valiant -- valiant2011 ] showed that consistent estimation of shannon entropy is possible if and only if the sample size $ n $ far exceeds $ \\frac { s } { \\log s } $ . in this paper we consider the problem of estimating the entropy rate of a stationary reversible markov chain with $ s $ states from a sample path of $ n $ observations . we show that : ( 1 ) as long as the markov chain mixes not too slowly , i.e . , the relaxation time is at most $ o ( \\frac { s } { \\ln^3 s } ) $ , consistent estimation is achievable when $ n \\gg \\frac { s^2 } { \\log s } $ . ( 2 ) as long as the markov chain has some slight dependency , i.e . , the relaxation time is at least $ 1+\\omega ( \\frac { \\ln^2 s } { \\sqrt { s } } ) $ , consistent estimation is impossible when $ n \\lesssim \\frac { s^2 } { \\log s } $ . under both assumptions , the optimal estimation accuracy is shown to be $ \\theta ( \\frac { s^2 } { n \\log s } ) $ . in comparison , the empirical entropy rate requires at least $ \\omega ( s^2 ) $ samples to be consistent , even when the markov chain is memoryless . in addition to synthetic experiments , we also apply the estimators that achieve the optimal sample complexity to estimate the entropy rate of the english language in the penn treebank and the google one billion words corpora , which provides a natural benchmark for language modeling and relates it directly to the widely used perplexity measure .", "topics": ["markov chain"]}
{"title": "on constructing the value function for optimal trajectory problem and its application to image processing", "abstract": "we proposed an algorithm for solving hamilton-jacobi equation associated to an optimal trajectory problem for a vehicle moving inside the pre-specified domain with the speed depending upon the direction of the motion and current position of the vehicle . the dynamics of the vehicle is defined by an ordinary differential equation , the right hand of which is given by product of control ( a time dependent fuction ) and a function dependent on trajectory and control . at some unspecified terminal time , the vehicle reaches the boundary of the pre-specified domain and incurs a terminal cost . we also associate the traveling cost with a type of integral to the trajectory followed by vehicle . we are interested in a numerical method for finding a trajectory that minimizes the sum of the traveling cost and terminal cost . we developed an algorithm solving the value function for general trajectory optimization problem . our algorithm is closely related to the tsitsiklis 's fast marching method and j . a . sethian 's oum and slf-lll [ 1-4 ] and is a generalization of them . on the basis of these results , we applied our algorithm to the image processing such as fingerprint verification .", "topics": ["image processing"]}
{"title": "mixing dirichlet topic models and word embeddings to make lda2vec", "abstract": "distributed dense word vectors have been shown to be effective at capturing token-level semantic and syntactic regularities in language , while topic models can form interpretable representations over documents . in this work , we describe lda2vec , a model that learns dense word vectors jointly with dirichlet-distributed latent document-level mixtures of topic vectors . in contrast to continuous dense document representations , this formulation produces sparse , interpretable document mixtures through a non-negative simplex constraint . our method is simple to incorporate into existing automatic differentiation frameworks and allows for unsupervised document representations geared for use by scientists while simultaneously learning word vectors and the linear relationships between them .", "topics": ["sparse matrix"]}
{"title": "automatic breast ultrasound image segmentation : a survey", "abstract": "breast cancer is one of the leading causes of cancer death among women worldwide . in clinical routine , automatic breast ultrasound ( bus ) image segmentation is very challenging and essential for cancer diagnosis and treatment planning . many bus segmentation approaches have been studied in the last two decades , and have been proved to be effective on private datasets . currently , the advancement of bus image segmentation seems to meet its bottleneck . the improvement of the performance is increasingly challenging , and only few new approaches were published in the last several years . it is the time to look at the field by reviewing previous approaches comprehensively and to investigate the future directions . in this paper , we study the basic ideas , theories , pros and cons of the approaches , group them into categories , and extensively review each category in depth by discussing the principles , application issues , and advantages/disadvantages .", "topics": ["image segmentation"]}
{"title": "cross-lingual sentiment analysis without ( good ) translation", "abstract": "current approaches to cross-lingual sentiment analysis try to leverage the wealth of labeled english data using bilingual lexicons , bilingual vector space embeddings , or machine translation systems . here we show that it is possible to use a single linear transformation , with as few as 2000 word pairs , to capture fine-grained sentiment relationships between words in a cross-lingual setting . we apply these cross-lingual sentiment models to a diverse set of tasks to demonstrate their functionality in a non-english context . by effectively leveraging english sentiment knowledge without the need for accurate translation , we can analyze and extract features from other languages with scarce data at a very low cost , thus making sentiment and related analyses for many languages inexpensive .", "topics": ["machine translation"]}
{"title": "lb2co : a semantic ontology framework for b2c ecommerce transaction on the internet", "abstract": "business ontology can enhance the successful development of complex enterprise system ; this is being achieved through knowledge sharing and the ease of communication between every entity in the domain . through human semantic interaction with the web resources , machines to interpret the data published in a machine interpretable form under web . however , the theoretical practice of business ontology in ecommerce domain is quite a few especially in the section of electronic transaction , and the various techniques used to obtain efficient communication across spheres are error prone and are not always guaranteed to be efficient in obtaining desired result due to poor semantic integration between entities . to overcome the poor semantic integration this research focuses on proposed ontology called lb2co , which combines the framework of idef5 & snap as an analysis tool , for automated recommendation of product and services and create effective ontological framework for b2c transaction & communication across different business domains that facilitates the interoperability & integration of b2c transactions over the web .", "topics": ["entity"]}
{"title": "near-optimal active learning of halfspaces via query synthesis in the noisy setting", "abstract": "in this paper , we consider the problem of actively learning a linear classifier through query synthesis where the learner can construct artificial queries in order to estimate the true decision boundaries . this problem has recently gained a lot of interest in automated science and adversarial reverse engineering for which only heuristic algorithms are known . in such applications , queries can be constructed de novo to elicit information ( e.g . , automated science ) or to evade detection with minimal cost ( e.g . , adversarial reverse engineering ) . we develop a general framework , called dimension coupling ( dc ) , that 1 ) reduces a d-dimensional learning problem to d-1 low dimensional sub-problems , 2 ) solves each sub-problem efficiently , 3 ) appropriately aggregates the results and outputs a linear classifier , and 4 ) provides a theoretical guarantee for all possible schemes of aggregation . the proposed method is proved resilient to noise . we show that the dc framework avoids the curse of dimensionality : its computational complexity scales linearly with the dimension . moreover , we show that the query complexity of dc is near optimal ( within a constant factor of the optimum algorithm ) . to further support our theoretical analysis , we compare the performance of dc with the existing work . we observe that dc consistently outperforms the prior arts in terms of query complexity while often running orders of magnitude faster .", "topics": ["computational complexity theory", "heuristic"]}
{"title": "machine learning on human connectome data from mri", "abstract": "functional mri ( fmri ) and diffusion mri ( dmri ) are non-invasive imaging modalities that allow in-vivo analysis of a patient 's brain network ( known as a connectome ) . use of these technologies has enabled faster and better diagnoses and treatments of neurological disorders and a deeper understanding of the human brain . recently , researchers have been exploring the application of machine learning models to connectome data in order to predict clinical outcomes and analyze the importance of subnetworks in the brain . connectome data has unique properties , which present both special challenges and opportunities when used for machine learning . the purpose of this work is to review the literature on the topic of applying machine learning models to mri-based connectome data . this field is growing rapidly and now encompasses a large body of research . to summarize the research done to date , we provide a comparative , structured summary of 77 relevant works , tabulated according to different criteria , that represent the majority of the literature on this topic . ( we also published a living version of this table online at http : //connectomelearning.cs.sfu.ca that the community can continue to contribute to . ) after giving an overview of how connectomes are constructed from dmri and fmri data , we discuss the variety of machine learning tasks that have been explored with connectome data . we then compare the advantages and drawbacks of different machine learning approaches that have been employed , discussing different feature selection and feature extraction schemes , as well as the learning models and regularization penalties themselves . throughout this discussion , we focus particularly on how the methods are adapted to the unique nature of graphical connectome data . finally , we conclude by summarizing the current state of the art and by outlining what we believe are strategic directions for future research .", "topics": ["feature extraction", "matrix regularization"]}
{"title": "online robust subspace tracking from partial information", "abstract": "this paper presents grasta ( grassmannian robust adaptive subspace tracking algorithm ) , an efficient and robust online algorithm for tracking subspaces from highly incomplete information . the algorithm uses a robust $ l^1 $ -norm cost function in order to estimate and track non-stationary subspaces when the streaming data vectors are corrupted with outliers . we apply grasta to the problems of robust matrix completion and real-time separation of background from foreground in video . in this second application , we show that grasta performs high-quality separation of moving objects from background at exceptional speeds : in one popular benchmark video example , grasta achieves a rate of 57 frames per second , even when run in matlab on a personal laptop .", "topics": ["loss function"]}
{"title": "ultraslow diffusion in language : dynamics of appearance of already popular adjectives on japanese blogs", "abstract": "what dynamics govern a time series representing the appearance of words in social media data ? in this paper , we investigate an elementary dynamics , from which word-dependent special effects are segregated , such as breaking news , increasing ( or decreasing ) concerns , or seasonality . to elucidate this problem , we investigated approximately three billion japanese blog articles over a period of six years , and analysed some corresponding solvable mathematical models . from the analysis , we found that a word appearance can be explained by the random diffusion model based on the power-law forgetting process , which is a type of long memory point process related to arfima ( 0,0.5,0 ) . in particular , we confirmed that ultraslow diffusion ( where the mean squared displacement grows logarithmically ) , which the model predicts in an approximate manner , reproduces the actual data . in addition , we also show that the model can reproduce other statistical properties of a time series : ( i ) the fluctuation scaling , ( ii ) spectrum density , and ( iii ) shapes of the probability density functions .", "topics": ["time series", "approximation algorithm"]}
{"title": "integrating boundary and center correlation filters for visual tracking with aspect ratio variation", "abstract": "the aspect ratio variation frequently appears in visual tracking and has a severe influence on performance . although many correlation filter ( cf ) -based trackers have also been suggested for scale adaptive tracking , few studies have been given to handle the aspect ratio variation for cf trackers . in this paper , we make the first attempt to address this issue by introducing a family of 1d boundary cfs to localize the left , right , top , and bottom boundaries in videos . this allows us cope with the aspect ratio variation flexibly during tracking . specifically , we present a novel tracking model to integrate 1d boundary and 2d center cfs ( ibccf ) where boundary and center filters are enforced by a near-orthogonality regularization term . to optimize our ibccf model , we develop an alternating direction method of multipliers . experiments on several datasets show that ibccf can effectively handle aspect ratio variation , and achieves state-of-the-art performance in terms of accuracy and robustness .", "topics": ["matrix regularization"]}
{"title": "privileged multi-label learning", "abstract": "this paper presents privileged multi-label learning ( prml ) to explore and exploit the relationship between labels in multi-label learning problems . we suggest that for each individual label , it can not only be implicitly connected with other labels via the low-rank constraint over label predictors , but also its performance on examples can receive the explicit comments from other labels together acting as an \\emph { oracle teacher } . we generate privileged label feature for each example and its individual label , and then integrate it into the framework of low-rank based multi-label learning . the proposed algorithm can therefore comprehensively explore and exploit label relationships by inheriting all the merits of privileged information and low-rank constraints . we show that prml can be efficiently solved by dual coordinate descent algorithm using iterative optimization strategy with cheap updates . experiments on benchmark datasets show that through privileged label features , the performance can be significantly improved and prml is superior to several competing methods in most cases .", "topics": ["reinforcement learning"]}
{"title": "a sequential monte carlo approach to thompson sampling for bayesian optimization", "abstract": "bayesian optimization through gaussian process regression is an effective method of optimizing an unknown function for which every measurement is expensive . it approximates the objective function and then recommends a new measurement point to try out . this recommendation is usually selected by optimizing a given acquisition function . after a sufficient number of measurements , a recommendation about the maximum is made . however , a key realization is that the maximum of a gaussian process is not a deterministic point , but a random variable with a distribution of its own . this distribution can not be calculated analytically . our main contribution is an algorithm , inspired by sequential monte carlo samplers , that approximates this maximum distribution . subsequently , by taking samples from this distribution , we enable thompson sampling to be applied to ( armed-bandit ) optimization problems with a continuous input space . all this is done without requiring the optimization of a nonlinear acquisition function . experiments have shown that the resulting optimization method has a competitive performance at keeping the cumulative regret limited .", "topics": ["approximation algorithm"]}
{"title": "neural speed reading via skim-rnn", "abstract": "inspired by the principles of speed reading , we introduce skim-rnn , a recurrent neural network ( rnn ) that dynamically decides to update only a small fraction of the hidden state for relatively unimportant input tokens . skim-rnn gives computational advantage over an rnn that always updates the entire hidden state . skim-rnn uses the same input and output interfaces as a standard rnn and can be easily used instead of rnns in existing models . in our experiments , we show that skim-rnn can achieve significantly reduced computational cost without losing accuracy compared to standard rnns across five different natural language tasks . in addition , we demonstrate that the trade-off between accuracy and speed of skim-rnn can be dynamically controlled during inference time in a stable manner . our analysis also shows that skim-rnn running on a single cpu offers lower latency compared to standard rnns on gpus .", "topics": ["recurrent neural network", "natural language"]}
{"title": "quantum pattern retrieval by qubit networks with hebb interactions", "abstract": "qubit networks with long-range interactions inspired by the hebb rule can be used as quantum associative memories . starting from a uniform superposition , the unitary evolution generated by these interactions drives the network through a quantum phase transition at a critical computation time , after which ferromagnetic order guarantees that a measurement retrieves the stored memory . the maximum memory capacity p of these qubit networks is reached at a memory density p/n=1 .", "topics": ["time complexity", "interaction"]}
{"title": "spectral networks and locally connected networks on graphs", "abstract": "convolutional neural networks are extremely efficient architectures in image and audio recognition tasks , thanks to their ability to exploit the local translational invariance of signal classes over their domain . in this paper we consider possible generalizations of cnns to signals defined on more general domains without the action of a translation group . in particular , we propose two constructions , one based upon a hierarchical clustering of the domain , and another based on the spectrum of the graph laplacian . we show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size , resulting in efficient deep architectures .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "the hasyv2 dataset", "abstract": "this paper describes the hasyv2 dataset . hasy is a publicly available , free of charge dataset of single symbols similar to mnist . it contains 168233 instances of 369 classes . hasy contains two challenges : a classification challenge with 10 pre-defined folds for 10-fold cross-validation and a verification challenge .", "topics": ["mnist database"]}
{"title": "compiling causal theories to successor state axioms and strips-like systems", "abstract": "we describe a system for specifying the effects of actions . unlike those commonly used in ai planning , our system uses an action description language that allows one to specify the effects of actions using domain rules , which are state constraints that can entail new action effects from old ones . declaratively , an action domain in our language corresponds to a nonmonotonic causal theory in the situation calculus . procedurally , such an action domain is compiled into a set of logical theories , one for each action in the domain , from which fully instantiated successor state-like axioms and strips-like systems are then generated . we expect the system to be a useful tool for knowledge engineers writing action specifications for classical ai planning systems , golog systems , and other systems where formal specifications of actions are needed .", "topics": ["artificial intelligence", "causality"]}
{"title": "ransac : identification of higher-order geometric features and applications in humanoid robot soccer", "abstract": "the ability for an autonomous agent to self-localise is directly proportional to the accuracy and precision with which it can perceive salient features within its local environment . the identification of such features by recognising geometric profile allows robustness against lighting variations , which is necessary in most industrial robotics applications . this paper details a framework by which the random sample consensus ( ransac ) algorithm , often applied to parameter fitting in linear models , can be extended to identify higher-order geometric features . goalpost identification within humanoid robot soccer is investigated as an application , with the developed system yielding an order-of-magnitude improvement in classification performance relative to a traditional histogramming methodology .", "topics": ["autonomous car"]}
{"title": "3d simulation for robot arm control with deep q-learning", "abstract": "recent trends in robot arm control have seen a shift towards end-to-end solutions , using deep reinforcement learning to learn a controller directly from raw sensor data , rather than relying on a hand-crafted , modular pipeline . however , the high dimensionality of the state space often means that it is impractical to generate sufficient training data with real-world experiments . as an alternative solution , we propose to learn a robot controller in simulation , with the potential of then transferring this to a real robot . building upon the recent success of deep q-networks , we present an approach which uses 3d simulations to train a 7-dof robotic arm in a control task without any prior knowledge . the controller accepts images of the environment as its only input , and outputs motor actions for the task of locating and grasping a cube , over a range of initial configurations . to encourage efficient learning , a structured reward function is designed with intermediate rewards . we also present preliminary results in direct transfer of policies over to a real robot , without any further training .", "topics": ["test set", "reinforcement learning"]}
{"title": "feature markov decision processes", "abstract": "general purpose intelligent learning agents cycle through ( complex , non-mdp ) sequences of observations , actions , and rewards . on the other hand , reinforcement learning is well-developed for small finite state markov decision processes ( mdps ) . so far it is an art performed by human designers to extract the right state representation out of the bare observations , i.e . to reduce the agent setup to the mdp framework . before we can think of mechanizing this search for suitable mdps , we need a formal objective criterion . the main contribution of this article is to develop such a criterion . i also integrate the various parts into one learning algorithm . extensions to more realistic dynamic bayesian networks are developed in a companion article .", "topics": ["reinforcement learning", "bayesian network"]}
{"title": "an upper bound on prototype set size for condensed nearest neighbor", "abstract": "the condensed nearest neighbor ( cnn ) algorithm is a heuristic for reducing the number of prototypical points stored by a nearest neighbor classifier , while keeping the classification rule given by the reduced prototypical set consistent with the full set . i present an upper bound on the number of prototypical points accumulated by cnn . the bound originates in a bound on the number of times the decision rule is updated during training in the multiclass perceptron algorithm , and thus is independent of training set size .", "topics": ["heuristic"]}
{"title": "context models on sequences of covers", "abstract": "we present a class of models that , via a simple construction , enables exact , incremental , non-parametric , polynomial-time , bayesian inference of conditional measures . the approach relies upon creating a sequence of covers on the conditioning variable and maintaining a different model for each set within a cover . inference remains tractable by specifying the probabilistic model in terms of a random walk within the sequence of covers . we demonstrate the approach on problems of conditional density estimation , which , to our knowledge is the first closed-form , non-parametric bayesian approach to this problem .", "topics": ["time complexity", "polynomial"]}
{"title": "semi-supervised learning on graphs through reach and distance diffusion", "abstract": "semi-supervised learning ( ssl ) is an indispensable tool when there are few labeled entities and many unlabeled entities for which we want to predict labels . with graph-based methods , entities correspond to nodes in a graph and edges represent strong relations . at the heart of ssl algorithms is the specification of a dense { \\em kernel } of pairwise affinity values from the graph structure . a learning algorithm is then trained on the kernel together with labeled entities . the most popular kernels are { \\em spectral } and include the highly scalable `` symmetric '' laplacian methods , that compute a soft labels using jacobi iterations , and `` asymmetric '' methods including personalized page rank ( ppr ) which use short random walks and apply with directed relations , such as like , follow , or hyperlinks . we introduce { \\em reach diffusion } and { \\em distance diffusion } kernels that build on powerful social and economic models of centrality and influence in networks and capture the directed pairwise relations that underline social influence . inspired by the success of social influence as an alternative to spectral centrality such as page rank , we explore ssl with our kernels and develop highly scalable algorithms for parameter setting , label learning , and sampling . we perform preliminary experiments that demonstrate the properties and potential of our kernels .", "topics": ["kernel ( operating system )", "supervised learning"]}
{"title": "sensitivity analysis for threshold decision making with dynamic networks", "abstract": "the effect of inaccuracies in the parameters of a dynamic bayesian network can be investigated by subjecting the network to a sensitivity analysis . having detailed the resulting sensitivity functions in our previous work , we now study the effect of parameter inaccuracies on a recommended decision in view of a threshold decision-making model . we detail the effect of varying a single and multiple parameters from a conditional probability table and present a computational procedure for establishing bounds between which assessments for these parameters can be varied without inducing a change in the recommended decision . we illustrate the various concepts involved by means of a real-life dynamic network in the field of infectious disease .", "topics": ["bayesian network"]}
{"title": "articulated hand pose estimation review", "abstract": "with the increase number of companies focusing on commercializing augmented reality ( ar ) , virtual reality ( vr ) and wearable devices , the need for a hand based input mechanism is becoming essential in order to make the experience natural , seamless and immersive . hand pose estimation has progressed drastically in recent years due to the introduction of commodity depth cameras . hand pose estimation based on vision is still a challenging problem due to its complexity from self-occlusion ( between fingers ) , close similarity between fingers , dexterity of the hands , speed of the pose and the high dimension of the hand kinematic parameters . articulated hand pose estimation is still an open problem and under intensive research from both academia and industry . the 2 approaches used for hand pose estimation are : discriminative and generative . generative approach is a model based that tries to fit a hand model to the observed data . discriminative approach is appearance based , usually implemented with machine learning ( ml ) and require a large amount of training data . recent hand pose estimation uses hybrid approach by combining both discriminative and generative methods into a single hand pipeline . in this paper , we focus on reviewing recent progress of hand pose estimation from depth sensor . we will survey discriminative methods , generative methods and hybrid methods . this paper is not a comprehensive review of all hand pose estimation techniques , it is a subset of some of the recent state-of-the-art techniques .", "topics": ["generative model", "test set"]}
{"title": "a formal framework for linguistic annotation ( revised version )", "abstract": "`linguistic annotation ' covers any descriptive or analytic notations applied to raw language data . the basic data may be in the form of time functions - audio , video and/or physiological recordings - or it may be textual . the added notations may include transcriptions of all sorts ( from phonetic features to discourse structures ) , part-of-speech and sense tagging , syntactic analysis , `named entity ' identification , co-reference annotation , and so on . while there are several ongoing efforts to provide formats and tools for such annotations and to publish annotated linguistic databases , the lack of widely accepted standards is becoming a critical problem . proposed standards , to the extent they exist , have focused on file formats . this paper focuses instead on the logical structure of linguistic annotations . we survey a wide variety of existing annotation formats and demonstrate a common conceptual core , the annotation graph . this provides a formal framework for constructing , maintaining and searching linguistic annotations , while remaining consistent with many alternative data structures and file formats .", "topics": ["parsing", "database"]}
{"title": "independence clustering ( without a matrix )", "abstract": "the independence clustering problem is considered in the following formulation : given a set $ s $ of random variables , it is required to find the finest partitioning $ \\ { u_1 , \\dots , u_k\\ } $ of $ s $ into clusters such that the clusters $ u_1 , \\dots , u_k $ are mutually independent . since mutual independence is the target , pairwise similarity measurements are of no use , and thus traditional clustering algorithms are inapplicable . the distribution of the random variables in $ s $ is , in general , unknown , but a sample is available . thus , the problem is cast in terms of time series . two forms of sampling are considered : i.i.d.\\ and stationary time series , with the main emphasis being on the latter , more general , case . a consistent , computationally tractable algorithm for each of the settings is proposed , and a number of open directions for further research are outlined .", "topics": ["sampling ( signal processing )", "cluster analysis"]}
{"title": "soft proposal networks for weakly supervised object localization", "abstract": "weakly supervised object localization remains challenging , where only image labels instead of bounding boxes are available during training . object proposal is an effective component in localization , but often computationally expensive and incapable of joint optimization with some of the remaining modules . in this paper , to the best of our knowledge , we for the first time integrate weakly supervised object proposal into convolutional neural networks ( cnns ) in an end-to-end learning manner . we design a network component , soft proposal ( sp ) , to be plugged into any standard convolutional architecture to introduce the nearly cost-free object proposal , orders of magnitude faster than state-of-the-art methods . in the sp-augmented cnns , referred to as soft proposal networks ( spns ) , iteratively evolved object proposals are generated based on the deep feature maps then projected back , and further jointly optimized with network parameters , with image-level supervision only . through the unified learning process , spns learn better object-centric filters , discover more discriminative visual evidence , and suppress background interference , significantly boosting both weakly supervised object localization and classification performance . we report the best results on popular benchmarks , including pascal voc , ms coco , and imagenet .", "topics": ["map", "end-to-end principle"]}
{"title": "an accurate arabic root-based lemmatizer for information retrieval purposes", "abstract": "in spite of its robust syntax , semantic cohesion , and less ambiguity , lemma level analysis and generation does not yet focused in arabic nlp literatures . in the current research , we propose the first non-statistical accurate arabic lemmatizer algorithm that is suitable for information retrieval ( ir ) systems . the proposed lemmatizer makes use of different arabic language knowledge resources to generate accurate lemma form and its relevant features that support ir purposes . as a pos tagger , the experimental results show that , the proposed algorithm achieves a maximum accuracy of 94.8 % . for first seen documents , an accuracy of 89.15 % is achieved , compared to 76.7 % of up to date stanford accurate arabic model , for the same , dataset .", "topics": ["natural language processing"]}
{"title": "some open problems in optimal adaboost and decision stumps", "abstract": "the significance of the study of the theoretical and practical properties of adaboost is unquestionable , given its simplicity , wide practical use , and effectiveness on real-world datasets . here we present a few open problems regarding the behavior of `` optimal adaboost , '' a term coined by rudin , daubechies , and schapire in 2004 to label the simple version of the standard adaboost algorithm in which the weak learner that adaboost uses always outputs the weak classifier with lowest weighted error among the respective hypothesis class of weak classifiers implicit in the weak learner . we concentrate on the standard , `` vanilla '' version of optimal adaboost for binary classification that results from using an exponential-loss upper bound on the misclassification training error . we present two types of open problems . one deals with general weak hypotheses . the other deals with the particular case of decision stumps , as often and commonly used in practice . answers to the open problems can have immediate significant impact to ( 1 ) cementing previously established results on asymptotic convergence properties of optimal adaboost , for finite datasets , which in turn can be the start to any convergence-rate analysis ; ( 2 ) understanding the weak-hypotheses class of effective decision stumps generated from data , which we have empirically observed to be significantly smaller than the typically obtained class , as well as the effect on the weak learner 's running time and previously established improved bounds on the generalization performance of optimal adaboost classifiers ; and ( 3 ) shedding some light on the `` self control '' that adaboost tends to exhibit in practice .", "topics": ["time complexity"]}
{"title": "joint voxel and coordinate regression for accurate 3d facial landmark localization", "abstract": "3d face shape is more expressive and viewpoint-consistent than its 2d counterpart . however , 3d facial landmark localization in a single image is challenging due to the ambiguous nature of landmarks under 3d perspective . existing approaches typically adopt a suboptimal two-step strategy , performing 2d landmark localization followed by depth estimation . in this paper , we propose the joint voxel and coordinate regression ( jvcr ) method for 3d facial landmark localization , addressing it more effectively in an end-to-end fashion . first , a compact volumetric representation is proposed to encode the per-voxel likelihood of positions being the 3d landmarks . the dimensionality of such a representation is fixed regardless of the number of target landmarks , so that the curse of dimensionality could be avoided . then , a stacked hourglass network is adopted to estimate the volumetric representation from coarse to fine , followed by a 3d convolution network that takes the estimated volume as input and regresses 3d coordinates of the face shape . in this way , the 3d structural constraints between landmarks could be learned by the neural network in a more efficient manner . moreover , the proposed pipeline enables end-to-end training and improves the robustness and accuracy of 3d facial landmark localization . the effectiveness of our approach is validated on the 3dfaw and aflw2000-3d datasets . experimental results show that the proposed method achieves state-of-the-art performance in comparison with existing methods .", "topics": ["end-to-end principle", "convolution"]}
{"title": "conceptualization of seeded region growing by pixels aggregation . part 4 : simple , generic and robust extraction of grains in granular materials obtained by x-ray tomography", "abstract": "this paper proposes a simple , generic and robust method to extract the grains from experimental tridimensionnal images of granular materials obtained by x-ray tomography . this extraction has two steps : segmentation and splitting . for the segmentation step , if there is a sufficient contrast between the different components , a classical threshold procedure followed by a succession of morphological filters can be applied . if not , and if the boundary needs to be localized precisely , a watershed transformation controlled by labels is applied . the basement of this transformation is to localize a label included in the component and another label in the component complementary . a `` soft '' threshold following by an opening is applied on the initial image to localize a label in a component . for any segmentation procedure , the visualisation shows a problem : some groups of two grains , close one to each other , become connected . so if a classical cluster procedure is applied on the segmented binary image , these numerical connected grains are considered as a single grain . to overcome this problem , we applied a procedure introduced by l. vincent in 1993 . this grains extraction is tested for various complexes porous media and granular material , to predict various properties ( diffusion , electrical conductivity , deformation field ) in a good agreement with experiment data .", "topics": ["cluster analysis", "numerical analysis"]}
{"title": "a survey on phrase structure learning methods for text classification", "abstract": "text classification is a task of automatic classification of text into one of the predefined categories . the problem of text classification has been widely studied in different communities like natural language processing , data mining and information retrieval . text classification is an important constituent in many information management tasks like topic identification , spam filtering , email routing , language identification , genre classification , readability assessment etc . the performance of text classification improves notably when phrase patterns are used . the use of phrase patterns helps in capturing non-local behaviours and thus helps in the improvement of text classification task . phrase structure extraction is the first step to continue with the phrase pattern identification . in this survey , detailed study of phrase structure learning methods have been carried out . this will enable future work in several nlp tasks , which uses syntactic information from phrase structure like grammar checkers , question answering , information extraction , machine translation , text classification . the paper also provides different levels of classification and detailed comparison of the phrase structure learning methods .", "topics": ["natural language processing", "data mining"]}
{"title": "continuous occurrence theory", "abstract": "usually gradual and continuous changes in entities will lead to appear events . but usually it is supposed that an event is occurred at once . in this research an integrated framework called continuous occurrence theory ( cot ) is presented to investigate respective path leading to occurrence of the events in the real world . for this purpose initially fundamental concepts are defined . afterwards , the appropriate tools such as occurrence variables computations , occurrence dependency function and occurrence model are introduced and explained in a systematic manner . indeed , cot provides the possibility to : ( a ) monitor occurrence of events during time ; ( b ) study background of the events ; ( c ) recognize the relevant issues of each event ; and ( d ) understand how these issues affect on the considered event . the developed framework ( cot ) provides the necessary context to analyze accurately continual changes of the issues and the relevant events in the various branches of science and business . finally , typical applications of cot and an applied modeling example of it have been explained and a mathematical programming example is modeled in the occurrence based environment .", "topics": ["mathematical optimization", "entity"]}
{"title": "deep learning as a tool for neural data analysis : speech classification and cross-frequency coupling in human sensorimotor cortex", "abstract": "a fundamental challenge in neuroscience is to understand what structure in the world is represented in spatially distributed patterns of neural activity from multiple single-trial measurements . this is often accomplished by learning a simple , linear transformations between neural features and features of the sensory stimuli or motor task . while successful in some early sensory processing areas , linear mappings are unlikely to be ideal tools for elucidating nonlinear , hierarchical representations of higher-order brain areas during complex tasks , such as the production of speech by humans . here , we apply deep networks to predict produced speech syllables from cortical surface electric potentials recorded from human sensorimotor cortex . we found that deep networks had higher decoding prediction accuracy compared to baseline models , and also exhibited greater improvements in accuracy with increasing dataset size . we further demonstrate that deep network 's confusions revealed hierarchical latent structure in the neural data , which recapitulated the underlying articulatory nature of speech motor control . finally , we used deep networks to compare task-relevant information in different neural frequency bands , and found that the high-gamma band contains the vast majority of information relevant for the speech prediction task , with little-to-no additional contribution from lower-frequencies . together , these results demonstrate the utility of deep networks as a data analysis tool for neuroscience .", "topics": ["baseline ( configuration management )", "feature learning"]}
{"title": "on statistics , computation and scalability", "abstract": "how should statistical procedures be designed so as to be scalable computationally to the massive datasets that are increasingly the norm ? when coupled with the requirement that an answer to an inferential question be delivered within a certain time budget , this question has significant repercussions for the field of statistics . with the goal of identifying `` time-data tradeoffs , '' we investigate some of the statistical consequences of computational perspectives on scability , in particular divide-and-conquer methodology and hierarchies of convex relaxations .", "topics": ["computation", "scalability"]}
{"title": "comparison of the c4.5 and a naive bayes classifier for the prediction of lung cancer survivability", "abstract": "numerous data mining techniques have been developed to extract information and identify patterns and predict trends from large data sets . in this study , two classification techniques , the j48 implementation of the c4.5 algorithm and a naive bayes classifier are applied to predict lung cancer survivability from an extensive data set with fifteen years of patient records . the purpose of the project is to verify the predictive effectiveness of the two techniques on real , historical data . besides the performance outcome that renders j48 marginally better than the naive bayes technique , there is a detailed description of the data and the required pre-processing activities . the performance results confirm expectations while some of the issues that appeared during experimentation , underscore the value of having domain-specific understanding to leverage any domain-specific characteristics inherent in the data .", "topics": ["data mining"]}
{"title": "an unsupervised approach for mapping between vector spaces", "abstract": "we present a language independent , unsupervised approach for transforming word embeddings from source language to target language using a transformation matrix . our model handles the problem of data scarcity which is faced by many languages in the world and yields improved word embeddings for words in the target language by relying on transformed embeddings of words of the source language . we initially evaluate our approach via word similarity tasks on a similar language pair - hindi as source and urdu as the target language , while we also evaluate our method on french and german as target languages and english as source language . our approach improves the current state of the art results - by 13 % for french and 19 % for german . for urdu , we saw an increment of 16 % over our initial baseline score . we further explore the prospects of our approach by applying it on multiple models of the same language and transferring words between the two models , thus solving the problem of missing words in a model . we evaluate this on word similarity and word analogy tasks .", "topics": ["baseline ( configuration management )", "unsupervised learning"]}
{"title": "gimp and wavelets for medical image processing : enhancing images of the fundus of the eye", "abstract": "the visual analysis of retina and of its vascular characteristics is important in the diagnosis and monitoring of diseases of visual perception . in the related medical diagnoses , the digital processing of the fundus images is used to obtain the segmentation of retinal vessels . however , an image segmentation is often requiring methods based on peculiar or complex algorithms : in this paper we will show some alternative approaches obtained by applying freely available tools to enhance , without a specific segmentation , the images of the fundus of the eye . we will see in particular , that combining the use of gimp , the gnu image manipulation program , with the wavelet filter of iris , a program well-known for processing astronomical images , the result is giving images which can be alternative of those obtained from segmentation .", "topics": ["image processing", "image segmentation"]}
{"title": "the effectiveness of data augmentation in image classification using deep learning", "abstract": "in this paper , we explore and compare multiple solutions to the problem of data augmentation in image classification . previous work has demonstrated the effectiveness of data augmentation through simple techniques , such as cropping , rotating , and flipping input images . we artificially constrain our access to data to a small subset of the imagenet dataset , and compare each data augmentation technique in turn . one of the more successful data augmentations strategies is the traditional transformations mentioned above . we also experiment with gans to generate images of different styles . finally , we propose a method to allow a neural net to learn augmentations that best improve the classifier , which we call neural augmentation . we discuss the successes and shortcomings of this method on various datasets .", "topics": ["computer vision"]}
{"title": "placeraider : virtual theft in physical spaces with smartphones", "abstract": "as smartphones become more pervasive , they are increasingly targeted by malware . at the same time , each new generation of smartphone features increasingly powerful onboard sensor suites . a new strain of sensor malware has been developing that leverages these sensors to steal information from the physical environment ( e.g . , researchers have recently demonstrated how malware can listen for spoken credit card numbers through the microphone , or feel keystroke vibrations using the accelerometer ) . yet the possibilities of what malware can see through a camera have been understudied . this paper introduces a novel visual malware called placeraider , which allows remote attackers to engage in remote reconnaissance and what we call virtual theft . through completely opportunistic use of the camera on the phone and other sensors , placeraider constructs rich , three dimensional models of indoor environments . remote burglars can thus download the physical space , study the environment carefully , and steal virtual objects from the environment ( such as financial documents , information on computer monitors , and personally identifiable information ) . through two human subject studies we demonstrate the effectiveness of using mobile devices as powerful surveillance and virtual theft platforms , and we suggest several possible defenses against visual malware .", "topics": ["sensor"]}
{"title": "a fatal point concept and a low-sensitivity quantitative measure for traffic safety analytics", "abstract": "the variability of the clusters generated by clustering techniques in the domain of latitude and longitude variables of fatal crash data are significantly unpredictable . this unpredictability , caused by the randomness of fatal crash incidents , reduces the accuracy of crash frequency ( i.e . , counts of fatal crashes per cluster ) which is used to measure traffic safety in practice . in this paper , a quantitative measure of traffic safety that is not significantly affected by the aforementioned variability is proposed . it introduces a fatal point -- a segment with the highest frequency of fatality -- concept based on cluster characteristics and detects them by imposing rounding errors to the hundredth decimal place of the longitude . the frequencies of the cluster and the cluster 's fatal point are combined to construct a low-sensitive quantitative measure of traffic safety for the cluster . the performance of the proposed measure of traffic safety is then studied by varying the parameter k of k-means clustering with the expectation that other clustering techniques can be adopted in a similar fashion . the 2015 north carolina fatal crash dataset of fatality analysis reporting system ( fars ) is used to evaluate the proposed fatal point concept and perform experimental analysis to determine the effectiveness of the proposed measure . the empirical study shows that the average traffic safety , measured by the proposed quantitative measure over several clusters , is not significantly affected by the variability , compared to that of the standard crash frequency .", "topics": ["cluster analysis"]}
{"title": "part of speech tagging in thai language using support vector machine", "abstract": "the elastic-input neuro tagger and hybrid tagger , combined with a neural network and brill 's error-driven learning , have already been proposed for the purpose of constructing a practical tagger using as little training data as possible . when a small thai corpus is used for training , these taggers have tagging accuracies of 94.4 % and 95.5 % ( accounting only for the ambiguous words in terms of the part of speech ) , respectively . in this study , in order to construct more accurate taggers we developed new tagging methods using three machine learning methods : the decision-list , maximum entropy , and support vector machine methods . we then performed tagging experiments by using these methods . our results showed that the support vector machine method has the best precision ( 96.1 % ) , and that it is capable of improving the accuracy of tagging in the thai language . finally , we theoretically examined all these methods and discussed how the improvements were achived .", "topics": ["test set", "support vector machine"]}
{"title": "a discriminative event based model for alzheimer 's disease progression modeling", "abstract": "the event-based model ( ebm ) for data-driven disease progression modeling estimates the sequence in which biomarkers for a disease become abnormal . this helps in understanding the dynamics of disease progression and facilitates early diagnosis by staging patients on a disease progression timeline . existing ebm methods are all generative in nature . in this work we propose a novel discriminative approach to ebm , which is shown to be more accurate as well as computationally more efficient than existing state-of-the art ebm methods . the method first estimates for each subject an approximate ordering of events , by ranking the posterior probabilities of individual biomarkers being abnormal . subsequently , the central ordering over all subjects is estimated by fitting a generalized mallows model to these approximate subject-specific orderings based on a novel probabilistic kendall 's tau distance . to evaluate the accuracy , we performed extensive experiments on synthetic data simulating the progression of alzheimer 's disease . subsequently , the method was applied to the alzheimer 's disease neuroimaging initiative ( adni ) data to estimate the central event ordering in the dataset . the experiments benchmark the accuracy of the new model under various conditions and compare it with existing state-of-the-art ebm methods . the results indicate that discriminative ebm could be a simple and elegant approach to disease progression modeling .", "topics": ["approximation algorithm", "synthetic data"]}
{"title": "deepbrain : functional representation of neural in-situ hybridization images for gene ontology classification using deep convolutional autoencoders", "abstract": "this paper presents a novel deep learning-based method for learning a functional representation of mammalian neural images . the method uses a deep convolutional denoising autoencoder ( cdae ) for generating an invariant , compact representation of in situ hybridization ( ish ) images . while most existing methods for bio-imaging analysis were not developed to handle images with highly complex anatomical structures , the results presented in this paper show that functional representation extracted by cdae can help learn features of functional gene ontology categories for their classification in a highly accurate manner . using this cdae representation , our method outperforms the previous state-of-the-art classification rate , by improving the average auc from 0.92 to 0.98 , i.e . , achieving 75 % reduction in error . the method operates on input images that were downsampled significantly with respect to the original ones to make it computationally feasible .", "topics": ["noise reduction", "autoencoder"]}
{"title": "lasso type classifiers with a reject option", "abstract": "we consider the problem of binary classification where one can , for a particular cost , choose not to classify an observation . we present a simple proof for the oracle inequality for the excess risk of structural risk minimizers using a lasso type penalty .", "topics": ["feature vector"]}
{"title": "predator confusion is sufficient to evolve swarming behavior", "abstract": "swarming behaviors in animals have been extensively studied due to their implications for the evolution of cooperation , social cognition , and predator-prey dynamics . an important goal of these studies is discerning which evolutionary pressures favor the formation of swarms . one hypothesis is that swarms arise because the presence of multiple moving prey in swarms causes confusion for attacking predators , but it remains unclear how important this selective force is . using an evolutionary model of a predator-prey system , we show that predator confusion provides a sufficient selection pressure to evolve swarming behavior in prey . furthermore , we demonstrate that the evolutionary effect of predator confusion on prey could in turn exert pressure on the structure of the predator 's visual field , favoring the frontally oriented , high-resolution visual systems commonly observed in predators that feed on swarming animals . finally , we provide evidence that when prey evolve swarming in response to predator confusion , there is a change in the shape of the functional response curve describing the predator 's consumption rate as prey density increases . thus , we show that a relatively simple perceptual constraint -- predator confusion -- could have pervasive evolutionary effects on prey behavior , predator sensory mechanisms , and the ecological interactions between predators and prey .", "topics": ["interaction"]}
{"title": "a novel vhr image change detection algorithm based on image fusion and fuzzy c-means clustering", "abstract": "this thesis describes a study to perform change detection on very high resolution satellite images using image fusion based on 2d discrete wavelet transform and fuzzy c-means clustering algorithm . multiple other methods are also quantitatively and qualitatively compared in this study .", "topics": ["cluster analysis"]}
{"title": "handling uncertainties in svm classification", "abstract": "this paper addresses the pattern classification problem arising when available target data include some uncertainty information . target data considered here is either qualitative ( a class label ) or quantitative ( an estimation of the posterior probability ) . our main contribution is a svm inspired formulation of this problem allowing to take into account class label through a hinge loss as well as probability estimates using epsilon-insensitive cost function together with a minimum norm ( maximum margin ) objective . this formulation shows a dual form leading to a quadratic problem and allows the use of a representer theorem and associated kernel . the solution provided can be used for both decision and posterior probability estimation . based on empirical evidence our method outperforms regular svm in terms of probability predictions and classification performances .", "topics": ["statistical classification", "loss function"]}
{"title": "exploring strategy-proofness , uniqueness , and pareto optimality for the stable matching problem with couples", "abstract": "the stable matching problem with couples ( smp-c ) is a ubiquitous real-world extension of the stable matching problem ( smp ) involving complementarities . although smp can be solved in polynomial time , smp-c is np-complete . hence , it is not clear which , if any , of the theoretical results surrounding the canonical smp problem apply in this setting . in this paper , we use a recently-developed sat encoding to solve smp-c exactly . this allows us to enumerate all stable matchings for any given instance of smp-c. with this tool , we empirically evaluate some of the properties that have been hypothesized to hold for smp-c . we take particular interest in investigating if , as the size of the market grows , the percentage of instances with unique stable matchings also grows . while we did not find this trend among the random problem instances we sampled , we did find that the percentage of instances with an resident optimal matching seems to more closely follow the trends predicted by previous conjectures . we also define and investigate resident pareto optimal stable matchings , finding that , even though this is important desideratum for the deferred acceptance style algorithms previously designed to solve smp-c , they do not always find one . we also investigate strategy-proofness for smp-c , showing that even if only one stable matching exists , residents still have incentive to misreport their preferences . however , if a problem has a resident optimal stable matching , we show that residents can not manipulate via truncation .", "topics": ["time complexity", "polynomial"]}
{"title": "meta learning framework for automated driving", "abstract": "the success of automated driving deployment is highly depending on the ability to develop an efficient and safe driving policy . the problem is well formulated under the framework of optimal control as a cost optimization problem . model based solutions using traditional planning are efficient , but require the knowledge of the environment model . on the other hand , model free solutions suffer sample inefficiency and require too many interactions with the environment , which is infeasible in practice . methods under the reinforcement learning framework usually require the notion of a reward function , which is not available in the real world . imitation learning helps in improving sample efficiency by introducing prior knowledge obtained from the demonstrated behavior , on the risk of exact behavior cloning without generalizing to unseen environments . in this paper we propose a meta learning framework , based on data set aggregation , to improve generalization of imitation learning algorithms . under the proposed framework , we propose metadagger , a novel algorithm which tackles the generalization issues in traditional imitation learning . we use the open race car simulator ( torcs ) to test our algorithm . results on unseen test tracks show significant improvement over traditional imitation learning algorithms , improving the learning time and sample efficiency in the same time . the results are also supported by visualization of the learnt features to prove generalization of the captured details .", "topics": ["optimization problem", "reinforcement learning"]}
{"title": "food recognition and recipe analysis : integrating visual content , context and external knowledge", "abstract": "the central role of food in our individual and social life , combined with recent technological advances , has motivated a growing interest in applications that help to better monitor dietary habits as well as the exploration and retrieval of food-related information . we review how visual content , context and external knowledge can be integrated effectively into food-oriented applications , with special focus on recipe analysis and retrieval , food recommendation , and the restaurant context as emerging directions .", "topics": ["computer vision", "artificial intelligence"]}
{"title": "from photo streams to evolving situations", "abstract": "photos are becoming spontaneous , objective , and universal sources of information . this paper develops evolving situation recognition using photo streams coming from disparate sources combined with the advances of deep learning . using visual concepts in photos together with space and time information , we formulate the situation detection into a semi-supervised learning framework and propose new graph-based models to solve the problem . to extend the method for unknown situations , we introduce a soft label method which enables the traditional semi-supervised learning framework to accurately predict predefined labels as well as effectively form new clusters . to overcome the noisy data which degrades graph quality , leading to poor recognition results , we take advantage of two kinds of noise-robust norms which can eliminate the adverse effects of outliers in visual concepts and improve the accuracy of situation recognition . finally , we demonstrate the idea and the effectiveness of the proposed model on yahoo flickr creative commons 100 million .", "topics": ["supervised learning"]}
{"title": "dynamic zoom-in network for fast object detection in large images", "abstract": "we introduce a generic framework that reduces the computational cost of object detection while retaining accuracy for scenarios where objects with varied sizes appear in high resolution images . detection progresses in a coarse-to-fine manner , first on a down-sampled version of the image and then on a sequence of higher resolution regions identified as likely to improve the detection accuracy . built upon reinforcement learning , our approach consists of a model ( r-net ) that uses coarse detection results to predict the potential accuracy gain for analyzing a region at a higher resolution and another model ( q-net ) that sequentially selects regions to zoom in . experiments on the caltech pedestrians dataset show that our approach reduces the number of processed pixels by over 50 % without a drop in detection accuracy . the merits of our approach become more significant on a high resolution test set collected from yfcc100m dataset , where our approach maintains high detection performance while reducing the number of processed pixels by about 70 % and the detection time by over 50 % .", "topics": ["test set", "object detection"]}
{"title": "on theorem 2.3 in `` prediction , learning , and games '' by cesa-bianchi and lugosi", "abstract": "the note presents a modified proof of a loss bound for the exponentially weighted average forecaster with time-varying potential . the regret term of the algorithm is upper-bounded by sqrt { n ln ( n ) } ( uniformly in n ) , where n is the number of experts and n is the number of steps .", "topics": ["regret ( decision theory )", "loss function"]}
{"title": "automatic detection and decoding of honey bee waggle dances", "abstract": "the waggle dance is one of the most popular examples of animal communication . forager bees direct their nestmates to profitable resources via a complex motor display . essentially , the dance encodes the polar coordinates to the resource in the field . unemployed foragers follow the dancer 's movements and then search for the advertised spots in the field . throughout the last decades , biologists have employed different techniques to measure key characteristics of the waggle dance and decode the information it conveys . early techniques involved the use of protractors and stopwatches to measure the dance orientation and duration directly from the observation hive . recent approaches employ digital video recordings and manual measurements on screen . however , manual approaches are very time-consuming . most studies , therefore , regard only small numbers of animals in short periods of time . we have developed a system capable of automatically detecting , decoding and mapping communication dances in real-time . in this paper , we describe our recording setup , the image processing steps performed for dance detection and decoding and an algorithm to map dances to the field . the proposed system performs with a detection accuracy of 90.07\\ % . the decoded waggle orientation has an average error of -2.92 { \\deg } ( $ \\pm $ 7.37 { \\deg } ) , well within the range of human error . to evaluate and exemplify the system 's performance , a group of bees was trained to an artificial feeder , and all dances in the colony were automatically detected , decoded and mapped . the system presented here is the first of this kind made publicly available , including source code and hardware specifications . we hope this will foster quantitative analyses of the honey bee waggle dance .", "topics": ["image processing"]}
{"title": "detecting and recognizing human-object interactions", "abstract": "to understand the visual world , a machine must not only recognize individual object instances but also how they interact . humans are often at the center of such interactions and detecting human-object interactions is an important practical and scientific problem . in this paper , we address the task of detecting < human , verb , object > triplets in challenging everyday photos . we propose a novel model that is driven by a human-centric approach . our hypothesis is that the appearance of a person -- their pose , clothing , action -- is a powerful cue for localizing the objects they are interacting with . to exploit this cue , our model learns to predict an action-specific density over target object locations based on the appearance of a detected person . our model also jointly learns to detect people and objects , and by fusing these predictions it efficiently infers interaction triplets in a clean , jointly trained end-to-end system we call interactnet . we validate our approach on the recently introduced verbs in coco ( v-coco ) and hico-det datasets , where we show quantitatively compelling results .", "topics": ["interaction", "end-to-end principle"]}
{"title": "a saak transform approach to efficient , scalable and robust handwritten digits recognition", "abstract": "an efficient , scalable and robust approach to the handwritten digits recognition problem based on the saak transform is proposed in this work . first , multi-stage saak transforms are used to extract a family of joint spatial-spectral representations of input images . then , the saak coefficients are used as features and fed into the svm classifier for the classification task . in order to control the size of saak coefficients , we adopt a lossy saak transform that uses the principal component analysis ( pca ) to select a smaller set of transform kernels . the handwritten digits recognition problem is well solved by the convolutional neural network ( cnn ) such as the lenet-5 . we conduct a comparative study on the performance of the lenet-5 and the saak-transform-based solutions in terms of scalability and robustness as well as the efficiency of lossless and lossy saak transforms under a comparable accuracy level .", "topics": ["scalability", "mnist database"]}
{"title": "predicting visual exemplars of unseen classes for zero-shot learning", "abstract": "leveraging class semantic descriptions and examples of known objects , zero-shot learning makes it possible to train a recognition model for an object class whose examples are not available . in this paper , we propose a novel zero-shot learning model that takes advantage of clustering structures in the semantic embedding space . the key idea is to impose the structural constraint that semantic representations must be predictive of the locations of their corresponding visual exemplars . to this end , this reduces to training multiple kernel-based regressors from semantic representation-exemplar pairs from labeled data of the seen object categories . despite its simplicity , our approach significantly outperforms existing zero-shot learning methods on standard benchmark datasets , including the imagenet dataset with more than 20,000 unseen categories .", "topics": ["cluster analysis"]}
{"title": "binary classification from positive-confidence data", "abstract": "reducing labeling costs in supervised learning is a critical issue in many practical machine learning applications . in this paper , we consider positive-confidence ( pconf ) classification , the problem of training a binary classifier only from positive data equipped with confidence . pconf classification can be regarded as a discriminative extension of one-class classification ( which is aimed at `` describing '' the positive class by clustering-related methods ) , with ability to tune hyper-parameters for `` classifying '' positive and negative samples . pconf classification is also related to positive-unlabeled ( pu ) classification ( which uses hard-labeled positive data and unlabeled data ) , but the difference is that it enables us to avoid estimating the class priors , which is a critical bottleneck in typical pu classification methods . for the pconf classification problem , we provide a simple empirical risk minimization framework and give a formulation for linear-in-parameter models that can be implemented easily and computationally efficiently . we also theoretically establish the consistency and estimation error bound for pconf classification , and demonstrate the practical usefulness of the proposed method for deep neural networks through experiments .", "topics": ["supervised learning"]}
{"title": "video segmentation via diffusion bases", "abstract": "identifying moving objects in a video sequence , which is produced by a static camera , is a fundamental and critical task in many computer-vision applications . a common approach performs background subtraction , which identifies moving objects as the portion of a video frame that differs significantly from a background model . a good background subtraction algorithm has to be robust to changes in the illumination and it should avoid detecting non-stationary background objects such as moving leaves , rain , snow , and shadows . in addition , the internal background model should quickly respond to changes in background such as objects that start to move or stop . we present a new algorithm for video segmentation that processes the input video sequence as a 3d matrix where the third axis is the time domain . our approach identifies the background by reducing the input dimension using the \\emph { diffusion bases } methodology . furthermore , we describe an iterative method for extracting and deleting the background . the algorithm has two versions and thus covers the complete range of backgrounds : one for scenes with static backgrounds and the other for scenes with dynamic ( moving ) backgrounds .", "topics": ["computer vision"]}
{"title": "structure selection from streaming relational data", "abstract": "statistical relational learning techniques have been successfully applied in a wide range of relational domains . in most of these applications , the human designers capitalized on their background knowledge by following a trial-and-error trajectory , where relational features are manually defined by a human engineer , parameters are learned for those features on the training data , the resulting model is validated , and the cycle repeats as the engineer adjusts the set of features . this paper seeks to streamline application development in large relational domains by introducing a light-weight approach that efficiently evaluates relational features on pieces of the relational graph that are streamed to it one at a time . we evaluate our approach on two social media tasks and demonstrate that it leads to more accurate models that are learned faster .", "topics": ["test set"]}
{"title": "distributed submodular maximization", "abstract": "many large-scale machine learning problems -- clustering , non-parametric learning , kernel machines , etc . -- require selecting a small yet representative subset from a large dataset . such problems can often be reduced to maximizing a submodular set function subject to various constraints . classical approaches to submodular optimization require centralized access to the full dataset , which is impractical for truly large-scale problems . in this paper , we consider the problem of submodular function maximization in a distributed fashion . we develop a simple , two-stage protocol greedi , that is easily implemented using mapreduce style computations . we theoretically analyze our approach , and show that under certain natural conditions , performance close to the centralized approach can be achieved . we begin with monotone submodular maximization subject to a cardinality constraint , and then extend this approach to obtain approximation guarantees for ( not necessarily monotone ) submodular maximization subject to more general constraints including matroid or knapsack constraints . in our extensive experiments , we demonstrate the effectiveness of our approach on several applications , including sparse gaussian process inference and exemplar based clustering on tens of millions of examples using hadoop .", "topics": ["cluster analysis", "sparse matrix"]}
{"title": "learning to create and reuse words in open-vocabulary neural language modeling", "abstract": "fixed-vocabulary language models fail to account for one of the most characteristic statistical facts of natural language : the frequent creation and reuse of new word types . although character-level language models offer a partial solution in that they can create word types not attested in the training corpus , they do not capture the `` bursty '' distribution of such words . in this paper , we augment a hierarchical lstm language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words . to validate our model we construct a new open-vocabulary language modeling corpus ( the multilingual wikipedia corpus , mwc ) from comparable wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages .", "topics": ["natural language"]}
{"title": "quicknat : segmenting mri neuroanatomy in 20 seconds", "abstract": "whole brain segmentation from structural magnetic resonance imaging is a prerequisite for most morphological analyses , but requires hours of processing time and therefore delays the availability of image markers after scan acquisition . we introduce quicknat , a fully convolution neural network that segments a brain scan in 20 seconds . to enable training of the complex network with limited annotated data , we propose to pre-train on auxiliary labels created from existing segmentation software and to subsequently fine-tune on manual labels . in an extensive set of evaluations on eight datasets that cover a wide age range , pathology , and different scanners , we demonstrate that quicknat achieves superior performance to state-of-the-art methods , while being about 700 times faster . this drastic speed up greatly facilitates the processing of large data repositories and supports the translation of imaging biomarkers by making them almost instantaneously available .", "topics": ["convolution"]}
{"title": "weighted transformer network for machine translation", "abstract": "state-of-the-art results on neural machine translation often use attentional sequence-to-sequence models with some form of convolution or recursion . vaswani et al . ( 2017 ) propose a new architecture that avoids recurrence and convolution completely . instead , it uses only self-attention and feed-forward layers . while the proposed architecture achieves state-of-the-art results on several machine translation tasks , it requires a large number of parameters and training iterations to converge . we propose weighted transformer , a transformer with modified attention layers , that not only outperforms the baseline network in bleu score but also converges 15-40 % faster . specifically , we replace the multi-head attention by multiple self-attention branches that the model learns to combine during the training process . our model improves the state-of-the-art performance by 0.5 bleu points on the wmt 2014 english-to-german translation task and by 0.4 on the english-to-french translation task .", "topics": ["baseline ( configuration management )", "machine translation"]}
{"title": "automatic segmentation of lizard spots using an active contour model", "abstract": "animal biometrics is a challenging task . in the literature , many algorithms have been used , e.g . penguin chest recognition , elephant ears recognition and leopard stripes pattern recognition , but to use technology to a large extent in this area of research , still a lot of work has to be done . one important target in animal biometrics is to automate the segmentation process , so in this paper we propose a segmentation algorithm for extracting the spots of diploglossus millepunctatus , an endangered lizard species . the automatic segmentation is achieved with a combination of preprocessing , active contours and morphology . the parameters of each stage of the segmentation algorithm are found using an optimization procedure , which is guided by the ground truth . the results show that automatic segmentation of spots is possible . a 78.37 % of correct segmentation in average is reached .", "topics": ["ground truth"]}
{"title": "training and evaluating multimodal word embeddings with large-scale web annotated images", "abstract": "in this paper , we focus on training and evaluating effective word embeddings with both text and visual information . more specifically , we introduce a large-scale dataset with 300 million sentences describing over 40 million images crawled and downloaded from publicly available pins ( i.e . an image with sentence descriptions uploaded by users ) on pinterest . this dataset is more than 200 times larger than ms coco , the standard large-scale image dataset with sentence descriptions . in addition , we construct an evaluation dataset to directly assess the effectiveness of word embeddings in terms of finding semantically similar or related words and phrases . the word/phrase pairs in this evaluation dataset are collected from the click data with millions of users in an image search system , thus contain rich semantic relationships . based on these datasets , we propose and compare several recurrent neural networks ( rnns ) based multimodal ( text and image ) models . experiments show that our model benefits from incorporating the visual information into the word embeddings , and a weight sharing strategy is crucial for learning such multimodal embeddings . the project page is : http : //www.stat.ucla.edu/~junhua.mao/multimodal_embedding.html", "topics": ["recurrent neural network"]}
{"title": "input-output non-linear dynamical systems applied to physiological condition monitoring", "abstract": "we present a non-linear dynamical system for modelling the effect of drug infusions on the vital signs of patients admitted in intensive care units ( icus ) . more specifically we are interested in modelling the effect of a widely used anaesthetic drug ( propofol ) on a patient 's monitored depth of anaesthesia and haemodynamics . we compare our approach with one from the pharmacokinetics/pharmacodynamics ( pk/pd ) literature and show that we can provide significant improvements in performance without requiring the incorporation of expert physiological knowledge in our system .", "topics": ["nonlinear system"]}
{"title": "neural responding machine for short-text conversation", "abstract": "we propose neural responding machine ( nrm ) , a neural network-based response generator for short-text conversation . nrm takes the general encoder-decoder framework : it formalizes the generation of response as a decoding process based on the latent representation of the input text , while both encoding and decoding are realized with recurrent neural networks ( rnn ) . the nrm is trained with a large amount of one-round conversation data collected from a microblogging service . empirical study shows that nrm can generate grammatically correct and content-wise appropriate responses to over 75 % of the input text , outperforming state-of-the-arts in the same setting , including retrieval-based and smt-based models .", "topics": ["recurrent neural network"]}
{"title": "visual translation embedding network for visual relation detection", "abstract": "visual relations , such as `` person ride bike '' and `` bike next to car '' , offer a comprehensive scene understanding of an image , and have already shown their great utility in connecting computer vision and natural language . however , due to the challenging combinatorial complexity of modeling subject-predicate-object relation triplets , very little work has been done to localize and predict visual relations . inspired by the recent advances in relational representation learning of knowledge bases and convolutional object detection networks , we propose a visual translation embedding network ( vtranse ) for visual relation detection . vtranse places objects in a low-dimensional relation space where a relation can be modeled as a simple vector translation , i.e . , subject + predicate $ \\approx $ object . we propose a novel feature extraction layer that enables object-relation knowledge transfer in a fully-convolutional fashion that supports training and inference in a single forward/backward pass . to the best of our knowledge , vtranse is the first end-to-end relation detection network . we demonstrate the effectiveness of vtranse over other state-of-the-art methods on two large-scale datasets : visual relationship and visual genome . note that even though vtranse is a purely visual model , it is still competitive to the lu 's multi-modal model with language priors .", "topics": ["feature learning", "object detection"]}
{"title": "statistical compressed sensing of gaussian mixture models", "abstract": "a novel framework of compressed sensing , namely statistical compressed sensing ( scs ) , that aims at efficiently sampling a collection of signals that follow a statistical distribution , and achieving accurate reconstruction on average , is introduced . scs based on gaussian models is investigated in depth . for signals that follow a single gaussian model , with gaussian or bernoulli sensing matrices of o ( k ) measurements , considerably smaller than the o ( k log ( n/k ) ) required by conventional cs based on sparse models , where n is the signal dimension , and with an optimal decoder implemented via linear filtering , significantly faster than the pursuit decoders applied in conventional cs , the error of scs is shown tightly upper bounded by a constant times the best k-term approximation error , with overwhelming probability . the failure probability is also significantly smaller than that of conventional sparsity-oriented cs . stronger yet simpler results further show that for any sensing matrix , the error of gaussian scs is upper bounded by a constant times the best k-term approximation with probability one , and the bound constant can be efficiently calculated . for gaussian mixture models ( gmms ) , that assume multiple gaussian distributions and that each signal follows one of them with an unknown index , a piecewise linear estimator is introduced to decode scs . the accuracy of model selection , at the heart of the piecewise linear decoder , is analyzed in terms of the properties of the gaussian distributions and the number of sensing measurements . a maximum a posteriori expectation-maximization algorithm that iteratively estimates the gaussian models parameters , the signals model selection , and decodes the signals , is presented for gmm-based scs . in real image sensing applications , gmm-based scs is shown to lead to improved results compared to conventional cs , at a considerably lower computational cost .", "topics": ["sampling ( signal processing )", "approximation"]}
{"title": "agent behavior prediction and its generalization analysis", "abstract": "machine learning algorithms have been applied to predict agent behaviors in real-world dynamic systems , such as advertiser behaviors in sponsored search and worker behaviors in crowdsourcing . the behavior data in these systems are generated by live agents : once the systems change due to the adoption of the prediction models learnt from the behavior data , agents will observe and respond to these changes by changing their own behaviors accordingly . as a result , the behavior data will evolve and will not be identically and independently distributed , posing great challenges to the theoretical analysis on the machine learning algorithms for behavior prediction . to tackle this challenge , in this paper , we propose to use markov chain in random environments ( mcre ) to describe the behavior data , and perform generalization analysis of the machine learning algorithms on its basis . since the one-step transition probability matrix of mcre depends on both previous states and the random environment , conventional techniques for generalization analysis can not be directly applied . to address this issue , we propose a novel technique that transforms the original mcre into a higher-dimensional time-homogeneous markov chain . the new markov chain involves more variables but is more regular , and thus easier to deal with . we prove the convergence of the new markov chain when time approaches infinity . then we prove a generalization bound for the machine learning algorithms on the behavior data generated by the new markov chain , which depends on both the markovian parameters and the covering number of the function class compounded by the loss function for behavior prediction and the behavior prediction model . to the best of our knowledge , this is the first work that performs the generalization analysis on data generated by complex processes in real-world dynamic systems .", "topics": ["markov chain"]}
{"title": "a lagrangian gauss-newton-krylov solver for mass- and intensity-preserving diffeomorphic image registration", "abstract": "we present an efficient solver for diffeomorphic image registration problems in the framework of large deformations diffeomorphic metric mappings ( lddmm ) . we use an optimal control formulation , in which the velocity field of a hyperbolic pde needs to be found such that the distance between the final state of the system ( the transformed/transported template image ) and the observation ( the reference image ) is minimized . our solver supports both stationary and non-stationary ( i.e . , transient or time-dependent ) velocity fields . as transformation models , we consider both the transport equation ( assuming intensities are preserved during the deformation ) and the continuity equation ( assuming mass-preservation ) . we consider the reduced form of the optimal control problem and solve the resulting unconstrained optimization problem using a discretize-then-optimize approach . a key contribution is the elimination of the pde constraint using a lagrangian hyperbolic pde solver . lagrangian methods rely on the concept of characteristic curves that we approximate here using a fourth-order runge-kutta method . we also present an efficient algorithm for computing the derivatives of final state of the system with respect to the velocity field . this allows us to use fast gauss-newton based methods . we present quickly converging iterative linear solvers using spectral preconditioners that render the overall optimization efficient and scalable . our method is embedded into the image registration framework fair and , thus , supports the most commonly used similarity measures and regularization functionals . we demonstrate the potential of our new approach using several synthetic and real world test problems with up to 14.7 million degrees of freedom .", "topics": ["approximation algorithm", "optimization problem"]}
{"title": "convolutional neural networks combined with runge-kutta methods", "abstract": "a convolutional neural network for image classification can be constructed following some mathematical ways since it models the ventral stream in visual cortex which is regarded as a multi-period dynamical system . in this paper , a new point of view is proposed for constructing network models as well as providing a direction to get inspiration or explanation for neural network . if each period in ventral stream was deemed to be a dynamical system with time as the independent variable , there should be a set of ordinary differential equations ( odes ) for this system . runge-kutta methods are common means to solve ode . thus , network model ought to be built using these methods . moreover , convolutional networks could be employed to emulate the increments within every time-step . the model constructed in the above way is named runge-kutta convolutional neural network ( rknet ) . according to this idea , dense convolutional networks ( densenets ) were varied to rknets . to prove the feasibility of rknets , these variants were verified on benchmark datasets , cifar and imagenet . the experimental results show that the rknets transformed from densenets gained similar or even higher parameter efficiency . the success of the experiments denotes that runge-kutta methods can be utilized to construct convolutional neural networks for image classification efficiently . furthermore , the network models might be structured more rationally in the future basing on rknet and priori knowledge .", "topics": ["computer vision"]}
{"title": "neyman-pearson classification under high-dimensional settings", "abstract": "most existing binary classification methods target on the optimization of the overall classification risk and may fail to serve some real-world applications such as cancer diagnosis , where users are more concerned with the risk of misclassifying one specific class than the other . neyman-pearson ( np ) paradigm was introduced in this context as a novel statistical framework for handling asymmetric type i/ii error priorities . it seeks classifiers with a minimal type ii error and a constrained type i error under a user specified level . this article is the first attempt to construct classifiers with guaranteed theoretical performance under the np paradigm in high-dimensional settings . based on the fundamental neyman-pearson lemma , we used a plug-in approach to construct np-type classifiers for naive bayes models . the proposed classifiers satisfy the np oracle inequalities , which are natural np paradigm counterparts of the oracle inequalities in classical binary classification . besides their desirable theoretical properties , we also demonstrated their numerical advantages in prioritized error control via both simulation and real data studies .", "topics": ["numerical analysis", "simulation"]}
{"title": "optimisation of photometric stereo methods by non-convex variational minimisation", "abstract": "estimating shape and appearance of a three dimensional object from a given set of images is a classic research topic that is still actively pursued . among the various techniques available , ps is distinguished by the assumption that the underlying input images are taken from the same point of view but under different lighting conditions . the most common techniques provide the shape information in terms of surface normals . in this work , we instead propose to minimise a much more natural objective function , namely the reprojection error in terms of depth . minimising the resulting non-trivial variational model for ps allows to recover the depth of the photographed scene directly . as a solving strategy , we follow an approach based on a recently published optimisation scheme for non-convex and non-smooth cost functions . the main contributions of our paper are of theoretical nature . a technical novelty in our framework is the usage of matrix differential calculus . we supplement our approach by a detailed convergence analysis of the resulting optimisation algorithm and discuss possibilities to ease the computational complexity . at hand of an experimental evaluation we discuss important properties of the method . overall , our strategy achieves more accurate results than competing approaches . the experiments also highlights some practical aspects of the underlying optimisation algorithm that may be of interest in a more general context .", "topics": ["calculus of variations", "computational complexity theory"]}
{"title": "boosting with the logistic loss is consistent", "abstract": "this manuscript provides optimization guarantees , generalization bounds , and statistical consistency results for adaboost variants which replace the exponential loss with the logistic and similar losses ( specifically , twice differentiable convex losses which are lipschitz and tend to zero on one side ) . the heart of the analysis is to show that , in lieu of explicit regularization and constraints , the structure of the problem is fairly rigidly controlled by the source distribution itself . the first control of this type is in the separable case , where a distribution-dependent relaxed weak learning rate induces speedy convergence with high probability over any sample . otherwise , in the nonseparable case , the convex surrogate risk itself exhibits distribution-dependent levels of curvature , and consequently the algorithm 's output has small norm with high probability .", "topics": ["time complexity", "matrix regularization"]}
{"title": "quantum fuzzy sets : blending fuzzy set theory and quantum computation", "abstract": "in this article we investigate a way in which quantum computing can be used to extend the class of fuzzy sets . the core idea is to see states of a quantum register as characteristic functions of quantum fuzzy subsets of a given set . as the real unit interval is embedded in the bloch sphere , every fuzzy set is automatically a quantum fuzzy set . however , a generic quantum fuzzy set can be seen as a ( possibly entangled ) superposition of many fuzzy sets at once , offering new opportunities for modeling uncertainty . after introducing the main framework of quantum fuzzy set theory , we analyze the standard operations of fuzzification and defuzzification from our viewpoint . we conclude this preliminary paper with a list of possible applications of quantum fuzzy sets to pattern recognition , as well as future directions of pure research in quantum fuzzy set theory .", "topics": ["computation"]}
{"title": "splatnet : sparse lattice networks for point cloud processing", "abstract": "we present a network architecture for processing point clouds that directly operates on the collection of points represented as a sparse set of samples in a high-dimensional lattice . naively applying convolutions on this lattice scales poorly both in terms of memory and computational cost as the size of the lattice increases . instead , our network uses sparse bilateral convolutional layers as building blocks . these layers maintain efficiency by using indexing structures to apply convolutions only on occupied parts of the lattice , and allow flexible specification of the lattice structure enabling hierarchical and spatially-aware feature learning , as well as joint 2d-3d reasoning . both point-based and image-based representations can be easily incorporated in a network with such layers and the resulting model can be trained in an end-to-end manner . we present results on 3d segmentation tasks where our approach outperforms existing state-of-the-art techniques .", "topics": ["feature learning", "sparse matrix"]}
{"title": "new ideas for brain modelling", "abstract": "this paper describes some biologically-inspired processes that could be used to build the sort of networks that we associate with the human brain . new to this paper , a 'refined ' neuron will be proposed . this is a group of neurons that by joining together can produce a more analogue system , but with the same level of control and reliability that a binary neuron would have . with this new structure , it will be possible to think of an essentially binary system in terms of a more variable set of values . the paper also shows how recent research associated with the new model , can be combined with established theories , to produce a more complete picture . the propositions are largely in line with conventional thinking , but possibly with one or two more radical suggestions . an earlier cognitive model can be filled in with more specific details , based on the new research results , where the components appear to fit together almost seamlessly . the intention of the research has been to describe plausible 'mechanical ' processes that can produce the appropriate brain structures and mechanisms , but that could be used without the magical 'intelligence ' part that is still not fully understood . there are also some important updates from an earlier version of this paper .", "topics": ["value ( ethics )"]}
{"title": "self-regulated artificial ant colonies on digital image habitats", "abstract": "artificial life models , swarm intelligent and evolutionary computation algorithms are usually built on fixed size populations . some studies indicate however that varying the population size can increase the adaptability of these systems and their capability to react to changing environments . in this paper we present an extended model of an artificial ant colony system designed to evolve on digital image habitats . we will show that the present swarm can adapt the size of the population according to the type of image on which it is evolving and reacting faster to changing images , thus converging more rapidly to the new desired regions , regulating the number of his image foraging agents . finally , we will show evidences that the model can be associated with the mathematical morphology watershed algorithm to improve the segmentation of digital grey-scale images . keywords : swarm intelligence , perception and image processing , pattern recognition , mathematical morphology , social cognitive maps , social foraging , self-organization , distributed search .", "topics": ["computation"]}
{"title": "information content of coevolutionary game landscapes", "abstract": "coevolutionary game dynamics is the result of players that may change their strategies and their network of interaction . for such games , and based on interpreting strategies as configurations , strategy-to-payoff maps can be defined for every interaction network , which opens up to derive game landscapes . this paper presents an analysis of these game landscapes by their information content . by this analysis , we particularly study the effect of a rescaled payoff matrix generalizing social dilemmas and differences between well-mixed and structured populations .", "topics": ["map"]}
{"title": "english-lithuanian-english machine translation lexicon and engine : current state and future work", "abstract": "this article overviews the current state of the english-lithuanian-english machine translation system . the first part of the article describes the problems that system poses today and what actions will be taken to solve them in the future . the second part of the article tackles the main issue of the translation process . article briefly overviews the word sense disambiguation for mt technique using google .", "topics": ["machine translation"]}
{"title": "recurrent batch normalization", "abstract": "we propose a reparameterization of lstm that brings the benefits of batch normalization to recurrent neural networks . whereas previous works only apply batch normalization to the input-to-hidden transformation of rnns , we demonstrate that it is both possible and beneficial to batch-normalize the hidden-to-hidden transition , thereby reducing internal covariate shift between time steps . we evaluate our proposal on various sequential problems such as sequence classification , language modeling and question answering . our empirical results show that our batch-normalized lstm consistently leads to faster convergence and improved generalization .", "topics": ["recurrent neural network"]}
{"title": "robust kronecker-decomposable component analysis for low-rank modeling", "abstract": "dictionary learning and component analysis are part of one of the most well-studied and active research fields , at the intersection of signal and image processing , computer vision , and statistical machine learning . in dictionary learning , the current methods of choice are arguably k-svd and its variants , which learn a dictionary ( i.e . , a decomposition ) for sparse coding via singular value decomposition . in robust component analysis , leading methods derive from principal component pursuit ( pcp ) , which recovers a low-rank matrix from sparse corruptions of unknown magnitude and support . however , k-svd is sensitive to the presence of noise and outliers in the training set . additionally , pcp does not provide a dictionary that respects the structure of the data ( e.g . , images ) , and requires expensive svd computations when solved by convex relaxation . in this paper , we introduce a new robust decomposition of images by combining ideas from sparse dictionary learning and pcp . we propose a novel kronecker-decomposable component analysis which is robust to gross corruption , can be used for low-rank modeling , and leverages separability to solve significantly smaller problems . we design an efficient learning algorithm by drawing links with a restricted form of tensor factorization . the effectiveness of the proposed approach is demonstrated on real-world applications , namely background subtraction and image denoising , by performing a thorough comparison with the current state of the art .", "topics": ["image processing", "noise reduction"]}
{"title": "a study of scaling issues in bayesian belief networks for ship classification", "abstract": "the problems associated with scaling involve active and challenging research topics in the area of artificial intelligence . the purpose is to solve real world problems by means of ai technologies , in cases where the complexity of representation of the real world problem is potentially combinatorial . in this paper , we present a novel approach to cope with the scaling issues in bayesian belief networks for ship classification . the proposed approach divides the conceptual model of a complex ship classification problem into a set of small modules that work together to solve the classification problem while preserving the functionality of the original model . the possible ways of explaining sensor returns ( e.g . , the evidence ) for some features , such as portholes along the length of a ship , are sometimes combinatorial . thus , using an exhaustive approach , which entails the enumeration of all possible explanations , is impractical for larger problems . we present a network structure ( referred to as sequential decomposition , sd ) in which each observation is associated with a set of legitimate outcomes which are consistent with the explanation of each observed piece of evidence . the results show that the sd approach allows one to represent feature-observation relations in a manageable way and achieve the same explanatory power as an exhaustive approach .", "topics": ["bayesian network", "artificial intelligence"]}
{"title": "using crowdsourcing system for creating site-specific statistical machine translation engine", "abstract": "a crowdsourcing translation approach is an effective tool for globalization of site content , but it is also an important source of parallel linguistic data . for the given site , processed with a crowdsourcing system , a sentence-aligned corpus can be fetched , which covers a very narrow domain of terminology and language patterns - a site-specific domain . these data can be used for training and estimation of site-specific statistical machine translation engine", "topics": ["test set", "machine translation"]}
{"title": "unsupervised feature learning for writer identification and writer retrieval", "abstract": "deep convolutional neural networks ( cnn ) have shown great success in supervised classification tasks such as character classification or dating . deep learning methods typically need a lot of annotated training data , which is not available in many scenarios . in these cases , traditional methods are often better than or equivalent to deep learning methods . in this paper , we propose a simple , yet effective , way to learn cnn activation features in an unsupervised manner . therefore , we train a deep residual network using surrogate classes . the surrogate classes are created by clustering the training dataset , where each cluster index represents one surrogate class . the activations from the penultimate cnn layer serve as features for subsequent classification tasks . we evaluate the feature representations on two publicly available datasets . the focus lies on the icdar17 competition dataset on historical document writer identification ( historical-wi ) . we show that the activation features trained without supervision are superior to descriptors of state-of-the-art writer identification methods . additionally , we achieve comparable results in the case of handwriting classification using the icfhr16 competition dataset on historical latin script types ( clamm16 ) .", "topics": ["feature learning", "test set"]}
{"title": "structural weights in ontology matching", "abstract": "ontology matching finds correspondences between similar entities of different ontologies . two ontologies may be similar in some aspects such as structure , semantic etc . most ontology matching systems integrate multiple matchers to extract all the similarities that two ontologies may have . thus , we face a major problem to aggregate different similarities . some matching systems use experimental weights for aggregation of similarities among different matchers while others use machine learning approaches and optimization algorithms to find optimal weights to assign to different matchers . however , both approaches have their own deficiencies . in this paper , we will point out the problems and shortcomings of current similarity aggregation strategies . then , we propose a new strategy , which enables us to utilize the structural information of ontologies to get weights of matchers , for the similarity aggregation task . for achieving this goal , we create a new ontology matching system which it uses three available matchers , namely gmo , isub and vdoc . we have tested our similarity aggregation strategy on the oaei 2012 data set . experimental results show significant improvements in accuracies of several cases , especially in matching the classes of ontologies . we will compare the performance of our similarity aggregation strategy with other well-known strategies", "topics": ["entity"]}
{"title": "evolutionary optimization in an algorithmic setting", "abstract": "evolutionary processes proved very useful for solving optimization problems . in this work , we build a formalization of the notion of cooperation and competition of multiple systems working toward a common optimization goal of the population using evolutionary computation techniques . it is justified that evolutionary algorithms are more expressive than conventional recursive algorithms . three subclasses of evolutionary algorithms are proposed here : bounded finite , unbounded finite and infinite types . some results on completeness , optimality and search decidability for the above classes are presented . a natural extension of evolutionary turing machine model developed in this paper allows one to mathematically represent and study properties of cooperation and competition in a population of optimized species .", "topics": ["mathematical optimization", "computation"]}
{"title": "long short-term memory ( lstm ) networks with jet constituents for boosted top tagging at the lhc", "abstract": "multivariate techniques based on engineered features have found wide adoption in the identification of jets resulting from hadronic top decays at the large hadron collider ( lhc ) . recent deep learning developments in this area include the treatment of the calorimeter activation as an image or supplying a list of jet constituent momenta to a fully connected network . this latter approach lends itself well to the use of recurrent neural networks . in this work the applicability of architectures incorporating long short-term memory ( lstm ) networks is explored . several network architectures , methods of ordering of jet constituents , and input pre-processing are studied . the best performing lstm network achieves a background rejection of 100 for 50 % signal efficiency . this represents more than a factor of two improvement over a fully connected deep neural network ( dnn ) trained on similar types of inputs .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "slac : a sparsely labeled dataset for action classification and localization", "abstract": "this paper describes a procedure for the creation of large-scale video datasets for action classification and localization from unconstrained , realistic web data . the scalability of the proposed procedure is demonstrated by building a novel video benchmark , named slac ( sparsely labeled actions ) , consisting of over 520k untrimmed videos and 1.75m clip annotations spanning 200 action categories . using our proposed framework , annotating a clip takes merely 8.8 seconds on average . this represents a saving in labeling time of over 95 % compared to the traditional procedure of manual trimming and localization of actions . our approach dramatically reduces the amount of human labeling by automatically identifying hard clips , i.e . , clips that contain coherent actions but lead to prediction disagreement between action classifiers . a human annotator can disambiguate whether such a clip truly contains the hypothesized action in a handful of seconds , thus generating labels for highly informative samples at little cost . we show that our large-scale dataset can be used to effectively pre-train action recognition models , significantly improving final metrics on smaller-scale benchmarks after fine-tuning . on kinetics , ucf-101 and hmdb-51 , models pre-trained on slac outperform baselines trained from scratch , by 2.0 % , 20.1 % and 35.4 % in top-1 accuracy , respectively when rgb input is used . furthermore , we introduce a simple procedure that leverages the sparse labels in slac to pre-train action localization models . on thumos14 and activitynet-v1.3 , our localization model improves the map of baseline model by 8.6 % and 2.5 % , respectively .", "topics": ["baseline ( configuration management )", "sparse matrix"]}
{"title": "a biomimetic approach based on immune systems for classification of unstructured data", "abstract": "in this paper we present the results of unstructured data clustering in this case a textual data from reuters 21578 corpus with a new biomimetic approach using immune system . before experimenting our immune system , we digitalized textual data by the n-grams approach . the novelty lies on hybridization of n-grams and immune systems for clustering . the experimental results show that the recommended ideas are promising and prove that this method can solve the text clustering problem .", "topics": ["cluster analysis", "text corpus"]}
{"title": "a generalized hybrid real-coded quantum evolutionary algorithm based on particle swarm theory with arithmetic crossover", "abstract": "this paper proposes a generalized hybrid real-coded quantum evolutionary algorithm ( hrcqea ) for optimizing complex functions as well as combinatorial optimization . the main idea of hrcqea is to devise a new technique for mutation and crossover operators . using the evolutionary equation of pso a single-multiple gene mutation ( smm ) is designed and the concept of arithmetic crossover ( ac ) is used in the new crossover operator . in hrcqea , each triploid chromosome represents a particle and the position of the particle is updated using smm and quantum rotation gate ( qrg ) , which can make the balance between exploration and exploitation . crossover is employed to expand the search space , hill climbing selection ( hcs ) and elitism help to accelerate the convergence speed . simulation results on knapsack problem and five benchmark complex functions with high dimension show that hrcqea performs better in terms of ability to discover the global optimum and convergence speed .", "topics": ["simulation"]}
{"title": "approximating solution structure of the weighted sentence alignment problem", "abstract": "we study the complexity of approximating solution structure of the bijective weighted sentence alignment problem of denero and klein ( 2008 ) . in particular , we consider the complexity of finding an alignment that has a significant overlap with an optimal alignment . we discuss ways of representing the solution for the general weighted sentence alignment as well as phrases-to-words alignment problem , and show that computing a string which agrees with the optimal sentence partition on more than half ( plus an arbitrarily small polynomial fraction ) positions for the phrases-to-words alignment is np-hard . for the general weighted sentence alignment we obtain such bound from the agreement on a little over 2/3 of the bits . additionally , we generalize the hamming distance approximation of a solution structure to approximating it with respect to the edit distance metric , obtaining similar lower bounds .", "topics": ["approximation algorithm", "polynomial"]}
{"title": "fault detection of broken rotor bar in ls-pmsm using random forests", "abstract": "this paper proposes a new approach to diagnose broken rotor bar failure in a line start-permanent magnet synchronous motor ( ls-pmsm ) using random forests . the transient current signal during the motor startup was acquired from a healthy motor and a faulty motor with a broken rotor bar fault . we extracted 13 statistical time domain features from the startup transient current signal , and used these features to train and test a random forest to determine whether the motor was operating under normal or faulty conditions . for feature selection , we used the feature importances from the random forest to reduce the number of features to two features . the results showed that the random forest classifies the motor condition as healthy or faulty with an accuracy of 98.8 % using all features and with an accuracy of 98.4 % by using only the mean-index and impulsion features . the performance of the random forest was compared with a decision tree , na\\ '' ive bayes classifier , logistic regression , linear ridge , and a support vector machine , with the random forest consistently having a higher accuracy than the other algorithms . the proposed approach can be used in industry for online monitoring and fault diagnostic of ls-pmsm motors and the results can be helpful for the establishment of preventive maintenance plans in factories .", "topics": ["support vector machine"]}
{"title": "adversarial inverse graphics networks : learning 2d-to-3d lifting and image-to-image translation from unpaired supervision", "abstract": "researchers have developed excellent feed-forward models that learn to map images to desired outputs , such as to the images ' latent factors , or to other images , using supervised learning . learning such mappings from unlabelled data , or improving upon supervised models by exploiting unlabelled data , remains elusive . we argue that there are two important parts to learning without annotations : ( i ) matching the predictions to the input observations , and ( ii ) matching the predictions to known priors . we propose adversarial inverse graphics networks ( aigns ) : weakly supervised neural network models that combine feedback from rendering their predictions , with distribution matching between their predictions and a collection of ground-truth factors . we apply aigns to 3d human pose estimation and 3d structure and egomotion estimation , and outperform models supervised by only paired annotations . we further apply aigns to facial image transformation using super-resolution and inpainting renderers , while deliberately adding biases in the ground-truth datasets . our model seamlessly incorporates such biases , rendering input faces towards young , old , feminine , masculine or tom cruise-like equivalents ( depending on the chosen bias ) , or adding lip and nose augmentations while inpainting concealed lips and noses .", "topics": ["supervised learning", "ground truth"]}
{"title": "how controlled english can improve semantic wikis", "abstract": "the motivation of semantic wikis is to make acquisition , maintenance , and mining of formal knowledge simpler , faster , and more flexible . however , most existing semantic wikis have a very technical interface and are restricted to a relatively low level of expressivity . in this paper , we explain how acewiki uses controlled english - concretely attempto controlled english ( ace ) - to provide a natural and intuitive interface while supporting a high degree of expressivity . we introduce recent improvements of the acewiki system and user studies that indicate that acewiki is usable and useful .", "topics": ["high- and low-level"]}
{"title": "explanation methods in deep learning : users , values , concerns and challenges", "abstract": "issues regarding explainable ai involve four components : users , laws & regulations , explanations and algorithms . together these components provide a context in which explanation methods can be evaluated regarding their adequacy . the goal of this chapter is to bridge the gap between expert users and lay users . different kinds of users are identified and their concerns revealed , relevant statements from the general data protection regulation are analyzed in the context of deep neural networks ( dnns ) , a taxonomy for the classification of existing explanation methods is introduced , and finally , the various classes of explanation methods are analyzed to verify if user concerns are justified . overall , it is clear that ( visual ) explanations can be given about various aspects of the influence of the input on the output . however , it is noted that explanation methods or interfaces for lay users are missing and we speculate which criteria these methods / interfaces should satisfy . finally it is noted that two important concerns are difficult to address with explanation methods : the concern about bias in datasets that leads to biased dnns , as well as the suspicion about unfair outcomes .", "topics": ["neural networks"]}
{"title": "deep convolutional denoising of low-light images", "abstract": "poisson distribution is used for modeling noise in photon-limited imaging . while canonical examples include relatively exotic types of sensing like spectral imaging or astronomy , the problem is relevant to regular photography now more than ever due to the booming market for mobile cameras . restricted form factor limits the amount of absorbed light , thus computational post-processing is called for . in this paper , we make use of the powerful framework of deep convolutional neural networks for poisson denoising . we demonstrate how by training the same network with images having a specific peak value , our denoiser outperforms previous state-of-the-art by a large margin both visually and quantitatively . being flexible and data-driven , our solution resolves the heavy ad hoc engineering used in previous methods and is an order of magnitude faster . we further show that by adding a reasonable prior on the class of the image being processed , another significant boost in performance is achieved .", "topics": ["noise reduction"]}
{"title": "fast convergent algorithms for expectation propagation approximate bayesian inference", "abstract": "we propose a novel algorithm to solve the expectation propagation relaxation of bayesian inference for continuous-variable graphical models . in contrast to most previous algorithms , our method is provably convergent . by marrying convergent ep ideas from ( opper & winther 05 ) with covariance decoupling techniques ( wipf & nagarajan 08 , nickisch & seeger 09 ) , it runs at least an order of magnitude faster than the most commonly used ep solver .", "topics": ["graphical model"]}
{"title": "generalized beta mixtures of gaussians", "abstract": "in recent years , a rich variety of shrinkage priors have been proposed that have great promise in addressing massive regression problems . in general , these new priors can be expressed as scale mixtures of normals , but have more complex forms and better properties than traditional cauchy and double exponential priors . we first propose a new class of normal scale mixtures through a novel generalized beta distribution that encompasses many interesting priors as special cases . this encompassing framework should prove useful in comparing competing priors , considering properties and revealing close connections . we then develop a class of variational bayes approximations through the new hierarchy presented that will scale more efficiently to the types of truly massive data sets that are now encountered routinely .", "topics": ["calculus of variations", "time complexity"]}
{"title": "on multiplicative multitask feature learning", "abstract": "we investigate a general framework of multiplicative multitask feature learning which decomposes each task 's model parameters into a multiplication of two components . one of the components is used across all tasks and the other component is task-specific . several previous methods have been proposed as special cases of our framework . we study the theoretical properties of this framework when different regularization conditions are applied to the two decomposed components . we prove that this framework is mathematically equivalent to the widely used multitask feature learning methods that are based on a joint regularization of all model parameters , but with a more general form of regularizers . further , an analytical formula is derived for the across-task component as related to the task-specific component for all these regularizers , leading to a better understanding of the shrinkage effect . study of this framework motivates new multitask learning algorithms . we propose two new learning formulations by varying the parameters in the proposed framework . empirical studies have revealed the relative advantages of the two new formulations by comparing with the state of the art , which provides instructive insights into the feature learning problem with multiple tasks .", "topics": ["feature learning", "matrix regularization"]}
{"title": "improving object localization with fitness nms and bounded iou loss", "abstract": "we demonstrate that many detection methods are designed to identify only a sufficently accurate bounding box , rather than the best available one . to address this issue we propose a simple and fast modification to the existing methods called fitness nms . this method is tested with the denet model and obtains a significantly improved map at greater localization accuracies without a loss in evaluation rate , and can be used in conjunction with soft nms for additional improvements . next we derive a novel bounding box regression loss based on a set of iou upper bounds that better matches the goal of iou maximization while still providing good convergence properties . following these novelties we investigate roi clustering schemes for improving evaluation rates for the denet wide model variants and provide an analysis of localization performance at various input image dimensions . we obtain a map of 33.6 % @ 79hz and 41.8 % @ 5hz for mscoco and a titan x ( maxwell ) . source code available from : https : //github.com/lachlants/denet", "topics": ["cluster analysis"]}
{"title": "parsing using a grammar of word association vectors", "abstract": "this paper was was first drafted in 2001 as a formalization of the system described in u.s. patent u.s. 7,392,174 . it describes a system for implementing a parser based on a kind of cross-product over vectors of contextually similar words . it is being published now in response to nascent interest in vector combination models of syntax and semantics . the method used aggressive substitution of contextually similar words and word groups to enable product vectors to stay in the same space as their operands and make entire sentences comparable syntactically , and potentially semantically . the vectors generated had sufficient representational strength to generate parse trees at least comparable with contemporary symbolic parsers .", "topics": ["parsing"]}
{"title": "miljs : brand new javascript libraries for matrix calculation and machine learning", "abstract": "miljs is a collection of state-of-the-art , platform-independent , scalable , fast javascript libraries for matrix calculation and machine learning . our core library offering a matrix calculation is called sushi , which exhibits far better performance than any other leading machine learning libraries written in javascript . especially , our matrix multiplication is 177 times faster than the fastest javascript benchmark . based on sushi , a machine learning library called tempura is provided , which supports various algorithms widely used in machine learning research . we also provide soba as a visualization library . the implementations of our libraries are clearly written , properly documented and thus can are easy to get started with , as long as there is a web browser . these libraries are available from http : //mil-tokyo.github.io/ under the mit license .", "topics": ["scalability"]}
{"title": "omnidirectional cnn for visual place recognition and navigation", "abstract": "$ $ visual place recognition is challenging , especially when only a few place exemplars are given . to mitigate the challenge , we consider place recognition method using omnidirectional cameras and propose a novel omnidirectional convolutional neural network ( o-cnn ) to handle severe camera pose variation . given a visual input , the task of the o-cnn is not to retrieve the matched place exemplar , but to retrieve the closest place exemplar and estimate the relative distance between the input and the closest place . with the ability to estimate relative distance , a heuristic policy is proposed to navigate a robot to the retrieved closest place . note that the network is designed to take advantage of the omnidirectional view by incorporating circular padding and rotation invariance . to train a powerful o-cnn , we build a virtual world for training on a large scale . we also propose a continuous lifted structured feature embedding loss to learn the concept of distance efficiently . finally , our experimental results confirm that our method achieves state-of-the-art accuracy and speed with both the virtual world and real-world datasets .", "topics": ["heuristic"]}
{"title": "deep deformation network for object landmark localization", "abstract": "we propose a novel cascaded framework , namely deep deformation network ( ddn ) , for localizing landmarks in non-rigid objects . the hallmarks of ddn are its incorporation of geometric constraints within a convolutional neural network ( cnn ) framework , ease and efficiency of training , as well as generality of application . a novel shape basis network ( sbn ) forms the first stage of the cascade , whereby landmarks are initialized by combining the benefits of cnn features and a learned shape basis to reduce the complexity of the highly nonlinear pose manifold . in the second stage , a point transformer network ( ptn ) estimates local deformation parameterized as thin-plate spline transformation for a finer refinement . our framework does not incorporate either handcrafted features or part connectivity , which enables an end-to-end shape prediction pipeline during both training and testing . in contrast to prior cascaded networks for landmark localization that learn a mapping from feature space to landmark locations , we demonstrate that the regularization induced through geometric priors in the ddn makes it easier to train , yet produces superior results . the efficacy and generality of the architecture is demonstrated through state-of-the-art performances on several benchmarks for multiple tasks such as facial landmark localization , human body pose estimation and bird part localization .", "topics": ["feature vector", "nonlinear system"]}
{"title": "kernel regression with sparse metric learning", "abstract": "kernel regression is a popular non-parametric fitting technique . it aims at learning a function which estimates the targets for test inputs as precise as possible . generally , the function value for a test input is estimated by a weighted average of the surrounding training examples . the weights are typically computed by a distance-based kernel function and they strongly depend on the distances between examples . in this paper , we first review the latest developments of sparse metric learning and kernel regression . then a novel kernel regression method involving sparse metric learning , which is called kernel regression with sparse metric learning ( kr $ \\_ $ sml ) , is proposed . the sparse kernel regression model is established by enforcing a mixed $ ( 2,1 ) $ -norm regularization over the metric matrix . it learns a mahalanobis distance metric by a gradient descent procedure , which can simultaneously conduct dimensionality reduction and lead to good prediction results . our work is the first to combine kernel regression with sparse metric learning . to verify the effectiveness of the proposed method , it is evaluated on 19 data sets for regression . furthermore , the new method is also applied to solving practical problems of forecasting short-term traffic flows . in the end , we compare the proposed method with other three related kernel regression methods on all test data sets under two criterions . experimental results show that the proposed method is much more competitive .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "principal polynomial analysis", "abstract": "this paper presents a new framework for manifold learning based on a sequence of principal polynomials that capture the possibly nonlinear nature of the data . the proposed principal polynomial analysis ( ppa ) generalizes pca by modeling the directions of maximal variance by means of curves , instead of straight lines . contrarily to previous approaches , ppa reduces to performing simple univariate regressions , which makes it computationally feasible and robust . moreover , ppa shows a number of interesting analytical properties . first , ppa is a volume-preserving map , which in turn guarantees the existence of the inverse . second , such an inverse can be obtained in closed form . invertibility is an important advantage over other learning methods , because it permits to understand the identified features in the input domain where the data has physical meaning . moreover , it allows to evaluate the performance of dimensionality reduction in sensible ( input-domain ) units . volume preservation also allows an easy computation of information theoretic quantities , such as the reduction in multi-information after the transform . third , the analytical nature of ppa leads to a clear geometrical interpretation of the manifold : it allows the computation of frenet-serret frames ( local features ) and of generalized curvatures at any point of the space . and fourth , the analytical jacobian allows the computation of the metric induced by the data , thus generalizing the mahalanobis distance . these properties are demonstrated theoretically and illustrated experimentally . the performance of ppa is evaluated in dimensionality and redundancy reduction , in both synthetic and real datasets from the uci repository .", "topics": ["nonlinear system", "synthetic data"]}
{"title": "knowledge transfer between artificial intelligence systems", "abstract": "we consider the fundamental question : how a legacy `` student '' artificial intelligent ( ai ) system could learn from a legacy `` teacher '' ai system or a human expert without complete re-training and , most importantly , without requiring significant computational resources . here `` learning '' is understood as an ability of one system to mimic responses of the other and vice-versa . we call such learning an artificial intelligence knowledge transfer . we show that if internal variables of the `` student '' artificial intelligent system have the structure of an $ n $ -dimensional topological vector space and $ n $ is sufficiently high then , with probability close to one , the required knowledge transfer can be implemented by simple cascades of linear functionals . in particular , for $ n $ sufficiently large , with probability close to one , the `` student '' system can successfully and non-iteratively learn $ k\\ll n $ new examples from the `` teacher '' ( or correct the same number of mistakes ) at the cost of two additional inner products . the concept is illustrated with an example of knowledge transfer from a pre-trained convolutional neural network to a simple linear classifier with hog features .", "topics": ["artificial intelligence"]}
{"title": "loopy belief propagation in bayesian networks : origin and possibilistic perspectives", "abstract": "in this paper we present a synthesis of the work performed on two inference algorithms : the pearl 's belief propagation ( bp ) algorithm applied to bayesian networks without loops ( i.e . polytree ) and the loopy belief propagation ( lbp ) algorithm ( inspired from the bp ) which is applied to networks containing undirected cycles . it is known that the bp algorithm , applied to bayesian networks with loops , gives incorrect numerical results i.e . incorrect posterior probabilities . murphy and al . [ 7 ] find that the lbp algorithm converges on several networks and when this occurs , lbp gives a good approximation of the exact posterior probabilities . however this algorithm presents an oscillatory behaviour when it is applied to qmr ( quick medical reference ) network [ 15 ] . this phenomenon prevents the lbp algorithm from converging towards a good approximation of posterior probabilities . we believe that the translation of the inference computation problem from the probabilistic framework to the possibilistic framework will allow performance improvement of lbp algorithm . we hope that an adaptation of this algorithm to a possibilistic causal network will show an improvement of the convergence of lbp .", "topics": ["approximation algorithm", "numerical analysis"]}
{"title": "toward a taxonomy and computational models of abnormalities in images", "abstract": "the human visual system can spot an abnormal image , and reason about what makes it strange . this task has not received enough attention in computer vision . in this paper we study various types of atypicalities in images in a more comprehensive way than has been done before . we propose a new dataset of abnormal images showing a wide range of atypicalities . we design human subject experiments to discover a coarse taxonomy of the reasons for abnormality . our experiments reveal three major categories of abnormality : object-centric , scene-centric , and contextual . based on this taxonomy , we propose a comprehensive computational model that can predict all different types of abnormality in images and outperform prior arts in abnormality recognition .", "topics": ["computer vision"]}
{"title": "kernel mean embedding of distributions : a review and beyond", "abstract": "a hilbert space embedding of a distribution -- -in short , a kernel mean embedding -- -has recently emerged as a powerful tool for machine learning and inference . the basic idea behind this framework is to map distributions into a reproducing kernel hilbert space ( rkhs ) in which the whole arsenal of kernel methods can be extended to probability measures . it can be viewed as a generalization of the original `` feature map '' common to support vector machines ( svms ) and other kernel methods . while initially closely associated with the latter , it has meanwhile found application in fields ranging from kernel machines and probabilistic modeling to statistical inference , causal discovery , and deep learning . the goal of this survey is to give a comprehensive review of existing work and recent advances in this research area , and to discuss the most challenging issues and open problems that could lead to new research directions . the survey begins with a brief introduction to the rkhs and positive definite kernels which forms the backbone of this survey , followed by a thorough discussion of the hilbert space embedding of marginal distributions , theoretical guarantees , and a review of its applications . the embedding of distributions enables us to apply rkhs methods to probability measures which prompts a wide range of applications such as kernel two-sample testing , independent testing , and learning on distributional data . next , we discuss the hilbert space embedding for conditional distributions , give theoretical insights , and review some applications . the conditional mean embedding enables us to perform sum , product , and bayes ' rules -- -which are ubiquitous in graphical model , probabilistic inference , and reinforcement learning -- -in a non-parametric way . we then discuss relationships between this framework and other related areas . lastly , we give some suggestions on future research directions .", "topics": ["kernel ( operating system )", "graphical model"]}
{"title": "gaussian process optimization in the bandit setting : no regret and experimental design", "abstract": "many applications require optimizing an unknown , noisy function that is expensive to evaluate . we formalize this task as a multi-armed bandit problem , where the payoff function is either sampled from a gaussian process ( gp ) or has low rkhs norm . we resolve the important open problem of deriving regret bounds for this setting , which imply novel convergence rates for gp optimization . we analyze gp-ucb , an intuitive upper-confidence based algorithm , and bound its cumulative regret in terms of maximal information gain , establishing a novel connection between gp optimization and experimental design . moreover , by bounding the latter in terms of operator spectra , we obtain explicit sublinear regret bounds for many commonly used covariance functions . in some important cases , our bounds have surprisingly weak dependence on the dimensionality . in our experiments on real sensor data , gp-ucb compares favorably with other heuristical gp optimization approaches .", "topics": ["regret ( decision theory )", "mathematical optimization"]}
{"title": "on sparsity inducing regularization methods for machine learning", "abstract": "during the past years there has been an explosion of interest in learning methods based on sparsity regularization . in this paper , we discuss a general class of such methods , in which the regularizer can be expressed as the composition of a convex function $ \\omega $ with a linear function . this setting includes several methods such the group lasso , the fused lasso , multi-task learning and many more . we present a general approach for solving regularization problems of this kind , under the assumption that the proximity operator of the function $ \\omega $ is available . furthermore , we comment on the application of this approach to support vector machines , a technique pioneered by the groundbreaking work of vladimir vapnik .", "topics": ["support vector machine", "matrix regularization"]}
{"title": "statistical inference using sgd", "abstract": "we present a novel method for frequentist statistical inference in $ m $ -estimation problems , based on stochastic gradient descent ( sgd ) with a fixed step size : we demonstrate that the average of such sgd sequences can be used for statistical inference , after proper scaling . an intuitive analysis using the ornstein-uhlenbeck process suggests that such averages are asymptotically normal . from a practical perspective , our sgd-based inference procedure is a first order method , and is well-suited for large scale problems . to show its merits , we apply it to both synthetic and real datasets , and demonstrate that its accuracy is comparable to classical statistical methods , while requiring potentially far less computation .", "topics": ["synthetic data", "gradient descent"]}
{"title": "learning with $ \\ell^ { 0 } $ -graph : $ \\ell^ { 0 } $ -induced sparse subspace clustering", "abstract": "sparse subspace clustering methods , such as sparse subspace clustering ( ssc ) \\cite { elhamifarv13 } and $ \\ell^ { 1 } $ -graph \\cite { yanw09 , chengyyfh10 } , are effective in partitioning the data that lie in a union of subspaces . most of those methods use $ \\ell^ { 1 } $ -norm or $ \\ell^ { 2 } $ -norm with thresholding to impose the sparsity of the constructed sparse similarity graph , and certain assumptions , e.g . independence or disjointness , on the subspaces are required to obtain the subspace-sparse representation , which is the key to their success . such assumptions are not guaranteed to hold in practice and they limit the application of sparse subspace clustering on subspaces with general location . in this paper , we propose a new sparse subspace clustering method named $ \\ell^ { 0 } $ -graph . in contrast to the required assumptions on subspaces for most existing sparse subspace clustering methods , it is proved that subspace-sparse representation can be obtained by $ \\ell^ { 0 } $ -graph for arbitrary distinct underlying subspaces almost surely under the mild i.i.d . assumption on the data generation . we develop a proximal method to obtain the sub-optimal solution to the optimization problem of $ \\ell^ { 0 } $ -graph with proved guarantee of convergence . moreover , we propose a regularized $ \\ell^ { 0 } $ -graph that encourages nearby data to have similar neighbors so that the similarity graph is more aligned within each cluster and the graph connectivity issue is alleviated . extensive experimental results on various data sets demonstrate the superiority of $ \\ell^ { 0 } $ -graph compared to other competing clustering methods , as well as the effectiveness of regularized $ \\ell^ { 0 } $ -graph .", "topics": ["cluster analysis", "optimization problem"]}
{"title": "visual explanations from hadamard product in multimodal deep networks", "abstract": "the visual explanation of learned representation of models helps to understand the fundamentals of learning . the attentional models of previous works used to visualize the attended regions over an image or text using their learned weights to confirm their intended mechanism . kim et al . ( 2016 ) show that the hadamard product in multimodal deep networks , which is well-known for the joint function of visual question answering tasks , implicitly performs an attentional mechanism for visual inputs . in this work , we extend their work to show that the hadamard product in multimodal deep networks performs not only for visual inputs but also for textual inputs simultaneously using the proposed gradient-based visualization technique . the attentional effect of hadamard product is visualized for both visual and textual inputs by analyzing the two inputs and an output of the hadamard product with the proposed method and compared with learned attentional weights of a visual question answering model .", "topics": ["gradient"]}
{"title": "multilayered model of speech", "abstract": "human speech is the most important part of general artificial intelligence and subject of much research . the hypothesis proposed in this article provides explanation of difficulties that modern science tackles in the field of human brain simulation . the hypothesis is based on the author 's conviction that the brain of any given person has different ability to process and store information . therefore , the approaches that are currently used to create general artificial intelligence have to be altered .", "topics": ["simulation", "artificial intelligence"]}
{"title": "autonomous ingress of a uav through a window using monocular vision", "abstract": "the use of autonomous uavs for surveillance purposes and other reconnaissance tasks is increasingly becoming popular and convenient.these tasks requires the ability to successfully ingress through the rectangular openings or windows of the target structure.in this paper , a method to robustly detect the window in the surrounding using basic image processing techniques and efficient distance measure , is proposed.furthermore , a navigation scheme which incorporates this detection method for performing navigation task has also been proposed.the whole navigation task is performed and tested in the simulation environment gazebo .", "topics": ["image processing", "simulation"]}
{"title": "ultrametric model of mind , i : review", "abstract": "we mathematically model ignacio matte blanco 's principles of symmetric and asymmetric being through use of an ultrametric topology . we use for this the highly regarded 1975 book of this chilean psychiatrist and pyschoanalyst ( born 1908 , died 1995 ) . such an ultrametric model corresponds to hierarchical clustering in the empirical data , e.g . text . we show how an ultrametric topology can be used as a mathematical model for the structure of the logic that reflects or expresses matte blanco 's symmetric being , and hence of the reasoning and thought processes involved in conscious reasoning or in reasoning that is lacking , perhaps entirely , in consciousness or awareness of itself . in a companion paper we study how symmetric ( in the sense of matte blanco 's ) reasoning can be demarcated in a context of symmetric and asymmetric reasoning provided by narrative text .", "topics": ["cluster analysis"]}
{"title": "perforatedcnns : acceleration through elimination of redundant convolutions", "abstract": "we propose a novel approach to reduce the computational cost of evaluation of convolutional neural networks , a factor that has hindered their deployment in low-power devices such as mobile phones . inspired by the loop perforation technique from source code optimization , we speed up the bottleneck convolutional layers by skipping their evaluation in some of the spatial positions . we propose and analyze several strategies of choosing these positions . we demonstrate that perforation can accelerate modern convolutional networks such as alexnet and vgg-16 by a factor of 2x - 4x . additionally , we show that perforation is complementary to the recently proposed acceleration method of zhang et al .", "topics": ["convolution"]}
{"title": "exploiting particle swarm optimization in multiple faults fuzzy detection", "abstract": "in this paper an on-line multiple faults detection approach is first of all proposed . for efficiency , an optimal design of membership functions is required . thus , the proposed approach is improved using particle swarm optimization ( pso ) technique . the inputs of the proposed approaches are residuals representing the numerical evaluation of analytical redundancy relations . these residuals are generated due to the use of bond graph modeling . the results of the fuzzy detection modules are displayed as a colored causal graph . a comparison between the results obtained by using pso and those given by the use of genetic algorithms ( ga ) is finally made . the experiments focus on a simulation of the three-tank hydraulic system , a benchmark in the diagnosis domain .", "topics": ["simulation"]}
{"title": "scaling multidimensional inference for structured gaussian processes", "abstract": "exact gaussian process ( gp ) regression has o ( n^3 ) runtime for data size n , making it intractable for large n. many algorithms for improving gp scaling approximate the covariance with lower rank matrices . other work has exploited structure inherent in particular covariance functions , including gps with implied markov structure , and equispaced inputs ( both enable o ( n ) runtime ) . however , these gp advances have not been extended to the multidimensional input setting , despite the preponderance of multidimensional applications . this paper introduces and tests novel extensions of structured gps to multidimensional inputs . we present new methods for additive gps , showing a novel connection between the classic backfitting method and the bayesian framework . to achieve optimal accuracy-complexity tradeoff , we extend this model with a novel variant of projection pursuit regression . our primary result -- projection pursuit gaussian process regression -- shows orders of magnitude speedup while preserving high accuracy . the natural second and third steps include non-gaussian observations and higher dimensional equispaced grid methods . we introduce novel techniques to address both of these necessary directions . we thoroughly illustrate the power of these three advances on several datasets , achieving close performance to the naive full gp at orders of magnitude less cost .", "topics": ["approximation algorithm"]}
{"title": "analysing quality of english-hindi machine translation engine outputs using bayesian classification", "abstract": "this paper considers the problem for estimating the quality of machine translation outputs which are independent of human intervention and are generally addressed using machine learning techniques.there are various measures through which a machine learns translations quality . automatic evaluation metrics produce good co-relation at corpus level but can not produce the same results at the same segment or sentence level . in this paper 16 features are extracted from the input sentences and their translations and a quality score is obtained based on bayesian inference produced from training data .", "topics": ["test set", "machine translation"]}
{"title": "what properties are desirable from an electron microscopy segmentation algorithm", "abstract": "the prospect of neural reconstruction from electron microscopy ( em ) images has been elucidated by the automatic segmentation algorithms . although segmentation algorithms eliminate the necessity of tracing the neurons by hand , significant manual effort is still essential for correcting the mistakes they make . a considerable amount of human labor is also required for annotating groundtruth volumes for training the classifiers of a segmentation framework . it is critically important to diminish the dependence on human interaction in the overall reconstruction system . this study proposes a novel classifier training algorithm for em segmentation aimed to reduce the amount of manual effort demanded by the groundtruth annotation and error refinement tasks . instead of using an exhaustive pixel level groundtruth , an active learning algorithm is proposed for sparse labeling of pixel and boundaries of superpixels . because over-segmentation errors are in general more tolerable and easier to correct than the under-segmentation errors , our algorithm is designed to prioritize minimization of false-merges over false-split mistakes . our experiments on both 2d and 3d data suggest that the proposed method yields segmentation outputs that are more amenable to neural reconstruction than those of existing methods .", "topics": ["pixel"]}
{"title": "minimax rates and efficient algorithms for noisy sorting", "abstract": "there has been a recent surge of interest in studying permutation-based models for ranking from pairwise comparison data . despite being structurally richer and more robust than parametric ranking models , permutation-based models are less well understood statistically and generally lack efficient learning algorithms . in this work , we study a prototype of permutation-based ranking models , namely , the noisy sorting model . we establish the optimal rates of learning the model under two sampling procedures . furthermore , we provide a fast algorithm to achieve near-optimal rates if the observations are sampled independently . along the way , we discover properties of the symmetric group which are of theoretical interest .", "topics": ["sampling ( signal processing )"]}
{"title": "classification analysis of authorship fiction texts in the space of semantic fields", "abstract": "the use of naive bayesian classifier ( nb ) and the classifier by the k nearest neighbors ( knn ) in classification semantic analysis of authors ' texts of english fiction has been analysed . the authors ' works are considered in the vector space the basis of which is formed by the frequency characteristics of semantic fields of nouns and verbs . highly precise classification of authors ' texts in the vector space of semantic fields indicates about the presence of particular spheres of author 's idiolect in this space which characterizes the individual author 's style .", "topics": ["bayesian network"]}
{"title": "convolutional radio modulation recognition networks", "abstract": "we study the adaptation of convolutional neural networks to the complex temporal radio signal domain . we compare the efficacy of radio modulation classification using naively learned features against using expert features which are widely used in the field today and we show significant performance improvements . we show that blind temporal learning on large and densely encoded time series using deep convolutional neural networks is viable and a strong candidate approach for this task especially at low signal to noise ratio .", "topics": ["time series"]}
{"title": "causal inference for cloud computing", "abstract": "cloud computing involves complex technical and economical systems and interactions . this brings about various challenges , two of which are : ( 1 ) debugging and control of computing systems with the help of sandbox experiments , and ( 2 ) prediction of the cost of `` spot '' resources for decision making of cloud clients . in this paper , we formalize debugging by counterfactual probabilities and control by post- ( soft- ) interventional probabilities . we prove that counterfactuals can approximately be calculated from a `` stochastic '' graphical causal model ( while they are originally defined only for `` deterministic '' functional causal models ) , and based on this sketch an approach to address problem ( 1 ) . to address problem ( 2 ) , we formalize bidding by post- ( soft- ) interventional probabilities and present a simple mathematical result on approximate integration of `` incomplete '' conditional probability distributions . we show how this can be used by cloud clients to trade off privacy against predictability of the outcome of their bidding actions in a toy scenario . we report experiments on simulated and real data .", "topics": ["approximation algorithm", "simulation"]}
{"title": "natural language feature selection via cooccurrence", "abstract": "specificity is important for extracting collocations , keyphrases , multi-word and index terms [ newman et al . 2012 ] . it is also useful for tagging , ontology construction [ ryu and choi 2006 ] , and automatic summarization of documents [ louis and nenkova 2011 , chali and hassan 2012 ] . term frequency and inverse-document frequency ( tf-idf ) are typically used to do this , but fail to take advantage of the semantic relationships between terms [ church and gale 1995 ] . the result is that general idiomatic terms are mistaken for specific terms . we demonstrate use of relational data for estimation of term specificity . the specificity of a term can be learned from its distribution of relations with other terms . this technique is useful for identifying relevant words or terms for other natural language processing tasks .", "topics": ["natural language processing", "natural language"]}
{"title": "bitwise neural networks", "abstract": "based on the assumption that there exists a neural network that efficiently represents a set of boolean functions between all binary inputs and outputs , we propose a process for developing and deploying neural networks whose weight parameters , bias terms , input , and intermediate hidden layer output signals , are all binary-valued , and require only basic bit logic for the feedforward pass . the proposed bitwise neural network ( bnn ) is especially suitable for resource-constrained environments , since it replaces either floating or fixed-point arithmetic with significantly more efficient bitwise operations . hence , the bnn requires for less spatial complexity , less memory bandwidth , and less power consumption in hardware . in order to design such networks , we propose to add a few training schemes , such as weight compression and noisy backpropagation , which result in a bitwise network that performs almost as well as its corresponding real-valued network . we test the proposed network on the mnist dataset , represented using binary features , and show that bnns result in competitive performance while offering dramatic computational savings .", "topics": ["neural networks", "mnist database"]}
{"title": "dimmwitted : a study of main-memory statistical analytics", "abstract": "we perform the first study of the tradeoff space of access methods and replication to support statistical analytics using first-order methods executed in the main memory of a non-uniform memory access ( numa ) machine . statistical analytics systems differ from conventional sql-analytics in the amount and types of memory incoherence they can tolerate . our goal is to understand tradeoffs in accessing the data in row- or column-order and at what granularity one should share the model and data for a statistical task . we study this new tradeoff space , and discover there are tradeoffs between hardware and statistical efficiency . we argue that our tradeoff study may provide valuable information for designers of analytics engines : for each system we consider , our prototype engine can run at least one popular task at least 100x faster . we conduct our study across five architectures using popular models including svms , logistic regression , gibbs sampling , and neural networks .", "topics": ["sampling ( signal processing )", "support vector machine"]}
{"title": "upal : unbiased pool based active learning", "abstract": "in this paper we address the problem of pool based active learning , and provide an algorithm , called upal , that works by minimizing the unbiased estimator of the risk of a hypothesis in a given hypothesis space . for the space of linear classifiers and the squared loss we show that upal is equivalent to an exponentially weighted average forecaster . exploiting some recent results regarding the spectra of random matrices allows us to establish consistency of upal when the true hypothesis is a linear hypothesis . empirical comparison with an active learner implementation in vowpal wabbit , and a previously proposed pool based active learner implementation show good empirical performance and better scalability .", "topics": ["scalability"]}
{"title": "sufficient dimensionality reduction with irrelevant statistics", "abstract": "the problem of finding a reduced dimensionality representation of categorical variables while preserving their most relevant characteristics is fundamental for the analysis of complex data . specifically , given a co-occurrence matrix of two variables , one often seeks a compact representation of one variable which preserves information about the other variable . we have recently introduced `` sufficient dimensionality reduction ' [ gt-2003 ] , a method that extracts continuous reduced dimensional features whose measurements ( i.e . , expectation values ) capture maximal mutual information among the variables . however , such measurements often capture information that is irrelevant for a given task . widely known examples are illumination conditions , which are irrelevant as features for face recognition , writing style which is irrelevant as a feature for content classification , and intonation which is irrelevant as a feature for speech recognition . such irrelevance can not be deduced apriori , since it depends on the details of the task , and is thus inherently ill defined in the purely unsupervised case . separating relevant from irrelevant features can be achieved using additional side data that contains such irrelevant structures . this approach was taken in [ ct-2002 ] , extending the information bottleneck method , which uses clustering to compress the data . here we use this side-information framework to identify features whose measurements are maximally informative for the original data set , but carry as little information as possible on a side data set . in statistical terms this can be understood as extracting statistics which are maximally sufficient for the original dataset , while simultaneously maximally ancillary for the side dataset . we formulate this tradeoff as a constrained optimization problem and characterize its solutions . we then derive a gradient descent algorithm for this problem , which is based on the generalized iterative scaling method for finding maximum entropy distributions . the method is demonstrated on synthetic data , as well as on real face recognition datasets , and is shown to outperform standard methods such as oriented pca .", "topics": ["cluster analysis", "feature extraction"]}
{"title": "learning natural language inference using bidirectional lstm model and inner-attention", "abstract": "in this paper , we proposed a sentence encoding-based model for recognizing text entailment . in our approach , the encoding of sentence is a two-stage process . firstly , average pooling was used over word-level bidirectional lstm ( bilstm ) to generate a first-stage sentence representation . secondly , attention mechanism was employed to replace average pooling on the same sentence for better representations . instead of using target sentence to attend words in source sentence , we utilized the sentence 's first-stage representation to attend words appeared in itself , which is called `` inner-attention '' in our paper . experiments conducted on stanford natural language inference ( snli ) corpus has proved the effectiveness of `` inner-attention '' mechanism . with less number of parameters , our model outperformed the existing best sentence encoding-based approach by a large margin .", "topics": ["natural language"]}
{"title": "fast randomized semi-supervised clustering", "abstract": "we consider the problem of clustering partially labeled data from a minimal number of randomly chosen pairwise comparisons between the items . we introduce an efficient local algorithm based on a power iteration of the non-backtracking operator and study its performance on a simple model . for the case of two clusters , we give bounds on the classification error and show that a small error can be achieved from $ o ( n ) $ randomly chosen measurements , where $ n $ is the number of items in the dataset . our algorithm is therefore efficient both in terms of time and space complexities . we also investigate numerically the performance of the algorithm on synthetic and real world data .", "topics": ["cluster analysis", "numerical analysis"]}
{"title": "dense multi-view 3d-reconstruction without dense correspondences", "abstract": "we introduce a variational method for multi-view shape-from-shading under natural illumination . the key idea is to couple pde-based solutions for single-image based shape-from-shading problems across multiple images and multiple color channels by means of a variational formulation . rather than alternatingly solving the individual sfs problems and optimizing the consistency across images and channels which is known to lead to suboptimal results , we propose an efficient solution of the coupled problem by means of an admm algorithm . in numerous experiments on both simulated and real imagery , we demonstrate that the proposed fusion of multiple-view reconstruction and shape-from-shading provides highly accurate dense reconstructions without the need to compute dense correspondences . with the proposed variational integration across multiple views shape-from-shading techniques become applicable to challenging real-world reconstruction problems , giving rise to highly detailed geometry even in areas of smooth brightness variation and lacking texture .", "topics": ["calculus of variations", "simulation"]}
{"title": "on overfitting and asymptotic bias in batch reinforcement learning with partial observability", "abstract": "this paper stands in the context of reinforcement learning with partial observability and limited data . in this setting , we focus on the tradeoff between asymptotic bias ( suboptimality with unlimited data ) and overfitting ( additional suboptimality due to limited data ) , and theoretically show that while potentially increasing the asymptotic bias , a smaller state representation decreases the risk of overfitting . our analysis relies on expressing the quality of a state representation by bounding l1 error terms of the associated belief states . theoretical results are empirically illustrated when the state representation is a truncated history of observations . finally , we also discuss and empirically illustrate how using function approximators and adapting the discount factor may enhance the tradeoff between asymptotic bias and overfitting .", "topics": ["reinforcement learning"]}
{"title": "latent hypernet : exploring all layers from convolutional neural networks", "abstract": "since convolutional neural networks ( convnets ) are able to simultaneously learn features and classifiers to discriminate different categories of activities , recent works have employed convnets approaches to perform human activity recognition ( har ) based on wearable sensors , allowing the removal of expensive human work and expert knowledge . however , these approaches have their power of discrimination limited mainly by the large number of parameters that compose the network and the reduced number of samples available for training . inspired by this , we propose an accurate and robust approach , referred to as latent hypernet ( lhn ) . the lhn uses feature maps from early layers ( hyper ) and projects them , individually , onto a low dimensionality space ( latent ) . then , these latent features are concatenated and presented to a classifier . to demonstrate the robustness and accuracy of the lhn , we evaluate it using four different networks architectures in five publicly available har datasets based on wearable sensors , which vary in the sampling rate and number of activities . our experiments demonstrate that the proposed lhn is able to produce rich information , improving the results regarding the original convnets . furthermore , the method outperforms existing state-of-the-art methods .", "topics": ["sampling ( signal processing )", "statistical classification"]}
{"title": "diffusion independent semi-bandit influence maximization", "abstract": "we consider \\emph { influence maximization } ( im ) in social networks , which is the problem of maximizing the number of users that become aware of a product by selecting a set of `` seed '' users to expose the product to . while prior work assumes a known model of information diffusion , we propose a parametrization in terms of pairwise reachability which makes our framework agnostic to the underlying diffusion model . we give a corresponding monotone , submodular surrogate function , and show that it is a good approximation to the original im objective . we also consider the case of a new marketer looking to exploit an existing social network , while simultaneously learning the factors governing information propagation . for this , we propose a pairwise-influence semi-bandit feedback model and develop a linucb-based bandit algorithm . our model-independent regret analysis shows that our bound on the cumulative regret has a better ( as compared to previous work ) dependence on the size of the network . by using the graph laplacian eigenbasis to construct features , we describe a practical linucb implementation . experimental evaluation suggests that our framework is robust to the underlying diffusion model and can efficiently learn a near-optimal solution .", "topics": ["regret ( decision theory )", "optimization problem"]}
{"title": "image processing", "abstract": "gabor filters can extract multi-orientation and multiscale features from face images . researchers have designed different ways to use the magnitude of the filtered results for face recognition : gabor fisher classifier exploited only the magnitude information of gabor magnitude pictures ( gmps ) ; local gabor binary pattern uses only the gradient information . in this paper , we regard gmps as smooth surfaces . by completely describing the shape of gmps , we get a face representation method called gabor surface feature ( gsf ) . first , we compute the magnitude , 1st and 2nd derivatives of gmps , then binarize them and transform them into decimal values . finally we construct joint histograms and use subspace methods for classification . experiments on feret , orl and frgc 1.0.4 database show the effectiveness of gsf .", "topics": ["image processing"]}
{"title": "fastmmd : ensemble of circular discrepancy for efficient two-sample test", "abstract": "the maximum mean discrepancy ( mmd ) is a recently proposed test statistic for two-sample test . its quadratic time complexity , however , greatly hampers its availability to large-scale applications . to accelerate the mmd calculation , in this study we propose an efficient method called fastmmd . the core idea of fastmmd is to equivalently transform the mmd with shift-invariant kernels into the amplitude expectation of a linear combination of sinusoid components based on bochner 's theorem and fourier transform ( rahimi & recht , 2007 ) . taking advantage of sampling of fourier transform , fastmmd decreases the time complexity for mmd calculation from $ o ( n^2 d ) $ to $ o ( l n d ) $ , where $ n $ and $ d $ are the size and dimension of the sample set , respectively . here $ l $ is the number of basis functions for approximating kernels which determines the approximation accuracy . for kernels that are spherically invariant , the computation can be further accelerated to $ o ( l n \\log d ) $ by using the fastfood technique ( le et al . , 2013 ) . the uniform convergence of our method has also been theoretically proved in both unbiased and biased estimates . we have further provided a geometric explanation for our method , namely ensemble of circular discrepancy , which facilitates us to understand the insight of mmd , and is hopeful to help arouse more extensive metrics for assessing two-sample test . experimental results substantiate that fastmmd is with similar accuracy as exact mmd , while with faster computation speed and lower variance than the existing mmd approximation methods .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "adaptive subgradient methods for online auc maximization", "abstract": "learning for maximizing auc performance is an important research problem in machine learning and artificial intelligence . unlike traditional batch learning methods for maximizing auc which often suffer from poor scalability , recent years have witnessed some emerging studies that attempt to maximize auc by single-pass online learning approaches . despite their encouraging results reported , the existing online auc maximization algorithms often adopt simple online gradient descent approaches that fail to exploit the geometrical knowledge of the data observed during the online learning process , and thus could suffer from relatively larger regret . to address the above limitation , in this work , we explore a novel algorithm of adaptive online auc maximization ( adaoam ) which employs an adaptive gradient method that exploits the knowledge of historical gradients to perform more informative online learning . the new adaptive updating strategy of the adaoam is less sensitive to the parameter settings and maintains the same time complexity as previous non-adaptive counterparts . additionally , we extend the algorithm to handle high-dimensional sparse data ( sadaoam ) and address sparsity in the solution by performing lazy gradient updating . we analyze the theoretical bounds and evaluate their empirical performance on various types of data sets . the encouraging empirical results obtained clearly highlighted the effectiveness and efficiency of the proposed algorithms .", "topics": ["regret ( decision theory )", "time complexity"]}
{"title": "normalized total gradient : a new measure for multispectral image registration", "abstract": "image registration is a fundamental issue in multispectral image processing . in filter wheel based multispectral imaging systems , the non-coplanar placement of the filters always causes the misalignment of multiple channel images . the selective characteristic of spectral response in multispectral imaging raises two challenges to image registration . first , the intensity levels of a local region may be different in individual channel images . second , the local intensity may vary rapidly in some channel images while keeps stationary in others . conventional multimodal measures , such as mutual information , correlation coefficient , and correlation ratio , can register images with different regional intensity levels , but will fail in the circumstance of severe local intensity variation . in this paper , a new measure , namely normalized total gradient ( ntg ) , is proposed for multispectral image registration . the ntg is applied on the difference between two channel images . this measure is based on the key assumption ( observation ) that the gradient of difference image between two aligned channel images is sparser than that between two misaligned ones . a registration framework , which incorporates image pyramid and global/local optimization , is further introduced for rigid transform . experimental results validate that the proposed method is effective for multispectral image registration and performs better than conventional methods .", "topics": ["image processing", "gradient"]}
{"title": "learning low dimensional convolutional neural networks for high-resolution remote sensing image retrieval", "abstract": "learning powerful feature representations for image retrieval has always been a challenging task in the field of remote sensing . traditional methods focus on extracting low-level hand-crafted features which are not only time-consuming but also tend to achieve unsatisfactory performance due to the content complexity of remote sensing images . in this paper , we investigate how to extract deep feature representations based on convolutional neural networks ( cnn ) for high-resolution remote sensing image retrieval ( hrrsir ) . to this end , two effective schemes are proposed to generate powerful feature representations for hrrsir . in the first scheme , the deep features are extracted from the fully-connected and convolutional layers of the pre-trained cnn models , respectively ; in the second scheme , we propose a novel cnn architecture based on conventional convolution layers and a three-layer perceptron . the novel cnn model is then trained on a large remote sensing dataset to learn low dimensional features . the two schemes are evaluated on several public and challenging datasets , and the results indicate that the proposed schemes and in particular the novel cnn are able to achieve state-of-the-art performance .", "topics": ["high- and low-level"]}
{"title": "learning high-level prior with convolutional neural networks for semantic segmentation", "abstract": "this paper proposes a convolutional neural network that can fuse high-level prior for semantic image segmentation . motivated by humans ' vision recognition system , our key design is a three-layer generative structure consisting of high-level coding , middle-level segmentation and low-level image to introduce global prior for semantic segmentation . based on this structure , we proposed a generative model called conditional variational auto-encoder ( cvae ) that can build up the links behind these three layers . these important links include an image encoder that extracts high level info from image , a segmentation encoder that extracts high level info from segmentation , and a hybrid decoder that outputs semantic segmentation from the high level prior and input image . we theoretically derive the semantic segmentation as an optimization problem parameterized by these links . finally , the optimization problem enables us to take advantage of state-of-the-art fully convolutional network structure for the implementation of the above encoders and decoder . experimental results on several representative datasets demonstrate our supreme performance for semantic segmentation .", "topics": ["generative model", "image segmentation"]}
{"title": "drawing and recognizing chinese characters with recurrent neural network", "abstract": "recent deep learning based approaches have achieved great success on handwriting recognition . chinese characters are among the most widely adopted writing systems in the world . previous research has mainly focused on recognizing handwritten chinese characters . however , recognition is only one aspect for understanding a language , another challenging and interesting task is to teach a machine to automatically write ( pictographic ) chinese characters . in this paper , we propose a framework by using the recurrent neural network ( rnn ) as both a discriminative model for recognizing chinese characters and a generative model for drawing ( generating ) chinese characters . to recognize chinese characters , previous methods usually adopt the convolutional neural network ( cnn ) models which require transforming the online handwriting trajectory into image-like representations . instead , our rnn based approach is an end-to-end system which directly deals with the sequential structure and does not require any domain-specific knowledge . with the rnn system ( combining an lstm and gru ) , state-of-the-art performance can be achieved on the icdar-2013 competition database . furthermore , under the rnn framework , a conditional generative model with character embedding is proposed for automatically drawing recognizable chinese characters . the generated characters ( in vector format ) are human-readable and also can be recognized by the discriminative rnn model with high accuracy . experimental results verify the effectiveness of using rnns as both generative and discriminative models for the tasks of drawing and recognizing chinese characters .", "topics": ["generative model", "recurrent neural network"]}
{"title": "giant : globally improved approximate newton method for distributed optimization", "abstract": "for distributed computing environments , we consider the canonical machine learning problem of empirical risk minimization ( erm ) with quadratic regularization , and we propose a distributed and communication-efficient newton-type optimization method . at every iteration , each worker locally finds an approximate newton ( ant ) direction , and then it sends this direction to the main driver . the driver , then , averages all the ant directions received from workers to form a globally improved ant ( giant ) direction . giant naturally exploits the trade-offs between local computations and global communications in that more local computations result in fewer overall rounds of communications . giant is highly communication efficient in that , for $ d $ -dimensional data uniformly distributed across $ m $ workers , it has $ 4 $ or $ 6 $ rounds of communication and $ o ( d \\log m ) $ communication complexity per iteration . theoretically , we show that giant 's convergence rate is faster than first-order methods and existing distributed newton-type methods . from a practical point-of-view , a highly beneficial feature of giant is that it has only one tuning parameter -- -the iterations of the local solver for computing an ant direction . this is indeed in sharp contrast with many existing distributed newton-type methods , as well as popular first-order methods , which have several tuning parameters , and whose performance can be greatly affected by the specific choices of such parameters . in this light , we empirically demonstrate the superior performance of giant compared with other competing methods .", "topics": ["matrix regularization", "iteration"]}
{"title": "adversarial examples are not easily detected : bypassing ten detection methods", "abstract": "neural networks are known to be vulnerable to adversarial examples : inputs that are close to natural inputs but classified incorrectly . in order to better understand the space of adversarial examples , we survey ten recent proposals that are designed for detection and compare their efficacy . we show that all can be defeated by constructing new loss functions . we conclude that adversarial examples are significantly harder to detect than previously appreciated , and the properties believed to be intrinsic to adversarial examples are in fact not . finally , we propose several simple guidelines for evaluating future proposed defenses .", "topics": ["neural networks", "loss function"]}
{"title": "a fast greedy algorithm for generalized column subset selection", "abstract": "this paper defines a generalized column subset selection problem which is concerned with the selection of a few columns from a source matrix a that best approximate the span of a target matrix b . the paper then proposes a fast greedy algorithm for solving this problem and draws connections to different problems that can be efficiently solved using the proposed algorithm .", "topics": ["approximation algorithm", "optimization problem"]}
{"title": "invariant backpropagation : how to train a transformation-invariant neural network", "abstract": "in many classification problems a classifier should be robust to small variations in the input vector . this is a desired property not only for particular transformations , such as translation and rotation in image classification problems , but also for all others for which the change is small enough to retain the object perceptually indistinguishable . we propose two extensions of the backpropagation algorithm that train a neural network to be robust to variations in the feature vector . while the first of them enforces robustness of the loss function to all variations , the second method trains the predictions to be robust to a particular variation which changes the loss function the most . the second methods demonstrates better results , but is slightly slower . we analytically compare the proposed algorithm with two the most similar approaches ( tangent bp and adversarial training ) , and propose their fast versions . in the experimental part we perform comparison of all algorithms in terms of classification accuracy and robustness to noise on mnist and cifar-10 datasets . additionally we analyze how the performance of the proposed algorithm depends on the dataset size and data augmentation .", "topics": ["feature vector", "loss function"]}
{"title": "edge detection : a collection of pixel based approach for colored images", "abstract": "the existing traditional edge detection algorithms process a single pixel on an image at a time , thereby calculating a value which shows the edge magnitude of the pixel and the edge orientation . most of these existing algorithms convert the coloured images into gray scale before detection of edges . however , this process leads to inaccurate precision of recognized edges , thus producing false and broken edges in the image . this paper presents a profile modelling scheme for collection of pixels based on the step and ramp edges , with a view to reducing the false and broken edges present in the image . the collection of pixel scheme generated is used with the vector order statistics to reduce the imprecision of recognized edges when converting from coloured to gray scale images . the pratt figure of merit ( pfom ) is used as a quantitative comparison between the existing traditional edge detection algorithm and the developed algorithm as a means of validation . the pfom value obtained for the developed algorithm is 0.8480 , which showed an improvement over the existing traditional edge detection algorithms .", "topics": ["pixel"]}
{"title": "union of intersections ( uoi ) for interpretable data driven discovery and prediction", "abstract": "the increasing size and complexity of scientific data could dramatically enhance discovery and prediction for basic scientific applications . realizing this potential , however , requires novel statistical analysis methods that are both interpretable and predictive . we introduce union of intersections ( uoi ) , a flexible , modular , and scalable framework for enhanced model selection and estimation . methods based on uoi perform model selection and model estimation through intersection and union operations , respectively . we show that uoi-based methods achieve low-variance and nearly unbiased estimation of a small number of interpretable features , while maintaining high-quality prediction accuracy . we perform extensive numerical investigation to evaluate a uoi algorithm ( $ uoi_ { lasso } $ ) on synthetic and real data . in doing so , we demonstrate the extraction of interpretable functional networks from human electrophysiology recordings as well as accurate prediction of phenotypes from genotype-phenotype data with reduced features . we also show ( with the $ uoi_ { l1logistic } $ and $ uoi_ { cur } $ variants of the basic framework ) improved prediction parsimony for classification and matrix factorization on several benchmark biomedical data sets . these results suggest that methods based on the uoi framework could improve interpretation and prediction in data-driven discovery across scientific fields .", "topics": ["numerical analysis", "synthetic data"]}
{"title": "minimal exploration in structured stochastic bandits", "abstract": "this paper introduces and addresses a wide class of stochastic bandit problems where the function mapping the arm to the corresponding reward exhibits some known structural properties . most existing structures ( e.g . linear , lipschitz , unimodal , combinatorial , dueling , ... ) are covered by our framework . we derive an asymptotic instance-specific regret lower bound for these problems , and develop ossb , an algorithm whose regret matches this fundamental limit . ossb is not based on the classical principle of `` optimism in the face of uncertainty '' or on thompson sampling , and rather aims at matching the minimal exploration rates of sub-optimal arms as characterized in the derivation of the regret lower bound . we illustrate the efficiency of ossb using numerical experiments in the case of the linear bandit problem and show that ossb outperforms existing algorithms , including thompson sampling .", "topics": ["sampling ( signal processing )", "regret ( decision theory )"]}
{"title": "a hierarchical graphical model for record linkage", "abstract": "the task of matching co-referent records is known among other names as rocord linkage . for large record-linkage problems , often there is little or no labeled data available , but unlabeled data shows a reasonable clear structure . for such problems , unsupervised or semi-supervised methods are preferable to supervised methods . in this paper , we describe a hierarchical graphical model framework for the linakge-problem in an unsupervised setting . in addition to proposing new methods , we also cast existing unsupervised probabilistic record-linkage methods in this framework . some of the techniques we propose to minimize overfitting in the above model are of interest in the general graphical model setting . we describe a method for incorporating monotinicity constraints in a graphical model . we also outline a bootstrapping approach of using `` single-field '' classifiers to noisily label latent variables in a hierarchical model . experimental results show that our proposed unsupervised methods perform quite competitively even with fully supervised record-linkage methods .", "topics": ["graphical model", "supervised learning"]}
{"title": "truecluster matching", "abstract": "cluster matching by permuting cluster labels is important in many clustering contexts such as cluster validation and cluster ensemble techniques . the classic approach is to minimize the euclidean distance between two cluster solutions which induces inappropriate stability in certain settings . therefore , we present the truematch algorithm that introduces two improvements best explained in the crisp case . first , instead of maximizing the trace of the cluster crosstable , we propose to maximize a chi-square transformation of this crosstable . thus , the trace will not be dominated by the cells with the largest counts but by the cells with the most non-random observations , taking into account the marginals . second , we suggest a probabilistic component in order to break ties and to make the matching algorithm truly random on random data . the truematch algorithm is designed as a building block of the truecluster framework and scales in polynomial time . first simulation results confirm that the truematch algorithm gives more consistent truecluster results for unequal cluster sizes . free r software is available .", "topics": ["cluster analysis", "time complexity"]}
{"title": "neural network clustering based on distances between objects", "abstract": "we present an algorithm of clustering of many-dimensional objects , where only the distances between objects are used . centers of classes are found with the aid of neuron-like procedure with lateral inhibition . the result of clustering does not depend on starting conditions . our algorithm makes it possible to give an idea about classes that really exist in the empirical data . the results of computer simulations are presented .", "topics": ["cluster analysis", "simulation"]}
{"title": "the wreath process : a totally generative model of geometric shape based on nested symmetries", "abstract": "we consider the problem of modelling noisy but highly symmetric shapes that can be viewed as hierarchies of whole-part relationships in which higher level objects are composed of transformed collections of lower level objects . to this end , we propose the stochastic wreath process , a fully generative probabilistic model of drawings . following leyton 's `` generative theory of shape '' , we represent shapes as sequences of transformation groups composed through a wreath product . this representation emphasizes the maximization of transfer -- - the idea that the most compact and meaningful representation of a given shape is achieved by maximizing the re-use of existing building blocks or parts . the proposed stochastic wreath process extends leyton 's theory by defining a probability distribution over geometric shapes in terms of noise processes that are aligned with the generative group structure of the shape . we propose an inference scheme for recovering the generative history of given images in terms of the wreath process using reversible jump markov chain monte carlo methods and approximate bayesian computation . in the context of sketching we demonstrate the feasibility and limitations of this approach on model-generated and real data .", "topics": ["generative model", "computation"]}
{"title": "the enemy among us : detecting hate speech with threats based 'othering ' language embeddings", "abstract": "offensive or antagonistic language targeted at individuals and social groups based on their personal characteristics ( also known as cyber hate speech or cyberhate ) has been frequently posted and widely circulated viathe world wide web . this can be considered as a key risk factor for individual and societal tension linked toregional instability . automated web-based cyberhate detection is important for observing and understandingcommunity and regional societal tension - especially in online social networks where posts can be rapidlyand widely viewed and disseminated . while previous work has involved using lexicons , bags-of-words orprobabilistic language parsing approaches , they often suffer from a similar issue which is that cyberhate can besubtle and indirect - thus depending on the occurrence of individual words or phrases can lead to a significantnumber of false negatives , providing inaccurate representation of the trends in cyberhate . this problemmotivated us to challenge thinking around the representation of subtle language use , such as references toperceived threats from `` the other '' including immigration or job prosperity in a hateful context . we propose anovel framework that utilises language use around the concept of `` othering '' and intergroup threat theory toidentify these subtleties and we implement a novel classification method using embedding learning to computesemantic distances between parts of speech considered to be part of an `` othering '' narrative . to validate ourapproach we conduct several experiments on different types of cyberhate , namely religion , disability , race andsexual orientation , with f-measure scores for classifying hateful instances obtained through applying ourmodel of 0.93 , 0.86 , 0.97 and 0.98 respectively , providing a significant improvement in classifier accuracy overthe state-of-the-art", "topics": ["data mining", "feature extraction"]}
{"title": "warped-linear models for time series classification", "abstract": "this article proposes and studies warped-linear models for time series classification . the proposed models are time-warp invariant analogues of linear models . their construction is in line with time series averaging and extensions of k-means and learning vector quantization to dynamic time warping ( dtw ) spaces . the main theoretical result is that warped-linear models correspond to polyhedral classifiers in euclidean spaces . this result simplifies the analysis of time-warp invariant models by reducing to max-linear functions . we exploit this relationship and derive solutions to the label-dependency problem and the problem of learning warped-linear models . empirical results on time series classification suggest that warped-linear functions better trade solution quality against computation time than nearest-neighbor and prototype-based methods .", "topics": ["time series", "time complexity"]}
{"title": "a unified approach to adaptive regularization in online and stochastic optimization", "abstract": "we describe a framework for deriving and analyzing online optimization algorithms that incorporate adaptive , data-dependent regularization , also termed preconditioning . such algorithms have been proven useful in stochastic optimization by reshaping the gradients according to the geometry of the data . our framework captures and unifies much of the existing literature on adaptive online methods , including the adagrad and online newton step algorithms as well as their diagonal versions . as a result , we obtain new convergence proofs for these algorithms that are substantially simpler than previous analyses . our framework also exposes the rationale for the different preconditioned updates used in common stochastic optimization methods .", "topics": ["matrix regularization"]}
{"title": "segmenting dermoscopic images", "abstract": "we propose an automatic algorithm , named sdi , for the segmentation of skin lesions in dermoscopic images , articulated into three main steps : selection of the image roi , selection of the segmentation band , and segmentation . we present extensive experimental results achieved by the sdi algorithm on the lesion segmentation dataset made available for the isic 2017 challenge on skin lesion analysis towards melanoma detection , highlighting its advantages and disadvantages .", "topics": ["image segmentation"]}
{"title": "exploring the entire regularization path for the asymmetric cost linear support vector machine", "abstract": "we propose an algorithm for exploring the entire regularization path of asymmetric-cost linear support vector machines . empirical evidence suggests the predictive power of support vector machines depends on the regularization parameters of the training algorithms . the algorithms exploring the entire regularization paths have been proposed for single-cost support vector machines thereby providing the complete knowledge on the behavior of the trained model over the hyperparameter space . considering the problem in two-dimensional hyperparameter space though enables our algorithm to maintain greater flexibility in dealing with special cases and sheds light on problems encountered by algorithms building the paths in one-dimensional spaces . we demonstrate two-dimensional regularization paths for linear support vector machines that we train on synthetic and real data .", "topics": ["support vector machine", "synthetic data"]}
{"title": "emotion : appraisal-coping model for the `` cascades '' problem", "abstract": "modelling emotion has become a challenge nowadays . therefore , several models have been produced in order to express human emotional activity . however , only a few of them are currently able to express the close relationship existing between emotion and cognition . an appraisal-coping model is presented here , with the aim to simulate the emotional impact caused by the evaluation of a particular situation ( appraisal ) , along with the consequent cognitive reaction intended to face the situation ( coping ) . this model is applied to the `` cascades '' problem , a small arithmetical exercise designed for ten-year-old pupils . the goal is to create a model corresponding to a child 's behaviour when solving the problem using his own strategies .", "topics": ["simulation", "causality"]}
{"title": "dynamic parallel and distributed graph cuts", "abstract": "graph-cuts are widely used in computer vision . in order to speed up the optimization process and improve the scalability for large graphs , strandmark and kahl introduced a splitting method to split a graph into multiple subgraphs for parallel computation in both shared and distributed memory models . however , this parallel algorithm ( parallel bk-algorithm ) does not have a polynomial bound on the number of iterations and is found non-convergent in some cases due to the possible multiple optimal solutions of its sub-problems . to remedy this non-convergence problem , in this work we first introduce a merging method capable of merging any number of those adjacent sub-graphs which could hardly reach an agreement on their overlapped region in the parallel bk algorithm . based on the pseudo-boolean representations of graph cuts , our merging method is shown able to effectively reuse all the computed flows in these sub-graphs . through both the splitting and merging , we further propose a dynamic parallel and distributed graph-cuts algorithm with guaranteed convergence to the globally optimal solutions within a predefined number of iterations . in essence , this work provides a general framework to allow more sophisticated splitting and merging strategies to be employed to further boost performance . our dynamic parallel algorithm is validated with extensive experimental results .", "topics": ["computer vision", "iteration"]}
{"title": "the responsibility weighted mahalanobis kernel for semi-supervised training of support vector machines for classification", "abstract": "kernel functions in support vector machines ( svm ) are needed to assess the similarity of input samples in order to classify these samples , for instance . besides standard kernels such as gaussian ( i.e . , radial basis function , rbf ) or polynomial kernels , there are also specific kernels tailored to consider structure in the data for similarity assessment . in this article , we will capture structure in data by means of probabilistic mixture density models , for example gaussian mixtures in the case of real-valued input spaces . from the distance measures that are inherently contained in these models , e.g . , mahalanobis distances in the case of gaussian mixtures , we derive a new kernel , the responsibility weighted mahalanobis ( rwm ) kernel . basically , this kernel emphasizes the influence of model components from which any two samples that are compared are assumed to originate ( that is , the `` responsible '' model components ) . we will see that this kernel outperforms the rbf kernel and other kernels capturing structure in data ( such as the lap kernel in laplacian svm ) in many applications where partially labeled data are available , i.e . , for semi-supervised training of svm . other key advantages are that the rwm kernel can easily be used with standard svm implementations and training algorithms such as sequential minimal optimization , and heuristics known for the parametrization of rbf kernels in a c-svm can easily be transferred to this new kernel . properties of the rwm kernel are demonstrated with 20 benchmark data sets and an increasing percentage of labeled samples in the training data .", "topics": ["kernel ( operating system )", "test set"]}
{"title": "deep gradient compression : reducing the communication bandwidth for distributed training", "abstract": "large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training , and requires expensive high-bandwidth network infrastructure . the situation gets even worse with distributed training on mobile devices ( federated learning ) , which suffers from higher latency , lower throughput , and intermittent poor connections . in this paper , we find 99.9 % of the gradient exchange in distributed sgd is redundant , and propose deep gradient compression ( dgc ) to greatly reduce the communication bandwidth . to preserve accuracy during compression , dgc employs four methods : momentum correction , local gradient clipping , momentum factor masking , and warm-up training . we have applied deep gradient compression to image classification , speech recognition , and language modeling with multiple datasets including cifar10 , imagenet , penn treebank , and librispeech corpus . on these scenarios , deep gradient compression achieves a gradient compression ratio from 270x to 600x without losing accuracy , cutting the gradient size of resnet-50 from 97mb to 0.35mb , and for deepspeech from 488mb to 0.74mb . deep gradient compression enables large-scale distributed training on inexpensive commodity 1gbps ethernet and facilitates distributed training on mobile .", "topics": ["computer vision", "gradient"]}
{"title": "view subspaces for indexing and retrieval of 3d models", "abstract": "view-based indexing schemes for 3d object retrieval are gaining popularity since they provide good retrieval results . these schemes are coherent with the theory that humans recognize objects based on their 2d appearances . the viewbased techniques also allow users to search with various queries such as binary images , range images and even 2d sketches . the previous view-based techniques use classical 2d shape descriptors such as fourier invariants , zernike moments , scale invariant feature transform-based local features and 2d digital fourier transform coefficients . these methods describe each object independent of others . in this work , we explore data driven subspace models , such as principal component analysis , independent component analysis and nonnegative matrix factorization to describe the shape information of the views . we treat the depth images obtained from various points of the view sphere as 2d intensity images and train a subspace to extract the inherent structure of the views within a database . we also show the benefit of categorizing shapes according to their eigenvalue spread . both the shape categorization and data-driven feature set conjectures are tested on the psb database and compared with the competitor view-based 3d shape retrieval algorithms", "topics": ["coefficient"]}
{"title": "a sensitivity analysis of pathfinder", "abstract": "knowledge elicitation is one of the major bottlenecks in expert system design . systems based on bayes nets require two types of information -- network structure and parameters ( or probabilities ) . both must be elicited from the domain expert . in general , parameters have greater opacity than structure , and more time is spent in their refinement than in any other phase of elicitation . thus , it is important to determine the point of diminishing returns , beyond which further refinements will promise little ( if any ) improvement . sensitivity analyses address precisely this issue -- the sensitivity of a model to the precision of its parameters . in this paper , we report the results of a sensitivity analysis of pathfinder , a bayes net based system for diagnosing pathologies of the lymph system . this analysis is intended to shed some light on the relative importance of structure and parameters to system performance , as well as the sensitivity of a system based on a bayes net to noise in its assessed parameters .", "topics": ["bayesian network"]}
{"title": "a real world implementation of answer extraction", "abstract": "in this paper we describe extrans , an answer extraction system . answer extraction ( ae ) aims at retrieving those exact passages of a document that directly answer a given user question . ae is more ambitious than information retrieval and information extraction in that the retrieval results are phrases , not entire documents , and in that the queries may be arbitrarily specific . it is less ambitious than full-fledged question answering in that the answers are not generated from a knowledge base but looked up in the text of documents . the current version of extrans is able to parse unedited unix `` man pages '' , and derive the logical form of their sentences . user queries are also translated into logical forms . a theorem prover then retrieves the relevant phrases , which are presented through selective highlighting in their context .", "topics": ["parsing"]}
{"title": "bijective faithful translations among default logics", "abstract": "in this article , we study translations between variants of defaults logics such that the extensions of the theories that are the input and the output of the translation are in a bijective correspondence . we assume that a translation can introduce new variables and that the result of translating a theory can either be produced in time polynomial in the size of the theory or its output is polynomial in that size ; we however restrict to the case in which the original theory has extensions . this study fills a gap between two previous pieces of work , one studying bijective translations among restrictions of default logics , and the other one studying non-bijective translations between default logics variants .", "topics": ["polynomial"]}
{"title": "orthogonal machine learning : power and limitations", "abstract": "double machine learning provides $ \\sqrt { n } $ -consistent estimates of parameters of interest even when high-dimensional or nonparametric nuisance parameters are estimated at an $ n^ { -1/4 } $ rate . the key is to employ neyman-orthogonal moment equations which are first-order insensitive to perturbations in the nuisance parameters . we show that the $ n^ { -1/4 } $ requirement can be improved to $ n^ { -1/ ( 2k+2 ) } $ by employing a $ k $ -th order notion of orthogonality that grants robustness to more complex or higher-dimensional nuisance parameters . in the partially linear regression setting popular in causal inference , we show that we can construct second-order orthogonal moments if and only if the treatment residual is not normally distributed . our proof relies on stein 's lemma and may be of independent interest . we conclude by demonstrating the robustness benefits of an explicit doubly-orthogonal estimation procedure for treatment effect .", "topics": ["causality"]}
{"title": "the rank-frequency analysis for the functional style corpora in the ukrainian language", "abstract": "we use the rank-frequency analysis for the estimation of kernel vocabulary size within specific corpora of ukrainian . the extrapolation of high-rank behaviour is utilized for estimation of the total vocabulary size .", "topics": ["text corpus"]}
{"title": "online learning with feedback graphs : beyond bandits", "abstract": "we study a general class of online learning problems where the feedback is specified by a graph . this class includes online prediction with expert advice and the multi-armed bandit problem , but also several learning problems where the online player does not necessarily observe his own loss . we analyze how the structure of the feedback graph controls the inherent difficulty of the induced $ t $ -round learning problem . specifically , we show that any feedback graph belongs to one of three classes : strongly observable graphs , weakly observable graphs , and unobservable graphs . we prove that the first class induces learning problems with $ \\widetilde\\theta ( \\alpha^ { 1/2 } t^ { 1/2 } ) $ minimax regret , where $ \\alpha $ is the independence number of the underlying graph ; the second class induces problems with $ \\widetilde\\theta ( \\delta^ { 1/3 } t^ { 2/3 } ) $ minimax regret , where $ \\delta $ is the domination number of a certain portion of the graph ; and the third class induces problems with linear minimax regret . our results subsume much of the previous work on learning with feedback graphs and reveal new connections to partial monitoring games . we also show how the regret is affected if the graphs are allowed to vary with time .", "topics": ["regret ( decision theory )"]}
{"title": "learning soft linear constraints with application to citation field extraction", "abstract": "accurately segmenting a citation string into fields for authors , titles , etc . is a challenging task because the output typically obeys various global constraints . previous work has shown that modeling soft constraints , where the model is encouraged , but not require to obey the constraints , can substantially improve segmentation performance . on the other hand , for imposing hard constraints , dual decomposition is a popular technique for efficient prediction given existing algorithms for unconstrained inference . we extend the technique to perform prediction subject to soft constraints . moreover , with a technique for performing inference given soft constraints , it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training . this allows us to obtain substantial gains in accuracy on a new , challenging citation extraction dataset .", "topics": ["optimization problem"]}
{"title": "switching between hidden markov models using fixed share", "abstract": "in prediction with expert advice the goal is to design online prediction algorithms that achieve small regret ( additional loss on the whole data ) compared to a reference scheme . in the simplest such scheme one compares to the loss of the best expert in hindsight . a more ambitious goal is to split the data into segments and compare to the best expert on each segment . this is appropriate if the nature of the data changes between segments . the standard fixed-share algorithm is fast and achieves small regret compared to this scheme . fixed share treats the experts as black boxes : there are no assumptions about how they generate their predictions . but if the experts are learning , the following question arises : should the experts learn from all data or only from data in their own segment ? the original algorithm naturally addresses the first case . here we consider the second option , which is more appropriate exactly when the nature of the data changes between segments . in general extending fixed share to this second case will slow it down by a factor of t on t outcomes . we show , however , that no such slowdown is necessary if the experts are hidden markov models .", "topics": ["regret ( decision theory )"]}
{"title": "delving deep into rectifiers : surpassing human-level performance on imagenet classification", "abstract": "rectified activation units ( rectifiers ) are essential for state-of-the-art neural networks . in this work , we study rectifier neural networks for image classification from two aspects . first , we propose a parametric rectified linear unit ( prelu ) that generalizes the traditional rectified unit . prelu improves model fitting with nearly zero extra computational cost and little overfitting risk . second , we derive a robust initialization method that particularly considers the rectifier nonlinearities . this method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures . based on our prelu networks ( prelu-nets ) , we achieve 4.94 % top-5 test error on the imagenet 2012 classification dataset . this is a 26 % relative improvement over the ilsvrc 2014 winner ( googlenet , 6.66 % ) . to our knowledge , our result is the first to surpass human-level performance ( 5.1 % , russakovsky et al . ) on this visual recognition challenge .", "topics": ["computer vision"]}
{"title": "communication-efficient distributed sparse linear discriminant analysis", "abstract": "we propose a communication-efficient distributed estimation method for sparse linear discriminant analysis ( lda ) in the high dimensional regime . our method distributes the data of size $ n $ into $ m $ machines , and estimates a local sparse lda estimator on each machine using the data subset of size $ n/m $ . after the distributed estimation , our method aggregates the debiased local estimators from $ m $ machines , and sparsifies the aggregated estimator . we show that the aggregated estimator attains the same statistical rate as the centralized estimation method , as long as the number of machines $ m $ is chosen appropriately . moreover , we prove that our method can attain the model selection consistency under a milder condition than the centralized method . experiments on both synthetic and real datasets corroborate our theory .", "topics": ["synthetic data", "sparse matrix"]}
{"title": "online adaptive decision fusion framework based on entropic projections onto convex sets with application to wildfire detection in video", "abstract": "in this paper , an entropy functional based online adaptive decision fusion ( eadf ) framework is developed for image analysis and computer vision applications . in this framework , it is assumed that the compound algorithm consists of several sub-algorithms each of which yielding its own decision as a real number centered around zero , representing the confidence level of that particular sub-algorithm . decision values are linearly combined with weights which are updated online according to an active fusion method based on performing entropic projections onto convex sets describing sub-algorithms . it is assumed that there is an oracle , who is usually a human operator , providing feedback to the decision fusion method . a video based wildfire detection system is developed to evaluate the performance of the algorithm in handling the problems where data arrives sequentially . in this case , the oracle is the security guard of the forest lookout tower verifying the decision of the combined algorithm . simulation results are presented . the eadf framework is also tested with a standard dataset .", "topics": ["computer vision", "simulation"]}
{"title": "the extended parameter filter", "abstract": "the parameters of temporal models , such as dynamic bayesian networks , may be modelled in a bayesian context as static or atemporal variables that influence transition probabilities at every time step . particle filters fail for models that include such variables , while methods that use gibbs sampling of parameter variables may incur a per-sample cost that grows linearly with the length of the observation sequence . storvik devised a method for incremental computation of exact sufficient statistics that , for some cases , reduces the per-sample cost to a constant . in this paper , we demonstrate a connection between storvik 's filter and a kalman filter in parameter space and establish more general conditions under which storvik 's filter works . drawing on an analogy to the extended kalman filter , we develop and analyze , both theoretically and experimentally , a taylor approximation to the parameter posterior that allows storvik 's method to be applied to a broader class of models . our experiments on both synthetic examples and real applications show improvement over existing methods .", "topics": ["sampling ( signal processing )", "synthetic data"]}
{"title": "zero-shot hashing via transferring supervised knowledge", "abstract": "hashing has shown its efficiency and effectiveness in facilitating large-scale multimedia applications . supervised knowledge e.g . semantic labels or pair-wise relationship ) associated to data is capable of significantly improving the quality of hash codes and hash functions . however , confronted with the rapid growth of newly-emerging concepts and multimedia data on the web , existing supervised hashing approaches may easily suffer from the scarcity and validity of supervised information due to the expensive cost of manual labelling . in this paper , we propose a novel hashing scheme , termed \\emph { zero-shot hashing } ( zsh ) , which compresses images of `` unseen '' categories to binary codes with hash functions learned from limited training data of `` seen '' categories . specifically , we project independent data labels i.e . 0/1-form label vectors ) into semantic embedding space , where semantic relationships among all the labels can be precisely characterized and thus seen supervised knowledge can be transferred to unseen classes . moreover , in order to cope with the semantic shift problem , we rotate the embedded space to more suitably align the embedded semantics with the low-level visual feature space , thereby alleviating the influence of semantic gap . in the meantime , to exert positive effects on learning high-quality hash functions , we further propose to preserve local structural property and discrete nature in binary codes . besides , we develop an efficient alternating algorithm to solve the zsh model . extensive experiments conducted on various real-life datasets show the superior zero-shot image retrieval performance of zsh as compared to several state-of-the-art hashing methods .", "topics": ["test set", "supervised learning"]}
{"title": "linearized alternating direction method with adaptive penalty and warm starts for fast solving transform invariant low-rank textures", "abstract": "transform invariant low-rank textures ( tilt ) is a novel and powerful tool that can effectively rectify a rich class of low-rank textures in 3d scenes from 2d images despite significant deformation and corruption . the existing algorithm for solving tilt is based on the alternating direction method ( adm ) . it suffers from high computational cost and is not theoretically guaranteed to converge to a correct solution . in this paper , we propose a novel algorithm to speed up solving tilt , with guaranteed convergence . our method is based on the recently proposed linearized alternating direction method with adaptive penalty ( ladmap ) . to further reduce computation , warm starts are also introduced to initialize the variables better and cut the cost on singular value decomposition . extensive experimental results on both synthetic and real data demonstrate that this new algorithm works much more efficiently and robustly than the existing algorithm . it could be at least five times faster than the previous method .", "topics": ["synthetic data", "computation"]}
{"title": "a generalised seizure prediction with convolutional neural networks for intracranial and scalp electroencephalogram data analysis", "abstract": "seizure prediction has attracted a growing attention as one of the most challenging predictive data analysis efforts in order to improve the life of patients living with drug-resistant epilepsy and tonic seizures . many outstanding works have been reporting great results in providing a sensible indirect ( warning systems ) or direct ( interactive neural-stimulation ) control over refractory seizures , some of which achieved high performance . however , many works put heavily handcraft feature extraction and/or carefully tailored feature engineering to each patient to achieve very high sensitivity and low false prediction rate for a particular dataset . this limits the benefit of their approaches if a different dataset is used . in this paper we apply convolutional neural networks ( cnns ) on different intracranial and scalp electroencephalogram ( eeg ) datasets and proposed a generalized retrospective and patient-specific seizure prediction method . we use short-time fourier transform ( stft ) on 30-second eeg windows with 50 % overlapping to extract information in both frequency and time domains . a standardization step is then applied on stft components across the whole frequency range to prevent high frequencies features being influenced by those at lower frequencies . a convolutional neural network model is used for both feature extraction and classification to separate preictal segments from interictal ones . the proposed approach achieves sensitivity of 81.4 % , 81.2 % , 82.3 % and false prediction rate ( fpr ) of 0.06/h , 0.16/h , 0.22/h on freiburg hospital intracranial eeg ( ieeg ) dataset , children 's hospital of boston-mit scalp eeg ( seeg ) dataset , and kaggle american epilepsy society seizure prediction challenge 's dataset , respectively . our prediction method is also statistically better than an unspecific random predictor for most of patients in all three datasets .", "topics": ["feature extraction"]}
{"title": "best viewpoint tracking for camera mounted on robotic arm with dynamic obstacles", "abstract": "the problem of finding a next best viewpoint for 3d modeling or scene mapping has been explored in computer vision over the last decade . this paper tackles a similar problem , but with different characteristics . it proposes a method for dynamic next best viewpoint recovery of a target point while avoiding possible occlusions . since the environment can change , the method has to iteratively find the next best view with a global understanding of the free and occupied parts . we model the problem as a set of possible viewpoints which correspond to the centers of the facets of a virtual tessellated hemisphere covering the scene . taking into account occlusions , distances between current and future viewpoints , quality of the viewpoint and joint constraints ( robot arm joint distances or limits ) , we evaluate the next best viewpoint . the proposal has been evaluated on 8 different scenarios with different occlusions and a short 3d video sequence to validate its dynamic performance .", "topics": ["computer vision"]}
{"title": "dual memory architectures for fast deep learning of stream data via an online-incremental-transfer strategy", "abstract": "the online learning of deep neural networks is an interesting problem of machine learning because , for example , major it companies want to manage the information of the massive data uploaded on the web daily , and this technology can contribute to the next generation of lifelong learning . we aim to train deep models from new data that consists of new classes , distributions , and tasks at minimal computational cost , which we call online deep learning . unfortunately , deep neural network learning through classical online and incremental methods does not work well in both theory and practice . in this paper , we introduce dual memory architectures for online incremental deep learning . the proposed architecture consists of deep representation learners and fast learnable shallow kernel networks , both of which synergize to track the information of new data . during the training phase , we use various online , incremental ensemble , and transfer learning techniques in order to achieve lower error of the architecture . on the mnist , cifar-10 , and imagenet image recognition tasks , the proposed dual memory architectures performs much better than the classical online and incremental ensemble algorithm , and their accuracies are similar to that of the batch learner .", "topics": ["computer vision", "mnist database"]}
{"title": "an approach to reducing annotation costs for bionlp", "abstract": "there is a broad range of bionlp tasks for which active learning ( al ) can significantly reduce annotation costs and a specific al algorithm we have developed is particularly effective in reducing annotation costs for these tasks . we have previously developed an al algorithm called closestinitpa that works best with tasks that have the following characteristics : redundancy in training material , burdensome annotation costs , support vector machines ( svms ) work well for the task , and imbalanced datasets ( i.e . when set up as a binary classification problem , one class is substantially rarer than the other ) . many bionlp tasks have these characteristics and thus our al algorithm is a natural approach to apply to bionlp tasks .", "topics": ["support vector machine"]}
{"title": "extended object tracking : introduction , overview and applications", "abstract": "this article provides an elaborate overview of current research in extended object tracking . we provide a clear definition of the extended object tracking problem and discuss its delimitation to other types of object tracking . next , different aspects of extended object modelling are extensively discussed . subsequently , we give a tutorial introduction to two basic and well used extended object tracking approaches - the random matrix approach and the kalman filter-based approach for star-convex shapes . the next part treats the tracking of multiple extended objects and elaborates how the large number of feasible association hypotheses can be tackled using both random finite set ( rfs ) and non-rfs multi-object trackers . the article concludes with a summary of current applications , where four example applications involving camera , x-band radar , light detection and ranging ( lidar ) , red-green-blue-depth ( rgb-d ) sensors are highlighted .", "topics": ["sensor"]}
{"title": "coloring black boxes : visualization of neural network decisions", "abstract": "neural networks are commonly regarded as black boxes performing incomprehensible functions . for classification problems networks provide maps from high dimensional feature space to k-dimensional image space . images of training vector are projected on polygon vertices , providing visualization of network function . such visualization may show the dynamics of learning , allow for comparison of different networks , display training vectors around which potential problems may arise , show differences due to regularization and optimization procedures , investigate stability of network classification under perturbation of original vectors , and place new data sample in relation to training data , allowing for estimation of confidence in classification of a given sample . an illustrative example for the three-class wine data and five-class satimage data is described . the visualization method proposed here is applicable to any black box system that provides continuous outputs .", "topics": ["test set", "feature vector"]}
{"title": "analysis of nonautonomous adversarial systems", "abstract": "generative adversarial networks are used to generate images but still their convergence properties are not well understood . there have been a few studies who intended to investigate the stability properties of gans as a dynamical system . this short writing can be seen in that direction . among the proposed methods for stabilizing training of gans , { \\ss } -gan was the first who proposed a complete annealing strategy to change high-level conditions of the gan objective . in this note , we show by a simple example how annealing strategy works in gans . the theoretical analysis is supported by simple simulations .", "topics": ["high- and low-level", "simulation"]}
{"title": "designing kernel scheme for classifiers fusion", "abstract": "in this paper , we propose a special fusion method for combining ensembles of base classifiers utilizing new neural networks in order to improve overall efficiency of classification . while ensembles are designed such that each classifier is trained independently while the decision fusion is performed as a final procedure , in this method , we would be interested in making the fusion process more adaptive and efficient . this new combiner , called neural network kernel least mean square1 , attempts to fuse outputs of the ensembles of classifiers . the proposed neural network has some special properties such as kernel abilities , least mean square features , easy learning over variants of patterns and traditional neuron capabilities . neural network kernel least mean square is a special neuron which is trained with kernel least mean square properties . this new neuron is used as a classifiers combiner to fuse outputs of base neural network classifiers . performance of this method is analyzed and compared with other fusion methods . the analysis represents higher performance of our new method as opposed to others .", "topics": ["kernel ( operating system )"]}
{"title": "closed-form solution to cooperative visual-inertial structure from motion", "abstract": "this paper considers the problem of visual-inertial sensor fusion in the cooperative case and it provides new theoretical contributions , which regard its observability and its resolvability in closed form . the case of two agents is investigated . each agent is equipped with inertial sensors ( accelerometer and gyroscope ) and with a monocular camera . by using the monocular camera , each agent can observe the other agent . no additional camera observations ( e.g . , of external point features in the environment ) are considered . all the inertial sensors are assumed to be affected by a bias . first , the entire observable state is analytically derived . this state includes the absolute scale , the relative velocity between the two agents , the three euler angles that express the rotation between the two agent frames and all the accelerometer and gyroscope biases . second , the paper provides the extension of the closed-form solution given in [ 19 ] ( which holds for a single agent ) to the aforementioned cooperative case . the impact of the presence of the bias on the performance of this closed-form solution is investigated . as in the case of a single agent , this performance is significantly sensitive to the presence of a bias on the gyroscope , while , the presence of a bias on the accelerometer is negligible . finally , a simple and effective method to obtain the gyroscope bias is proposed . extensive simulations clearly show that the proposed method is successful . it is amazing that , it is possible to automatically retrieve the absolute scale and simultaneously calibrate the gyroscopes not only without any prior knowledge ( as in [ 13 ] ) , but also without external point features in the environment .", "topics": ["simulation", "sensor"]}
{"title": "an exploration of softmax alternatives belonging to the spherical loss family", "abstract": "in a multi-class classification problem , it is standard to model the output of a neural network as a categorical distribution conditioned on the inputs . the output must therefore be positive and sum to one , which is traditionally enforced by a softmax . this probabilistic mapping allows to use the maximum likelihood principle , which leads to the well-known log-softmax loss . however the choice of the softmax function seems somehow arbitrary as there are many other possible normalizing functions . it is thus unclear why the log-softmax loss would perform better than other loss alternatives . in particular vincent et al . ( 2015 ) recently introduced a class of loss functions , called the spherical family , for which there exists an efficient algorithm to compute the updates of the output weights irrespective of the output size . in this paper , we explore several loss functions from this family as possible alternatives to the traditional log-softmax . in particular , we focus our investigation on spherical bounds of the log-softmax loss and on two spherical log-likelihood losses , namely the log-spherical softmax suggested by vincent et al . ( 2015 ) and the log-taylor softmax that we introduce . although these alternatives do not yield as good results as the log-softmax loss on two language modeling tasks , they surprisingly outperform it in our experiments on mnist and cifar-10 , suggesting that they might be relevant in a broad range of applications .", "topics": ["loss function", "mnist database"]}
{"title": "fuzzy rules and evidence theory for satellite image analysis", "abstract": "design of a fuzzy rule based classifier is proposed . the performance of the classifier for multispectral satellite image classification is improved using dempster- shafer theory of evidence that exploits information of the neighboring pixels . the classifiers are tested rigorously with two known images and their performance are found to be better than the results available in the literature . we also demonstrate the improvement of performance while using d-s theory along with fuzzy rule based classifiers over the basic fuzzy rule based classifiers for all the test cases .", "topics": ["computer vision", "pixel"]}
{"title": "session-based recommendations with recurrent neural networks", "abstract": "we apply recurrent neural networks ( rnn ) on a new domain , namely recommender systems . real-life recommender systems often face the problem of having to base recommendations only on short session-based data ( e.g . a small sportsware website ) instead of long user histories ( as in the case of netflix ) . in this situation the frequently praised matrix factorization approaches are not accurate . this problem is usually overcome in practice by resorting to item-to-item recommendations , i.e . recommending similar items . we argue that by modeling the whole session , more accurate recommendations can be provided . we therefore propose an rnn-based approach for session-based recommendations . our approach also considers practical aspects of the task and introduces several modifications to classic rnns such as a ranking loss function that make it more viable for this specific problem . experimental results on two data-sets show marked improvements over widely used approaches .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "building program vector representations for deep learning", "abstract": "deep learning has made significant breakthroughs in various fields of artificial intelligence . advantages of deep learning include the ability to capture highly complicated features , weak involvement of human engineering , etc . however , it is still virtually impossible to use deep learning to analyze programs since deep architectures can not be trained effectively with pure back propagation . in this pioneering paper , we propose the `` coding criterion '' to build program vector representations , which are the premise of deep learning for program analysis . our representation learning approach directly makes deep learning a reality in this new field . we evaluate the learned vector representations both qualitatively and quantitatively . we conclude , based on the experiments , the coding criterion is successful in building program representations . to evaluate whether deep learning is beneficial for program analysis , we feed the representations to deep neural networks , and achieve higher accuracy in the program classification task than `` shallow '' methods , such as logistic regression and the support vector machine . this result confirms the feasibility of deep learning to analyze programs . it also gives primary evidence of its success in this new field . we believe deep learning will become an outstanding technique for program analysis in the near future .", "topics": ["feature learning", "support vector machine"]}
{"title": "impact of feature selection on micro-text classification", "abstract": "social media datasets , especially twitter tweets , are popular in the field of text classification . tweets are a valuable source of micro-text ( sometimes referred to as `` micro-blogs '' ) , and have been studied in domains such as sentiment analysis , recommendation systems , spam detection , clustering , among others . tweets often include keywords referred to as `` hashtags '' that can be used as labels for the tweet . using tweets encompassing 50 labels , we studied the impact of word versus character-level feature selection and extraction on different learners to solve a multi-class classification task . we show that feature extraction of simple character-level groups performs better than simple word groups and pre-processing methods like normalizing using porter 's stemming and part-of-speech ( `` pos '' ) -lemmatization .", "topics": ["cluster analysis", "feature extraction"]}
{"title": "random projections for linear support vector machines", "abstract": "let x be a data matrix of rank \\rho , whose rows represent n points in d-dimensional space . the linear support vector machine constructs a hyperplane separator that maximizes the 1-norm soft margin . we develop a new oblivious dimension reduction technique which is precomputed and can be applied to any input matrix x . we prove that , with high probability , the margin and minimum enclosing ball in the feature space are preserved to within \\epsilon-relative error , ensuring comparable generalization as in the original space in the case of classification . for regression , we show that the margin is preserved to \\epsilon-relative error with high probability . we present extensive experiments with real and synthetic data to support our theory .", "topics": ["feature vector", "support vector machine"]}
{"title": "quantization based fast inner product search", "abstract": "we propose a quantization based approach for fast approximate maximum inner product search ( mips ) . each database vector is quantized in multiple subspaces via a set of codebooks , learned directly by minimizing the inner product quantization error . then , the inner product of a query to a database vector is approximated as the sum of inner products with the subspace quantizers . different from recently proposed lsh approaches to mips , the database vectors and queries do not need to be augmented in a higher dimensional feature space . we also provide a theoretical analysis of the proposed approach , consisting of the concentration results under mild assumptions . furthermore , if a small sample of example queries is given at the training time , we propose a modified codebook learning procedure which further improves the accuracy . experimental results on a variety of datasets including those arising from deep neural networks show that the proposed approach significantly outperforms the existing state-of-the-art .", "topics": ["approximation algorithm", "feature vector"]}
{"title": "unification-based glossing", "abstract": "we present an approach to syntax-based machine translation that combines unification-style interpretation with statistical processing . this approach enables us to translate any japanese newspaper article into english , with quality far better than a word-for-word translation . novel ideas include the use of feature structures to encode word lattices and the use of unification to compose and manipulate lattices . unification also allows us to specify abstract features that delay target-language synthesis until enough source-language information is assembled . our statistical component enables us to search efficiently among competing translations and locate those with high english fluency .", "topics": ["machine translation", "parsing"]}
{"title": "on the compressive power of deep rectifier networks for high resolution representation of class boundaries", "abstract": "this paper provides a theoretical justification of the superior classification performance of deep rectifier networks over shallow rectifier networks from the geometrical perspective of piecewise linear ( pwl ) classifier boundaries . we show that , for a given threshold on the approximation error , the required number of boundary facets to approximate a general smooth boundary grows exponentially with the dimension of the data , and thus the number of boundary facets , referred to as boundary resolution , of a pwl classifier is an important quality measure that can be used to estimate a lower bound on the classification errors . however , learning naively an exponentially large number of boundary facets requires the determination of an exponentially large number of parameters and also requires an exponentially large number of training patterns . to overcome this issue of `` curse of dimensionality '' , compressive representations of high resolution classifier boundaries are required . to show the superior compressive power of deep rectifier networks over shallow rectifier networks , we prove that the maximum boundary resolution of a single hidden layer rectifier network classifier grows exponentially with the number of units when this number is smaller than the dimension of the patterns . when the number of units is larger than the dimension of the patterns , the growth rate is reduced to a polynomial order . consequently , the capacity of generating a high resolution boundary will increase if the same large number of units are arranged in multiple layers instead of a single hidden layer . taking high dimensional spherical boundaries as examples , we show how deep rectifier networks can utilize geometric symmetries to approximate a boundary with the same accuracy but with a significantly fewer number of parameters than single hidden layer nets .", "topics": ["approximation algorithm", "polynomial"]}
{"title": "iterative random forests to detect predictive and stable high-order interactions", "abstract": "genomics has revolutionized biology , enabling the interrogation of whole transcriptomes , genome-wide binding sites for proteins , and many other molecular processes . however , individual genomic assays measure elements that interact in vivo as components of larger molecular machines . understanding how these high-order interactions drive gene expression presents a substantial statistical challenge . building on random forests ( rf ) , random intersection trees ( rits ) , and through extensive , biologically inspired simulations , we developed the iterative random forest algorithm ( irf ) . irf trains a feature-weighted ensemble of decision trees to detect stable , high-order interactions with same order of computational cost as rf . we demonstrate the utility of irf for high-order interaction discovery in two prediction problems : enhancer activity in the early drosophila embryo and alternative splicing of primary transcripts in human derived cell lines . in drosophila , among the 20 pairwise transcription factor interactions irf identifies as stable ( returned in more than half of bootstrap replicates ) , 80 % have been previously reported as physical interactions . moreover , novel third-order interactions , e.g . between zelda ( zld ) , giant ( gt ) , and twist ( twi ) , suggest high-order relationships that are candidates for follow-up experiments . in human-derived cells , irf re-discovered a central role of h3k36me3 in chromatin-mediated splicing regulation , and identified novel 5th and 6th order interactions , indicative of multi-valent nucleosomes with specific roles in splicing regulation . by decoupling the order of interactions from the computational cost of identification , irf opens new avenues of inquiry into the molecular mechanisms underlying genome biology .", "topics": ["interaction", "simulation"]}
{"title": "categorizing variants of goodhart 's law", "abstract": "there are several distinct failure modes for overoptimization of systems on the basis of metrics . this occurs when a metric which can be used to improve a system is used to an extent that further optimization is ineffective or harmful , and is sometimes termed goodhart 's law . this class of failure is often poorly understood , partly because terminology for discussing them is ambiguous , and partly because discussion using this ambiguous terminology ignores distinctions between different failure modes of this general type . this paper expands on an earlier discussion by garrabrant , which notes there are `` ( at least ) four different mechanisms '' that relate to goodhart 's law . this paper is intended to explore these mechanisms further , and specify more clearly how they occur . this discussion should be helpful in better understanding these types of failures in economic regulation , in public policy , in machine learning , and in artificial intelligence alignment . the importance of goodhart effects depends on the amount of power directed towards optimizing the proxy , and so the increased optimization power offered by artificial intelligence makes it especially critical for that field .", "topics": ["artificial intelligence", "artificial intelligence"]}
{"title": "dual learning for machine translation", "abstract": "while neural machine translation ( nmt ) is making good progress in the past two years , tens of millions of bilingual sentence pairs are needed for its training . however , human labeling is very costly . to tackle this training data bottleneck , we develop a dual-learning mechanism , which can enable an nmt system to automatically learn from unlabeled data through a dual-learning game . this mechanism is inspired by the following observation : any machine translation task has a dual task , e.g . , english-to-french translation ( primal ) versus french-to-english translation ( dual ) ; the primal and dual tasks can form a closed loop , and generate informative feedback signals to train the translation models , even if without the involvement of a human labeler . in the dual-learning mechanism , we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task , then ask them to teach each other through a reinforcement learning process . based on the feedback signals generated during this process ( e.g . , the language-model likelihood of the output of a model , and the reconstruction error of the original sentence after the primal and dual translations ) , we can iteratively update the two models until convergence ( e.g . , using the policy gradient methods ) . we call the corresponding approach to neural machine translation \\emph { dual-nmt } . experiments show that dual-nmt works very well on english $ \\leftrightarrow $ french translation ; especially , by learning from monolingual data ( with 10 % bilingual data for warm start ) , it achieves a comparable accuracy to nmt trained from the full bilingual data for the french-to-english translation task .", "topics": ["test set", "machine translation"]}
{"title": "enhanced experience replay generation for efficient reinforcement learning", "abstract": "applying deep reinforcement learning ( rl ) on real systems suffers from slow data sampling . we propose an enhanced generative adversarial network ( egan ) to initialize an rl agent in order to achieve faster learning . the egan utilizes the relation between states and actions to enhance the quality of data samples generated by a gan . pre-training the agent with the egan shows a steeper learning curve with a 20 % improvement of training time in the beginning of learning , compared to no pre-training , and an improvement compared to training with gan by about 5 % with smaller variations . for real time systems with sparse and slow data sampling the egan could be used to speed up the early phases of the training process .", "topics": ["sampling ( signal processing )", "reinforcement learning"]}
{"title": "on the expressibility of stable logic programming", "abstract": "( we apologize for pidgin latex ) schlipf \\cite { sch91 } proved that stable logic programming ( slp ) solves all $ \\mathit { np } $ decision problems . we extend schlipf 's result to prove that slp solves all search problems in the class $ \\mathit { np } $ . moreover , we do this in a uniform way as defined in \\cite { mt99 } . specifically , we show that there is a single $ \\mathrm { datalog } ^ { \\neg } $ program $ p_ { \\mathit { trg } } $ such that given any turing machine $ m $ , any polynomial $ p $ with non-negative integer coefficients and any input $ \\sigma $ of size $ n $ over a fixed alphabet $ \\sigma $ , there is an extensional database $ \\mathit { edb } _ { m , p , \\sigma } $ such that there is a one-to-one correspondence between the stable models of $ \\mathit { edb } _ { m , p , \\sigma } \\cup p_ { \\mathit { trg } } $ and the accepting computations of the machine $ m $ that reach the final state in at most $ p ( n ) $ steps . moreover , $ \\mathit { edb } _ { m , p , \\sigma } $ can be computed in polynomial time from $ p $ , $ \\sigma $ and the description of $ m $ and the decoding of such accepting computations from its corresponding stable model of $ \\mathit { edb } _ { m , p , \\sigma } \\cup p_ { \\mathit { trg } } $ can be computed in linear time . a similar statement holds for default logic with respect to $ \\sigma_2^\\mathrm { p } $ -search problems\\footnote { the proof of this result involves additional technical complications and will be a subject of another publication . } .", "topics": ["time complexity", "computation"]}
{"title": "joint learning of siamese cnns and temporally constrained metrics for tracklet association", "abstract": "in this paper , we study the challenging problem of multi-object tracking in a complex scene captured by a single camera . different from the existing tracklet association-based tracking methods , we propose a novel and efficient way to obtain discriminative appearance-based tracklet affinity models . our proposed method jointly learns the convolutional neural networks ( cnns ) and temporally constrained metrics . in our method , a siamese convolutional neural network ( cnn ) is first pre-trained on the auxiliary data . then the siamese cnn and temporally constrained metrics are jointly learned online to construct the appearance-based tracklet affinity models . the proposed method can jointly learn the hierarchical deep features and temporally constrained segment-wise metrics under a unified framework . for reliable association between tracklets , a novel loss function incorporating temporally constrained multi-task learning mechanism is proposed . by employing the proposed method , tracklet association can be accomplished even in challenging situations . moreover , a new dataset with 40 fully annotated sequences is created to facilitate the tracking evaluation . experimental results on five public datasets and the new large-scale dataset show that our method outperforms several state-of-the-art approaches in multi-object tracking .", "topics": ["loss function"]}
{"title": "handcrafted local features are convolutional neural networks", "abstract": "image and video classification research has made great progress through the development of handcrafted local features and learning based features . these two architectures were proposed roughly at the same time and have flourished at overlapping stages of history . however , they are typically viewed as distinct approaches . in this paper , we emphasize their structural similarities and show how such a unified view helps us in designing features that balance efficiency and effectiveness . as an example , we study the problem of designing efficient video feature learning algorithms for action recognition . we approach this problem by first showing that local handcrafted features and convolutional neural networks ( cnns ) share the same convolution-pooling network structure . we then propose a two-stream convolutional isa ( convisa ) that adopts the convolution-pooling structure of the state-of-the-art handcrafted video feature with greater modeling capacities and a cost-effective training algorithm . through custom designed network structures for pixels and optical flow , our method also reflects distinctive characteristics of these two data sources . our experimental results on standard action recognition benchmarks show that by focusing on the structure of cnns , rather than end-to-end training methods , we are able to design an efficient and powerful video feature learning algorithm .", "topics": ["computational complexity theory", "pixel"]}
{"title": "population empirical bayes", "abstract": "bayesian predictive inference analyzes a dataset to make predictions about new observations . when a model does not match the data , predictive accuracy suffers . we develop population empirical bayes ( pop-eb ) , a hierarchical framework that explicitly models the empirical population distribution as part of bayesian analysis . we introduce a new concept , the latent dataset , as a hierarchical variable and set the empirical population as its prior . this leads to a new predictive density that mitigates model mismatch . we efficiently apply this method to complex models by proposing a stochastic variational inference algorithm , called bumping variational inference ( bump-vi ) . we demonstrate improved predictive accuracy over classical bayesian inference in three models : a linear regression model of health data , a bayesian mixture model of natural images , and a latent dirichlet allocation topic model of scientific documents .", "topics": ["calculus of variations", "bayesian network"]}
{"title": "a sequence-based mesh classifier for the prediction of protein-protein interactions", "abstract": "the worldwide surge of multiresistant microbial strains has propelled the search for alternative treatment options . the study of protein-protein interactions ( ppis ) has been a cornerstone in the clarification of complex physiological and pathogenic processes , thus being a priority for the identification of vital components and mechanisms in pathogens . despite the advances of laboratorial techniques , computational models allow the screening of protein interactions between entire proteomes in a fast and inexpensive manner . here , we present a supervised machine learning model for the prediction of ppis based on the protein sequence . we cluster amino acids regarding their physicochemical properties , and use the discrete cosine transform to represent protein sequences . a mesh of classifiers was constructed to create hyper-specialised classifiers dedicated to the most relevant pairs of molecular function annotations from gene ontology . based on an exhaustive evaluation that includes datasets with different configurations , cross-validation and out-of-sampling validation , the obtained results outscore the state-of-the-art for sequence-based methods . for the final mesh model using svm with rbf , a consistent average auc of 0.84 was attained .", "topics": ["sampling ( signal processing )", "supervised learning"]}
{"title": "geometry of compositionality", "abstract": "this paper proposes a simple test for compositionality ( i.e . , literal usage ) of a word or phrase in a context-specific way . the test is computationally simple , relying on no external resources and only uses a set of trained word vectors . experiments show that the proposed method is competitive with state of the art and displays high accuracy in context-specific compositionality detection of a variety of natural language phenomena ( idiomaticity , sarcasm , metaphor ) for different datasets in multiple languages . the key insight is to connect compositionality to a curious geometric property of word embeddings , which is of independent interest .", "topics": ["natural language"]}
{"title": "a short introduction to nile", "abstract": "in this paper , we briefly introduce the narrative information linear extraction ( nile ) system , a natural language processing library for clinical narratives . nile is an experiment of our ideas on efficient and effective medical language processing . we introduce the overall design of nile and its major components , and show the performance of it in real projects .", "topics": ["natural language processing"]}
{"title": "classical planning in deep latent space : bridging the subsymbolic-symbolic boundary", "abstract": "current domain-independent , classical planners require symbolic models of the problem domain and instance as input , resulting in a knowledge acquisition bottleneck . meanwhile , although deep learning has achieved significant success in many fields , the knowledge is encoded in a subsymbolic representation which is incompatible with symbolic systems such as planners . we propose latplan , an unsupervised architecture combining deep learning and classical planning . given only an unlabeled set of image pairs showing a subset of transitions allowed in the environment ( training inputs ) , and a pair of images representing the initial and the goal states ( planning inputs ) , latplan finds a plan to the goal state in a symbolic latent space and returns a visualized plan execution . the contribution of this paper is twofold : ( 1 ) state autoencoder , which finds a propositional state representation of the environment using a variational autoencoder . it generates a discrete latent vector from the images , based on which a pddl model can be constructed and then solved by an off-the-shelf planner . ( 2 ) action autoencoder / discriminator , a neural architecture which jointly finds the action symbols and the implicit action models ( preconditions/effects ) , and provides a successor function for the implicit graph search . we evaluate latplan using image-based versions of 3 planning domains : 8-puzzle , towers of hanoi and lightsout .", "topics": ["calculus of variations", "artificial intelligence"]}
{"title": "a rough computing based performance evaluation approach for educational institutions", "abstract": "performance evaluation of various organizations especially educational institutions is a very important area of research and needs to be cultivated more . in this paper , we propose a performance evaluation for educational institutions using rough set on fuzzy approximation spaces with ordering rules and information entropy . in order to measure the performance of educational institutions , we construct an evaluation index system . rough set on fuzzy approximation spaces with ordering is applied to explore the evaluation index data of each level . furthermore , the concept of information entropy is used to determine the weighting coefficients of evaluation indexes . also , we find the most important indexes that influence the weighting coefficients . the proposed approach is validated and shows the practical viability . moreover , the proposed approach can be applicable to any organizations .", "topics": ["coefficient"]}
{"title": "optimization by a quantum reinforcement algorithm", "abstract": "a reinforcement algorithm solves a classical optimization problem by introducing a feedback to the system which slowly changes the energy landscape and converges the algorithm to an optimal solution in the configuration space . here , we use this strategy to concentrate ( localize ) preferentially the wave function of a quantum particle , which explores the configuration space of the problem , on an optimal configuration . we examine the method by solving numerically the equations governing the evolution of the system , which are similar to the nonlinear schr\\ '' odinger equations , for small problem sizes . in particular , we observe that reinforcement increases the minimal energy gap of the system in a quantum annealing algorithm . our numerical simulations and the latter observation show that such kind of quantum feedbacks might be helpful in solving a computationally hard optimization problem by a quantum reinforcement algorithm .", "topics": ["optimization problem", "nonlinear system"]}
{"title": "memory constraint online multitask classification", "abstract": "we investigate online kernel algorithms which simultaneously process multiple classification tasks while a fixed constraint is imposed on the size of their active sets . we focus in particular on the design of algorithms that can efficiently deal with problems where the number of tasks is extremely high and the task data are large scale . two new projection-based algorithms are introduced to efficiently tackle those issues while presenting different trade offs on how the available memory is managed with respect to the prior information about the learning tasks . theoretically sound budget algorithms are devised by coupling the randomized budget perceptron and the forgetron algorithms with the multitask kernel . we show how the two seemingly contrasting properties of learning from multiple tasks and keeping a constant memory footprint can be balanced , and how the sharing of the available space among different tasks is automatically taken care of . we propose and discuss new insights on the multitask kernel . experiments show that online kernel multitask algorithms running on a budget can efficiently tackle real world learning problems involving multiple tasks .", "topics": ["kernel ( operating system )"]}
{"title": "on reducing sampling variance in covariate shift using control variates", "abstract": "covariate shift classification problems can in principle be tackled by importance-weighting training samples . however , the sampling variance of the risk estimator is often scaled up dramatically by the weights . this means that during cross-validation - when the importance-weighted risk is repeatedly evaluated - suboptimal hyperparameter estimates are produced . we study the sampling variances of the importance-weighted versus the oracle estimator as a function of the relative scale of the training data . we show that introducing a control variate can reduce the variance of the importance-weighted risk estimator , which leads to superior regularization parameter estimates when the training data is much smaller in scale than the test data .", "topics": ["sampling ( signal processing )", "test set"]}
{"title": "resampling methods for parameter-free and robust feature selection with mutual information", "abstract": "combining the mutual information criterion with a forward feature selection strategy offers a good trade-off between optimality of the selected feature subset and computation time . however , it requires to set the parameter ( s ) of the mutual information estimator and to determine when to halt the forward procedure . these two choices are difficult to make because , as the dimensionality of the subset increases , the estimation of the mutual information becomes less and less reliable . this paper proposes to use resampling methods , a k-fold cross-validation and the permutation test , to address both issues . the resampling methods bring information about the variance of the estimator , information which can then be used to automatically set the parameter and to calculate a threshold to stop the forward procedure . the procedure is illustrated on a synthetic dataset as well as on real-world examples .", "topics": ["time complexity", "synthetic data"]}
{"title": "evaluation of hindi to punjabi machine translation system", "abstract": "machine translation in india is relatively young . the earliest efforts date from the late 80s and early 90s . the success of every system is judged from its evaluation experimental results . number of machine translation systems has been started for development but to the best of author knowledge , no high quality system has been completed which can be used in real applications . recently , punjabi university , patiala , india has developed punjabi to hindi machine translation system with high accuracy of about 92 % . both the systems i.e . system under question and developed system are between same closely related languages . thus , this paper presents the evaluation results of hindi to punjabi machine translation system . it makes sense to use same evaluation criteria as that of punjabi to hindi punjabi machine translation system . after evaluation , the accuracy of the system is found to be about 95 % .", "topics": ["machine translation"]}
{"title": "an outlier detection-based tree selection approach to extreme pruning of random forests", "abstract": "random forest ( rf ) is an ensemble classification technique that was developed by breiman over a decade ago . compared with other ensemble techniques , it has proved its accuracy and superiority . many researchers , however , believe that there is still room for enhancing and improving its performance in terms of predictive accuracy . this explains why , over the past decade , there have been many extensions of rf where each extension employed a variety of techniques and strategies to improve certain aspect ( s ) of rf . since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the constituent models , the objective of this paper is twofolds . first , it investigates how an unsupervised learning technique , namely , local outlier factor ( lof ) can be used to identify diverse trees in the rf . second , trees with the highest lof scores are then used to produce an extension of rf termed lofb-drf that is much smaller in size than rf , and yet performs at least as good as rf , but mostly exhibits higher performance in terms of accuracy . the latter refers to a known technique called ensemble pruning . experimental results on 10 real datasets prove the superiority of our proposed extension over the traditional rf . unprecedented pruning levels reaching 99 % have been achieved at the time of boosting the predictive accuracy of the ensemble . the notably high pruning level makes the technique a good candidate for real-time applications .", "topics": ["unsupervised learning"]}
{"title": "detecting burnscar from hyperspectral imagery via sparse representation with low-rank interference", "abstract": "in this paper , we propose a burnscar detection model for hyperspectral imaging ( hsi ) data . the proposed model contains two-processing steps in which the first step separate and then suppress the cloud information presenting in the data set using an rpca algorithm and the second step detect the burnscar area in the low-rank component output of the first step . experiments are conducted on the public modis dataset available at nasa official website .", "topics": ["computer vision", "sparse matrix"]}
{"title": "approximate inference algorithms for hybrid bayesian networks with discrete constraints", "abstract": "in this paper , we consider hybrid mixed networks ( hmn ) which are hybrid bayesian networks that allow discrete deterministic information to be modeled explicitly in the form of constraints . we present two approximate inference algorithms for hmns that integrate and adjust well known algorithmic principles such as generalized belief propagation , rao-blackwellised importance sampling and constraint propagation to address the complexity of modeling and reasoning in hmns . we demonstrate the performance of our approximate inference algorithms on randomly generated hmns .", "topics": ["approximation algorithm", "bayesian network"]}
{"title": "an agent based classification model", "abstract": "the major function of this model is to access the uci wisconsin breast can- cer data-set [ 1 ] and classify the data items into two categories , which are normal and anomalous . this kind of classifi cation can be referred as anomaly detection , which discriminates anomalous behaviour from normal behaviour in computer systems . one popular solution for anomaly detection is artifi cial immune sys- tems ( ais ) . ais are adaptive systems inspired by theoretical immunology and observed immune functions , principles and models which are applied to prob- lem solving . the dendritic cell algorithm ( dca ) [ 2 ] is an ais algorithm that is developed specifi cally for anomaly detection . it has been successfully applied to intrusion detection in computer security . it is believed that agent-based mod- elling is an ideal approach for implementing ais , as intelligent agents could be the perfect representations of immune entities in ais . this model evaluates the feasibility of re-implementing the dca in an agent-based simulation environ- ment called anylogic , where the immune entities in the dca are represented by intelligent agents . if this model can be successfully implemented , it makes it possible to implement more complicated and adaptive ais models in the agent-based simulation environment .", "topics": ["simulation", "entity"]}
{"title": "an approach to reachability analysis for feed-forward relu neural networks", "abstract": "we study the reachability problem for systems implemented as feed-forward neural networks whose activation function is implemented via relu functions . we draw a correspondence between establishing whether some arbitrary output can ever be outputed by a neural system and linear problems characterising a neural system of interest . we present a methodology to solve cases of practical interest by means of a state-of-the-art linear programs solver . we evaluate the technique presented by discussing the experimental results obtained by analysing reachability properties for a number of benchmarks in the literature .", "topics": ["numerical analysis"]}
{"title": "searching for representative modes on hypergraphs for robust geometric model fitting", "abstract": "in this paper , we propose a simple and effective { geometric } model fitting method to fit and segment multi-structure data even in the presence of severe outliers . we cast the task of geometric model fitting as a representative mode-seeking problem on hypergraphs . specifically , a hypergraph is firstly constructed , where the vertices represent model hypotheses and the hyperedges denote data points . the hypergraph involves higher-order similarities ( instead of pairwise similarities used on a simple graph ) , and it can characterize complex relationships between model hypotheses and data points . { in addition , we develop a hypergraph reduction technique to remove `` insignificant '' vertices while retaining as many `` significant '' vertices as possible in the hypergraph } . based on the { simplified hypergraph , we then propose a novel mode-seeking algorithm to search for representative modes within reasonable time . finally , the } proposed mode-seeking algorithm detects modes according to two key elements , i.e . , the weighting scores of vertices and the similarity analysis between vertices . overall , the proposed fitting method is able to efficiently and effectively estimate the number and the parameters of model instances in the data simultaneously . experimental results demonstrate that the proposed method achieves significant superiority over { several } state-of-the-art model fitting methods on both synthetic data and real images .", "topics": ["synthetic data"]}
{"title": "an unsupervised game-theoretic approach to saliency detection", "abstract": "we propose a novel unsupervised game-theoretic salient object detection algorithm that does not require labeled training data . first , saliency detection problem is formulated as a non-cooperative game , hereinafter referred to as saliency game , in which image regions are players who choose to be `` background '' or `` foreground '' as their pure strategies . a payoff function is constructed by exploiting multiple cues and combining complementary features . saliency maps are generated according to each region 's strategy in the nash equilibrium of the proposed saliency game . second , we explore the complementary relationship between color and deep features and propose an iterative random walk algorithm to combine saliency maps produced by the saliency game using different features . iterative random walk allows sharing information across feature spaces , and detecting objects that are otherwise very hard to detect . extensive experiments over 6 challenging datasets demonstrate the superiority of our proposed unsupervised algorithm compared to several state of the art supervised algorithms .", "topics": ["test set", "unsupervised learning"]}
{"title": "blind , greedy , and random : ordinal approximation algorithms for matching and clustering", "abstract": "we study matching and other related problems in a partial information setting where the agents ' utilities for being matched to other agents are hidden and the mechanism only has access to ordinal preference information . our model is motivated by the fact that in many settings , agents can not express the numerical values of their utility for different outcomes , but are still able to rank the outcomes in their order of preference . specifically , we study problems where the ground truth exists in the form of a weighted graph , and look to design algorithms that approximate the true optimum matching using only the preference orderings for each agent ( induced by the hidden weights ) as input . if no restrictions are placed on the weights , then one can not hope to do better than the simple greedy algorithm , which yields a half optimal matching . perhaps surprisingly , we show that by imposing a little structure on the weights , we can improve upon the trivial algorithm significantly : we design a 1.6-approximation algorithm for instances where the hidden weights obey the metric inequality . using our algorithms for matching as a black-box , we also design new approximation algorithms for other closely related problems : these include a a 3.2-approximation for the problem of clustering agents into equal sized partitions , a 4-approximation algorithm for densest k-subgraph , and a 2.14-approximation algorithm for max tsp . these results are the first non-trivial ordinal approximation algorithms for such problems , and indicate that we can design robust algorithms even when we are agnostic to the precise agent utilities .", "topics": ["approximation algorithm", "cluster analysis"]}
{"title": "early stopping without a validation set", "abstract": "early stopping is a widely used technique to prevent poor generalization performance when training an over-expressive model by means of gradient-based optimization . to find a good point to halt the optimizer , a common practice is to split the dataset into a training and a smaller validation set to obtain an ongoing estimate of the generalization performance . we propose a novel early stopping criterion based on fast-to-compute local statistics of the computed gradients and entirely removes the need for a held-out validation set . our experiments show that this is a viable approach in the setting of least-squares and logistic regression , as well as neural networks .", "topics": ["test set", "mathematical optimization"]}
{"title": "reuse of neural modules for general video game playing", "abstract": "a general approach to knowledge transfer is introduced in which an agent controlled by a neural network adapts how it reuses existing networks as it learns in a new domain . networks trained for a new domain can improve their performance by routing activation selectively through previously learned neural structure , regardless of how or for what it was learned . a neuroevolution implementation of this approach is presented with application to high-dimensional sequential decision-making domains . this approach is more general than previous approaches to neural transfer for reinforcement learning . it is domain-agnostic and requires no prior assumptions about the nature of task relatedness or mappings . the method is analyzed in a stochastic version of the arcade learning environment , demonstrating that it improves performance in some of the more complex atari 2600 games , and that the success of transfer can be predicted based on a high-level characterization of game dynamics .", "topics": ["high- and low-level", "reinforcement learning"]}
{"title": "robust causal estimation in the large-sample limit without strict faithfulness", "abstract": "causal effect estimation from observational data is an important and much studied research topic . the instrumental variable ( iv ) and local causal discovery ( lcd ) patterns are canonical examples of settings where a closed-form expression exists for the causal effect of one variable on another , given the presence of a third variable . both rely on faithfulness to infer that the latter only influences the target effect via the cause variable . in reality , it is likely that this assumption only holds approximately and that there will be at least some form of weak interaction . this brings about the paradoxical situation that , in the large-sample limit , no predictions are made , as detecting the weak edge invalidates the setting . we introduce an alternative approach by replacing strict faithfulness with a prior that reflects the existence of many 'weak ' ( irrelevant ) and 'strong ' interactions . we obtain a posterior distribution over the target causal effect estimator which shows that , in many cases , we can still make good estimates . we demonstrate the approach in an application on a simple linear-gaussian setting , using the multinest sampling algorithm , and compare it with established techniques to show our method is robust even when strict faithfulness is violated .", "topics": ["sampling ( signal processing )", "interaction"]}
{"title": "pseudo-positive regularization for deep person re-identification", "abstract": "an intrinsic challenge of person re-identification ( re-id ) is the annotation difficulty . this typically means 1 ) few training samples per identity , and 2 ) thus the lack of diversity among the training samples . consequently , we face high risk of over-fitting when training the convolutional neural network ( cnn ) , a state-of-the-art method in person re-id . to reduce the risk of over-fitting , this paper proposes a pseudo positive regularization ( ppr ) method to enrich the diversity of the training data . specifically , unlabeled data from an independent pedestrian database is retrieved using the target training data as query . a small proportion of these retrieved samples are randomly selected as the pseudo positive samples and added to the target training set for the supervised cnn training . the addition of pseudo positive samples is therefore a data augmentation method to reduce the risk of over-fitting during cnn training . we implement our idea in the identification cnn models ( i.e . , caffenet , vggnet-16 and resnet-50 ) . on cuhk03 and market-1501 datasets , experimental results demonstrate that the proposed method consistently improves the baseline and yields competitive performance to the state-of-the-art person re-id methods .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "multi-task learning of pairwise sequence classification tasks over disparate label spaces", "abstract": "we combine multi-task learning and semi-supervised learning by inducing a joint embedding space between disparate label spaces and learning transfer functions between label embeddings , enabling us to jointly leverage unlabelled data and auxiliary , annotated datasets . we evaluate our approach on a variety of sequence classification tasks with disparate label spaces . we outperform strong single and multi-task baselines and achieve a new state-of-the-art for aspect- and topic-based sentiment analysis .", "topics": ["baseline ( configuration management )"]}
{"title": "ot simple - a construction-kit approach to optimality theory implementation", "abstract": "this paper details a simple approach to the implementation of optimality theory ( ot , prince and smolensky 1993 ) on a computer , in part reusing standard system software . in a nutshell , ot 's generating source is implemented as a binprolog program interpreting a context-free specification of a gen structural grammar according to a user-supplied input form . the resulting set of textually flattened candidate tree representations is passed to the constraint stage . constraints are implemented by finite-state transducers specified as `sed ' stream editor scripts that typically map ill-formed portions of the candidate to violation marks . evaluation of candidates reduces to simple sorting : the violation-mark-annotated output leaving con is fed into `sort ' , which orders candidates on the basis of the violation vector column of each line , thereby bringing the optimal candidate to the top . this approach gave rise to ot simple , the first freely available software tool for the ot framework to provide generic facilities for both gen and constraint definition . its practical applicability is demonstrated by modelling the ot analysis of apparent subtractive pluralization in upper hessian presented in golston and wiese ( 1996 ) .", "topics": ["map"]}
{"title": "atpboost : learning premise selection in binary setting with atp feedback", "abstract": "atpboost is a system for solving sets of large-theory problems by interleaving atp runs with state-of-the-art machine learning of premise selection from the proofs . unlike many previous approaches that use multi-label setting , the learning is implemented as binary classification that estimates the pairwise-relevance of ( theorem , premise ) pairs . atpboost uses for this the xgboost gradient boosting algorithm , which is fast and has state-of-the-art performance on many tasks . learning in the binary setting however requires negative examples , which is nontrivial due to many alternative proofs . we discuss and implement several solutions in the context of the atp/ml feedback loop , and show that atpboost with such methods significantly outperforms the k-nearest neighbors multilabel classifier .", "topics": ["gradient descent", "relevance"]}
{"title": "softening fuzzy knowledge representation tool with the learning of new words in natural language", "abstract": "the approach described here allows using membership function to represent imprecise and uncertain knowledge by learning in fuzzy semantic networks . this representation has a great practical interest due to the possibility to realize on the one hand , the construction of this membership function from a simple value expressing the degree of interpretation of an object or a goal as compared to an other and on the other hand , the adjustment of the membership function during the apprenticeship . we show , how to use these membership functions to represent the interpretation of an object ( respectively of a goal ) user as compared to an system object ( respectively to a goal ) . we also show the possibility to make decision for each representation of an user object compared to a system object . this decision is taken by determining decision coefficient calculates according to the nucleus of the membership function of the user object .", "topics": ["natural language"]}
{"title": "survey of the state of the art in natural language generation : core tasks , applications and evaluation", "abstract": "this paper surveys the current state of the art in natural language generation ( nlg ) , defined as the task of generating text or speech from non-linguistic input . a survey of nlg is timely in view of the changes that the field has undergone over the past decade or so , especially in relation to new ( usually data-driven ) methods , as well as new applications of nlg technology . this survey therefore aims to ( a ) give an up-to-date synthesis of research on the core tasks in nlg and the architectures adopted in which such tasks are organised ; ( b ) highlight a number of relatively recent research topics that have arisen partly as a result of growing synergies between nlg and other areas of artificial intelligence ; ( c ) draw attention to the challenges in nlg evaluation , relating them to similar challenges faced in other areas of natural language processing , with an emphasis on different evaluation methods and the relationships between them .", "topics": ["natural language processing", "natural language"]}
{"title": "understanding deep architectures using a recursive convolutional network", "abstract": "a key challenge in designing convolutional network models is sizing them appropriately . many factors are involved in these decisions , including number of layers , feature maps , kernel sizes , etc . complicating this further is the fact that each of these influence not only the numbers and dimensions of the activation units , but also the total number of parameters . in this paper we focus on assessing the independent contributions of three of these linked variables : the numbers of layers , feature maps , and parameters . to accomplish this , we employ a recursive convolutional network whose weights are tied between layers ; this allows us to vary each of the three factors in a controlled setting . we find that while increasing the numbers of layers and parameters each have clear benefit , the number of feature maps ( and hence dimensionality of the representation ) appears ancillary , and finds most of its benefit through the introduction of more weights . our results ( i ) empirically confirm the notion that adding layers alone increases computational power , within the context of convolutional layers , and ( ii ) suggest that precise sizing of convolutional feature map dimensions is itself of little concern ; more attention should be paid to the number of parameters in these layers instead .", "topics": ["map"]}
{"title": "deep networks with internal selective attention through feedback connections", "abstract": "traditional convolutional neural networks ( cnn ) are stationary and feedforward . they neither change their parameters during evaluation nor use feedback from higher to lower layers . real brains , however , do . so does our deep attention selective network ( dasnet ) architecture . dasnets feedback structure can dynamically alter its convolutional filter sensitivities during classification . it harnesses the power of sequential processing to improve classification performance , by allowing the network to iteratively focus its internal attention on some of its convolutional filters . feedback is trained through direct policy search in a huge million-dimensional parameter space , through scalable natural evolution strategies ( snes ) . on the cifar-10 and cifar-100 datasets , dasnet outperforms the previous state-of-the-art model .", "topics": ["scalability"]}
{"title": "visualizing and interacting with concept hierarchies", "abstract": "concept hierarchies and formal concept analysis are theoretically well grounded and largely experimented methods . they rely on line diagrams called galois lattices for visualizing and analysing object-attribute sets . galois lattices are visually seducing and conceptually rich for experts . however they present important drawbacks due to their concept oriented overall structure : analysing what they show is difficult for non experts , navigation is cumbersome , interaction is poor , and scalability is a deep bottleneck for visual interpretation even for experts . in this paper we introduce semantic probes as a means to overcome many of these problems and extend usability and application possibilities of traditional fca visualization methods . semantic probes are visual user centred objects which extract and organize reduced galois sub-hierarchies . they are simpler , clearer , and they provide a better navigation support through a rich set of interaction possibilities . since probe driven sub-hierarchies are limited to users focus , scalability is under control and interpretation is facilitated . after some successful experiments , several applications are being developed with the remaining problem of finding a compromise between simplicity and conceptual expressivity .", "topics": ["scalability"]}
{"title": "heteroscedastic relevance vector machine", "abstract": "in this work we propose a heteroscedastic generalization to rvm , a fast bayesian framework for regression , based on some recent similar works . we use variational approximation and expectation propagation to tackle the problem . the work is still under progress and we are examining the results and comparing with the previous works .", "topics": ["calculus of variations", "approximation"]}
{"title": "local search for minimum weight dominating set with two-level configuration checking and frequency based scoring function", "abstract": "the minimum weight dominating set ( mwds ) problem is an important generalization of the minimum dominating set ( mds ) problem with extensive applications . this paper proposes a new local search algorithm for the mwds problem , which is based on two new ideas . the first idea is a heuristic called two-level configuration checking ( cc2 ) , which is a new variant of a recent powerful configuration checking strategy ( cc ) for effectively avoiding the recent search paths . the second idea is a novel scoring function based on the frequency of being uncovered of vertices . our algorithm is called cc2fs , according to the names of the two ideas . the experimental results show that , cc2fs performs much better than some state-of-the-art algorithms in terms of solution quality on a broad range of mwds benchmarks .", "topics": ["heuristic"]}
{"title": "resnet in resnet : generalizing residual architectures", "abstract": "residual networks ( resnets ) have recently achieved state-of-the-art on challenging computer vision tasks . we introduce resnet in resnet ( rir ) : a deep dual-stream architecture that generalizes resnets and standard cnns and is easily implemented with no computational overhead . rir consistently improves performance over resnets , outperforms architectures with similar amounts of augmentation on cifar-10 , and establishes a new state-of-the-art on cifar-100 .", "topics": ["computer vision"]}
{"title": "nighttime sky/cloud image segmentation", "abstract": "imaging the atmosphere using ground-based sky cameras is a popular approach to study various atmospheric phenomena . however , it usually focuses on the daytime . nighttime sky/cloud images are darker and noisier , and thus harder to analyze . an accurate segmentation of sky/cloud images is already challenging because of the clouds ' non-rigid structure and size , and the lower and less stable illumination of the night sky increases the difficulty . nonetheless , nighttime cloud imaging is essential in certain applications , such as continuous weather analysis and satellite communication . in this paper , we propose a superpixel-based method to segment nighttime sky/cloud images . we also release the first nighttime sky/cloud image segmentation database to the research community . the experimental results show the efficacy of our proposed algorithm for nighttime images .", "topics": ["image segmentation"]}
{"title": "reaping the benefits of interactive syntax and semantics", "abstract": "semantic feedback is an important source of information that a parser could use to deal with local ambiguities in syntax . however , it is difficult to devise a systematic communication mechanism for interactive syntax and semantics . in this article , i propose a variant of left-corner parsing to define the points at which syntax and semantics should interact , an account of grammatical relations and thematic roles to define the content of the communication , and a conflict resolution strategy based on independent preferences from syntax and semantics . the resulting interactive model has been implemented in a program called compere and shown to account for a wide variety of psycholinguistic data on structural and lexical ambiguities .", "topics": ["natural language", "parsing"]}
{"title": "synthesis of supervised classification algorithm using intelligent and statistical tools", "abstract": "a fundamental task in detecting foreground objects in both static and dynamic scenes is to take the best choice of color system representation and the efficient technique for background modeling . we propose in this paper a non-parametric algorithm dedicated to segment and to detect objects in color images issued from a football sports meeting . indeed segmentation by pixel concern many applications and revealed how the method is robust to detect objects , even in presence of strong shadows and highlights . in the other hand to refine their playing strategy such as in football , handball , volley ball , rugby ... , the coach need to have a maximum of technical-tactics information about the on-going of the game and the players . we propose in this paper a range of algorithms allowing the resolution of many problems appearing in the automated process of team identification , where each player is affected to his corresponding team relying on visual data . the developed system was tested on a match of the tunisian national competition . this work is prominent for many next computer vision studies as it 's detailed in this study .", "topics": ["supervised learning", "computer vision"]}
{"title": "generating sentences from a continuous space", "abstract": "the standard recurrent neural network language model ( rnnlm ) generates sentences one word at a time and does not work from an explicit global sentence representation . in this work , we introduce and study an rnn-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences . this factorization allows it to explicitly model holistic properties of sentences such as style , topic , and high-level syntactic features . samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding . by examining paths through this latent space , we are able to generate coherent novel sentences that interpolate between known sentences . we present techniques for solving the difficult learning problem presented by this model , demonstrate its effectiveness in imputing missing words , explore many interesting properties of the model 's latent sentence space , and present negative results on the use of the model in language modeling .", "topics": ["generative model", "calculus of variations"]}
{"title": "black box modelling of hvac system : improving the performances of neural networks", "abstract": "this paper deals with neural networks modelling of hvac systems . in order to increase the neural networks performances , a method based on sensitivity analysis is applied . the same technique is also used to compute the relevance of each input . to avoid the prediction errors in dry coil conditions , a metamodel for each capacity is derived from the neural networks . the regression coefficients of the polynomial forms are identified through the use of spectral analysis . these methods based on sensitivity and spectral analysis lead to an optimized neural network model , as regard to its architecture and predictions .", "topics": ["nonlinear system", "polynomial"]}
{"title": "the segmentation fusion method on10 multi-sensors", "abstract": "the most significant problem may be undesirable effects for the spectral signatures of fused images as well as the benefits of using fused images mostly compared to their source images were acquired at the same time by one sensor . they may or may not be suitable for the fusion of other images . it becomes therefore increasingly important to investigate techniques that allow multi-sensor , multi-date image fusion to make final conclusions can be drawn on the most suitable method of fusion . so , in this study we present a new method segmentation fusion method ( sf ) for remotely sensed images is presented by considering the physical characteristics of sensors , which uses a feature level processing paradigm . in a particularly , attempts to test the proposed method performance on 10 multi-sensor images and comparing it with different fusion techniques for estimating the quality and degree of information improvement quantitatively by using various spatial and spectral metrics .", "topics": ["sensor"]}
{"title": "adversarial variational bayes : unifying variational autoencoders and generative adversarial networks", "abstract": "variational autoencoders ( vaes ) are expressive latent variable models that can be used to learn complex probability distributions from training data . however , the quality of the resulting model crucially relies on the expressiveness of the inference model . we introduce adversarial variational bayes ( avb ) , a technique for training variational autoencoders with arbitrarily expressive inference models . we achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game , hence establishing a principled connection between vaes and generative adversarial networks ( gans ) . we show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model , as well as the exact posterior distribution over the latent variables given an observation . contrary to competing approaches which combine vaes with gans , our approach has a clear theoretical justification , retains most advantages of standard variational autoencoders and is easy to implement .", "topics": ["generative model", "test set"]}
{"title": "a primal-dual method for training recurrent neural networks constrained by the echo-state property", "abstract": "we present an architecture of a recurrent neural network ( rnn ) with a fully-connected deep neural network ( dnn ) as its feature extractor . the rnn is equipped with both causal temporal prediction and non-causal look-ahead , via auto-regression ( ar ) and moving-average ( ma ) , respectively . the focus of this paper is a primal-dual training method that formulates the learning of the rnn as a formal optimization problem with an inequality constraint that provides a sufficient condition for the stability of the network dynamics . experimental results demonstrate the effectiveness of this new method , which achieves 18.86 % phone recognition error on the timit benchmark for the core test set . the result approaches the best result of 17.7 % , which was obtained by using rnn with long short-term memory ( lstm ) . the results also show that the proposed primal-dual training method produces lower recognition errors than the popular rnn methods developed earlier based on the carefully tuned threshold parameter that heuristically prevents the gradient from exploding .", "topics": ["test set", "recurrent neural network"]}
{"title": "using convolutional neural networks in robots with limited computational resources : detecting nao robots while playing soccer", "abstract": "the main goal of this paper is to analyze the general problem of using convolutional neural networks ( cnns ) in robots with limited computational capabilities , and to propose general design guidelines for their use . in addition , two different cnn based nao robot detectors that are able to run in real-time while playing soccer are proposed . one of the detectors is based on the xnor-net and the other on the squeezenet . each detector is able to process a robot object-proposal in ~1ms , with an average number of 1.5 proposals per frame obtained by the upper camera of the nao . the obtained detection rate is ~97 % .", "topics": ["robot"]}
{"title": "semantic understanding of professional soccer commentaries", "abstract": "this paper presents a novel approach to the problem of semantic parsing via learning the correspondences between complex sentences and rich sets of events . our main intuition is that correct correspondences tend to occur more frequently . our model benefits from a discriminative notion of similarity to learn the correspondence between sentence and an event and a ranking machinery that scores the popularity of each correspondence . our method can discover a group of events ( called macro-events ) that best describes a sentence . we evaluate our method on our novel dataset of professional soccer commentaries . the empirical results show that our method significantly outperforms the state-of-theart .", "topics": ["parsing"]}
{"title": "kernel mean estimation and stein 's effect", "abstract": "a mean function in reproducing kernel hilbert space , or a kernel mean , is an important part of many applications ranging from kernel principal component analysis to hilbert-space embedding of distributions . given finite samples , an empirical average is the standard estimate for the true kernel mean . we show that this estimator can be improved via a well-known phenomenon in statistics called stein 's phenomenon . after consideration , our theoretical analysis reveals the existence of a wide class of estimators that are better than the standard . focusing on a subset of this class , we propose efficient shrinkage estimators for the kernel mean . empirical evaluations on several benchmark applications clearly demonstrate that the proposed estimators outperform the standard kernel mean estimator .", "topics": ["kernel ( operating system )"]}
{"title": "false discovery rate control via debiased lasso", "abstract": "we consider the problem of variable selection in high-dimensional statistical models where the goal is to report a set of variables , out of many predictors $ x_1 , \\dotsc , x_p $ , that are relevant to a response of interest . for linear high-dimensional model , where the number of parameters exceeds the number of samples $ ( p > n ) $ , we propose a procedure for variables selection and prove that it controls the \\emph { directional } false discovery rate ( fdr ) below a pre-assigned significance level $ q\\in [ 0,1 ] $ . we further analyze the statistical power of our framework and show that for designs with subgaussian rows and a common precision matrix $ \\omega\\in\\mathbb { r } ^ { p\\times p } $ , if the minimum nonzero parameter $ \\theta_ { \\min } $ satisfies $ $ \\sqrt { n } \\theta_ { \\min } - \\sigma \\sqrt { 2 ( \\max_ { i\\in [ p ] } \\omega_ { ii } ) \\log\\left ( \\frac { 2p } { qs_0 } \\right ) } \\to \\infty\\ , , $ $ then this procedure achieves asymptotic power one . our framework is built upon the debiasing approach and assumes the standard condition $ s_0 = o ( \\sqrt { n } / ( \\log p ) ^2 ) $ , where $ s_0 $ indicates the number of true positives among the $ p $ features . notably , this framework achieves exact directional fdr control without any assumption on the amplitude of unknown regression parameters , and does not require any knowledge of the distribution of covariates or the noise level . we test our method in synthetic and real data experiments to asses its performance and to corroborate our theoretical results .", "topics": ["synthetic data"]}
{"title": "multiplicative nonholonomic/newton -like algorithm", "abstract": "we construct new algorithms from scratch , which use the fourth order cumulant of stochastic variables for the cost function . the multiplicative updating rule here constructed is natural from the homogeneous nature of the lie group and has numerous merits for the rigorous treatment of the dynamics . as one consequence , the second order convergence is shown . for the cost function , functions invariant under the componentwise scaling are choosen . by identifying points which can be transformed to each other by the scaling , we assume that the dynamics is in a coset space . in our method , a point can move toward any direction in this coset . thus , no prewhitening is required .", "topics": ["loss function"]}
{"title": "detection , recognition and tracking of moving objects from real-time video via sp theory of intelligence and species inspired pso", "abstract": "in this paper , we address the basic problem of recognizing moving objects in video images using sp theory of intelligence . the concept of sp theory of intelligence which is a framework of artificial intelligence , was first introduced by gerard j wolff , where s stands for simplicity and p stands for power . using the concept of multiple alignment , we detect and recognize object of our interest in video frames with multilevel hierarchical parts and subparts , based on polythetic categories . we track the recognized objects using the species based particle swarm optimization ( pso ) . first , we extract the multiple alignment of our object of interest from training images . in order to recognize accurately and handle occlusion , we use the polythetic concepts on raw data line to omit the redundant noise via searching for best alignment representing the features from the extracted alignments . we recognize the domain of interest from the video scenes in form of wide variety of multiple alignments to handle scene variability . unsupervised learning is done in the sp model following the donsvic principle and natural structures are discovered via information compression and pattern analysis . after successful recognition of objects , we use species based pso algorithm as the alignments of our object of interest is analogues to observation likelihood and fitness ability of species . subsequently , we analyze the competition and repulsion among species with annealed gaussian based pso . we have tested our algorithms on david , walking2 , faceocc1 , jogging and dudek , obtaining very satisfactory and competitive results .", "topics": ["unsupervised learning", "artificial intelligence"]}
{"title": "ensemble classifier approach in breast cancer detection and malignancy grading- a review", "abstract": "the diagnosed cases of breast cancer is increasing annually and unfortunately getting converted into a high mortality rate . cancer , at the early stages , is hard to detect because the malicious cells show similar properties ( density ) as shown by the non-malicious cells . the mortality ratio could have been minimized if the breast cancer could have been detected in its early stages . but the current systems have not been able to achieve a fully automatic system which is not just capable of detecting the breast cancer but also can detect the stage of it . estimation of malignancy grading is important in diagnosing the degree of growth of malicious cells as well as in selecting a proper therapy for the patient . therefore , a complete and efficient clinical decision support system is proposed which is capable of achieving breast cancer malignancy grading scheme very efficiently . the system is based on image processing and machine learning domains . classification imbalance problem , a machine learning problem , occurs when instances of one class is much higher than the instances of the other class resulting in an inefficient classification of samples and hence a bad decision support system . therefore eusboost , ensemble based classifier is proposed which is efficient and is able to outperform other classifiers as it takes the benefits of both-boosting algorithm with random undersampling techniques . also comparison of eusboost with other techniques is shown in the paper .", "topics": ["image processing"]}
{"title": "a siamese deep forest", "abstract": "a siamese deep forest ( sdf ) is proposed in the paper . it is based on the deep forest or gcforest proposed by zhou and feng and can be viewed as a gcforest modification . it can be also regarded as an alternative to the well-known siamese neural networks . the sdf uses a modified training set consisting of concatenated pairs of vectors . moreover , it defines the class distributions in the deep forest as the weighted sum of the tree class probabilities such that the weights are determined in order to reduce distances between similar pairs and to increase them between dissimilar points . we show that the weights can be obtained by solving a quadratic optimization problem . the sdf aims to prevent overfitting which takes place in neural networks when only limited training data are available . the numerical experiments illustrate the proposed distance metric method .", "topics": ["test set", "optimization problem"]}
{"title": "theory and algorithms for partial order based reduction in planning", "abstract": "search is a major technique for planning . it amounts to exploring a state space of planning domains typically modeled as a directed graph . however , prohibitively large sizes of the search space make search expensive . developing better heuristic functions has been the main technique for improving search efficiency . nevertheless , recent studies have shown that improving heuristics alone has certain fundamental limits on improving search efficiency . recently , a new direction of research called partial order based reduction ( por ) has been proposed as an alternative to improving heuristics . por has shown promise in speeding up searches . por has been extensively studied in model checking research and is a key enabling technique for scalability of model checking systems . although the por theory has been extensively studied in model checking , it has never been developed systematically for planning before . in addition , the conditions for por in the model checking theory are abstract and not directly applicable in planning . previous works on por algorithms for planning did not establish the connection between these algorithms and existing theory in model checking . in this paper , we develop a theory for por in planning . the new theory we develop connects the stubborn set theory in model checking and por methods in planning . we show that previous por algorithms in planning can be explained by the new theory . based on the new theory , we propose a new , stronger por algorithm . experimental results on various planning domains show further search cost reduction using the new algorithm .", "topics": ["scalability", "heuristic"]}
{"title": "coherent online video style transfer", "abstract": "training a feed-forward network for fast neural style transfer of images is proven to be successful . however , the naive extension to process video frame by frame is prone to producing flickering results . we propose the first end-to-end network for online video style transfer , which generates temporally coherent stylized video sequences in near real-time . two key ideas include an efficient network by incorporating short-term coherence , and propagating short-term coherence to long-term , which ensures the consistency over larger period of time . our network can incorporate different image stylization networks . we show that the proposed method clearly outperforms the per-frame baseline both qualitatively and quantitatively . moreover , it can achieve visually comparable coherence to optimization-based video style transfer , but is three orders of magnitudes faster in runtime .", "topics": ["baseline ( configuration management )"]}
{"title": "exploring large feature spaces with hierarchical multiple kernel learning", "abstract": "for supervised and unsupervised learning , positive definite kernels allow to use large and potentially infinite dimensional feature spaces with a computational cost that only depends on the number of observations . this is usually done through the penalization of predictor functions by euclidean or hilbertian norms . in this paper , we explore penalizing by sparsity-inducing norms such as the l1-norm or the block l1-norm . we assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph ; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework , in polynomial time in the number of selected kernels . this framework is naturally applied to non linear variable selection ; our extensive simulations on synthetic datasets and datasets from the uci repository show that efficiently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance .", "topics": ["kernel ( operating system )", "feature vector"]}
{"title": "moneybarl : exploiting pitcher decision-making using reinforcement learning", "abstract": "this manuscript uses machine learning techniques to exploit baseball pitchers ' decision making , so-called `` baseball iq , '' by modeling the at-bat information , pitch selection and counts , as a markov decision process ( mdp ) . each state of the mdp models the pitcher 's current pitch selection in a markovian fashion , conditional on the information immediately prior to making the current pitch . this includes the count prior to the previous pitch , his ensuing pitch selection , the batter 's ensuing action and the result of the pitch .", "topics": ["value ( ethics )", "reinforcement learning"]}
{"title": "dimensionally tight running time bounds for second-order hamiltonian monte carlo", "abstract": "hamiltonian monte carlo ( hmc ) is a widely deployed method to sample from a given high-dimensional distribution in statistics and machine learning . hmc is known to run very efficiently in practice and its second-order variant was conjectured to run in $ d^ { 1/4 } $ steps in 1988 . here we show that this conjecture is true when sampling from strongly log-concave target distributions that satisfy weak third-order regularity properties associated with the input data . this improves upon a recent result that shows that the number of steps of the second-order discretization of hmc grows like $ d^ { 1/4 } $ under the much stronger assumption that the distribution is separable and its first four fr\\'echet derivatives are bounded . our result also compares favorably with the best available running time bounds for the class of strongly log-concave distributions , namely the current best bounds for both the overdamped and underdamped langevin , and first-order hmc algorithms , which all grow like $ d^ { { 1 } / { 2 } } $ with the dimension . key to our result is a new regularity condition for the hessian that may be of independent interest . the class of distributions that satisfy this condition are natural and include posterior distributions used in bayesian logistic `` ridge '' regression .", "topics": ["sampling ( signal processing )", "time complexity"]}
{"title": "evolutionary estimation of a coupled markov chain credit risk model", "abstract": "there exists a range of different models for estimating and simulating credit risk transitions to optimally manage credit risk portfolios and products . in this chapter we present a coupled markov chain approach to model rating transitions and thereby default probabilities of companies . as the likelihood of the model turns out to be a non-convex function of the parameters to be estimated , we apply heuristics to find the ml estimators . to this extent , we outline the model and its likelihood function , and present both a particle swarm optimization algorithm , as well as an evolutionary optimization algorithm to maximize the likelihood function . numerical results are shown which suggest a further application of evolutionary optimization techniques for credit risk management .", "topics": ["simulation", "heuristic"]}
{"title": "evolutionary solving of the debts ' clearing problem", "abstract": "the debts ' clearing problem is about clearing all the debts in a group of n entities ( persons , companies etc . ) using a minimal number of money transaction operations . the problem is known to be np-hard in the strong sense . as for many intractable problems , techniques from the field of artificial intelligence are useful in finding solutions close to optimum for large inputs . an evolutionary algorithm for solving the debts ' clearing problem is proposed .", "topics": ["entity", "artificial intelligence"]}
{"title": "vehicle detection in aerial images", "abstract": "the detection of vehicles in aerial images is widely applied in many applications . comparing with object detection in the ground view images , vehicle detection in aerial images remains a challenging problem because of small vehicle size , monotone appearance and the complex background . in this paper , we propose a novel double focal loss convolutional neural network framework ( dfl-cnn ) . in the proposed framework , the skip connection is used in the cnn structure to enhance the feature learning . also , the focal loss function is used to substitute for conventional cross entropy loss function in both of the region proposed network and the final classifier . we further introduce the first large-scale vehicle detection dataset itcvd with ground truth annotations for all the vehicles in the scene . we demonstrate the performance of our model on the existing benchmark dlr 3k dataset as well as the itcvd dataset . the experimental results show that our dfl-cnn outperforms the baselines on vehicle detection .", "topics": ["baseline ( configuration management )", "feature learning"]}
{"title": "graph based over-segmentation methods for 3d point clouds", "abstract": "over-segmentation , or super-pixel generation , is a common preliminary stage for many computer vision applications . new acquisition technologies enable the capturing of 3d point clouds that contain color and geometrical information . this 3d information introduces a new conceptual change that can be utilized to improve the results of over-segmentation , which uses mainly color information , and to generate clusters of points we call super-points . we consider a variety of possible 3d extensions of the local variation ( lv ) graph based over-segmentation algorithms , and compare them thoroughly . we consider different alternatives for constructing the connectivity graph , for assigning the edge weights , and for defining the merge criterion , which must now account for the geometric information and not only color . following this evaluation , we derive a new generic algorithm for over-segmentation of 3d point clouds . we call this new algorithm point cloud local variation ( pclv ) . the advantages of the new over-segmentation algorithm are demonstrated on both outdoor and cluttered indoor scenes . performance analysis of the proposed approach compared to state-of-the-art 2d and 3d over-segmentation algorithms shows significant improvement according to the common performance measures .", "topics": ["computer vision", "eisenstein 's criterion"]}
{"title": "towards vision-based smart hospitals : a system for tracking and monitoring hand hygiene compliance", "abstract": "one in twenty-five patients admitted to a hospital will suffer from a hospital acquired infection . if we can intelligently track healthcare staff , patients , and visitors , we can better understand the sources of such infections . we envision a smart hospital capable of increasing operational efficiency and improving patient care with less spending . in this paper , we propose a non-intrusive vision-based system for tracking people 's activity in hospitals . we evaluate our method for the problem of measuring hand hygiene compliance . empirically , our method outperforms existing solutions such as proximity-based techniques and covert in-person observational studies . we present intuitive , qualitative results that analyze human movement patterns and conduct spatial analytics which convey our method 's interpretability . this work is a first step towards a computer-vision based smart hospital and demonstrates promising results for reducing hospital acquired infections .", "topics": ["computer vision"]}
{"title": "tube convolutional neural network ( t-cnn ) for action detection in videos", "abstract": "deep learning has been demonstrated to achieve excellent results for image classification and object detection . however , the impact of deep learning on video analysis ( e.g . action detection and recognition ) has been limited due to complexity of video data and lack of annotations . previous convolutional neural networks ( cnn ) based video action detection approaches usually consist of two major steps : frame-level action proposal detection and association of proposals across frames . also , these methods employ two-stream cnn framework to handle spatial and temporal feature separately . in this paper , we propose an end-to-end deep network called tube convolutional neural network ( t-cnn ) for action detection in videos . the proposed architecture is a unified network that is able to recognize and localize action based on 3d convolution features . a video is first divided into equal length clips and for each clip a set of tube proposals are generated next based on 3d convolutional network ( convnet ) features . finally , the tube proposals of different clips are linked together employing network flow and spatio-temporal action detection is performed using these linked video proposals . extensive experiments on several video datasets demonstrate the superior performance of t-cnn for classifying and localizing actions in both trimmed and untrimmed videos compared to state-of-the-arts .", "topics": ["object detection", "computer vision"]}
{"title": "minimum message length estimation of mixtures of multivariate gaussian and von mises-fisher distributions", "abstract": "mixture modelling involves explaining some observed evidence using a combination of probability distributions . the crux of the problem is the inference of an optimal number of mixture components and their corresponding parameters . this paper discusses unsupervised learning of mixture models using the bayesian minimum message length ( mml ) criterion . to demonstrate the effectiveness of search and inference of mixture parameters using the proposed approach , we select two key probability distributions , each handling fundamentally different types of data : the multivariate gaussian distribution to address mixture modelling of data distributed in euclidean space , and the multivariate von mises-fisher ( vmf ) distribution to address mixture modelling of directional data distributed on a unit hypersphere . the key contributions of this paper , in addition to the general search and inference methodology , include the derivation of mml expressions for encoding the data using multivariate gaussian and von mises-fisher distributions , and the analytical derivation of the mml estimates of the parameters of the two distributions . our approach is tested on simulated and real world data sets . for instance , we infer vmf mixtures that concisely explain experimentally determined three-dimensional protein conformations , providing an effective null model description of protein structures that is central to many inference problems in structural bioinformatics . the experimental results demonstrate that the performance of our proposed search and inference method along with the encoding schemes improve on the state of the art mixture modelling techniques .", "topics": ["unsupervised learning", "simulation"]}
{"title": "urban morphology meets deep learning : exploring urban forms in one million cities , town and villages across the planet", "abstract": "study of urban form is an important area of research in urban planning/design that contributes to our understanding of how cities function and evolve . however , classical approaches are based on very limited observations and inconsistent methods . as an alternative , availability of massive urban data collections such as open street map from the one hand and the recent advancements in machine learning methods such as deep learning techniques on the other have opened up new possibilities to automatically investigate urban forms at the global scale . in this work for the first time , by collecting a large data set of street networks in more than one million cities , towns and villages all over the world , we trained a deep convolutional auto-encoder , that automatically learns the hierarchical structures of urban forms and represents them via dense and comparable vectors . we showed how the learned urban vectors could be used for different investigations . using the learned urban vectors , one is able to easily find and compare similar urban forms all over the world , considering their overall spatial structure and other factors such as orientation , graphical structure , and density and partial deformations . further cluster analysis reveals the distribution of the main patterns of urban forms all over the planet .", "topics": ["cluster analysis", "encoder"]}
{"title": "show , attend and tell : neural image caption generation with visual attention", "abstract": "inspired by recent work in machine translation and object detection , we introduce an attention based model that automatically learns to describe the content of images . we describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound . we also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence . we validate the use of attention with state-of-the-art performance on three benchmark datasets : flickr8k , flickr30k and ms coco .", "topics": ["calculus of variations", "object detection"]}
{"title": "deep video deblurring", "abstract": "motion blur from camera shake is a major problem in videos captured by hand-held devices . unlike single-image deblurring , video-based approaches can take advantage of the abundant information that exists across neighboring frames . as a result the best performing methods rely on aligning nearby frames . however , aligning images is a computationally expensive and fragile procedure , and methods that aggregate information must therefore be able to identify which regions have been accurately aligned and which have not , a task which requires high level scene understanding . in this work , we introduce a deep learning solution to video deblurring , where a cnn is trained end-to-end to learn how to accumulate information across frames . to train this network , we collected a dataset of real videos recorded with a high framerate camera , which we use to generate synthetic motion blur for supervision . we show that the features learned from this dataset extend to deblurring motion blur that arises due to camera shake in a wide range of videos , and compare the quality of results to a number of other baselines .", "topics": ["synthetic data"]}
{"title": "learning the kernel matrix via predictive low-rank approximations", "abstract": "efficient and accurate low-rank approximations of multiple data sources are essential in the era of big data . the scaling of kernel-based learning algorithms to large datasets is limited by the o ( n^2 ) computation and storage complexity of the full kernel matrix , which is required by most of the recent kernel learning algorithms . we present the mklaren algorithm to approximate multiple kernel matrices learn a regression model , which is entirely based on geometrical concepts . the algorithm does not require access to full kernel matrices yet it accounts for the correlations between all kernels . it uses incomplete cholesky decomposition , where pivot selection is based on least-angle regression in the combined , low-dimensional feature space . the algorithm has linear complexity in the number of data points and kernels . when explicit feature space induced by the kernel can be constructed , a mapping from the dual to the primal ridge regression weights is used for model interpretation . the mklaren algorithm was tested on eight standard regression datasets . it outperforms contemporary kernel matrix approximation approaches when learning with multiple kernels . it identifies relevant kernels , achieving highest explained variance than other multiple kernel learning methods for the same number of iterations . test accuracy , equivalent to the one using full kernel matrices , was achieved with at significantly lower approximation ranks . a difference in run times of two orders of magnitude was observed when either the number of samples or kernels exceeds 3000 .", "topics": ["kernel ( operating system )", "feature vector"]}
{"title": "saccade sequence prediction : beyond static saliency maps", "abstract": "visual attention is a field with a considerable history , with eye movement control and prediction forming an important subfield . fixation modeling in the past decades has been largely dominated computationally by a number of highly influential bottom-up saliency models , such as the itti-koch-niebur model . the accuracy of such models has dramatically increased recently due to deep learning . however , on static images the emphasis of these models has largely been based on non-ordered prediction of fixations through a saliency map . very few implemented models can generate temporally ordered human-like sequences of saccades beyond an initial fixation point . towards addressing these shortcomings we present star-fc , a novel multi-saccade generator based on a central/peripheral integration of deep learning-based saliency and lower-level feature-based saliency . we have evaluated our model using the cat2000 database , successfully predicting human patterns of fixation with equivalent accuracy and quality compared to what can be achieved by using one human sequence to predict another . this is a significant improvement over fixation sequences predicted by state-of-the-art saliency algorithms .", "topics": ["map"]}
{"title": "learning mixtures of gaussians in high dimensions", "abstract": "efficiently learning mixture of gaussians is a fundamental problem in statistics and learning theory . given samples coming from a random one out of k gaussian distributions in rn , the learning problem asks to estimate the means and the covariance matrices of these gaussians . this learning problem arises in many areas ranging from the natural sciences to the social sciences , and has also found many machine learning applications . unfortunately , learning mixture of gaussians is an information theoretically hard problem : in order to learn the parameters up to a reasonable accuracy , the number of samples required is exponential in the number of gaussian components in the worst case . in this work , we show that provided we are in high enough dimensions , the class of gaussian mixtures is learnable in its most general form under a smoothed analysis framework , where the parameters are randomly perturbed from an adversarial starting point . in particular , given samples from a mixture of gaussians with randomly perturbed parameters , when n > { \\omega } ( k^2 ) , we give an algorithm that learns the parameters with polynomial running time and using polynomial number of samples . the central algorithmic ideas consist of new ways to decompose the moment tensor of the gaussian mixture by exploiting its structural properties . the symmetries of this tensor are derived from the combinatorial structure of higher order moments of gaussian distributions ( sometimes referred to as isserlis ' theorem or wick 's theorem ) . we also develop new tools for bounding smallest singular values of structured random matrices , which could be useful in other smoothed analysis settings .", "topics": ["computational complexity theory", "time complexity"]}
{"title": "flexibly instructable agents", "abstract": "this paper presents an approach to learning from situated , interactive tutorial instruction within an ongoing agent . tutorial instruction is a flexible ( and thus powerful ) paradigm for teaching tasks because it allows an instructor to communicate whatever types of knowledge an agent might need in whatever situations might arise . to support this flexibility , however , the agent must be able to learn multiple kinds of knowledge from a broad range of instructional interactions . our approach , called situated explanation , achieves such learning through a combination of analytic and inductive techniques . it combines a form of explanation-based learning that is situated for each instruction with a full suite of contextually guided responses to incomplete explanations . the approach is implemented in an agent called instructo-soar that learns hierarchies of new tasks and other domain knowledge from interactive natural language instructions . instructo-soar meets three key requirements of flexible instructability that distinguish it from previous systems : ( 1 ) it can take known or unknown commands at any instruction point ; ( 2 ) it can handle instructions that apply to either its current situation or to a hypothetical situation specified in language ( as in , for instance , conditional instructions ) ; and ( 3 ) it can learn , from instructions , each class of knowledge it uses to perform tasks .", "topics": ["natural language", "interaction"]}
{"title": "understanding the abstract dialectical framework ( preliminary report )", "abstract": "among the most general structures extending the framework by dung are the abstract dialectical frameworks ( adfs ) . they come equipped with various types of semantics , with the most prominent - the labeling-based one - analyzed in the context of computational complexity , signatures , instantiations and software support . this makes the abstract dialectical frameworks valuable tools for argumentation . however , there are fewer results available concerning the relation between the adfs and other argumentation frameworks . in this paper we would like to address this issue by introducing a number of translations from various formalisms into adfs . the results of our study show the similarities and differences between them , thus promoting the use and understanding of adfs . moreover , our analysis also proves their capability to model many of the existing frameworks , including those that go beyond the attack relation . finally , translations allow other structures to benefit from the research on adfs in general and from the existing software in particular .", "topics": ["computational complexity theory"]}
{"title": "automated variational inference in probabilistic programming", "abstract": "we present a new algorithm for approximate inference in probabilistic programs , based on a stochastic gradient for variational programs . this method is efficient without restrictions on the probabilistic program ; it is particularly practical for distributions which are not analytically tractable , including highly structured distributions that arise in probabilistic programs . we show how to automatically derive mean-field probabilistic programs and optimize them , and demonstrate that our perspective improves inference efficiency over other algorithms .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "concurrent lexicalized dependency parsing : the parsetalk model", "abstract": "a grammar model for concurrent , object-oriented natural language parsing is introduced . complete lexical distribution of grammatical knowledge is achieved building upon the head-oriented notions of valency and dependency , while inheritance mechanisms are used to capture lexical generalizations . the underlying concurrent computation model relies upon the actor paradigm . we consider message passing protocols for establishing dependency relations and ambiguity handling .", "topics": ["natural language", "parsing"]}
{"title": "optimal and approximate q-value functions for decentralized pomdps", "abstract": "decision-theoretic planning is a popular approach to sequential decision making problems , because it treats uncertainty in sensing and acting in a principled way . in single-agent frameworks like mdps and pomdps , planning can be carried out by resorting to q-value functions : an optimal q-value function q* is computed in a recursive manner by dynamic programming , and then an optimal policy is extracted from q* . in this paper we study whether similar q-value functions can be defined for decentralized pomdp models ( dec-pomdps ) , and how policies can be extracted from such value functions . we define two forms of the optimal q-value function for dec-pomdps : one that gives a normative description as the q-value function of an optimal pure joint policy and another one that is sequentially rational and thus gives a recipe for computation . this computation , however , is infeasible for all but the smallest problems . therefore , we analyze various approximate q-value functions that allow for efficient computation . we describe how they relate , and we prove that they all provide an upper bound to the optimal q-value function q* . finally , unifying some previous approaches for solving dec-pomdps , we describe a family of algorithms for extracting policies from such q-value functions , and perform an experimental evaluation on existing test problems , including a new firefighting benchmark problem .", "topics": ["approximation algorithm", "value ( ethics )"]}
{"title": "correlation of data reconstruction error and shrinkages in pair-wise distances under principal component analysis ( pca )", "abstract": "in this on-going work , i explore certain theoretical and empirical implications of data transformations under the pca . in particular , i state and prove three theorems about pca , which i paraphrase as follows : 1 ) . pca without discarding eigenvector rows is injective , but looses this injectivity when eigenvector rows are discarded 2 ) . pca without discarding eigen- vector rows preserves pair-wise distances , but tends to cause pair-wise distances to shrink when eigenvector rows are discarded . 3 ) . for any pair of points , the shrinkage in pair-wise distance is bounded above by an l1 norm reconstruction error associated with the points . clearly , 3 ) . suggests that there might exist some correlation between shrinkages in pair-wise distances and mean square reconstruction error which is defined as the sum of those eigenvalues associated with the discarded eigenvectors . i therefore decided to perform numerical experiments to obtain the corre- lation between the sum of those eigenvalues and shrinkages in pair-wise distances . in addition , i have also performed some experiments to check respectively the effect of the sum of those eigenvalues and the effect of the shrinkages on classification accuracies under the pca map . so far , i have obtained the following results on some publicly available data from the uci machine learning repository : 1 ) . there seems to be a strong cor- relation between the sum of those eigenvalues associated with discarded eigenvectors and shrinkages in pair-wise distances . 2 ) . neither the sum of those eigenvalues nor pair-wise distances have any strong correlations with classification accuracies . 1", "topics": ["numerical analysis"]}
{"title": "deep latent dirichlet allocation with topic-layer-adaptive stochastic gradient riemannian mcmc", "abstract": "it is challenging to develop stochastic gradient based scalable inference for deep discrete latent variable models ( lvms ) , due to the difficulties in not only computing the gradients , but also adapting the step sizes to different latent factors and hidden layers . for the poisson gamma belief network ( pgbn ) , a recently proposed deep discrete lvm , we derive an alternative representation that is referred to as deep latent dirichlet allocation ( dlda ) . exploiting data augmentation and marginalization techniques , we derive a block-diagonal fisher information matrix and its inverse for the simplex-constrained global model parameters of dlda . exploiting that fisher information matrix with stochastic gradient mcmc , we present topic-layer-adaptive stochastic gradient riemannian ( tlasgr ) mcmc that jointly learns simplex-constrained global parameters across all layers and topics , with topic and layer specific learning rates . state-of-the-art results are demonstrated on big data sets .", "topics": ["bayesian network", "gradient"]}
{"title": "the computational principles of learning ability", "abstract": "it has been quite a long time since ai researchers in the field of computer science stop talking about simulating human intelligence or trying to explain how brain works . recently , represented by deep learning techniques , the field of machine learning is experiencing unprecedented prosperity and some applications with near human-level performance bring researchers confidence to imply that their approaches are the promising candidate for understanding the mechanism of human brain . however apart from several ancient philological criteria and some imaginary black box tests ( turing test , chinese room ) there is no computational level explanation , definition or criteria about intelligence or any of its components . base on the common sense that learning ability is one critical component of intelligence and inspect from the viewpoint of mapping relations , this paper presents two laws which explains what is the `` learning ability '' as we familiar with and under what conditions a mapping relation can be acknowledged as `` learning model '' .", "topics": ["simulation"]}
{"title": "direct visual odometry using bit-planes", "abstract": "feature descriptors , such as sift and orb , are well-known for their robustness to illumination changes , which has made them popular for feature-based vslam\\ @ . however , in degraded imaging conditions such as low light , low texture , blur and specular reflections , feature extraction is often unreliable . in contrast , direct vslam methods which estimate the camera pose by minimizing the photometric error using raw pixel intensities are often more robust to low textured environments and blur . nonetheless , at the core of direct vslam is the reliance on a consistent photometric appearance across images , otherwise known as the brightness constancy assumption . unfortunately , brightness constancy seldom holds in real world applications . in this work , we overcome brightness constancy by incorporating feature descriptors into a direct visual odometry framework . this combination results in an efficient algorithm that combines the strength of both feature-based algorithms and direct methods . namely , we achieve robustness to arbitrary photometric variations while operating in low-textured and poorly lit environments . our approach utilizes an efficient binary descriptor , which we call bit-planes , and show how it can be used in the gradient-based optimization required by direct methods . moreover , we show that the squared euclidean distance between bit-planes is equivalent to the hamming distance . hence , the descriptor may be used in least squares optimization without sacrificing its photometric invariance . finally , we present empirical results that demonstrate the robustness of the approach in poorly lit underground environments .", "topics": ["feature extraction", "pixel"]}
{"title": "accelerated dual learning by homotopic initialization", "abstract": "gradient descent and coordinate descent are well understood in terms of their asymptotic behavior , but less so in a transient regime often used for approximations in machine learning . we investigate how proper initialization can have a profound effect on finding near-optimal solutions quickly . we show that a certain property of a data set , namely the boundedness of the correlations between eigenfeatures and the response variable , can lead to faster initial progress than expected by commonplace analysis . convex optimization problems can tacitly benefit from that , but this automatism does not apply to their dual formulation . we analyze this phenomenon and devise provably good initialization strategies for dual optimization as well as heuristics for the non-convex case , relevant for deep learning . we find our predictions and methods to be experimentally well-supported .", "topics": ["gradient descent", "approximation"]}
{"title": "identifying discourse markers in spoken dialog", "abstract": "in this paper , we present a method for identifying discourse marker usage in spontaneous speech based on machine learning . discourse markers are denoted by special pos tags , and thus the process of pos tagging can be used to identify discourse markers . by incorporating pos tagging into language modeling , discourse markers can be identified during speech recognition , in which the timeliness of the information can be used to help predict the following words . we contrast this approach with an alternative machine learning approach proposed by litman ( 1996 ) . this paper also argues that discourse markers can be used to help the hearer predict the role that the upcoming utterance plays in the dialog . thus discourse markers should provide valuable evidence for automatic dialog act prediction .", "topics": ["speech recognition"]}
{"title": "a survey of predictive modelling under imbalanced distributions", "abstract": "many real world data mining applications involve obtaining predictive models using data sets with strongly imbalanced distributions of the target variable . frequently , the least common values of this target variable are associated with events that are highly relevant for end users ( e.g . fraud detection , unusual returns on stock markets , anticipation of catastrophes , etc . ) . moreover , the events may have different costs and benefits , which when associated with the rarity of some of them on the available training data creates serious problems to predictive modelling techniques . this paper presents a survey of existing techniques for handling these important applications of predictive analytics . although most of the existing work addresses classification tasks ( nominal target variables ) , we also describe methods designed to handle similar problems within regression tasks ( numeric target variables ) . in this survey we discuss the main challenges raised by imbalanced distributions , describe the main approaches to these problems , propose a taxonomy of these methods and refer to some related problems within predictive modelling .", "topics": ["test set", "data mining"]}
{"title": "relaynet : retinal layer and fluid segmentation of macular optical coherence tomography using fully convolutional network", "abstract": "optical coherence tomography ( oct ) is used for non-invasive diagnosis of diabetic macular edema assessing the retinal layers . in this paper , we propose a new fully convolutional deep architecture , termed relaynet , for end-to-end segmentation of retinal layers and fluid masses in eye oct scans . relaynet uses a contracting path of convolutional blocks ( encoders ) to learn a hierarchy of contextual features , followed by an expansive path of convolutional blocks ( decoders ) for semantic segmentation . relaynet is trained to optimize a joint loss function comprising of weighted logistic regression and dice overlap loss . the framework is validated on a publicly available benchmark dataset with comparisons against five state-of-the-art segmentation methods including two deep learning based approaches to substantiate its effectiveness .", "topics": ["loss function", "end-to-end principle"]}
{"title": "weakly-supervised disentangling with recurrent transformations for 3d view synthesis", "abstract": "an important problem for both graphics and vision is to synthesize novel views of a 3d object from a single image . this is particularly challenging due to the partial observability inherent in projecting a 3d object onto the image space , and the ill-posedness of inferring object shape and pose . however , we can train a neural network to address the problem if we restrict our attention to specific object categories ( in our case faces and chairs ) for which we can gather ample training data . in this paper , we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image . the recurrent structure allows our model to capture long-term dependencies along a sequence of transformations . we demonstrate the quality of its predictions for human faces on the multi-pie dataset and for a dataset of 3d chair models , and also show its ability to disentangle latent factors of variation ( e.g . , identity and pose ) without using full supervision .", "topics": ["test set", "end-to-end principle"]}
{"title": "improving one-shot learning through fusing side information", "abstract": "deep neural networks ( dnns ) often struggle with one-shot learning where we have only one or a few labeled training examples per category . in this paper , we argue that by using side information , we may compensate the missing information across classes . we introduce two statistical approaches for fusing side information into data representation learning to improve one-shot learning . first , we propose to enforce the statistical dependency between data representations and multiple types of side information . second , we introduce an attention mechanism to efficiently treat examples belonging to the 'lots-of-examples ' classes as quasi-samples ( additional training samples ) for 'one-example ' classes . we empirically show that our learning architecture improves over traditional softmax regression networks as well as state-of-the-art attentional regression networks on one-shot recognition tasks .", "topics": ["feature learning", "neural networks"]}
{"title": "using local alignments for relation recognition", "abstract": "this paper discusses the problem of marrying structural similarity with semantic relatedness for information extraction from text . aiming at accurate recognition of relations , we introduce local alignment kernels and explore various possibilities of using them for this task . we give a definition of a local alignment ( la ) kernel based on the smith-waterman score as a sequence similarity measure and proceed with a range of possibilities for computing similarity between elements of sequences . we show how distributional similarity measures obtained from unlabeled data can be incorporated into the learning task as semantic knowledge . our experiments suggest that the la kernel yields promising results on various biomedical corpora outperforming two baselines by a large margin . additional series of experiments have been conducted on the data sets of seven general relation types , where the performance of the la kernel is comparable to the current state-of-the-art results .", "topics": ["kernel ( operating system )", "text corpus"]}
{"title": "robust ensemble classifier combination based on noise removal with one-class svm", "abstract": "in machine learning area , as the number of labeled input samples becomes very large , it is very difficult to build a classification model because of input data set is not fit in a memory in training phase of the algorithm , therefore , it is necessary to utilize data partitioning to handle overall data set . bagging and boosting based data partitioning methods have been broadly used in data mining and pattern recognition area . both of these methods have shown a great possibility for improving classification model performance . this study is concerned with the analysis of data set partitioning with noise removal and its impact on the performance of multiple classifier models . in this study , we propose noise filtering preprocessing at each data set partition to increment classifier model performance . we applied gini impurity approach to find the best split percentage of noise filter ratio . the filtered sub data set is then used to train individual ensemble models .", "topics": ["data mining", "noise reduction"]}
{"title": "a balanced k-means algorithm for weighted point sets", "abstract": "the classical $ k $ -means algorithm for partitioning $ n $ points in $ \\mathbb { r } ^d $ into $ k $ clusters is one of the most popular and widely spread clustering methods . the need to respect prescribed lower bounds on the cluster sizes has been observed in many scientific and business applications . in this paper , we present and analyze a generalization of $ k $ -means that is capable of handling weighted point sets and prescribed lower and upper bounds on the cluster sizes . we call it weight-balanced $ k $ -means . the key difference to existing models lies in the ability to handle the combination of weighted point sets with prescribed bounds on the cluster sizes . this imposes the need to perform partial membership clustering , and leads to significant differences . for example , while finite termination of all $ k $ -means variants for unweighted point sets is a simple consequence of the existence of only finitely many partitions of a given set of points , the situation is more involved for weighted point sets , as there are infinitely many partial membership clusterings . using polyhedral theory , we show that the number of iterations of weight-balanced $ k $ -means is bounded above by $ n^ { o ( dk ) } $ , so in particular it is polynomial for fixed $ k $ and $ d $ . this is similar to the known worst-case upper bound for classical $ k $ -means for unweighted point sets and unrestricted cluster sizes , despite the much more general framework . we conclude with the discussion of some additional favorable properties of our method .", "topics": ["cluster analysis", "computation"]}
{"title": "articulated pose estimation using hierarchical exemplar-based models", "abstract": "exemplar-based models have achieved great success on localizing the parts of semi-rigid objects . however , their efficacy on highly articulated objects such as humans is yet to be explored . inspired by hierarchical object representation and recent application of deep convolutional neural networks ( dcnns ) on human pose estimation , we propose a novel formulation that incorporates both hierarchical exemplar-based models and dcnns in the spatial terms . specifically , we obtain more expressive spatial models by assuming independence between exemplars at different levels in the hierarchy ; we also obtain stronger spatial constraints by inferring the spatial relations between parts at the same level . as our method strikes a good balance between expressiveness and strength of spatial models , it is both effective and generalizable , achieving state-of-the-art results on different benchmarks : leeds sports dataset and cub-200-2011 .", "topics": ["neural networks"]}
{"title": "robust high-dimensional linear regression", "abstract": "the effectiveness of supervised learning techniques has made them ubiquitous in research and practice . in high-dimensional settings , supervised learning commonly relies on dimensionality reduction to improve performance and identify the most important factors in predicting outcomes . however , the economic importance of learning has made it a natural target for adversarial manipulation of training data , which we term poisoning attacks . prior approaches to dealing with robust supervised learning rely on strong assumptions about the nature of the feature matrix , such as feature independence and sub-gaussian noise with low variance . we propose an integrated method for robust regression that relaxes these assumptions , assuming only that the feature matrix can be well approximated by a low-rank matrix . our techniques integrate improved robust low-rank matrix approximation and robust principle component regression , and yield strong performance guarantees . moreover , we experimentally show that our methods significantly outperform state of the art both in running time and prediction error .", "topics": ["test set", "supervised learning"]}
{"title": "classifying unordered feature sets with convolutional deep averaging networks", "abstract": "unordered feature sets are a nonstandard data structure that traditional neural networks are incapable of addressing in a principled manner . providing a concatenation of features in an arbitrary order may lead to the learning of spurious patterns or biases that do not actually exist . another complication is introduced if the number of features varies between each set . we propose convolutional deep averaging networks ( cdans ) for classifying and learning representations of datasets whose instances comprise variable-size , unordered feature sets . cdans are efficient , permutation-invariant , and capable of accepting sets of arbitrary size . we emphasize the importance of nonlinear feature embeddings for obtaining effective cdan classifiers and illustrate their advantages in experiments versus linear embeddings and alternative permutation-invariant and -equivariant architectures .", "topics": ["nonlinear system"]}
{"title": "modeling the formation of social conventions in multi-agent populations", "abstract": "in order to understand the formation of social conventions we need to know the specific role of control and learning in multi-agent systems . to advance in this direction , we propose , within the framework of the distributed adaptive control ( dac ) theory , a novel control-based reinforcement learning architecture ( crl ) that can account for the acquisition of social conventions in multi-agent populations that are solving a benchmark social decision-making problem . our new crl architecture , as a concrete realization of dac multi-agent theory , implements a low-level sensorimotor control loop handling the agent 's reactive behaviors ( pre-wired reflexes ) , along with a layer based on model-free reinforcement learning that maximizes long-term reward . we apply crl in a multi-agent game-theoretic task in which coordination must be achieved in order to find an optimal solution . we show that our crl architecture is able to both find optimal solutions in discrete and continuous time and reproduce human experimental data on standard game-theoretic metrics such as efficiency in acquiring rewards , fairness in reward distribution and stability of convention formation .", "topics": ["optimization problem", "reinforcement learning"]}
{"title": "a locally adapting technique for boundary detection using image segmentation", "abstract": "rapid growth in the field of quantitative digital image analysis is paving the way for researchers to make precise measurements about objects in an image . to compute quantities from the image such as the density of compressed materials or the velocity of a shockwave , we must determine object boundaries . images containing regions that each have a spatial trend in intensity are of particular interest . we present a supervised image segmentation method that incorporates spatial information to locate boundaries between regions with overlapping intensity histograms . the segmentation of a pixel is determined by comparing its intensity to distributions from local , nearby pixel intensities . because of the statistical nature of the algorithm , we use maximum likelihood estimation theory to quantify uncertainty about each boundary . we demonstrate the success of this algorithm on a radiograph of a multicomponent cylinder and on an optical image of a laser-induced shockwave , and we provide final boundary locations with associated bands of uncertainty .", "topics": ["image segmentation", "supervised learning"]}
{"title": "spatial pyramid convolutional neural network for social event detection in static image", "abstract": "social event detection in a static image is a very challenging problem and it 's very useful for internet of things applications including automatic photo organization , ads recommender system , or image captioning . several publications show that variety of objects , scene , and people can be very ambiguous for the system to decide the event that occurs in the image . we proposed the spatial pyramid configuration of convolutional neural network ( cnn ) classifier for social event detection in a static image . by applying the spatial pyramid configuration to the cnn classifier , the detail that occurs in the image can observe more accurately by the classifier . used dataset provided by ahmad et al . is used to evaluate our proposed method , which consists of two different image sets , eimm , and sed dataset . as a result , the average accuracy of our system outperforms the baseline method by 15 % and 2 % respectively .", "topics": ["baseline ( configuration management )"]}
{"title": "adversarial examples detection in deep networks with convolutional filter statistics", "abstract": "deep learning has greatly improved visual recognition in recent years . however , recent research has shown that there exist many adversarial examples that can negatively impact the performance of such an architecture . this paper focuses on detecting those adversarial examples by analyzing whether they come from the same distribution as the normal examples . instead of directly training a deep neural network to detect adversarials , a much simpler approach was proposed based on statistics on outputs from convolutional layers . a cascade classifier was designed to efficiently detect adversarials . furthermore , trained from one particular adversarial generating mechanism , the resulting classifier can successfully detect adversarials from a completely different mechanism as well . the resulting classifier is non-subdifferentiable , hence creates a difficulty for adversaries to attack by using the gradient of the classifier . after detecting adversarial examples , we show that many of them can be recovered by simply performing a small average filter on the image . those findings should lead to more insights about the classification mechanisms in deep convolutional neural networks .", "topics": ["gradient"]}
{"title": "detecting multiword phrases in mathematical text corpora", "abstract": "we present an approach for detecting multiword phrases in mathematical text corpora . the method used is based on characteristic features of mathematical terminology . it makes use of a software tool named lingo which allows to identify words by means of previously defined dictionaries for specific word classes as adjectives , personal names or nouns . the detection of multiword groups is done algorithmically . possible advantages of the method for indexing and information retrieval and conclusions for applying dictionary-based methods of automatic indexing instead of stemming procedures are discussed .", "topics": ["text corpus", "dictionary"]}
{"title": "unsupervised grammar induction with depth-bounded pcfg", "abstract": "there has been recent interest in applying cognitively or empirically motivated bounds on recursion depth to limit the search space of grammar induction models ( ponvert et al . , 2011 ; noji and johnson , 2016 ; shain et al . , 2016 ) . this work extends this depth-bounding approach to probabilistic context-free grammar induction ( db-pcfg ) , which has a smaller parameter space than hierarchical sequence models , and therefore more fully exploits the space reductions of depth-bounding . results for this model on grammar acquisition from transcribed child-directed speech and newswire text exceed or are competitive with those of other models when evaluated on parse accuracy . moreover , gram- mars acquired from this model demonstrate a consistent use of category labels , something which has not been demonstrated by other acquisition models .", "topics": ["parsing"]}
{"title": "a novel mutation operator based on the union of fitness and design spaces information for differential evolution", "abstract": "differential evolution ( de ) is one of the most successful and powerful evolutionary algorithms for global optimization problem . the most important operator in this algorithm is mutation operator which parents are selected randomly to participate in it . recently , numerous papers are tried to make this operator more intelligent by selection of parents for mutation intelligently . the intelligent selection for mutation vectors is performed by applying design space ( also known as decision space ) criterion or fitness space criterion , however , in both cases , half of valuable information of the problem space is disregarded . in this article , a universal differential evolution ( ude ) is proposed which takes advantage of both design and fitness spaces criteria for intelligent selection of mutation vectors . the experimental analysis on ude are performed on cec2005 benchmarks and the results stated that ude significantly improved the performance of differential evolution in comparison with other methods that only use one criterion for intelligent selection .", "topics": ["optimization problem", "eisenstein 's criterion"]}
{"title": "free form based active contours for image segmentation and free space perception", "abstract": "in this paper we present a novel approach for representing and evolving deformable active contours . the method combines piecewise regular b { \\'e } zier models and curve evolution defined by local free form deformation . the contour deformation is locally constrained which allows contour convergence with almost linear complexity while adapting to various shape settings and handling topology changes of the active contour . we demonstrate the effectiveness of the new active contour scheme for visual free space perception and segmentation using omnidirectional images acquired by a robot exploring unknown indoor and outdoor environments . several experiments validate the approach with comparison to state-of-the art parametric and geometric active contours and provide fast and real-time robot free space segmentation and navigation .", "topics": ["image segmentation", "robot"]}
{"title": "pca with gaussian perturbations", "abstract": "most of machine learning deals with vector parameters . ideally we would like to take higher order information into account and make use of matrix or even tensor parameters . however the resulting algorithms are usually inefficient . here we address on-line learning with matrix parameters . it is often easy to obtain online algorithm with good generalization performance if you eigendecompose the current parameter matrix in each trial ( at a cost of $ o ( n^3 ) $ per trial ) . ideally we want to avoid the decompositions and spend $ o ( n^2 ) $ per trial , i.e . linear time in the size of the matrix data . there is a core trade-off between the running time and the generalization performance , here measured by the regret of the on-line algorithm ( total gain of the best off-line predictor minus the total gain of the on-line algorithm ) . we focus on the key matrix problem of rank $ k $ principal component analysis in $ \\mathbb { r } ^n $ where $ k \\ll n $ . there are $ o ( n^3 ) $ algorithms that achieve the optimum regret but require eigendecompositions . we develop a simple algorithm that needs $ o ( kn^2 ) $ per trial whose regret is off by a small factor of $ o ( n^ { 1/4 } ) $ . the algorithm is based on the follow the perturbed leader paradigm . it replaces full eigendecompositions at each trial by the problem finding $ k $ principal components of the current covariance matrix that is perturbed by gaussian noise .", "topics": ["regret ( decision theory )", "time complexity"]}
{"title": "using self-contradiction to learn confidence measures in stereo vision", "abstract": "learned confidence measures gain increasing importance for outlier removal and quality improvement in stereo vision . however , acquiring the necessary training data is typically a tedious and time consuming task that involves manual interaction , active sensing devices and/or synthetic scenes . to overcome this problem , we propose a new , flexible , and scalable way for generating training data that only requires a set of stereo images as input . the key idea of our approach is to use different view points for reasoning about contradictions and consistencies between multiple depth maps generated with the same stereo algorithm . this enables us to generate a huge amount of training data in a fully automated manner . among other experiments , we demonstrate the potential of our approach by boosting the performance of three learned confidence measures on the kitti2012 dataset by simply training them on a vast amount of automatically generated training data rather than a limited amount of laser ground truth data .", "topics": ["test set", "synthetic data"]}
{"title": "exploiting n-best hypotheses to improve an smt approach to grammatical error correction", "abstract": "grammatical error correction ( gec ) is the task of detecting and correcting grammatical errors in texts written by second language learners . the statistical machine translation ( smt ) approach to gec , in which sentences written by second language learners are translated to grammatically correct sentences , has achieved state-of-the-art accuracy . however , the smt approach is unable to utilize global context . in this paper , we propose a novel approach to improve the accuracy of gec , by exploiting the n-best hypotheses generated by an smt approach . specifically , we build a classifier to score the edits in the n-best hypotheses . the classifier can be used to select appropriate edits or re-rank the n-best hypotheses . we apply these methods to a state-of-the-art gec system that uses the smt approach . our experiments show that our methods achieve statistically significant improvements in accuracy over the best published results on a benchmark test dataset on gec .", "topics": ["statistical classification", "machine translation"]}
{"title": "fast algorithms for robust pca via gradient descent", "abstract": "we consider the problem of robust pca in the fully and partially observed settings . without corruptions , this is the well-known matrix completion problem . from a statistical standpoint this problem has been recently well-studied , and conditions on when recovery is possible ( how many observations do we need , how many corruptions can we tolerate ) via polynomial-time algorithms is by now understood . this paper presents and analyzes a non-convex optimization approach that greatly reduces the computational complexity of the above problems , compared to the best available algorithms . in particular , in the fully observed case , with $ r $ denoting rank and $ d $ dimension , we reduce the complexity from $ \\mathcal { o } ( r^2d^2\\log ( 1/\\varepsilon ) ) $ to $ \\mathcal { o } ( rd^2\\log ( 1/\\varepsilon ) ) $ -- a big savings when the rank is big . for the partially observed case , we show the complexity of our algorithm is no more than $ \\mathcal { o } ( r^4d \\log d \\log ( 1/\\varepsilon ) ) $ . not only is this the best-known run-time for a provable algorithm under partial observation , but in the setting where $ r $ is small compared to $ d $ , it also allows for near-linear-in- $ d $ run-time that can be exploited in the fully-observed case as well , by simply running our algorithm on a subset of the observations .", "topics": ["computational complexity theory", "gradient descent"]}
{"title": "deep sequential neural network", "abstract": "neural networks sequentially build high-level features through their successive layers . we propose here a new neural network model where each layer is associated with a set of candidate mappings . when an input is processed , at each layer , one mapping among these candidates is selected according to a sequential decision process . the resulting model is structured according to a dag like architecture , so that a path from the root to a leaf node defines a sequence of transformations . instead of considering global transformations , like in classical multilayer networks , this model allows us for learning a set of local transformations . it is thus able to process data with different characteristics through specific sequences of such local transformations , increasing the expression power of this model w.r.t a classical multilayered network . the learning algorithm is inspired from policy gradient techniques coming from the reinforcement learning domain and is used here instead of the classical back-propagation based gradient descent techniques . experiments on different datasets show the relevance of this approach .", "topics": ["high- and low-level", "neural networks"]}
{"title": "persistent clustering and a theorem of j. kleinberg", "abstract": "we construct a framework for studying clustering algorithms , which includes two key ideas : persistence and functoriality . the first encodes the idea that the output of a clustering scheme should carry a multiresolution structure , the second the idea that one should be able to compare the results of clustering algorithms as one varies the data set , for example by adding points or by applying functions to it . we show that within this framework , one can prove a theorem analogous to one of j. kleinberg , in which one obtains an existence and uniqueness theorem instead of a non-existence result . we explore further properties of this unique scheme , stability and convergence are established .", "topics": ["cluster analysis"]}
{"title": "balancing interpretability and predictive accuracy for unsupervised tensor mining", "abstract": "the parafac tensor decomposition has enjoyed an increasing success in exploratory multi-aspect data mining scenarios . a major challenge remains the estimation of the number of latent factors ( i.e . , the rank ) of the decomposition , which yields high-quality , interpretable results . previously , we have proposed an automated tensor mining method which leverages a well-known quality heuristic from the field of chemometrics , the core consistency diagnostic ( corcondia ) , in order to automatically determine the rank for the parafac decomposition . in this work we set out to explore the trade-off between 1 ) the interpretability/quality of the results ( as expressed by corcondia ) , and 2 ) the predictive accuracy of the results , in order to further improve the rank estimation quality . our preliminary results indicate that striking a good balance in that trade-off benefits rank estimation .", "topics": ["data mining", "heuristic"]}
{"title": "transfer learning for melanoma detection : participation in isic 2017 skin lesion classification challenge", "abstract": "this manuscript describes our participation in the international skin imaging collaboration 's 2017 skin lesion analysis towards melanoma detection competition . we participated in part 3 : lesion classification . the two stated goals of this binary image classification challenge were to distinguish between ( a ) melanoma and ( b ) nevus and seborrheic keratosis , followed by distinguishing between ( a ) seborrheic keratosis and ( b ) nevus and melanoma . we chose a deep neural network approach with a transfer learning strategy , using a pre-trained inception v3 network as both a feature extractor to provide input for a multi-layer perceptron as well as fine-tuning an augmented inception network . this approach yielded validation set auc 's of 0.84 on the second task and 0.76 on the first task , for an average auc of 0.80 . we joined the competition unfortunately late , and we look forward to improving on these results .", "topics": ["test set", "computer vision"]}
{"title": "context-aware bandits", "abstract": "we propose an efficient context-aware clustering of bandits ( cab ) algorithm , which can capture collaborative effects . cab can be easily deployed in a real-world recommendation system , where multi-armed bandits have been shown to perform well in particular with respect to the cold-start problem . cab utilizes a context-aware clustering augmented by exploration-exploitation strategies . cab dynamically clusters the users based on the content universe under consideration . we give a theoretical analysis in the standard stochastic multi-armed bandits setting . we show the efficiency of our approach on production and real-world datasets , demonstrate the scalability , and , more importantly , the significant increased prediction performance against several state-of-the-art methods .", "topics": ["cluster analysis", "scalability"]}
{"title": "distributed computation as hierarchy", "abstract": "this paper presents a new distributed computational model of distributed systems called the phase web that extends v. pratt 's orthocurrence relation from 1986 . the model uses mutual-exclusion to express sequence , and a new kind of hierarchy to replace event sequences , posets , and pomsets . the model explicitly connects computation to a discrete clifford algebra that is in turn extended into homology and co-homology , wherein the recursive nature of objects and boundaries becomes apparent and itself subject to hierarchical recursion . topsy , a programming environment embodying the phase web , is available from www.cs.auc.dk/topsy .", "topics": ["computation"]}
{"title": "computational narrative intelligence : a human-centered goal for artificial intelligence", "abstract": "narrative intelligence is the ability to craft , tell , understand , and respond affectively to stories . we argue that instilling artificial intelligences with computational narrative intelligence affords a number of applications beneficial to humans . we lay out some of the machine learning challenges necessary to solve to achieve computational narrative intelligence . finally , we argue that computational narrative is a practical step towards machine enculturation , the teaching of sociocultural values to machines .", "topics": ["artificial intelligence"]}
{"title": "bayesian hypernetworks", "abstract": "we propose bayesian hypernetworks : a framework for approximate bayesian inference in neural networks . a bayesian hypernetwork , $ h $ , is a neural network which learns to transform a simple noise distribution , $ p ( \\epsilon ) = \\mathcal { n } ( 0 , i ) $ , to a distribution $ q ( \\theta ) \\doteq q ( h ( \\epsilon ) ) $ over the parameters $ \\theta $ of another neural network ( the `` primary network '' ) . we train $ q $ with variational inference , using an invertible $ h $ to enable efficient estimation of the variational lower bound on the posterior $ p ( \\theta | \\mathcal { d } ) $ via sampling . in contrast to most methods for bayesian deep learning , bayesian hypernets can represent a complex multimodal approximate posterior with correlations between parameters , while enabling cheap i.i.d . sampling of $ q ( \\theta ) $ . we demonstrate these qualitative advantages of bayesian hypernets , which also achieve competitive performance on a suite of tasks that demonstrate the advantage of estimating model uncertainty , including active learning and anomaly detection .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "low-rank passthrough neural networks", "abstract": "deep learning consists in training neural networks to perform computations that sequentially unfold in many steps over a time dimension or an intrinsic depth dimension . effective learning in this setting is usually accomplished by specialized network architectures that are designed to mitigate the vanishing gradient problem of naive deep networks . many of these architectures , such as lstms , grus , highway networks and deep residual network , are based on a single structural principle : the state passthrough . we observe that these architectures , hereby characterized as passthrough networks , in addition to the mitigation of the vanishing gradient problem , enable the decoupling of the network state size from the number of parameters of the network , a possibility that is exploited in some recent works but not thoroughly explored . in this work we propose simple , yet effective , low-rank and low-rank plus diagonal matrix parametrizations for passthrough networks which exploit this decoupling property , reducing the data complexity and memory requirements of the network while preserving its memory capacity . we present competitive experimental results on synthetic tasks and a near state of the art result on sequential randomly-permuted mnist classification , a hard task on natural data .", "topics": ["computation", "mnist database"]}
{"title": "dynamic screening : accelerating first-order algorithms for the lasso and group-lasso", "abstract": "recent computational strategies based on screening tests have been proposed to accelerate algorithms addressing penalized sparse regression problems such as the lasso . such approaches build upon the idea that it is worth dedicating some small computational effort to locate inactive atoms and remove them from the dictionary in a preprocessing stage so that the regression algorithm working with a smaller dictionary will then converge faster to the solution of the initial problem . we believe that there is an even more efficient way to screen the dictionary and obtain a greater acceleration : inside each iteration of the regression algorithm , one may take advantage of the algorithm computations to obtain a new screening test for free with increasing screening effects along the iterations . the dictionary is henceforth dynamically screened instead of being screened statically , once and for all , before the first iteration . we formalize this dynamic screening principle in a general algorithmic scheme and apply it by embedding inside a number of first-order algorithms adapted existing screening tests to solve the lasso or new screening tests to solve the group-lasso . computational gains are assessed in a large set of experiments on synthetic data as well as real-world sounds and images . they show both the screening efficiency and the gain in terms running times .", "topics": ["synthetic data", "iteration"]}
{"title": "deep neural network compression with single and multiple level quantization", "abstract": "network quantization is an effective solution to compress deep neural networks for practical usage . existing network quantization methods can not sufficiently exploit the depth information to generate low-bit compressed network . in this paper , we propose two novel network quantization approaches , single-level network quantization ( slq ) for high-bit quantization and multi-level network quantization ( mlq ) for extremely low-bit quantization ( ternary ) .we are the first to consider the network quantization from both width and depth level . in the width level , parameters are divided into two parts : one for quantization and the other for re-training to eliminate the quantization loss . slq leverages the distribution of the parameters to improve the width level . in the depth level , we introduce incremental layer compensation to quantize layers iteratively which decreases the quantization loss in each iteration . the proposed approaches are validated with extensive experiments based on the state-of-the-art neural networks including alexnet , vgg-16 , googlenet and resnet-18 . both slq and mlq achieve impressive results .", "topics": ["iteration"]}
{"title": "improved representation learning for predicting commonsense ontologies", "abstract": "recent work in learning ontologies ( hierarchical and partially-ordered structures ) has leveraged the intrinsic geometry of spaces of learned representations to make predictions that automatically obey complex structural constraints . we explore two extensions of one such model , the order-embedding model for hierarchical relation learning , with an aim towards improved performance on text data for commonsense knowledge representation . our first model jointly learns ordering relations and non-hierarchical knowledge in the form of raw text . our second extension exploits the partial order structure of the training data to find long-distance triplet constraints among embeddings which are poorly enforced by the pairwise training procedure . we find that both incorporating free text and augmented training constraints improve over the original order-embedding model and other strong baselines .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "randomer forests", "abstract": "ensemble methods -- particularly those based on decision trees -- have recently demonstrated superior performance in a variety of machine learning settings . specifically , random forest ( rf ) was found to outperform > 100 other methods in several manuscripts , and gradient boosting trees have been a crucial component of several recent kaggle competition victories . building off these successes and recent advances in sparse learning and random matrix theory , we propose a novel ensemble tree method called `` randomer forest '' ( rerf ) . the key intuition behind rerf is that we can use sparse linear combinations at each decision node rather than just one feature ( as in rf ) or all of them ( as in rotation forests ) . rerf significantly outperforms other methods on a standard benchmark suite containing 105 problems with varying dimension , sample size , and number of classes . moreover , we provide an implementation that scales as or more efficiently than other available packages . via a combination of basic principles , theory , and extensive numerical experiments , we demonstrate why , when , and how rerf achieves its performance properties .", "topics": ["simulation"]}
{"title": "taming numbers and durations in the model checking integrated planning system", "abstract": "the model checking integrated planning system ( mips ) is a temporal least commitment heuristic search planner based on a flexible object-oriented workbench architecture . its design clearly separates explicit and symbolic directed exploration algorithms from the set of on-line and off-line computed estimates and associated data structures . mips has shown distinguished performance in the last two international planning competitions . in the last event the description language was extended from pure propositional planning to include numerical state variables , action durations , and plan quality objective functions . plans were no longer sequences of actions but time-stamped schedules . as a participant of the fully automated track of the competition , mips has proven to be a general system ; in each track and every benchmark domain it efficiently computed plans of remarkable quality . this article introduces and analyzes the most important algorithmic novelties that were necessary to tackle the new layers of expressiveness in the benchmark problems and to achieve a high level of performance . the extensions include critical path analysis of sequentially generated plans to generate corresponding optimal parallel plans . the linear time algorithm to compute the parallel plan bypasses known np hardness results for partial ordering by scheduling plans with respect to the set of actions and the imposed precedence relations . the efficiency of this algorithm also allows us to improve the exploration guidance : for each encountered planning state the corresponding approximate sequential plan is scheduled . one major strength of mips is its static analysis phase that grounds and simplifies parameterized predicates , functions and operators , that infers knowledge to minimize the state description length , and that detects domain object symmetries . the latter aspect is analyzed in detail . mips has been developed to serve as a complete and optimal state space planner , with admissible estimates , exploration engines and branching cuts . in the competition version , however , certain performance compromises had to be made , including floating point arithmetic , weighted heuristic search exploration according to an inadmissible estimate and parameterized optimization .", "topics": ["time complexity", "numerical analysis"]}
{"title": "feature selection : a data perspective", "abstract": "feature selection , as a data preprocessing strategy , has been proven to be effective and efficient in preparing high-dimensional data for data mining and machine learning problems . the objectives of feature selection include : building simpler and more comprehensible models , improving data mining performance , and preparing clean , understandable data . the recent proliferation of big data has presented some substantial challenges and opportunities for feature selection research . in this survey , we provide a comprehensive and structured overview of recent advances in feature selection research . motivated by current challenges and opportunities in the big data age , we revisit feature selection research from a data perspective , and review representative feature selection algorithms for generic data , structured data , heterogeneous data and streaming data . methodologically , to emphasize the differences and similarities of most existing feature selection algorithms for generic data , we generally categorize them into four groups : similarity based , information theoretical based , sparse learning based and statistical based methods . finally , to facilitate and promote the research in this community , we also present an open-source feature selection repository that consists of most of the popular feature selection algorithms ( \\url { http : //featureselection.asu.edu/ } ) . at the end of this survey , we also have a discussion about some open problems and challenges that need to be paid more attention in future research .", "topics": ["data mining", "sparse matrix"]}
{"title": "rough clustering based unsupervised image change detection", "abstract": "this paper introduces an unsupervised technique to detect the changed region of multitemporal images on a same reference plane with the help of rough clustering . the proposed technique is a soft-computing approach , based on the concept of rough set with rough clustering and pawlak 's accuracy . it is less noisy and avoids pre-deterministic knowledge about the distribution of the changed and unchanged regions . to show the effectiveness , the proposed technique is compared with some other approaches .", "topics": ["cluster analysis"]}
{"title": "towards automatic abdominal multi-organ segmentation in dual energy ct using cascaded 3d fully convolutional network", "abstract": "automatic multi-organ segmentation of the dual energy computed tomography ( dect ) data can be beneficial for biomedical research and clinical applications . however , it is a challenging task . recent advances in deep learning showed the feasibility to use 3-d fully convolutional networks ( fcn ) for voxel-wise dense predictions in single energy computed tomography ( sect ) . in this paper , we proposed a 3d fcn based method for automatic multi-organ segmentation in dect . the work was based on a cascaded fcn and a general model for the major organs trained on a large set of sect data . we preprocessed the dect data by using linear weighting and fine-tuned the model for the dect data . the method was evaluated using 42 torso dect data acquired with a clinical dual-source ct system . four abdominal organs ( liver , spleen , left and right kidneys ) were evaluated . cross-validation was tested . effect of the weight on the accuracy was researched . in all the tests , we achieved an average dice coefficient of 93 % for the liver , 90 % for the spleen , 91 % for the right kidney and 89 % for the left kidney , respectively . the results show our method is feasible and promising .", "topics": ["coefficient"]}
{"title": "predicting adolescent suicide attempts with neural networks", "abstract": "though suicide is a major public health problem in the us , machine learning methods are not commonly used to predict an individual 's risk of attempting/committing suicide . in the present work , starting with an anonymized collection of electronic health records for 522,056 unique , california-resident adolescents , we develop neural network models to predict suicide attempts . we frame the problem as a binary classification problem in which we use a patient 's data from 2006-2009 to predict either the presence ( 1 ) or absence ( 0 ) of a suicide attempt in 2010 . after addressing issues such as severely imbalanced classes and the variable length of a patient 's history , we build neural networks with depths varying from two to eight hidden layers . for test set observations where we have at least five ed/hospital visits ' worth of data on a patient , our depth-4 model achieves a sensitivity of 0.703 , specificity of 0.980 , and auc of 0.958 .", "topics": ["test set", "neural networks"]}
{"title": "towards an indexical model of situated language comprehension for cognitive agents in physical worlds", "abstract": "we propose a computational model of situated language comprehension based on the indexical hypothesis that generates meaning representations by translating amodal linguistic symbols to modal representations of beliefs , knowledge , and experience external to the linguistic system . this indexical model incorporates multiple information sources , including perceptions , domain knowledge , and short-term and long-term experiences during comprehension . we show that exploiting diverse information sources can alleviate ambiguities that arise from contextual use of underspecific referring expressions and unexpressed argument alternations of verbs . the model is being used to support linguistic interactions in rosie , an agent implemented in soar that learns from instruction .", "topics": ["interaction"]}
{"title": "tensor graph convolutional neural network", "abstract": "in this paper , we propose a novel tensor graph convolutional neural network ( tgcnn ) to conduct convolution on factorizable graphs , for which here two types of problems are focused , one is sequential dynamic graphs and the other is cross-attribute graphs . especially , we propose a graph preserving layer to memorize salient nodes of those factorized subgraphs , i.e . cross graph convolution and graph pooling . for cross graph convolution , a parameterized kronecker sum operation is proposed to generate a conjunctive adjacency matrix characterizing the relationship between every pair of nodes across two subgraphs . taking this operation , then general graph convolution may be efficiently performed followed by the composition of small matrices , which thus reduces high memory and computational burden . encapsuling sequence graphs into a recursive learning , the dynamics of graphs can be efficiently encoded as well as the spatial layout of graphs . to validate the proposed tgcnn , experiments are conducted on skeleton action datasets as well as matrix completion dataset . the experiment results demonstrate that our method can achieve more competitive performance with the state-of-the-art methods .", "topics": ["convolution"]}
{"title": "one model to rule them all : multitask and multilingual modelling for lexical analysis", "abstract": "when learning a new skill , you take advantage of your preexisting skills and knowledge . for instance , if you are a skilled violinist , you will likely have an easier time learning to play cello . similarly , when learning a new language you take advantage of the languages you already speak . for instance , if your native language is norwegian and you decide to learn dutch , the lexical overlap between these two languages will likely benefit your rate of language acquisition . this thesis deals with the intersection of learning multiple tasks and learning multiple languages in the context of natural language processing ( nlp ) , which can be defined as the study of computational processing of human language . although these two types of learning may seem different on the surface , we will see that they share many similarities . the traditional approach in nlp is to consider a single task for a single language at a time . however , recent advances allow for broadening this approach , by considering data for multiple tasks and languages simultaneously . this is an important approach to explore further as the key to improving the reliability of nlp , especially for low-resource languages , is to take advantage of all relevant data whenever possible . in doing so , the hope is that in the long term , low-resource languages can benefit from the advances made in nlp which are currently to a large extent reserved for high-resource languages . this , in turn , may then have positive consequences for , e.g . , language preservation , as speakers of minority languages will have a lower degree of pressure to using high-resource languages . in the short term , answering the specific research questions posed should be of use to nlp researchers working towards the same goal .", "topics": ["natural language processing", "recurrent neural network"]}
{"title": "reinforcement learning for agents with many sensors and actuators acting in categorizable environments", "abstract": "in this paper , we confront the problem of applying reinforcement learning to agents that perceive the environment through many sensors and that can perform parallel actions using many actuators as is the case in complex autonomous robots . we argue that reinforcement learning can only be successfully applied to this case if strong assumptions are made on the characteristics of the environment in which the learning is performed , so that the relevant sensor readings and motor commands can be readily identified . the introduction of such assumptions leads to strongly-biased learning systems that can eventually lose the generality of traditional reinforcement-learning algorithms . in this line , we observe that , in realistic situations , the reward received by the robot depends only on a reduced subset of all the executed actions and that only a reduced subset of the sensor inputs ( possibly different in each situation and for each action ) are relevant to predict the reward . we formalize this property in the so called 'categorizability assumption ' and we present an algorithm that takes advantage of the categorizability of the environment , allowing a decrease in the learning time with respect to existing reinforcement-learning algorithms . results of the application of the algorithm to a couple of simulated realistic-robotic problems ( landmark-based navigation and the six-legged robot gait generation ) are reported to validate our approach and to compare it to existing flat and generalization-based reinforcement-learning approaches .", "topics": ["reinforcement learning", "simulation"]}
{"title": "deep region hashing for efficient large-scale instance search from images", "abstract": "instance search ( ins ) is a fundamental problem for many applications , while it is more challenging comparing to traditional image search since the relevancy is defined at the instance level . existing works have demonstrated the success of many complex ensemble systems that are typically conducted by firstly generating object proposals , and then extracting handcrafted and/or cnn features of each proposal for matching . however , object bounding box proposals and feature extraction are often conducted in two separated steps , thus the effectiveness of these methods collapses . also , due to the large amount of generated proposals , matching speed becomes the bottleneck that limits its application to large-scale datasets . to tackle these issues , in this paper we propose an effective and efficient deep region hashing ( drh ) approach for large-scale ins using an image patch as the query . specifically , drh is an end-to-end deep neural network which consists of object proposal , feature extraction , and hash code generation . drh shares full-image convolutional feature map with the region proposal network , thus enabling nearly cost-free region proposals . also , each high-dimensional , real-valued region features are mapped onto a low-dimensional , compact binary codes for the efficient object region level matching on large-scale dataset . experimental results on four datasets show that our drh can achieve even better performance than the state-of-the-arts in terms of map , while the efficiency is improved by nearly 100 times .", "topics": ["feature extraction", "map"]}
{"title": "traffic prediction based on random connectivity in deep learning with long short-term memory", "abstract": "traffic prediction plays an important role in evaluating the performance of telecommunication networks and attracts intense research interests . a significant number of algorithms and models have been proposed to learn knowledge from traffic data and improve the prediction accuracy . in the recent big data era , the relevant research enthusiasm remains and deep learning has been exploited to extract the useful information in depth . in particular , long short-term memory ( lstm ) , one kind of recurrent neural network ( rnn ) schemes , has attracted significant attentions due to the long-range dependency embedded in the sequential traffic data . however , lstm has considerable computational cost , which can not be tolerated in tasks with stringent latency requirement . in this paper , we propose a deep learning model based on lstm , called random connectivity lstm ( rclstm ) . compared to the conventional lstm , rclstm achieves a significant breakthrough in the architecture formation of neural network , whose connectivity is determined in a stochastic manner rather than full connected . so , the neural network in rclstm can exhibit certain sparsity , which means many neural connections are absent ( distinguished from the full connectivity ) and thus the number of parameters to be trained is reduced and much fewer computations are required . we apply the rclstm solution to predict traffic and validate that the rclstm with even 35 % neural connectivity still shows a strong capability in traffic prediction . also , along with increasing the number of training samples , the performance of rclstm becomes closer to the conventional lstm . moreover , the rclstm exhibits even superior prediction accuracy than the conventional lstm when the length of input traffic sequences increases .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "credal nets under epistemic irrelevance", "abstract": "we present a new approach to credal nets , which are graphical models that generalise bayesian nets to imprecise probability . instead of applying the commonly used notion of strong independence , we replace it by the weaker notion of epistemic irrelevance . we show how assessments of epistemic irrelevance allow us to construct a global model out of given local uncertainty models and mention some useful properties . the main results and proofs are presented using the language of sets of desirable gambles , which provides a very general and expressive way of representing imprecise probability models .", "topics": ["graphical model", "relevance"]}
{"title": "subsampled online matrix factorization with convergence guarantees", "abstract": "we present a matrix factorization algorithm that scales to input matrices that are large in both dimensions ( i.e . , that contains morethan 1tb of data ) . the algorithm streams the matrix columns while subsampling them , resulting in low complexity per iteration andreasonable memory footprint . in contrast to previous online matrix factorization methods , our approach relies on low-dimensional statistics from past iterates to control the extra variance introduced by subsampling . we present a convergence analysis that guarantees us to reach a stationary point of the problem . large speed-ups can be obtained compared to previous online algorithms that do not perform subsampling , thanks to the feature redundancy that often exists in high-dimensional settings .", "topics": ["optimization problem", "sparse matrix"]}
{"title": "paddle : proximal algorithm for dual dictionaries learning", "abstract": "recently , considerable research efforts have been devoted to the design of methods to learn from data overcomplete dictionaries for sparse coding . however , learned dictionaries require the solution of an optimization problem for coding new data . in order to overcome this drawback , we propose an algorithm aimed at learning both a dictionary and its dual : a linear mapping directly performing the coding . by leveraging on proximal methods , our algorithm jointly minimizes the reconstruction error of the dictionary and the coding error of its dual ; the sparsity of the representation is induced by an $ \\ell_1 $ -based penalty on its coefficients . the results obtained on synthetic data and real images show that the algorithm is capable of recovering the expected dictionaries . furthermore , on a benchmark dataset , we show that the image features obtained from the dual matrix yield state-of-the-art classification performance while being much less computational intensive .", "topics": ["optimization problem", "synthetic data"]}
{"title": "large scale variational inference and experimental design for sparse generalized linear models", "abstract": "many problems of low-level computer vision and image processing , such as denoising , deconvolution , tomographic reconstruction or super-resolution , can be addressed by maximizing the posterior distribution of a sparse linear model ( slm ) . we show how higher-order bayesian decision-making problems , such as optimizing image acquisition in magnetic resonance scanners , can be addressed by querying the slm posterior covariance , unrelated to the density 's mode . we propose a scalable algorithmic framework , with which slm posteriors over full , high-resolution images can be approximated for the first time , solving a variational optimization problem which is convex iff posterior mode finding is convex . these methods successfully drive the optimization of sampling trajectories for real-world magnetic resonance imaging through bayesian experimental design , which has not been attempted before . our methodology provides new insight into similarities and differences between sparse reconstruction and approximate bayesian inference , and has important implications for compressive sensing of real-world images .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "probabilistic feature selection and classification vector machine", "abstract": "sparse bayesian learning is one of the state-of- the-art machine learning algorithms , which is able to make stable and reliable probabilistic predictions . however , some of these algorithms , e.g . probabilistic classification vector machine ( pcvm ) and relevant vector machine ( rvm ) , are not capable of eliminating irrelevant and redundant features which could lead to performance degradation . to tackle this problem , in this paper , we propose a sparse bayesian classifier which simultaneously selects the relevant samples and features . we name this classifier a probabilistic feature selection and classification vector machine ( pfcvm ) , in which truncated gaussian distributions are em- ployed as both sample and feature priors . in order to derive the analytical solution for the proposed algorithm , we use laplace approximation to calculate approximate posteriors and marginal likelihoods . finally , we obtain the optimized parameters and hyperparameters by the type-ii maximum likelihood method . the experiments on synthetic data set , benchmark data sets and high dimensional data sets validate the performance of pfcvm under two criteria : accuracy of classification and efficacy of selected features . finally , we analyze the generalization performance of pfcvm and derive a generalization error bound for pfcvm . then by tightening the bound , we demonstrate the significance of the sparseness for the model .", "topics": ["approximation algorithm", "synthetic data"]}
{"title": "a short variational proof of equivalence between policy gradients and soft q learning", "abstract": "two main families of reinforcement learning algorithms , q-learning and policy gradients , have recently been proven to be equivalent when using a softmax relaxation on one part , and an entropic regularization on the other . we relate this result to the well-known convex duality of shannon entropy and the softmax function . such a result is also known as the donsker-varadhan formula . this provides a short proof of the equivalence . we then interpret this duality further , and use ideas of convex analysis to prove a new policy inequality relative to soft q-learning .", "topics": ["calculus of variations", "reinforcement learning"]}
{"title": "face destylization", "abstract": "numerous style transfer methods which produce artistic styles of portraits have been proposed to date . however , the inverse problem of converting the stylized portraits back into realistic faces is yet to be investigated thoroughly . reverting an artistic portrait to its original photo-realistic face image has potential to facilitate human perception and identity analysis . in this paper , we propose a novel face destylization neural network ( fdnn ) to restore the latent photo-realistic faces from the stylized ones . we develop a style removal network composed of convolutional , fully-connected and deconvolutional layers . the convolutional layers are designed to extract facial components from stylized face images . consecutively , the fully-connected layer transfers the extracted feature maps of stylized images into the corresponding feature maps of real faces and the deconvolutional layers generate real faces from the transferred feature maps . to enforce the destylized faces to be similar to authentic face images , we employ a discriminative network , which consists of convolutional and fully connected layers . we demonstrate the effectiveness of our network by conducting experiments on an extensive set of synthetic images . furthermore , we illustrate our network can recover faces from stylized portraits and real paintings for which the stylized data was unavailable during the training phase .", "topics": ["synthetic data", "map"]}
{"title": "ii-fcn for skin lesion analysis towards melanoma detection", "abstract": "dermoscopy image detection stays a tough task due to the weak distinguishable property of the object.although the deep convolution neural network signifigantly boosted the performance on prevelance computer vision tasks in recent years , there remains a room to explore more robust and precise models to the problem of low contrast image segmentation.towards the challenge of lesion segmentation in isbi 2017 , we built a symmetrical identity inception fully convolution network which is based on only 10 reversible inception blocks , every block composed of four convolution branches with combination of different layer depth and kernel size to extract sundry semantic features.then we proposed an approximate loss function for jaccard index metrics to train our model.to overcome the drawbacks of traditional convolution , we adopted the dilation convolution and conditional random field method to rectify our segmentation.we also introduced multiple ways to prevent the problem of overfitting.the experimental results shows that our model achived jaccard index of 0.82 and kept learning from epoch to epoch .", "topics": ["image segmentation", "computer vision"]}
{"title": "exploring person context and local scene context for object detection", "abstract": "in this paper we explore two ways of using context for object detection . the first model focusses on people and the objects they commonly interact with , such as fashion and sports accessories . the second model considers more general object detection and uses the spatial relationships between objects and between objects and scenes . our models are able to capture precise spatial relationships between the context and the object of interest , and make effective use of the appearance of the contextual region . on the newly released coco dataset , our models provide relative improvements of up to 5 % over cnn-based state-of-the-art detectors , with the gains concentrated on hard cases such as small objects ( 10 % relative improvement ) .", "topics": ["object detection"]}
{"title": "sentiment classification of food reviews", "abstract": "sentiment analysis of reviews is a popular task in natural language processing . in this work , the goal is to predict the score of food reviews on a scale of 1 to 5 with two recurrent neural networks that are carefully tuned . as for baseline , we train a simple rnn for classification . then we extend the baseline to gru . in addition , we present two different methods to deal with highly skewed data , which is a common problem for reviews . models are evaluated using accuracies .", "topics": ["baseline ( configuration management )", "natural language processing"]}
{"title": "pg-causality : identifying spatiotemporal causal pathways for air pollutants with urban big data", "abstract": "many countries are suffering from severe air pollution . understanding how different air pollutants accumulate and propagate is critical to making relevant public policies . in this paper , we use urban big data ( air quality data and meteorological data ) to identify the \\emph { spatiotemporal ( st ) causal pathways } for air pollutants . this problem is challenging because : ( 1 ) there are numerous noisy and low-pollution periods in the raw air quality data , which may lead to unreliable causality analysis , ( 2 ) for large-scale data in the st space , the computational complexity of constructing a causal structure is very high , and ( 3 ) the \\emph { st causal pathways } are complex due to the interactions of multiple pollutants and the influence of environmental factors . therefore , we present \\emph { p-causality } , a novel pattern-aided causality analysis approach that combines the strengths of \\emph { pattern mining } and \\emph { bayesian learning } to efficiently and faithfully identify the \\emph { st causal pathways } . first , \\emph { pattern mining } helps suppress the noise by capturing frequent evolving patterns ( feps ) of each monitoring sensor , and greatly reduce the complexity by selecting the pattern-matched sensors as `` causers '' . then , \\emph { bayesian learning } carefully encodes the local and st causal relations with a gaussian bayesian network ( gbn ) -based graphical model , which also integrates environmental influences to minimize biases in the final results . we evaluate our approach with three real-world data sets containing 982 air quality sensors , in three regions of china from 01-jun-2013 to 19-dec-2015 . results show that our approach outperforms the traditional causal structure learning methods in time efficiency , inference accuracy and interpretability .", "topics": ["graphical model", "data mining"]}
{"title": "efficient exploration with double uncertain value networks", "abstract": "this paper studies directed exploration for reinforcement learning agents by tracking uncertainty about the value of each available action . we identify two sources of uncertainty that are relevant for exploration . the first originates from limited data ( parametric uncertainty ) , while the second originates from the distribution of the returns ( return uncertainty ) . we identify methods to learn these distributions with deep neural networks , where we estimate parametric uncertainty with bayesian drop-out , while return uncertainty is propagated through the bellman equation as a gaussian distribution . then , we identify that both can be jointly estimated in one network , which we call the double uncertain value network . the policy is directly derived from the learned distributions based on thompson sampling . experimental results show that both types of uncertainty may vastly improve learning in domains with a strong exploration challenge .", "topics": ["sampling ( signal processing )", "value ( ethics )"]}
{"title": "sum of previous inpatient serum creatinine measurements predicts acute kidney injury in rehospitalized patients", "abstract": "acute kidney injury ( aki ) , the abrupt decline in kidney function due to temporary or permanent injury , is associated with increased mortality , morbidity , length of stay , and hospital cost . sometimes , simple interventions such as medication review or hydration can prevent aki . there is therefore interest in estimating risk of aki at hospitalization . to gain insight into this task , we employ multilayer perceptron ( mlp ) and recurrent neural networks ( rnns ) using serum creatinine ( scr ) as a lone feature . we explore different feature input structures , including variable-length look-backs and a nested formulation for rehospitalized patients with previous scr measurements . experimental results show that the simplest model , mlp processing the sum of scr , had best performance : auroc 0.92 and auprc 0.70 . such a simple model could be easily integrated into an ehr . preliminary results also suggest that inpatient data streams with missing outpatient measurements -- -common in the medical setting -- -might be best modeled with a tailored architecture .", "topics": ["recurrent neural network"]}
{"title": "generative adversarial network for abstractive text summarization", "abstract": "in this paper , we propose an adversarial process for abstractive text summarization , in which we simultaneously train a generative model g and a discriminative model d. in particular , we build the generator g as an agent of reinforcement learning , which takes the raw text as input and predicts the abstractive summarization . we also build a discriminator which attempts to distinguish the generated summary from the ground truth summary . extensive experiments demonstrate that our model achieves competitive rouge scores with the state-of-the-art methods on cnn/daily mail dataset . qualitatively , we show that our model is able to generate more abstractive , readable and diverse summaries .", "topics": ["generative model", "reinforcement learning"]}
{"title": "deep convolutional neural networks for microscopy-based point of care diagnostics", "abstract": "point of care diagnostics using microscopy and computer vision methods have been applied to a number of practical problems , and are particularly relevant to low-income , high disease burden areas . however , this is subject to the limitations in sensitivity and specificity of the computer vision methods used . in general , deep learning has recently revolutionised the field of computer vision , in some cases surpassing human performance for other object recognition tasks . in this paper , we evaluate the performance of deep convolutional neural networks on three different microscopy tasks : diagnosis of malaria in thick blood smears , tuberculosis in sputum samples , and intestinal parasite eggs in stool samples . in all cases accuracy is very high and substantially better than an alternative approach more representative of traditional medical imaging techniques .", "topics": ["computer vision"]}
{"title": "multi-output artificial neural network for storm surge prediction in north carolina", "abstract": "during hurricane seasons , emergency managers and other decision makers need accurate and `on-time ' information on potential storm surge impacts . fully dynamical computer models , such as the adcirc tide , storm surge , and wind-wave model take several hours to complete a forecast when configured at high spatial resolution . additionally , statically meaningful ensembles of high-resolution models ( needed for uncertainty estimation ) can not easily be computed in near real-time . this paper discusses an artificial neural network model for storm surge prediction in north carolina . the network model provides fast , real-time storm surge estimates at coastal locations in north carolina . the paper studies the performance of the neural network model vs. other models on synthetic and real hurricane data .", "topics": ["synthetic data"]}
{"title": "evolution of things", "abstract": "evolution is one of the major omnipresent powers in the universe that has been studied for about two centuries . recent scientific and technical developments make it possible to make the transition from passively understanding to actively mastering evolution . as of today , the only area where human experimenters can design and manipulate evolutionary processes in full is that of evolutionary computing , where evolutionary processes are carried out in a digital space , inside computers , in simulation . we argue that in the near future it will be possible to move evolutionary computing outside such imaginary spaces and make it physically embodied . in other words , we envision the `` evolution of things '' , rather than just the evolution of code , leading to a new field of embodied artificial evolution ( eae ) . the main objective of the present paper is to offer an umbrella term and vision in order to aid the development of this high potential research area . to this end , we introduce the notion of eae , discuss a few examples and applications , and elaborate on the expected benefits as well as the grand challenges this developing field will have to address .", "topics": ["simulation"]}
{"title": "gatekeeping algorithms with human ethical bias : the ethics of algorithms in archives , libraries and society", "abstract": "in the age of algorithms , i focus on the question of how to ensure algorithms that will take over many of our familiar archival and library tasks , will behave according to human ethical norms that have evolved over many years . i start by characterizing physical archives in the context of related institutions such as libraries and museums . in this setting i analyze how ethical principles , in particular about access to information , have been formalized and communicated in the form of ethical codes , or : codes of conducts . after that i describe two main developments : digitalization , in which physical aspects of the world are turned into digital data , and algorithmization , in which intelligent computer programs turn this data into predictions and decisions . both affect interactions that were once physical but now digital . in this new setting i survey and analyze the ethical aspects of algorithms and how they shape a vision on the future of archivists and librarians , in the form of algorithmic documentalists , or : codementalists . finally i outline a general research strategy , called intermeedium , to obtain algorithms that obey are human ethical values encoded in code of ethics .", "topics": ["value ( ethics )", "interaction"]}
{"title": "online gradient boosting", "abstract": "we extend the theory of boosting for regression problems to the online learning setting . generalizing from the batch setting for boosting , the notion of a weak learning algorithm is modeled as an online learning algorithm with linear loss functions that competes with a base class of regression functions , while a strong learning algorithm is an online learning algorithm with convex loss functions that competes with a larger class of regression functions . our main result is an online gradient boosting algorithm which converts a weak online learning algorithm into a strong one where the larger class of functions is the linear span of the base class . we also give a simpler boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the convex hull of the base class , and prove its optimality .", "topics": ["loss function", "gradient"]}
{"title": "robustness evaluation of two ccg , a pcfg and a link grammar parsers", "abstract": "robustness in a parser refers to an ability to deal with exceptional phenomena . a parser is robust if it deals with phenomena outside its normal range of inputs . this paper reports on a series of robustness evaluations of state-of-the-art parsers in which we concentrated on one aspect of robustness : its ability to parse sentences containing misspelled words . we propose two measures for robustness evaluation based on a comparison of a parser 's output for grammatical input sentences and their noisy counterparts . in this paper , we use these measures to compare the overall robustness of the four evaluated parsers , and we present an analysis of the decline in parser performance with increasing error levels . our results indicate that performance typically declines tens of percentage units when parsers are presented with texts containing misspellings . when it was tested on our purpose-built test set of 443 sentences , the best parser in the experiment ( c & c parser ) was able to return exactly the same parse tree for the grammatical and ungrammatical sentences for 60.8 % , 34.0 % and 14.9 % of the sentences with one , two or three misspelled words respectively .", "topics": ["test set", "parsing"]}
{"title": "font acknowledgment and character extraction of digital and scanned images", "abstract": "the font recognition and character extraction is of immense importance as these are many scenarios where data are in such a form , which can not be processed like in image form or as a hard copy . so the procedure developed in this paper is basically related to identifying the font ( times new roman , arial and comic sans ms ) and afterwards recovering the text using simple correlation based method where the binary templates are correlated to the input image text characters . all of this extraction is done in the presence of a little noise as images may have noisy patterns due to photocopying . the significance of this method exists in extraction of data from various monitoring ( surveillance ) camera footages or even more . the method is developed on matlab\\c { opyright } which takes input image and recovers text and font information from it in a text file .", "topics": ["image processing"]}
{"title": "learning mixtures of linear regressions with nearly optimal complexity", "abstract": "mixtures of linear regressions ( mlr ) is an important mixture model with many applications . in this model , each observation is generated from one of the several unknown linear regression components , where the identity of the generated component is also unknown . previous works either assume strong assumptions on the data distribution or have high complexity . this paper proposes a fixed parameter tractable algorithm for the problem under general conditions , which achieves global convergence and the sample complexity scales nearly linearly in the dimension . in particular , different from previous works that require the data to be from the standard gaussian , the algorithm allows the data from gaussians with different covariances . when the conditional number of the covariances and the number of components are fixed , the algorithm has nearly optimal sample complexity $ n = \\tilde { o } ( d ) $ as well as nearly optimal computational complexity $ \\tilde { o } ( nd ) $ , where $ d $ is the dimension of the data space . to the best of our knowledge , this approach provides the first such recovery guarantee for this general setting .", "topics": ["computational complexity theory"]}
{"title": "domain adaptation of recurrent neural networks for natural language understanding", "abstract": "the goal of this paper is to use multi-task learning to efficiently scale slot filling models for natural language understanding to handle multiple target tasks or domains . the key to scalability is reducing the amount of training data needed to learn a model for a new task . the proposed multi-task model delivers better performance with less data by leveraging patterns that it learns from the other tasks . the approach supports an open vocabulary , which allows the models to generalize to unseen words , which is particularly important when very little training data is used . a newly collected crowd-sourced data set , covering four different domains , is used to demonstrate the effectiveness of the domain adaptation and open vocabulary techniques .", "topics": ["test set", "recurrent neural network"]}
{"title": "end-to-end attention-based distant speech recognition with highway lstm", "abstract": "end-to-end attention-based models have been shown to be competitive alternatives to conventional dnn-hmm models in the speech recognition systems . in this paper , we extend existing end-to-end attention-based models that can be applied for distant speech recognition ( dsr ) task . specifically , we propose an end-to-end attention-based speech recognizer with multichannel input that performs sequence prediction directly at the character level . to gain a better performance , we also incorporate highway long short-term memory ( hlstm ) which outperforms previous models on ami distant speech recognition task .", "topics": ["speech recognition", "end-to-end principle"]}
{"title": "multiscale dictionary learning for estimating conditional distributions", "abstract": "nonparametric estimation of the conditional distribution of a response given high-dimensional features is a challenging problem . it is important to allow not only the mean but also the variance and shape of the response density to change flexibly with features , which are massive-dimensional . we propose a multiscale dictionary learning model , which expresses the conditional response density as a convex combination of dictionary densities , with the densities used and their weights dependent on the path through a tree decomposition of the feature space . a fast graph partitioning algorithm is applied to obtain the tree decomposition , with bayesian methods then used to adaptively prune and average over different sub-trees in a soft probabilistic manner . the algorithm scales efficiently to approximately one million features . state of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features .", "topics": ["feature vector", "dictionary"]}
{"title": "the metric-ff planning system : translating `` ignoring delete lists '' to numeric state variables", "abstract": "planning with numeric state variables has been a challenge for many years , and was a part of the 3rd international planning competition ( ipc-3 ) . currently one of the most popular and successful algorithmic techniques in strips planning is to guide search by a heuristic function , where the heuristic is based on relaxing the planning task by ignoring the delete lists of the available actions . we present a natural extension of `` ignoring delete lists '' to numeric state variables , preserving the relevant theoretical properties of the strips relaxation under the condition that the numeric task at hand is `` monotonic '' . we then identify a subset of the numeric ipc-3 competition language , `` linear tasks '' , where monotonicity can be achieved by pre-processing . based on that , we extend the algorithms used in the heuristic planning system ff to linear tasks . the resulting system metric-ff is , according to the ipc-3 results which we discuss , one of the two currently most efficient numeric planners .", "topics": ["heuristic"]}
{"title": "autoencoder trees", "abstract": "we discuss an autoencoder model in which the encoding and decoding functions are implemented by decision trees . we use the soft decision tree where internal nodes realize soft multivariate splits given by a gating function and the overall output is the average of all leaves weighted by the gating values on their path . the encoder tree takes the input and generates a lower dimensional representation in the leaves and the decoder tree takes this and reconstructs the original input . exploiting the continuity of the trees , autoencoder trees are trained with stochastic gradient descent . on handwritten digit and news data , we see that the autoencoder trees yield good reconstruction error compared to traditional autoencoder perceptrons . we also see that the autoencoder tree captures hierarchical representations at different granularities of the data on its different levels and the leaves capture the localities in the input space .", "topics": ["feature learning", "encoder"]}
{"title": "texture defect detection in gradient space", "abstract": "in this paper , we propose a machine vision algorithm for automatically detecting defects in patterned textures with the help of gradient space and its energy . experiments on real fabric images with defects show that the proposed method can be used for automatic detection of fabric defects in textile industries .", "topics": ["cluster analysis", "feature vector"]}
{"title": "multiple instance hyperspectral target characterization", "abstract": "in this paper , two methods for multiple instance target characterization , mi-smf and mi-ace , are presented . mi-smf and mi-ace estimate a discriminative target signature from imprecisely-labeled and mixed training data . in many applications , such as sub-pixel target detection in remotely-sensed hyperspectral imagery , accurate pixel-level labels on training data is often unavailable and infeasible to obtain . furthermore , since sub-pixel targets are smaller in size than the resolution of a single pixel , training data is comprised only of mixed data points ( in which target training points are mixtures of responses from both target and non-target classes ) . results show improved , consistent performance over existing multiple instance concept learning methods on several hyperspectral sub-pixel target detection problems .", "topics": ["test set", "pixel"]}
{"title": "foundations of a multi-way spectral clustering framework for hybrid linear modeling", "abstract": "the problem of hybrid linear modeling ( hlm ) is to model and segment data using a mixture of affine subspaces . different strategies have been proposed to solve this problem , however , rigorous analysis justifying their performance is missing . this paper suggests the theoretical spectral curvature clustering ( tscc ) algorithm for solving the hlm problem , and provides careful analysis to justify it . the tscc algorithm is practically a combination of govindu 's multi-way spectral clustering framework ( cvpr 2005 ) and ng et al . 's spectral clustering algorithm ( nips 2001 ) . the main result of this paper states that if the given data is sampled from a mixture of distributions concentrated around affine subspaces , then with high sampling probability the tscc algorithm segments well the different underlying clusters . the goodness of clustering depends on the within-cluster errors , the between-clusters interaction , and a tuning parameter applied by tscc . the proof also provides new insights for the analysis of ng et al . ( nips 2001 ) .", "topics": ["cluster analysis"]}
{"title": "deep learning applied to nlp", "abstract": "convolutional neural network ( cnns ) are typically associated with computer vision . cnns are responsible for major breakthroughs in image classification and are the core of most computer vision systems today . more recently cnns have been applied to problems in natural language processing and gotten some interesting results . in this paper , we will try to explain the basics of cnns , its different variations and how they have been applied to nlp .", "topics": ["natural language processing", "computer vision"]}
{"title": "consciousness is pattern recognition", "abstract": "this is a proof of the strong ai hypothesis , i.e . that machines can be conscious . it is a phenomenological proof that pattern-recognition and subjective consciousness are the same activity in different terms . therefore , it proves that essential subjective processes of consciousness are computable , and identifies significant traits and requirements of a conscious system . since husserl , many philosophers have accepted that consciousness consists of memories of logical connections between an ego and external objects . these connections are called `` intentions . '' pattern recognition systems are achievable technical artifacts . the proof links this respected introspective philosophical theory of consciousness with technical art . the proof therefore endorses the strong ai hypothesis and may therefore also enable a theoretically-grounded form of artificial intelligence called a `` synthetic intentionality , '' able to synthesize , generalize , select and repeat intentions . if the pattern recognition is reflexive , able to operate on the set of intentions , and flexible , with several methods of synthesizing intentions , an si may be a particularly strong form of ai . similarities and possible applications to several ai paradigms are discussed . the article then addresses some problems : the proof 's limitations , reflexive cognition , searles ' chinese room , and how an si could `` understand '' `` meanings '' and `` be creative . ''", "topics": ["synthetic data", "artificial intelligence"]}
{"title": "semantic projection : recovering human knowledge of multiple , distinct object features from word embeddings", "abstract": "the words of a language reflect the structure of the human mind , allowing us to transmit thoughts between individuals . however , language can represent only a subset of our rich and detailed cognitive architecture . here , we ask what kinds of common knowledge ( semantic memory ) are captured by word meanings ( lexical semantics ) . we examine a prominent computational model that represents words as vectors in a multidimensional space , such that proximity between word-vectors approximates semantic relatedness . because related words appear in similar contexts , such spaces - called `` word embeddings '' - can be learned from patterns of lexical co-occurrences in natural language . despite their popularity , a fundamental concern about word embeddings is that they appear to be semantically `` rigid '' : inter-word proximity captures only overall similarity , yet human judgments about object similarities are highly context-dependent and involve multiple , distinct semantic features . for example , dolphins and alligators appear similar in size , but differ in intelligence and aggressiveness . could such context-dependent relationships be recovered from word embeddings ? to address this issue , we introduce a powerful , domain-general solution : `` semantic projection '' of word-vectors onto lines that represent various object features , like size ( the line extending from the word `` small '' to `` big '' ) , intelligence ( from `` dumb '' to `` smart '' ) , or danger ( from `` safe '' to `` dangerous '' ) . this method , which is intuitively analogous to placing objects `` on a mental scale '' between two extremes , recovers human judgments across a range of object categories and properties . we thus show that word embeddings inherit a wealth of common knowledge from word co-occurrence statistics and can be flexibly manipulated to express context-dependent meanings .", "topics": ["natural language"]}
{"title": "voxelnet : end-to-end learning for point cloud based 3d object detection", "abstract": "accurate detection of objects in 3d point clouds is a central problem in many applications , such as autonomous navigation , housekeeping robots , and augmented/virtual reality . to interface a highly sparse lidar point cloud with a region proposal network ( rpn ) , most existing efforts have focused on hand-crafted feature representations , for example , a bird 's eye view projection . in this work , we remove the need of manual feature engineering for 3d point clouds and propose voxelnet , a generic 3d detection network that unifies feature extraction and bounding box prediction into a single stage , end-to-end trainable deep network . specifically , voxelnet divides a point cloud into equally spaced 3d voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding ( vfe ) layer . in this way , the point cloud is encoded as a descriptive volumetric representation , which is then connected to a rpn to generate detections . experiments on the kitti car detection benchmark show that voxelnet outperforms the state-of-the-art lidar based 3d detection methods by a large margin . furthermore , our network learns an effective discriminative representation of objects with various geometries , leading to encouraging results in 3d detection of pedestrians and cyclists , based on only lidar .", "topics": ["object detection", "feature extraction"]}
{"title": "anmm : ranking short answer texts with attention-based neural matching model", "abstract": "as an alternative to question answering methods based on feature engineering , deep learning approaches such as convolutional neural networks ( cnns ) and long short-term memory models ( lstms ) have recently been proposed for semantic matching of questions and answers . to achieve good results , however , these models have been combined with additional features such as word overlap or bm25 scores . without this combination , these models perform significantly worse than methods based on linguistic feature engineering . in this paper , we propose an attention based neural matching model for ranking short answer text . we adopt value-shared weighting scheme instead of position-shared weighting scheme for combining different matching signals and incorporate question term importance learning using question attention network . using the popular benchmark trec qa data , we show that the relatively simple anmm model can significantly outperform other neural network models that have been used for the question answering task , and is competitive with models that are combined with additional features . when anmm is combined with additional features , it outperforms all baselines .", "topics": ["baseline ( configuration management )"]}
{"title": "improving recall of in situ sequencing by self-learned features and a graphical model", "abstract": "image-based sequencing of mrna makes it possible to see where in a tissue sample a given gene is active , and thus discern large numbers of different cell types in parallel . this is crucial for gaining a better understanding of tissue development and disease such as cancer . signals are collected over multiple staining and imaging cycles , and signal density together with noise makes signal decoding challenging . previous approaches have led to low signal recall in efforts to maintain high sensitivity . we propose an approach where signal candidates are generously included , and true-signal probability at the cycle level is self-learned using a convolutional neural network . signal candidates and probability predictions are thereafter fed into a graphical model searching for signal candidates across sequencing cycles . the graphical model combines intensity , probability and spatial distance to find optimal paths representing decoded signal sequences . we evaluate our approach in relation to state-of-the-art , and show that we increase recall by $ 27\\ % $ at maintained sensitivity . furthermore , visual examination shows that most of the now correctly resolved signals were previously lost due to high signal density . thus , the proposed approach has the potential to significantly improve further analysis of spatial statistics in in situ sequencing experiments .", "topics": ["graphical model"]}
{"title": "multi-space variational encoder-decoders for semi-supervised labeled sequence transduction", "abstract": "labeled sequence transduction is a task of transforming one sequence into another sequence that satisfies desiderata specified by a set of labels . in this paper we propose multi-space variational encoder-decoders , a new model for labeled sequence transduction with semi-supervised learning . the generative model can use neural networks to handle both discrete and continuous latent variables to exploit various features of data . experiments show that our model provides not only a powerful supervised framework but also can effectively take advantage of the unlabeled data . on the sigmorphon morphological inflection benchmark , our model outperforms single-model state-of-art results by a large margin for the majority of languages .", "topics": ["generative model", "supervised learning"]}
{"title": "improving ecological niche models by data mining large environmental datasets for surrogate models", "abstract": "whywhere is a new ecological niche modeling ( enm ) algorithm for mapping and explaining the distribution of species . the algorithm uses image processing methods to efficiently sift through large amounts of data to find the few variables that best predict species occurrence . the purpose of this paper is to describe and justify the main parameterizations and to show preliminary success at rapidly providing accurate , scalable , and simple enms . preliminary results for 6 species of plants and animals in different regions indicate a significant ( p < 0.01 ) 14 % increase in accuracy over the garp algorithm using models with few , typically two , variables . the increase is attributed to access to additional data , particularly monthly vs. annual climate averages . whywhere is also 6 times faster than garp on large data sets . a data mining based approach with transparent access to remote data archives is a new paradigm for enm , particularly suited to finding correlates in large databases of fine resolution surfaces . software for whywhere is freely available , both as a service and in a desktop downloadable form from the web site http : //biodi.sdsc.edu/ww_home.html .", "topics": ["data mining", "image processing"]}
{"title": "the advantage of doubling : a deep reinforcement learning approach to studying the double team in the nba", "abstract": "during the 2017 nba playoffs , celtics coach brad stevens was faced with a difficult decision when defending against the cavaliers : `` do you double and risk giving up easy shots , or stay at home and do the best you can ? '' it 's a tough call , but finding a good defensive strategy that effectively incorporates doubling can make all the difference in the nba . in this paper , we analyze double teaming in the nba , quantifying the trade-off between risk and reward . using player trajectory data pertaining to over 643,000 possessions , we identified when the ball handler was double teamed . given these data and the corresponding outcome ( i.e . , was the defense successful ) , we used deep reinforcement learning to estimate the quality of the defensive actions . we present qualitative and quantitative results summarizing our learned defensive strategy for defending . we show that our policy value estimates are predictive of points per possession and win percentage . overall , the proposed framework represents a step toward a more comprehensive understanding of defensive strategies in the nba .", "topics": ["reinforcement learning"]}
{"title": "fast cross-validation for incremental learning", "abstract": "cross-validation ( cv ) is one of the main tools for performance estimation and parameter tuning in machine learning . the general recipe for computing cv estimate is to run a learning algorithm separately for each cv fold , a computationally expensive process . in this paper , we propose a new approach to reduce the computational burden of cv-based performance estimation . as opposed to all previous attempts , which are specific to a particular learning model or problem domain , we propose a general method applicable to a large class of incremental learning algorithms , which are uniquely fitted to big data problems . in particular , our method applies to a wide range of supervised and unsupervised learning tasks with different performance criteria , as long as the base learning algorithm is incremental . we show that the running time of the algorithm scales logarithmically , rather than linearly , in the number of cv folds . furthermore , the algorithm has favorable properties for parallel and distributed implementation . experiments with state-of-the-art incremental learning algorithms confirm the practicality of the proposed method .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "a comparative study of counterfactual estimators", "abstract": "we provide a comparative study of several widely used off-policy estimators ( empirical average , basic importance sampling and normalized importance sampling ) , detailing the different regimes where they are individually suboptimal . we then exhibit properties optimal estimators should possess . in the case where examples have been gathered using multiple policies , we show that fused estimators dominate basic ones but can still be improved .", "topics": ["sampling ( signal processing )"]}
{"title": "euretile d7.3 - dynamic dal benchmark coding , measurements on mpi version of dpsnn-stdp ( distributed plastic spiking neural net ) and improvements to other dal codes", "abstract": "the euretile project required the selection and coding of a set of dedicated benchmarks . the project is about the software and hardware architecture of future many-tile distributed fault-tolerant systems . we focus on dynamic workloads characterised by heavy numerical processing requirements . the ambition is to identify common techniques that could be applied to both the embedded systems and hpc domains . this document is the first public deliverable of work package 7 : challenging tiled applications .", "topics": ["numerical analysis"]}
{"title": "3dof pedestrian trajectory prediction learned from long-term autonomous mobile robot deployment data", "abstract": "this paper presents a novel 3dof pedestrian trajectory prediction approach for autonomous mobile service robots . while most previously reported methods are based on learning of 2d positions in monocular camera images , our approach uses range-finder sensors to learn and predict 3dof pose trajectories ( i.e . 2d position plus 1d rotation within the world coordinate system ) . our approach , t-pose-lstm ( temporal 3dof-pose long-short-term memory ) , is trained using long-term data from real-world robot deployments and aims to learn context-dependent ( environment- and time-specific ) human activities . our approach incorporates long-term temporal information ( i.e . date and time ) with short-term pose observations as input . a sequence-to-sequence lstm encoder-decoder is trained , which encodes observations into lstm and then decodes as predictions . for deployment , it can perform on-the-fly prediction in real-time . instead of using manually annotated data , we rely on a robust human detection , tracking and slam system , providing us with examples in a global coordinate system . we validate the approach using more than 15k pedestrian trajectories recorded in a care home environment over a period of three months . the experiment shows that the proposed t-pose-lstm model advances the state-of-the-art 2d-based method for human trajectory prediction in long-term mobile robot deployments .", "topics": ["sensor", "autonomous car"]}
{"title": "data granulation by the principles of uncertainty", "abstract": "researches in granular modeling produced a variety of mathematical models , such as intervals , ( higher-order ) fuzzy sets , rough sets , and shadowed sets , which are all suitable to characterize the so-called information granules . modeling of the input data uncertainty is recognized as a crucial aspect in information granulation . moreover , the uncertainty is a well-studied concept in many mathematical settings , such as those of probability theory , fuzzy set theory , and possibility theory . this fact suggests that an appropriate quantification of the uncertainty expressed by the information granule model could be used to define an invariant property , to be exploited in practical situations of information granulation . in this perspective , a procedure of information granulation is effective if the uncertainty conveyed by the synthesized information granule is in a monotonically increasing relation with the uncertainty of the input data . in this paper , we present a data granulation framework that elaborates over the principles of uncertainty introduced by klir . being the uncertainty a mesoscopic descriptor of systems and data , it is possible to apply such principles regardless of the input data type and the specific mathematical setting adopted for the information granules . the proposed framework is conceived ( i ) to offer a guideline for the synthesis of information granules and ( ii ) to build a groundwork to compare and quantitatively judge over different data granulation procedures . to provide a suitable case study , we introduce a new data granulation technique based on the minimum sum of distances , which is designed to generate type-2 fuzzy sets . we analyze the procedure by performing different experiments on two distinct data types : feature vectors and labeled graphs . results show that the uncertainty of the input data is suitably conveyed by the generated type-2 fuzzy set models .", "topics": ["feature vector"]}
{"title": "shadows and headless shadows : an autobiographical approach to narrative reasoning", "abstract": "the xapagy architecture is a story-oriented cognitive system which relies exclusively on the autobiographical memory implemented as a raw collection of events . reasoning is performed by shadowing current events with events from the autobiography . the shadows are then extrapolated into headless shadows ( hlss ) . in a story following mood , hlss can be used to track the level of surprise of the agent , to infer hidden actions or relations between the participants , and to summarize ongoing events . in recall mood , the hlss can be used to create new stories ranging from exact recall to free-form confabulation .", "topics": ["artificial intelligence"]}
{"title": "improving ocr accuracy on early printed books by utilizing cross fold training and voting", "abstract": "in this paper we introduce a method that significantly reduces the character error rates for ocr text obtained from ocropus models trained on early printed books . the method uses a combination of cross fold training and confidence based voting . after allocating the available ground truth in different subsets several training processes are performed , each resulting in a specific ocr model . the ocr text generated by these models then gets voted to determine the final output by taking the recognized characters , their alternatives , and the confidence values assigned to each character into consideration . experiments on seven early printed books show that the proposed method outperforms the standard approach considerably by reducing the amount of errors by up to 50 % and more .", "topics": ["value ( ethics )", "ground truth"]}
{"title": "normal bandits of unknown means and variances : asymptotic optimality , finite horizon regret bounds , and a solution to an open problem", "abstract": "consider the problem of sampling sequentially from a finite number of $ n \\geq 2 $ populations , specified by random variables $ x^i_k $ , $ i = 1 , \\ldots , n , $ and $ k = 1 , 2 , \\ldots $ ; where $ x^i_k $ denotes the outcome from population $ i $ the $ k^ { th } $ time it is sampled . it is assumed that for each fixed $ i $ , $ \\ { x^i_k \\ } _ { k \\geq 1 } $ is a sequence of i.i.d . normal random variables , with unknown mean $ \\mu_i $ and unknown variance $ \\sigma_i^2 $ . the objective is to have a policy $ \\pi $ for deciding from which of the $ n $ populations to sample form at any time $ n=1,2 , \\ldots $ so as to maximize the expected sum of outcomes of $ n $ samples or equivalently to minimize the regret due to lack on information of the parameters $ \\mu_i $ and $ \\sigma_i^2 $ . in this paper , we present a simple inflated sample mean ( ism ) index policy that is asymptotically optimal in the sense of theorem 4 below . this resolves a standing open problem from burnetas and katehakis ( 1996 ) . additionally , finite horizon regret bounds are given .", "topics": ["sampling ( signal processing )", "regret ( decision theory )"]}
{"title": "fine-grained decision-theoretic search control", "abstract": "decision-theoretic control of search has previously used as its basic unit . of computation the generation and evaluation of a complete set of successors . although this simplifies analysis , it results in some lost opportunities for pruning and satisficing . this paper therefore extends the analysis of the value of computation to cover individual successor evaluations . the analytic techniques used may prove useful for control of reasoning in more general settings . a formula is developed for the expected value of a node , k of whose n successors have been evaluated . this formula is used to estimate the value of expanding further successors , using a general formula for the value of a computation in game-playing developed in earlier work . we exhibit an improved version of the mgss* algorithm , giving empirical results for the game of othello .", "topics": ["computation"]}
{"title": "dimensionality detection and integration of multiple data sources via the gp-lvm", "abstract": "the gaussian process latent variable model ( gp-lvm ) is a non-linear probabilistic method of embedding a high dimensional dataset in terms low dimensional `latent ' variables . in this paper we illustrate that maximum a posteriori ( map ) estimation of the latent variables and hyperparameters can be used for model selection and hence we can determine the optimal number or latent variables and the most appropriate model . this is an alternative to the variational approaches developed recently and may be useful when we want to use a non-gaussian prior or kernel functions that do n't have automatic relevance determination ( ard ) parameters . using a second order expansion of the latent variable posterior we can marginalise the latent variables and obtain an estimate for the hyperparameter posterior . secondly , we use the gp-lvm to integrate multiple data sources by simultaneously embedding them in terms of common latent variables . we present results from synthetic data to illustrate the successful detection and retrieval of low dimensional structure from high dimensional data . we demonstrate that the integration of multiple data sources leads to more robust performance . finally , we show that when the data are used for binary classification tasks we can attain a significant gain in prediction accuracy when the low dimensional representation is used .", "topics": ["calculus of variations", "nonlinear system"]}
{"title": "online planning algorithms for pomdps", "abstract": "partially observable markov decision processes ( pomdps ) provide a rich framework for sequential decision-making under uncertainty in stochastic domains . however , solving a pomdp is often intractable except for small problems due to their complexity . here , we focus on online approaches that alleviate the computational complexity by computing good local policies at each decision step during the execution . online algorithms generally consist of a lookahead search to find the best action to execute at each time step in an environment . our objectives here are to survey the various existing online pomdp methods , analyze their properties and discuss their advantages and disadvantages ; and to thoroughly evaluate these online approaches in different environments under various metrics ( return , error bound reduction , lower bound improvement ) . our experimental results indicate that state-of-the-art online heuristic search methods can handle large pomdp domains efficiently .", "topics": ["computational complexity theory", "heuristic"]}
{"title": "artificial neural networks for detection of malaria in rbcs", "abstract": "malaria is one of the most common diseases caused by mosquitoes and is a great public health problem worldwide . currently , for malaria diagnosis the standard technique is microscopic examination of a stained blood film . we propose use of artificial neural networks ( ann ) for the diagnosis of the disease in the red blood cell . for this purpose features / parameters are computed from the data obtained by the digital holographic images of the blood cells and is given as input to ann which classifies the cell as the infected one or otherwise .", "topics": ["neural networks"]}
{"title": "the role of dimensionality reduction in linear classification", "abstract": "dimensionality reduction ( dr ) is often used as a preprocessing step in classification , but usually one first fixes the dr mapping , possibly using label information , and then learns a classifier ( a filter approach ) . best performance would be obtained by optimizing the classification error jointly over dr mapping and classifier ( a wrapper approach ) , but this is a difficult nonconvex problem , particularly with nonlinear dr . using the method of auxiliary coordinates , we give a simple , efficient algorithm to train a combination of nonlinear dr and a classifier , and apply it to a rbf mapping with a linear svm . this alternates steps where we train the rbf mapping and a linear svm as usual regression and classification , respectively , with a closed-form step that coordinates both . the resulting nonlinear low-dimensional classifier achieves classification errors competitive with the state-of-the-art but is fast at training and testing , and allows the user to trade off runtime for classification accuracy easily . we then study the role of nonlinear dr in linear classification , and the interplay between the dr mapping , the number of latent dimensions and the number of classes . when trained jointly , the dr mapping takes an extreme role in eliminating variation : it tends to collapse classes in latent space , erasing all manifold structure , and lay out class centroids so they are linearly separable with maximum margin .", "topics": ["nonlinear system"]}
{"title": "improved descriptors for patch matching and reconstruction", "abstract": "we propose a convolutional neural network ( convnet ) based approach for learning local image descriptors which can be used for significantly improved patch matching and 3d reconstructions . a multi-resolution convnet is used for learning keypoint descriptors . we also propose a new dataset consisting of an order of magnitude more number of scenes , images , and positive and negative correspondences compared to the currently available multi-view stereo ( mvs ) [ 18 ] dataset . the new dataset also has better coverage of the overall viewpoint , scale , and lighting changes in comparison to the mvs dataset . we evaluate our approach on publicly available datasets , such as oxford affine covariant regions dataset ( acrd ) [ 12 ] , mvs [ 18 ] , synthetic [ 6 ] and strecha [ 15 ] datasets to quantify the image descriptor performance . scenes from the oxford acrd , mvs and synthetic datasets are used for evaluating the patch matching performance of the learnt descriptors while the strecha dataset is used to evaluate the 3d reconstruction task . experiments show that the proposed descriptor outperforms the current state-of-the-art descriptors in both the evaluation tasks .", "topics": ["synthetic data"]}
{"title": "an attentive sequence model for adverse drug event extraction from biomedical text", "abstract": "adverse reaction caused by drugs is a potentially dangerous problem which may lead to mortality and morbidity in patients . adverse drug event ( ade ) extraction is a significant problem in biomedical research . we model ade extraction as a question-answering problem and take inspiration from machine reading comprehension ( mrc ) literature , to design our model . our objective in designing such a model , is to exploit the local linguistic context in clinical text and enable intra-sequence interaction , in order to jointly learn to classify drug and disease entities , and to extract adverse reactions caused by a given drug . our model makes use of a self-attention mechanism to facilitate intra-sequence interaction in a text sequence . this enables us to visualize and understand how the network makes use of the local and wider context for classification .", "topics": ["entity", "causality"]}
{"title": "integrating tabu search and vlsn search to develop enhanced algorithms : a case study using bipartite boolean quadratic programs", "abstract": "the bipartite boolean quadratic programming problem ( bbqp ) is a generalization of the well studied boolean quadratic programming problem . the model has a variety of real life applications ; however , empirical studies of the model are not available in the literature , except in a few isolated instances . in this paper , we develop efficient heuristic algorithms based on tabu search , very large scale neighborhood ( vlsn ) search , and a hybrid algorithm that integrates the two . the computational study establishes that effective integration of simple tabu search with vlsn search results in superior outcomes , and suggests the value of such an integration in other settings . complexity analysis and implementation details are provided along with conclusions drawn from experimental analysis . in addition , we obtain solutions better than the best previously known for almost all medium and large size benchmark instances .", "topics": ["numerical analysis", "heuristic"]}
{"title": "artificial intelligence in reverse supply chain management : the state of the art", "abstract": "product take-back legislation forces manufacturers to bear the costs of collection and disposal of products that have reached the end of their useful lives . in order to reduce these costs , manufacturers can consider reuse , remanufacturing and/or recycling of components as an alternative to disposal . the implementation of such alternatives usually requires an appropriate reverse supply chain management . with the concepts of reverse supply chain are gaining popularity in practice , the use of artificial intelligence approaches in these areas is also becoming popular . as a result , the purpose of this paper is to give an overview of the recent publications concerning the application of artificial intelligence techniques to reverse supply chain with emphasis on certain types of product returns .", "topics": ["artificial intelligence"]}
{"title": "a paraboost stereoscopic image quality assessment ( pbsiqa ) system", "abstract": "the problem of stereoscopic image quality assessment , which finds applications in 3d visual content delivery such as 3dtv , is investigated in this work . specifically , we propose a new paraboost ( parallel-boosting ) stereoscopic image quality assessment ( pbsiqa ) system . the system consists of two stages . in the first stage , various distortions are classified into a few types , and individual quality scorers targeting at a specific distortion type are developed . these scorers offer complementary performance in face of a database consisting of heterogeneous distortion types . in the second stage , scores from multiple quality scorers are fused to achieve the best overall performance , where the fuser is designed based on the parallel boosting idea borrowed from machine learning . extensive experimental results are conducted to compare the performance of the proposed pbsiqa system with those of existing stereo image quality assessment ( siqa ) metrics . the developed quality metric can serve as an objective function to optimize the performance of a 3d content delivery system .", "topics": ["loss function"]}
{"title": "performance of stanford and minipar parser on biomedical texts", "abstract": "in this paper , the performance of two dependency parsers , namely stanford and minipar , on biomedical texts has been reported . the performance of te parsers to assignm dependencies between two biomedical concepts that are already proved to be connected is not satisfying . both stanford and minipar , being statistical parsers , fail to assign dependency relation between two connected concepts if they are distant by at least one clause . minipar 's performance , in terms of precision , recall and the f-score of the attachment score ( e.g . , correctly identified head in a dependency ) , to parse biomedical text is also measured taking the stanford 's as a gold standard . the results suggest that minipar is not suitable yet to parse biomedical texts . in addition , a qualitative investigation reveals that the difference between working principles of the parsers also play a vital role for minipar 's degraded performance .", "topics": ["parsing"]}
{"title": "dynamic oracle for neural machine translation in decoding phase", "abstract": "the past several years have witnessed the rapid progress of end-to-end neural machine translation ( nmt ) . however , there exists discrepancy between training and inference in nmt when decoding , which may lead to serious problems since the model might be in a part of the state space it has never seen during training . to address the issue , scheduled sampling has been proposed . however , there are certain limitations in scheduled sampling and we propose two dynamic oracle-based methods to improve it . we manage to mitigate the discrepancy by changing the training process towards a less guided scheme and meanwhile aggregating the oracle 's demonstrations . experimental results show that the proposed approaches improve translation quality over standard nmt system .", "topics": ["machine translation", "end-to-end principle"]}
{"title": "generalized fast approximate energy minimization via graph cuts : alpha-expansion beta-shrink moves", "abstract": "we present alpha-expansion beta-shrink moves , a simple generalization of the widely-used alpha-beta swap and alpha-expansion algorithms for approximate energy minimization . we show that in a certain sense , these moves dominate both alpha-beta-swap and alpha-expansion moves , but unlike previous generalizations the new moves require no additional assumptions and are still solvable in polynomial-time . we show promising experimental results with the new moves , which we believe could be used in any context where alpha-expansions are currently employed .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "ct sinogram-consistency learning for metal-induced beam hardening correction", "abstract": "this paper proposes a sinogram consistency learning method to deal with beam-hardening related artifacts in polychromatic computerized tomography ( ct ) . the presence of highly attenuating materials in the scan field causes an inconsistent sinogram , that does not match the range space of the radon transform . when the mismatched data are entered into the range space during ct reconstruction , streaking and shading artifacts are generated owing to the inherent nature of the inverse radon transform . the proposed learning method aims to repair inconsistent sinograms by removing the primary metal-induced beam-hardening factors along the metal trace in the sinogram . taking account of the fundamental difficulty in obtaining sufficient training data in a medical environment , the learning method is designed to use simulated training data and a patient-type specific learning model is used to simplify the learning process . the feasibility of the proposed method is investigated using a dataset , consisting of real ct scan of pelvises containing hip prostheses . the anatomical areas in training and test data are different , in order to demonstrate that the proposed method extracts the beam hardening features , selectively . the results show that our method successfully corrects sinogram inconsistency by extracting beam-hardening sources by means of deep learning . this paper proposed a deep learning method of sinogram correction for beam hardening reduction in ct for the first time . conventional methods for beam hardening reduction are based on regularizations , and have the fundamental drawback of being not easily able to use manifold ct images , while a deep learning approach has the potential to do so .", "topics": ["test set", "simulation"]}
{"title": "recurrent neural networks for online video popularity prediction", "abstract": "in this paper , we address the problem of popularity prediction of online videos shared in social media . we prove that this challenging task can be approached using recently proposed deep neural network architectures . we cast the popularity prediction problem as a classification task and we aim to solve it using only visual cues extracted from videos . to that end , we propose a new method based on a long-term recurrent convolutional network ( lrcn ) that incorporates the sequentiality of the information in the model . results obtained on a dataset of over 37'000 videos published on facebook show that using our method leads to over 30 % improvement in prediction performance over the traditional shallow approaches and can provide valuable insights for content creators .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "block-sparse recurrent neural networks", "abstract": "recurrent neural networks ( rnns ) are used in state-of-the-art models in domains such as speech recognition , machine translation , and language modelling . sparsity is a technique to reduce compute and memory requirements of deep learning models . sparse rnns are easier to deploy on devices and high-end server processors . even though sparse operations need less compute and memory relative to their dense counterparts , the speed-up observed by using sparse operations is less than expected on different hardware platforms . in order to address this issue , we investigate two different approaches to induce block sparsity in rnns : pruning blocks of weights in a layer and using group lasso regularization to create blocks of weights with zeros . using these techniques , we demonstrate that we can create block-sparse rnns with sparsity ranging from 80 % to 90 % with small loss in accuracy . this allows us to reduce the model size by roughly 10x . additionally , we can prune a larger dense network to recover this loss in accuracy while maintaining high block sparsity and reducing the overall parameter count . our technique works with a variety of block sizes up to 32x32 . block-sparse rnns eliminate overheads related to data storage and irregular memory accesses while increasing hardware efficiency compared to unstructured sparsity .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "inar : inverse augmented reality", "abstract": "augmented reality is the art to seamlessly fuse virtual objects into real ones . in this short note , we address the opposite problem , the inverse augmented reality , that is , given a perfectly augmented reality scene where human is unable to distinguish real objects from virtual ones , how the machine could help do the job . we show by structure from motion ( sfm ) , a simple 3d reconstruction technique from images in computer vision , the real and virtual objects can be easily separated in the reconstructed 3d scene .", "topics": ["computer vision"]}
{"title": "mufold-ss : protein secondary structure prediction using deep inception-inside-inception networks", "abstract": "motivation : protein secondary structure prediction can provide important information for protein 3d structure prediction and protein functions . deep learning , which has been successfully applied to various research fields such as image classification and voice recognition , provides a new opportunity to significantly improve the secondary structure prediction accuracy . although several deep-learning methods have been developed for secondary structure prediction , there is room for improvement . mufold-ss was developed to address these issues . results : here , a very deep neural network , the deep inception-inside-inception networks ( deep3i ) , is proposed for protein secondary structure prediction and a software tool was implemented using this network . this network takes two inputs : a protein sequence and a profile generated by psi-blast . the output is the predicted eight states ( q8 ) or three states ( q3 ) of secondary structures . the proposed deep3i not only achieves the state-of-the-art performance but also runs faster than other tools . deep3i achieves q3 82.8 % and q8 71.1 % accuracies on the cb513 benchmark .", "topics": ["computer vision", "speech recognition"]}
{"title": "perishability of data : dynamic pricing under varying-coefficient models", "abstract": "we consider a firm that sells a large number of products to its customers in an online fashion . each product is described by a high dimensional feature vector , and the market value of a product is assumed to be linear in the values of its features . parameters of the valuation model are unknown and can change over time . the firm sequentially observes a product 's features and can use the historical sales data ( binary sale/no sale feedbacks ) to set the price of current product , with the objective of maximizing the collected revenue . we measure the performance of a dynamic pricing policy via regret , which is the expected revenue loss compared to a clairvoyant that knows the sequence of model parameters in advance . we propose a pricing policy based on projected stochastic gradient descent ( psgd ) and characterize its regret in terms of time $ t $ , features dimension $ d $ , and the temporal variability in the model parameters , $ \\delta_t $ . we consider two settings . in the first one , feature vectors are chosen antagonistically by nature and we prove that the regret of psgd pricing policy is of order $ o ( \\sqrt { t } + \\sum_ { t=1 } ^t \\sqrt { t } \\delta_t ) $ . in the second setting ( referred to as stochastic features model ) , the feature vectors are drawn independently from an unknown distribution . we show that in this case , the regret of psgd pricing policy is of order $ o ( d^2 \\log t + \\sum_ { t=1 } ^t t\\delta_t/d ) $ .", "topics": ["regret ( decision theory )", "value ( ethics )"]}
{"title": "egoreid : cross-view self-identification and human re-identification in egocentric and surveillance videos", "abstract": "human identification remains to be one of the challenging tasks in computer vision community due to drastic changes in visual features across different viewpoints , lighting conditions , occlusion , etc . most of the literature has been focused on exploring human re-identification across viewpoints that are not too drastically different in nature . cameras usually capture oblique or side views of humans , leaving room for a lot of geometric and visual reasoning . given the recent popularity of egocentric and top-view vision , re-identification across these two drastically different views can now be explored . having an egocentric and a top view video , our goal is to identify the cameraman in the content of the top-view video , and also re-identify the people visible in the egocentric video , by matching them to the identities present in the top-view video . we propose a crf-based method to address the two problems . our experimental results demonstrates the efficiency of the proposed approach over a variety of video recorded from two views .", "topics": ["computer vision"]}
{"title": "efficient online bootstrapping for large scale learning", "abstract": "bootstrapping is a useful technique for estimating the uncertainty of a predictor , for example , confidence intervals for prediction . it is typically used on small to moderate sized datasets , due to its high computation cost . this work describes a highly scalable online bootstrapping strategy , implemented inside vowpal wabbit , that is several times faster than traditional strategies . our experiments indicate that , in addition to providing a black box-like method for estimating uncertainty , our implementation of online bootstrapping may also help to train models with better prediction performance due to model averaging .", "topics": ["computation", "scalability"]}
{"title": "deep co-space : sample mining across feature transformation for semi-supervised learning", "abstract": "aiming at improving performance of visual classification in a cost-effective manner , this paper proposes an incremental semi-supervised learning paradigm called deep co-space ( dcs ) . unlike many conventional semi-supervised learning methods usually performing within a fixed feature space , our dcs gradually propagates information from labeled samples to unlabeled ones along with deep feature learning . we regard deep feature learning as a series of steps pursuing feature transformation , i.e . , projecting the samples from a previous space into a new one , which tends to select the reliable unlabeled samples with respect to this setting . specifically , for each unlabeled image instance , we measure its reliability by calculating the category variations of feature transformation from two different neighborhood variation perspectives , and merged them into an unified sample mining criterion deriving from hellinger distance . then , those samples keeping stable correlation to their neighboring samples ( i.e . , having small category variation in distribution ) across the successive feature space transformation , are automatically received labels and incorporated into the model for incrementally training in terms of classification . our extensive experiments on standard image classification benchmarks ( e.g . , caltech-256 and sun-397 ) demonstrate that the proposed framework is capable of effectively mining from large-scale unlabeled images , which boosts image classification performance and achieves promising results compared to other semi-supervised learning methods .", "topics": ["feature learning", "supervised learning"]}
{"title": "heterogeneous tensor decomposition for clustering via manifold optimization", "abstract": "tensors or multiarray data are generalizations of matrices . tensor clustering has become a very important research topic due to the intrinsically rich structures in real-world multiarray datasets . subspace clustering based on vectorizing multiarray data has been extensively researched . however , vectorization of tensorial data does not exploit complete structure information . in this paper , we propose a subspace clustering algorithm without adopting any vectorization process . our approach is based on a novel heterogeneous tucker decomposition model . in contrast to existing techniques , we propose a new clustering algorithm that alternates between different modes of the proposed heterogeneous tensor model . all but the last mode have closed-form updates . updating the last mode reduces to optimizing over the so-called multinomial manifold , for which we investigate second order riemannian geometry and propose a trust-region algorithm . numerical experiments show that our proposed algorithm compete effectively with state-of-the-art clustering algorithms that are based on tensor factorization .", "topics": ["cluster analysis"]}
{"title": "a multi-modal approach to infer image affect", "abstract": "the group affect or emotion in an image of people can be inferred by extracting features about both the people in the picture and the overall makeup of the scene . the state-of-the-art on this problem investigates a combination of facial features , scene extraction and even audio tonality . this paper combines three additional modalities , namely , human pose , text-based tagging and cnn extracted features / predictions . to the best of our knowledge , this is the first time all of the modalities were extracted using deep neural networks . we evaluate the performance of our approach against baselines and identify insights throughout this paper .", "topics": ["baseline ( configuration management )"]}
{"title": "struc2vec : learning node representations from structural identity", "abstract": "structural identity is a concept of symmetry in which network nodes are identified according to the network structure and their relationship to other nodes . structural identity has been studied in theory and practice over the past decades , but only recently has it been addressed with representational learning techniques . this work presents struc2vec , a novel and flexible framework for learning latent representations for the structural identity of nodes . struc2vec uses a hierarchy to measure node similarity at different scales , and constructs a multilayer graph to encode structural similarities and generate structural context for nodes . numerical experiments indicate that state-of-the-art techniques for learning node representations fail in capturing stronger notions of structural identity , while struc2vec exhibits much superior performance in this task , as it overcomes limitations of prior approaches . as a consequence , numerical experiments indicate that struc2vec improves performance on classification tasks that depend more on structural identity .", "topics": ["numerical analysis"]}
{"title": "automatic differentiation for tensor algebras", "abstract": "kjolstad et . al . proposed a tensor algebra compiler . it takes expressions that define a tensor element-wise , such as $ f_ { ij } ( a , b , c , d ) = \\exp\\left [ -\\sum_ { k=0 } ^4 \\left ( ( a_ { ik } +b_ { jk } ) ^2\\ , c_ { ii } + d_ { i+k } ^3 \\right ) \\right ] $ , and generates the corresponding compute kernel code . for machine learning , especially deep learning , it is often necessary to compute the gradient of a loss function $ l ( a , b , c , d ) =l ( f ( a , b , c , d ) ) $ with respect to parameters $ a , b , c , d $ . if tensor compilers are to be applied in this field , it is necessary to derive expressions for the derivatives of element-wise defined tensors , i.e . expressions for $ ( da ) _ { ik } =\\partial l/\\partial a_ { ik } $ . when the mapping between function indices and argument indices is not 1:1 , special attention is required . for the function $ f_ { ij } ( x ) = x_i^2 $ , the derivative of the loss is $ ( dx ) _i=\\partial l/\\partial x_i=\\sum_j ( df ) _ { ij } 2x_i $ ; the sum is necessary because index $ j $ does not appear in the indices of $ f $ . another example is $ f_ { i } ( x ) =x_ { ii } ^2 $ , where $ x $ is a matrix ; here we have $ ( dx ) _ { ij } =\\delta_ { ij } ( df ) _i2x_ { ii } $ ; the kronecker delta is necessary because the derivative is zero for off-diagonal elements . another indexing scheme is used by $ f_ { ij } ( x ) =\\exp x_ { i+j } $ ; here the correct derivative is $ ( dx ) _ { k } =\\sum_i ( df ) _ { i , k-i } \\exp x_ { k } $ , where the range of the sum must be chosen appropriately . in this publication we present an algorithm that can handle any case in which the indices of an argument are an arbitrary linear combination of the indices of the function , thus all the above examples can be handled . sums ( and their ranges ) and kronecker deltas are automatically inserted into the derivatives as necessary . additionally , the indices are transformed , if required ( as in the last example ) . the algorithm outputs a symbolic expression that can be subsequently fed into a tensor algebra compiler . source code is provided .", "topics": ["loss function", "gradient"]}
{"title": "online anomaly detection systems using incremental commute time", "abstract": "commute time distance ( ctd ) is a random walk based metric on graphs . ctd has found widespread applications in many domains including personalized search , collaborative filtering and making search engines robust against manipulation . our interest is inspired by the use of ctd as a metric for anomaly detection . it has been shown that ctd can be used to simultaneously identify both global and local anomalies . here we propose an accurate and efficient approximation for computing the ctd in an incremental fashion in order to facilitate real-time applications . an online anomaly detection algorithm is designed where the ctd of each new arriving data point to any point in the current graph can be estimated in constant time ensuring a real-time response . moreover , the proposed approach can also be applied in many other applications that utilize commute time distance .", "topics": ["time complexity"]}
{"title": "a structural query system for han characters", "abstract": "the idsgrep structural query system for han character dictionaries is presented . this system includes a data model and syntax for describing the spatial structure of han characters using extended ideographic description sequences ( eidses ) based on the unicode ids syntax ; a language for querying eids databases , designed to suit the needs of font developers and foreign language learners ; a bit vector index inspired by bloom filters for faster query operations ; a freely available implementation ; and format translation from popular third-party ids and xml character databases . experimental results are included , with a comparison to other software used for similar applications .", "topics": ["database", "dictionary"]}
{"title": "comparisons of reasoning mechanisms for computer vision", "abstract": "an evidential reasoning mechanism based on the dempster-shafer theory of evidence is introduced . its performance in real-world image analysis is compared with other mechanisms based on the bayesian formalism and a simple weight combination method .", "topics": ["computer vision"]}
{"title": "phase transition and network structure in realistic sat problems", "abstract": "a fundamental question in computer science is understanding when a specific class of problems go from being computationally easy to hard . because of its generality and applications , the problem of boolean satisfiability ( aka sat ) is often used as a vehicle for investigating this question . a signal result from these studies is that the hardness of sat problems exhibits a dramatic easy-to-hard phase transition with respect to the problem constrainedness . past studies have however focused mostly on sat instances generated using uniform random distributions , where all constraints are independently generated , and the problem variables are all considered of equal importance . these assumptions are unfortunately not satisfied by most real problems . our project aims for a deeper understanding of hardness of sat problems that arise in practice . we study two key questions : ( i ) how does easy-to-hard transition change with more realistic distributions that capture neighborhood sensitivity and rich-get-richer aspects of real problems and ( ii ) can these changes be explained in terms of the network properties ( such as node centrality and small-worldness ) of the clausal networks of the sat problems . our results , based on extensive empirical studies and network analyses , provide important structural and computational insights into realistic sat problems . our extensive empirical studies show that sat instances from realistic distributions do exhibit phase transition , but the transition occurs sooner ( at lower values of constrainedness ) than the instances from uniform random distribution . we show that this behavior can be explained in terms of their clausal network properties such as eigenvector centrality and small-worldness ( measured indirectly in terms of the clustering coefficients and average node distance ) .", "topics": ["cluster analysis", "coefficient"]}
{"title": "semantic change detection with hypermaps", "abstract": "change detection is the study of detecting changes between two different images of a scene taken at different times . by the detected change areas , however , a human can not understand how different the two images . therefore , a semantic understanding is required in the change detection research such as disaster investigation . the paper proposes the concept of semantic change detection , which involves intuitively inserting semantic meaning into detected change areas . we mainly focus on the novel semantic segmentation in addition to a conventional change detection approach . in order to solve this problem and obtain a high-level of performance , we propose an improvement to the hypercolumns representation , hereafter known as hypermaps , which effectively uses convolutional maps obtained from convolutional neural networks ( cnns ) . we also employ multi-scale feature representation captured by different image patches . we applied our method to the tsunami panoramic change detection dataset , and re-annotated the changed areas of the dataset via semantic classes . the results show that our multi-scale hypermaps provided outstanding performance on the re-annotated tsunami dataset .", "topics": ["high- and low-level", "map"]}
{"title": "deep learning : an introduction for applied mathematicians", "abstract": "multilayered artificial neural networks are becoming a pervasive tool in a host of application fields . at the heart of this deep learning revolution are familiar concepts from applied and computational mathematics ; notably , in calculus , approximation theory , optimization and linear algebra . this article provides a very brief introduction to the basic ideas that underlie deep learning from an applied mathematics perspective . our target audience includes postgraduate and final year undergraduate students in mathematics who are keen to learn about the area . the article may also be useful for instructors in mathematics who wish to enliven their classes with references to the application of deep learning techniques . we focus on three fundamental questions : what is a deep neural network ? how is a network trained ? what is the stochastic gradient method ? we illustrate the ideas with a short matlab code that sets up and trains a network . we also show the use of state-of-the art software on a large scale image classification problem . we finish with references to the current literature .", "topics": ["computer vision"]}
{"title": "multi-view fuzzy clustering with minimax optimization for effective clustering of data from multiple sources", "abstract": "multi-view data clustering refers to categorizing a data set by making good use of related information from multiple representations of the data . it becomes important nowadays because more and more data can be collected in a variety of ways , in different settings and from different sources , so each data set can be represented by different sets of features to form different views of it . many approaches have been proposed to improve clustering performance by exploring and integrating heterogeneous information underlying different views . in this paper , we propose a new multi-view fuzzy clustering approach called minimaxfcm by using minimax optimization based on well-known fuzzy c means . in minimaxfcm the consensus clustering results are generated based on minimax optimization in which the maximum disagreements of different weighted views are minimized . moreover , the weight of each view can be learned automatically in the clustering process . in addition , there is only one parameter to be set besides the fuzzifier . the detailed problem formulation , updating rules derivation , and the in-depth analysis of the proposed minimaxfcm are provided here . experimental studies on nine multi-view data sets including real world image and document data sets have been conducted . we observed that minimaxfcm outperforms related multi-view clustering approaches in terms of clustering accuracy , demonstrating the great potential of minimaxfcm for multi-view data analysis .", "topics": ["cluster analysis", "mathematical optimization"]}
{"title": "an algorithm for online tensor prediction", "abstract": "we present a new method for online prediction and learning of tensors ( $ n $ -way arrays , $ n > 2 $ ) from sequential measurements . we focus on the specific case of 3-d tensors and exploit a recently developed framework of structured tensor decompositions proposed in [ 1 ] . in this framework it is possible to treat 3-d tensors as linear operators and appropriately generalize notions of rank and positive definiteness to tensors in a natural way . using these notions we propose a generalization of the matrix exponentiated gradient descent algorithm [ 2 ] to a tensor exponentiated gradient descent algorithm using an extension of the notion of von-neumann divergence to tensors . then following a similar construction as in [ 3 ] , we exploit this algorithm to propose an online algorithm for learning and prediction of tensors with provable regret guarantees . simulations results are presented on semi-synthetic data sets of ratings evolving in time under local influence over a social network . the result indicate superior performance compared to other ( online ) convex tensor completion methods .", "topics": ["regret ( decision theory )", "synthetic data"]}
{"title": "neural decision trees", "abstract": "in this paper we propose a synergistic melting of neural networks and decision trees ( dt ) we call neural decision trees ( ndt ) . ndt is an architecture a la decision tree where each splitting node is an independent multilayer perceptron allowing oblique decision functions or arbritrary nonlinear decision function if more than one layer is used . this way , each mlp can be seen as a node of the tree . we then show that with the weight sharing asumption among those units , we end up with a hashing neural network ( hnn ) which is a multilayer perceptron with sigmoid activation function for the last layer as opposed to the standard softmax . the output units then jointly represent the probability to be in a particular region . the proposed framework allows for global optimization as opposed to greedy in dt and differentiability w.r.t . all parameters and the input , allowing easy integration in any learnable pipeline , for example after cnns for computer vision tasks . we also demonstrate the modeling power of hnn allowing to learn union of disjoint regions for final clustering or classification making it more general and powerful than standard softmax mlp requiring linear separability thus reducing the need on the inner layer to perform complex data transformations . we finally show experiments for supervised , semi-suppervised and unsupervised tasks and compare results with standard dts and mlps .", "topics": ["cluster analysis", "nonlinear system"]}
{"title": "bitewing radiography semantic segmentation base on conditional generative adversarial nets", "abstract": "currently , segmentation of bitewing radiograpy images is a very challenging task . the focus of the study is to segment it into caries , enamel , dentin , pulp , crowns , restoration and root canal treatments . the main method of semantic segmentation of bitewing radiograpy images at this stage is the u-shaped deep convolution neural network , but its accuracy is low . in order to improve the accuracy of semantic segmentation of bitewing radiograpy images , this paper proposes the use of conditional generative adversarial network ( cgan ) combined with u-shaped network structure ( u-net ) approach to semantic segmentation of bitewing radiograpy images . the experimental results show that the accuracy of cgan combined with u-net is 69.7 % , which is 13.3 % higher than the accuracy of u-shaped deep convolution neural network of 56.4 % .", "topics": ["convolution"]}
{"title": "deep k-nearest neighbors : towards confident , interpretable and robust deep learning", "abstract": "deep neural networks ( dnns ) enable innovative applications of machine learning like image recognition , machine translation , or malware detection . however , deep learning is often criticized for its lack of robustness in adversarial settings ( e.g . , vulnerability to adversarial inputs ) and general inability to rationalize its predictions . in this work , we exploit the structure of deep learning to enable new learning-based inference and decision strategies that achieve desirable properties such as robustness and interpretability . we take a first step in this direction and introduce the deep k-nearest neighbors ( dknn ) . this hybrid classifier combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the dnn : a test input is compared to its neighboring training points according to the distance that separates them in the representations . we show the labels of these neighboring points afford confidence estimates for inputs outside the model 's training manifold , including on malicious inputs like adversarial examples -- and therein provides protections against inputs that are outside the models understanding . this is because the nearest neighbors can be used to estimate the nonconformity of , i.e . , the lack of support for , a prediction in the training data . the neighbors also constitute human-interpretable explanations of predictions . we evaluate the dknn algorithm on several datasets , and show the confidence estimates accurately identify inputs outside the model , and that the explanations provided by nearest neighbors are intuitive and useful in understanding model failures .", "topics": ["test set", "machine translation"]}
{"title": "feature selection using classifier in high dimensional data", "abstract": "feature selection is frequently used as a pre-processing step to machine learning . it is a process of choosing a subset of original features so that the feature space is optimally reduced according to a certain evaluation criterion . the central objective of this paper is to reduce the dimension of the data by finding a small set of important features which can give good classification performance . we have applied filter and wrapper approach with different classifiers qda and lda respectively . a widely-used filter method is used for bioinformatics data i.e . a univariate criterion separately on each feature , assuming that there is no interaction between features and then applied sequential feature selection method . experimental results show that filter approach gives better performance in respect of misclassification error rate .", "topics": ["feature vector", "eisenstein 's criterion"]}
{"title": "sharp threshold for multivariate multi-response linear regression via block regularized lasso", "abstract": "in this paper , we investigate a multivariate multi-response ( mvmr ) linear regression problem , which contains multiple linear regression models with differently distributed design matrices , and different regression and output vectors . the goal is to recover the support union of all regression vectors using $ l_1/l_2 $ -regularized lasso . we characterize sufficient and necessary conditions on sample complexity \\emph { as a sharp threshold } to guarantee successful recovery of the support union . namely , if the sample size is above the threshold , then $ l_1/l_2 $ -regularized lasso correctly recovers the support union ; and if the sample size is below the threshold , $ l_1/l_2 $ -regularized lasso fails to recover the support union . in particular , the threshold precisely captures the impact of the sparsity of regression vectors and the statistical properties of the design matrices on sample complexity . therefore , the threshold function also captures the advantages of joint support union recovery using multi-task lasso over individual support recovery using single-task lasso .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "a constraint logic programming approach for computing ordinal conditional functions", "abstract": "in order to give appropriate semantics to qualitative conditionals of the form `` if a then normally b '' , ordinal conditional functions ( ocfs ) ranking the possible worlds according to their degree of plausibility can be used . an ocf accepting all conditionals of a knowledge base r can be characterized as the solution of a constraint satisfaction problem . we present a high-level , declarative approach using constraint logic programming techniques for solving this constraint satisfaction problem . in particular , the approach developed here supports the generation of all minimal solutions ; these minimal solutions are of special interest as they provide a basis for model-based inference from r .", "topics": ["high- and low-level"]}
{"title": "vista : a visually , socially , and temporally-aware model for artistic recommendation", "abstract": "understanding users ' interactions with highly subjective content -- -like artistic images -- -is challenging due to the complex semantics that guide our preferences . on the one hand one has to overcome `standard ' recommender systems challenges , such as dealing with large , sparse , and long-tailed datasets . on the other , several new challenges present themselves , such as the need to model content in terms of its visual appearance , or even social dynamics , such as a preference toward a particular artist that is independent of the art they create . in this paper we build large-scale recommender systems to model the dynamics of a vibrant digital art community , behance , consisting of tens of millions of interactions ( clicks and `appreciates ' ) of users toward digital art . methodologically , our main contributions are to model ( a ) rich content , especially in terms of its visual appearance ; ( b ) temporal dynamics , in terms of how users prefer `visually consistent ' content within and across sessions ; and ( c ) social dynamics , in terms of how users exhibit preferences both towards certain art styles , as well as the artists themselves .", "topics": ["interaction", "sparse matrix"]}
{"title": "training recurrent neural networks as a constraint satisfaction problem", "abstract": "this paper presents a new approach for training artificial neural networks using techniques for solving the constraint satisfaction problem ( csp ) . the quotient gradient system ( qgs ) is a trajectory based method for solving the csp . this study converts the training set of a neural network into a csp and uses the qgs to find its solutions . the qgs finds the global minimum of the optimization problem by tracking trajectories of a nonlinear dynamical system and does not stop at a local minimum of the optimization problem . lyapunov theory is used to prove the asymptotic stability of the solutions with and without the presence of measurement errors . numerical examples illustrate the effectiveness of the proposed methodology and compare it to a genetic algorithm and error backpropagation", "topics": ["optimization problem", "recurrent neural network"]}
{"title": "stochastic processes and feedback-linearisation for online identification and bayesian adaptive control of fully-actuated mechanical systems", "abstract": "this work proposes a new method for simultaneous probabilistic identification and control of an observable , fully-actuated mechanical system . identification is achieved by conditioning stochastic process priors on observations of configurations and noisy estimates of configuration derivatives . in contrast to previous work that has used stochastic processes for identification , we leverage the structural knowledge afforded by lagrangian mechanics and learn the drift and control input matrix functions of the control-affine system separately . we utilise feedback-linearisation to reduce , in expectation , the uncertain nonlinear control problem to one that is easy to regulate in a desired manner . thereby , our method combines the flexibility of nonparametric bayesian learning with epistemological guarantees on the expected closed-loop trajectory . we illustrate our method in the context of torque-actuated pendula where the dynamics are learned with a combination of normal and log-normal processes .", "topics": ["nonlinear system"]}
{"title": "a comprehensive survey of ontology summarization : measures and methods", "abstract": "the semantic web is becoming a large scale framework that enables data to be published , shared , and reused in the form of ontologies . the ontology which is considered as basic building block of semantic web consists of two layers including data and schema layer . with the current exponential development of ontologies in both data size and complexity of schemas , ontology understanding which is playing an important role in different tasks such as ontology engineering , ontology learning , etc . , is becoming more difficult . ontology summarization as a way to distill knowledge from an ontology and generate an abridge version to facilitate a better understanding is getting more attention recently . there are various approaches available for ontology summarization which are focusing on different measures in order to produce a proper summary for a given ontology . in this paper , we mainly focus on the common metrics which are using for ontology summarization and meet the state-of-the-art in ontology summarization .", "topics": ["time complexity"]}
{"title": "deep supervised hashing with triplet labels", "abstract": "hashing is one of the most popular and powerful approximate nearest neighbor search techniques for large-scale image retrieval . most traditional hashing methods first represent images as off-the-shelf visual features and then produce hashing codes in a separate stage . however , off-the-shelf visual features may not be optimally compatible with the hash code learning procedure , which may result in sub-optimal hash codes . recently , deep hashing methods have been proposed to simultaneously learn image features and hash codes using deep neural networks and have shown superior performance over traditional hashing methods . most deep hashing methods are given supervised information in the form of pairwise labels or triplet labels . the current state-of-the-art deep hashing method dpsh~\\cite { li2015feature } , which is based on pairwise labels , performs image feature learning and hash code learning simultaneously by maximizing the likelihood of pairwise similarities . inspired by dpsh~\\cite { li2015feature } , we propose a triplet label based deep hashing method which aims to maximize the likelihood of the given triplet labels . experimental results show that our method outperforms all the baselines on cifar-10 and nus-wide datasets , including the state-of-the-art method dpsh~\\cite { li2015feature } and all the previous triplet label based deep hashing methods .", "topics": ["feature learning"]}
{"title": "exponentially consistent kernel two-sample tests", "abstract": "given two sets of independent samples from unknown distributions $ p $ and $ q $ , a two-sample test decides whether to reject the null hypothesis that $ p=q $ . recent attention has focused on kernel two-sample tests as the test statistics are easy to compute , converge fast , and have low bias with their finite sample estimates . however , there still lacks an exact characterization on the asymptotic performance of such tests , and in particular , the rate at which the type-ii error probability decays to zero in the large sample limit . in this work , we show that a class of kernel two-sample tests are exponentially consistent on polish , locally compact hausdorff space , e.g . , $ \\mathbb r^d $ . the obtained exponential decay rate is further shown to be optimal among all two-sample tests meeting the given level constraint , and is independent of particular kernels provided that they are bounded continuous and characteristic . key to our approach are an extended version of sanov 's theorem and a recent result that identifies the maximum mean discrepancy as a metric of weak convergence of probability measures .", "topics": ["kernel ( operating system )", "time complexity"]}
{"title": "a general theory of additive state space abstractions", "abstract": "informally , a set of abstractions of a state space s is additive if the distance between any two states in s is always greater than or equal to the sum of the corresponding distances in the abstract spaces . the first known additive abstractions , called disjoint pattern databases , were experimentally demonstrated to produce state of the art performance on certain state spaces . however , previous applications were restricted to state spaces with special properties , which precludes disjoint pattern databases from being defined for several commonly used testbeds , such as rubiks cube , topspin and the pancake puzzle . in this paper we give a general definition of additive abstractions that can be applied to any state space and prove that heuristics based on additive abstractions are consistent as well as admissible . we use this new definition to create additive abstractions for these testbeds and show experimentally that well chosen additive abstractions can reduce search time substantially for the ( 18,4 ) -topspin puzzle and by three orders of magnitude over state of the art methods for the 17-pancake puzzle . we also derive a way of testing if the heuristic value returned by additive abstractions is provably too low and show that the use of this test can reduce search time for the 15-puzzle and topspin by roughly a factor of two .", "topics": ["heuristic", "database"]}
{"title": "face clustering : representation and pairwise constraints", "abstract": "clustering face images according to their identity has two important applications : ( i ) grouping a collection of face images when no external labels are associated with images , and ( ii ) indexing for efficient large scale face retrieval . the clustering problem is composed of two key parts : face representation and choice of similarity for grouping faces . we first propose a representation based on resnet , which has been shown to perform very well in image classification problems . given this representation , we design a clustering algorithm , conditional pairwise clustering ( conpac ) , which directly estimates the adjacency matrix only based on the similarity between face images . this allows a dynamic selection of number of clusters and retains pairwise similarity between faces . conpac formulates the clustering problem as a conditional random field ( crf ) model and uses loopy belief propagation to find an approximate solution for maximizing the posterior probability of the adjacency matrix . experimental results on two benchmark face datasets ( lfw and ijb-b ) show that conpac outperforms well known clustering algorithms such as k-means , spectral clustering and approximate rank-order . additionally , our algorithm can naturally incorporate pairwise constraints to obtain a semi-supervised version that leads to improved clustering performance . we also propose an k-nn variant of conpac , which has a linear time complexity given a k-nn graph , suitable for large datasets .", "topics": ["cluster analysis", "approximation algorithm"]}
{"title": "fast spectral ranking for similarity search", "abstract": "despite the success of deep learning on representing images for particular object retrieval , recent studies show that the learned representations still lie on manifolds in a high dimensional space . therefore , nearest neighbor search can not be expected to be optimal for this task . even if a nearest neighbor graph is computed offline , exploring the manifolds online remains expensive . this work introduces an explicit embedding reducing manifold search to euclidean search followed by dot product similarity search . we show this is equivalent to linear graph filtering of a sparse signal in the frequency domain , and we introduce a scalable offline computation of an approximate fourier basis of the graph . we improve the state of art on standard particular object retrieval datasets including a challenging one containing small objects . at a scale of $ 10^5 $ images , the offline cost is only a few hours , while query time is comparable to standard similarity search .", "topics": ["approximation algorithm", "sparse matrix"]}
{"title": "preserving intermediate objectives : one simple trick to improve learning for hierarchical models", "abstract": "hierarchical models are utilized in a wide variety of problems which are characterized by task hierarchies , where predictions on smaller subtasks are useful for trying to predict a final task . typically , neural networks are first trained for the subtasks , and the predictions of these networks are subsequently used as additional features when training a model and doing inference for a final task . in this work , we focus on improving learning for such hierarchical models and demonstrate our method on the task of speaker trait prediction . speaker trait prediction aims to computationally identify which personality traits a speaker might be perceived to have , and has been of great interest to both the artificial intelligence and social science communities . persuasiveness prediction in particular has been of interest , as persuasive speakers have a large amount of influence on our thoughts , opinions and beliefs . in this work , we examine how leveraging the relationship between related speaker traits in a hierarchical structure can help improve our ability to predict how persuasive a speaker is . we present a novel algorithm that allows us to backpropagate through this hierarchy . this hierarchical model achieves a 25 % relative error reduction in classification accuracy over current state-of-the art methods on the publicly available pom dataset .", "topics": ["bayesian network", "artificial intelligence"]}
{"title": "efficient representation for natural language processing via kernelized hashcodes", "abstract": "kernel methods have been used widely in a number of tasks , but have had limited success in natural language processing ( nlp ) due to high cost of computing kernel similarities between discrete natural language structures . a recently proposed technique , kernelized locality sensitive hashing ( klsh ) , can significantly reduce the computational cost , but is only applicable to classifiers operating on knn graphs . here we propose to use random subspaces of klsh codes for efficiently constructing explicit representation of natural language structures suitable for general classification methods . further , we propose an approach for optimizing a klsh model for classification problems , by maximizing a variational lower bound on the mutual information between the klsh codes ( feature vectors ) and the class labels . we apply the proposed approach to a biomedical information extraction task , and observe robust improvements in accuracy , along with significant speedup compared to conventional kernel methods .", "topics": ["calculus of variations", "natural language processing"]}
{"title": "keeping it short and simple : summarising complex event sequences with multivariate patterns", "abstract": "we study how to obtain concise descriptions of discrete multivariate sequential data . in particular , how to do so in terms of rich multivariate sequential patterns that can capture potentially highly interesting ( cor ) relations between sequences . to this end we allow our pattern language to span over the domains ( alphabets ) of all sequences , allow patterns to overlap temporally , as well as allow for gaps in their occurrences . we formalise our goal by the minimum description length principle , by which our objective is to discover the set of patterns that provides the most succinct description of the data . to discover high-quality pattern sets directly from data , we introduce ditto , a highly efficient algorithm that approximates the ideal result very well . experiments show that ditto correctly discovers the patterns planted in synthetic data . moreover , it scales favourably with the length of the data , the number of attributes , the alphabet sizes . on real data , ranging from sensor networks to annotated text , ditto discovers easily interpretable summaries that provide clear insight in both the univariate and multivariate structure .", "topics": ["synthetic data"]}
{"title": "towards practical conditional risk minimization", "abstract": "we study conditional risk minimization ( crm ) , i.e . the problem of learning a hypothesis of minimal risk for prediction at the next step of a sequentially arriving dependent data . despite it being a fundamental problem , successful learning in the crm sense has so far only been demonstrated using theoretical algorithms that can not be used for real problems as they would require storing all incoming data . in this work , we introduce macro , a meta-algorithm for crm that does not suffer from this shortcoming , as instead of storing all data it maintains and iteratively updates a set of learning subroutines . using suitable approximations , macro can be implemented and applied to real data , leading , as we illustrate experimentally , to improved prediction performance compared to traditional non-conditional learning .", "topics": ["approximation"]}
{"title": "adversarial transformation networks : learning to generate adversarial examples", "abstract": "multiple different approaches of generating adversarial examples have been proposed to attack deep neural networks . these approaches involve either directly computing gradients with respect to the image pixels , or directly solving an optimization on the image pixels . in this work , we present a fundamentally new method for generating adversarial examples that is fast to execute and provides exceptional diversity of output . we efficiently train feed-forward neural networks in a self-supervised manner to generate adversarial examples against a target network or set of networks . we call such a network an adversarial transformation network ( atn ) . atns are trained to generate adversarial examples that minimally modify the classifier 's outputs given the original input , while constraining the new classification to match an adversarial target class . we present methods to train atns and analyze their effectiveness targeting a variety of mnist classifiers as well as the latest state-of-the-art imagenet classifier inception resnet v2 .", "topics": ["mnist database", "pixel"]}
{"title": "robust intrinsic and extrinsic calibration of rgb-d cameras", "abstract": "color-depth cameras ( rgb-d cameras ) have become the primary sensors in most robotics systems , from service robotics to industrial robotics applications . typical consumer-grade rgb-d cameras are provided with a coarse intrinsic and extrinsic calibration that generally does not meet the accuracy requirements needed by many robotics applications ( e.g . , high accuracy 3d environment reconstruction and mapping , high precision object recognition and localization , ... ) . in this paper , we propose a human-friendly , reliable and accurate calibration framework that enables to easily estimate both the intrinsic and extrinsic parameters of a general color-depth sensor couple . our approach is based on a novel , two components , measurement error model that unifies the error sources of different rgb-d pairs based on different technologies , such as structured-light 3d cameras and time-of-flight cameras . the proposed correction model is implemented using two different parametric undistortion maps that provide the calibrated readings by means of linear combinations of control functions . our method provides some important advantages compared to other state-of-the-art systems : it is general ( i.e . , well suited for different types of sensors ) , it is based on an easy and stable calibration protocol , it provides a greater calibration accuracy , and it has been implemented within the ros robotics framework . we report detailed and comprehensive experimental validations and performance comparisons to support our statements .", "topics": ["map", "sensor"]}
{"title": "mapping out narrative structures and dynamics using networks and textual information", "abstract": "human communication is often executed in the form of a narrative , an account of connected events composed of characters , actions , and settings . a coherent narrative structure is therefore a requisite for a well-formulated narrative -- be it fictional or nonfictional -- for informative and effective communication , opening up the possibility of a deeper understanding of a narrative by studying its structural properties . in this paper we present a network-based framework for modeling and analyzing the structure of a narrative , which is further expanded by incorporating methods from computational linguistics to utilize the narrative text . modeling a narrative as a dynamically unfolding system , we characterize its progression via the growth patterns of the character network , and use sentiment analysis and topic modeling to represent the actual content of the narrative in the form of interaction maps between characters with associated sentiment values and keywords . this is a network framework advanced beyond the simple occurrence-based one most often used until now , allowing one to utilize the unique characteristics of a given narrative to a high degree . given the ubiquity and importance of narratives , such advanced network-based representation and analysis framework may lead to a more systematic modeling and understanding of narratives for social interactions , expression of human sentiments , and communication .", "topics": ["interaction", "map"]}
{"title": "perceptual losses for real-time style transfer and super-resolution", "abstract": "we consider image transformation problems , where an input image is transformed into an output image . recent methods for such problems typically train feed-forward convolutional neural networks using a \\emph { per-pixel } loss between the output and ground-truth images . parallel work has shown that high-quality images can be generated by defining and optimizing \\emph { perceptual } loss functions based on high-level features extracted from pretrained networks . we combine the benefits of both approaches , and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks . we show results on image style transfer , where a feed-forward network is trained to solve the optimization problem proposed by gatys et al in real-time . compared to the optimization-based method , our network gives similar qualitative results but is three orders of magnitude faster . we also experiment with single-image super-resolution , where replacing a per-pixel loss with a perceptual loss gives visually pleasing results .", "topics": ["high- and low-level", "optimization problem"]}
{"title": "order-independent constraint-based causal structure learning", "abstract": "we consider constraint-based methods for causal structure learning , such as the pc- , fci- , rfci- and ccd- algorithms ( spirtes et al . ( 2000 , 1993 ) , richardson ( 1996 ) , colombo et al . ( 2012 ) , claassen et al . ( 2013 ) ) . the first step of all these algorithms consists of the pc-algorithm . this algorithm is known to be order-dependent , in the sense that the output can depend on the order in which the variables are given . this order-dependence is a minor issue in low-dimensional settings . we show , however , that it can be very pronounced in high-dimensional settings , where it can lead to highly variable results . we propose several modifications of the pc-algorithm ( and hence also of the other algorithms ) that remove part or all of this order-dependence . all proposed modifications are consistent in high-dimensional settings under the same conditions as their original counterparts . we compare the pc- , fci- , and rfci-algorithms and their modifications in simulation studies and on a yeast gene expression data set . we show that our modifications yield similar performance in low-dimensional settings and improved performance in high-dimensional settings . all software is implemented in the r-package pcalg .", "topics": ["simulation", "causality"]}
{"title": "se3-pose-nets : structured deep dynamics models for visuomotor planning and control", "abstract": "in this work , we present an approach to deep visuomotor control using structured deep dynamics models . our deep dynamics model , a variant of se3-nets , learns a low-dimensional pose embedding for visuomotor control via an encoder-decoder structure . unlike prior work , our dynamics model is structured : given an input scene , our network explicitly learns to segment salient parts and predict their pose-embedding along with their motion modeled as a change in the pose space due to the applied actions . we train our model using a pair of point clouds separated by an action and show that given supervision only in the form of point-wise data associations between the frames our network is able to learn a meaningful segmentation of the scene along with consistent poses . we further show that our model can be used for closed-loop control directly in the learned low-dimensional pose space , where the actions are computed by minimizing error in the pose space using gradient-based methods , similar to traditional model-based control . we present results on controlling a baxter robot from raw depth data in simulation and in the real world and compare against two baseline deep networks . our method runs in real-time , achieves good prediction of scene dynamics and outperforms the baseline methods on multiple control runs . video results can be found at : https : //rse-lab.cs.washington.edu/se3-structured-deep-ctrl/", "topics": ["baseline ( configuration management )", "simulation"]}
{"title": "fashion-mnist : a novel image dataset for benchmarking machine learning algorithms", "abstract": "we present fashion-mnist , a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories , with 7,000 images per category . the training set has 60,000 images and the test set has 10,000 images . fashion-mnist is intended to serve as a direct drop-in replacement for the original mnist dataset for benchmarking machine learning algorithms , as it shares the same image size , data format and the structure of training and testing splits . the dataset is freely available at https : //github.com/zalandoresearch/fashion-mnist", "topics": ["test set", "mnist database"]}
{"title": "parallel gaussian process optimization with upper confidence bound and pure exploration", "abstract": "in this paper , we consider the challenge of maximizing an unknown function f for which evaluations are noisy and are acquired with high cost . an iterative procedure uses the previous measures to actively select the next estimation of f which is predicted to be the most useful . we focus on the case where the function can be evaluated in parallel with batches of fixed size and analyze the benefit compared to the purely sequential procedure in terms of cumulative regret . we introduce the gaussian process upper confidence bound and pure exploration algorithm ( gp-ucb-pe ) which combines the ucb strategy and pure exploration in the same batch of evaluations along the parallel iterations . we prove theoretical upper bounds on the regret with batches of size k for this procedure which show the improvement of the order of sqrt { k } for fixed iteration cost over purely sequential versions . moreover , the multiplicative constants involved have the property of being dimension-free . we also confirm empirically the efficiency of gp-ucb-pe on real and synthetic problems compared to state-of-the-art competitors .", "topics": ["regret ( decision theory )", "synthetic data"]}
{"title": "a sequential model for multi-class classification", "abstract": "many classification problems require decisions among a large number of competing classes . these tasks , however , are not handled well by general purpose learning methods and are usually addressed in an ad-hoc fashion . we suggest a general approach -- a sequential learning model that utilizes classifiers to sequentially restrict the number of competing classes while maintaining , with high probability , the presence of the true outcome in the candidates set . some theoretical and computational properties of the model are discussed and we argue that these are important in nlp-like domains . the advantages of the model are illustrated in an experiment in part-of-speech tagging .", "topics": ["natural language processing", "speech recognition"]}
{"title": "solving relational mdps with exogenous events and additive rewards", "abstract": "we formalize a simple but natural subclass of service domains for relational planning problems with object-centered , independent exogenous events and additive rewards capturing , for example , problems in inventory control . focusing on this subclass , we present a new symbolic planning algorithm which is the first algorithm that has explicit performance guarantees for relational mdps with exogenous events . in particular , under some technical conditions , our planning algorithm provides a monotonic lower bound on the optimal value function . to support this algorithm we present novel evaluation and reduction techniques for generalized first order decision diagrams , a knowledge representation for real-valued functions over relational world states . our planning algorithm uses a set of focus states , which serves as a training set , to simplify and approximate the symbolic solution , and can thus be seen to perform learning for planning . a preliminary experimental evaluation demonstrates the validity of our approach .", "topics": ["test set", "approximation algorithm"]}
{"title": "private and online optimization of piecewise lipschitz functions", "abstract": "in this work , we present online and differentially private optimization algorithms for a large family of nonconvex functions . this family consists of piecewise lipschitz functions , which are ubiquitous across diverse domains . for example , problems in computational economics and algorithm configuration ( also known as parameter tuning ) often reduce to maximizing piecewise lipschitz functions . these functions are challenging to optimize privately and online since a small error can push an optimal point into a nonoptimal region . we introduce a sufficient and general dispersion condition on these functions that ensures well-known private and online algorithms have strong utility guarantees . we show that several important problems from computational economics and algorithm configuration reduce to optimizing functions that satisfy this condition . we apply our results to obtain private and online algorithms for these problems . we thus answer several open questions : cohen-addad and kanade [ '17 ] asked how to optimize piecewise lipschitz functions online and gupta and roughgarden [ '17 ] asked what algorithm configuration problems can be solved online with no regret algorithms . in algorithm configuration , the goal is to tune an algorithm 's parameters to optimize its performance over a specific application domain . we analyze greedy techniques for subset selection problems and sdp-rounding schemes for problems that can be formulated as integer quadratic programs . in mechanism design and other pricing problems , the goal is to use information about past consumers to design auctions and set prices that extract high profit from future consumers . we analyze the classic classes of second price auctions with reserves and posted price mechanisms . for all of these settings , our general technique implies strong utility bounds in the private setting and strong regret bounds in the online learning setting .", "topics": ["regret ( decision theory )", "mathematical optimization"]}
{"title": "player co-modelling in a strategy board game : discovering how to play fast", "abstract": "in this paper we experiment with a 2-player strategy board game where playing models are evolved using reinforcement learning and neural networks . the models are evolved to speed up automatic game development based on human involvement at varying levels of sophistication and density when compared to fully autonomous playing . the experimental results suggest a clear and measurable association between the ability to win games and the ability to do that fast , while at the same time demonstrating that there is a minimum level of human involvement beyond which no learning really occurs .", "topics": ["reinforcement learning", "autonomous car"]}
{"title": "approximate muscle guided beam search for three-index assignment problem", "abstract": "as a well-known np-hard problem , the three-index assignment problem ( ap3 ) has attracted lots of research efforts for developing heuristics . however , existing heuristics either obtain less competitive solutions or consume too much time . in this paper , a new heuristic named approximate muscle guided beam search ( ambs ) is developed to achieve a good trade-off between solution quality and running time . by combining the approximate muscle with beam search , the solution space size can be significantly decreased , thus the time for searching the solution can be sharply reduced . extensive experimental results on the benchmark indicate that the new algorithm is able to obtain solutions with competitive quality and it can be employed on instances with largescale . work of this paper not only proposes a new efficient heuristic , but also provides a promising method to improve the efficiency of beam search .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "greedy biomarker discovery in the genome with applications to antimicrobial resistance", "abstract": "the set covering machine ( scm ) is a greedy learning algorithm that produces sparse classifiers . we extend the scm for datasets that contain a huge number of features . the whole genetic material of living organisms is an example of such a case , where the number of feature exceeds 10^7 . three human pathogens were used to evaluate the performance of the scm at predicting antimicrobial resistance . our results show that the scm compares favorably in terms of sparsity and accuracy against l1 and l2 regularized support vector machines and cart decision trees . moreover , the scm was the only algorithm that could consider the full feature space . for all other algorithms , the latter had to be filtered as a preprocessing step .", "topics": ["feature vector", "support vector machine"]}
{"title": "deep image matting", "abstract": "image matting is a fundamental computer vision problem and has many applications . previous algorithms have poor performance when an image has similar foreground and background colors or complicated textures . the main reasons are prior methods 1 ) only use low-level features and 2 ) lack high-level context . in this paper , we propose a novel deep learning based algorithm that can tackle both these problems . our deep model has two parts . the first part is a deep convolutional encoder-decoder network that takes an image and the corresponding trimap as inputs and predict the alpha matte of the image . the second part is a small convolutional network that refines the alpha matte predictions of the first network to have more accurate alpha values and sharper edges . in addition , we also create a large-scale image matting dataset including 49300 training images and 1000 testing images . we evaluate our algorithm on the image matting benchmark , our testing set , and a wide variety of real images . experimental results clearly demonstrate the superiority of our algorithm over previous methods .", "topics": ["high- and low-level", "computer vision"]}
{"title": "kernelized bayesian matrix factorization", "abstract": "we extend kernelized matrix factorization with a fully bayesian treatment and with an ability to work with multiple side information sources expressed as different kernels . kernel functions have been introduced to matrix factorization to integrate side information about the rows and columns ( e.g . , objects and users in recommender systems ) , which is necessary for making out-of-matrix ( i.e . , cold start ) predictions . we discuss specifically bipartite graph inference , where the output matrix is binary , but extensions to more general matrices are straightforward . we extend the state of the art in two key aspects : ( i ) a fully conjugate probabilistic formulation of the kernelized matrix factorization problem enables an efficient variational approximation , whereas fully bayesian treatments are not computationally feasible in the earlier approaches . ( ii ) multiple side information sources are included , treated as different kernels in multiple kernel learning that additionally reveals which side information sources are informative . our method outperforms alternatives in predicting drug-protein interactions on two data sets . we then show that our framework can also be used for solving multilabel learning problems by considering samples and labels as the two domains where matrix factorization operates on . our algorithm obtains the lowest hamming loss values on 10 out of 14 multilabel classification data sets compared to five state-of-the-art multilabel learning algorithms .", "topics": ["calculus of variations", "supervised learning"]}
{"title": "visual-hint boundary to segment algorithm for image segmentation", "abstract": "image segmentation has been a very active research topic in image analysis area . currently , most of the image segmentation algorithms are designed based on the idea that images are partitioned into a set of regions preserving homogeneous intra-regions and inhomogeneous inter-regions . however , human visual intuition does not always follow this pattern . a new image segmentation method named visual-hint boundary to segment ( vhbs ) is introduced , which is more consistent with human perceptions . vhbs abides by two visual hint rules based on human perceptions : ( i ) the global scale boundaries tend to be the real boundaries of the objects ; ( ii ) two adjacent regions with quite different colors or textures tend to result in the real boundaries between them . it has been demonstrated by experiments that , compared with traditional image segmentation method , vhbs has better performance and also preserves higher computational efficiency .", "topics": ["image segmentation"]}
{"title": "sparse approximate inference for spatio-temporal point process models", "abstract": "spatio-temporal point process models play a central role in the analysis of spatially distributed systems in several disciplines . yet , scalable inference remains computa- tionally challenging both due to the high resolution modelling generally required and the analytically intractable likelihood function . here , we exploit the sparsity structure typical of ( spatially ) discretised log-gaussian cox process models by using approximate message-passing algorithms . the proposed algorithms scale well with the state dimension and the length of the temporal horizon with moderate loss in distributional accuracy . they hence provide a flexible and faster alternative to both non-linear filtering-smoothing type algorithms and to approaches that implement the laplace method or expectation propagation on ( block ) sparse latent gaussian models . we infer the parameters of the latent gaussian model using a structured variational bayes approach . we demonstrate the proposed framework on simulation studies with both gaussian and point-process observations and use it to reconstruct the conflict intensity and dynamics in afghanistan from the wikileaks afghan war diary .", "topics": ["calculus of variations", "approximation algorithm"]}
{"title": "realisation d'un systeme de reconnaissance automatique de la parole arabe base sur cmu sphinx", "abstract": "this paper presents the continuation of the work completed by satori and all . [ sch07 ] by the realization of an automatic speech recognition system ( asr ) for arabic language based sphinx 4 system . the previous work was limited to the recognition of the first ten digits , whereas the present work is a remarkable projection consisting in continuous arabic speech recognition with a rate of recognition of surroundings 96 % .", "topics": ["speech recognition"]}
{"title": "detecting sockpuppets in deceptive opinion spam", "abstract": "this paper explores the problem of sockpuppet detection in deceptive opinion spam using authorship attribution and verification approaches . two methods are explored . the first is a feature subsampling scheme that uses the kl-divergence on stylistic language models of an author to find discriminative features . the second is a transduction scheme , spy induction that leverages the diversity of authors in the unlabeled test set by sending a set of spies ( positive samples ) from the training set to retrieve hidden samples in the unlabeled test set using nearest and farthest neighbors . experiments using ground truth sockpuppet data show the effectiveness of the proposed schemes .", "topics": ["test set", "ground truth"]}
{"title": "generalization in machine learning via analytical learning theory", "abstract": "this paper introduces a novel measure-theoretic learning theory to analyze generalization behaviors of practical interest . the proposed learning theory has the following abilities : 1 ) to utilize the qualities of each learned representation on the path from raw inputs to outputs in representation learning , 2 ) to guarantee good generalization errors possibly with arbitrarily rich hypothesis spaces ( e.g . , arbitrarily large capacity and rademacher complexity ) and non-stable/non-robust learning algorithms , and 3 ) to clearly distinguish each individual problem instance from each other . our generalization bounds are relative to a representation of the data , and hold true even if the representation is learned . we discuss several consequences of our results on deep learning , one-shot learning and curriculum learning . unlike statistical learning theory , the proposed learning theory analyzes each problem instance individually via measure theory , rather than a set of problem instances via statistics . because of the differences in the assumptions and the objectives , the proposed learning theory is meant to be complementary to previous learning theory and is not designed to compete with it .", "topics": ["feature learning"]}
{"title": "a randomized approximation algorithm of logic sampling", "abstract": "in recent years , researchers in decision analysis and artificial intelligence ( ai ) have used bayesian belief networks to build models of expert opinion . using standard methods drawn from the theory of computational complexity , workers in the field have shown that the problem of exact probabilistic inference on belief networks almost certainly requires exponential computation in the worst ease [ 3 ] . we have previously described a randomized approximation scheme , called bn-ras , for computation on belief networks [ 1 , 2 , 4 ] . we gave precise analytic bounds on the convergence of bn-ras and showed how to trade running time for accuracy in the evaluation of posterior marginal probabilities . we now extend our previous results and demonstrate the generality of our framework by applying similar mathematical techniques to the analysis of convergence for logic sampling [ 7 ] , an alternative simulation algorithm for probabilistic inference .", "topics": ["approximation algorithm", "computational complexity theory"]}
{"title": "sketching meets random projection in the dual : a provable recovery algorithm for big and high-dimensional data", "abstract": "sketching techniques have become popular for scaling up machine learning algorithms by reducing the sample size or dimensionality of massive data sets , while still maintaining the statistical power of big data . in this paper , we study sketching from an optimization point of view : we first show that the iterative hessian sketch is an optimization process with preconditioning , and develop accelerated iterative hessian sketch via the searching the conjugate direction ; we then establish primal-dual connections between the hessian sketch and dual random projection , and apply the preconditioned conjugate gradient approach on the dual problem , which leads to the accelerated iterative dual random projection methods . finally to tackle the challenges from both large sample size and high-dimensionality , we propose the primal-dual sketch , which iteratively sketches the primal and dual formulations . we show that using a logarithmic number of calls to solvers of small scale problem , primal-dual sketch is able to recover the optimum of the original problem up to arbitrary precision . the proposed algorithms are validated via extensive experiments on synthetic and real data sets which complements our theoretical results .", "topics": ["gradient descent", "gradient"]}
{"title": "quadratically constrained quadratic programming for classification using particle swarms and applications", "abstract": "particle swarm optimization is used in several combinatorial optimization problems . in this work , particle swarms are used to solve quadratic programming problems with quadratic constraints . the approach of particle swarms is an example for interior point methods in optimization as an iterative technique . this approach is novel and deals with classification problems without the use of a traditional classifier . our method determines the optimal hyperplane or classification boundary for a data set . in a binary classification problem , we constrain each class as a cluster , which is enclosed by an ellipsoid . the estimation of the optimal hyperplane between the two clusters is posed as a quadratically constrained quadratic problem . the optimization problem is solved in distributed format using modified particle swarms . our method has the advantage of using the direction towards optimal solution rather than searching the entire feasible region . our results on the iris , pima , wine , and thyroid datasets show that the proposed method works better than a neural network and the performance is close to that of svm .", "topics": ["optimization problem", "statistical classification"]}
{"title": "spatio-temporal attention models for grounded video captioning", "abstract": "automatic video captioning is challenging due to the complex interactions in dynamic real scenes . a comprehensive system would ultimately localize and track the objects , actions and interactions present in a video and generate a description that relies on temporal localization in order to ground the visual concepts . however , most existing automatic video captioning systems map from raw video data to high level textual description , bypassing localization and recognition , thus discarding potentially valuable information for content localization and generalization . in this work we present an automatic video captioning model that combines spatio-temporal attention and image classification by means of deep neural network structures based on long short-term memory . the resulting system is demonstrated to produce state-of-the-art results in the standard youtube captioning benchmark while also offering the advantage of localizing the visual concepts ( subjects , verbs , objects ) , with no grounding supervision , over space and time .", "topics": ["computer vision", "interaction"]}
{"title": "learning from multiple sources for video summarisation", "abstract": "many visual surveillance tasks , e.g.video summarisation , is conventionally accomplished through analysing imagerybased features . relying solely on visual cues for public surveillance video understanding is unreliable , since visual observations obtained from public space cctv video data are often not sufficiently trustworthy and events of interest can be subtle . on the other hand , non-visual data sources such as weather reports and traffic sensory signals are readily accessible but are not explored jointly to complement visual data for video content analysis and summarisation . in this paper , we present a novel unsupervised framework to learn jointly from both visual and independently-drawn non-visual data sources for discovering meaningful latent structure of surveillance video data . in particular , we investigate ways to cope with discrepant dimension and representation whist associating these heterogeneous data sources , and derive effective mechanism to tolerate with missing and incomplete data from different sources . we show that the proposed multi-source learning framework not only achieves better video content clustering than state-of-the-art methods , but also is capable of accurately inferring missing non-visual semantics from previously unseen videos . in addition , a comprehensive user study is conducted to validate the quality of video summarisation generated using the proposed multi-source model .", "topics": ["cluster analysis", "unsupervised learning"]}
{"title": "learning and tuning meta-heuristics in plan space planning", "abstract": "in recent years , the planning community has observed that techniques for learning heuristic functions have yielded improvements in performance . one approach is to use offline learning to learn predictive models from existing heuristics in a domain dependent manner . these learned models are deployed as new heuristic functions . the learned models can in turn be tuned online using a domain independent error correction approach to further enhance their informativeness . the online tuning approach is domain independent but instance specific , and contributes to improved performance for individual instances as planning proceeds . consequently it is more effective in larger problems . in this paper , we mention two approaches applicable in partial order causal link ( pocl ) planning that is also known as plan space planning . first , we endeavor to enhance the performance of a pocl planner by giving an algorithm for supervised learning . second , we then discuss an online error minimization approach in pocl framework to minimize the step-error associated with the offline learned models thus enhancing their informativeness . our evaluation shows that the learning approaches scale up the performance of the planner over standard benchmarks , specially for larger problems .", "topics": ["supervised learning", "heuristic"]}
{"title": "connecting phrase based statistical machine translation adaptation", "abstract": "although more additional corpora are now available for statistical machine translation ( smt ) , only the ones which belong to the same or similar domains with the original corpus can indeed enhance smt performance directly . most of the existing adaptation methods focus on sentence selection . in comparison , phrase is a smaller and more fine grained unit for data selection , therefore we propose a straightforward and efficient connecting phrase based adaptation method , which is applied to both bilingual phrase pair and monolingual n-gram adaptation . the proposed method is evaluated on iwslt/nist data sets , and the results show that phrase based smt performance are significantly improved ( up to +1.6 in comparison with phrase based smt baseline system and +0.9 in comparison with existing methods ) .", "topics": ["baseline ( configuration management )", "machine translation"]}
{"title": "solving rubik 's cube using sat solvers", "abstract": "rubik 's cube is an easily-understood puzzle , which is originally called the `` magic cube '' . it is a well-known planning problem , which has been studied for a long time . yet many simple properties remain unknown . this paper studies whether modern sat solvers are applicable to this puzzle . to our best knowledge , we are the first to translate rubik 's cube to a sat problem . to reduce the number of variables and clauses needed for the encoding , we replace a naive approach of 6 boolean variables to represent each color on each facelet with a new approach of 3 or 2 boolean variables . in order to be able to solve quickly rubik 's cube , we replace the direct encoding of 18 turns with the layer encoding of 18-subtype turns based on 6-type turns . to speed up the solving further , we encode some properties of two-phase algorithm as an additional constraint , and restrict some move sequences by adding some constraint clauses . using only efficient encoding can not solve this puzzle . for this reason , we improve the existing sat solvers , and develop a new sat solver based on precosat , though it is suited only for rubik 's cube . the new sat solver replaces the lookahead solving strategy with an alo ( \\emph { at-least-one } ) solving strategy , and decomposes the original problem into sub-problems . each sub-problem is solved by precosat . the empirical results demonstrate both our sat translation and new solving technique are efficient . without the efficient sat encoding and the new solving technique , rubik 's cube will not be able to be solved still by any sat solver . using the improved sat solver , we can find always a solution of length 20 in a reasonable time . although our solver is slower than kociemba 's algorithm using lookup tables , but does not require a huge lookup table .", "topics": ["parsing"]}
{"title": "dcco : towards deformable continuous convolution operators", "abstract": "discriminative correlation filter ( dcf ) based methods have shown competitive performance on tracking benchmarks in recent years . generally , dcf based trackers learn a rigid appearance model of the target . however , this reliance on a single rigid appearance model is insufficient in situations where the target undergoes non-rigid transformations . in this paper , we propose a unified formulation for learning a deformable convolution filter . in our framework , the deformable filter is represented as a linear combination of sub-filters . both the sub-filter coefficients and their relative locations are inferred jointly in our formulation . experiments are performed on three challenging tracking benchmarks : otb- 2015 , templecolor and vot2016 . our approach improves the baseline method , leading to performance comparable to state-of-the-art .", "topics": ["baseline ( configuration management )", "convolution"]}
{"title": "temporal plannability by variance of the episode length", "abstract": "optimization of decision problems in stochastic environments is usually concerned with maximizing the probability of achieving the goal and minimizing the expected episode length . for interacting agents in time-critical applications , learning of the possibility of scheduling of subtasks ( events ) or the full task is an additional relevant issue . besides , there exist highly stochastic problems where the actual trajectories show great variety from episode to episode , but completing the task takes almost the same amount of time . the identification of sub-problems of this nature may promote e.g . , planning , scheduling and segmenting markov decision processes . in this work , formulae for the average duration as well as the standard deviation of the duration of events are derived . the emerging bellman-type equation is a simple extension of sobel 's work ( 1982 ) . methods of dynamic programming as well as methods of reinforcement learning can be applied for our extension . computer demonstration on a toy problem serve to highlight the principle .", "topics": ["reinforcement learning"]}
{"title": "an empirical evaluation of probabilistic lexicalized tree insertion grammars", "abstract": "we present an empirical study of the applicability of probabilistic lexicalized tree insertion grammars ( pltig ) , a lexicalized counterpart to probabilistic context-free grammars ( pcfg ) , to problems in stochastic natural-language processing . comparing the performance of pltigs with non-hierarchical n-gram models and pcfgs , we show that pltig combines the best aspects of both , with language modeling capability comparable to n-grams , and improved parsing performance over its non-lexicalized counterpart . furthermore , training of pltigs displays faster convergence than pcfgs .", "topics": ["parsing"]}
{"title": "hardware acceleration for boolean satisfiability solver by applying belief propagation algorithm", "abstract": "boolean satisfiability ( sat ) has an extensive application domain in computer science , especially in electronic design automation applications . circuit synthesis , optimization , and verification problems can be solved by transforming original problems to sat problems . however , the sat problem is known as np-complete , which means there is no efficient method to solve it . therefore , an efficient sat solver to enhance the performance is always desired . we propose a hardware acceleration method for sat problems . by surveying the properties of sat problems and the decoding of low-density parity-check ( ldpc ) codes , a special class of error-correcting codes , we discover that both of them are constraint satisfaction problems . the belief propagation algorithm has been successfully applied to the decoding of ldpc , and the corresponding decoder hardware designs are extensively studied . therefore , we proposed a belief propagation based algorithm to solve sat problems . with this algorithm , the sat solver can be accelerated by hardware . a software simulator is implemented to verify the proposed algorithm and the performance improvement is estimated . our experiment results show that time complexity does not increase with the size of sat problems and the proposed method can achieve at least 30x speedup compared to minisat .", "topics": ["time complexity", "simulation"]}
{"title": "robust temporally coherent laplacian protrusion segmentation of 3d articulated bodies", "abstract": "in motion analysis and understanding it is important to be able to fit a suitable model or structure to the temporal series of observed data , in order to describe motion patterns in a compact way , and to discriminate between them . in an unsupervised context , i.e . , no prior model of the moving object ( s ) is available , such a structure has to be learned from the data in a bottom-up fashion . in recent times , volumetric approaches in which the motion is captured from a number of cameras and a voxel-set representation of the body is built from the camera views , have gained ground due to attractive features such as inherent view-invariance and robustness to occlusions . automatic , unsupervised segmentation of moving bodies along entire sequences , in a temporally-coherent and robust way , has the potential to provide a means of constructing a bottom-up model of the moving body , and track motion cues that may be later exploited for motion classification . spectral methods such as locally linear embedding ( lle ) can be useful in this context , as they preserve `` protrusions '' , i.e . , high-curvature regions of the 3d volume , of articulated shapes , while improving their separation in a lower dimensional space , making them in this way easier to cluster . in this paper we therefore propose a spectral approach to unsupervised and temporally-coherent body-protrusion segmentation along time sequences . volumetric shapes are clustered in an embedding space , clusters are propagated in time to ensure coherence , and merged or split to accommodate changes in the body 's topology . experiments on both synthetic and real sequences of dense voxel-set data are shown . this supports the ability of the proposed method to cluster body-parts consistently over time in a totally unsupervised fashion , its robustness to sampling density and shape quality , and its potential for bottom-up model construction", "topics": ["sampling ( signal processing )", "unsupervised learning"]}
{"title": "category enhanced word embedding", "abstract": "distributed word representations have been demonstrated to be effective in capturing semantic and syntactic regularities . unsupervised representation learning from large unlabeled corpora can learn similar representations for those words that present similar co-occurrence statistics . besides local occurrence statistics , global topical information is also important knowledge that may help discriminate a word from another . in this paper , we incorporate category information of documents in the learning of word representations and to learn the proposed models in a document-wise manner . our models outperform several state-of-the-art models in word analogy and word similarity tasks . moreover , we evaluate the learned word vectors on sentiment analysis and text classification tasks , which shows the superiority of our learned word vectors . we also learn high-quality category embeddings that reflect topical meanings .", "topics": ["feature learning", "unsupervised learning"]}
{"title": "policy search with high-dimensional context variables", "abstract": "direct contextual policy search methods learn to improve policy parameters and simultaneously generalize these parameters to different context or task variables . however , learning from high-dimensional context variables , such as camera images , is still a prominent problem in many real-world tasks . a naive application of unsupervised dimensionality reduction methods to the context variables , such as principal component analysis , is insufficient as task-relevant input may be ignored . in this paper , we propose a contextual policy search method in the model-based relative entropy stochastic search framework with integrated dimensionality reduction . we learn a model of the reward that is locally quadratic in both the policy parameters and the context variables . furthermore , we perform supervised linear dimensionality reduction on the context variables by nuclear norm regularization . the experimental results show that the proposed method outperforms naive dimensionality reduction via principal component analysis and a state-of-the-art contextual policy search method .", "topics": ["matrix regularization"]}
{"title": "extended comment on language trees and zipping", "abstract": "this is the extended version of a comment submitted to physical review letters . i first point out the inappropriateness of publishing a letter unrelated to physics . next , i give experimental results showing that the technique used in the letter is 3 times worse and 17 times slower than a simple baseline . and finally , i review the literature , showing that the ideas of the letter are not novel . i conclude by suggesting that physical review letters should not publish letters unrelated to physics .", "topics": ["baseline ( configuration management )", "time series"]}
{"title": "bridge the gap between group sparse coding and rank minimization via adaptive dictionary learning", "abstract": "both sparse coding and rank minimization have led to great successes in various image processing tasks . though the underlying principles of these two approaches are similar , no theory is available to demonstrate the correspondence . in this paper , starting by designing an adaptive dictionary for each group of image patches , we analyze the sparsity of image patches in each group using the rank minimization approach . based on this , we prove that the group-based sparse coding is equivalent to the rank minimization problem under our proposed adaptive dictionary . therefore , the sparsity of the coefficients for each group can be measured by estimating the singular values of this group . inspired by our theoretical analysis , four nuclear norm like minimization methods including the standard nuclear norm minimization ( nnm ) , weighted nuclear norm minimization ( wnnm ) , schatten $ p $ -norm minimization ( snm ) , and weighted schatten $ p $ -norm minimization ( wsnm ) , are employed to analyze the sparsity of the coefficients and wsnm is found to be the closest solution to the singular values of each group . based on this , wsnm is then translated to a non-convex weighted $ \\ell_p $ -norm minimization problem in group-based sparse coding , and in order to solve this problem , a new algorithm based on the alternating direction method of multipliers ( admm ) framework is developed . experimental results on two low-level vision tasks : image inpainting and image compressive sensing recovery , demonstrate that the proposed scheme is feasible and outperforms state-of-the-art methods .", "topics": ["image processing", "high- and low-level"]}
{"title": "multivariate normal mixture modeling , clustering and classification with the rebmix package", "abstract": "the rebmix package provides r functions for random univariate and multivariate finite mixture model generation , estimation , clustering and classification . the paper is focused on multivariate normal mixture models with unrestricted variance-covariance matrices . the objective is to show how to generate datasets for a known number of components , numbers of observations and component parameters , how to estimate the number of components , component weights and component parameters and how to predict cluster and class membership based upon a model trained by the rebmix algorithm . the accompanying plotting , bootstrapping and other features of the package are dealt with , too . for demonstration purpose a multivariate normal dataset with unrestricted variance-covariance matrices is studied .", "topics": ["cluster analysis", "statistical classification"]}
{"title": "piecewise training for undirected models", "abstract": "for many large undirected models that arise in real-world applications , exact maximumlikelihood training is intractable , because it requires computing marginal distributions of the model . conditional training is even more difficult , because the partition function depends not only on the parameters , but also on the observed input , requiring repeated inference over each training example . an appealing idea for such models is to independently train a local undirected classifier over each clique , afterwards combining the learned weights into a single global model . in this paper , we show that this piecewise method can be justified as minimizing a new family of upper bounds on the log partition function . on three natural-language data sets , piecewise training is more accurate than pseudolikelihood , and often performs comparably to global training using belief propagation .", "topics": ["natural language"]}
{"title": "multi-domain collaborative filtering", "abstract": "collaborative filtering is an effective recommendation approach in which the preference of a user on an item is predicted based on the preferences of other users with similar interests . a big challenge in using collaborative filtering methods is the data sparsity problem which often arises because each user typically only rates very few items and hence the rating matrix is extremely sparse . in this paper , we address this problem by considering multiple collaborative filtering tasks in different domains simultaneously and exploiting the relationships between domains . we refer to it as a multi-domain collaborative filtering ( mcf ) problem . to solve the mcf problem , we propose a probabilistic framework which uses probabilistic matrix factorization to model the rating problem in each domain and allows the knowledge to be adaptively transferred across different domains by automatically learning the correlation between domains . we also introduce the link function for different domains to correct their biases . experiments conducted on several real-world applications demonstrate the effectiveness of our methods when compared with some representative methods .", "topics": ["sparse matrix"]}
{"title": "the cyborg astrobiologist : porting from a wearable computer to the astrobiology phone-cam", "abstract": "we have used a simple camera phone to significantly improve an `exploration system ' for astrobiology and geology . this camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration . we envision that the `astrobiology phone-cam ' exploration system can be fruitfully used in other problem domains as well .", "topics": ["computer vision"]}
{"title": "applications of data mining ( dm ) in science and engineering : state of the art and perspectives", "abstract": "the continuous increase in the availability of data of any kind , coupled with the development of networks of high-speed communications , the popularization of cloud computing and the growth of data centers and the emergence of high-performance computing does essential the task to develop techniques that allow more efficient data processing and analyzing of large volumes datasets and extraction of valuable information . in the following pages we will discuss about development of this field in recent decades , and its potential and applicability present in the various branches of scientific research . also , we try to review briefly the different families of algorithms that are included in data mining research area , its scalability with increasing dimensionality of the input data and how they can be addressed and what behavior different methods in a scenario in which the information is distributed or decentralized processed so as to increment performance optimization in heterogeneous environments .", "topics": ["data mining"]}
{"title": "bayesian active learning for classification and preference learning", "abstract": "information theoretic active learning has been widely studied for probabilistic models . for simple regression an optimal myopic policy is easily tractable . however , for other tasks and with more complex models , such as classification with nonparametric models , the optimal solution is harder to compute . current approaches make approximations to achieve tractability . we propose an approach that expresses information gain in terms of predictive entropies , and apply this method to the gaussian process classifier ( gpc ) . our approach makes minimal approximations to the full information theoretic objective . our experimental performance compares favourably to many popular active learning algorithms , and has equal or lower computational complexity . we compare well to decision theoretic approaches also , which are privy to more information and require much more computational time . secondly , by developing further a reformulation of binary preference learning to a classification problem , we extend our algorithm to gaussian process preference learning .", "topics": ["computational complexity theory", "optimization problem"]}
{"title": "fast sampling for strongly rayleigh measures with application to determinantal point processes", "abstract": "in this note we consider sampling from ( non-homogeneous ) strongly rayleigh probability measures . as an important corollary , we obtain a fast mixing markov chain sampler for determinantal point processes .", "topics": ["sampling ( signal processing )", "markov chain"]}
{"title": "belief change with noisy sensing in the situation calculus", "abstract": "situation calculus has been applied widely in artificial intelligence to model and reason about actions and changes in dynamic systems . since actions carried out by agents will cause constant changes of the agents ' beliefs , how to manage these changes is a very important issue . shapiro et al . [ 22 ] is one of the studies that considered this issue . however , in this framework , the problem of noisy sensing , which often presents in real-world applications , is not considered . as a consequence , noisy sensing actions in this framework will lead to an agent facing inconsistent situation and subsequently the agent can not proceed further . in this paper , we investigate how noisy sensing actions can be handled in iterated belief change within the situation calculus formalism . we extend the framework proposed in [ 22 ] with the capability of managing noisy sensings . we demonstrate that an agent can still detect the actual situation when the ratio of noisy sensing actions vs. accurate sensing actions is limited . we prove that our framework subsumes the iterated belief change strategy in [ 22 ] when all sensing actions are accurate . furthermore , we prove that our framework can adequately handle belief introspection , mistaken beliefs , belief revision and belief update even with noisy sensing , as done in [ 22 ] with accurate sensing actions only .", "topics": ["artificial intelligence"]}
{"title": "convolutional neural networks for text categorization : shallow word-level vs . deep character-level", "abstract": "this paper reports the performances of shallow word-level convolutional neural networks ( cnn ) , our earlier work ( 2015 ) , on the eight datasets with relatively large training data that were used for testing the very deep character-level cnn in conneau et al . ( 2016 ) . our findings are as follows . the shallow word-level cnns achieve better error rates than the error rates reported in conneau et al . , though the results should be interpreted with some consideration due to the unique pre-processing of conneau et al . the shallow word-level cnn uses more parameters and therefore requires more storage than the deep character-level cnn ; however , the shallow word-level cnn computes much faster .", "topics": ["test set"]}
{"title": "on data-independent properties for density-based dissimilarity measures in hybrid clustering", "abstract": "hybrid clustering combines partitional and hierarchical clustering for computational effectiveness and versatility in cluster shape . in such clustering , a dissimilarity measure plays a crucial role in the hierarchical merging . the dissimilarity measure has great impact on the final clustering , and data-independent properties are needed to choose the right dissimilarity measure for the problem at hand . properties for distance-based dissimilarity measures have been studied for decades , but properties for density-based dissimilarity measures have so far received little attention . here , we propose six data-independent properties to evaluate density-based dissimilarity measures associated with hybrid clustering , regarding equality , orthogonality , symmetry , outlier and noise observations , and light-tailed models for heavy-tailed clusters . the significance of the properties is investigated , and we study some well-known dissimilarity measures based on shannon entropy , misclassification rate , bhattacharyya distance and kullback-leibler divergence with respect to the proposed properties . as none of them satisfy all the proposed properties , we introduce a new dissimilarity measure based on the kullback-leibler information and show that it satisfies all proposed properties . the effect of the proposed properties is also illustrated on several real and simulated data sets .", "topics": ["cluster analysis", "simulation"]}
{"title": "application of data mining techniques to a selected business organisation with special reference to buying behaviour", "abstract": "data mining is a new concept & an exploration and analysis of large data sets , in order to discover meaningful patterns and rules . many organizations are now using the data mining techniques to find out meaningful patterns from the database . the present paper studies how data mining techniques can be apply to the large database . these data mining techniques give certain behavioral pattern from the database . the results which come after analysis of the database are useful for organization . this paper examines the result after applying association rule mining technique , rule induction technique and apriori algorithm . these techniques are applied to the database of shopping mall . market basket analysis is performing by the above mentioned techniques and some important results are found such as buying behavior .", "topics": ["data mining"]}
{"title": "robustness and regularization of support vector machines", "abstract": "we consider regularized support vector machines ( svms ) and show that they are precisely equivalent to a new robust optimization formulation . we show that this equivalence of robust optimization and regularization has implications for both algorithms , and analysis . in terms of algorithms , the equivalence suggests more general svm-like algorithms for classification that explicitly build in protection to noise , and at the same time control overfitting . on the analysis front , the equivalence of robustness and regularization , provides a robust optimization interpretation for the success of regularized svms . we use the this new robustness interpretation of svms to give a new proof of consistency of ( kernelized ) svms , thus establishing robustness as the reason regularized svms generalize well .", "topics": ["support vector machine", "matrix regularization"]}
{"title": "deep quantization : encoding convolutional activations with deep generative model", "abstract": "deep convolutional neural networks ( cnns ) have proven highly effective for visual recognition , where learning a universal representation from activations of convolutional layer plays a fundamental problem . in this paper , we present fisher vector encoding with variational auto-encoder ( fv-vae ) , a novel deep architecture that quantizes the local activations of convolutional layer in a deep generative model , by training them in an end-to-end manner . to incorporate fv encoding strategy into deep generative models , we introduce variational auto-encoder model , which steers a variational inference and learning in a neural network which can be straightforwardly optimized using standard stochastic gradient method . different from the fv characterized by conventional generative models ( e.g . , gaussian mixture model ) which parsimoniously fit a discrete mixture model to data distribution , the proposed fv-vae is more flexible to represent the natural property of data for better generalization . extensive experiments are conducted on three public datasets , i.e . , ucf101 , activitynet , and cub-200-2011 in the context of video action recognition and fine-grained image classification , respectively . superior results are reported when compared to state-of-the-art representations . most remarkably , our proposed fv-vae achieves to-date the best published accuracy of 94.2 % on ucf101 .", "topics": ["generative model", "calculus of variations"]}
{"title": "cognitive aging as interplay between hebbian learning and criticality", "abstract": "cognitive ageing seems to be a story of global degradation . as one ages there are a number of physical , chemical and biological changes that take place . therefore it is logical to assume that the brain is no exception to this phenomenon . the principle purpose of this project is to use models of neural dynamics and learning based on the underlying principle of self-organised criticality , to account for the age related cognitive effects . in this regard learning in neural networks can serve as a model for the acquisition of skills and knowledge in early development stages i.e . the ageing process and criticality in the network serves as the optimum state of cognitive abilities . possible candidate mechanisms for ageing in a neural network are loss of connectivity and neurons , increase in the level of noise , reduction in white matter or more interestingly longer learning history and the competition among several optimization objectives . in this paper we are primarily interested in the affect of the longer learning history on memory and thus the optimality in the brain . hence it is hypothesized that prolonged learning in the form of associative memory patterns can destroy the state of criticality in the network . we base our model on tsodyks and markrams [ 49 ] model of dynamic synapses , in the process to explore the effect of combining standard hebbian learning with the phenomenon of self-organised criticality . the project mainly consists of evaluations and simulations of networks of integrate and fire-neurons that have been subjected to various combinations of neural-level ageing effects , with the aim of establishing the primary hypothesis and understanding the decline of cognitive abilities due to ageing , using one of its important characteristics , a longer learning history .", "topics": ["simulation"]}
{"title": "time-dependent representation for neural event sequence prediction", "abstract": "existing sequence prediction methods are mostly concerned with time-independent sequences , in which the actual time span between events is irrelevant and the distance between events is simply the difference between their order positions in the sequence . while this time-independent view of sequences is applicable for data such as natural languages , e.g . , dealing with words in a sentence , it is inappropriate and inefficient for many real world events that are observed and collected at unequally spaced points of time as they naturally arise , e.g . , when a person goes to a grocery store or makes a phone call . the time span between events can carry important information about the sequence dependence of human behaviors . in this work , we propose a set of methods for using time in sequence prediction . because neural sequence models such as rnn are more amenable for handling token-like input , we propose two methods for time-dependent event representation , based on the intuition on how time is tokenized in everyday life and previous work on embedding contextualization . we also introduce two methods for using next event duration as regularization for training a sequence prediction model . we discuss these methods based on recurrent neural nets . we evaluate these methods as well as baseline models on five datasets that resemble a variety of sequence prediction tasks . the experiments revealed that the proposed methods offer accuracy gain over baseline models in a range of settings .", "topics": ["baseline ( configuration management )", "natural language"]}
{"title": "hybrid fuzzy logic and pid controller based ph neutralization pilot plant", "abstract": "use of control theory within process control industries has changed rapidly due to the increase complexity of instrumentation , real time requirements , minimization of operating costs and highly nonlinear characteristics of chemical process . previously developed process control technologies which are mostly based on a single controller are not efficient in terms of signal transmission delays , processing power for computational needs and signal to noise ratio . hybrid controller with efficient system modelling is essential to cope with the current challenges of process control in terms of control performance . this paper presents an optimized mathematical modelling and advance hybrid controller ( fuzzy logic and pid ) design along with practical implementation and validation of ph neutralization pilot plant . this procedure is particularly important for control design and automation of physico-chemical systems for process control industry .", "topics": ["nonlinear system"]}
{"title": "flexpoint : an adaptive numerical format for efficient training of deep neural networks", "abstract": "deep neural networks are commonly developed and trained in 32-bit floating point format . significant gains in performance and energy efficiency could be realized by training and inference in numerical formats optimized for deep learning . despite advances in limited precision inference in recent years , training of neural networks in low bit-width remains a challenging problem . here we present the flexpoint data format , aiming at a complete replacement of 32-bit floating point format training and inference , designed to support modern deep network topologies without modifications . flexpoint tensors have a shared exponent that is dynamically adjusted to minimize overflows and maximize available dynamic range . we validate flexpoint by training alexnet , a deep residual network and a generative adversarial network , using a simulator implemented with the neon deep learning framework . we demonstrate that 16-bit flexpoint closely matches 32-bit floating point in training all three models , without any need for tuning of model hyperparameters . our results suggest flexpoint as a promising numerical format for future hardware for training and inference .", "topics": ["neural networks", "numerical analysis"]}
{"title": "stochastic structured prediction under bandit feedback", "abstract": "stochastic structured prediction under bandit feedback follows a learning protocol where on each of a sequence of iterations , the learner receives an input , predicts an output structure , and receives partial feedback in form of a task loss evaluation of the predicted structure . we present applications of this learning scenario to convex and non-convex objectives for structured prediction and analyze them as stochastic first-order methods . we present an experimental evaluation on problems of natural language processing over exponential output spaces , and compare convergence speed across different objectives under the practical criterion of optimal task performance on development data and the optimization-theoretic criterion of minimal squared gradient norm . best results under both criteria are obtained for a non-convex objective for pairwise preference learning under bandit feedback .", "topics": ["natural language processing", "time complexity"]}
{"title": "fast learning rates with heavy-tailed losses", "abstract": "we study fast learning rates when the losses are not necessarily bounded and may have a distribution with heavy tails . to enable such analyses , we introduce two new conditions : ( i ) the envelope function $ \\sup_ { f \\in \\mathcal { f } } |\\ell \\circ f| $ , where $ \\ell $ is the loss function and $ \\mathcal { f } $ is the hypothesis class , exists and is $ l^r $ -integrable , and ( ii ) $ \\ell $ satisfies the multi-scale bernstein 's condition on $ \\mathcal { f } $ . under these assumptions , we prove that learning rate faster than $ o ( n^ { -1/2 } ) $ can be obtained and , depending on $ r $ and the multi-scale bernstein 's powers , can be arbitrarily close to $ o ( n^ { -1 } ) $ . we then verify these assumptions and derive fast learning rates for the problem of vector quantization by $ k $ -means clustering with heavy-tailed distributions . the analyses enable us to obtain novel learning rates that extend and complement existing results in the literature from both theoretical and practical viewpoints .", "topics": ["cluster analysis", "loss function"]}
{"title": "multimodal latent variable analysis", "abstract": "consider a set of multiple , multimodal sensors capturing a complex system or a physical phenomenon of interest . our primary goal is to distinguish the underlying sources of variability manifested in the measured data . the first step in our analysis is to find the common source of variability present in all sensor measurements . we base our work on a recent paper , which tackles this problem with alternating diffusion ( ad ) . in this work , we suggest to further the analysis by extracting the sensor-specific variables in addition to the common source . we propose an algorithm , which we analyze theoretically , and then demonstrate on three different applications : a synthetic example , a toy problem , and the task of fetal ecg extraction .", "topics": ["synthetic data", "sensor"]}
{"title": "tighter lifting-free convex relaxations for quadratic matching problems", "abstract": "in this work we study convex relaxations of quadratic optimisation problems over permutation matrices . while existing semidefinite programming approaches can achieve remarkably tight relaxations , they have the strong disadvantage that they lift the original $ n { \\times } n $ -dimensional variable to an $ n^2 { \\times } n^2 $ -dimensional variable , which limits their practical applicability . in contrast , here we present a lifting-free convex relaxation that is provably at least as tight as existing ( lifting-free ) convex relaxations . we demonstrate experimentally that our approach is superior to existing convex and non-convex methods for various problems , including image arrangement and multi-graph matching .", "topics": ["mathematical optimization"]}
{"title": "online categorical subspace learning for sketching big data with misses", "abstract": "with the scale of data growing every day , reducing the dimensionality ( a.k.a . sketching ) of high-dimensional data has emerged as a task of paramount importance . relevant issues to address in this context include the sheer volume of data that may consist of categorical samples , the typically streaming format of acquisition , and the possibly missing entries . to cope with these challenges , the present paper develops a novel categorical subspace learning approach to unravel the latent structure for three prominent categorical ( bilinear ) models , namely , probit , tobit , and logit . the deterministic probit and tobit models treat data as quantized values of an analog-valued process lying in a low-dimensional subspace , while the probabilistic logit model relies on low dimensionality of the data log-likelihood ratios . leveraging the low intrinsic dimensionality of the sought models , a rank regularized maximum-likelihood estimator is devised , which is then solved recursively via alternating majorization-minimization to sketch high-dimensional categorical data `on the fly . ' the resultant procedure alternates between sketching the new incomplete datum and refining the latent subspace , leading to lightweight first-order algorithms with highly parallelizable tasks per iteration . as an extra degree of freedom , the quantization thresholds are also learned jointly along with the subspace to enhance the predictive power of the sought models . performance of the subspace iterates is analyzed for both infinite and finite data streams , where for the former asymptotic convergence to the stationary point set of the batch estimator is established , while for the latter sublinear regret bounds are derived for the empirical cost . simulated tests with both synthetic and real-world datasets corroborate the merits of the novel schemes for real-time movie recommendation and chess-game classification .", "topics": ["regret ( decision theory )", "synthetic data"]}
{"title": "greedy approximation in convex optimization", "abstract": "we study sparse approximate solutions to convex optimization problems . it is known that in many engineering applications researchers are interested in an approximate solution of an optimization problem as a linear combination of elements from a given system of elements . there is an increasing interest in building such sparse approximate solutions using different greedy-type algorithms . the problem of approximation of a given element of a banach space by linear combinations of elements from a given system ( dictionary ) is well studied in nonlinear approximation theory . at a first glance the settings of approximation and optimization problems are very different . in the approximation problem an element is given and our task is to find a sparse approximation of it . in optimization theory an energy function is given and we should find an approximate sparse solution to the minimization problem . it turns out that the same technique can be used for solving both problems . we show how the technique developed in nonlinear approximation theory , in particular , the greedy approximation technique can be adjusted for finding a sparse solution of an optimization problem .", "topics": ["approximation algorithm", "optimization problem"]}
{"title": "the danger theory and its application to artificial immune systems", "abstract": "over the last decade , a new idea challenging the classical self-non-self viewpoint has become popular amongst immunologists . it is called the danger theory . in this conceptual paper , we look at this theory from the perspective of artificial immune system practitioners . an overview of the danger theory is presented with particular emphasis on analogies in the artificial immune systems world . a number of potential application areas are then used to provide a framing for a critical assessment of the concept , and its relevance for artificial immune systems .", "topics": ["relevance"]}
{"title": "semantic segmentation of trajectories with agent models", "abstract": "in many cases , such as trajectories clustering and classification , we often divide a trajectory into segments as preprocessing . in this paper , we propose a trajectory semantic segmentation method based on learned behavior models . in the proposed method , we learn some behavior models from video sequences . next , using learned behavior models and a hidden markov model , we segment a trajectory into semantic segments . comparing with the ramer-douglas-peucker algorithm , we show the effectiveness of the proposed method .", "topics": ["cluster analysis", "statistical classification"]}
{"title": "comparing learning algorithms in neural network for diagnosing cardiovascular disease", "abstract": "today data mining techniques are exploited in medical science for diagnosing , overcoming and treating diseases . neural network is one of the techniques which are widely used for diagnosis in medical field . in this article efficiency of nine algorithms , which are basis of neural network learning in diagnosing cardiovascular diseases , will be assessed . algorithms are assessed in terms of accuracy , sensitivity , transparency , aroc and convergence rate by means of 10 fold cross validation . the results suggest that in training phase , lonberg-m algorithm has the best efficiency in terms of all metrics , algorithm oss has maximum accuracy in testing phase , algorithm scg has the maximum transparency and algorithm cgb has the maximum sensitivity .", "topics": ["data mining"]}
{"title": "what does attention in neural machine translation pay attention to ?", "abstract": "attention in neural machine translation provides the possibility to encode relevant parts of the source sentence at each translation step . as a result , attention is considered to be an alignment model as well . however , there is no work that specifically studies attention and provides analysis of what is being learned by attention models . thus , the question still remains that how attention is similar or different from the traditional alignment . in this paper , we provide detailed analysis of attention and compare it to traditional alignment . we answer the question of whether attention is only capable of modelling translational equivalent or it captures more information . we show that attention is different from alignment in some cases and is capturing useful information other than alignments .", "topics": ["machine translation"]}
{"title": "mask-cnn : localizing parts and selecting descriptors for fine-grained image recognition", "abstract": "fine-grained image recognition is a challenging computer vision problem , due to the small inter-class variations caused by highly similar subordinate categories , and the large intra-class variations in poses , scales and rotations . in this paper , we propose a novel end-to-end mask-cnn model without the fully connected layers for fine-grained recognition . based on the part annotations of fine-grained images , the proposed model consists of a fully convolutional network to both locate the discriminative parts ( e.g . , head and torso ) , and more importantly generate object/part masks for selecting useful and meaningful convolutional descriptors . after that , a four-stream mask-cnn model is built for aggregating the selected object- and part-level descriptors simultaneously . the proposed mask-cnn model has the smallest number of parameters , lowest feature dimensionality and highest recognition accuracy when compared with state-of-the-arts fine-grained approaches .", "topics": ["computer vision", "end-to-end principle"]}
{"title": "speech recognition front end without information loss", "abstract": "speech representation and modelling in high-dimensional spaces of acoustic waveforms , or a linear transformation thereof , is investigated with the aim of improving the robustness of automatic speech recognition to additive noise . the motivation behind this approach is twofold : ( i ) the information in acoustic waveforms that is usually removed in the process of extracting low-dimensional features might aid robust recognition by virtue of structured redundancy analogous to channel coding , ( ii ) linear feature domains allow for exact noise adaptation , as opposed to representations that involve non-linear processing which makes noise adaptation challenging . thus , we develop a generative framework for phoneme modelling in high-dimensional linear feature domains , and use it in phoneme classification and recognition tasks . results show that classification and recognition in this framework perform better than analogous plp and mfcc classifiers below 18 db snr . a combination of the high-dimensional and mfcc features at the likelihood level performs uniformly better than either of the individual representations across all noise levels .", "topics": ["nonlinear system", "speech recognition"]}
{"title": "runtime analysis of probabilistic crowding and restricted tournament selection for bimodal optimisation", "abstract": "many real optimisation problems lead to multimodal domains and so require the identification of multiple optima . niching methods have been developed to maintain the population diversity , to investigate many peaks in parallel and to reduce the effect of genetic drift . using rigorous runtime analysis , we analyse for the first time two well known niching methods : probabilistic crowding and restricted tournament selection ( rts ) . we incorporate both methods into a $ ( \\mu+1 ) ~ea $ on the bimodal function twomax where the goal is to find two optima at opposite ends of the search space . in probabilistic crowding , the offspring compete with their parents and the survivor is chosen proportionally to its fitness . on twomax probabilistic crowding fails to find any reasonable solution quality even in exponential time . in rts the offspring compete against the closest individual amongst $ w $ ( window size ) individuals . we prove that rts fails if $ w $ is too small , leading to exponential times with high probability . however , if w is chosen large enough , it finds both optima for twomax in time $ o ( \\mu n \\log { n } ) $ with high probability . our theoretical results are accompanied by experimental studies that match the theoretical results and also shed light on parameters not covered by the theoretical results .", "topics": ["time complexity", "mathematical optimization"]}
{"title": "gradual training of deep denoising auto encoders", "abstract": "stacked denoising auto encoders ( daes ) are well known to learn useful deep representations , which can be used to improve supervised training by initializing a deep network . we investigate a training scheme of a deep dae , where dae layers are gradually added and keep adapting as additional layers are added . we show that in the regime of mid-sized datasets , this gradual training provides a small but consistent improvement over stacked training in both reconstruction quality and classification error over stacked training on mnist and cifar datasets .", "topics": ["noise reduction", "encoder"]}
{"title": "analyzing learned convnet features with dirichlet process gaussian mixture models", "abstract": "convolutional neural networks ( convnets ) have achieved good results in a range of computer vision tasks the recent years . though given a lot of attention , visualizing the learned representations to interpret convnets , still remains a challenging task . the high dimensionality of internal representations and the high abstractions of deep layers are the main challenges when visualizing convnet functionality . we present in this paper a technique based on clustering internal convnet representations with a dirichlet process gaussian mixture model , for visualization of learned representations in convnets . our method copes with the high dimensionality of a convnet by clustering representations across all nodes of each layer . we will discuss how this application is useful when considering transfer learning , i.e.\\ transferring a model trained on one dataset to solve a task on a different one .", "topics": ["cluster analysis", "computer vision"]}
{"title": "on the role of canonicity in bottom-up knowledge compilation", "abstract": "we consider the problem of bottom-up compilation of knowledge bases , which is usually predicated on the existence of a polytime function for combining compilations using boolean operators ( usually called an apply function ) . while such a polytime apply function is known to exist for certain languages ( e.g . , obdds ) and not exist for others ( e.g . , dnnf ) , its existence for certain languages remains unknown . among the latter is the recently introduced language of sentential decision diagrams ( sdds ) , for which a polytime apply function exists for unreduced sdds , but remains unknown for reduced ones ( i.e . canonical sdds ) . we resolve this open question in this paper and consider some of its theoretical and practical implications . some of the findings we report question the common wisdom on the relationship between bottom-up compilation , language canonicity and the complexity of the apply function .", "topics": ["time complexity"]}
{"title": "selective image super-resolution", "abstract": "in this paper we propose a vision system that performs image super resolution ( sr ) with selectivity . conventional sr techniques , either by multi-image fusion or example-based construction , have failed to capitalize on the intrinsic structural and semantic context in the image , and performed `` blind '' resolution recovery to the entire image area . by comparison , we advocate example-based selective sr whereby selectivity is exemplified in three aspects : region selectivity ( sr only at object regions ) , source selectivity ( object sr with trained object dictionaries ) , and refinement selectivity ( object boundaries refinement using matting ) . the proposed system takes over-segmented low-resolution images as inputs , assimilates recent learning techniques of sparse coding ( sc ) and grouped multi-task lasso ( gmtl ) , and leads eventually to a framework for joint figure-ground separation and interest object sr . the efficiency of our framework is manifested in our experiments with subsets of the voc2009 and msrc datasets . we also demonstrate several interesting vision applications that can build on our system .", "topics": ["sparse matrix", "dictionary"]}
{"title": "consistent on-line off-policy evaluation", "abstract": "the problem of on-line off-policy evaluation ( ope ) has been actively studied in the last decade due to its importance both as a stand-alone problem and as a module in a policy improvement scheme . however , most temporal difference ( td ) based solutions ignore the discrepancy between the stationary distribution of the behavior and target policies and its effect on the convergence limit when function approximation is applied . in this paper we propose the consistent off-policy temporal difference ( cop-td ( $ \\lambda $ , $ \\beta $ ) ) algorithm that addresses this issue and reduces this bias at some computational expense . we show that cop-td ( $ \\lambda $ , $ \\beta $ ) can be designed to converge to the same value that would have been obtained by using on-policy td ( $ \\lambda $ ) with the target policy . subsequently , the proposed scheme leads to a related and promising heuristic we call log-cop-td ( $ \\lambda $ , $ \\beta $ ) . both algorithms have favorable empirical results to the current state of the art on-line ope algorithms . finally , our formulation sheds some new light on the recently proposed emphatic td learning .", "topics": ["heuristic"]}
{"title": "automatic mapping of french discourse connectives to pdtb discourse relations", "abstract": "in this paper , we present an approach to exploit phrase tables generated by statistical machine translation in order to map french discourse connectives to discourse relations . using this approach , we created concoledisco , a lexicon of french discourse connectives and their pdtb relations . when evaluated against lexconn , concoledisco achieves a recall of 0.81 and an average precision of 0.68 for the concession and condition relations .", "topics": ["machine translation"]}
{"title": "brain-inspired photonic signal processor for periodic pattern generation and chaotic system emulation", "abstract": "reservoir computing is a bio-inspired computing paradigm for processing time-dependent signals . its hardware implementations have received much attention because of their simplicity and remarkable performance on a series of benchmark tasks . in previous experiments the output was uncoupled from the system and in most cases simply computed offline on a post-processing computer . however , numerical investigations have shown that feeding the output back into the reservoir would open the possibility of long-horizon time series forecasting . here we present a photonic reservoir computer with output feedback , and demonstrate its capacity to generate periodic time series and to emulate chaotic systems . we study in detail the effect of experimental noise on system performance . in the case of chaotic systems , this leads us to introduce several metrics , based on standard signal processing techniques , to evaluate the quality of the emulation . our work significantly enlarges the range of tasks that can be solved by hardware reservoir computers , and therefore the range of applications they could potentially tackle . it also raises novel questions in nonlinear dynamics and chaos theory .", "topics": ["time series", "nonlinear system"]}
{"title": "when are tree structures necessary for deep learning of representations ?", "abstract": "recursive neural models , which use syntactic parse trees to recursively generate representations bottom-up , are a popular architecture . but there have not been rigorous evaluations showing for exactly which tasks this syntax-based method is appropriate . in this paper we benchmark { \\bf recursive } neural models against sequential { \\bf recurrent } neural models ( simple recurrent and lstm models ) , enforcing apples-to-apples comparison as much as possible . we investigate 4 tasks : ( 1 ) sentiment classification at the sentence level and phrase level ; ( 2 ) matching questions to answer-phrases ; ( 3 ) discourse parsing ; ( 4 ) semantic relation extraction ( e.g . , { \\em component-whole } between nouns ) . our goal is to understand better when , and why , recursive models can outperform simpler models . we find that recursive models help mainly on tasks ( like semantic relation extraction ) that require associating headwords across a long distance , particularly on very long sequences . we then introduce a method for allowing recurrent models to achieve similar performance : breaking long sentences into clause-like units at punctuation and processing them separately before combining . our results thus help understand the limitations of both classes of models , and suggest directions for improving recurrent models .", "topics": ["parsing"]}
{"title": "non-adaptive learning a hidden hipergraph", "abstract": "we give a new deterministic algorithm that non-adaptively learns a hidden hypergraph from edge-detecting queries . all previous non-adaptive algorithms either run in exponential time or have non-optimal query complexity . we give the first polynomial time non-adaptive learning algorithm for learning hypergraph that asks almost optimal number of queries .", "topics": ["time complexity", "polynomial"]}
{"title": "reasoning with topological and directional spatial information", "abstract": "current research on qualitative spatial representation and reasoning mainly focuses on one single aspect of space . in real world applications , however , multiple spatial aspects are often involved simultaneously . this paper investigates problems arising in reasoning with combined topological and directional information . we use the rcc8 algebra and the rectangle algebra ( ra ) for expressing topological and directional information respectively . we give examples to show that the bipath-consistency algorithm bipath is incomplete for solving even basic rcc8 and ra constraints . if topological constraints are taken from some maximal tractable subclasses of rcc8 , and directional constraints are taken from a subalgebra , termed dir49 , of ra , then we show that bipath is able to separate topological constraints from directional ones . this means , given a set of hybrid topological and directional constraints from the above subclasses of rcc8 and ra , we can transfer the joint satisfaction problem in polynomial time to two independent satisfaction problems in rcc8 and ra . for general ra constraints , we give a method to compute solutions that satisfy all topological constraints and approximately satisfy each ra constraint to any prescribed precision .", "topics": ["time complexity"]}
{"title": "code similarity on high level programs", "abstract": "this paper presents a new approach for code similarity on high level programs . our technique is based on fast dynamic time warping , that builds a warp path or points relation with local restrictions . the source code is represented into time series using the operators inside programming languages that makes possible the comparison . this makes possible subsequence detection that represent similar code instructions . in contrast with other code similarity algorithms , we do not make features extraction . the experiments show that two source codes are similar when their respective time series are similar .", "topics": ["time series"]}
{"title": "understanding non-optical remote-sensed images : needs , challenges and ways forward", "abstract": "non-optical remote-sensed images are going to be used more often in man- aging disaster , crime and precision agriculture . with more small satellites and unmanned air vehicles planning to carry radar and hyperspectral image sensors there is going to be an abundance of such data in the recent future . understanding these data in real-time will be crucial in attaining some of the important sustain- able development goals . processing non-optical images is , in many ways , different from that of optical images . most of the recent advances in the domain of image understanding has been using optical images . in this article we shall explain the needs for image understanding in non-optical domain and the typical challenges . then we shall describe the existing approaches and how we can move from there to the desired goal of a reliable real-time image understanding system .", "topics": ["computer vision", "sensor"]}
{"title": "program induction by rationale generation : learning to solve and explain algebraic word problems", "abstract": "solving algebraic word problems requires executing a series of arithmetic operations -- -a program -- -to obtain a final answer . however , since programs can be arbitrarily complicated , inducing them directly from question-answer pairs is a formidable challenge . to make this task more feasible , we solve these problems by generating answer rationales , sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps . although rationales do not explicitly specify programs , they provide a scaffolding for their structure via intermediate milestones . to evaluate our approach , we have created a new 100,000-sample dataset of questions , answers and rationales . experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs .", "topics": ["natural language"]}
{"title": "an open diachronic corpus of historical spanish : annotation criteria and automatic modernisation of spelling", "abstract": "the impact-es diachronic corpus of historical spanish compiles over one hundred books -- containing approximately 8 million words -- in addition to a complementary lexicon which links more than 10 thousand lemmas with attestations of the different variants found in the documents . this textual corpus and the accompanying lexicon have been released under an open license ( creative commons by-nc-sa ) in order to permit their intensive exploitation in linguistic research . approximately 7 % of the words in the corpus ( a selection aimed at enhancing the coverage of the most frequent word forms ) have been annotated with their lemma , part of speech , and modern equivalent . this paper describes the annotation criteria followed and the standards , based on the text encoding initiative recommendations , used to the represent the texts in digital form . as an illustration of the possible synergies between diachronic textual resources and linguistic research , we describe the application of statistical machine translation techniques to infer probabilistic context-sensitive rules for the automatic modernisation of spelling . the automatic modernisation with this type of statistical methods leads to very low character error rates when the output is compared with the supervised modern version of the text .", "topics": ["machine translation", "text corpus"]}
{"title": "adaptive laplace mechanism : differential privacy preservation in deep learning", "abstract": "in this paper , we focus on developing a novel mechanism to preserve differential privacy in deep neural networks , such that : ( 1 ) the privacy budget consumption is totally independent of the number of training steps ; ( 2 ) it has the ability to adaptively inject noise into features based on the contribution of each to the output ; and ( 3 ) it could be applied in a variety of different deep neural networks . to achieve this , we figure out a way to perturb affine transformations of neurons , and loss functions used in deep neural networks . in addition , our mechanism intentionally adds `` more noise '' into features which are `` less relevant '' to the model output , and vice-versa . our theoretical analysis further derives the sensitivities and error bounds of our mechanism . rigorous experiments conducted on mnist and cifar-10 datasets show that our mechanism is highly effective and outperforms existing solutions .", "topics": ["loss function", "mnist database"]}
{"title": "an additive model view to sparse gaussian process classifier design", "abstract": "we consider the problem of designing a sparse gaussian process classifier ( sgpc ) that generalizes well . viewing sgpc design as constructing an additive model like in boosting , we present an efficient and effective sgpc design method to perform a stage-wise optimization of a predictive loss function . we introduce new methods for two key components viz . , site parameter estimation and basis vector selection in any sgpc design . the proposed adaptive sampling based basis vector selection method aids in achieving improved generalization performance at a reduced computational cost . this method can also be used in conjunction with any other site parameter estimation methods . it has similar computational and storage complexities as the well-known information vector machine and is suitable for large datasets . the hyperparameters can be determined by optimizing a predictive loss function . the experimental results show better generalization performance of the proposed basis vector selection method on several benchmark datasets , particularly for relatively smaller basis vector set sizes or on difficult datasets .", "topics": ["sampling ( signal processing )", "loss function"]}
{"title": "integrating selectional preferences in wordnet", "abstract": "selectional preference learning methods have usually focused on word-to-class relations , e.g . , a verb selects as its subject a given nominal class . this paper extends previous statistical models to class-to-class preferences , and presents a model that learns selectional preferences for classes of verbs , together with an algorithm to integrate the learned preferences in wordnet . the theoretical motivation is twofold : different senses of a verb may have different preferences , and classes of verbs may share preferences . on the practical side , class-to-class selectional preferences can be learned from untagged corpora ( the same as word-to-class ) , they provide selectional preferences for less frequent word senses via inheritance , and more important , they allow for easy integration in wordnet . the model is trained on subject-verb and object-verb relationships extracted from a small corpus disambiguated with wordnet senses . examples are provided illustrating that the theoretical motivations are well founded , and showing that the approach is feasible . experimental results on a word sense disambiguation task are also provided .", "topics": ["text corpus"]}
{"title": "distribution of the search of evolutionary product unit neural networks for classification", "abstract": "this paper deals with the distributed processing in the search for an optimum classification model using evolutionary product unit neural networks . for this distributed search we used a cluster of computers . our objective is to obtain a more efficient design than those net architectures which do not use a distributed process and which thus result in simpler designs . in order to get the best classification models we use evolutionary algorithms to train and design neural networks , which require a very time consuming computation . the reasons behind the need for this distribution are various . it is complicated to train this type of nets because of the difficulty entailed in determining their architecture due to the complex error surface . on the other hand , the use of evolutionary algorithms involves running a great number of tests with different seeds and parameters , thus resulting in a high computational cost", "topics": ["computation"]}
{"title": "sql-rank : a listwise approach to collaborative ranking", "abstract": "in this paper , we propose a listwise approach for constructing user-specific rankings in recommendation systems in a collaborative fashion . we contrast the listwise approach to previous pointwise and pairwise approaches , which are based on treating either each rating or each pairwise comparison as an independent instance respectively . by extending the work of ( cao et al . 2007 ) , we cast listwise collaborative ranking as maximum likelihood under a permutation model which applies probability mass to permutations based on a low rank latent score matrix . we present a novel algorithm called sql-rank , which can accommodate ties and missing data and can run in linear time . we develop a theoretical framework for analyzing listwise ranking methods based on a novel representation theory for the permutation model . applying this framework to collaborative ranking , we derive asymptotic statistical rates as the number of users and items grow together . we conclude by demonstrating that our sql-rank method often outperforms current state-of-the-art algorithms for implicit feedback such as weighted-mf and bpr and achieve favorable results when compared to explicit feedback algorithms such as matrix factorization and collaborative ranking .", "topics": ["time complexity"]}
{"title": "a new algorithm of speckle filtering using stochastic distances", "abstract": "this paper presents a new approach for filter design based on stochastic distances and tests between distributions . a window is defined around each pixel , overlapping samples are compared and only those which pass a goodness-of-fit test are used to compute the filtered value . the technique is applied to intensity sar data with homogeneous regions using the gamma model . the proposal is compared with the lee 's filter using a protocol based on monte carlo . among the criteria used to quantify the quality of filters , we employ the equivalent number of looks , line and edge preservation . moreover , we also assessed the filters by the universal image quality index and the pearson 's correlation on edges regions .", "topics": ["pixel"]}
{"title": "global and local consistent age generative adversarial networks", "abstract": "age progression/regression is a challenging task due to the complicated and non-linear transformation in human aging process . many researches have shown that both global and local facial features are essential for face representation , but previous gan based methods mainly focused on the global feature in age synthesis . to utilize both global and local facial information , we propose a global and local consistent age generative adversarial network ( glca-gan ) . in our generator , a global network learns the whole facial structure and simulates the aging trend of the whole face , while three crucial facial patches are progressed or regressed by three local networks aiming at imitating subtle changes of crucial facial subregions . to preserve most of the details in age-attribute-irrelevant areas , our generator learns the residual face . moreover , we employ an identity preserving loss to better preserve the identity information , as well as age preserving loss to enhance the accuracy of age synthesis . a pixel loss is also adopted to preserve detailed facial information of the input face . our proposed method is evaluated on three face aging datasets , i.e . , cacd dataset , morph dataset and fg-net dataset . experimental results show appealing performance of the proposed method by comparing with the state-of-the-art .", "topics": ["nonlinear system", "relevance"]}
{"title": "the reaction ruleml classification of the event / action / state processing and reasoning space", "abstract": "reaction ruleml is a general , practical , compact and user-friendly xml-serialized language for the family of reaction rules . in this white paper we give a review of the history of event / action /state processing and reaction rule approaches and systems in different domains , define basic concepts and give a classification of the event , action , state processing and reasoning space as well as a discussion of relevant / related work", "topics": ["database"]}
{"title": "active learning with partial feedback", "abstract": "in the large-scale multiclass setting , assigning labels often consists of answering multiple questions to drill down through a hierarchy of classes . here , the labor required per annotation scales with the number of questions asked . we propose active learning with partial feedback . in this setup , the learner asks the annotator if a chosen example belongs to a ( possibly composite ) chosen class . the answer eliminates some classes , leaving the agent with a partial label . success requires ( i ) a sampling strategy to choose ( example , class ) pairs , and ( ii ) learning from partial labels . experiments on the tinyimagenet dataset demonstrate that our most effective method achieves a 26 % relative improvement ( 8.1 % absolute ) in top1 classification accuracy for a 250k ( or 30 % ) binary question budget , compared to a naive baseline . our work may also impact traditional data annotation . for example , our best method fully annotates tinyimagenet with only 482k ( with edc though , erc is 491 ) binary questions ( vs 827k for naive method ) .", "topics": ["baseline ( configuration management )", "sampling ( signal processing )"]}
{"title": "mag : a multilingual , knowledge-base agnostic and deterministic entity linking approach", "abstract": "entity linking has recently been the subject of a significant body of research . currently , the best performing approaches rely on trained mono-lingual models . porting these approaches to other languages is consequently a difficult endeavor as it requires corresponding training data and retraining of the models . we address this drawback by presenting a novel multilingual , knowledge-based agnostic and deterministic approach to entity linking , dubbed mag . mag is based on a combination of context-based retrieval on structured knowledge bases and graph algorithms . we evaluate mag on 23 data sets and in 7 languages . our results show that the best approach trained on english datasets ( pboh ) achieves a micro f-measure that is up to 4 times worse on datasets in other languages . mag , on the other hand , achieves state-of-the-art performance on english datasets and reaches a micro f-measure that is up to 0.6 higher than that of pboh on non-english languages .", "topics": ["test set"]}
{"title": "learning to act greedily : polymatroid semi-bandits", "abstract": "many important optimization problems , such as the minimum spanning tree and minimum-cost flow , can be solved optimally by a greedy method . in this work , we study a learning variant of these problems , where the model of the problem is unknown and has to be learned by interacting repeatedly with the environment in the bandit setting . we formalize our learning problem quite generally , as learning how to maximize an unknown modular function on a known polymatroid . we propose a computationally efficient algorithm for solving our problem and bound its expected cumulative regret . our gap-dependent upper bound is tight up to a constant and our gap-free upper bound is tight up to polylogarithmic factors . finally , we evaluate our method on three problems and demonstrate that it is practical .", "topics": ["regret ( decision theory )", "mathematical optimization"]}
{"title": "abnormal spatial-temporal pattern analysis for niagara frontier border wait times", "abstract": "border crossing delays cause problems like huge economics loss and heavy environmental pollutions . to understand more about the nature of border crossing delay , this study applies a dictionary-based compression algorithm to process the historical niagara frontier border wait times data . it can identify the abnormal spatial-temporal patterns for both passenger vehicles and trucks at three bridges connecting us and canada . furthermore , it provides a quantitate anomaly score to rank the wait times patterns across the three bridges for each vehicle type and each direction . by analyzing the top three most abnormal patterns , we find that there are at least two factors contributing the anomaly of the patterns . the weekends and holidays may cause unusual heave congestions at the three bridges at the same time , and the freight transportation demand may be uneven from canada to the usa at peace bridge and lewiston-queenston bridge , which may lead to a high anomaly score . by calculating the frequency of the top 5 % abnormal patterns by hour of the day , the results show that for cars from the usa to canada , the frequency of abnormal waiting time patterns is the highest during noon while for trucks in the same direction , it is the highest during the afternoon peak hours . for canada to us direction , the frequency of abnormal border wait time patterns for both cars and trucks reaches to the peak during the afternoon . the analysis of abnormal spatial-temporal wait times patterns is promising to improve the border crossing management", "topics": ["dictionary"]}
{"title": "deep descriptor transforming for image co-localization", "abstract": "reusable model design becomes desirable with the rapid expansion of machine learning applications . in this paper , we focus on the reusability of pre-trained deep convolutional models . specifically , different from treating pre-trained models as feature extractors , we reveal more treasures beneath convolutional layers , i.e . , the convolutional activations could act as a detector for the common object in the image co-localization problem . we propose a simple but effective method , named deep descriptor transforming ( ddt ) , for evaluating the correlations of descriptors and then obtaining the category-consistent regions , which can accurately locate the common object in a set of images . empirical studies validate the effectiveness of the proposed ddt method . on benchmark image co-localization datasets , ddt consistently outperforms existing state-of-the-art methods by a large margin . moreover , ddt also demonstrates good generalization ability for unseen categories and robustness for dealing with noisy data .", "topics": ["feature extraction"]}
{"title": "efficient evolutionary algorithm for single-objective bilevel optimization", "abstract": "bilevel optimization problems are a class of challenging optimization problems , which contain two levels of optimization tasks . in these problems , the optimal solutions to the lower level problem become possible feasible candidates to the upper level problem . such a requirement makes the optimization problem difficult to solve , and has kept the researchers busy towards devising methodologies , which can efficiently handle the problem . despite the efforts , there hardly exists any effective methodology , which is capable of handling a complex bilevel problem . in this paper , we introduce bilevel evolutionary algorithm based on quadratic approximations ( bleaq ) of optimal lower level variables with respect to the upper level variables . the approach is capable of handling bilevel problems with different kinds of complexities in relatively smaller number of function evaluations . ideas from classical optimization have been hybridized with evolutionary methods to generate an efficient optimization algorithm for generic bilevel problems . the efficacy of the algorithm has been shown on two sets of test problems . the first set is a recently proposed smd test set , which contains problems with controllable complexities , and the second set contains standard test problems collected from the literature . the proposed method has been evaluated against two benchmarks , and the performance gain is observed to be significant .", "topics": ["test set", "optimization problem"]}
{"title": "neural programming by example", "abstract": "programming by example ( pbe ) targets at automatically inferring a computer program for accomplishing a certain task from sample input and output . in this paper , we propose a deep neural networks ( dnn ) based pbe model called neural programming by example ( npbe ) , which can learn from input-output strings and induce programs that solve the string manipulation problems . our npbe model has four neural network based components : a string encoder , an input-output analyzer , a program generator , and a symbol selector . we demonstrate the effectiveness of npbe by training it end-to-end to solve some common string manipulation problems in spreadsheet systems . the results show that our model can induce string manipulation programs effectively . our work is one step towards teaching dnn to generate computer programs .", "topics": ["encoder"]}
{"title": "off-policy reward shaping with ensembles", "abstract": "potential-based reward shaping ( pbrs ) is an effective and popular technique to speed up reinforcement learning by leveraging domain knowledge . while pbrs is proven to always preserve optimal policies , its effect on learning speed is determined by the quality of its potential function , which , in turn , depends on both the underlying heuristic and the scale . knowing which heuristic will prove effective requires testing the options beforehand , and determining the appropriate scale requires tuning , both of which introduce additional sample complexity . we formulate a pbrs framework that reduces learning speed , but does not incur extra sample complexity . for this , we propose to simultaneously learn an ensemble of policies , shaped w.r.t . many heuristics and on a range of scales . the target policy is then obtained by voting . the ensemble needs to be able to efficiently and reliably learn off-policy : requirements fulfilled by the recent horde architecture , which we take as our basis . we demonstrate empirically that ( 1 ) our ensemble policy outperforms both the base policy , and its single-heuristic components , and ( 2 ) an ensemble over a general range of scales performs at least as well as one with optimally tuned components .", "topics": ["mathematical optimization", "reinforcement learning"]}
{"title": "a study of the classification of low-dimensional data with supervised manifold learning", "abstract": "supervised manifold learning methods learn data representations by preserving the geometric structure of data while enhancing the separation between data samples from different classes . in this work , we propose a theoretical study of supervised manifold learning for classification . we consider nonlinear dimensionality reduction algorithms that yield linearly separable embeddings of training data and present generalization bounds for this type of algorithms . a necessary condition for satisfactory generalization performance is that the embedding allow the construction of a sufficiently regular interpolation function in relation with the separation margin of the embedding . we show that for supervised embeddings satisfying this condition , the classification error decays at an exponential rate with the number of training samples . finally , we examine the separability of supervised nonlinear embeddings that aim to preserve the low-dimensional geometric structure of data based on graph representations . the proposed analysis is supported by experiments on several real data sets .", "topics": ["synthetic data"]}
{"title": "empirical gaussian priors for cross-lingual transfer learning", "abstract": "sequence model learning algorithms typically maximize log-likelihood minus the norm of the model ( or minimize hamming loss + norm ) . in cross-lingual part-of-speech ( pos ) tagging , our target language training data consists of sequences of sentences with word-by-word labels projected from translations in $ k $ languages for which we have labeled data , via word alignments . our training data is therefore very noisy , and if rademacher complexity is high , learning algorithms are prone to overfit . norm-based regularization assumes a constant width and zero mean prior . we instead propose to use the $ k $ source language models to estimate the parameters of a gaussian prior for learning new pos taggers . this leads to significantly better performance in multi-source transfer set-ups . we also present a drop-out version that injects ( empirical ) gaussian noise during online learning . finally , we note that using empirical gaussian priors leads to much lower rademacher complexity , and is superior to optimally weighted model interpolation .", "topics": ["test set", "natural language processing"]}
{"title": "price updating in combinatorial prediction markets with bayesian networks", "abstract": "to overcome the # p-hardness of computing/updating prices in logarithm market scoring rule-based ( lmsr-based ) combinatorial prediction markets , chen et al . [ 5 ] recently used a simple bayesian network to represent the prices of securities in combinatorial predictionmarkets for tournaments , and showed that two types of popular securities are structure preserving . in this paper , we significantly extend this idea by employing bayesian networks in general combinatorial prediction markets . we reveal a very natural connection between lmsr-based combinatorial prediction markets and probabilistic belief aggregation , which leads to a complete characterization of all structure preserving securities for decomposable network structures . notably , the main results by chen et al . [ 5 ] are corollaries of our characterization . we then prove that in order for a very basic set of securities to be structure preserving , the graph of the bayesian network must be decomposable . we also discuss some approximation techniques for securities that are not structure preserving .", "topics": ["bayesian network"]}
{"title": "grid long short-term memory", "abstract": "this paper introduces grid long short-term memory , a network of lstm cells arranged in a multidimensional grid that can be applied to vectors , sequences or higher dimensional data such as images . the network differs from existing deep lstm architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data . the network provides a unified way of using lstm for both deep and sequential computation . we apply the model to algorithmic tasks such as 15-digit integer addition and sequence memorization , where it is able to significantly outperform the standard lstm . we then give results for two empirical tasks . we find that 2d grid lstm achieves 1.47 bits per character on the wikipedia character prediction benchmark , which is state-of-the-art among neural approaches . in addition , we use the grid lstm to define a novel two-dimensional translation model , the reencoder , and show that it outperforms a phrase-based reference system on a chinese-to-english translation task .", "topics": ["computation"]}
{"title": "introduction to cross-entropy clustering the r package cec", "abstract": "the r package cec performs clustering based on the cross-entropy clustering ( cec ) method , which was recently developed with the use of information theory . the main advantage of cec is that it combines the speed and simplicity of $ k $ -means with the ability to use various gaussian mixture models and reduce unnecessary clusters . in this work we present a practical tutorial to cec based on the r package cec . functions are provided to encompass the whole process of clustering .", "topics": ["cluster analysis"]}
{"title": "metatheory of actions : beyond consistency", "abstract": "consistency check has been the only criterion for theory evaluation in logic-based approaches to reasoning about actions . this work goes beyond that and contributes to the metatheory of actions by investigating what other properties a good domain description in reasoning about actions should have . we state some metatheoretical postulates concerning this sore spot . when all postulates are satisfied together we have a modular action theory . besides being easier to understand and more elaboration tolerant in mccarthy 's sense , modular theories have interesting properties . we point out the problems that arise when the postulates about modularity are violated and propose algorithmic checks that can help the designer of an action theory to overcome them .", "topics": ["eisenstein 's criterion"]}
{"title": "learning discriminative stein kernel for spd matrices and its applications", "abstract": "stein kernel has recently shown promising performance on classifying images represented by symmetric positive definite ( spd ) matrices . it evaluates the similarity between two spd matrices through their eigenvalues . in this paper , we argue that directly using the original eigenvalues may be problematic because : i ) eigenvalue estimation becomes biased when the number of samples is inadequate , which may lead to unreliable kernel evaluation ; ii ) more importantly , eigenvalues only reflect the property of an individual spd matrix . they are not necessarily optimal for computing stein kernel when the goal is to discriminate different sets of spd matrices . to address the two issues in one shot , we propose a discriminative stein kernel , in which an extra parameter vector is defined to adjust the eigenvalues of the input spd matrices . the optimal parameter values are sought by optimizing a proxy of classification performance . to show the generality of the proposed method , three different kernel learning criteria that are commonly used in the literature are employed respectively as a proxy . a comprehensive experimental study is conducted on a variety of image classification tasks to compare our proposed discriminative stein kernel with the original stein kernel and other commonly used methods for evaluating the similarity between spd matrices . the experimental results demonstrate that , the discriminative stein kernel can attain greater discrimination and better align with classification tasks by altering the eigenvalues . this makes it produce higher classification performance than the original stein kernel and other commonly used methods .", "topics": ["kernel ( operating system )", "computer vision"]}
{"title": "laplacian support vector machines trained in the primal", "abstract": "in the last few years , due to the growing ubiquity of unlabeled data , much effort has been spent by the machine learning community to develop better understanding and improve the quality of classifiers exploiting unlabeled data . following the manifold regularization approach , laplacian support vector machines ( lapsvms ) have shown the state of the art performance in semi -- supervised classification . in this paper we present two strategies to solve the primal lapsvm problem , in order to overcome some issues of the original dual formulation . whereas training a lapsvm in the dual requires two steps , using the primal form allows us to collapse training to a single step . moreover , the computational complexity of the training algorithm is reduced from o ( n^3 ) to o ( n^2 ) using preconditioned conjugate gradient , where n is the combined number of labeled and unlabeled examples . we speed up training by using an early stopping strategy based on the prediction on unlabeled data or , if available , on labeled validation examples . this allows the algorithm to quickly compute approximate solutions with roughly the same classification accuracy as the optimal ones , considerably reducing the training time . due to its simplicity , training lapsvm in the primal can be the starting point for additional enhancements of the original lapsvm formulation , such as those for dealing with large datasets . we present an extensive experimental evaluation on real world data showing the benefits of the proposed approach .", "topics": ["approximation algorithm", "supervised learning"]}
{"title": "an elitist approach for extracting automatically well-realized speech sounds with high confidence", "abstract": "this paper presents an `` elitist approach '' for extracting automatically well-realized speech sounds with high confidence . the elitist approach uses a speech recognition system based on hidden markov models ( hmm ) . the hmm are trained on speech sounds which are systematically well-detected in an iterative procedure . the results show that , by using the hmm models defined in the training phase , the speech recognizer detects reliably specific speech sounds with a small rate of errors .", "topics": ["speech recognition", "iteration"]}
{"title": "fast fourier single-pixel imaging using binary illumination", "abstract": "fourier single-pixel imaging ( fsi ) has proven capable of reconstructing high-quality two-dimensional and three-dimensional images . the utilization of the sparsity of natural images in fourier domain allows high-resolution images to be reconstructed from far fewer measurements than effective image pixels . however , applying original fsi in digital micro-mirror device ( dmd ) based high-speed imaging system turns out to be challenging , because the original fsi uses grayscale fourier basis patterns for illumination while dmds generate grayscale patterns at a relatively low rate . dmds are a binary device which can only generate a black-and-white pattern at each instance . in this paper , we adopt binary fourier patterns for illumination to achieve dmd-based high-speed single-pixel imaging . binary fourier patterns are generated by upsampling and then applying error diffusion based dithering to the grayscale patterns . experiments demonstrate the proposed technique able to achieve static imaging with high quality and dynamic imaging in real time . the proposed technique potentially allows high-quality and high-speed imaging over broad wavebands .", "topics": ["sampling ( signal processing )", "sparse matrix"]}
{"title": "digital makeup from internet images", "abstract": "we present a novel approach of color transfer between images by exploring their high-level semantic information . first , we set up a database which consists of the collection of downloaded images from the internet , which are segmented automatically by using matting techniques . we then , extract image foregrounds from both source and multiple target images . then by using image matting algorithms , the system extracts the semantic information such as faces , lips , teeth , eyes , eyebrows , etc . , from the extracted foregrounds of the source image . and , then the color is transferred between corresponding parts with the same semantic information . next we get the color transferred result by seamlessly compositing different parts together using alpha blending . in the final step , we present an efficient method of color consistency to optimize the color of a collection of images showing the common scene . the main advantage of our method over existing techniques is that it does not need face matching , as one could use more than one target images . it is not restricted to head shot images as we can also change the color style in the wild . moreover , our algorithm does not require to choose the same color style , same pose and image size between source and target images . our algorithm is not restricted to one-to-one image color transfer and can make use of more than one target images to transfer the color in different parts in the source image . comparing with other approaches , our algorithm is much better in color blending in the input data .", "topics": ["high- and low-level"]}
{"title": "zaliql : a sql-based framework for drawing causal inference from big data", "abstract": "causal inference from observational data is a subject of active research and development in statistics and computer science . many toolkits have been developed for this purpose that depends on statistical software . however , these toolkits do not scale to large datasets . in this paper we describe a suite of techniques for expressing causal inference tasks from observational data in sql . this suite supports the state-of-the-art methods for causal inference and run at scale within a database engine . in addition , we introduce several optimization techniques that significantly speedup causal inference , both in the online and offline setting . we evaluate the quality and performance of our techniques by experiments of real datasets .", "topics": ["causality"]}
{"title": "decision-theoretic troubleshooting : hardness of approximation", "abstract": "decision-theoretic troubleshooting is one of the areas to which bayesian networks can be applied . given a probabilistic model of a malfunctioning man-made device , the task is to construct a repair strategy with minimal expected cost . the problem has received considerable attention over the past two decades . efficient solution algorithms have been found for simple cases , whereas other variants have been proven np-complete . we study several variants of the problem found in literature , and prove that computing approximate troubleshooting strategies is np-hard . in the proofs , we exploit a close connection to set-covering problems .", "topics": ["approximation algorithm", "approximation"]}
{"title": "proceedings of the twenty-fifth conference on uncertainty in artificial intelligence ( 2009 )", "abstract": "this is the proceedings of the twenty-fifth conference on uncertainty in artificial intelligence , which was held in montreal , qc , canada , june 18 - 21 2009 .", "topics": ["artificial intelligence"]}
{"title": "quantifying the probable approximation error of probabilistic inference programs", "abstract": "this paper introduces a new technique for quantifying the approximation error of a broad class of probabilistic inference programs , including ones based on both variational and monte carlo approaches . the key idea is to derive a subjective bound on the symmetrized kl divergence between the distribution achieved by an approximate inference program and its true target distribution . the bound 's validity ( and subjectivity ) rests on the accuracy of two auxiliary probabilistic programs : ( i ) a `` reference '' inference program that defines a gold standard of accuracy and ( ii ) a `` meta-inference '' program that answers the question `` what internal random choices did the original approximate inference program probably make given that it produced a particular result ? '' the paper includes empirical results on inference problems drawn from linear regression , dirichlet process mixture modeling , hmms , and bayesian networks . the experiments show that the technique is robust to the quality of the reference inference program and that it can detect implementation bugs that are not apparent from predictive performance .", "topics": ["calculus of variations", "bayesian network"]}
{"title": "training a feed-forward neural network with artificial bee colony based backpropagation method", "abstract": "back-propagation algorithm is one of the most widely used and popular techniques to optimize the feed forward neural network training . nature inspired meta-heuristic algorithms also provide derivative-free solution to optimize complex problem . artificial bee colony algorithm is a nature inspired meta-heuristic algorithm , mimicking the foraging or food source searching behaviour of bees in a bee colony and this algorithm is implemented in several applications for an improved optimized outcome . the proposed method in this paper includes an improved artificial bee colony algorithm based back-propagation neural network training method for fast and improved convergence rate of the hybrid neural network learning method . the result is analysed with the genetic algorithm based back-propagation method , and it is another hybridized procedure of its kind . analysis is performed over standard data sets , reflecting the light of efficiency of proposed method in terms of convergence speed and rate .", "topics": ["heuristic"]}
{"title": "stochastic variational inference for fully bayesian sparse gaussian process regression models", "abstract": "this paper presents a novel variational inference framework for deriving a family of bayesian sparse gaussian process regression ( sgpr ) models whose approximations are variationally optimal with respect to the full-rank gpr model enriched with various corresponding correlation structures of the observation noises . our variational bayesian sgpr ( vbsgpr ) models jointly treat both the distributions of the inducing variables and hyperparameters as variational parameters , which enables the decomposability of the variational lower bound that in turn can be exploited for stochastic optimization . such a stochastic optimization involves iteratively following the stochastic gradient of the variational lower bound to improve its estimates of the optimal variational distributions of the inducing variables and hyperparameters ( and hence the predictive distribution ) of our vbsgpr models and is guaranteed to achieve asymptotic convergence to them . we show that the stochastic gradient is an unbiased estimator of the exact gradient and can be computed in constant time per iteration , hence achieving scalability to big data . we empirically evaluate the performance of our proposed framework on two real-world , massive datasets .", "topics": ["sparse matrix"]}
{"title": "on maximum a posteriori estimation of hidden markov processes", "abstract": "we present a theoretical analysis of maximum a posteriori ( map ) sequence estimation for binary symmetric hidden markov processes . we reduce the map estimation to the energy minimization of an appropriately defined ising spin model , and focus on the performance of map as characterized by its accuracy and the number of solutions corresponding to a typical observed sequence . it is shown that for a finite range of sufficiently low noise levels , the solution is uniquely related to the observed sequence , while the accuracy degrades linearly with increasing the noise strength . for intermediate noise values , the accuracy is nearly noise-independent , but now there are exponentially many solutions to the estimation problem , which is reflected in non-zero ground-state entropy for the ising model . finally , for even larger noise intensities , the number of solutions reduces again , but the accuracy is poor . it is shown that these regimes are different thermodynamic phases of the ising model that are related to each other via first-order phase transitions .", "topics": ["markov chain"]}
{"title": "using learned predictions as feedback to improve control and communication with an artificial limb : preliminary findings", "abstract": "many people suffer from the loss of a limb . learning to get by without an arm or hand can be very challenging , and existing prostheses do not yet fulfil the needs of individuals with amputations . one promising solution is to provide greater communication between a prosthesis and its user . towards this end , we present a simple machine learning interface to supplement the control of a robotic limb with feedback to the user about what the limb will be experiencing in the near future . a real-time prediction learner was implemented to predict impact-related electrical load experienced by a robot limb ; the learning system 's predictions were then communicated to the device 's user to aid in their interactions with a workspace . we tested this system with five able-bodied subjects . each subject manipulated the robot arm while receiving different forms of vibrotactile feedback regarding the arm 's contact with its workspace . our trials showed that communicable predictions could be learned quickly during human control of the robot arm . using these predictions as a basis for feedback led to a statistically significant improvement in task performance when compared to purely reactive feedback from the device . our study therefore contributes initial evidence that prediction learning and machine intelligence can benefit not just control , but also feedback from an artificial limb . we expect that a greater level of acceptance and ownership can be achieved if the prosthesis itself takes an active role in transmitting learned knowledge about its state and its situation of use .", "topics": ["interaction", "artificial intelligence"]}
{"title": "a comprehensive trainable error model for sung music queries", "abstract": "we propose a model for errors in sung queries , a variant of the hidden markov model ( hmm ) . this is a solution to the problem of identifying the degree of similarity between a ( typically error-laden ) sung query and a potential target in a database of musical works , an important problem in the field of music information retrieval . similarity metrics are a critical component of query-by-humming ( qbh ) applications which search audio and multimedia databases for strong matches to oral queries . our model comprehensively expresses the types of error or variation between target and query : cumulative and non-cumulative local errors , transposition , tempo and tempo changes , insertions , deletions and modulation . the model is not only expressive , but automatically trainable , or able to learn and generalize from query examples . we present results of simulations , designed to assess the discriminatory potential of the model , and tests with real sung queries , to demonstrate relevance to real-world applications .", "topics": ["simulation", "relevance"]}
{"title": "cumulative step-size adaptation on linear functions : technical report", "abstract": "the csa-es is an evolution strategy with cumulative step size adaptation , where the step size is adapted measuring the length of a so-called cumulative path . the cumulative path is a combination of the previous steps realized by the algorithm , where the importance of each step decreases with time . this article studies the csa-es on composites of strictly increasing with affine linear functions through the investigation of its underlying markov chains . rigorous results on the change and the variation of the step size are derived with and without cumulation . the step-size diverges geometrically fast in most cases . furthermore , the influence of the cumulation parameter is studied .", "topics": ["markov chain"]}
{"title": "distributed vector representation of shopping items , the customer and shopping cart to build a three fold recommendation system", "abstract": "the main idea of this paper is to represent shopping items through vectors because these vectors act as the base for building em- beddings for customers and shopping carts . also , these vectors are input to the mathematical models that act as either a recommendation engine or help in targeting potential customers . we have used exponential family embeddings as the tool to construct two basic vectors - product embeddings and context vectors . using the basic vectors , we build combined embeddings , trip embeddings and customer embeddings . combined embeddings mix linguistic properties of product names with their shopping patterns . the customer embeddings establish an understand- ing of the buying pattern of customers in a group and help in building customer profile . for example a customer profile can represent customers frequently buying pet-food . identifying such profiles can help us bring out offers and discounts . similarly , trip embeddings are used to build trip profiles . people happen to buy similar set of products in a trip and hence their trip embeddings can be used to predict the next product they would like to buy . this is a novel technique and the first of its kind to make recommendation using product , trip and customer embeddings .", "topics": ["time complexity"]}
{"title": "multilevel image encryption", "abstract": "with the fast evolution of digital data exchange and increased usage of multi media images , it is essential to protect the confidential image data from unauthorized access . in natural images the values and position of the neighbouring pixels are strongly correlated . the method proposed in this paper , breaks this correlation increasing entropy of the position and entropy of pixel values using block shuffling and encryption by chaotic sequence respectively . the plain-image is initially row wise shuffled and first level of encryption is performed using addition modulo operation . the image is divided into blocks and then block based shuffling is performed using arnold cat transformation , further the blocks are uniformly scrambled across the image . finally the shuffled image undergoes second level of encryption by bitwise xor operation , and then the image as a whole is shuffled column wise to produce the ciphered image for transmission . the experimental results show that the proposed algorithm can successfully encrypt or decrypt the image with the secret keys , and the analysis of the algorithm also demonstrates that the encrypted image has good information entropy and low correlation coefficients .", "topics": ["coefficient", "pixel"]}
{"title": "theoretical analyses of cross-validation error and voting in instance-based learning", "abstract": "this paper begins with a general theory of error in cross-validation testing of algorithms for supervised learning from examples . it is assumed that the examples are described by attribute-value pairs , where the values are symbolic . cross-validation requires a set of training examples and a set of testing examples . the value of the attribute that is to be predicted is known to the learner in the training set , but unknown in the testing set . the theory demonstrates that cross-validation error has two components : error on the training set ( inaccuracy ) and sensitivity to noise ( instability ) . this general theory is then applied to voting in instance-based learning . given an example in the testing set , a typical instance-based learning algorithm predicts the designated attribute by voting among the k nearest neighbors ( the k most similar examples ) to the testing example in the training set . voting is intended to increase the stability ( resistance to noise ) of instance-based learning , but a theoretical analysis shows that there are circumstances in which voting can be destabilizing . the theory suggests ways to minimize cross-validation error , by insuring that voting is stable and does not adversely affect accuracy .", "topics": ["test set", "value ( ethics )"]}
{"title": "very deep convolutional networks for text classification", "abstract": "the dominant approach for many nlp tasks are recurrent neural networks , in particular lstms , and convolutional neural networks . however , these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision . we present a new architecture ( vdcnn ) for text processing which operates directly at the character level and uses only small convolutions and pooling operations . we are able to show that the performance of this model increases with depth : using up to 29 convolutional layers , we report improvements over the state-of-the-art on several public text classification tasks . to the best of our knowledge , this is the first time that very deep convolutional nets have been applied to text processing .", "topics": ["recurrent neural network", "natural language processing"]}
{"title": "a deep learning approach to data-driven parameterizations for statistical parametric speech synthesis", "abstract": "nearly all statistical parametric speech synthesizers today use mel cepstral coefficients as the vocal tract parameterization of the speech signal . mel cepstral coefficients were never intended to work in a parametric speech synthesis framework , but as yet , there has been little success in creating a better parameterization that is more suited to synthesis . in this paper , we use deep learning algorithms to investigate a data-driven parameterization technique that is designed for the specific requirements of synthesis . we create an invertible , low-dimensional , noise-robust encoding of the mel log spectrum by training a tapered stacked denoising autoencoder ( sda ) . this sda is then unwrapped and used as the initialization for a multi-layer perceptron ( mlp ) . the mlp is fine-tuned by training it to reconstruct the input at the output layer . this mlp is then split down the middle to form encoding and decoding networks . these networks produce a parameterization of the mel log spectrum that is intended to better fulfill the requirements of synthesis . results are reported for experiments conducted using this resulting parameterization with the clustergen speech synthesizer .", "topics": ["noise reduction", "autoencoder"]}
{"title": "supersparse linear integer models for predictive scoring systems", "abstract": "we introduce supersparse linear integer models ( slim ) as a tool to create scoring systems for binary classification . we derive theoretical bounds on the true risk of slim scoring systems , and present experimental results to show that slim scoring systems are accurate , sparse , and interpretable classification models .", "topics": ["mathematical optimization", "matrix regularization"]}
{"title": "learning deep networks from noisy labels with dropout regularization", "abstract": "large datasets often have unreliable labels-such as those obtained from amazon 's mechanical turk or social media platforms-and classifiers trained on mislabeled datasets often exhibit poor performance . we present a simple , effective technique for accounting for label noise when training deep neural networks . we augment a standard deep network with a softmax layer that models the label noise statistics . then , we train the deep network and noise model jointly via end-to-end stochastic gradient descent on the ( perhaps mislabeled ) dataset . the augmented model is overdetermined , so in order to encourage the learning of a non-trivial noise model , we apply dropout regularization to the weights of the noise model during training . numerical experiments on noisy versions of the cifar-10 and mnist datasets show that the proposed dropout technique outperforms state-of-the-art methods .", "topics": ["matrix regularization", "gradient descent"]}
{"title": "a new pseudo-color technique based on intensity information protection for passive sensor imagery", "abstract": "remote sensing image processing is so important in geo-sciences . images which are obtained by different types of sensors might initially be unrecognizable . to make an acceptable visual perception in the images , some pre-processing steps ( for removing noises and etc ) are preformed which they affect the analysis of images . there are different types of processing according to the types of remote sensing images . the method that we are going to introduce in this paper is to use virtual colors to colorize the gray-scale images of satellite sensors . this approach helps us to have a better analysis on a sample single-band image which has been taken by landsat-8 ( oli ) sensor ( as a multi-band sensor with natural color bands , its images ' natural color can be compared to synthetic color by our approach ) . a good feature of this method is the original image reversibility in order to keep the suitable resolution of output images .", "topics": ["image processing", "synthetic data"]}
{"title": "automatic goal generation for reinforcement learning agents", "abstract": "reinforcement learning is a powerful technique to train an agent to perform a task . however , an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function . such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks , such as navigating to varying positions in a room or moving objects to varying locations . instead , we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing . we use a generator network to propose tasks for the agent to try to achieve , specified as goal states . the generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent . our method thus automatically produces a curriculum of tasks for the agent to learn . we show that , by using this framework , an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment . our method can also learn to achieve tasks with sparse rewards , which traditionally pose significant challenges .", "topics": ["reinforcement learning", "sparse matrix"]}
{"title": "systematic testing of convolutional neural networks for autonomous driving", "abstract": "we present a framework to systematically analyze convolutional neural networks ( cnns ) used in classification of cars in autonomous vehicles . our analysis procedure comprises an image generator that produces synthetic pictures by sampling in a lower dimension image modification subspace and a suite of visualization tools . the image generator produces images which can be used to test the cnn and hence expose its vulnerabilities . the presented framework can be used to extract insights of the cnn classifier , compare across classification models , or generate training and validation datasets .", "topics": ["statistical classification", "neural networks"]}
{"title": "submodularity of a set label disagreement function", "abstract": "a set label disagreement function is defined over the number of variables that deviates from the dominant label . the dominant label is the value assumed by the largest number of variables within a set of binary variables . the submodularity of a certain family of set label disagreement function is discussed in this manuscript . such disagreement function could be utilized as a cost function in combinatorial optimization approaches for problems defined over hypergraphs .", "topics": ["loss function"]}
{"title": "learning to learn from noisy web videos", "abstract": "understanding the simultaneously very diverse and intricately fine-grained set of possible human actions is a critical open problem in computer vision . manually labeling training videos is feasible for some action classes but does n't scale to the full long-tailed distribution of actions . a promising way to address this is to leverage noisy data from web queries to learn new actions , using semi-supervised or `` webly-supervised '' approaches . however , these methods typically do not learn domain-specific knowledge , or rely on iterative hand-tuned data labeling policies . in this work , we instead propose a reinforcement learning-based formulation for selecting the right examples for training a classifier from noisy web search results . our method uses q-learning to learn a data labeling policy on a small labeled training dataset , and then uses this to automatically label noisy web data for new visual concepts . experiments on the challenging sports-1m action recognition benchmark as well as on additional fine-grained and newly emerging action classes demonstrate that our method is able to learn good labeling policies for noisy data and use this to learn accurate visual concept classifiers .", "topics": ["supervised learning", "reinforcement learning"]}
{"title": "data distillation for controlling specificity in dialogue generation", "abstract": "people speak at different levels of specificity in different situations . depending on their knowledge , interlocutors , mood , etc . } a conversational agent should have this ability and know when to be specific and when to be general . we propose an approach that gives a neural network -- based conversational agent this ability . our approach involves alternating between \\emph { data distillation } and model training : removing training examples that are closest to the responses most commonly produced by the model trained from the last round and then retrain the model on the remaining dataset . dialogue generation models trained with different degrees of data distillation manifest different levels of specificity . we then train a reinforcement learning system for selecting among this pool of generation models , to choose the best level of specificity for a given input . compared to the original generative model trained without distillation , the proposed system is capable of generating more interesting and higher-quality responses , in addition to appropriately adjusting specificity depending on the context . our research constitutes a specific case of a broader approach involving training multiple subsystems from a single dataset distinguished by differences in a specific property one wishes to model . we show that from such a set of subsystems , one can use reinforcement learning to build a system that tailors its output to different input contexts at test time .", "topics": ["generative model", "reinforcement learning"]}
{"title": "efficient feature group sequencing for anytime linear prediction", "abstract": "we consider \\textit { anytime } linear prediction in the common machine learning setting , where features are in groups that have costs . we achieve anytime ( or interruptible ) predictions by sequencing the computation of feature groups and reporting results using the computed features at interruption . we extend orthogonal matching pursuit ( omp ) and forward regression ( fr ) to learn the sequencing greedily under this group setting with costs . we theoretically guarantee that our algorithms achieve near-optimal linear predictions at each budget when a feature group is chosen . with a novel analysis of omp , we improve its theoretical bound to the same strength as that of fr . in addition , we develop a novel algorithm that consumes cost $ 4b $ to approximate the optimal performance of \\textit { any } cost $ b $ , and prove that with cost less than $ 4b $ , such an approximation is impossible . to our knowledge , these are the first anytime bounds at \\textit { all } budgets . we test our algorithms on two real-world data-sets and evaluate them in terms of anytime linear prediction performance against cost-weighted group lasso and alternative greedy algorithms .", "topics": ["approximation algorithm", "approximation"]}
{"title": "imaging around corners with single-pixel detector by computational ghost imaging", "abstract": "we have designed a single-pixel camera with imaging around corners based on computational ghost imaging . it can obtain the image of an object when the camera can not look at the object directly . our imaging system explores the fact that a bucket detector in a ghost imaging setup has no spatial resolution capability . a series of experiments have been designed to confirm our predictions . this camera has potential applications for imaging around corner or other similar environments where the object can not be observed directly .", "topics": ["pixel"]}
{"title": "discriminative clustering with relative constraints", "abstract": "we study the problem of clustering with relative constraints , where each constraint specifies relative similarities among instances . in particular , each constraint $ ( x_i , x_j , x_k ) $ is acquired by posing a query : is instance $ x_i $ more similar to $ x_j $ than to $ x_k $ ? we consider the scenario where answers to such queries are based on an underlying ( but unknown ) class concept , which we aim to discover via clustering . different from most existing methods that only consider constraints derived from yes and no answers , we also incorporate do n't know responses . we introduce a discriminative clustering method with relative constraints ( dcrc ) which assumes a natural probabilistic relationship between instances , their underlying cluster memberships , and the observed constraints . the objective is to maximize the model likelihood given the constraints , and in the meantime enforce cluster separation and cluster balance by also making use of the unlabeled instances . we evaluated the proposed method using constraints generated from ground-truth class labels , and from ( noisy ) human judgments from a user study . experimental results demonstrate : 1 ) the usefulness of relative constraints , in particular when do n't know answers are considered ; 2 ) the improved performance of the proposed method over state-of-the-art methods that utilize either relative or pairwise constraints ; and 3 ) the robustness of our method in the presence of noisy constraints , such as those provided by human judgement .", "topics": ["cluster analysis", "ground truth"]}
{"title": "feature specific sentiment analysis for product reviews", "abstract": "in this paper , we present a novel approach to identify feature specific expressions of opinion in product reviews with different features and mixed emotions . the objective is realized by identifying a set of potential features in the review and extracting opinion expressions about those features by exploiting their associations . capitalizing on the view that more closely associated words come together to express an opinion about a certain feature , dependency parsing is used to identify relations between the opinion expressions . the system learns the set of significant relations to be used by dependency parsing and a threshold parameter which allows us to merge closely associated opinion expressions . the data requirement is minimal as this is a one time learning of the domain independent parameters . the associations are represented in the form of a graph which is partitioned to finally retrieve the opinion expression describing the user specified feature . we show that the system achieves a high accuracy across all domains and performs at par with state-of-the-art systems despite its data limitations .", "topics": ["parsing"]}
{"title": "scalable computation of optimized queries for sequential diagnosis", "abstract": "in many model-based diagnosis applications it is impossible to provide such a set of observations and/or measurements that allow to identify the real cause of a fault . therefore , diagnosis systems often return many possible candidates , leaving the burden of selecting the correct diagnosis to a user . sequential diagnosis techniques solve this problem by automatically generating a sequence of queries to some oracle . the answers to these queries provide additional information necessary to gradually restrict the search space by removing diagnosis candidates inconsistent with the answers . during query computation , existing sequential diagnosis methods often require the generation of many unnecessary query candidates and strongly rely on expensive logical reasoners . we tackle this issue by devising efficient heuristic query search methods . the proposed methods enable for the first time a completely reasoner-free query generation while at the same time guaranteeing optimality conditions , e.g . minimal cardinality or best understandability , of the returned query that existing methods can not realize . hence , the performance of this approach is independent of the ( complexity of the ) diagnosed system . experiments conducted using real-world problems show that the new approach is highly scalable and outperforms existing methods by orders of magnitude .", "topics": ["computation", "scalability"]}
{"title": "named entity recognition using conditional random fields with non-local relational constraints", "abstract": "we begin by introducing the computer science branch of natural language processing , then narrowing the attention on its subbranch of information extraction and particularly on named entity recognition , discussing briefly its main methodological approaches . it follows an introduction to state-of-the-art conditional random fields under the form of linear chains . subsequently , the idea of constrained inference as a way to model long-distance relationships in a text is presented , based on an integer linear programming representation of the problem . adding such relationships to the problem as automatically inferred logical formulas , translatable into linear conditions , we propose to solve the resulting more complex problem with the aid of lagrangian relaxation , of which some technical details are explained . lastly , we give some experimental results .", "topics": ["natural language processing"]}
{"title": "neural factorization machines for sparse predictive analytics", "abstract": "many predictive tasks of web applications need to model categorical variables , such as user ids and demographics like genders and occupations . to apply standard machine learning techniques , these categorical predictors are always converted to a set of binary features via one-hot encoding , making the resultant feature vector highly sparse . to learn from such sparse data effectively , it is crucial to account for the interactions between features . factorization machines ( fms ) are a popular solution for efficiently using the second-order feature interactions . however , fm models feature interactions in a linear way , which can be insufficient for capturing the non-linear and complex inherent structure of real-world data . while deep neural networks have recently been applied to learn non-linear feature interactions in industry , such as the wide & deep by google and deepcross by microsoft , the deep structure meanwhile makes them difficult to train . in this paper , we propose a novel model neural factorization machine ( nfm ) for prediction under sparse settings . nfm seamlessly combines the linearity of fm in modelling second-order feature interactions and the non-linearity of neural network in modelling higher-order feature interactions . conceptually , nfm is more expressive than fm since fm can be seen as a special case of nfm without hidden layers . empirical results on two regression tasks show that with one hidden layer only , nfm significantly outperforms fm with a 7.3 % relative improvement . compared to the recent deep learning methods wide & deep and deepcross , our nfm uses a shallower structure but offers better performance , being much easier to train and tune in practice .", "topics": ["feature vector", "nonlinear system"]}
{"title": "on semidefinite relaxations for the block model", "abstract": "the stochastic block model ( sbm ) is a popular tool for community detection in networks , but fitting it by maximum likelihood ( mle ) involves a computationally infeasible optimization problem . we propose a new semidefinite programming ( sdp ) solution to the problem of fitting the sbm , derived as a relaxation of the mle . we put ours and previously proposed sdps in a unified framework , as relaxations of the mle over various sub-classes of the sbm , revealing a connection to sparse pca . our main relaxation , which we call sdp-1 , is tighter than other recently proposed sdp relaxations , and thus previously established theoretical guarantees carry over . however , we show that sdp-1 exactly recovers true communities over a wider class of sbms than those covered by current results . in particular , the assumption of strong assortativity of the sbm , implicit in consistency conditions for previously proposed sdps , can be relaxed to weak assortativity for our approach , thus significantly broadening the class of sbms covered by the consistency results . we also show that strong assortativity is indeed a necessary condition for exact recovery for previously proposed sdp approaches and not an artifact of the proofs . our analysis of sdps is based on primal-dual witness constructions , which provides some insight into the nature of the solutions of various sdps . we show how to combine features from sdp-1 and already available sdps to achieve the most flexibility in terms of both assortativity and block-size constraints , as our relaxation has the tendency to produce communities of similar sizes . this tendency makes it the ideal tool for fitting network histograms , a method gaining popularity in the graphon estimation literature , as we illustrate on an example of a social networks of dolphins . we also provide empirical evidence that sdps outperform spectral methods for fitting sbms with a large number of blocks .", "topics": ["optimization problem", "sparse matrix"]}
{"title": "data-driven sparse structure selection for deep neural networks", "abstract": "deep convolutional neural networks have liberated its extraordinary power on various tasks . however , it is still very challenging to deploy state-of-the-art models into real-world applications due to their high computational complexity . how can we design a compact and effective network without massive experiments and expert knowledge ? in this paper , we propose a simple and effective framework to learn and prune deep models in an end-to-end manner . in our framework , a new type of parameter -- scaling factor is first introduced to scale the outputs of specific structures , such as neurons , groups or residual blocks . then we add sparsity regularizations on these factors , and solve this optimization problem by a modified stochastic accelerated proximal gradient ( apg ) method . by forcing some of the factors to zero , we can safely remove the corresponding structures , thus prune the unimportant parts of a cnn . comparing with other structure selection methods that may need thousands of trials or iterative fine-tuning , our method is trained fully end-to-end in one training pass without bells and whistles . we evaluate our method , sparse structure selection with several state-of-the-art cnns , and demonstrate very promising results with adaptive depth and width selection .", "topics": ["computational complexity theory", "optimization problem"]}
{"title": "recurrent residual module for fast inference in videos", "abstract": "deep convolutional neural networks ( cnns ) have made impressive progress in many video recognition tasks such as video pose estimation and video object detection . however , cnn inference on video is computationally expensive due to processing dense frames individually . in this work , we propose a framework called recurrent residual module ( rrm ) to accelerate the cnn inference for video recognition tasks . this framework has a novel design of using the similarity of the intermediate feature maps of two consecutive frames , to largely reduce the redundant computation . one unique property of the proposed method compared to previous work is that feature maps of each frame are precisely computed . the experiments show that , while maintaining the similar recognition performance , our rrm yields averagely 2x acceleration on the commonly used cnns such as alexnet , resnet , deep compression model ( thus 8-12x faster than the original dense models using the efficient inference engine ) , and impressively 9x acceleration on some binary networks such as xnor-nets ( thus 500x faster than the original model ) . we further verify the effectiveness of the rrm on speeding up cnns for video pose estimation and video object detection .", "topics": ["object detection", "computation"]}
{"title": "on the relation between two rotation metrics", "abstract": "in their work `` global optimization through rotation space search '' , richard hartley and fredrik kahl introduce a global optimization strategy for problems in geometric computer vision , based on rotation space search using a branch-and-bound algorithm . in its core , lemma 2 of their publication is the important foundation for a class of global optimization algorithms , which is adopted over a wide range of problems in subsequent publications . this lemma relates a metric on rotations represented by rotation matrices with a metric on rotations in axis-angle representation . this work focuses on a proof for this relationship , which is based on rodrigues ' rotation theorem for the composition of rotations in axis-angle representation .", "topics": ["computer vision"]}
{"title": "neural program search : solving programming tasks from description and examples", "abstract": "we present a neural program search , an algorithm to generate programs from natural language description and a small number of input/output examples . the algorithm combines methods from deep learning and program synthesis fields by designing rich domain-specific language ( dsl ) and defining efficient search algorithm guided by a seq2tree model on it . to evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs . we show that our algorithm significantly outperforms a sequence-to-sequence model with attention baseline .", "topics": ["baseline ( configuration management )", "synthetic data"]}
{"title": "the weighted kendall and high-order kernels for permutations", "abstract": "we propose new positive definite kernels for permutations . first we introduce a weighted version of the kendall kernel , which allows to weight unequally the contributions of different item pairs in the permutations depending on their ranks . like the kendall kernel , we show that the weighted version is invariant to relabeling of items and can be computed efficiently in $ o ( n \\ln ( n ) ) $ operations , where $ n $ is the number of items in the permutation . second , we propose a supervised approach to learn the weights by jointly optimizing them with the function estimated by a kernel machine . third , while the kendall kernel considers pairwise comparison between items , we extend it by considering higher-order comparisons among tuples of items and show that the supervised approach of learning the weights can be systematically generalized to higher-order permutation kernels .", "topics": ["kernel ( operating system )", "supervised learning"]}
{"title": "fine-grained human evaluation of neural versus phrase-based machine translation", "abstract": "we compare three approaches to statistical machine translation ( pure phrase-based , factored phrase-based and neural ) by performing a fine-grained manual evaluation via error annotation of the systems ' outputs . the error types in our annotation are compliant with the multidimensional quality metrics ( mqm ) , and the annotation is performed by two annotators . inter-annotator agreement is high for such a task , and results show that the best performing system ( neural ) reduces the errors produced by the worst system ( phrase-based ) by 54 % .", "topics": ["machine translation"]}
{"title": "on the convergence speed of mdl predictions for bernoulli sequences", "abstract": "we consider the minimum description length principle for online sequence prediction . if the underlying model class is discrete , then the total expected square loss is a particularly interesting performance measure : ( a ) this quantity is bounded , implying convergence with probability one , and ( b ) it additionally specifies a `rate of convergence ' . generally , for mdl only exponential loss bounds hold , as opposed to the linear bounds for a bayes mixture . we show that this is even the case if the model class contains only bernoulli distributions . we derive a new upper bound on the prediction error for countable bernoulli classes . this implies a small bound ( comparable to the one for bayes mixtures ) for certain important model classes . the results apply to many machine learning tasks including classification and hypothesis testing . we provide arguments that our theorems generalize to countable classes of i.i.d . models .", "topics": ["time complexity"]}
{"title": "simple one-pass algorithm for penalized linear regression with cross-validation on mapreduce", "abstract": "in this paper , we propose a one-pass algorithm on mapreduce for penalized linear regression \\ [ f_\\lambda ( \\alpha , \\beta ) = \\|y - \\alpha\\mathbf { 1 } - x\\beta\\|_2^2 + p_ { \\lambda } ( \\beta ) \\ ] where $ \\alpha $ is the intercept which can be omitted depending on application ; $ \\beta $ is the coefficients and $ p_ { \\lambda } $ is the penalized function with penalizing parameter $ \\lambda $ . $ f_\\lambda ( \\alpha , \\beta ) $ includes interesting classes such as lasso , ridge regression and elastic-net . compared to latest iterative distributed algorithms requiring multiple mapreduce jobs , our algorithm achieves huge performance improvement ; moreover , our algorithm is exact compared to the approximate algorithms such as parallel stochastic gradient decent . moreover , what our algorithm distinguishes with others is that it trains the model with cross validation to choose optimal $ \\lambda $ instead of user specified one . key words : penalized linear regression , lasso , elastic-net , ridge , mapreduce", "topics": ["approximation algorithm", "gradient"]}
{"title": "artificial immune systems", "abstract": "the biological immune system is a robust , complex , adaptive system that defends the body from foreign pathogens . it is able to categorize all cells ( or molecules ) within the body as self-cells or non-self cells . it does this with the help of a distributed task force that has the intelligence to take action from a local and also a global perspective using its network of chemical messengers for communication . there are two major branches of the immune system . the innate immune system is an unchanging mechanism that detects and destroys certain invading organisms , whilst the adaptive immune system responds to previously unknown foreign cells and builds a response to them that can remain in the body over a long period of time . this remarkable information processing biological system has caught the attention of computer science in recent years . a novel computational intelligence technique , inspired by immunology , has emerged , called artificial immune systems . several concepts from the immune have been extracted and applied for solution to real world science and engineering problems . in this tutorial , we briefly describe the immune system metaphors that are relevant to existing artificial immune systems methods . we will then show illustrative real-world problems suitable for artificial immune systems and give a step-by-step algorithm walkthrough for one such problem . a comparison of the artificial immune systems to other well-known algorithms , areas for future work , tips & tricks and a list of resources will round this tutorial off . it should be noted that as artificial immune systems is still a young and evolving field , there is not yet a fixed algorithm template and hence actual implementations might differ somewhat from time to time and from those examples given here .", "topics": ["mathematical optimization"]}
{"title": "modeling events as machines", "abstract": "the notion of events has occupied a central role in modeling and has an influence in computer science and philosophy . recent developments in diagrammatic modeling have made it possible to examine conceptual representation of events . this paper explores some aspects of the notion of events that are produced by applying a new diagrammatic methodology with a focus on the interaction of events with such concepts as time and space , objects . the proposed description applies to abstract machines where events form the dynamic phases of a system . the results of this nontechnical research can be utilized in many fields where the notion of an event is typically used in interdisciplinary application .", "topics": ["entity", "artificial intelligence"]}
{"title": "policy search : any local optimum enjoys a global performance guarantee", "abstract": "local policy search is a popular reinforcement learning approach for handling large state spaces . formally , it searches locally in a paramet erized policy space in order to maximize the associated value function averaged over some predefined distribution . it is probably commonly b elieved that the best one can hope in general from such an approach is to get a local optimum of this criterion . in this article , we show th e following surprising result : \\emph { any } ( approximate ) \\emph { local optimum } enjoys a \\emph { global performance guarantee } . we compare this g uarantee with the one that is satisfied by direct policy iteration , an approximate dynamic programming algorithm that does some form of poli cy search : if the approximation error of local policy search may generally be bigger ( because local search requires to consider a space of s tochastic policies ) , we argue that the concentrability coefficient that appears in the performance bound is much nicer . finally , we discuss several practical and theoretical consequences of our analysis .", "topics": ["approximation algorithm", "mathematical optimization"]}
{"title": "optimal decision-theoretic classification using non-decomposable performance metrics", "abstract": "we provide a general theoretical analysis of expected out-of-sample utility , also referred to as decision-theoretic classification , for non-decomposable binary classification metrics such as f-measure and jaccard coefficient . our key result is that the expected out-of-sample utility for many performance metrics is provably optimized by a classifier which is equivalent to a signed thresholding of the conditional probability of the positive class . our analysis bridges a gap in the literature on binary classification , revealed in light of recent results for non-decomposable metrics in population utility maximization style classification . our results identify checkable properties of a performance metric which are sufficient to guarantee a probability ranking principle . we propose consistent estimators for optimal expected out-of-sample classification . as a consequence of the probability ranking principle , computational requirements can be reduced from exponential to cubic complexity in the general case , and further reduced to quadratic complexity in special cases . we provide empirical results on simulated and benchmark datasets evaluating the performance of the proposed algorithms for decision-theoretic classification and comparing them to baseline and state-of-the-art methods in population utility maximization for non-decomposable metrics .", "topics": ["baseline ( configuration management )", "time complexity"]}
{"title": "joint defogging and demosaicking", "abstract": "image defogging is a technique used extensively for enhancing visual quality of images in bad weather condition . even though defogging algorithms have been well studied , defogging performance is degraded by demosaicking artifacts and sensor noise amplification in distant scenes . in order to improve visual quality of restored images , we propose a novel approach to perform defogging and demosaicking simultaneously . we conclude that better defogging performance with fewer artifacts can be achieved when a defogging algorithm is combined with a demosaicking algorithm simultaneously . we also demonstrate that the proposed joint algorithm has the benefit of suppressing noise amplification in distant scene . in addition , we validate our theoretical analysis and observations for both synthesized datasets with ground truth fog-free images and natural scene datasets captured in a raw format .", "topics": ["ground truth"]}
{"title": "hindsight policy gradients", "abstract": "goal-conditional policies allow reinforcement learning agents to pursue specific goals during different episodes . in addition to their potential to generalize desired behavior to unseen goals , such policies may also help in defining options for arbitrary subgoals , enabling higher-level planning . while trying to achieve a specific goal , an agent may also be able to exploit information about the degree to which it has achieved alternative goals . reinforcement learning agents have only recently been endowed with such capacity for hindsight , which is highly valuable in environments with sparse rewards . in this paper , we show how hindsight can be introduced to likelihood-ratio policy gradient methods , generalizing this capacity to an entire class of highly successful algorithms . our preliminary experiments suggest that hindsight may increase the sample efficiency of policy gradient methods .", "topics": ["reinforcement learning", "sparse matrix"]}
{"title": "efficient parallel methods for deep reinforcement learning", "abstract": "we propose a novel framework for efficient parallelization of deep reinforcement learning algorithms , enabling these algorithms to learn from multiple actors on a single machine . the framework is algorithm agnostic and can be applied to on-policy , off-policy , value based and policy gradient based algorithms . given its inherent parallelism , the framework can be efficiently implemented on a gpu , allowing the usage of powerful models while significantly reducing training time . we demonstrate the effectiveness of our framework by implementing an advantage actor-critic algorithm on a gpu , using on-policy experiences and employing synchronous updates . our algorithm achieves state-of-the-art performance on the atari domain after only a few hours of training . our framework thus opens the door for much faster experimentation on demanding problem domains . our implementation is open-source and is made public at https : //github.com/alfredvc/paac", "topics": ["reinforcement learning", "gradient"]}
{"title": "selective experience replay for lifelong learning", "abstract": "deep reinforcement learning has emerged as a powerful tool for a variety of learning tasks , however deep nets typically exhibit forgetting when learning multiple tasks in sequence . to mitigate forgetting , we propose an experience replay process that augments the standard fifo buffer and selectively stores experiences in a long-term memory . we explore four strategies for selecting which experiences will be stored : favoring surprise , favoring reward , matching the global training distribution , and maximizing coverage of the state space . we show that distribution matching successfully prevents catastrophic forgetting , and is consistently the best approach on all domains tested . while distribution matching has better and more consistent performance , we identify one case in which coverage maximization is beneficial - when tasks that receive less trained are more important . overall , our results show that selective experience replay , when suitable selection algorithms are employed , can prevent catastrophic forgetting .", "topics": ["reinforcement learning"]}
{"title": "hierarchy influenced differential evolution : a motor operation inspired approach", "abstract": "operational maturity of biological control systems have fuelled the inspiration for a large number of mathematical and logical models for control , automation and optimisation . the human brain represents the most sophisticated control architecture known to us and is a central motivation for several research attempts across various domains . in the present work , we introduce an algorithm for mathematical optimisation that derives its intuition from the hierarchical and distributed operations of the human motor system . the system comprises global leaders , local leaders and an effector population that adapt dynamically to attain global optimisation via a feedback mechanism coupled with the structural hierarchy . the hierarchical system operation is distributed into local control for movement and global controllers that facilitate gross motion and decision making . we present our algorithm as a variant of the classical differential evolution algorithm , introducing a hierarchical crossover operation . the discussed approach is tested exhaustively on standard test functions as well as the cec 2017 benchmark . our algorithm significantly outperforms various standard algorithms as well as their popular variants as discussed in the results .", "topics": ["mathematical optimization"]}
{"title": "video tracking using learned hierarchical features", "abstract": "in this paper , we propose an approach to learn hierarchical features for visual object tracking . first , we offline learn features robust to diverse motion patterns from auxiliary video sequences . the hierarchical features are learned via a two-layer convolutional neural network . embedding the temporal slowness constraint in the stacked architecture makes the learned features robust to complicated motion transformations , which is important for visual object tracking . then , given a target video sequence , we propose a domain adaptation module to online adapt the pre-learned features according to the specific target object . the adaptation is conducted in both layers of the deep feature learning module so as to include appearance information of the specific target object . as a result , the learned hierarchical features can be robust to both complicated motion transformations and appearance changes of target objects . we integrate our feature learning algorithm into three tracking methods . experimental results demonstrate that significant improvement can be achieved using our learned hierarchical features , especially on video sequences with complicated motion transformations .", "topics": ["feature learning"]}
{"title": "role of morphology injection in statistical machine translation", "abstract": "phrase-based statistical models are more commonly used as they perform optimally in terms of both , translation quality and complexity of the system . hindi and in general all indian languages are morphologically richer than english . hence , even though phrase-based systems perform very well for the less divergent language pairs , for english to indian language translation , we need more linguistic information ( such as morphology , parse tree , parts of speech tags , etc . ) on the source side . factored models seem to be useful in this case , as factored models consider word as a vector of factors . these factors can contain any information about the surface word and use it while translating . hence , the objective of this work is to handle morphological inflections in hindi and marathi using factored translation models while translating from english . smt approaches face the problem of data sparsity while translating into a morphologically rich language . it is very unlikely for a parallel corpus to contain all morphological forms of words . we propose a solution to generate these unseen morphological forms and inject them into original training corpora . in this paper , we study factored models and the problem of sparseness in context of translation to morphologically rich languages . we propose a simple and effective solution which is based on enriching the input with various morphological forms of words . we observe that morphology injection improves the quality of translation in terms of both adequacy and fluency . we verify this with the experiments on two morphologically rich languages : hindi and marathi , while translating from english .", "topics": ["machine translation"]}
{"title": "novel and tuneable method for skin detection based on hybrid color space and color statistical features", "abstract": "skin detection is one of the most important and primary stages in some of image processing applications such as face detection and human tracking . so far , many approaches are proposed to done this case . near all of these methods have tried to find best match intensity distribution with skin pixels based on popular color spaces such as rgb , cmyk or ycbcr . results show these methods can not provide an accurate approach for every kinds of skin . in this paper , an approach is proposed to solve this problem using statistical features technique . this approach is including two stages . in the first one , from pure skin statistical features were extracted and at the second stage , the skin pixels are detected using hsv and ycbcr color spaces . in the result part , the proposed approach is applied on fei database and the accuracy rate reached 99.25 + 0.2 . further proposed method is applied on complex background database and accuracy rate obtained 95.40+0.31 % . the proposed approach can be used for all kinds of skin using train stage which is the main advantages of it . low noise sensitivity and low computational complexity are some of other advantages .", "topics": ["image processing", "computational complexity theory"]}
{"title": "probability reversal and the disjunction effect in reasoning systems", "abstract": "data based judgments go into artificial intelligence applications but they undergo paradoxical reversal when seemingly unnecessary additional data is provided . examples of this are simpson 's reversal and the disjunction effect where the beliefs about the data change once it is presented or aggregated differently . sometimes the significance of the difference can be evaluated using statistical tests such as pearson 's chi-squared or fisher 's exact test , but this may not be helpful in threshold-based decision systems that operate with incomplete information . to mitigate risks in the use of algorithms in decision-making , we consider the question of modeling of beliefs . we argue that evidence supports that beliefs are not classical statistical variables and they should , in the general case , be considered as superposition states of disjoint or polar outcomes . we analyze the disjunction effect from the perspective of the belief as a quantum vector .", "topics": ["artificial intelligence"]}
{"title": "a taught-obesrve-ask ( toa ) method for object detection with critical supervision", "abstract": "being inspired by child 's learning experience - taught first and followed by observation and questioning , we investigate a critically supervised learning methodology for object detection in this work . specifically , we propose a taught-observe-ask ( toa ) method that consists of several novel components such as negative object proposal , critical example mining , and machine-guided question-answer ( qa ) labeling . to consider labeling time and performance jointly , new evaluation methods are developed to compare the performance of the toa method , with the fully and weakly supervised learning methods . extensive experiments are conducted on the pascal voc and the caltech benchmark datasets . the toa method provides significantly improved performance of weakly supervision yet demands only about 3-6 % of labeling time of full supervision . the effectiveness of each novel component is also analyzed .", "topics": ["supervised learning", "object detection"]}
{"title": "complexity results for manipulation , bribery and control of the kemeny procedure in judgment aggregation", "abstract": "we study the computational complexity of several scenarios of strategic behavior for the kemeny procedure in the setting of judgment aggregation . in particular , we investigate ( 1 ) manipulation , where an individual aims to achieve a better group outcome by reporting an insincere individual opinion , ( 2 ) bribery , where an external agent aims to achieve an outcome with certain properties by bribing a number of individuals , and ( 3 ) control ( by adding or deleting issues ) , where an external agent aims to achieve an outcome with certain properties by influencing the set of issues in the judgment aggregation situation . we show that determining whether these types of strategic behavior are possible ( and if so , computing a policy for successful strategic behavior ) is complete for the second level of the polynomial hierarchy . that is , we show that these problems are $ \\sigma^p_2 $ -complete .", "topics": ["computational complexity theory", "polynomial"]}
{"title": "inductive inference and the representation of uncertainty", "abstract": "the form and justification of inductive inference rules depend strongly on the representation of uncertainty . this paper examines one generic representation , namely , incomplete information . the notion can be formalized by presuming that the relevant probabilities in a decision problem are known only to the extent that they belong to a class k of probability distributions . the concept is a generalization of a frequent suggestion that uncertainty be represented by intervals or ranges on probabilities . to make the representation useful for decision making , an inductive rule can be formulated which determines , in a well-defined manner , a best approximation to the unknown probability , given the set k. in addition , the knowledge set notion entails a natural procedure for updating -- modifying the set k given new evidence . several non-intuitive consequences of updating emphasize the differences between inference with complete and inference with incomplete information .", "topics": ["approximation"]}
{"title": "interaction histories and short term memory : enactive development of turn-taking behaviors in a childlike humanoid robot", "abstract": "in this article , an enactive architecture is described that allows a humanoid robot to learn to compose simple actions into turn-taking behaviors while playing interaction games with a human partner . the robot 's action choices are reinforced by social feedback from the human in the form of visual attention and measures of behavioral synchronization . we demonstrate that the system can acquire and switch between behaviors learned through interaction based on social feedback from the human partner . the role of reinforcement based on a short term memory of the interaction is experimentally investigated . results indicate that feedback based only on the immediate state is insufficient to learn certain turn-taking behaviors . therefore some history of the interaction must be considered in the acquisition of turn-taking , which can be efficiently handled through the use of short term memory .", "topics": ["reinforcement learning"]}
{"title": "a pac-bayesian approach to spectrally-normalized margin bounds for neural networks", "abstract": "we present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the frobenius norm of the weights . the generalization bound is derived using a pac-bayes analysis .", "topics": ["neural networks", "numerical analysis"]}
{"title": "marrnet : 3d shape reconstruction via 2.5d sketches", "abstract": "3d object reconstruction from a single image is a highly under-determined problem , requiring strong prior knowledge of plausible 3d shapes . this introduces challenges for learning-based approaches , as 3d object annotations are scarce in real images . previous work chose to train on synthetic data with ground truth 3d information , but suffered from domain adaptation when tested on real data . in this work , we propose marrnet , an end-to-end trainable model that sequentially estimates 2.5d sketches and 3d object shape . our disentangled , two-step formulation has three advantages . first , compared to full 3d shape , 2.5d sketches are much easier to be recovered from a 2d image ; models that recover 2.5d sketches are also more likely to transfer from synthetic to real data . second , for 3d reconstruction from 2.5d sketches , systems can learn purely from synthetic data . this is because we can easily render realistic 2.5d sketches without modeling object appearance variations in real images , including lighting , texture , etc . this further relieves the domain adaptation problem . third , we derive differentiable projective functions from 3d shape to 2.5d sketches ; the framework is therefore end-to-end trainable on real images , requiring no human annotations . our model achieves state-of-the-art performance on 3d shape reconstruction .", "topics": ["synthetic data", "end-to-end principle"]}
{"title": "ontology module extraction via datalog reasoning", "abstract": "module extraction - the task of computing a ( preferably small ) fragment m of an ontology t that preserves entailments over a signature s - has found many applications in recent years . extracting modules of minimal size is , however , computationally hard , and often algorithmically infeasible . thus , practical techniques are based on approximations , where m provably captures the relevant entailments , but is not guaranteed to be minimal . existing approximations , however , ensure that m preserves all second-order entailments of t w.r.t . s , which is stronger than is required in many applications , and may lead to large modules in practice . in this paper we propose a novel approach in which module extraction is reduced to a reasoning problem in datalog . our approach not only generalises existing approximations in an elegant way , but it can also be tailored to preserve only specific kinds of entailments , which allows us to extract significantly smaller modules . an evaluation on widely-used ontologies has shown very encouraging results .", "topics": ["approximation"]}
{"title": "phrase database approach to structural and semantic disambiguation in english-korean machine translation", "abstract": "in machine translation it is common phenomenon that machine-readable dictionaries and standard parsing rules are not enough to ensure accuracy in parsing and translating english phrases into korean language , which is revealed in misleading translation results due to consequent structural and semantic ambiguities . this paper aims to suggest a solution to structural and semantic ambiguities due to the idiomaticity and non-grammaticalness of phrases commonly used in english language by applying bilingual phrase database in english-korean machine translation ( ekmt ) . this paper firstly clarifies what the phrase unit in ekmt is based on the definition of the english phrase , secondly clarifies what kind of language unit can be the target of the phrase database for ekmt , thirdly suggests a way to build the phrase database by presenting the format of the phrase database with examples , and finally discusses briefly the method to apply this bilingual phrase database to the ekmt for structural and semantic disambiguation .", "topics": ["machine translation", "parsing"]}
{"title": "interpretable deep neural networks for single-trial eeg classification", "abstract": "background : in cognitive neuroscience the potential of deep neural networks ( dnns ) for solving complex classification tasks is yet to be fully exploited . the most limiting factor is that dnns as notorious 'black boxes ' do not provide insight into neurophysiological phenomena underlying a decision . layer-wise relevance propagation ( lrp ) has been introduced as a novel method to explain individual network decisions . new method : we propose the application of dnns with lrp for the first time for eeg data analysis . through lrp the single-trial dnn decisions are transformed into heatmaps indicating each data point 's relevance for the outcome of the decision . results : dnn achieves classification accuracies comparable to those of csp-lda . in subjects with low performance subject-to-subject transfer of trained dnns can improve the results . the single-trial lrp heatmaps reveal neurophysiologically plausible patterns , resembling csp-derived scalp maps . critically , while csp patterns represent class-wise aggregated information , lrp heatmaps pinpoint neural patterns to single time points in single trials . comparison with existing method ( s ) : we compare the classification performance of dnns to that of linear csp-lda on two data sets related to motor-imaginery bci . conclusion : we have demonstrated that dnn is a powerful non-linear tool for eeg analysis . with lrp a new quality of high-resolution assessment of neural activity can be reached . lrp is a potential remedy for the lack of interpretability of dnns that has limited their utility in neuroscientific applications . the extreme specificity of the lrp-derived heatmaps opens up new avenues for investigating neural activity underlying complex perception or decision-related processes .", "topics": ["nonlinear system", "map"]}
{"title": "mobility enhancement for elderly", "abstract": "loss of mobility is a common handicap to senior citizens . it denies them the ease of movement they would like to have like outdoor visits , movement in hospitals , social outgoings , but more seriously in the day to day in-house routine functions necessary for living etc . trying to overcome this handicap by means of servant or domestic help and simple wheel chairs is not only costly in the long run , but forces the senior citizen to be at the mercy of sincerity of domestic helps and also the consequent loss of dignity . in order to give a dignified life , the mobility obtained must be at the complete discretion , will and control of the senior citizen . this can be provided only by a reasonably sophisticated and versatile wheel chair , giving enhanced ability of vision , hearing through man-machine interface , and sensor aided navigation and control . more often than not senior people have poor vision which makes it difficult for them to maker visual judgement and so calls for the use of artificial intelligence in visual image analysis and guided navigation systems . in this project , we deal with two important enhancement features for mobility enhancement , audio command and vision aided obstacle detection and navigation . we have implemented speech recognition algorithm using template of stored words for identifying the voice command given by the user . this frees the user of an agile hand to operate joystick or mouse control . also , we have developed a new appearance based obstacle detection system using stereo-vision cameras which estimates the distance of nearest obstacle to the wheel chair and takes necessary action . this helps user in making better judgement of route and navigate obstacles . the main challenge in this project is how to navigate in an unknown/unfamiliar environment by avoiding obstacles .", "topics": ["speech recognition", "artificial intelligence"]}
{"title": "bayesian nonparametric causal inference : information rates and learning algorithms", "abstract": "we investigate the problem of estimating the causal effect of a treatment on individual subjects from observational data , this is a central problem in various application domains , including healthcare , social sciences , and online advertising . within the neyman rubin potential outcomes model , we use the kullback leibler ( kl ) divergence between the estimated and true distributions as a measure of accuracy of the estimate , and we define the information rate of the bayesian causal inference procedure as the ( asymptotic equivalence class of the ) expected value of the kl divergence between the estimated and true distributions as a function of the number of samples . using fano method , we establish a fundamental limit on the information rate that can be achieved by any bayesian estimator , and show that this fundamental limit is independent of the selection bias in the observational data . we characterize the bayesian priors on the potential ( factual and counterfactual ) outcomes that achieve the optimal information rate . as a consequence , we show that a particular class of priors that have been widely used in the causal inference literature can not achieve the optimal information rate . on the other hand , a broader class of priors can achieve the optimal information rate . we go on to propose a prior adaptation procedure ( which we call the information based empirical bayes procedure ) that optimizes the bayesian prior by maximizing an information theoretic criterion on the recovered causal effects rather than maximizing the marginal likelihood of the observed ( factual ) data . building on our analysis , we construct an information optimal bayesian causal inference algorithm .", "topics": ["reinforcement learning", "bayesian network"]}
{"title": "image super-resolution via feature-augmented random forest", "abstract": "recent random-forest ( rf ) -based image super-resolution approaches inherit some properties from dictionary-learning-based algorithms , but the effectiveness of the properties in rf is overlooked in the literature . in this paper , we present a novel feature-augmented random forest ( farf ) for image super-resolution , where the conventional gradient-based features are augmented with gradient magnitudes and different feature recipes are formulated on different stages in an rf . the advantages of our method are that , firstly , the dictionary-learning-based features are enhanced by adding gradient magnitudes , based on the observation that the non-linear gradient magnitude are with highly discriminative property . secondly , generalized locality-sensitive hashing ( lsh ) is used to replace principal component analysis ( pca ) for feature dimensionality reduction and original high-dimensional features are employed , instead of the compressed ones , for the leaf-nodes ' regressors , since regressors can benefit from higher dimensional features . this original-compressed coupled feature sets scheme unifies the unsupervised lsh evaluation on both image super-resolution and content-based image retrieval ( cbir ) . finally , we present a generalized weighted ridge regression ( gwrr ) model for the leaf-nodes ' regressors . experiment results on several public benchmark datasets show that our farf method can achieve an average gain of about 0.3 db , compared to traditional rf-based methods . furthermore , a fine-tuned farf model can compare to or ( in many cases ) outperform some recent stateof-the-art deep-learning-based algorithms .", "topics": ["cluster analysis", "nonlinear system"]}
{"title": "rainbow : combining improvements in deep reinforcement learning", "abstract": "the deep reinforcement learning community has made several independent improvements to the dqn algorithm . however , it is unclear which of these extensions are complementary and can be fruitfully combined . this paper examines six extensions to the dqn algorithm and empirically studies their combination . our experiments show that the combination provides state-of-the-art performance on the atari 2600 benchmark , both in terms of data efficiency and final performance . we also provide results from a detailed ablation study that shows the contribution of each component to overall performance .", "topics": ["baseline ( configuration management )", "approximation algorithm"]}
{"title": "modular belief updates and confusion about measures of certainty in artificial intelligence research", "abstract": "over the last decade , there has been growing interest in the use or measures or change in belief for reasoning with uncertainty in artificial intelligence research . an important characteristic of several methodologies that reason with changes in belief or belief updates , is a property that we term modularity . we call updates that satisfy this property modular updates . whereas probabilistic measures of belief update - which satisfy the modularity property were first discovered in the nineteenth century , knowledge and discussion of these quantities remains obscure in artificial intelligence research . we define modular updates and discuss their inappropriate use in two influential expert systems .", "topics": ["artificial intelligence"]}
{"title": "a note on topology preservation in classification , and the construction of a universal neuron grid", "abstract": "it will be shown that according to theorems of k. menger , every neuron grid if identified with a curve is able to preserve the adopted qualitative structure of a data space . furthermore , if this identification is made , the neuron grid structure can always be mapped to a subset of a universal neuron grid which is constructable in three space dimensions . conclusions will be drawn for established neuron grid types as well as neural fields .", "topics": ["entity", "iteration"]}
{"title": "adapting the stochastic block model to edge-weighted networks", "abstract": "we generalize the stochastic block model to the important case in which edges are annotated with weights drawn from an exponential family distribution . this generalization introduces several technical difficulties for model estimation , which we solve using a bayesian approach . we introduce a variational algorithm that efficiently approximates the model 's posterior distribution for dense graphs . in specific numerical experiments on edge-weighted networks , this weighted stochastic block model outperforms the common approach of first applying a single threshold to all weights and then applying the classic stochastic block model , which can obscure latent block structure in networks . this model will enable the recovery of latent structure in a broader range of network data than was previously possible .", "topics": ["calculus of variations", "numerical analysis"]}
{"title": "black-box importance sampling", "abstract": "importance sampling is widely used in machine learning and statistics , but its power is limited by the restriction of using simple proposals for which the importance weights can be tractably calculated . we address this problem by studying black-box importance sampling methods that calculate importance weights for samples generated from any unknown proposal or black-box mechanism . our method allows us to use better and richer proposals to solve difficult problems , and ( somewhat counter-intuitively ) also has the additional benefit of improving the estimation accuracy beyond typical importance sampling . both theoretical and empirical analyses are provided .", "topics": ["sampling ( signal processing )"]}
{"title": "performance analysis of neural network models for oxazolines and oxazoles derivatives descriptor dataset", "abstract": "neural networks have been used successfully to a broad range of areas such as business , data mining , drug discovery and biology . in medicine , neural networks have been applied widely in medical diagnosis , detection and evaluation of new drugs and treatment cost estimation . in addition , neural networks have begin practice in data mining strategies for the aim of prediction , knowledge discovery . this paper will present the application of neural networks for the prediction and analysis of antitubercular activity of oxazolines and oxazoles derivatives . this study presents techniques based on the development of single hidden layer neural network ( shlffnn ) , gradient descent back propagation neural network ( gdbpnn ) , gradient descent back propagation with momentum neural network ( gdbpmnn ) , back propagation with weight decay neural network ( bpwdnn ) and quantile regression neural network ( qrnn ) of artificial neural network ( ann ) models here , we comparatively evaluate the performance of five neural network techniques . the evaluation of the efficiency of each model by ways of benchmark experiments is an accepted application . cross-validation and resampling techniques are commonly used to derive point estimates of the performances which are compared to identify methods with good properties . predictive accuracy was evaluated using the root mean squared error ( rmse ) , coefficient determination ( ? ? ? ) , mean absolute error ( mae ) , mean percentage error ( mpe ) and relative square error ( rse ) . we found that all five neural network models were able to produce feasible models . qrnn model is outperforms with all statistical tests amongst other four models .", "topics": ["data mining", "neural networks"]}
{"title": "on the gap between strict-saddles and true convexity : an omega ( log d ) lower bound for eigenvector approximation", "abstract": "we prove a \\emph { query complexity } lower bound on rank-one principal component analysis ( pca ) . we consider an oracle model where , given a symmetric matrix $ m \\in \\mathbb { r } ^ { d \\times d } $ , an algorithm is allowed to make $ t $ \\emph { exact } queries of the form $ w^ { ( i ) } = mv^ { ( i ) } $ for $ i \\in \\ { 1 , \\dots , t\\ } $ , where $ v^ { ( i ) } $ is drawn from a distribution which depends arbitrarily on the past queries and measurements $ \\ { v^ { ( j ) } , w^ { ( j ) } \\ } _ { 1 \\le j \\le i-1 } $ . we show that for a small constant $ \\epsilon $ , any adaptive , randomized algorithm which can find a unit vector $ \\widehat { v } $ for which $ \\widehat { v } ^ { \\top } m\\widehat { v } \\ge ( 1-\\epsilon ) \\|m\\| $ , with even small probability , must make $ t = \\omega ( \\log d ) $ queries . in addition to settling a widely-held folk conjecture , this bound demonstrates a fundamental gap between convex optimization and `` strict-saddle '' non-convex optimization of which pca is a canonical example : in the former , first-order methods can have dimension-free iteration complexity , whereas in pca , the iteration complexity of gradient-based methods must necessarily grow with the dimension . our argument proceeds via a reduction to estimating the rank-one spike in a deformed wigner model . we establish lower bounds for this model by developing a `` truncated '' analogue of the $ \\chi^2 $ bayes-risk lower bound of chen et al .", "topics": ["approximation", "iteration"]}
{"title": "matrix and graph operations for relationship inference : an illustration with the kinship inference in the china biographical database", "abstract": "biographical databases contain diverse information about individuals . person names , birth information , career , friends , family and special achievements are some possible items in the record for an individual . the relationships between individuals , such as kinship and friendship , provide invaluable insights about hidden communities which are not directly recorded in databases . we show that some simple matrix and graph-based operations are effective for inferring relationships among individuals , and illustrate the main ideas with the china biographical database ( cbdb ) .", "topics": ["database"]}
{"title": "deep regionlets for object detection", "abstract": "a key challenge in generic object detection is being to handle large variations in object scale , poses , viewpoints , especially part deformations when determining the location for specified object categories . recent advances in deep neural networks have achieved promising results for object detection by extending the traditional detection methodologies using the convolutional neural network architectures . in this paper , we make an attempt to incorporate another traditional detection schema , regionlet into an end-to-end trained deep learning framework , and perform ablation studies on its behavior on multiple object detection datasets . more specifically , we propose a `` region selection network '' and a `` gating network '' . the region selection network serves as a guidance on where to select regions to learn the features from . additionally , the gating network serves as a local feature selection module to select and transform feature maps to be suitable for detection task . it acts as soft regionlet selection and pooling . the proposed network is trained end-to-end without additional efforts . extensive experiments and analysis on the pascal voc dataset and microsoft coco dataset show that the proposed framework achieves comparable state-of-the-art results .", "topics": ["object detection", "map"]}
{"title": "transferring knowledge from text to predict disease onset", "abstract": "in many domains such as medicine , training data is in short supply . in such cases , external knowledge is often helpful in building predictive models . we propose a novel method to incorporate publicly available domain expertise to build accurate models . specifically , we use word2vec models trained on a domain-specific corpus to estimate the relevance of each feature 's text description to the prediction problem . we use these relevance estimates to rescale the features , causing more important features to experience weaker regularization . we apply our method to predict the onset of five chronic diseases in the next five years in two genders and two age groups . our rescaling approach improves the accuracy of the model , particularly when there are few positive examples . furthermore , our method selects 60 % fewer features , easing interpretation by physicians . our method is applicable to other domains where feature and outcome descriptions are available .", "topics": ["test set", "matrix regularization"]}
{"title": "response generation in collaborative negotiation", "abstract": "in collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process . in cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs . this paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution . our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification . furthermore , by capturing the negotiation process in a recursive propose-evaluate-modify cycle of actions , our model can successfully handle embedded negotiation subdialogues .", "topics": ["autonomous car"]}
{"title": "resource allocation of mu-ofdm based cognitive radio systems under partial channel state information", "abstract": "this paper has been withdrawn by the author due to some errors .", "topics": ["simulation"]}
{"title": "the good old davis-putnam procedure helps counting models", "abstract": "as was shown recently , many important ai problems require counting the number of models of propositional formulas . the problem of counting models of such formulas is , according to present knowledge , computationally intractable in a worst case . based on the davis-putnam procedure , we present an algorithm , cdp , that computes the exact number of models of a propositional cnf or dnf formula f. let m and n be the number of clauses and variables of f , respectively , and let p denote the probability that a literal l of f occurs in a clause c of f , then the average running time of cdp is shown to be o ( nm^d ) , where d=-1/log ( 1-p ) . the practical performance of cdp has been estimated in a series of experiments on a wide variety of cnf formulas .", "topics": ["computational complexity theory", "time complexity"]}
{"title": "epistemic irrelevance in credal nets : the case of imprecise markov trees", "abstract": "we focus on credal nets , which are graphical models that generalise bayesian nets to imprecise probability . we replace the notion of strong independence commonly used in credal nets with the weaker notion of epistemic irrelevance , which is arguably more suited for a behavioural theory of probability . focusing on directed trees , we show how to combine the given local uncertainty models in the nodes of the graph into a global model , and we use this to construct and justify an exact message-passing algorithm that computes updated beliefs for a variable in the tree . the algorithm , which is linear in the number of nodes , is formulated entirely in terms of coherent lower previsions , and is shown to satisfy a number of rationality requirements . we supply examples of the algorithm 's operation , and report an application to on-line character recognition that illustrates the advantages of our approach for prediction . we comment on the perspectives , opened by the availability , for the first time , of a truly efficient algorithm based on epistemic irrelevance .", "topics": ["graphical model", "relevance"]}
{"title": "obda constraints for effective query answering ( extended version )", "abstract": "in ontology based data access ( obda ) users pose sparql queries over an ontology that lies on top of relational datasources . these queries are translated on-the-fly into sql queries by obda systems . standard sparql-to-sql translation techniques in obda often produce sql queries containing redundant joins and unions , even after a number of semantic and structural optimizations . these redundancies are detrimental to the performance of query answering , especially in complex industrial obda scenarios with large enterprise databases . to address this issue , we introduce two novel notions of obda constraints and show how to exploit them for efficient query answering . we conduct an extensive set of experiments on large datasets using real world data and queries , showing that these techniques strongly improve the performance of query answering up to orders of magnitude .", "topics": ["database"]}
{"title": "the bayesian linear information filtering problem", "abstract": "we present a bayesian sequential decision-making formulation of the information filtering problem , in which an algorithm presents items ( news articles , scientific papers , tweets ) arriving in a stream , and learns relevance from user feedback on presented items . we model user preferences using a bayesian linear model , similar in spirit to a bayesian linear bandit . we compute a computational upper bound on the value of the optimal policy , which allows computing an optimality gap for implementable policies . we then use this analysis as motivation in introducing a pair of new decompose-then-decide ( dtd ) heuristic policies , dtd-dynamic-programming ( dtd-dp ) and dtd-upper-confidence-bound ( dtd-ucb ) . we compare dtd-dp and dtd-ucb against several benchmarks on real and simulated data , demonstrating significant improvement , and show that the achieved performance is close to the upper bound .", "topics": ["simulation", "relevance"]}
{"title": "deep tempering", "abstract": "restricted boltzmann machines ( rbms ) are one of the fundamental building blocks of deep learning . approximate maximum likelihood training of rbms typically necessitates sampling from these models . in many training scenarios , computationally efficient gibbs sampling procedures are crippled by poor mixing . in this work we propose a novel method of sampling from boltzmann machines that demonstrates a computationally efficient way to promote mixing . our approach leverages an under-appreciated property of deep generative models such as the deep belief network ( dbn ) , where gibbs sampling from deeper levels of the latent variable hierarchy results in dramatically increased ergodicity . our approach is thus to train an auxiliary latent hierarchical model , based on the dbn . when used in conjunction with parallel-tempering , the method is asymptotically guaranteed to simulate samples from the target rbm . experimental results confirm the effectiveness of this sampling strategy in the context of rbm training .", "topics": ["sampling ( signal processing )", "computational complexity theory"]}
{"title": "light-weight place recognition and loop detection using road markings", "abstract": "in this paper , we propose an efficient algorithm for robust place recognition and loop detection using camera information only . our pipeline purely relies on spatial localization and semantic information of road markings . the creation of the database of road markings sequences is performed online , which makes the method applicable for real-time loop closure for visual slam techniques . furthermore , our algorithm is robust to various weather conditions , occlusions from vehicles , and shadows . we have performed an extensive number of experiments which highlight the effectiveness and scalability of the proposed method .", "topics": ["scalability"]}
{"title": "formulas for counting the sizes of markov equivalence classes of directed acyclic graphs", "abstract": "the sizes of markov equivalence classes of directed acyclic graphs play important roles in measuring the uncertainty and complexity in causal learning . a markov equivalence class can be represented by an essential graph and its undirected subgraphs determine the size of the class . in this paper , we develop a method to derive the formulas for counting the sizes of markov equivalence classes . we first introduce a new concept of core graph . the size of a markov equivalence class of interest is a polynomial of the number of vertices given its core graph . then , we discuss the recursive and explicit formula of the polynomial , and provide an algorithm to derive the size formula via symbolic computation for any given core graph . the proposed size formula derivation sheds light on the relationships between the size of a markov equivalence class and its representation graph , and makes size counting efficient , even when the essential graphs contain non-sparse undirected subgraphs .", "topics": ["sparse matrix", "computation"]}
{"title": "investigating recurrence and eligibility traces in deep q-networks", "abstract": "eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update . we investigate the use of eligibility traces in combination with recurrent networks in the atari domain . we illustrate the benefits of both recurrent nets and eligibility traces in some atari games , and highlight also the importance of the optimization used in the training .", "topics": ["reinforcement learning"]}
{"title": "on the convergence of maximum variance unfolding", "abstract": "maximum variance unfolding is one of the main methods for ( nonlinear ) dimensionality reduction . we study its large sample limit , providing specific rates of convergence under standard assumptions . we find that it is consistent when the underlying submanifold is isometric to a convex subset , and we provide some simple examples where it fails to be consistent .", "topics": ["nonlinear system"]}
{"title": "on learning by exchanging advice", "abstract": "one of the main questions concerning learning in multi-agent systems is : ( how ) can agents benefit from mutual interaction during the learning process ? . this paper describes the study of an interactive advice-exchange mechanism as a possible way to improve agents ' learning performance . the advice-exchange technique , discussed here , uses supervised learning ( backpropagation ) , where reinforcement is not directly coming from the environment but is based on advice given by peers with better performance score ( higher confidence ) , to enhance the performance of a heterogeneous group of learning agents ( las ) . the las are facing similar problems , in an environment where only reinforcement information is available . each la applies a different , well known , learning technique : random walk ( hill-climbing ) , simulated annealing , evolutionary algorithms and q-learning . the problem used for evaluation is a simplified traffic-control simulation . initial results indicate that advice-exchange can improve learning speed , although bad advice and/or blind reliance can disturb the learning performance .", "topics": ["supervised learning", "simulation"]}
{"title": "pfax : predictable feature analysis to perform control", "abstract": "predictable feature analysis ( pfa ) ( richthofer , wiskott , icmla 2015 ) is an algorithm that performs dimensionality reduction on high dimensional input signal . it extracts those subsignals that are most predictable according to a certain prediction model . we refer to these extracted signals as predictable features . in this work we extend the notion of pfa to take supplementary information into account for improving its predictions . such information can be a multidimensional signal like the main input to pfa , but is regarded external . that means it wo n't participate in the feature extraction - no features get extracted or composed of it . features will be exclusively extracted from the main input such that they are most predictable based on themselves and the supplementary information . we refer to this enhanced pfa as pfax ( pfa extended ) . even more important than improving prediction quality is to observe the effect of supplementary information on feature selection . pfax transparently provides insight how the supplementary information adds to prediction quality and whether it is valuable at all . finally we show how to invert that relation and can generate the supplementary information such that it would yield a certain desired outcome of the main signal . we apply this to a setting inspired by reinforcement learning and let the algorithm learn how to control an agent in an environment . with this method it is feasible to locally optimize the agent 's state , i.e . reach a certain goal that is near enough . we are preparing a follow-up paper that extends this method such that also global optimization is feasible .", "topics": ["feature extraction", "reinforcement learning"]}
{"title": "probabilistic graphical model based approach for water mapping using gaofen-2 ( gf-2 ) high resolution imagery and landsat 8 time series", "abstract": "the objective of this paper is to evaluate the potential of gaofen-2 ( gf-2 ) high resolution multispectral sensor ( ms ) and panchromatic ( pan ) imagery on water mapping . difficulties of water mapping on high resolution data includes : 1 ) misclassification between water and shadows or other low-reflectance ground objects , which is mostly caused by the spectral similarity within the given band range ; 2 ) small water bodies with size smaller than the spatial resolution of ms image . to solve the confusion between water and low-reflectance objects , the landsat 8 time series with two shortwave infrared ( swir ) bands is added because water has extremely strong absorption in swir . in order to integrate the three multi-sensor , multi-resolution data sets , the probabilistic graphical model ( pgm ) is utilized here with conditional probability distribution defined mainly based on the size of each object . for comparison , results from the svm classifier on the pca fused and ms data , thresholding method on the pan image , and water index method on the landsat data are computed . the confusion matrices are calculated for all the methods . the results demonstrate that the pgm method can achieve the best performance with the highest overall accuracy . moreover , small rivers can also be extracted by adding weight on the pan result in pgm . finally , the post-classification procedure is applied on the pgm result to further exclude misclassification in shadow and water-land boundary regions . accordingly , the producer 's , user 's and overall accuracy are all increased , indicating the effectiveness of our method .", "topics": ["graphical model", "time series"]}
{"title": "quasi-recurrent neural networks", "abstract": "recurrent neural networks are a powerful tool for modeling sequential data , but the dependence of each timestep 's computation on the previous timestep 's output limits parallelism and makes rnns unwieldy for very long sequences . we introduce quasi-recurrent neural networks ( qrnns ) , an approach to neural sequence modeling that alternates convolutional layers , which apply in parallel across timesteps , and a minimalist recurrent pooling function that applies in parallel across channels . despite lacking trainable recurrent layers , stacked qrnns have better predictive accuracy than stacked lstms of the same hidden size . due to their increased parallelism , they are up to 16 times faster at train and test time . experiments on language modeling , sentiment classification , and character-level neural machine translation demonstrate these advantages and underline the viability of qrnns as a basic building block for a variety of sequence tasks .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "multilingual topic models for unaligned text", "abstract": "we develop the multilingual topic model for unaligned text ( muto ) , a probabilistic model of text that is designed to analyze corpora composed of documents in two languages . from these documents , muto uses stochastic em to simultaneously discover both a matching between the languages and multilingual latent topics . we demonstrate that muto is able to find shared topics on real-world multilingual corpora , successfully pairing related documents across languages . muto provides a new framework for creating multilingual topic models without needing carefully curated parallel corpora and allows applications built using the topic model formalism to be applied to a much wider class of corpora .", "topics": ["unsupervised learning", "text corpus"]}
{"title": "adascan : adaptive scan pooling in deep convolutional neural networks for human action recognition in videos", "abstract": "we propose a novel method for temporally pooling frames in a video for the task of human action recognition . the method is motivated by the observation that there are only a small number of frames which , together , contain sufficient information to discriminate an action class present in a video , from the rest . the proposed method learns to pool such discriminative and informative frames , while discarding a majority of the non-informative frames in a single temporal scan of the video . our algorithm does so by continuously predicting the discriminative importance of each video frame and subsequently pooling them in a deep learning framework . we show the effectiveness of our proposed pooling method on standard benchmarks where it consistently improves on baseline pooling methods , with both rgb and optical flow based convolutional networks . further , in combination with complementary video representations , we show results that are competitive with respect to the state-of-the-art results on two challenging and publicly available benchmark datasets .", "topics": ["baseline ( configuration management )"]}
{"title": "comparative study of image registration techniques for bladder video-endoscopy", "abstract": "bladder cancer is widely spread in the world . many adequate diagnosis techniques exist . video-endoscopy remains the standard clinical procedure for visual exploration of the bladder internal surface . however , video-endoscopy presents the limit that the imaged area for each image is about nearly 1cm2 . and , lesions are , typically , spread over several images . the aim of this contribution is to assess the performance of two mosaicing algorithms leading to the construction of panoramic maps ( one unique image ) of bladder walls . the quantitative comparison study is performed on a set of real endoscopic exam data and on simulated data relative to bladder phantom .", "topics": ["simulation", "map"]}
{"title": "incorporating semi-supervised features into discontinuous easy-first constituent parsing", "abstract": "this paper describes adaptations for eafi , a parser for easy-first parsing of discontinuous constituents , to adapt it to multiple languages as well as make use of the unlabeled data that was provided as part of the spmrl shared task 2014 .", "topics": ["parsing"]}
{"title": "the power of normalization : faster evasion of saddle points", "abstract": "a commonly used heuristic in non-convex optimization is normalized gradient descent ( ngd ) - a variant of gradient descent in which only the direction of the gradient is taken into account and its magnitude ignored . we analyze this heuristic and show that with carefully chosen parameters and noise injection , this method can provably evade saddle points . we establish the convergence of ngd to a local minimum , and demonstrate rates which improve upon the fastest known first order algorithm due to ge e al . ( 2015 ) . the effectiveness of our method is demonstrated via an application to the problem of online tensor decomposition ; a task for which saddle point evasion is known to result in convergence to global minima .", "topics": ["gradient descent", "gradient"]}
{"title": "cauchy principal component analysis", "abstract": "principal component analysis ( pca ) has wide applications in machine learning , text mining and computer vision . classical pca based on a gaussian noise model is fragile to noise of large magnitude . laplace noise assumption based pca methods can not deal with dense noise effectively . in this paper , we propose cauchy principal component analysis ( cauchy pca ) , a very simple yet effective pca method which is robust to various types of noise . we utilize cauchy distribution to model noise and derive cauchy pca under the maximum likelihood estimation ( mle ) framework with low rank constraint . our method can robustly estimate the low rank matrix regardless of whether noise is large or small , dense or sparse . we analyze the robustness of cauchy pca from a robust statistics view and present an efficient singular value projection optimization method . experimental results on both simulated data and real applications demonstrate the robustness of cauchy pca to various noise patterns .", "topics": ["sparse matrix", "computer vision"]}
{"title": "benchmark environments for multitask learning in continuous domains", "abstract": "as demand drives systems to generalize to various domains and problems , the study of multitask , transfer and lifelong learning has become an increasingly important pursuit . in discrete domains , performance on the atari game suite has emerged as the de facto benchmark for assessing multitask learning . however , in continuous domains there is a lack of agreement on standard multitask evaluation environments which makes it difficult to compare different approaches fairly . in this work , we describe a benchmark set of tasks that we have developed in an extendable framework based on openai gym . we run a simple baseline using trust region policy optimization and release the framework publicly to be expanded and used for the systematic comparison of multitask , transfer , and lifelong learning in continuous domains .", "topics": ["baseline ( configuration management )"]}
{"title": "a family of simplified geometric distortion models for camera calibration", "abstract": "the commonly used radial distortion model for camera calibration is in fact an assumption or a restriction . in practice , camera distortion could happen in a general geometrical manner that is not limited to the radial sense . this paper proposes a simplified geometrical distortion modeling method by using two different radial distortion functions in the two image axes . a family of simplified geometric distortion models is proposed , which are either simple polynomials or the rational functions of polynomials . analytical geometric undistortion is possible using two of the distortion functions discussed in this paper and their performance can be improved by applying a piecewise fitting idea . our experimental results show that the geometrical distortion models always perform better than their radial distortion counterparts . furthermore , the proposed geometric modeling method is more appropriate for cameras whose distortion is not perfectly radially symmetric around the center of distortion .", "topics": ["polynomial"]}
{"title": "erratum : simplified drift analysis for proving lower bounds in evolutionary computation", "abstract": "this erratum points out an error in the simplified drift theorem ( sdt ) [ algorithmica 59 ( 3 ) , 369-386 , 2011 ] . it is also shown that a minor modification of one of its conditions is sufficient to establish a valid result . in many respects , the new theorem is more general than before . we no longer assume a markov process nor a finite search space . furthermore , the proof of the theorem is more compact than the previous ones . finally , previous applications of the sdt are revisited . it turns out that all of these either meet the modified condition directly or by means of few additional arguments .", "topics": ["computation"]}
{"title": "attention-based recurrent neural network models for joint intent detection and slot filling", "abstract": "attention-based encoder-decoder neural network models have recently shown promising results in machine translation and speech recognition . in this work , we propose an attention-based neural network model for joint intent detection and slot filling , both of which are critical steps for many speech understanding and dialog systems . unlike in machine translation and speech recognition , alignment is explicit in slot filling . we explore different strategies in incorporating this alignment information to the encoder-decoder framework . learning from the attention mechanism in encoder-decoder model , we further propose introducing attention to the alignment-based rnn models . such attentions provide additional information to the intent classification and slot label prediction . our independent task models achieve state-of-the-art intent detection error rate and slot filling f1 score on the benchmark atis task . our joint training model further obtains 0.56 % absolute ( 23.8 % relative ) error reduction on intent detection and 0.23 % absolute gain on slot filling over the independent task models .", "topics": ["recurrent neural network", "machine translation"]}
{"title": "geometric decomposition of feed forward neural networks", "abstract": "there have been several attempts to mathematically understand neural networks and many more from biological and computational perspectives . the field has exploded in the last decade , yet neural networks are still treated much like a black box . in this work we describe a structure that is inherent to a feed forward neural network . this will provide a framework for future work on neural networks to improve training algorithms , compute the homology of the network , and other applications . our approach takes a more geometric point of view and is unlike other attempts to mathematically understand neural networks that rely on a functional perspective .", "topics": ["neural networks"]}
{"title": "statistical keyword detection in literary corpora", "abstract": "understanding the complexity of human language requires an appropriate analysis of the statistical distribution of words in texts . we consider the information retrieval problem of detecting and ranking the relevant words of a text by means of statistical information referring to the `` spatial '' use of the words . shannon 's entropy of information is used as a tool for automatic keyword extraction . by using the origin of species by charles darwin as a representative text sample , we show the performance of our detector and compare it with another proposals in the literature . the random shuffled text receives special attention as a tool for calibrating the ranking indices .", "topics": ["time series", "text corpus"]}
{"title": "a bilinear programming approach for multiagent planning", "abstract": "multiagent planning and coordination problems are common and known to be computationally hard . we show that a wide range of two-agent problems can be formulated as bilinear programs . we present a successive approximation algorithm that significantly outperforms the coverage set algorithm , which is the state-of-the-art method for this class of multiagent problems . because the algorithm is formulated for bilinear programs , it is more general and simpler to implement . the new algorithm can be terminated at any time and-unlike the coverage set algorithm-it facilitates the derivation of a useful online performance bound . it is also much more efficient , on average reducing the computation time of the optimal solution by about four orders of magnitude . finally , we introduce an automatic dimensionality reduction method that improves the effectiveness of the algorithm , extending its applicability to new domains and providing a new way to analyze a subclass of bilinear programs .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "building instance classification using street view images", "abstract": "land-use classification based on spaceborne or aerial remote sensing images has been extensively studied over the past decades . such classification is usually a patch-wise or pixel-wise labeling over the whole image . but for many applications , such as urban population density mapping or urban utility planning , a classification map based on individual buildings is much more informative . however , such semantic classification still poses some fundamental challenges , for example , how to retrieve fine boundaries of individual buildings . in this paper , we proposed a general framework for classifying the functionality of individual buildings . the proposed method is based on convolutional neural networks ( cnns ) which classify facade structures from street view images , such as google streetview , in addition to remote sensing images which usually only show roof structures . geographic information was utilized to mask out individual buildings , and to associate the corresponding street view images . we created a benchmark dataset which was used for training and evaluating cnns . in addition , the method was applied to generate building classification maps on both region and city scales of several cities in canada and the us . keywords : cnn , building instance classification , street view images , openstreetmap", "topics": ["map", "pixel"]}
{"title": "binary embeddings with structured hashed projections", "abstract": "we consider the hashing mechanism for constructing binary embeddings , that involves pseudo-random projections followed by nonlinear ( sign function ) mappings . the pseudo-random projection is described by a matrix , where not all entries are independent random variables but instead a fixed `` budget of randomness '' is distributed across the matrix . such matrices can be efficiently stored in sub-quadratic or even linear space , provide reduction in randomness usage ( i.e . number of required random values ) , and very often lead to computational speed ups . we prove several theoretical results showing that projections via various structured matrices followed by nonlinear mappings accurately preserve the angular distance between input high-dimensional vectors . to the best of our knowledge , these results are the first that give theoretical ground for the use of general structured matrices in the nonlinear setting . in particular , they generalize previous extensions of the johnson-lindenstrauss lemma and prove the plausibility of the approach that was so far only heuristically confirmed for some special structured matrices . consequently , we show that many structured matrices can be used as an efficient information compression mechanism . our findings build a better understanding of certain deep architectures , which contain randomly weighted and untrained layers , and yet achieve high performance on different learning tasks . we empirically verify our theoretical findings and show the dependence of learning via structured hashed projections on the performance of neural network as well as nearest neighbor classifier .", "topics": ["nonlinear system", "heuristic"]}
{"title": "controlled experiments for word embeddings", "abstract": "an experimental approach to studying the properties of word embeddings is proposed . controlled experiments , achieved through modifications of the training corpus , permit the demonstration of direct relations between word properties and word vector direction and length . the approach is demonstrated using the word2vec cbow model with experiments that independently vary word frequency and word co-occurrence noise . the experiments reveal that word vector length depends more or less linearly on both word frequency and the level of noise in the co-occurrence distribution of the word . the coefficients of linearity depend upon the word . the special point in feature space , defined by the ( artificial ) word with pure noise in its co-occurrence distribution , is found to be small but non-zero .", "topics": ["feature vector", "simulation"]}
{"title": "dr-bilstm : dependent reading bidirectional lstm for natural language inference", "abstract": "we present a novel deep learning architecture to address the natural language inference ( nli ) task . existing approaches mostly rely on simple reading mechanisms for independent encoding of the premise and hypothesis . instead , we propose a novel dependent reading bidirectional lstm network ( dr-bilstm ) to efficiently model the relationship between a premise and a hypothesis during encoding and inference . we also introduce a sophisticated ensemble strategy to combine our proposed models , which noticeably improves final predictions . finally , we demonstrate how the results can be improved further with an additional preprocessing step . our evaluation shows that dr-bilstm obtains the best single model and ensemble model results achieving the new state-of-the-art scores on the stanford nli dataset .", "topics": ["natural language"]}
{"title": "a geometric approach to harmonic color palette design", "abstract": "we address the problem of finding harmonic colors , this problem has many applications , from fashion to industrial design . in order to solve this problem we consider that colors follow normal distributions in tone ( chroma and lightness ) and hue . the proposed approach relies in the cie standard for representing colors and evaluate proximity . other approaches to this problem use a set of rules . experimental results show that lines with specific parameters angles of inclination , and distance from the reference point are preferred over others , and that uncertain line patterns outperform non-linear patterns .", "topics": ["nonlinear system"]}
{"title": "on the reducibility of submodular functions", "abstract": "the scalability of submodular optimization methods is critical for their usability in practice . in this paper , we study the reducibility of submodular functions , a property that enables us to reduce the solution space of submodular optimization problems without performance loss . we introduce the concept of reducibility using marginal gains . then we show that by adding perturbation , we can endow irreducible functions with reducibility , based on which we propose the perturbation-reduction optimization framework . our theoretical analysis proves that given the perturbation scales , the reducibility gain could be computed , and the performance loss has additive upper bounds . we further conduct empirical studies and the results demonstrate that our proposed framework significantly accelerates existing optimization methods for irreducible submodular functions with a cost of only small performance losses .", "topics": ["scalability"]}
{"title": "intelligent word embeddings of free-text radiology reports", "abstract": "radiology reports are a rich resource for advancing deep learning applications in medicine by leveraging the large volume of data continuously being updated , integrated , and shared . however , there are significant challenges as well , largely due to the ambiguity and subtlety of natural language . we propose a hybrid strategy that combines semantic-dictionary mapping and word2vec modeling for creating dense vector embeddings of free-text radiology reports . our method leverages the benefits of both semantic-dictionary mapping as well as unsupervised learning . using the vector representation , we automatically classify the radiology reports into three classes denoting confidence in the diagnosis of intracranial hemorrhage by the interpreting radiologist . we performed experiments with varying hyperparameter settings of the word embeddings and a range of different classifiers . best performance achieved was a weighted precision of 88 % and weighted recall of 90 % . our work offers the potential to leverage unstructured electronic health record data by allowing direct analysis of narrative clinical notes .", "topics": ["unsupervised learning", "natural language"]}
{"title": "algorithm selection for combinatorial search problems : a survey", "abstract": "the algorithm selection problem is concerned with selecting the best algorithm to solve a given problem on a case-by-case basis . it has become especially relevant in the last decade , as researchers are increasingly investigating how to identify the most suitable existing algorithm for solving a problem instead of developing new algorithms . this survey presents an overview of this work focusing on the contributions made in the area of combinatorial search problems , where algorithm selection techniques have achieved significant performance improvements . we unify and organise the vast literature according to criteria that determine algorithm selection systems in practice . the comprehensive classification of approaches identifies and analyses the different directions from which algorithm selection has been approached . this paper contrasts and compares different methods for solving the problem as well as ways of using these solutions . it closes by identifying directions of current and future research .", "topics": ["heuristic", "artificial intelligence"]}
{"title": "online matrix completion through nuclear norm regularisation", "abstract": "it is the main goal of this paper to propose a novel method to perform matrix completion on-line . motivated by a wide variety of applications , ranging from the design of recommender systems to sensor network localization through seismic data reconstruction , we consider the matrix completion problem when entries of the matrix of interest are observed gradually . precisely , we place ourselves in the situation where the predictive rule should be refined incrementally , rather than recomputed from scratch each time the sample of observed entries increases . the extension of existing matrix completion methods to the sequential prediction context is indeed a major issue in the big data era , and yet little addressed in the literature . the algorithm promoted in this article builds upon the soft impute approach introduced in mazumder et al . ( 2010 ) . the major novelty essentially arises from the use of a randomised technique for both computing and updating the singular value decomposition ( svd ) involved in the algorithm . though of disarming simplicity , the method proposed turns out to be very efficient , while requiring reduced computations . several numerical experiments based on real datasets illustrating its performance are displayed , together with preliminary results giving it a theoretical basis .", "topics": ["numerical analysis", "computation"]}
{"title": "hierarchical spatial transformer network", "abstract": "computer vision researchers have been expecting that neural networks have spatial transformation ability to eliminate the interference caused by geometric distortion for a long time . emergence of spatial transformer network makes dream come true . spatial transformer network and its variants can handle global displacement well , but lack the ability to deal with local spatial variance . hence how to achieve a better manner of deformation in the neural network has become a pressing matter of the moment . to address this issue , we analyze the advantages and disadvantages of approximation theory and optical flow theory , then we combine them to propose a novel way to achieve image deformation and implement it with a hierarchical convolutional neural network . this new approach solves for a linear deformation along with an optical flow field to model image deformation . in the experiments of cluttered mnist handwritten digits classification and image plane alignment , our method outperforms baseline methods by a large margin .", "topics": ["baseline ( configuration management )", "computer vision"]}
{"title": "deep extreme multi-label learning", "abstract": "extreme multi-label learning ( xml ) or classification has been a practical and important problem since the boom of big data . the main challenge lies in the exponential label space which involves $ 2^l $ possible label sets when the label dimension $ l $ is very large , e.g . , in millions for wikipedia labels . this paper is motivated to better explore the label space by building and modeling an explicit label graph . in the meanwhile , deep learning has been widely studied and used in various classification problems including multi-label classification , however it has not been sufficiently studied in this extreme but practical case , where the label space can be as large as in millions . in this paper , we propose a practical deep embedding method for extreme multi-label classification . our method harvests the ideas of non-linear embedding and modeling label space with graph priors at the same time . extensive experiments on public datasets for xml show that our method performs competitively against state-of-the-art result .", "topics": ["test set", "feature vector"]}
{"title": "alternative markov and causal properties for acyclic directed mixed graphs", "abstract": "we extend andersson-madigan-perlman chain graphs by ( i ) relaxing the semidirected acyclity constraint so that only directed cycles are forbidden , and ( ii ) allowing up to two edges between any pair of nodes . we introduce global , and ordered local and pairwise markov properties for the new models . we show the equivalence of these properties for strictly positive probability distributions . we also show that when the random variables are continuous , the new models can be interpreted as systems of structural equations with correlated errors . this enables us to adapt pearl 's do-calculus to them . finally , we describe an exact algorithm for learning the new models from observational and interventional data via answer set programming .", "topics": ["markov chain", "causality"]}
{"title": "is ( iris security )", "abstract": "in the paper will be presented a safety system based on iridology . the results suggest a new scenario where the security problem in supervised and unsupervised areas can be treat with the present system and the iris image recognition .", "topics": ["unsupervised learning", "computer vision"]}
{"title": "robust real-time multi-view eye tracking", "abstract": "despite significant advances in improving the gaze tracking accuracy under controlled conditions , the tracking robustness under real-world conditions , such as large head pose and movements , use of eyeglasses , illumination and eye type variations , remains a major challenge in eye tracking . in this paper , we revisit this challenge and introduce a real-time multi-camera eye tracking framework to improve the tracking robustness . first , differently from previous work , we design a multi-view tracking setup that allows for acquiring multiple eye appearances simultaneously . leveraging multi-view appearances enables to more reliably detect gaze features under challenging conditions , particularly when they are obstructed in conventional single-view appearance due to large head movements or eyewear effects . the features extracted on various appearances are then used for estimating multiple gaze outputs . second , we propose to combine estimated gaze outputs through an adaptive fusion mechanism to compute user 's overall point of regard . the proposed mechanism firstly determines the estimation reliability of each gaze output according to user 's momentary head pose and predicted gazing behavior , and then performs a reliability-based weighted fusion . we demonstrate the efficacy of our framework with extensive simulations and user experiments on a collected dataset featuring 20 subjects . our results show that in comparison with state-of-the-art eye trackers , the proposed framework provides not only a significant enhancement in accuracy but also a notable robustness . our prototype system runs at 30 frames-per-second ( fps ) and achieves 1 degree accuracy under challenging experimental scenarios , which makes it suitable for applications demanding high accuracy and robustness .", "topics": ["simulation"]}
{"title": "learning in an uncertain world : representing ambiguity through multiple hypotheses", "abstract": "many prediction tasks contain uncertainty . in some cases , uncertainty is inherent in the task itself . in future prediction , for example , many distinct outcomes are equally valid . in other cases , uncertainty arises from the way data is labeled . for example , in object detection , many objects of interest often go unlabeled , and in human pose estimation , occluded joints are often labeled with ambiguous values . in this work we focus on a principled approach for handling such scenarios . in particular , we propose a framework for reformulating existing single-prediction models as multiple hypothesis prediction ( mhp ) models and an associated meta loss and optimization procedure to train them . to demonstrate our approach , we consider four diverse applications : human pose estimation , future prediction , image classification and segmentation . we find that mhp models outperform their single-hypothesis counterparts in all cases , and that mhp models simultaneously expose valuable insights into the variability of predictions .", "topics": ["value ( ethics )", "object detection"]}
{"title": "scaling up dynamic topic models", "abstract": "dynamic topic models ( dtms ) are very effective in discovering topics and capturing their evolution trends in time series data . to do posterior inference of dtms , existing methods are all batch algorithms that scan the full dataset before each update of the model and make inexact variational approximations with mean-field assumptions . due to a lack of a more scalable inference algorithm , despite the usefulness , dtms have not captured large topic dynamics . this paper fills this research void , and presents a fast and parallelizable inference algorithm using gibbs sampling with stochastic gradient langevin dynamics that does not make any unwarranted assumptions . we also present a metropolis-hastings based $ o ( 1 ) $ sampler for topic assignments for each word token . in a distributed environment , our algorithm requires very little communication between workers during sampling ( almost embarrassingly parallel ) and scales up to large-scale applications . we are able to learn the largest dynamic topic model to our knowledge , and learned the dynamics of 1,000 topics from 2.6 million documents in less than half an hour , and our empirical results show that our algorithm is not only orders of magnitude faster than the baselines but also achieves lower perplexity .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "an evolutionary computational based approach towards automatic image registration", "abstract": "image registration is a key component of various image processing operations which involve the analysis of different image data sets . automatic image registration domains have witnessed the application of many intelligent methodologies over the past decade ; however inability to properly model object shape as well as contextual information had limited the attainable accuracy . in this paper , we propose a framework for accurate feature shape modeling and adaptive resampling using advanced techniques such as vector machines , cellular neural network ( cnn ) , sift , coreset , and cellular automata . cnn has found to be effective in improving feature matching as well as resampling stages of registration and complexity of the approach has been considerably reduced using corset optimization the salient features of this work are cellular neural network approach based sift feature point optimisation , adaptive resampling and intelligent object modelling . developed methodology has been compared with contemporary methods using different statistical measures . investigations over various satellite images revealed that considerable success was achieved with the approach . system has dynamically used spectral and spatial information for representing contextual knowledge using cnn-prolog approach . methodology also illustrated to be effective in providing intelligent interpretation and adaptive resampling .", "topics": ["image processing", "mathematical optimization"]}
{"title": "fast scene understanding for autonomous driving", "abstract": "most approaches for instance-aware semantic labeling traditionally focus on accuracy . other aspects like runtime and memory footprint are arguably as important for real-time applications such as autonomous driving . motivated by this observation and inspired by recent works that tackle multiple tasks with a single integrated architecture , in this paper we present a real-time efficient implementation based on enet that solves three autonomous driving related tasks at once : semantic scene segmentation , instance segmentation and monocular depth estimation . our approach builds upon a branched enet architecture with a shared encoder but different decoder branches for each of the three tasks . the presented method can run at 21 fps at a resolution of 1024x512 on the cityscapes dataset without sacrificing accuracy compared to running each task separately .", "topics": ["encoder", "autonomous car"]}
{"title": "lpar-05 workshop : empirically successfull automated reasoning in higher-order logic ( eshol )", "abstract": "this workshop brings together practioners and researchers who are involved in the everyday aspects of logical systems based on higher-order logic . we hope to create a friendly and highly interactive setting for discussions around the following four topics . implementation and development of proof assistants based on any notion of impredicativity , automated theorem proving tools for higher-order logic reasoning systems , logical framework technology for the representation of proofs in higher-order logic , formal digital libraries for storing , maintaining and querying databases of proofs . we envision attendees that are interested in fostering the development and visibility of reasoning systems for higher-order logics . we are particularly interested in a discusssion on the development of a higher-order version of the tptp and in comparisons of the practical strengths of automated higher-order reasoning systems . additionally , the workshop includes system demonstrations . eshol is the successor of the escar and esfor workshops held at cade 2005 and ijcar 2004 .", "topics": ["value ( ethics )"]}
{"title": "updating the vesicle-cnn synapse detector", "abstract": "we present an updated version of the vesicle-cnn algorithm presented by roncal et al . ( 2014 ) . the original implementation makes use of a patch-based approach . this methodology is known to be slow due to repeated computations . we update this implementation to be fully convolutional through the use of dilated convolutions , recovering the expanded field of view achieved through the use of strided maxpools , but without a degradation of spatial resolution . this updated implementation performs as well as the original implementation , but with a $ 600\\times $ speedup at test time . we release source code and data into the public domain .", "topics": ["computation", "convolution"]}
{"title": "a selective macro-learning algorithm and its application to the nxn sliding-tile puzzle", "abstract": "one of the most common mechanisms used for speeding up problem solvers is macro-learning . macros are sequences of basic operators acquired during problem solving . macros are used by the problem solver as if they were basic operators . the major problem that macro-learning presents is the vast number of macros that are available for acquisition . macros increase the branching factor of the search space and can severely degrade problem-solving efficiency . to make macro learning useful , a program must be selective in acquiring and utilizing macros . this paper describes a general method for selective acquisition of macros . solvable training problems are generated in increasing order of difficulty . the only macros acquired are those that take the problem solver out of a local minimum to a better state . the utility of the method is demonstrated in several domains , including the domain of nxn sliding-tile puzzles . after learning on small puzzles , the system is able to efficiently solve puzzles of any size .", "topics": ["computational complexity theory"]}
{"title": "encoding high dimensional local features by sparse coding based fisher vectors", "abstract": "deriving from the gradient vector of a generative model of local features , fisher vector coding ( fvc ) has been identified as an effective coding method for image classification . most , if not all , % fvc implementations employ the gaussian mixture model ( gmm ) to characterize the generation process of local features . this choice has shown to be sufficient for traditional low dimensional local features , e.g . , sift ; and typically , good performance can be achieved with only a few hundred gaussian distributions . however , the same number of gaussians is insufficient to model the feature space spanned by higher dimensional local features , which have become popular recently . in order to improve the modeling capacity for high dimensional features , it turns out to be inefficient and computationally impractical to simply increase the number of gaussians . in this paper , we propose a model in which each local feature is drawn from a gaussian distribution whose mean vector is sampled from a subspace . with certain approximation , this model can be converted to a sparse coding procedure and the learning/inference problems can be readily solved by standard sparse coding methods . by calculating the gradient vector of the proposed model , we derive a new fisher vector encoding strategy , termed sparse coding based fisher vector coding ( scfvc ) . moreover , we adopt the recently developed deep convolutional neural network ( cnn ) descriptor as a high dimensional local feature and implement image classification with the proposed scfvc . our experimental evaluations demonstrate that our method not only significantly outperforms the traditional gmm based fisher vector encoding but also achieves the state-of-the-art performance in generic object recognition , indoor scene , and fine-grained image classification problems .", "topics": ["generative model", "feature vector"]}
{"title": "glasgow 's stereo image database of garments", "abstract": "to provide insight into cloth perception and manipulation with an active binocular robotic vision system , we compiled a database of 80 stereo-pair colour images with corresponding horizontal and vertical disparity maps and mask annotations , for 3d garment point cloud rendering has been created and released . the stereo-image garment database is part of research conducted under the eu-fp7 clothes perception and manipulation ( clopema ) project and belongs to a wider database collection released through clopema ( www.clopema.eu ) . this database is based on 16 different off-the-shelve garments . each garment has been imaged in five different pose configurations on the project 's binocular robot head . a full copy of the database is made available for scientific research only at https : //sites.google.com/site/ugstereodatabase/ .", "topics": ["map", "robot"]}
{"title": "artificial intelligence and legal liability", "abstract": "a recent issue of a popular computing journal asked which laws would apply if a self-driving car killed a pedestrian . this paper considers the question of legal liability for artificially intelligent computer systems . it discusses whether criminal liability could ever apply ; to whom it might apply ; and , under civil law , whether an ai program is a product that is subject to product design legislation or a service to which the tort of negligence applies . the issue of sales warranties is also considered . a discussion of some of the practical limitations that ai systems are subject to is also included .", "topics": ["artificial intelligence", "autonomous car"]}
{"title": "computationally efficient estimators for dimension reductions using stable random projections", "abstract": "the method of stable random projections is a tool for efficiently computing the $ l_\\alpha $ distances using low memory , where $ 0 < \\alpha \\leq 2 $ is a tuning parameter . the method boils down to a statistical estimation task and various estimators have been proposed , based on the geometric mean , the harmonic mean , and the fractional power etc . this study proposes the optimal quantile estimator , whose main operation is selecting , which is considerably less expensive than taking fractional power , the main operation in previous estimators . our experiments report that the optimal quantile estimator is nearly one order of magnitude more computationally efficient than previous estimators . for large-scale learning tasks in which storing and computing pairwise distances is a serious bottleneck , this estimator should be desirable . in addition to its computational advantages , the optimal quantile estimator exhibits nice theoretical properties . it is more accurate than previous estimators when $ \\alpha > 1 $ . we derive its theoretical error bounds and establish the explicit ( i.e . , no hidden constants ) sample complexity bound .", "topics": ["computational complexity theory"]}
{"title": "multi-range reasoning for machine comprehension", "abstract": "we propose mru ( multi-range reasoning units ) , a new fast compositional encoder for machine comprehension ( mc ) . our proposed mru encoders are characterized by multi-ranged gating , executing a series of parameterized contract-and-expand layers for learning gating vectors that benefit from long and short-term dependencies . the aims of our approach are as follows : ( 1 ) learning representations that are concurrently aware of long and short-term context , ( 2 ) modeling relationships between intra-document blocks and ( 3 ) fast and efficient sequence encoding . we show that our proposed encoder demonstrates promising results both as a standalone encoder and as well as a complementary building block . we conduct extensive experiments on three challenging mc datasets , namely race , searchqa and narrativeqa , achieving highly competitive performance on all . on the race benchmark , our model outperforms dfn ( dynamic fusion networks ) by 1.5 % -6 % without using any recurrent or convolution layers . similarly , we achieve competitive performance relative to amanda on the searchqa benchmark and bidaf on the narrativeqa benchmark without using any lstm/gru layers . finally , incorporating mru encoders with standard bilstm architectures further improves performance , achieving state-of-the-art results .", "topics": ["encoder", "convolution"]}
{"title": "improving image restoration with soft-rounding", "abstract": "several important classes of images such as text , barcode and pattern images have the property that pixels can only take a distinct subset of values . this knowledge can benefit the restoration of such images , but it has not been widely considered in current restoration methods . in this work , we describe an effective and efficient approach to incorporate the knowledge of distinct pixel values of the pristine images into the general regularized least squares restoration framework . we introduce a new regularizer that attains zero at the designated pixel values and becomes a quadratic penalty function in the intervals between them . when incorporated into the regularized least squares restoration framework , this regularizer leads to a simple and efficient step that resembles and extends the rounding operation , which we term as soft-rounding . we apply the soft-rounding enhanced solution to the restoration of binary text/barcode images and pattern images with multiple distinct pixel values . experimental results show that soft-rounding enhanced restoration methods achieve significant improvement in both visual quality and quantitative measures ( psnr and ssim ) . furthermore , we show that this regularizer can also benefit the restoration of general natural images .", "topics": ["pixel"]}
{"title": "automated unsupervised segmentation of liver lesions in ct scans via cahn-hilliard phase separation", "abstract": "the segmentation of liver lesions is crucial for detection , diagnosis and monitoring progression of liver cancer . however , design of accurate automated methods remains challenging due to high noise in ct scans , low contrast between liver and lesions , as well as large lesion variability . we propose a 3d automatic , unsupervised method for liver lesions segmentation using a phase separation approach . it is assumed that liver is a mixture of two phases : healthy liver and lesions , represented by different image intensities polluted by noise . the cahn-hilliard equation is used to remove the noise and separate the mixture into two distinct phases with well-defined interfaces . this simplifies the lesion detection and segmentation task drastically and enables to segment liver lesions by thresholding the cahn-hilliard solution . the method was tested on 3dircadb and lits dataset .", "topics": ["unsupervised learning"]}
{"title": "a large annotated corpus for learning natural language inference", "abstract": "understanding entailment and contradiction is fundamental to understanding natural language , and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations . however , machine learning research in this area has been dramatically limited by the lack of large-scale resources . to address this , we introduce the stanford natural language inference corpus , a new , freely available collection of labeled sentence pairs , written by humans doing a novel grounded task based on image captioning . at 570k pairs , it is two orders of magnitude larger than all other resources of its type . this increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models , and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time .", "topics": ["natural language"]}
{"title": "variational chernoff bounds for graphical models", "abstract": "recent research has made significant progress on the problem of bounding log partition functions for exponential family graphical models . such bounds have associated dual parameters that are often used as heuristic estimates of the marginal probabilities required in inference and learning . however these variational estimates do not give rigorous bounds on marginal probabilities , nor do they give estimates for probabilities of more general events than simple marginals . in this paper we build on this recent work by deriving rigorous upper and lower bounds on event probabilities for graphical models . our approach is based on the use of generalized chernoff bounds to express bounds on event probabilities in terms of convex optimization problems ; these optimization problems , in turn , require estimates of generalized log partition functions . simulations indicate that this technique can result in useful , rigorous bounds to complement the heuristic variational estimates , with comparable computational cost .", "topics": ["graphical model", "calculus of variations"]}
{"title": "action tubelet detector for spatio-temporal action localization", "abstract": "current state-of-the-art approaches for spatio-temporal action localization rely on detections at the frame level that are then linked or tracked across time . in this paper , we leverage the temporal continuity of videos instead of operating at the frame level . we propose the action tubelet detector ( act-detector ) that takes as input a sequence of frames and outputs tubelets , i.e . , sequences of bounding boxes with associated scores . the same way state-of-the-art object detectors rely on anchor boxes , our act-detector is based on anchor cuboids . we build upon the ssd framework . convolutional features are extracted for each frame , while scores and regressions are based on the temporal stacking of these features , thus exploiting information from a sequence . our experimental results show that leveraging sequences of frames significantly improves detection performance over using individual frames . the gain of our tubelet detector can be explained by both more accurate scores and more precise localization . our act-detector outperforms the state-of-the-art methods for frame-map and video-map on the j-hmdb and ucf-101 datasets , in particular at high overlap thresholds .", "topics": ["sensor"]}
{"title": "matrix games , linear programming , and linear approximation", "abstract": "the following four classes of computational problems are equivalent : solving matrix games , solving linear programs , best $ l^ { \\infty } $ linear approximation , best $ l^1 $ linear approximation .", "topics": ["approximation"]}
{"title": "doubly-attentive decoder for multi-modal neural machine translation", "abstract": "we introduce a multi-modal neural machine translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks , bridging the gap between image description and translation . our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language . we find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only mt corpora . we also report state-of-the-art results on the multi30k data set .", "topics": ["machine translation", "text corpus"]}
{"title": "l2-nonexpansive neural networks", "abstract": "this paper proposes a class of well-conditioned neural networks in which a unit amount of change in the inputs causes at most a unit amount of change in the outputs or any of the internal layers . we develop the known methodology of controlling lipschitz constants to realize its full potential in maximizing robustness : our linear and convolution layers subsume those in the previous parseval networks as a special case and allow greater degrees of freedom ; aggregation , pooling , splitting and other operators are adapted in new ways , and a new loss function is proposed , all for the purpose of improving robustness . with mnist and cifar-10 classifiers , we demonstrate a number of advantages . without needing any adversarial training , the proposed classifiers exceed the state of the art in robustness against white-box l2-bounded adversarial attacks . their outputs are quantitatively more meaningful than ordinary networks and indicate levels of confidence . they are also free of exploding gradients , among other desirable properties .", "topics": ["neural networks", "loss function"]}
{"title": "proceedings of the sixth conference on uncertainty in artificial intelligence ( 1990 )", "abstract": "this is the proceedings of the sixth conference on uncertainty in artificial intelligence , which was held in cambridge , ma , jul 27 - jul 29 , 1990", "topics": ["artificial intelligence"]}
{"title": "new approach using bayesian network to improve content based image classification systems", "abstract": "this paper proposes a new approach based on augmented naive bayes for image classification . initially , each image is cutting in a whole of blocks . for each block , we compute a vector of descriptors . then , we propose to carry out a classification of the vectors of descriptors to build a vector of labels for each image . finally , we propose three variants of bayesian networks such as naive bayesian network ( nb ) , tree augmented naive bayes ( tan ) and forest augmented naive bayes ( fan ) to classify the image using the vector of labels . the results showed a marked improvement over the fan , nb and tan .", "topics": ["computer vision", "bayesian network"]}
{"title": "segan : segmenting and generating the invisible", "abstract": "objects often occlude each other in scenes ; inferring their appearance beyond their visible parts plays an important role in scene understanding , depth estimation , object interaction and manipulation . in this paper , we study the challenging problem of completing the appearance of occluded objects . doing so requires knowing which pixels to paint ( segmenting the invisible parts of objects ) and what color to paint them ( generating the invisible parts ) . our proposed novel solution , segan , jointly optimizes for both segmentation and generation of the invisible parts of objects . our experimental results show that : ( a ) segan can learn to generate the appearance of the occluded parts of objects ; ( b ) segan outperforms state-of-the-art segmentation baselines for the invisible parts of objects ; ( c ) trained on synthetic photo realistic images , segan can reliably segment natural images ; ( d ) by reasoning about occluder occludee relations , our method can infer depth layering .", "topics": ["mathematical optimization", "synthetic data"]}
{"title": "error rate bounds in crowdsourcing models", "abstract": "crowdsourcing is an effective tool for human-powered computation on many tasks challenging for computers . in this paper , we provide finite-sample exponential bounds on the error rate ( in probability and in expectation ) of hyperplane binary labeling rules under the dawid-skene crowdsourcing model . the bounds can be applied to analyze many common prediction methods , including the majority voting and weighted majority voting . these bound results could be useful for controlling the error rate and designing better algorithms . we show that the oracle maximum a posterior ( map ) rule approximately optimizes our upper bound on the mean error rate for any hyperplane binary labeling rule , and propose a simple data-driven weighted majority voting ( wmv ) rule ( called one-step wmv ) that attempts to approximate the oracle map and has a provable theoretical guarantee on the error rate . moreover , we use simulated and real data to demonstrate that the data-driven em-map rule is a good approximation to the oracle map rule , and to demonstrate that the mean error rate of the data-driven em-map rule is also bounded by the mean error rate bound of the oracle map rule with estimated parameters plugging into the bound .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "an artificial agent for robust image registration", "abstract": "3-d image registration , which involves aligning two or more images , is a critical step in a variety of medical applications from diagnosis to therapy . image registration is commonly performed by optimizing an image matching metric as a cost function . however , this task is challenging due to the non-convex nature of the matching metric over the plausible registration parameter space and insufficient approaches for a robust optimization . as a result , current approaches are often customized to a specific problem and sensitive to image quality and artifacts . in this paper , we propose a completely different approach to image registration , inspired by how experts perform the task . we first cast the image registration problem as a `` strategy learning '' process , where the goal is to find the best sequence of motion actions ( e.g . up , down , etc . ) that yields image alignment . within this approach , an artificial agent is learned , modeled using deep convolutional neural networks , with 3d raw image data as the input , and the next optimal action as the output . to cope with the dimensionality of the problem , we propose a greedy supervised approach for an end-to-end training , coupled with attention-driven hierarchical strategy . the resulting registration approach inherently encodes both a data-driven matching metric and an optimal registration strategy ( policy ) . we demonstrate , on two 3-d/3-d medical image registration examples with drastically different nature of challenges , that the artificial agent outperforms several state-of-art registration methods by a large margin in terms of both accuracy and robustness .", "topics": ["loss function", "end-to-end principle"]}
{"title": "towards unsupervised learning of temporal relations between events", "abstract": "automatic extraction of temporal relations between event pairs is an important task for several natural language processing applications such as question answering , information extraction , and summarization . since most existing methods are supervised and require large corpora , which for many languages do not exist , we have concentrated our efforts to reduce the need for annotated data as much as possible . this paper presents two different algorithms towards this goal . the first algorithm is a weakly supervised machine learning approach for classification of temporal relations between events . in the first stage , the algorithm learns a general classifier from an annotated corpus . then , inspired by the hypothesis of `` one type of temporal relation per discourse , it extracts useful information from a cluster of topically related documents . we show that by combining the global information of such a cluster with local decisions of a general classifier , a bootstrapping cross-document classifier can be built to extract temporal relations between events . our experiments show that without any additional annotated data , the accuracy of the proposed algorithm is higher than that of several previous successful systems . the second proposed method for temporal relation extraction is based on the expectation maximization ( em ) algorithm . within em , we used different techniques such as a greedy best-first search and integer linear programming for temporal inconsistency removal . we think that the experimental results of our em based algorithm , as a first step toward a fully unsupervised temporal relation extraction method , is encouraging .", "topics": ["supervised learning", "natural language processing"]}
{"title": "long short-term memory for japanese word segmentation", "abstract": "this study presents a long short-term memory ( lstm ) neural network approach to japanese word segmentation ( jws ) . previous studies on chinese word segmentation ( cws ) succeeded in using recurrent neural networks such as lstm and gated recurrent units ( gru ) . however , in contrast to chinese , japanese includes several character types , such as hiragana , katakana , and kanji , that produce orthographic variations and increase the difficulty of word segmentation . additionally , it is important for jws tasks to consider a global context , and yet traditional jws approaches rely on local features . in order to address this problem , this study proposes employing an lstm-based approach to jws . the experimental results indicate that the proposed model achieves state-of-the-art accuracy with respect to various japanese corpora .", "topics": ["recurrent neural network", "text corpus"]}
{"title": "an approximate nonmyopic computation for value of information", "abstract": "value-of-information analyses provide a straightforward means for selecting the best next observation to make , and for determining whether it is better to gather additional information or to act immediately . determining the next best test to perform , given a state of uncertainty about the world , requires a consideration of the value of making all possible sequences of observations . in practice , decision analysts and expert-system designers have avoided the intractability of exact computation of the value of information by relying on a myopic approximation . myopic analyses are based on the assumption that only one additional test will be performed , even when there is an opportunity to make a large number of observations . we present a nonmyopic approximation for value of information that bypasses the traditional myopic analyses by exploiting the statistical properties of large samples .", "topics": ["computation"]}
{"title": "sentence directed video object codetection", "abstract": "we tackle the problem of video object codetection by leveraging the weak semantic constraint implied by sentences that describe the video content . unlike most existing work that focuses on codetecting large objects which are usually salient both in size and appearance , we can codetect objects that are small or medium sized . our method assumes no human pose or depth information such as is required by the most recent state-of-the-art method . we employ weak semantic constraint on the codetection process by pairing the video with sentences . although the semantic information is usually simple and weak , it can greatly boost the performance of our codetection framework by reducing the search space of the hypothesized object detections . our experiment demonstrates an average iou score of 0.423 on a new challenging dataset which contains 15 object classes and 150 videos with 12,509 frames in total , and an average iou score of 0.373 on a subset of an existing dataset , originally intended for activity recognition , which contains 5 object classes and 75 videos with 8,854 frames in total .", "topics": ["sensor"]}
{"title": "a* sampling", "abstract": "the problem of drawing samples from a discrete distribution can be converted into a discrete optimization problem . in this work , we show how sampling from a continuous distribution can be converted into an optimization problem over continuous space . central to the method is a stochastic process recently described in mathematical statistics that we call the gumbel process . we present a new construction of the gumbel process and a* sampling , a practical generic sampling algorithm that searches for the maximum of a gumbel process using a* search . we analyze the correctness and convergence time of a* sampling and demonstrate empirically that it makes more efficient use of bound and likelihood evaluations than the most closely related adaptive rejection sampling-based algorithms .", "topics": ["sampling ( signal processing )", "optimization problem"]}
{"title": "joint pos tagging and dependency parsing with transition-based neural networks", "abstract": "while part-of-speech ( pos ) tagging and dependency parsing are observed to be closely related , existing work on joint modeling with manually crafted feature templates suffers from the feature sparsity and incompleteness problems . in this paper , we propose an approach to joint pos tagging and dependency parsing using transition-based neural networks . three neural network based classifiers are designed to resolve shift/reduce , tagging , and labeling conflicts . experiments show that our approach significantly outperforms previous methods for joint pos tagging and dependency parsing across a variety of natural languages .", "topics": ["parsing", "natural language"]}
{"title": "an algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits", "abstract": "we present an algorithm that achieves almost optimal pseudo-regret bounds against adversarial and stochastic bandits . against adversarial bandits the pseudo-regret is $ o ( k\\sqrt { n \\log n } ) $ and against stochastic bandits the pseudo-regret is $ o ( \\sum_i ( \\log n ) /\\delta_i ) $ . we also show that no algorithm with $ o ( \\log n ) $ pseudo-regret against stochastic bandits can achieve $ \\tilde { o } ( \\sqrt { n } ) $ expected regret against adaptive adversarial bandits . this complements previous results of bubeck and slivkins ( 2012 ) that show $ \\tilde { o } ( \\sqrt { n } ) $ expected adversarial regret with $ o ( ( \\log n ) ^2 ) $ stochastic pseudo-regret .", "topics": ["regret ( decision theory )", "time complexity"]}
{"title": "privacy-preserving multi-document summarization", "abstract": "state-of-the-art extractive multi-document summarization systems are usually designed without any concern about privacy issues , meaning that all documents are open to third parties . in this paper we propose a privacy-preserving approach to multi-document summarization . our approach enables other parties to obtain summaries without learning anything else about the original documents ' content . we use a hashing scheme known as secure binary embeddings to convert documents representation containing key phrases and bag-of-words into bit strings , allowing the computation of approximate distances , instead of exact ones . our experiments indicate that our system yields similar results to its non-private counterpart on standard multi-document evaluation datasets .", "topics": ["approximation algorithm", "computation"]}
{"title": "an effective edge -- directed frequency filter for removal of aliasing in upsampled images", "abstract": "raster images can have a range of various distortions connected to their raster structure . upsampling them might in effect substantially yield the raster structure of the original image , known as aliasing . the upsampling itself may introduce aliasing into the upsampled image as well . the presented method attempts to remove the aliasing using frequency filters based on the discrete fast fourier transform , and applied directionally in certain regions placed along the edges in the image . as opposed to some anisotropic smoothing methods , the presented algorithm aims to selectively reduce only the aliasing , preserving the sharpness of image details . the method can be used as a post -- processing filter along with various upsampling algorithms . it was experimentally shown that the method can improve the visual quality of the upsampled images .", "topics": ["image processing"]}
{"title": "neural program meta-induction", "abstract": "most recently proposed methods for neural program induction work under the assumption of having a large set of input/output ( i/o ) examples for learning any underlying input-output mapping . this paper aims to address the problem of data and computation efficiency of program induction by leveraging information from related tasks . specifically , we propose two approaches for cross-task knowledge transfer to improve program induction in limited-data scenarios . in our first proposal , portfolio adaptation , a set of induction models is pretrained on a set of related tasks , and the best model is adapted towards the new task using transfer learning . in our second approach , meta program induction , a $ k $ -shot learning approach is used to make a model generalize to new tasks without additional training . to test the efficacy of our methods , we constructed a new benchmark of programs written in the karel programming language . using an extensive experimental evaluation on the karel benchmark , we demonstrate that our proposals dramatically outperform the baseline induction method that does not use knowledge transfer . we also analyze the relative performance of the two approaches and study conditions in which they perform best . in particular , meta induction outperforms all existing approaches under extreme data sparsity ( when a very small number of examples are available ) , i.e . , fewer than ten . as the number of available i/o examples increase ( i.e . a thousand or more ) , portfolio adapted program induction becomes the best approach . for intermediate data sizes , we demonstrate that the combined method of adapted meta program induction has the strongest performance .", "topics": ["baseline ( configuration management )", "computation"]}
{"title": "synthetic database for evaluation of general , fundamental biometric principles", "abstract": "we create synthetic biometric databases to study general , fundamental , biometric principles . first , we check the validity of the synthetic database design by comparing it to real data in terms of biometric performance . the real data used for this validity check was from an eye-movement related biometric database . next , we employ our database to evaluate the impact of variations of temporal persistence of features on biometric performance . we index temporal persistence with the intraclass correlation coefficient ( icc ) . we find that variations in temporal persistence are extremely highly correlated with variations in biometric performance . finally , we use our synthetic database strategy to determine how many features are required to achieve particular levels of performance as the number of subjects in the database increases from 100 to 10,000 . an important finding is that the number of features required to achieve various eer values ( 2 % , 0.3 % , 0.15 % ) is essentially constant in the database sizes that we studied . we hypothesize that the insights obtained from our study would be applicable to many biometric modalities where extracted feature properties resemble the properties of the synthetic features we discuss in this work .", "topics": ["synthetic data", "database"]}
{"title": "domain adaptation and transfer learning in stochasticnets", "abstract": "transfer learning is a recent field of machine learning research that aims to resolve the challenge of dealing with insufficient training data in the domain of interest . this is a particular issue with traditional deep neural networks where a large amount of training data is needed . recently , stochasticnets was proposed to take advantage of sparse connectivity in order to decrease the number of parameters that needs to be learned , which in turn may relax training data size requirements . in this paper , we study the efficacy of transfer learning on stochasticnet frameworks . experimental results show ~7 % improvement on stochasticnet performance when the transfer learning is applied in training step .", "topics": ["test set", "sparse matrix"]}
{"title": "deep learning for electromyographic hand gesture signal classification by leveraging transfer learning", "abstract": "in recent years , the use of deep learning algorithms has become increasingly more prominent . within the field of electromyography-based gesture recognition however , deep learning algorithms are seldom employed . this is due in part to the large quantity of data required for the network to train on . the data sparsity arises from the fact that it would take an unreasonable amount of time for a single person to generate tens of thousands of examples for training such algorithms . in this paper , two datasets are recorded with the myo armband ( thalmic labs ) , a low-cost , low-sampling rate ( 200hz ) , 8-channel , consumer-grade , dry electrode semg armband . these datasets , referred to as the pre-training and evaluation dataset , are comprised of 19 and 17 able-bodied participants respectively . a convolutional network ( convnet ) is augmented with transfer learning techniques to leverage inter-user data from the first dataset , alleviating the burden imposed on a single individual to generate a vast quantity of training data for semg-based gesture recognition . this transfer learning scheme is shown to outperform the current state-of-the-art in gesture recognition achieving an average accuracy of 98.31 % for 7 hand/wrist gestures over 17 able-bodied participants . finally , a use-case study of eight able-bodied participants is presented to evaluate the impact of feedback on the degradation accuracy normally experienced from a classifier over time .", "topics": ["sampling ( signal processing )", "test set"]}
{"title": "syntax-aware language modeling with recurrent neural networks", "abstract": "neural language models ( lms ) are typically trained using only lexical features , such as surface forms of words . in this paper , we argue this deprives the lm of crucial syntactic signals that can be detected at high confidence using existing parsers . we present a simple but highly effective approach for training neural lms using both lexical and syntactic information , and a novel approach for applying such lms to unparsed text using sequential monte carlo sampling . in experiments on a range of corpora and corpus sizes , we show our approach consistently outperforms standard lexical lms in character-level language modeling ; on the other hand , for word-level models the models are on a par with standard language models . these results indicate potential for expanding lms beyond lexical surface features to higher-level nlp features for character-level models .", "topics": ["natural language processing", "recurrent neural network"]}
{"title": "applying evolutionary optimisation to robot obstacle avoidance", "abstract": "this paper presents an artificial evolutionbased method for stereo image analysis and its application to real-time obstacle detection and avoidance for a mobile robot . it uses the parisian approach , which consists here in splitting the representation of the robot 's environment into a large number of simple primitives , the `` flies '' , which are evolved following a biologically inspired scheme and give a fast , low-cost solution to the obstacle detection problem in mobile robotics .", "topics": ["mathematical optimization", "robot"]}
{"title": "a data set for evaluating the performance of multi-class multi-object video tracking", "abstract": "one of the challenges in evaluating multi-object video detection , tracking and classification systems is having publically available data sets with which to compare different systems . however , the measures of performance for tracking and classification are different . data sets that are suitable for evaluating tracking systems may not be appropriate for classification . tracking video data sets typically only have ground truth track ids , while classification video data sets only have ground truth class-label ids . the former identifies the same object over multiple frames , while the latter identifies the type of object in individual frames . this paper describes an advancement of the ground truth meta-data for the darpa neovision2 tower data set to allow both the evaluation of tracking and classification . the ground truth data sets presented in this paper contain unique object ids across 5 different classes of object ( car , bus , truck , person , cyclist ) for 24 videos of 871 image frames each . in addition to the object ids and class labels , the ground truth data also contains the original bounding box coordinates together with new bounding boxes in instances where un-annotated objects were present . the unique ids are maintained during occlusions between multiple objects or when objects re-enter the field of view . this will provide : a solid foundation for evaluating the performance of multi-object tracking of different types of objects , a straightforward comparison of tracking system performance using the standard multi object tracking ( mot ) framework , and classification performance using the neovision2 metrics . these data have been hosted publically .", "topics": ["ground truth"]}
{"title": "reformulating inference problems through selective conditioning", "abstract": "we describe how we selectively reformulate portions of a belief network that pose difficulties for solution with a stochastic-simulation algorithm . with employ the selective conditioning approach to target specific nodes in a belief network for decomposition , based on the contribution the nodes make to the tractability of stochastic simulation . we review previous work on bnras algorithms- randomized approximation algorithms for probabilistic inference . we show how selective conditioning can be employed to reformulate a single bnras problem into multiple tractable bnras simulation problems . we discuss how we can use another simulation algorithm-logic sampling-to solve a component of the inference problem that provides a means for knitting the solutions of individual subproblems into a final result . finally , we analyze tradeoffs among the computational subtasks associated with the selective conditioning approach to reformulation .", "topics": ["approximation algorithm", "simulation"]}
{"title": "structured variable selection with sparsity-inducing norms", "abstract": "we consider the empirical risk minimization problem for linear supervised learning , with regularization by structured sparsity-inducing norms . these are defined as sums of euclidean norms on certain subsets of variables , extending the usual $ \\ell_1 $ -norm and the group $ \\ell_1 $ -norm by allowing the subsets to overlap . this leads to a specific set of allowed nonzero patterns for the solutions of such problems . we first explore the relationship between the groups defining the norm and the resulting nonzero patterns , providing both forward and backward algorithms to go back and forth from groups to patterns . this allows the design of norms adapted to specific prior knowledge expressed in terms of nonzero patterns . we also present an efficient active set algorithm , and analyze the consistency of variable selection for least-squares linear regression in low and high-dimensional settings .", "topics": ["supervised learning", "matrix regularization"]}
{"title": "learning feed-forward one-shot learners", "abstract": "one-shot learning is usually tackled by using generative models or discriminative embeddings . discriminative methods based on deep learning , which are very effective in other learning scenarios , are ill-suited for one-shot learning as they need large amounts of training data . in this paper , we propose a method to learn the parameters of a deep model in one shot . we construct the learner as a second deep network , called a learnet , which predicts the parameters of a pupil network from a single exemplar . in this manner we obtain an efficient feed-forward one-shot learner , trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation . in order to make the construction feasible , we propose a number of factorizations of the parameters of the pupil network . we demonstrate encouraging results by learning characters from single exemplars in omniglot , and by tracking visual objects from a single initial exemplar in the visual object tracking benchmark .", "topics": ["test set", "statistical classification"]}
{"title": "action recognition in still images by latent superpixel classification", "abstract": "action recognition from still images is an important task of computer vision applications such as image annotation , robotic navigation , video surveillance and several others . existing approaches mainly rely on either bag-of-feature representations or articulated body-part models . however , the relationship between the action and the image segments is still substantially unexplored . for this reason , in this paper we propose to approach action recognition by leveraging an intermediate layer of `` superpixels '' whose latent classes can act as attributes of the action . in the proposed approach , the action class is predicted by a structural model ( learnt by latent structural svm ) based on measurements from the image superpixels and their latent classes . experimental results over the challenging stanford 40 actions dataset report a significant average accuracy of 74.06 % for the positive class and 88.50 % for the negative class , giving evidence to the performance of the proposed approach .", "topics": ["computer vision"]}
{"title": "best-first and/or search for most probable explanations", "abstract": "the paper evaluates the power of best-first search over and/or search spaces for solving the most probable explanation ( mpe ) task in bayesian networks . the main virtue of the and/or representation of the search space is its sensitivity to the structure of the problem , which can translate into significant time savings . in recent years depth-first and/or branch-and- bound algorithms were shown to be very effective when exploring such search spaces , especially when using caching . since best-first strategies are known to be superior to depth-first when memory is utilized , exploring the best-first control strategy is called for . the main contribution of this paper is in showing that a recent extension of and/or search algorithms from depth-first branch-and-bound to best-first is indeed very effective for computing the mpe in bayesian networks . we demonstrate empirically the superiority of the best-first search approach on various probabilistic networks .", "topics": ["bayesian network"]}
{"title": "tracing liquid level and material boundaries in transparent vessels using the graph cut computer vision approach", "abstract": "detection of boundaries of materials stored in transparent vessels is essential for identifying properties such as liquid level and phase boundaries , which are vital for controlling numerous processes in the industry and chemistry laboratory . this work presents a computer vision method for identifying the boundary of materials in transparent vessels using the graph-cut algorithm . the method receives an image of a transparent vessel containing a material and the contour of the vessel in the image . the boundary of the material in the vessel is found by the graph cut method . in general the method uses the vessel region of the image to create a graph , where pixels are vertices , and the cost of an edge between two pixels is inversely correlated with their intensity difference . the bottom 10 % of the vessel region in the image is assumed to correspond to the material phase and defined as the graph and source . the top 10 % of the pixels in the vessels are assumed to correspond to the air phase and defined as the graph sink . the minimal cut that splits the resulting graph between the source and sink ( hence , material and air ) is traced using the max-flow/min-cut approach . this cut corresponds to the boundary of the material in the image . the method gave high accuracy in boundary recognition for a wide range of liquid , solid , granular and powder materials in various glass vessels from everyday life and the chemistry laboratory , such as bottles , jars , glasses , chromotography colums and separatory funnels .", "topics": ["computer vision", "pixel"]}
{"title": "convergence of unregularized online learning algorithms", "abstract": "in this paper we study the convergence of online gradient descent algorithms in reproducing kernel hilbert spaces ( rkhss ) without regularization . we establish a sufficient condition and a necessary condition for the convergence of excess generalization errors in expectation . a sufficient condition for the almost sure convergence is also given . with high probability , we provide explicit convergence rates of the excess generalization errors for both averaged iterates and the last iterate , which in turn also imply convergence rates with probability one . to our best knowledge , this is the first high-probability convergence rate for the last iterate of online gradient descent algorithms without strong convexity . without any boundedness assumptions on iterates , our results are derived by a novel use of two measures of the algorithm 's one-step progress , respectively by generalization errors and by distances in rkhss , where the variances of the involved martingales are cancelled out by the descent property of the algorithm .", "topics": ["gradient descent", "matrix regularization"]}
{"title": "fuzzy mixed integer linear programming for air vehicles operations optimization", "abstract": "multiple air vehicles ( avs ) to prosecute geographically dispersed targets is an important optimization problem . associated multiple tasks viz . , target classification , attack and verification are successively performed on each target . the optimal minimum time performance of these tasks requires cooperation among vehicles such that critical time constraints are satisfied i.e . target must be classified before it can be attacked and av is sent to target area to verify its destruction after target has been attacked . here , optimal task scheduling problem from indian air force is formulated as fuzzy mixed integer linear programming ( fmilp ) problem . the solution assigns all tasks to vehicles and performs scheduling in an optimal manner including scheduled staged departure times . coupled tasks involving time and task order constraints are addressed . when avs have sufficient endurance , existence of optimal solution is guaranteed . the solution developed can serve as an effective heuristic for different categories of av optimization problems .", "topics": ["optimization problem", "heuristic"]}
{"title": "scalable influence maximization for multiple products in continuous-time diffusion networks", "abstract": "a typical viral marketing model identifies influential users in a social network to maximize a single product adoption assuming unlimited user attention , campaign budgets , and time . in reality , multiple products need campaigns , users have limited attention , convincing users incurs costs , and advertisers have limited budgets and expect the adoptions to be maximized soon . facing these user , monetary , and timing constraints , we formulate the problem as a submodular maximization task in a continuous-time diffusion model under the intersection of a matroid and multiple knapsack constraints . we propose a randomized algorithm estimating the user influence in a network ( $ |\\mathcal { v } | $ nodes , $ |\\mathcal { e } | $ edges ) to an accuracy of $ \\epsilon $ with $ n=\\mathcal { o } ( 1/\\epsilon^2 ) $ randomizations and $ \\tilde { \\mathcal { o } } ( n|\\mathcal { e } |+n|\\mathcal { v } | ) $ computations . by exploiting the influence estimation algorithm as a subroutine , we develop an adaptive threshold greedy algorithm achieving an approximation factor $ k_a/ ( 2+2 k ) $ of the optimal when $ k_a $ out of the $ k $ knapsack constraints are active . extensive experiments on networks of millions of nodes demonstrate that the proposed algorithms achieve the state-of-the-art in terms of effectiveness and scalability .", "topics": ["computation", "scalability"]}
{"title": "multi-channel cnn-based object detection for enhanced situation awareness", "abstract": "object detection is critical for automatic military operations . however , the performance of current object detection algorithms is deficient in terms of the requirements in military scenarios . this is mainly because the object presence is hard to detect due to the indistinguishable appearance and dramatic changes of object 's size which is determined by the distance to the detection sensors . recent advances in deep learning have achieved promising results in many challenging tasks . the state-of-the-art in object detection is represented by convolutional neural networks ( cnns ) , such as the fast r-cnn algorithm . these cnn-based methods improve the detection performance significantly on several public generic object detection datasets . however , their performance on detecting small objects or undistinguishable objects in visible spectrum images is still insufficient . in this study , we propose a novel detection algorithm for military objects by fusing multi-channel cnns . we combine spatial , temporal and thermal information by generating a three-channel image , and they will be fused as cnn feature maps in an unsupervised manner . the backbone of our object detection framework is from the fast r-cnn algorithm , and we utilize cross-domain transfer learning technique to fine-tune the cnn model on generated multi-channel images . in the experiments , we validated the proposed method with the images from sensiac ( military sensing information analysis centre ) database and compared it with the state-of-the-art . the experimental results demonstrated the effectiveness of the proposed method on both accuracy and computational efficiency .", "topics": ["object detection", "unsupervised learning"]}
{"title": "efficient training of very deep neural networks for supervised hashing", "abstract": "in this paper , we propose training very deep neural networks ( dnns ) for supervised learning of hash codes . existing methods in this context train relatively `` shallow '' networks limited by the issues arising in back propagation ( e.e . vanishing gradients ) as well as computational efficiency . we propose a novel and efficient training algorithm inspired by alternating direction method of multipliers ( admm ) that overcomes some of these limitations . our method decomposes the training process into independent layer-wise local updates through auxiliary variables . empirically we observe that our training algorithm always converges and its computational complexity is linearly proportional to the number of edges in the networks . empirically we manage to train dnns with 64 hidden layers and 1024 nodes per layer for supervised hashing in about 3 hours using a single gpu . our proposed very deep supervised hashing ( vdsh ) method significantly outperforms the state-of-the-art on several benchmark datasets .", "topics": ["supervised learning", "computational complexity theory"]}
{"title": "analyzing hidden representations in end-to-end automatic speech recognition systems", "abstract": "neural models have become ubiquitous in automatic speech recognition systems . while neural networks are typically used as acoustic models in more complex systems , recent studies have explored end-to-end speech recognition systems based on neural networks , which can be trained to directly predict text from input acoustic features . although such systems are conceptually elegant and simpler than traditional systems , it is less obvious how to interpret the trained models . in this work , we analyze the speech representations learned by a deep end-to-end model that is based on convolutional and recurrent layers , and trained with a connectionist temporal classification ( ctc ) loss . we use a pre-trained model to generate frame-level features which are given to a classifier that is trained on frame classification into phones . we evaluate representations from different layers of the deep model and compare their quality for predicting phone labels . our experiments shed light on important aspects of the end-to-end model such as layer depth , model complexity , and other design choices .", "topics": ["neural networks", "speech recognition"]}
{"title": "end-to-end information extraction without token-level supervision", "abstract": "most state-of-the-art information extraction approaches rely on token-level labels to find the areas of interest in text . unfortunately , these labels are time-consuming and costly to create , and consequently , not available for many real-life ie tasks . to make matters worse , token-level labels are usually not the desired output , but just an intermediary step . end-to-end ( e2e ) models , which take raw text as input and produce the desired output directly , need not depend on token-level labels . we propose an e2e model based on pointer networks , which can be trained directly on pairs of raw input and output text . we evaluate our model on the atis data set , mit restaurant corpus and the mit movie corpus and compare to neural baselines that do use token-level labels . we achieve competitive results , within a few percentage points of the baselines , showing the feasibility of e2e information extraction without the need for token-level labels . this opens up new possibilities , as for many tasks currently addressed by human extractors , raw input and output data are available , but not token-level labels .", "topics": ["text corpus", "end-to-end principle"]}
{"title": "visual tracking using particle swarm optimization", "abstract": "the problem of robust extraction of visual odometry from a sequence of images obtained by an eye in hand camera configuration is addressed . a novel approach toward solving planar template based tracking is proposed which performs a non-linear image alignment for successful retrieval of camera transformations . in order to obtain global optimum a bio-metaheuristic is used for optimization of similarity among the planar regions . the proposed method is validated on image sequences with real as well as synthetic transformations and found to be resilient to intensity variations . a comparative analysis of the various similarity measures as well as various state-of-art methods reveal that the algorithm succeeds in tracking the planar regions robustly and has good potential to be used in real applications .", "topics": ["nonlinear system", "synthetic data"]}
{"title": "visual psychophysics for making face recognition algorithms more explainable", "abstract": "scientific fields that are interested in faces have developed their own sets of concepts and procedures for understanding how a target model system ( be it a person or algorithm ) perceives a face under varying conditions . in computer vision , this has largely been in the form of dataset evaluation for recognition tasks where summary statistics are used to measure progress . while aggregate performance has continued to improve , understanding individual causes of failure has been difficult , as it is not always clear why a particular face fails to be recognized , or why an impostor is recognized by an algorithm . importantly , other fields studying vision have addressed this via the use of visual psychophysics : the controlled manipulation of stimuli and careful study of the responses they evoke in a model system . in this paper , we suggest that visual psychophysics is a viable methodology for making face recognition algorithms more explainable . a comprehensive set of procedures is developed for assessing face recognition algorithm behavior , which is then deployed over state-of-the-art convolutional neural networks and more basic , yet still widely used , shallow and handcrafted feature-based approaches .", "topics": ["computer vision", "speech recognition"]}
{"title": "profiling of a network behind an infectious disease outbreak", "abstract": "stochasticity and spatial heterogeneity are of great interest recently in studying the spread of an infectious disease . the presented method solves an inverse problem to discover the effectively decisive topology of a heterogeneous network and reveal the transmission parameters which govern the stochastic spreads over the network from a dataset on an infectious disease outbreak in the early growth phase . populations in a combination of epidemiological compartment models and a meta-population network model are described by stochastic differential equations . probability density functions are derived from the equations and used for the maximal likelihood estimation of the topology and parameters . the method is tested with computationally synthesized datasets and the who dataset on sars outbreak .", "topics": ["time series"]}
{"title": "learning to start for sequence to sequence architecture", "abstract": "the sequence to sequence architecture is widely used in the response generation and neural machine translation to model the potential relationship between two sentences . it typically consists of two parts : an encoder that reads from the source sentence and a decoder that generates the target sentence word by word according to the encoder 's output and the last generated word . however , it faces to the cold start problem when generating the first word as there is no previous word to refer . existing work mainly use a special start symbol < /s > to generate the first word . an obvious drawback of these work is that there is not a learnable relationship between words and the start symbol . furthermore , it may lead to the error accumulation for decoding when the first word is incorrectly generated . in this paper , we proposed a novel approach to learning to generate the first word in the sequence to sequence architecture rather than using the start symbol . experimental results on the task of response generation of short text conversation show that the proposed approach outperforms the state-of-the-art approach in both of the automatic and manual evaluations .", "topics": ["machine translation", "encoder"]}
{"title": "an empirical study of adequate vision span for attention-based neural machine translation", "abstract": "recently , the attention mechanism plays a key role to achieve high performance for neural machine translation models . however , as it computes a score function for the encoder states in all positions at each decoding step , the attention model greatly increases the computational complexity . in this paper , we investigate the adequate vision span of attention models in the context of machine translation , by proposing a novel attention framework that is capable of reducing redundant score computation dynamically . the term `` vision span '' means a window of the encoder states considered by the attention model in one step . in our experiments , we found that the average window size of vision span can be reduced by over 50 % with modest loss in accuracy on english-japanese and german-english translation tasks . % this results indicate that the conventional attention mechanism performs a significant amount of redundant computation .", "topics": ["computational complexity theory", "machine translation"]}
{"title": "attention with intention for a neural network conversation model", "abstract": "in a conversation or a dialogue process , attention and intention play intrinsic roles . this paper proposes a neural network based approach that models the attention and intention processes . it essentially consists of three recurrent networks . the encoder network is a word-level model representing source side sentences . the intention network is a recurrent network that models the dynamics of the intention process . the decoder network is a recurrent network produces responses to the input from the source side . it is a language model that is dependent on the intention and has an attention mechanism to attend to particular source side words , when predicting a symbol in the response . the model is trained end-to-end without labeling data . experiments show that this model generates natural responses to user inputs .", "topics": ["recurrent neural network", "encoder"]}
{"title": "studying the wikipedia hyperlink graph for relatedness and disambiguation", "abstract": "hyperlinks and other relations in wikipedia are a extraordinary resource which is still not fully understood . in this paper we study the different types of links in wikipedia , and contrast the use of the full graph with respect to just direct links . we apply a well-known random walk algorithm on two tasks , word relatedness and named-entity disambiguation . we show that using the full graph is more effective than just direct links by a large margin , that non-reciprocal links harm performance , and that there is no benefit from categories and infoboxes , with coherent results on both tasks . we set new state-of-the-art figures for systems based on wikipedia links , comparable to systems exploiting several information sources and/or supervised machine learning . our approach is open source , with instruction to reproduce results , and amenable to be integrated with complementary text-based methods .", "topics": ["supervised learning"]}
{"title": "convolutional neural networks for medical image analysis : full training or fine tuning ?", "abstract": "training a deep convolutional neural network ( cnn ) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence . a promising alternative is to fine-tune a cnn that has been pre-trained using , for instance , a large set of labeled natural images . however , the substantial differences between natural and medical images may advise against such knowledge transfer . in this paper , we seek to answer the following central question in the context of medical image analysis : \\emph { can the use of pre-trained deep cnns with sufficient fine-tuning eliminate the need for training a deep cnn from scratch ? } to address this question , we considered 4 distinct medical imaging applications in 3 specialties ( radiology , cardiology , and gastroenterology ) involving classification , detection , and segmentation from 3 different imaging modalities , and investigated how the performance of deep cnns trained from scratch compared with the pre-trained cnns fine-tuned in a layer-wise manner . our experiments consistently demonstrated that ( 1 ) the use of a pre-trained cnn with adequate fine-tuning outperformed or , in the worst case , performed as well as a cnn trained from scratch ; ( 2 ) fine-tuned cnns were more robust to the size of training sets than cnns trained from scratch ; ( 3 ) neither shallow tuning nor deep tuning was the optimal choice for a particular application ; and ( 4 ) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data .", "topics": ["test set"]}
{"title": "adversarial patrolling with spatially uncertain alarm signals", "abstract": "when securing complex infrastructures or large environments , constant surveillance of every area is not affordable . to cope with this issue , a common countermeasure is the usage of cheap but wide-ranged sensors , able to detect suspicious events that occur in large areas , supporting patrollers to improve the effectiveness of their strategies . however , such sensors are commonly affected by uncertainty . in the present paper , we focus on spatially uncertain alarm signals . that is , the alarm system is able to detect an attack but it is uncertain on the exact position where the attack is taking place . this is common when the area to be secured is wide such as in border patrolling and fair site surveillance . we propose , to the best of our knowledge , the first patrolling security game model where a defender is supported by a spatially uncertain alarm system which non-deterministically generates signals once a target is under attack . we show that finding the optimal strategy in arbitrary graphs is apx-hard even in zero-sum games and we provide two ( exponential time ) exact algorithms and two ( polynomial time ) approximation algorithms . furthermore , we analyse what happens in environments with special topologies , showing that in linear and cycle graphs the optimal patrolling strategy can be found in polynomial time , de facto allowing our algorithms to be used in real-life scenarios , while in trees the problem is np-hard . finally , we show that without false positives and missed detections , the best patrolling strategy reduces to stay in a place , wait for a signal , and respond to it at best . this strategy is optimal even with non-negligible missed detection rates , which , unfortunately , affect every commercial alarm system . we evaluate our methods in simulation , assessing both quantitative and qualitative aspects .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "robust speech recognition using generative adversarial networks", "abstract": "this paper describes a general , scalable , end-to-end framework that uses the generative adversarial network ( gan ) objective to enable robust speech recognition . encoders trained with the proposed approach enjoy improved invariance by learning to map noisy audio to the same embedding space as that of clean audio . unlike previous methods , the new framework does not rely on domain expertise or simplifying assumptions as are often needed in signal processing , and directly encourages robustness in a data-driven way . we show the new approach improves simulated far-field speech recognition of vanilla sequence-to-sequence models without specialized front-ends or preprocessing .", "topics": ["speech recognition", "end-to-end principle"]}
{"title": "deep over-sampling framework for classifying imbalanced data", "abstract": "class imbalance is a challenging issue in practical classification problems for deep learning models as well as traditional models . traditionally successful countermeasures such as synthetic over-sampling have had limited success with complex , structured data handled by deep learning models . in this paper , we propose deep over-sampling ( dos ) , a framework for extending the synthetic over-sampling method to exploit the deep feature space acquired by a convolutional neural network ( cnn ) . its key feature is an explicit , supervised representation learning , for which the training data presents each raw input sample with a synthetic embedding target in the deep feature space , which is sampled from the linear subspace of in-class neighbors . we implement an iterative process of training the cnn and updating the targets , which induces smaller in-class variance among the embeddings , to increase the discriminative power of the deep representation . we present an empirical study using public benchmarks , which shows that the dos framework not only counteracts class imbalance better than the existing method , but also improves the performance of the cnn in the standard , balanced settings .", "topics": ["sampling ( signal processing )", "feature learning"]}
{"title": "skill2vec : machine learning approaches for determining the relevant skill from job description", "abstract": "un-supervise learned word embeddings have seen tremendous success in numerous natural language processing ( nlp ) tasks in recent years . the main contribution of this paper is to develop a technique called skill2vec , which applies machine learning techniques in recruitment to enhance the search strategy to find the candidates who possess the right skills . skill2vec is a neural network architecture which inspired by word2vec , developed by mikolov et al . in 2013 , to transform a skill to a new vector space . this vector space has the characteristics of calculation and present their relationship . we conducted an experiment using ab testing in a recruitment company to demonstrate the effectiveness of our approach .", "topics": ["natural language processing", "natural language"]}
{"title": "long timescale credit assignment in neuralnetworks with external memory", "abstract": "credit assignment in traditional recurrent neural networks usually involves back-propagating through a long chain of tied weight matrices . the length of this chain scales linearly with the number of time-steps as the same network is run at each time-step . this creates many problems , such as vanishing gradients , that have been well studied . in contrast , a nnem 's architecture recurrent activity does n't involve a long chain of activity ( though some architectures such as the ntm do utilize a traditional recurrent architecture as a controller ) . rather , the externally stored embedding vectors are used at each time-step , but no messages are passed from previous time-steps . this means that vanishing gradients are n't a problem , as all of the necessary gradient paths are short . however , these paths are extremely numerous ( one per embedding vector in memory ) and reused for a very long time ( until it leaves the memory ) . thus , the forward-pass information of each memory must be stored for the entire duration of the memory . this is problematic as this additional storage far surpasses that of the actual memories , to the extent that large memories on infeasible to back-propagate through in high dimensional settings . one way to get around the need to hold onto forward-pass information is to recalculate the forward-pass whenever gradient information is available . however , if the observations are too large to store in the domain of interest , direct reinstatement of a forward pass can not occur . instead , we rely on a learned autoencoder to reinstate the observation , and then use the embedding network to recalculate the forward-pass . since the recalculated embedding vector is unlikely to perfectly match the one stored in memory , we try out 2 approximations to utilize error gradient w.r.t . the vector in memory .", "topics": ["neural networks"]}
{"title": "an n-dimensional approach towards object based classification of remotely sensed imagery", "abstract": "remote sensing techniques are widely used for land cover classification and urban analysis . the availability of high resolution remote sensing imagery limits the level of classification accuracy attainable from pixel-based approach . in this paper object-based classification scheme based on a hierarchical support vector machine is introduced . by combining spatial and spectral information , the amount of overlap between classes can be decreased ; thereby yielding higher classification accuracy and more accurate land cover maps . we have adopted certain automatic approaches based on the advanced techniques as cellular automata and genetic algorithm for kernel and tuning parameter selection . performance evaluation of the proposed methodology in comparison with the existing approaches is performed with reference to the bhopal city study area .", "topics": ["support vector machine", "map"]}
{"title": "fast inference in sparse coding algorithms with applications to object recognition", "abstract": "adaptive sparse coding methods learn a possibly overcomplete set of basis functions , such that natural image patches can be reconstructed by linearly combining a small subset of these bases . the applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation . in this work we propose a simple and efficient algorithm to learn basis functions . after training , this model also provides a fast and smooth approximator to the optimal representation , achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks .", "topics": ["sparse matrix"]}
{"title": "clustering is difficult only when it does not matter", "abstract": "numerous papers ask how difficult it is to cluster data . we suggest that the more relevant and interesting question is how difficult it is to cluster data sets { \\em that can be clustered well } . more generally , despite the ubiquity and the great importance of clustering , we still do not have a satisfactory mathematical theory of clustering . in order to properly understand clustering , it is clearly necessary to develop a solid theoretical basis for the area . for example , from the perspective of computational complexity theory the clustering problem seems very hard . numerous papers introduce various criteria and numerical measures to quantify the quality of a given clustering . the resulting conclusions are pessimistic , since it is computationally difficult to find an optimal clustering of a given data set , if we go by any of these popular criteria . in contrast , the practitioners ' perspective is much more optimistic . our explanation for this disparity of opinions is that complexity theory concentrates on the worst case , whereas in reality we only care for data sets that can be clustered well . we introduce a theoretical framework of clustering in metric spaces that revolves around a notion of `` good clustering '' . we show that if a good clustering exists , then in many cases it can be efficiently found . our conclusion is that contrary to popular belief , clustering should not be considered a hard task .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "an epsilon hierarchical fuzzy twin support vector regression", "abstract": "the research presents epsilon hierarchical fuzzy twin support vector regression based on epsilon fuzzy twin support vector regression and epsilon twin support vector regression . epsilon ftsvr is achieved by incorporating trapezoidal fuzzy numbers to epsilon tsvr which takes care of uncertainty existing in forecasting problems . epsilon ftsvr determines a pair of epsilon insensitive proximal functions by solving two related quadratic programming problems . the structural risk minimization principle is implemented by introducing regularization term in primal problems of epsilon ftsvr . this yields dual stable positive definite problems which improves regression performance . epsilon ftsvr is then reformulated as epsilon hftsvr consisting of a set of hierarchical layers each containing epsilon ftsvr . experimental results on both synthetic and real datasets reveal that epsilon hftsvr has remarkable generalization performance with minimum training time .", "topics": ["support vector machine", "matrix regularization"]}
{"title": "an integrated semantic web service discovery and composition framework", "abstract": "in this paper we present a theoretical analysis of graph-based service composition in terms of its dependency with service discovery . driven by this analysis we define a composition framework by means of integration with fine-grained i/o service discovery that enables the generation of a graph-based composition which contains the set of services that are semantically relevant for an input-output request . the proposed framework also includes an optimal composition search algorithm to extract the best composition from the graph minimising the length and the number of services , and different graph optimisations to improve the scalability of the system . a practical implementation used for the empirical analysis is also provided . this analysis proves the scalability and flexibility of our proposal and provides insights on how integrated composition systems can be designed in order to achieve good performance in real scenarios for the web .", "topics": ["scalability"]}
{"title": "lv-rover : lexicon verified recognizer output voting error reduction", "abstract": "offline handwritten text line recognition is a hard task that requires both an efficient optical character recognizer and language model . handwriting recognition state of the art methods are based on long short term memory ( lstm ) recurrent neural networks ( rnn ) coupled with the use of linguistic knowledge . most of the proposed approaches in the literature focus on improving one of the two components and use constraint , dedicated to a database lexicon . however , state of the art performance is achieved by combining multiple optical models , and possibly multiple language models with the recognizer output voting error reduction ( rover ) framework . though handwritten line recognition with rover has been implemented by combining only few recognizers because training multiple complete recognizers is hard . in this paper we propose a lexicon verified rover : lv-rover , that has a reduce complexity compare to the original one and that can combine hundreds of recognizers without language models . we achieve state of the art for handwritten line text on the rimes dataset .", "topics": ["recurrent neural network"]}
{"title": "language detection for short text messages in social media", "abstract": "with the constant growth of the world wide web and the number of documents in different languages accordingly , the need for reliable language detection tools has increased as well . platforms such as twitter with predominantly short texts are becoming important information resources , which additionally imposes the need for short texts language detection algorithms . in this paper , we show how incorporating personalized user-specific information into the language detection algorithm leads to an important improvement of detection results . to choose the best algorithm for language detection for short text messages , we investigate several machine learning approaches . these approaches include the use of the well-known classifiers such as svm and logistic regression , a dictionary based approach , and a probabilistic model based on modified kneser-ney smoothing . furthermore , the extension of the probabilistic model to include additional user-specific information such as evidence accumulation per user and user interface language is explored , with the goal of improving the classification performance . the proposed approaches are evaluated on randomly collected twitter data containing latin as well as non-latin alphabet languages and the quality of the obtained results is compared , followed by the selection of the best performing algorithm . this algorithm is then evaluated against two already existing general language detection tools : chromium compact language detector 2 ( cld2 ) and langid , where our method significantly outperforms the results achieved by both of the mentioned methods . additionally , a preview of benefits and possible applications of having a reliable language detection algorithm is given .", "topics": ["support vector machine", "dictionary"]}
{"title": "real-time dynamic mri reconstruction using stacked denoising autoencoder", "abstract": "in this work we address the problem of real-time dynamic mri reconstruction . there are a handful of studies on this topic ; these techniques are either based on compressed sensing or employ kalman filtering . these techniques can not achieve the reconstruction speed necessary for real-time reconstruction . in this work , we propose a new approach to mri reconstruction . we learn a non-linear mapping from the unstructured aliased images to the corresponding clean images using a stacked denoising autoencoder ( sdae ) . the training for sdae is slow , but the reconstruction is very fast - only requiring a few matrix vector multiplications . in this work , we have shown that using sdae one can reconstruct the mri frame faster than the data acquisition rate , thereby achieving real-time reconstruction . the quality of reconstruction is of the same order as a previous compressed sensing based online reconstruction technique .", "topics": ["noise reduction", "nonlinear system"]}
{"title": "multi-label annotation aggregation in crowdsourcing", "abstract": "as a means of human-based computation , crowdsourcing has been widely used to annotate large-scale unlabeled datasets . one of the obvious challenges is how to aggregate these possibly noisy labels provided by a set of heterogeneous annotators . another challenge stems from the difficulty in evaluating the annotator reliability without even knowing the ground truth , which can be used to build incentive mechanisms in crowdsourcing platforms . when each instance is associated with many possible labels simultaneously , the problem becomes even harder because of its combinatorial nature . in this paper , we present new flexible bayesian models and efficient inference algorithms for multi-label annotation aggregation by taking both annotator reliability and label dependency into account . extensive experiments on real-world datasets confirm that the proposed methods outperform other competitive alternatives , and the model can recover the type of the annotators with high accuracy . besides , we empirically find that the mixture of multiple independent bernoulli distribution is able to accurately capture label dependency in this unsupervised multi-label annotation aggregation scenario .", "topics": ["ground truth", "computation"]}
{"title": "active orthogonal matching pursuit for sparse subspace clustering", "abstract": "sparse subspace clustering ( ssc ) is a state-of-the-art method for clustering high-dimensional data points lying in a union of low-dimensional subspaces . however , while $ \\ell_1 $ optimization-based ssc algorithms suffer from high computational complexity , other variants of ssc , such as orthogonal matching pursuit-based ssc ( omp-ssc ) , lose clustering accuracy in pursuit of improving time efficiency . in this letter , we propose a novel active omp-ssc , which improves clustering accuracy of omp-ssc by adaptively updating data points and randomly dropping data points in the omp process , while still enjoying the low computational complexity of greedy pursuit algorithms . we provide heuristic analysis of our approach , and explain how these two active steps achieve a better tradeoff between connectivity and separation . numerical results on both synthetic data and real-world data validate our analyses and show the advantages of the proposed active algorithm .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "meta reinforcement learning with latent variable gaussian processes", "abstract": "data efficiency , i.e . , learning from small data sets , is critical in many practical applications where data collection is time consuming or expensive , e.g . , robotics , animal experiments or drug design . meta learning is one way to increase the data efficiency of learning algorithms by generalizing learned concepts from a set of training tasks to unseen , but related , tasks . often , this relationship between tasks is hard coded or relies in some other way on human expertise . in this paper , we propose to automatically learn the relationship between tasks using a latent variable model . our approach finds a variational posterior over tasks and averages over all plausible ( according to this posterior ) tasks when making predictions . we apply this framework within a model-based reinforcement learning setting for learning dynamics models and controllers of many related tasks . we apply our framework in a model-based reinforcement learning setting , and show that our model effectively generalizes to novel tasks , and that it reduces the average interaction time needed to solve tasks by up to 60 % compared to strong baselines .", "topics": ["calculus of variations", "reinforcement learning"]}
{"title": "tagset design and inflected languages", "abstract": "an experiment designed to explore the relationship between tagging accuracy and the nature of the tagset is described , using corpora in english , french and swedish . in particular , the question of internal versus external criteria for tagset design is considered , with the general conclusion that external ( linguistic ) criteria should be followed . some problems associated with tagging unknown words in inflected languages are briefly considered .", "topics": ["text corpus"]}
{"title": "spatially-adaptive reconstruction in computed tomography based on statistical learning", "abstract": "we propose a direct reconstruction algorithm for computed tomography , based on a local fusion of a few preliminary image estimates by means of a non-linear fusion rule . one such rule is based on a signal denoising technique which is spatially adaptive to the unknown local smoothness . another , more powerful fusion rule , is based on a neural network trained off-line with a high-quality training set of images . two types of linear reconstruction algorithms for the preliminary images are employed for two different reconstruction tasks . for an entire image reconstruction from full projection data , the proposed scheme uses a sequence of filtered back-projection algorithms with a gradually growing cut-off frequency . to recover a region of interest only from local projections , statistically-trained linear reconstruction algorithms are employed . numerical experiments display the improvement in reconstruction quality when compared to linear reconstruction algorithms .", "topics": ["numerical analysis", "simulation"]}
{"title": "a basic recurrent neural network model", "abstract": "we present a model of a basic recurrent neural network ( or brnn ) that includes a separate linear term with a slightly `` stable '' fixed matrix to guarantee bounded solutions and fast dynamic response . we formulate a state space viewpoint and adapt the constrained optimization lagrange multiplier ( clm ) technique and the vector calculus of variations ( cov ) to derive the ( stochastic ) gradient descent . in this process , one avoids the commonly used re-application of the circular chain-rule and identifies the error back-propagation with the co-state backward dynamic equations . we assert that this brnn can successfully perform regression tracking of time-series . moreover , the `` vanishing and exploding '' gradients are explicitly quantified and explained through the co-state dynamics and the update laws . the adapted cov framework , in addition , can correctly and principally integrate new loss functions in the network on any variable and for varied goals , e.g . , for supervised learning on the outputs and unsupervised learning on the internal ( hidden ) states .", "topics": ["calculus of variations", "supervised learning"]}
{"title": "how much does your data exploration overfit ? controlling bias via information usage", "abstract": "modern data is messy and high-dimensional , and it is often not clear a priori what are the right questions to ask . instead , the analyst typically needs to use the data to search for interesting analyses to perform and hypotheses to test . this is an adaptive process , where the choice of analysis to be performed next depends on the results of the previous analyses on the same data . ultimately , which results are reported can be heavily influenced by the data . it is widely recognized that this process , even if well-intentioned , can lead to biases and false discoveries , contributing to the crisis of reproducibility in science . but while % the adaptive nature of exploration any data-exploration renders standard statistical theory invalid , experience suggests that different types of exploratory analysis can lead to disparate levels of bias , and the degree of bias also depends on the particulars of the data set . in this paper , we propose a general information usage framework to quantify and provably bound the bias and other error metrics of an arbitrary exploratory analysis . we prove that our mutual information based bound is tight in natural settings , and then use it to give rigorous insights into when commonly used procedures do or do not lead to substantially biased estimation . through the lens of information usage , we analyze the bias of specific exploration procedures such as filtering , rank selection and clustering . our general framework also naturally motivates randomization techniques that provably reduces exploration bias while preserving the utility of the data analysis . we discuss the connections between our approach and related ideas from differential privacy and blinded data analysis , and supplement our results with illustrative simulations .", "topics": ["cluster analysis", "simulation"]}
{"title": "a batching and scheduling optimisation for a cutting work-center : acta-mobilier case study", "abstract": "the purpose of this study is to investigate an approach to group lots in batches and to schedule these batches on acta-mobilier cutting work-center while taking into account numerous constraints and objectives . the specific batching method was proposed to handle the acta-mobilier problem and a mathematical formalisation and genetic algorithm were proposed to deal with the scheduling problem . the proposed algorithm has been embedded in software to optimise production costs and emphasis the visual management on the production line . the application is currently being used in acta-mobilier plant and shows significant results", "topics": ["mathematical optimization"]}
{"title": "on the approximation by single hidden layer feedforward neural networks with fixed weights", "abstract": "feedforward neural networks have wide applicability in various disciplines of science due to their universal approximation property . some authors have shown that single hidden layer feedforward neural networks ( slfns ) with fixed weights still possess the universal approximation property provided that approximated functions are univariate . but this phenomenon does not lay any restrictions on the number of neurons in the hidden layer . the more this number , the more the probability of the considered network to give precise results . in this note , we constructively prove that slfns with the fixed weight $ 1 $ and two neurons in the hidden layer can approximate any continuous function on a compact subset of the real line . the applicability of this result is demonstrated in various numerical examples . finally , we show that slfns with fixed weights can not approximate all continuous multivariate functions .", "topics": ["approximation algorithm", "numerical analysis"]}
{"title": "precise but natural specification for robot tasks", "abstract": "we present flipper , a natural language interface for describing high level task specifications for robots that are compiled into robot actions . flipper starts with a formal core language for task planning that allows expressing rich temporal specifications and uses a semantic parser to provide a natural language interface . flipper provides immediate visual feedback by executing an automatically constructed plan of the task in a graphical user interface . this allows the user to resolve potentially ambiguous interpretations . flipper extends itself via naturalization : users of flipper can define new commands , which are generalized and added as new rules to the core language , gradually growing a more and more natural task specification language . unlike other task-specification systems , flipper enables natural language interactions while maintaining the expressive power and formal precision of a programming language . we show through an initial user study that natural language interactions and generalization can considerably ease the description of tasks . moreover , over time , users employ more and more concepts outside of the initial core language . such extensions are available to the flipper community , and users can use concepts that others have defined .", "topics": ["natural language", "interaction"]}
{"title": "`` let me convince you to buy my product ... `` : a case study of an automated persuasive system for fashion products", "abstract": "persuasivenes is a creative art aimed at making people believe in certain set of beliefs . many a times , such creativity is about adapting richness of one domain into another to strike a chord with the target audience . in this research , we present persuaide ! - a persuasive system based on linguistic creativity to transform given sentence to generate various forms of persuading sentences . these various forms cover multiple focus of persuasion such as memorability and sentiment . for a given simple product line , the algorithm is composed of several steps including : ( i ) select an appropriate well-known expression for the target domain to add memorability , ( ii ) identify keywords and entities in the given sentence and expression and transform it to produce creative persuading sentence , and ( iii ) adding positive or negative sentiment for further persuasion . the persuasive conversion were manually verified using qualitative results and the effectiveness of the proposed approach is empirically discussed .", "topics": ["entity"]}
{"title": "sparse multiple kernel learning with geometric convergence rate", "abstract": "in this paper , we study the problem of sparse multiple kernel learning ( mkl ) , where the goal is to efficiently learn a combination of a fixed small number of kernels from a large pool that could lead to a kernel classifier with a small prediction error . we develop an efficient algorithm based on the greedy coordinate descent algorithm , that is able to achieve a geometric convergence rate under appropriate conditions . the convergence rate is achieved by measuring the size of functional gradients by an empirical $ \\ell_2 $ norm that depends on the empirical data distribution . this is in contrast to previous algorithms that use a functional norm to measure the size of gradients , which is independent from the data samples . we also establish a generalization error bound of the learned sparse kernel classifier using the technique of local rademacher complexity .", "topics": ["kernel ( operating system )", "sparse matrix"]}
{"title": "deep learning for multi-label classification", "abstract": "in multi-label classification , the main focus has been to develop ways of learning the underlying dependencies between labels , and to take advantage of this at classification time . developing better feature-space representations has been predominantly employed to reduce complexity , e.g . , by eliminating non-helpful feature attributes from the input space prior to ( or during ) training . this is an important task , since many multi-label methods typically create many different copies or views of the same input data as they transform it , and considerable memory can be saved by taking advantage of redundancy . in this paper , we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time . for this task we use a deep learning approach with restricted boltzmann machines . we present a deep network that , in an empirical evaluation , outperforms a number of competitive methods from the literature", "topics": ["feature vector"]}
{"title": "ds-mlr : exploiting double separability for scaling up distributed multinomial logistic regression", "abstract": "scaling multinomial logistic regression to datasets with very large number of data points and classes has not been trivial . this is primarily because one needs to compute the log-partition function on every data point . this makes distributing the computation hard . in this paper , we present a distributed stochastic gradient descent based optimization method ( ds-mlr ) for scaling up multinomial logistic regression problems to massive scale datasets without hitting any storage constraints on the data and model parameters . our algorithm exploits double-separability , an attractive property we observe in the objective functions of several models in machine learning , that allows us to achieve both data as well as model parallelism simultaneously . in addition to being parallelizable , our algorithm can also easily be made non-blocking and asynchronous . we demonstrate the effectiveness of ds-mlr empirically on several real-world datasets , the largest being a reddit dataset created out of 1.7 billion user comments , where the data and parameter sizes are 228 gb and 358 gb respectively .", "topics": ["gradient descent", "computation"]}
{"title": "learning body-affordances to simplify action spaces", "abstract": "controlling embodied agents with many actuated degrees of freedom is a challenging task . we propose a method that can discover and interpolate between context dependent high-level actions or body-affordances . these provide an abstract , low-dimensional interface indexing high-dimensional and time- extended action policies . our method is related to recent ap- proaches in the machine learning literature but is conceptually simpler and easier to implement . more specifically our method requires the choice of a n-dimensional target sensor space that is endowed with a distance metric . the method then learns an also n-dimensional embedding of possibly reactive body-affordances that spread as far as possible throughout the target sensor space .", "topics": ["high- and low-level"]}
{"title": "disaggregation of smap l3 brightness temperatures to 9km using kernel machines", "abstract": "in this study , a machine learning algorithm is used for disaggregation of smap brightness temperatures ( t $ _ { \\textrm { b } } $ ) from 36km to 9km . it uses image segmentation to cluster the study region based on meteorological and land cover similarity , followed by a support vector machine based regression that computes the value of the disaggregated t $ _ { \\textrm { b } } $ at all pixels . high resolution remote sensing products such as land surface temperature , normalized difference vegetation index , enhanced vegetation index , precipitation , soil texture , and land-cover were used for disaggregation . the algorithm was implemented in iowa , united states , from april to july 2015 , and compared with the smap l3_sm_ap t $ _ { \\textrm { b } } $ product at 9km . it was found that the disaggregated t $ _ { \\textrm { b } } $ were very similar to the smap-t $ _ { \\textrm { b } } $ product , even for vegetated areas with a mean difference $ \\leq $ 5k . however , the standard deviation of the disaggregation was lower by 7k than that of the ap product . the probability density functions of the disaggregated t $ _ { \\textrm { b } } $ were similar to the smap-t $ _ { \\textrm { b } } $ . the results indicate that this algorithm may be used for disaggregating t $ _ { \\textrm { b } } $ using complex non-linear correlations on a grid .", "topics": ["support vector machine", "nonlinear system"]}
{"title": "end-to-end neural segmental models for speech recognition", "abstract": "segmental models are an alternative to frame-based models for sequence prediction , where hypothesized path weights are based on entire segment scores rather than a single frame at a time . neural segmental models are segmental models that use neural network-based weight functions . neural segmental models have achieved competitive results for speech recognition , and their end-to-end training has been explored in several studies . in this work , we review neural segmental models , which can be viewed as consisting of a neural network-based acoustic encoder and a finite-state transducer decoder . we study end-to-end segmental models with different weight functions , including ones based on frame-level neural classifiers and on segmental recurrent neural networks . we study how reducing the search space size impacts performance under different weight functions . we also compare several loss functions for end-to-end training . finally , we explore training approaches , including multi-stage vs . end-to-end training and multitask training that combines segmental and frame-level losses .", "topics": ["recurrent neural network", "loss function"]}
{"title": "federated multi-task learning", "abstract": "federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices . in this work , we show that multi-task learning is naturally suited to handle the statistical challenges of this setting , and propose a novel systems-aware optimization method , mocha , that is robust to practical systems issues . our method and theory for the first time consider issues of high communication cost , stragglers , and fault tolerance for distributed multi-task learning . the resulting method achieves significant speedups compared to alternatives in the federated setting , as we demonstrate through simulations on real-world federated datasets .", "topics": ["simulation"]}
{"title": "exploiting multi-layer graph factorization for multi-attributed graph matching", "abstract": "multi-attributed graph matching is a problem of finding correspondences between two sets of data while considering their complex properties described in multiple attributes . however , the information of multiple attributes is likely to be oversimplified during a process that makes an integrated attribute , and this degrades the matching accuracy . for that reason , a multi-layer graph structure-based algorithm has been proposed recently . it can effectively avoid the problem by separating attributes into multiple layers . nonetheless , there are several remaining issues such as a scalability problem caused by the huge matrix to describe the multi-layer structure and a back-projection problem caused by the continuous relaxation of the quadratic assignment problem . in this work , we propose a novel multi-attributed graph matching algorithm based on the multi-layer graph factorization . we reformulate the problem to be solved with several small matrices that are obtained by factorizing the multi-layer structure . then , we solve the problem using a convex-concave relaxation procedure for the multi-layer structure . the proposed algorithm exhibits better performance than state-of-the-art algorithms based on the single-layer structure .", "topics": ["scalability"]}
{"title": "learning mobile app usage routine through learning automata", "abstract": "since its conception , smart app market has grown exponentially . success in the app market depends on many factors among which the quality of the app is a significant contributor , such as energy use . nevertheless , smartphones , as a subset of mobile computing devices . inherit the limited power resource constraint . therefore , there is a challenge of maintaining the resource while increasing the target app quality . this paper introduces learning automata ( la ) as an online learning method to learn and predict the app usage routines of the users . such prediction can leverage the app cache functionality of the operating system and thus ( i ) decreases app launch time and ( ii ) preserve battery . our algorithm , which is an online learning approach , temporally updates and improves the internal states of itself . in particular , it learns the transition probabilities between app launching . each app launching instance updates the transition probabilities related to that app , and this will result in improving the prediction . we benefit from a real-world lifelogging dataset and our experimental results show considerable success with respect to the two baseline methods that are used currently for smartphone app prediction approaches .", "topics": ["baseline ( configuration management )"]}
{"title": "optimizing for measure of performance in max-margin parsing", "abstract": "many statistical learning problems in the area of natural language processing including sequence tagging , sequence segmentation and syntactic parsing has been successfully approached by means of structured prediction methods . an appealing property of the corresponding discriminative learning algorithms is their ability to integrate the loss function of interest directly into the optimization process , which potentially can increase the resulting performance accuracy . here , we demonstrate on the example of constituency parsing how to optimize for f1-score in the max-margin framework of structural svm . in particular , the optimization is with respect to the original ( not binarized ) trees .", "topics": ["natural language processing", "parsing"]}
{"title": "natural language processing : state of the art , current trends and challenges", "abstract": "natural language processing ( nlp ) has recently gained much attention for representing and analysing human language computationally . it has spread its applications in various fields such as machine translation , email spam detection , information extraction , summarization , medical , and question answering etc . the paper distinguishes four phases by discussing different levels of nlp and components of natural language generation ( nlg ) followed by presenting the history and evolution of nlp , state of the art presenting the various applications of nlp and current trends and challenges .", "topics": ["natural language processing", "machine translation"]}
{"title": "a compressed sensing based decomposition of electrodermal activity signals", "abstract": "the measurement and analysis of electrodermal activity ( eda ) offers applications in diverse areas ranging from market research , to seizure detection , to human stress analysis . unfortunately , the analysis of eda signals is made difficult by the superposition of numerous components which can obscure the signal information related to a user 's response to a stimulus . we show how simple pre-processing followed by a novel compressed sensing based decomposition can mitigate the effects of the undesired noise components and help reveal the underlying physiological signal . the proposed framework allows for decomposition of eda signals with provable bounds on the recovery of user responses . we test our procedure on both synthetic and real-world eda signals from wearable sensors and demonstrate that our approach allows for more accurate recovery of user responses as compared to the existing techniques .", "topics": ["synthetic data", "sensor"]}
{"title": "segmentation of drosophila heart in optical coherence microscopy images using convolutional neural networks", "abstract": "convolutional neural networks are powerful tools for image segmentation and classification . here , we use this method to identify and mark the heart region of drosophila at different developmental stages in the cross-sectional images acquired by a custom optical coherence microscopy ( ocm ) system . with our well-trained convolutional neural network model , the heart regions through multiple heartbeat cycles can be marked with an intersection over union ( iou ) of ~86 % . various morphological and dynamical cardiac parameters can be quantified accurately with automatically segmented heart regions . this study demonstrates an efficient heart segmentation method to analyze ocm images of the beating heart in drosophila .", "topics": ["image segmentation", "neural networks"]}
{"title": "modelling observation correlations for active exploration and robust object detection", "abstract": "today , mobile robots are expected to carry out increasingly complex tasks in multifarious , real-world environments . often , the tasks require a certain semantic understanding of the workspace . consider , for example , spoken instructions from a human collaborator referring to objects of interest ; the robot must be able to accurately detect these objects to correctly understand the instructions . however , existing object detection , while competent , is not perfect . in particular , the performance of detection algorithms is commonly sensitive to the position of the sensor relative to the objects in the scene . this paper presents an online planning algorithm which learns an explicit model of the spatial dependence of object detection and generates plans which maximize the expected performance of the detection , and by extension the overall plan performance . crucially , the learned sensor model incorporates spatial correlations between measurements , capturing the fact that successive measurements taken at the same or nearby locations are not independent . we show how this sensor model can be incorporated into an efficient forward search algorithm in the information space of detected objects , allowing the robot to generate motion plans efficiently . we investigate the performance of our approach by addressing the tasks of door and text detection in indoor environments and demonstrate significant improvement in detection performance during task execution over alternative methods in simulated and real robot experiments .", "topics": ["object detection", "simulation"]}
{"title": "scalable semidefinite relaxation for maximum a posterior estimation", "abstract": "maximum a posteriori ( map ) inference over discrete markov random fields is a fundamental task spanning a wide spectrum of real-world applications , which is known to be np-hard for general graphs . in this paper , we propose a novel semidefinite relaxation formulation ( referred to as sdr ) to estimate the map assignment . algorithmically , we develop an accelerated variant of the alternating direction method of multipliers ( referred to as sdpad-lr ) that can effectively exploit the special structure of the new relaxation . encouragingly , the proposed procedure allows solving sdr for large-scale problems , e.g . , problems on a grid graph comprising hundreds of thousands of variables with multiple states per node . compared with prior sdp solvers , sdpad-lr is capable of attaining comparable accuracy while exhibiting remarkably improved scalability , in contrast to the commonly held belief that semidefinite relaxation can only been applied on small-scale mrf problems . we have evaluated the performance of sdr on various benchmark datasets including opengm2 and pic in terms of both the quality of the solutions and computation time . experimental results demonstrate that for a broad class of problems , sdpad-lr outperforms state-of-the-art algorithms in producing better map assignment in an efficient manner .", "topics": ["time complexity", "computation"]}
{"title": "point convolutional neural networks by extension operators", "abstract": "this paper presents point convolutional neural networks ( pcnn ) : a novel framework for applying convolutional neural networks to point clouds . the framework consists of two operators : extension and restriction , mapping point cloud functions to volumetric functions and vise-versa . a point cloud convolution is defined by pull-back of the euclidean volumetric convolution via an extension-restriction mechanism . the point cloud convolution is computationally efficient , invariant to the order of points in the point cloud , robust to different samplings and varying densities , and translation invariant , that is the same convolution kernel is used at all points . pcnn generalizes image cnns and allows readily adapting their architectures to the point cloud setting . evaluation of pcnn on three central point cloud learning benchmarks convincingly outperform competing point cloud learning methods , and the vast majority of methods working with more informative shape representations such as surfaces and/or normals .", "topics": ["computational complexity theory", "neural networks"]}
{"title": "rolling shutter camera relative pose : generalized epipolar geometry", "abstract": "the vast majority of modern consumer-grade cameras employ a rolling shutter mechanism . in dynamic geometric computer vision applications such as visual slam , the so-called rolling shutter effect therefore needs to be properly taken into account . a dedicated relative pose solver appears to be the first problem to solve , as it is of eminent importance to bootstrap any derivation of multi-view geometry . however , despite its significance , it has received inadequate attention to date . this paper presents a detailed investigation of the geometry of the rolling shutter relative pose problem . we introduce the rolling shutter essential matrix , and establish its link to existing models such as the push-broom cameras , summarized in a clean hierarchy of multi-perspective cameras . the generalization of well-established concepts from epipolar geometry is completed by a definition of the sampson distance in the rolling shutter case . the work is concluded with a careful investigation of the introduced epipolar geometry for rolling shutter cameras on several dedicated benchmarks .", "topics": ["computer vision"]}
{"title": "multi-layer perceptrons and symbolic data", "abstract": "in some real world situations , linear models are not sufficient to represent accurately complex relations between input variables and output variables of a studied system . multilayer perceptrons are one of the most successful non-linear regression tool but they are unfortunately restricted to inputs and outputs that belong to a normed vector space . in this chapter , we propose a general recoding method that allows to use symbolic data both as inputs and outputs to multilayer perceptrons . the recoding is quite simple to implement and yet provides a flexible framework that allows to deal with almost all practical cases . the proposed method is illustrated on a real world data set .", "topics": ["nonlinear system"]}
{"title": "hyperspectral classification with adaptively weighted l1-norm regularization and spatial postprocessing", "abstract": "sparse regression methods have been proven effective in a wide range of signal processing problems such as image compression , speech coding , channel equalization , linear regression and classification . in this paper a new convex method of hyperspectral image classification is developed based on the sparse unmixing algorithm sunsal for which a pixel adaptive l1-norm regularization term is introduced . to further enhance class separability , the algorithm is kernelized using an rbf kernel and the final results are improved by a combination of spatial pre and post-processing operations . it is shown that the proposed method is competitive with state of the art algorithms such as svm-ck , ksomp-ck and kssp-ck .", "topics": ["test set", "statistical classification"]}
{"title": "disease prediction from electronic health records using generative adversarial networks", "abstract": "electronic health records ( ehrs ) have contributed to the computerization of patient records so that they can be used not only for efficient and systematic medical services , but also for research on data science . in this paper , we compared the disease prediction performance of generative adversarial networks ( gans ) and conventional learning algorithms in combination with missing value prediction methods . as a result , the highest accuracy of 98.05 % was obtained using a stacked autoencoder as the missing value prediction method and an auxiliary classifier gans ( ac-gans ) as the disease predicting method . our results show that the combination of the stacked autoencoder and the ac-gans significantly outperforms existing algorithms for the problem of disease prediction in which missing values and class imbalance exist .", "topics": ["autoencoder"]}
{"title": "high density noise removal by cascading algorithms", "abstract": "an advanced non-linear cascading filter algorithm for the removal of high density salt and pepper noise from the digital images is proposed . the proposed method consists of two stages . the first stage decision base median filter ( dmf ) acts as the preliminary noise removal algorithm . the second stage is either modified decision base partial trimmed global mean filter ( mdbptgmf ) or modified decision based unsymmetric trimmed median filter ( mdbutmf ) which is used to remove the remaining noise and enhance the image quality . the dmf algorithm performs well at low noise density but it fails to remove the noise at medium and high level . the mdbptgmf and mdutmf have excellent performance at low , medium and high noise density but these reduce the image quality and blur the image at high noise level . so the basic idea behind this paper is to combine the advantages of the filters used in both the stages to remove the salt and pepper noise and enhance the image quality at all the noise density level . the proposed method is tested against different gray scale images and it gives better mean absolute error ( mae ) , peak signal to noise ratio ( psnr ) and image enhancement factor ( ief ) than the adaptive median filter ( amf ) , decision base unsymmetric trimmed median filter ( dbutmf ) , modified decision base unsymmetric trimmed median filter ( mdbutmf ) and decision base partial trimmed global mean filter ( dbptgmf ) .", "topics": ["noise reduction", "nonlinear system"]}
{"title": "a multimodal biometric system using linear discriminant analysis for improved performance", "abstract": "essentially a biometric system is a pattern recognition system which recognizes a user by determining the authenticity of a specific anatomical or behavioral characteristic possessed by the user . with the ever increasing integration of computers and internet into daily life style , it has become necessary to protect sensitive and personal data . this paper proposes a multimodal biometric system which incorporates more than one biometric trait to attain higher security and to handle failure to enroll situations for some users . this paper is aimed at investigating a multimodal biometric identity system using linear discriminant analysis as backbone to both facial and speech recognition and implementing such system in real-time using signalwave .", "topics": ["speech recognition"]}
{"title": "bridging information criteria and parameter shrinkage for model selection", "abstract": "model selection based on classical information criteria , such as bic , is generally computationally demanding , but its properties are well studied . on the other hand , model selection based on parameter shrinkage by $ \\ell_1 $ -type penalties is computationally efficient . in this paper we make an attempt to combine their strengths , and propose a simple approach that penalizes the likelihood with data-dependent $ \\ell_1 $ penalties as in adaptive lasso and exploits a fixed penalization parameter . even for finite samples , its model selection results approximately coincide with those based on information criteria ; in particular , we show that in some special cases , this approach and the corresponding information criterion produce exactly the same model . one can also consider this approach as a way to directly determine the penalization parameter in adaptive lasso to achieve information criteria-like model selection . as extensions , we apply this idea to complex models including gaussian mixture model and mixture of factor analyzers , whose model selection is traditionally difficult to do ; by adopting suitable penalties , we provide continuous approximators to the corresponding information criteria , which are easy to optimize and enable efficient model selection .", "topics": ["computational complexity theory", "eisenstein 's criterion"]}
{"title": "fast : a framework to accelerate super-resolution processing on compressed videos", "abstract": "state-of-the-art super-resolution ( sr ) algorithms require significant computational resources to achieve real-time throughput ( e.g . , 60mpixels/s for hd video ) . this paper introduces fast ( free adaptive super-resolution via transfer ) , a framework to accelerate any sr algorithm applied to compressed videos . fast exploits the temporal correlation between adjacent frames such that sr is only applied to a subset of frames ; sr pixels are then transferred to the other frames . the transferring process has negligible computation cost as it uses information already embedded in the compressed video ( e.g . , motion vectors and residual ) . adaptive processing is used to retain accuracy when the temporal correlation is not present ( e.g . , occlusions ) . fast accelerates state-of-the-art sr algorithms by up to 15x with a visual quality loss of 0.2db . fast is an important step towards real-time sr algorithms for ultra-hd displays and energy constrained devices ( e.g . , phones and tablets ) .", "topics": ["computation", "pixel"]}
{"title": "multilinear grammar : ranks and interpretations", "abstract": "multilinear grammar provides a framework for integrating the many different syntagmatic structures of language into a coherent semiotically based rank interpretation architecture , with default linear grammars at each rank . the architecture defines a sui generis condition on ranks , from discourse through utterance and phrasal structures to the word , with its sub-ranks of morphology and phonology . each rank has unique structures and its own semantic-pragmatic and prosodic-phonetic interpretation models . default computational models for each rank are proposed , based on a procedural plausibility condition : incremental processing in linear time with finite working memory . we suggest that the rank interpretation architecture and its multilinear properties provide systematic design features of human languages , contrasting with unordered lists of key properties or single structural properties at one rank , such as recursion , which have previously been been put forward as language design features . the framework provides a realistic background for the gradual development of complexity in the phylogeny and ontogeny of language , and clarifies a range of challenges for the evaluation of realistic linguistic theories and applications . the empirical objective of the paper is to demonstrate unique multilinear properties at each rank and thereby motivate the multilinear grammar and rank interpretation architecture framework as a coherent approach to capturing the complexity of human languages in the simplest possible way .", "topics": ["time complexity"]}
{"title": "scalable kernel-based variable selection with sparsistency", "abstract": "variable selection is central to high-dimensional data analysis , and various algorithms have been developed . ideally , a variable selection algorithm shall be flexible , scalable , and with theoretical guarantee , yet most existing algorithms can not attain these properties at the same time . in this article , a three-step variable selection algorithm is developed , involving kernel-based estimation of the regression function and its gradient functions as well as a hard thresholding . its key advantage is that it assumes no explicit model assumption , admits general predictor effects , allows for scalable computation , and attains desirable asymptotic sparsistency . the proposed algorithm can be adapted to any reproducing kernel hilbert space ( rkhs ) with different kernel functions , and can be extended to interaction selection with slight modification . its computational cost is only linear in the data dimension , and can be further improved through parallel computing . the sparsistency of the proposed algorithm is established for general rkhs under mild conditions , including linear and gaussian kernels as special cases . its effectiveness is also supported by a variety of simulated and real examples .", "topics": ["simulation", "computation"]}
{"title": "generating adversarial examples with adversarial networks", "abstract": "deep neural networks ( dnns ) have been found to be vulnerable to adversarial examples resulting from adding small-magnitude perturbations to inputs . such adversarial examples can mislead dnns to produce adversary-selected results . different attack strategies have been proposed to generate adversarial examples , but how to produce them with high perceptual quality and more efficiently requires more research efforts . in this paper , we propose advgan to generate adversarial examples with generative adversarial networks ( gans ) , which can learn and approximate the distribution of original instances . for advgan , once the generator is trained , it can generate adversarial perturbations efficiently for any instance , so as to potentially accelerate adversarial training as defenses . we apply advgan in both semi-whitebox and black-box attack settings . in semi-whitebox attacks , there is no need to access the original target model after the generator is trained , in contrast to traditional white-box attacks . in black-box attacks , we dynamically train a distilled model for the black-box model and optimize the generator accordingly . adversarial examples generated by advgan on different target models have high attack success rate under state-of-the-art defenses compared to other attacks . our attack has placed the first with 92.76 % accuracy on a public mnist black-box attack challenge .", "topics": ["approximation algorithm", "mnist database"]}
{"title": "a learning approach to natural language understanding", "abstract": "in this paper we propose a learning paradigm for the problem of understanding spoken language . the basis of the work is in a formalization of the understanding problem as a communication problem . this results in the definition of a stochastic model of the production of speech or text starting from the meaning of a sentence . the resulting understanding algorithm consists in a viterbi maximization procedure , analogous to that commonly used for recognizing speech . the algorithm was implemented for building", "topics": ["natural language"]}
{"title": "simultaneous surface reflectance and fluorescence spectra estimation", "abstract": "there is widespread interest in estimating the fluorescence properties of natural materials in an image . however , the separation between reflected and fluoresced components is difficult , because it is impossible to distinguish reflected and fluoresced photons without controlling the illuminant spectrum . we show how to jointly estimate the reflectance and fluorescence from a single set of images acquired under multiple illuminants . we present a framework based on a linear approximation to the physical equations describing image formation in terms of surface spectral reflectance and fluorescence due to multiple fluorophores . we relax the non-convex , inverse estimation problem in order to jointly estimate the reflectance and fluorescence properties in a single optimization step and we use the alternating direction method of multipliers ( admm ) approach to efficiently find a solution . we provide a software implementation of the solver for our method and prior methods . we evaluate the accuracy and reliability of the method using both simulations and experimental data . to acquire data to test the methods , we built a custom imaging system using a monochrome camera , a filter wheel with bandpass transmissive filters and a small number of light emitting diodes . we compared the system and algorithm performance with the ground truth as well as with prior methods . our approach produces lower errors compared to earlier algorithms .", "topics": ["simulation", "ground truth"]}
{"title": "thermal human face recognition based on haar wavelet transform and series matching technique", "abstract": "thermal infrared ( ir ) images represent the heat patterns emitted from hot object and they do not consider the energies reflected from an object . objects living or non-living emit different amounts of ir energy according to their body temperature and characteristics . humans are homoeothermic and hence capable of maintaining constant temperature under different surrounding temperature . face recognition from thermal ( ir ) images should focus on changes of temperature on facial blood vessels . these temperature changes can be regarded as texture features of images and wavelet transform is a very good tool to analyze multi-scale and multi-directional texture . wavelet transform is also used for image dimensionality reduction , by removing redundancies and preserving original features of the image . the sizes of the facial images are normally large . so , the wavelet transform is used before image similarity is measured . therefore this paper describes an efficient approach of human face recognition based on wavelet transform from thermal ir images . the system consists of three steps . at the very first step , human thermal ir face image is preprocessed and the face region is only cropped from the entire image . secondly , haar wavelet is used to extract low frequency band from the cropped face region . lastly , the image classification between the training images and the test images is done , which is based on low-frequency components . the proposed approach is tested on a number of human thermal infrared face images created at our own laboratory and terravic facial ir database . experimental results indicated that the thermal infra red face images can be recognized by the proposed system effectively . the maximum success of 95 % recognition has been achieved .", "topics": ["computer vision"]}
{"title": "classifying symmetrical differences and temporal change in mammography using deep neural networks", "abstract": "we investigate the addition of symmetry and temporal context information to a deep convolutional neural network ( cnn ) with the purpose of detecting malignant soft tissue lesions in mammography . we employ a simple linear mapping that takes the location of a mass candidate and maps it to either the contra-lateral or prior mammogram and regions of interest ( roi ) are extracted around each location . we subsequently explore two different architectures ( 1 ) a fusion model employing two datastreams were both rois are fed to the network during training and testing and ( 2 ) a stage-wise approach where a single roi cnn is trained on the primary image and subsequently used as feature extractor for both primary and symmetrical or prior rois . a 'shallow ' gradient boosted tree ( gbt ) classifier is then trained on the concatenation of these features and used to classify the joint representation . results shown a significant increase in performance using the first architecture and symmetry information , but only marginal gains in performance using temporal data and the other setting . we feel results are promising and can greatly be improved when more temporal data becomes available .", "topics": ["baseline ( configuration management )", "neural networks"]}
{"title": "learning factor graphs in polynomial time & sample complexity", "abstract": "we study computational and sample complexity of parameter and structure learning in graphical models . our main result shows that the class of factor graphs with bounded factor size and bounded connectivity can be learned in polynomial time and polynomial number of samples , assuming that the data is generated by a network in this class . this result covers both parameter estimation for a known network structure and structure learning . it implies as a corollary that we can learn factor graphs for both bayesian networks and markov networks of bounded degree , in polynomial time and sample complexity . unlike maximum likelihood estimation , our method does not require inference in the underlying network , and so applies to networks where inference is intractable . we also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks .", "topics": ["graphical model", "time complexity"]}
{"title": "tensor2tensor for neural machine translation", "abstract": "tensor2tensor is a library for deep learning models that is well-suited for neural machine translation and includes the reference implementation of the state-of-the-art transformer model .", "topics": ["machine translation", "recurrent neural network"]}
{"title": "human-in-the-loop artificial intelligence", "abstract": "little by little , newspapers are revealing the bright future that artificial intelligence ( ai ) is building . intelligent machines will help everywhere . however , this bright future has a dark side : a dramatic job market contraction before its unpredictable transformation . hence , in a near future , large numbers of job seekers will need financial support while catching up with these novel unpredictable jobs . this possible job market crisis has an antidote inside . in fact , the rise of ai is sustained by the biggest knowledge theft of the recent years . learning ai machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions . by passionately doing their jobs , these workers are digging their own graves . in this paper , we propose human-in-the-loop artificial intelligence ( hit-ai ) as a fairer paradigm for artificial intelligence systems . hit-ai will reward aware and unaware knowledge producers with a different scheme : decisions of ai systems generating revenues will repay the legitimate owners of the knowledge used for taking those decisions . as modern robin hoods , hit-ai researchers should fight for a fairer artificial intelligence that gives back what it steals .", "topics": ["artificial intelligence", "interaction"]}
{"title": "learning character-level compositionality with visual features", "abstract": "previous work has modeled the compositionality of words by creating character-level models of meaning , reducing problems of sparsity for rare words . however , in many writing systems compositionality has an effect even on the character-level : the meaning of a character is derived by the sum of its parts . in this paper , we model this effect by creating embeddings for characters based on their visual characteristics , creating an image for the character and running it through a convolutional neural network to produce a visual character embedding . experiments on a text classification task demonstrate that such model allows for better processing of instances with rare characters in languages such as chinese , japanese , and korean . additionally , qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry semantic content , resulting in embeddings that are coherent in visual space .", "topics": ["sparse matrix"]}
{"title": "leveraging sentiment to compute word similarity", "abstract": "in this paper , we introduce a new wordnet based similarity metric , sensim , which incorporates sentiment content ( i.e . , degree of positive or negative sentiment ) of the words being compared to measure the similarity between them . the proposed metric is based on the hypothesis that knowing the sentiment is beneficial in measuring the similarity . to verify this hypothesis , we measure and compare the annotator agreement for 2 annotation strategies : 1 ) sentiment information of a pair of words is considered while annotating and 2 ) sentiment information of a pair of words is not considered while annotating . inter-annotator correlation scores show that the agreement is better when the two annotators consider sentiment information while assigning a similarity score to a pair of words . we use this hypothesis to measure the similarity between a pair of words . specifically , we represent each word as a vector containing sentiment scores of all the content words in the wordnet gloss of the sense of that word . these sentiment scores are derived from a sentiment lexicon . we then measure the cosine similarity between the two vectors . we perform both intrinsic and extrinsic evaluation of sensim and compare the performance with other widely usedwordnet similarity metrics .", "topics": ["natural language processing"]}
{"title": "neural network based next-song recommendation", "abstract": "recently , the next-item/basket recommendation system , which considers the sequential relation between bought items , has drawn attention of researchers . the utilization of sequential patterns has boosted performance on several kinds of recommendation tasks . inspired by natural language processing ( nlp ) techniques , we propose a novel neural network ( nn ) based next-song recommender , cnn-rec , in this paper . then , we compare the proposed system with several nn based and classic recommendation systems on the next-song recommendation task . verification results indicate the proposed system outperforms classic systems and has comparable performance with the state-of-the-art system .", "topics": ["natural language processing", "natural language"]}
{"title": "burn-in demonstrations for multi-modal imitation learning", "abstract": "recent work on imitation learning has generated policies that reproduce expert behavior from multi-modal data . however , past approaches have focused only on recreating a small number of distinct , expert maneuvers , or have relied on supervised learning techniques that produce unstable policies . this work extends infogail , an algorithm for multi-modal imitation learning , to reproduce behavior over an extended period of time . our approach involves reformulating the typical imitation learning setting to include `` burn-in demonstrations '' upon which policies are conditioned at test time . we demonstrate that our approach outperforms standard infogail in maximizing the mutual information between predicted and unseen style labels in road scene simulations , and we show that our method leads to policies that imitate expert autonomous driving systems over long time horizons .", "topics": ["supervised learning", "simulation"]}
{"title": "variance-reduced stochastic learning by networked agents under random reshuffling", "abstract": "a new amortized variance-reduced gradient ( avrg ) algorithm was developed in [ 1 ] , which has constant storage requirement in comparison to saga and balanced gradient computations in comparison to svrg . one key advantage of the avrg strategy is its amenability to decentralized implementations . in this work , we show how avrg can be extended to the network case where multiple learning agents are assumed to be connected by a graph topology . in this scenario , each agent observes data that is spatially distributed and all agents are only allowed to communicate with direct neighbors . moreover , the amount of data observed by the individual agents may differ drastically . for such situations , the balanced gradient computation property of avrg becomes a real advantage in reducing idle time caused by unbalanced local data storage requirements , which is characteristic of other reduced-variance gradient algorithms . the resulting diffusion-avrg algorithm is shown to have linear convergence to the exact solution , and is much more memory efficient than other alternative algorithms . in addition , by using a mini-batch strategy , it is shown that diffusion-avrg is more computationally efficient than exact diffusion or extra while maintaining almost the same amount of communications .", "topics": ["scalability"]}
{"title": "robust feature selection by mutual information distributions", "abstract": "mutual information is widely used in artificial intelligence , in a descriptive way , to measure the stochastic dependence of discrete random variables . in order to address questions such as the reliability of the empirical value , one must consider sample-to-population inferential approaches . this paper deals with the distribution of mutual information , as obtained in a bayesian framework by a second-order dirichlet prior distribution . the exact analytical expression for the mean and an analytical approximation of the variance are reported . asymptotic approximations of the distribution are proposed . the results are applied to the problem of selecting features for incremental learning and classification of the naive bayes classifier . a fast , newly defined method is shown to outperform the traditional approach based on empirical mutual information on a number of real data sets . finally , a theoretical development is reported that allows one to efficiently extend the above methods to incomplete samples in an easy and effective way .", "topics": ["approximation", "artificial intelligence"]}
{"title": "learning the matching function", "abstract": "the matching function for the problem of stereo reconstruction or optical flow has been traditionally designed as a function of the distance between the features describing matched pixels . this approach works under assumption , that the appearance of pixels in two stereo cameras or in two consecutive video frames does not change dramatically . however , this might not be the case , if we try to match pixels over a large interval of time . in this paper we propose a method , which learns the matching function , that automatically finds the space of allowed changes in visual appearance , such as due to the motion blur , chromatic distortions , different colour calibration or seasonal changes . furthermore , it automatically learns the importance of matching scores of contextual features at different relative locations and scales . proposed classifier gives reliable estimations of pixel disparities already without any form of regularization . we evaluated our method on two standard problems - stereo matching on kitti outdoor dataset , optical flow on sintel data set , and on newly introduced timelapse change detection dataset . our algorithm obtained very promising results comparable to the state-of-the-art .", "topics": ["matrix regularization", "pixel"]}
{"title": "higher order matching pursuit for low rank tensor learning", "abstract": "low rank tensor learning , such as tensor completion and multilinear multitask learning , has received much attention in recent years . in this paper , we propose higher order matching pursuit for low rank tensor learning problems with a convex or a nonconvex cost function , which is a generalization of the matching pursuit type methods . at each iteration , the main cost of the proposed methods is only to compute a rank-one tensor , which can be done efficiently , making the proposed methods scalable to large scale problems . moreover , storing the resulting rank-one tensors is of low storage requirement , which can help to break the curse of dimensionality . the linear convergence rate of the proposed methods is established in various circumstances . along with the main methods , we also provide a method of low computational complexity for approximately computing the rank-one tensors , with provable approximation ratio , which helps to improve the efficiency of the main methods and to analyze the convergence rate . experimental results on synthetic as well as real datasets verify the efficiency and effectiveness of the proposed methods .", "topics": ["approximation algorithm", "computational complexity theory"]}
{"title": "learning temporal alignment uncertainty for efficient event detection", "abstract": "in this paper we tackle the problem of efficient video event detection . we argue that linear detection functions should be preferred in this regard due to their scalability and efficiency during estimation and evaluation . a popular approach in this regard is to represent a sequence using a bag of words ( bow ) representation due to its : ( i ) fixed dimensionality irrespective of the sequence length , and ( ii ) its ability to compactly model the statistics in the sequence . a drawback to the bow representation , however , is the intrinsic destruction of the temporal ordering information . in this paper we propose a new representation that leverages the uncertainty in relative temporal alignments between pairs of sequences while not destroying temporal ordering . our representation , like bow , is of a fixed dimensionality making it easily integrated with a linear detection function . extensive experiments on ck+ , 6dmg , and uva-nemo databases show significant performance improvements across both isolated and continuous event detection tasks .", "topics": ["scalability", "database"]}
{"title": "convolutional dictionary learning via local processing", "abstract": "convolutional sparse coding ( csc ) is an increasingly popular model in the signal and image processing communities , tackling some of the limitations of traditional patch-based sparse representations . although several works have addressed the dictionary learning problem under this model , these relied on an admm formulation in the fourier domain , losing the sense of locality and the relation to the traditional patch-based sparse pursuit . a recent work suggested a novel theoretical analysis of this global model , providing guarantees that rely on a localized sparsity measure . herein , we extend this local-global relation by showing how one can efficiently solve the convolutional sparse pursuit problem and train the filters involved , while operating locally on image patches . our approach provides an intuitive algorithm that can leverage standard techniques from the sparse representations field . the proposed method is fast to train , simple to implement , and flexible enough that it can be easily deployed in a variety of applications . we demonstrate the proposed training scheme for image inpainting and image separation , while achieving state-of-the-art results .", "topics": ["image processing", "sparse matrix"]}
{"title": "sequence modeling using gated recurrent neural networks", "abstract": "in this paper , we have used recurrent neural networks to capture and model human motion data and generate motions by prediction of the next immediate data point at each time-step . our rnn is armed with recently proposed gated recurrent units which has shown promising results in some sequence modeling problems such as machine translation and speech synthesis . we demonstrate that this model is able to capture long-term dependencies in data and generate realistic motions .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "unsupervised temporal segmentation of repetitive human actions based on kinematic modeling and frequency analysis", "abstract": "in this paper , we propose a method for temporal segmentation of human repetitive actions based on frequency analysis of kinematic parameters , zero-velocity crossing detection , and adaptive k-means clustering . since the human motion data may be captured with different modalities which have different temporal sampling rate and accuracy ( e.g . , optical motion capture systems vs. microsoft kinect ) , we first apply a generic full-body kinematic model with an unscented kalman filter to convert the motion data into a unified representation that is robust to noise . furthermore , we extract the most representative kinematic parameters via the primary frequency analysis . the sequences are segmented based on zero-velocity crossing of the selected parameters followed by an adaptive k-means clustering to identify the repetition segments . experimental results demonstrate that for the motion data captured by both the motion capture system and the microsoft kinect , our proposed algorithm obtains robust segmentation of repetitive action sequences .", "topics": ["sampling ( signal processing )", "cluster analysis"]}
{"title": "raisonner avec des diagrammes : perspectives cognitives et computationnelles", "abstract": "diagrammatic , analogical or iconic representations are often contrasted with linguistic or logical representations , in which the shape of the symbols is arbitrary . the aim of this paper is to make a case for the usefulness of diagrams in inferential knowledge representation systems . although commonly used , diagrams have for a long time suffered from the reputation of being only a heuristic tool or a mere support for intuition . the first part of this paper is an historical background paying tribute to the logicians , psychologists and computer scientists who put an end to this formal prejudice against diagrams . the second part is a discussion of their characteristics as opposed to those of linguistic forms . the last part is aimed at reviving the interest for heterogeneous representation systems including both linguistic and diagrammatic representations .", "topics": ["heuristic"]}
{"title": "image registration based flicker solving in video face replacement and analysis based sub-pixel image registration", "abstract": "in this paper , a framework of video face replacement is proposed and it deals with the flicker of swapped face in video sequence . this framework contains two main innovations : 1 ) the technique of image registration is exploited to align the source and target video faces for eliminating the flicker or jitter of the segmented video face sequence ; 2 ) a fast subpixel image registration method is proposed for farther accuracy and efficiency . unlike the priori works , it minimizes the overlapping region and takes spatiotemporal coherence into account . flicker in resulted videos is usually caused by the frequently changed bound of the blending target face and unregistered faces between and along video sequences . the subpixel image registration method is proposed to solve the flicker problem . during the alignment process , integer pixel registration is formulated by maximizing the similarity of images with down sampling strategy speeding up the process and sub-pixel image registration is a single-step image match via analytic method . experimental results show the proposed algorithm reduces the computation time and gets a high accuracy when conducting experiments on different data sets .", "topics": ["sampling ( signal processing )", "time complexity"]}
{"title": "improved dropout for shallow and deep learning", "abstract": "dropout has been witnessed with great success in training deep neural networks by independently zeroing out the outputs of neurons at random . it has also received a surge of interest for shallow learning , e.g . , logistic regression . however , the independent sampling for dropout could be suboptimal for the sake of convergence . in this paper , we propose to use multinomial sampling for dropout , i.e . , sampling features or neurons according to a multinomial distribution with different probabilities for different features/neurons . to exhibit the optimal dropout probabilities , we analyze the shallow learning with multinomial dropout and establish the risk bound for stochastic optimization . by minimizing a sampling dependent factor in the risk bound , we obtain a distribution-dependent dropout with sampling probabilities dependent on the second order statistics of the data distribution . to tackle the issue of evolving distribution of neurons in deep learning , we propose an efficient adaptive dropout ( named \\textbf { evolutional dropout } ) that computes the sampling probabilities on-the-fly from a mini-batch of examples . empirical studies on several benchmark datasets demonstrate that the proposed dropouts achieve not only much faster convergence and but also a smaller testing error than the standard dropout . for example , on the cifar-100 data , the evolutional dropout achieves relative improvements over 10\\ % on the prediction performance and over 50\\ % on the convergence speed compared to the standard dropout .", "topics": ["sampling ( signal processing )"]}
{"title": "loss factorization , weakly supervised learning and label noise robustness", "abstract": "we prove that the empirical risk of most well-known loss functions factors into a linear term aggregating all labels with a term that is label free , and can further be expressed by sums of the loss . this holds true even for non-smooth , non-convex losses and in any rkhs . the first term is a ( kernel ) mean operator -- the focal quantity of this work -- which we characterize as the sufficient statistic for the labels . the result tightens known generalization bounds and sheds new light on their interpretation . factorization has a direct application on weakly supervised learning . in particular , we demonstrate that algorithms like sgd and proximal methods can be adapted with minimal effort to handle weak supervision , once the mean operator has been estimated . we apply this idea to learning with asymmetric noisy labels , connecting and extending prior work . furthermore , we show that most losses enjoy a data-dependent ( by the mean operator ) form of noise robustness , in contrast with known negative results .", "topics": ["supervised learning", "loss function"]}
{"title": "a comparative study of collaborative filtering algorithms", "abstract": "collaborative filtering is a rapidly advancing research area . every year several new techniques are proposed and yet it is not clear which of the techniques work best and under what conditions . in this paper we conduct a study comparing several collaborative filtering techniques -- both classic and recent state-of-the-art -- in a variety of experimental contexts . specifically , we report conclusions controlling for number of items , number of users , sparsity level , performance criteria , and computational complexity . our conclusions identify what algorithms work well and in what conditions , and contribute to both industrial deployment collaborative filtering algorithms and to the research community .", "topics": ["computational complexity theory", "sparse matrix"]}
{"title": "flatness of the energy landscape for horn clauses", "abstract": "the little-hopfield neural network programmed with horn clauses is studied . we argue that the energy landscape of the system , corresponding to the inconsistency function for logical interpretations of the sets of horn clauses , has minimal ruggedness . this is supported by computer simulations .", "topics": ["simulation"]}
{"title": "cross-lingual annotation projection for semantic roles", "abstract": "this article considers the task of automatically inducing role-semantic annotations in the framenet paradigm for new languages . we propose a general framework that is based on annotation projection , phrased as a graph optimization problem . it is relatively inexpensive and has the potential to reduce the human effort involved in creating role-semantic resources . within this framework , we present projection models that exploit lexical and syntactic information . we provide an experimental evaluation on an english-german parallel corpus which demonstrates the feasibility of inducing high-precision german semantic role annotation both for manually and automatically annotated english data .", "topics": ["optimization problem"]}
{"title": "control variates for stochastic gradient mcmc", "abstract": "it is well known that markov chain monte carlo ( mcmc ) methods scale poorly with dataset size . a popular class of methods for solving this issue is stochastic gradient mcmc . these methods use a noisy estimate of the gradient of the log posterior , which reduces the per iteration computational cost of the algorithm . despite this , there are a number of results suggesting that stochastic gradient langevin dynamics ( sgld ) , probably the most popular of these methods , still has computational cost proportional to the dataset size . we suggest an alternative log posterior gradient estimate for stochastic gradient mcmc , which uses control variates to reduce the variance . we analyse sgld using this gradient estimate , and show that , under log-concavity assumptions on the target distribution , the computational cost required for a given level of accuracy is independent of the dataset size . next we show that a different control variate technique , known as zero variance control variates can be applied to sgmcmc algorithms for free . this post-processing step improves the inference of the algorithm by reducing the variance of the mcmc output . zero variance control variates rely on the gradient of the log posterior ; we explore how the variance reduction is affected by replacing this with the noisy gradient estimate calculated by sgmcmc .", "topics": ["gradient", "iteration"]}
{"title": "invariances and data augmentation for supervised music transcription", "abstract": "this paper explores a variety of models for frame-based music transcription , with an emphasis on the methods needed to reach state-of-the-art on human recordings . the translation-invariant network discussed in this paper , which combines a traditional filterbank with a convolutional neural network , was the top-performing model in the 2017 mirex multiple fundamental frequency estimation evaluation . this class of models shares parameters in the log-frequency domain , which exploits the frequency invariance of music to reduce the number of model parameters and avoid overfitting to the training data . all models in this paper were trained with supervision by labeled data from the musicnet dataset , augmented by random label-preserving pitch-shift transformations .", "topics": ["test set"]}
{"title": "totally looks like - how humans compare , compared to machines", "abstract": "perceptual judgment of image similarity by humans relies on rich internal representations ranging from low-level features to high-level concepts , scene properties and even cultural associations . however , existing methods and datasets attempting to explain perceived similarity use stimuli which arguably do not cover the full breadth of factors that affect human similarity judgments , even those geared toward this goal . we introduce a new dataset dubbed \\textbf { totally-looks-like } ( tll ) after a popular entertainment website , which contains images paired by humans as being visually similar . the dataset contains 6016 image-pairs from the wild , shedding light upon a rich and diverse set of criteria employed by human beings . we conduct experiments to try to reproduce the pairings via features extracted from state-of-the-art deep convolutional neural networks , as well as additional human experiments to verify the consistency of the collected data . though we create conditions to artificially make the matching task increasingly easier , we show that machine-extracted representations perform very poorly in terms of reproducing the matching selected by humans . we discuss and analyze these results , suggesting future directions for improvement of learned image representations .", "topics": ["high- and low-level"]}
{"title": "evaluating layers of representation in neural machine translation on part-of-speech and semantic tagging tasks", "abstract": "while neural machine translation ( nmt ) models provide improved translation quality in an elegant , end-to-end framework , it is less clear what they learn about language . recent work has started evaluating the quality of vector representations learned by nmt models on morphological and syntactic tasks . in this paper , we investigate the representations learned at different layers of nmt encoders . we train nmt systems on parallel data and use the trained models to extract features for training a classifier on two tasks : part-of-speech and semantic tagging . we then measure the performance of the classifier as a proxy to the quality of the original nmt model for the given task . our quantitative analysis yields interesting insights regarding representation learning in nmt models . for instance , we find that higher layers are better at learning semantics while lower layers tend to be better for part-of-speech tagging . we also observe little effect of the target language on source-side representations , especially with higher quality nmt models .", "topics": ["feature learning", "machine translation"]}
{"title": "a method for modeling co-occurrence propensity of clinical codes with application to icd-10-pcs auto-coding", "abstract": "objective . natural language processing methods for medical auto-coding , or automatic generation of medical billing codes from electronic health records , generally assign each code independently of the others . they may thus assign codes for closely related procedures or diagnoses to the same document , even when they do not tend to occur together in practice , simply because the right choice can be difficult to infer from the clinical narrative . materials and methods . we propose a method that injects awareness of the propensities for code co-occurrence into this process . first , a model is trained to estimate the conditional probability that one code is assigned by a human coder , given than another code is known to have been assigned to the same document . then , at runtime , an iterative algorithm is used to apply this model to the output of an existing statistical auto-coder to modify the confidence scores of the codes . results . we tested this method in combination with a primary auto-coder for icd-10 procedure codes , achieving a 12 % relative improvement in f-score over the primary auto-coder baseline . discussion . the proposed method can be used , with appropriate features , in combination with any auto-coder that generates codes with different levels of confidence . conclusion . the promising results obtained for icd-10 procedure codes suggest that the proposed method may have wider applications in auto-coding .", "topics": ["baseline ( configuration management )", "statistical classification"]}
{"title": "light field reconstruction using shearlet transform", "abstract": "in this article we develop an image based rendering technique based on light field reconstruction from a limited set of perspective views acquired by cameras . our approach utilizes sparse representation of epipolar-plane images in a directionally sensitive transform domain , obtained by an adapted discrete shearlet transform . the used iterative thresholding algorithm provides high-quality reconstruction results for relatively big disparities between neighboring views . the generated densely sampled light field of a given 3d scene is thus suitable for all applications which requires light field reconstruction . the proposed algorithm is compared favorably against state of the art depth image based rendering techniques .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "real-time pedestrian surveillance with top view cumulative grids", "abstract": "this manuscript presents an efficient approach to map pedestrian surveillance footage to an aerial view for global assessment of features . the analysis of the footages relies on low level computer vision and enable real-time surveillance . while we neglect object tracking , we introduce cumulative grids on top view scene flow visualization to highlight situations of interest in the footage . our approach is tested on multiview footage both from rgb cameras and , for the first time in the field , on rgb-d-sensors .", "topics": ["high- and low-level", "computer vision"]}
{"title": "homophilic clustering by locally asymmetric geometry", "abstract": "clustering is indispensable for data analysis in many scientific disciplines . detecting clusters from heavy noise remains challenging , particularly for high-dimensional sparse data . based on graph-theoretic framework , the present paper proposes a novel algorithm to address this issue . the locally asymmetric geometries of neighborhoods between data points result in a directed similarity graph to model the structural connectivity of data points . performing similarity propagation on this directed graph simply by its adjacency matrix powers leads to an interesting discovery , in the sense that if the in-degrees are ordered by the corresponding sorted out-degrees , they will be self-organized to be homophilic layers according to the different distributions of cluster densities , which is dubbed the homophilic in-degree figure ( the hi figure ) . with the hi figure , we can easily single out all cores of clusters , identify the boundary between cluster and noise , and visualize the intrinsic structures of clusters . based on the in-degree homophily , we also develop a simple efficient algorithm of linear space complexity to cluster noisy data . extensive experiments on toy and real-world scientific data validate the effectiveness of our algorithms .", "topics": ["cluster analysis", "sparse matrix"]}
{"title": "viser : visual self-regularization", "abstract": "in this work , we propose the use of large set of unlabeled images as a source of regularization data for learning robust visual representation . given a visual model trained by a labeled dataset in a supervised fashion , we augment our training samples by incorporating large number of unlabeled data and train a semi-supervised model . we demonstrate that our proposed learning approach leverages an abundance of unlabeled images and boosts the visual recognition performance which alleviates the need to rely on large labeled datasets for learning robust representation . to increment the number of image instances needed to learn robust visual models in our approach , each labeled image propagates its label to its nearest unlabeled image instances . these retrieved unlabeled images serve as local perturbations of each labeled image to perform visual self-regularization ( viser ) . to retrieve such visual self regularizers , we compute the cosine similarity in a semantic space defined by the penultimate layer in a fully convolutional neural network . we use the publicly available yahoo flickr creative commons 100m dataset as the source of our unlabeled image set and propose a distributed approximate nearest neighbor algorithm to make retrieval practical at that scale . using the labeled instances and their regularizer samples we show that we significantly improve object categorization and localization performance on the ms coco and visual genome datasets where objects appear in context .", "topics": ["supervised learning", "approximation algorithm"]}
{"title": "monocular depth estimation by learning from heterogeneous datasets", "abstract": "depth estimation provides essential information to perform autonomous driving and driver assistance . especially , monocular depth estimation is interesting from a practical point of view , since using a single camera is cheaper than many other options and avoids the need for continuous calibration strategies as required by stereo-vision approaches . state-of-the-art methods for monocular depth estimation are based on convolutional neural networks ( cnns ) . a promising line of work consists of introducing additional semantic information about the traffic scene when training cnns for depth estimation . in practice , this means that the depth data used for cnn training is complemented with images having pixel-wise semantic labels , which usually are difficult to annotate ( e.g . crowded urban images ) . moreover , so far it is common practice to assume that the same raw training data is associated with both types of ground truth , i.e . , depth and semantic labels . the main contribution of this paper is to show that this hard constraint can be circumvented , i.e . , that we can train cnns for depth estimation by leveraging the depth and semantic information coming from heterogeneous datasets . in order to illustrate the benefits of our approach , we combine kitti depth and cityscapes semantic segmentation datasets , outperforming state-of-the-art results on monocular depth estimation .", "topics": ["test set", "neural networks"]}
{"title": "feature engineering and ensemble modeling for paper acceptance rank prediction", "abstract": "measuring research impact and ranking academic achievement are important and challenging problems . having an objective picture of research institution is particularly valuable for students , parents and funding agencies , and also attracts attention from government and industry . kdd cup 2016 proposes the paper acceptance rank prediction task , in which the participants are asked to rank the importance of institutions based on predicting how many of their papers will be accepted at the 8 top conferences in computer science . in our work , we adopt a three-step feature engineering method , including basic features definition , finding similar conferences to enhance the feature set , and dimension reduction using pca . we propose three ranking models and the ensemble methods for combining such models . our experiment verifies the effectiveness of our approach . in kdd cup 2016 , we achieved the overall rank of the 2nd place .", "topics": ["data mining"]}
{"title": "evaluating scoped meaning representations", "abstract": "semantic parsing offers many opportunities to improve natural language understanding . we present a semantically annotated parallel corpus for english , german , italian , and dutch where sentences are aligned with scoped meaning representations in order to capture the semantics of negation , modals , quantification , and presupposition triggers . the semantic formalism is based on discourse representation theory , but concepts are represented by wordnet synsets and thematic roles by verbnet relations . translating scoped meaning representations to sets of clauses enables us to compare them for the purpose of semantic parser evaluation and checking translations . this is done by computing precision and recall on matching clauses , in a similar way as is done for abstract meaning representations . we show that our matching tool for evaluating scoped meaning representations is both accurate and efficient . applying this matching tool to three baseline semantic parsers yields f-scores between 43 % and 54 % . a pilot study is performed to automatically find changes in meaning by comparing meaning representations of translations . this comparison turns out to be an additional way of ( i ) finding annotation mistakes and ( ii ) finding instances where our semantic analysis needs to be improved .", "topics": ["baseline ( configuration management )", "natural language"]}
{"title": "metaheuristic optimization : algorithm analysis and open problems", "abstract": "metaheuristic algorithms are becoming an important part of modern optimization . a wide range of metaheuristic algorithms have emerged over the last two decades , and many metaheuristics such as particle swarm optimization are becoming increasingly popular . despite their popularity , mathematical analysis of these algorithms lacks behind . convergence analysis still remains unsolved for the majority of metaheuristic algorithms , while efficiency analysis is equally challenging . in this paper , we intend to provide an overview of convergence and efficiency studies of metaheuristics , and try to provide a framework for analyzing metaheuristics in terms of convergence and efficiency . this can form a basis for analyzing other algorithms . we also outline some open questions as further research topics .", "topics": ["mathematical optimization"]}
{"title": "learning two-branch neural networks for image-text matching tasks", "abstract": "image-language matching tasks have recently attracted a lot of attention in the computer vision field . these tasks include image-sentence matching , i.e . , given an image query , retrieving relevant sentences and vice versa , and region-phrase matching or visual grounding , i.e . , matching a phrase to relevant regions . this paper investigates two-branch neural networks for learning the similarity between these two data modalities . we propose two network structures that produce different output representations . the first one , referred to as an embedding network , learns an explicit shared latent embedding space with a maximum-margin ranking loss and novel neighborhood constraints . compared to standard triplet sampling , we perform improved neighborhood sampling that takes neighborhood information into consideration while constructing mini-batches . the second network structure , referred to as a similarity network , fuses the two branches via element-wise product and is trained with regression loss to directly predict a similarity score . extensive experiments show that our networks achieve high accuracies for phrase localization on the flickr30k entities dataset and for bi-directional image-sentence retrieval on flickr30k and mscoco datasets .", "topics": ["sampling ( signal processing )", "computer vision"]}
{"title": "pix2code : generating code from a graphical user interface screenshot", "abstract": "transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software , websites , and mobile applications . in this paper , we show that deep learning methods can be leveraged to train a model end-to-end to automatically generate code from a single input image with over 77 % of accuracy for three different platforms ( i.e . ios , android and web-based technologies ) .", "topics": ["end-to-end principle"]}
{"title": "single pass pca of matrix products", "abstract": "in this paper we present a new algorithm for computing a low rank approximation of the product $ a^tb $ by taking only a single pass of the two matrices $ a $ and $ b $ . the straightforward way to do this is to ( a ) first sketch $ a $ and $ b $ individually , and then ( b ) find the top components using pca on the sketch . our algorithm in contrast retains additional summary information about $ a , b $ ( e.g . row and column norms etc . ) and uses this additional information to obtain an improved approximation from the sketches . our main analytical result establishes a comparable spectral norm guarantee to existing two-pass methods ; in addition we also provide results from an apache spark implementation that shows better computational and statistical performance on real-world and synthetic evaluation datasets .", "topics": ["synthetic data"]}
{"title": "unifying computing and cognition : the sp theory and its applications", "abstract": "this book develops the conjecture that all kinds of information processing in computers and in brains may usefully be understood as `` information compression by multiple alignment , unification and search '' . this `` sp theory '' , which has been under development since 1987 , provides a unified view of such things as the workings of a universal turing machine , the nature of 'knowledge ' , the interpretation and production of natural language , pattern recognition and best-match information retrieval , several kinds of probabilistic reasoning , planning and problem solving , unsupervised learning , and a range of concepts in mathematics and logic . the theory also provides a basis for the design of an 'sp ' computer with several potential advantages compared with traditional digital computers .", "topics": ["unsupervised learning", "natural language"]}
{"title": "event and anomaly detection using tucker3 decomposition", "abstract": "failure detection in telecommunication networks is a vital task . so far , several supervised and unsupervised solutions have been provided for discovering failures in such networks . among them unsupervised approaches has attracted more attention since no label data is required . often , network devices are not able to provide information about the type of failure . in such cases the type of failure is not known in advance and the unsupervised setting is more appropriate for diagnosis . among unsupervised approaches , principal component analysis ( pca ) is a well-known solution which has been widely used in the anomaly detection literature and can be applied to matrix data ( e.g . users-features ) . however , one of the important properties of network data is their temporal sequential nature . so considering the interaction of dimensions over a third dimension , such as time , may provide us better insights into the nature of network failures . in this paper we demonstrate the power of three-way analysis to detect events and anomalies in time-evolving network data .", "topics": ["unsupervised learning"]}
{"title": "most likely separation of intensity and warping effects in image registration", "abstract": "this paper introduces a class of mixed-effects models for joint modeling of spatially correlated intensity variation and warping variation in 2d images . spatially correlated intensity variation and warp variation are modeled as random effects , resulting in a nonlinear mixed-effects model that enables simultaneous estimation of template and model parameters by optimization of the likelihood function . we propose an algorithm for fitting the model which alternates estimation of variance parameters and image registration . this approach avoids the potential estimation bias in the template estimate that arises when treating registration as a preprocessing step . we apply the model to datasets of facial images and 2d brain magnetic resonance images to illustrate the simultaneous estimation and prediction of intensity and warp effects .", "topics": ["nonlinear system"]}
{"title": "sparse signal recovery for binary compressed sensing by majority voting neural networks", "abstract": "in this paper , we propose majority voting neural networks for sparse signal recovery in binary compressed sensing . the majority voting neural network is composed of several independently trained feedforward neural networks employing the sigmoid function as an activation function . our empirical study shows that a choice of a loss function used in training processes for the network is of prime importance . we found a loss function suitable for sparse signal recovery , which includes a cross entropy-like term and an $ l_1 $ regularized term . from the experimental results , we observed that the majority voting neural network achieves excellent recovery performance , which is approaching the optimal performance as the number of component nets grows . the simple architecture of the majority voting neural networks would be beneficial for both software and hardware implementations .", "topics": ["neural networks", "loss function"]}
{"title": "turning speech into scripts", "abstract": "we describe an architecture for implementing spoken natural language dialogue interfaces to semi-autonomous systems , in which the central idea is to transform the input speech signal through successive levels of representation corresponding roughly to linguistic knowledge , dialogue knowledge , and domain knowledge . the final representation is an executable program in a simple scripting language equivalent to a subset of cshell . at each stage of the translation process , an input is transformed into an output , producing as a byproduct a `` meta-output '' which describes the nature of the transformation performed . we show how consistent use of the output/meta-output distinction permits a simple and perspicuous treatment of apparently diverse topics including resolution of pronouns , correction of user misconceptions , and optimization of scripts . the methods described have been concretely realized in a prototype speech interface to a simulation of the personal satellite assistant .", "topics": ["natural language", "simulation"]}
{"title": "a local-global approach to semantic segmentation in aerial images", "abstract": "aerial images are often taken under poor lighting conditions and contain low resolution objects , many times occluded by other objects . in this domain , visual context could be of great help , but there are still very few papers that consider context in aerial image understanding and still remains an open problem in computer vision . we propose a dual-stream deep neural network that processes information along two independent pathways . our model learns to combine local and global appearance in a complementary way , such that together form a powerful classifier . we test our dual-stream network on the task of buildings segmentation in aerial images and obtain state-of-the-art results on the massachusetts buildings dataset . we study the relative importance of local appearance versus the larger scene , as well as their performance in combination on three new buildings datasets . we clearly demonstrate the effectiveness of visual context in conjunction with deep neural networks for aerial image understanding .", "topics": ["statistical classification", "computer vision"]}
{"title": "speech-driven facial reenactment using conditional generative adversarial networks", "abstract": "we present a novel approach to generating photo-realistic images of a face with accurate lip sync , given an audio input . by using a recurrent neural network , we achieved mouth landmarks based on audio features . we exploited the power of conditional generative adversarial networks to produce highly-realistic face conditioned on a set of landmarks . these two networks together are capable of producing a sequence of natural faces in sync with an input audio track .", "topics": ["recurrent neural network"]}
{"title": "co-indexing labelled drss to represent and reason with ambiguities", "abstract": "the paper addresses the problem of representing ambiguities in a way that allows for monotonic disambiguation and for direct deductive computation . the paper focuses on an extension of the formalism of underspecified drss to ambiguities introduced by plural nps . it deals with the collective/distributive distinction , and also with generic and cumulative readings . in addition it provides a systematic account for an underspecified treatment of plural pronoun resolution .", "topics": ["computation"]}
{"title": "an investigation of the sequential sampling method for crossdocking simulation output variance reduction", "abstract": "this paper investigates the reduction of variance associated with a simulation output performance measure , using the sequential sampling method while applying minimum simulation replications , for a class of jit ( just in time ) warehousing system called crossdocking . we initially used the sequential sampling method to attain a desired 95 % confidence interval half width of plus/minus 0.5 for our chosen performance measure ( total usage cost , given the mean maximum level of 157,000 pounds and a mean minimum level of 149,000 pounds ) . from our results , we achieved a 95 % confidence interval half width of plus/minus 2.8 for our chosen performance measure ( total usage cost , with an average mean value of 115,000 pounds ) . however , the sequential sampling method requires a huge number of simulation replications to reduce variance for our simulation output value to the target level . arena ( version 11 ) simulation software was used to conduct this study .", "topics": ["simulation"]}
{"title": "statistical comparison of classifiers through bayesian hierarchical modelling", "abstract": "usually one compares the accuracy of two competing classifiers via null hypothesis significance tests ( nhst ) . yet the nhst tests suffer from important shortcomings , which can be overcome by switching to bayesian hypothesis testing . we propose a bayesian hierarchical model which jointly analyzes the cross-validation results obtained by two classifiers on multiple data sets . it returns the posterior probability of the accuracies of the two classifiers being practically equivalent or significantly different . a further strength of the hierarchical model is that , by jointly analyzing the results obtained on all data sets , it reduces the estimation error compared to the usual approach of averaging the cross-validation results obtained on a given data set .", "topics": ["bayesian network"]}
{"title": "pearl : a probabilistic chart parser", "abstract": "this paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the `` best '' parse of a sentence . the parser , pearl , is a time-asynchronous bottom-up chart parser with earley-type top-down prediction which pursues the highest-scoring theory in the chart , where the score of a theory represents the extent to which the context of the sentence predicts that interpretation . this parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context to predict likelihood . pearl also provides a framework for incorporating the results of previous work in part-of-speech assignment , unknown word models , and other probabilistic models of linguistic features into one parsing tool , interleaving these techniques instead of using the traditional pipeline architecture . in preliminary tests , pearl has been successful at resolving part-of-speech and word ( in speech processing ) ambiguity , determining categories for unknown words , and selecting correct parses first using a very loosely fitting covering grammar .", "topics": ["parsing", "natural language"]}
{"title": "geometric robustness of deep networks : analysis and improvement", "abstract": "deep convolutional neural networks have been shown to be vulnerable to arbitrary geometric transformations . however , there is no systematic method to measure the invariance properties of deep networks to such transformations . we propose manifool as a simple yet scalable algorithm to measure the invariance of deep networks . in particular , our algorithm measures the robustness of deep networks to geometric transformations in a worst-case regime as they can be problematic for sensitive applications . our extensive experimental results show that manifool can be used to measure the invariance of fairly complex networks on high dimensional datasets and these values can be used for analyzing the reasons for it . furthermore , we build on manifool to propose a new adversarial training scheme and we show its effectiveness on improving the invariance properties of deep neural networks .", "topics": ["scalability"]}
{"title": "multi-output polynomial networks and factorization machines", "abstract": "factorization machines and polynomial networks are supervised polynomial models based on an efficient low-rank decomposition . we extend these models to the multi-output setting , i.e . , for learning vector-valued functions , with application to multi-class or multi-task problems . we cast this as the problem of learning a 3-way tensor whose slices share a common basis and propose a convex formulation of that problem . we then develop an efficient conditional gradient algorithm and prove its global convergence , despite the fact that it involves a non-convex basis selection step . on classification tasks , we show that our algorithm achieves excellent accuracy with much sparser models than existing methods . on recommendation system tasks , we show how to combine our algorithm with a reduction from ordinal regression to multi-output classification and show that the resulting algorithm outperforms simple baselines in terms of ranking accuracy .", "topics": ["baseline ( configuration management )", "gradient"]}
{"title": "towards instance optimal bounds for best arm identification", "abstract": "in the classical best arm identification ( best- $ 1 $ -arm ) problem , we are given $ n $ stochastic bandit arms , each associated with a reward distribution with an unknown mean . we would like to identify the arm with the largest mean with probability at least $ 1-\\delta $ , using as few samples as possible . understanding the sample complexity of best- $ 1 $ -arm has attracted significant attention since the last decade . however , the exact sample complexity of the problem is still unknown . recently , chen and li made the gap-entropy conjecture concerning the instance sample complexity of best- $ 1 $ -arm . given an instance $ i $ , let $ \\mu_ { [ i ] } $ be the $ i $ th largest mean and $ \\delta_ { [ i ] } =\\mu_ { [ 1 ] } -\\mu_ { [ i ] } $ be the corresponding gap . $ h ( i ) =\\sum_ { i=2 } ^n\\delta_ { [ i ] } ^ { -2 } $ is the complexity of the instance . the gap-entropy conjecture states that $ \\omega\\left ( h ( i ) \\cdot\\left ( \\ln\\delta^ { -1 } +\\mathsf { ent } ( i ) \\right ) \\right ) $ is an instance lower bound , where $ \\mathsf { ent } ( i ) $ is an entropy-like term determined by the gaps , and there is a $ \\delta $ -correct algorithm for best- $ 1 $ -arm with sample complexity $ o\\left ( h ( i ) \\cdot\\left ( \\ln\\delta^ { -1 } +\\mathsf { ent } ( i ) \\right ) +\\delta_ { [ 2 ] } ^ { -2 } \\ln\\ln\\delta_ { [ 2 ] } ^ { -1 } \\right ) $ . if the conjecture is true , we would have a complete understanding of the instance-wise sample complexity of best- $ 1 $ -arm . we make significant progress towards the resolution of the gap-entropy conjecture . for the upper bound , we provide a highly nontrivial algorithm which requires \\ [ o\\left ( h ( i ) \\cdot\\left ( \\ln\\delta^ { -1 } +\\mathsf { ent } ( i ) \\right ) +\\delta_ { [ 2 ] } ^ { -2 } \\ln\\ln\\delta_ { [ 2 ] } ^ { -1 } \\mathrm { polylog } ( n , \\delta^ { -1 } ) \\right ) \\ ] samples in expectation . for the lower bound , we show that for any gaussian best- $ 1 $ -arm instance with gaps of the form $ 2^ { -k } $ , any $ \\delta $ -correct monotone algorithm requires $ \\omega\\left ( h ( i ) \\cdot\\left ( \\ln\\delta^ { -1 } + \\mathsf { ent } ( i ) \\right ) \\right ) $ samples in expectation .", "topics": ["sampling ( signal processing )"]}
{"title": "interspecies knowledge transfer for facial keypoint detection", "abstract": "we present a method for localizing facial keypoints on animals by transferring knowledge gained from human faces . instead of directly finetuning a network trained to detect keypoints on human faces to animal faces ( which is sub-optimal since human and animal faces can look quite different ) , we propose to first adapt the animal images to the pre-trained human detection network by correcting for the differences in animal and human face shape . we first find the nearest human neighbors for each animal image using an unsupervised shape matching method . we use these matches to train a thin plate spline warping network to warp each animal face to look more human-like . the warping network is then jointly finetuned with a pre-trained human facial keypoint detection network using an animal dataset . we demonstrate state-of-the-art results on both horse and sheep facial keypoint detection , and significant improvement over simple finetuning , especially when training data is scarce . additionally , we present a new dataset with 3717 images with horse face and facial keypoint annotations .", "topics": ["test set", "unsupervised learning"]}
{"title": "automatic detection of pulmonary embolism using computational intelligence", "abstract": "this article describes the implementation of a system designed to automatically detect the presence of pulmonary embolism in lung scans . these images are firstly segmented , before alignment and feature extraction using pca . the neural network was trained using the hybrid monte carlo method , resulting in a committee of 250 neural networks and good results are obtained .", "topics": ["image processing", "neural networks"]}
{"title": "compressing neural networks using the variational information bottleneck", "abstract": "neural networks can be compressed to reduce memory and computational requirements , or to increase accuracy by facilitating the use of a larger base architecture . in this paper we focus on pruning individual neurons , which can simultaneously trim model size , flops , and run-time memory . to improve upon the performance of existing compression algorithms we utilize the information bottleneck principle instantiated via a tractable variational bound . minimization of this information theoretic bound reduces the redundancy between adjacent layers by aggregating useful information into a subset of neurons that can be preserved . in contrast , the activations of disposable neurons are shut off via an attractive form of sparse regularization that emerges naturally from this framework , providing tangible advantages over traditional sparsity penalties without contributing additional tuning parameters to the energy landscape . we demonstrate state-of-the-art compression rates across an array of datasets and network architectures .", "topics": ["calculus of variations", "neural networks"]}
{"title": "generalized multi-view embedding for visual recognition and cross-modal retrieval", "abstract": "in this paper , the problem of multi-view embedding from different visual cues and modalities is considered . we propose a unified solution for subspace learning methods using the rayleigh quotient , which is extensible for multiple views , supervised learning , and non-linear embeddings . numerous methods including canonical correlation analysis , partial least sqaure regression and linear discriminant analysis are studied using specific intrinsic and penalty graphs within the same framework . non-linear extensions based on kernels and ( deep ) neural networks are derived , achieving better performance than the linear ones . moreover , a novel multi-view modular discriminant analysis ( mvmda ) is proposed by taking the view difference into consideration . we demonstrate the effectiveness of the proposed multi-view embedding methods on visual object recognition and cross-modal image retrieval , and obtain superior results in both applications compared to related methods .", "topics": ["supervised learning", "nonlinear system"]}
{"title": "verifiable source code documentation in controlled natural language", "abstract": "writing documentation about software internals is rarely considered a rewarding activity . it is highly time-consuming and the resulting documentation is fragile when the software is continuously evolving in a multi-developer setting . unfortunately , traditional programming environments poorly support the writing and maintenance of documentation . consequences are severe as the lack of documentation on software structure negatively impacts the overall quality of the software product . we show that using a controlled natural language with a reasoner and a query engine is a viable technique for verifying the consistency and accuracy of documentation and source code . using ace , a state-of-the-art controlled natural language , we present positive results on the comprehensibility and the general feasibility of creating and verifying documentation . as a case study , we used automatic documentation verification to identify and fix severe flaws in the architecture of a non-trivial piece of software . moreover , a user experiment shows that our language is faster and easier to learn and understand than other formal languages for software documentation .", "topics": ["natural language"]}
{"title": "adversarial examples for semantic image segmentation", "abstract": "machine learning methods in general and deep neural networks in particular have shown to be vulnerable to adversarial perturbations . so far this phenomenon has mainly been studied in the context of whole-image classification . in this contribution , we analyse how adversarial perturbations can affect the task of semantic segmentation . we show how existing adversarial attackers can be transferred to this task and that it is possible to create imperceptible adversarial perturbations that lead a deep network to misclassify almost all pixels of a chosen class while leaving network prediction nearly unchanged outside this class .", "topics": ["neural networks", "image segmentation"]}
{"title": "realizing rcc8 networks using convex regions", "abstract": "rcc8 is a popular fragment of the region connection calculus , in which qualitative spatial relations between regions , such as adjacency , overlap and parthood , can be expressed . while rcc8 is essentially dimensionless , most current applications are confined to reasoning about two-dimensional or three-dimensional physical space . in this paper , however , we are mainly interested in conceptual spaces , which typically are high-dimensional euclidean spaces in which the meaning of natural language concepts can be represented using convex regions . the aim of this paper is to analyze how the restriction to convex regions constrains the realizability of networks of rcc8 relations . first , we identify all ways in which the set of rcc8 base relations can be restricted to guarantee that consistent networks can be convexly realized in respectively 1d , 2d , 3d , and 4d . most surprisingly , we find that if the relation 'partially overlaps ' is disallowed , all consistent atomic rcc8 networks can be convexly realized in 4d . if instead refinements of the relation 'part of ' are disallowed , all consistent atomic rcc8 relations can be convexly realized in 3d . we furthermore show , among others , that any consistent rcc8 network with 2n+1 variables can be realized using convex regions in the n-dimensional euclidean space .", "topics": ["natural language"]}
{"title": "variational cumulant expansions for intractable distributions", "abstract": "intractable distributions present a common difficulty in inference within the probabilistic knowledge representation framework and variational methods have recently been popular in providing an approximate solution . in this article , we describe a perturbational approach in the form of a cumulant expansion which , to lowest order , recovers the standard kullback-leibler variational bound . higher-order terms describe corrections on the variational approach without incurring much further computational cost . the relationship to other perturbational approaches such as tap is also elucidated . we demonstrate the method on a particular class of undirected graphical models , boltzmann machines , for which our simulation results confirm improved accuracy and enhanced stability during learning .", "topics": ["graphical model", "approximation algorithm"]}
{"title": "real-time facial segmentation and performance capture from rgb input", "abstract": "we introduce the concept of unconstrained real-time 3d facial performance capture through explicit semantic segmentation in the rgb input . to ensure robustness , cutting edge supervised learning approaches rely on large training datasets of face images captured in the wild . while impressive tracking quality has been demonstrated for faces that are largely visible , any occlusion due to hair , accessories , or hand-to-face gestures would result in significant visual artifacts and loss of tracking accuracy . the modeling of occlusions has been mostly avoided due to its immense space of appearance variability . to address this curse of high dimensionality , we perform tracking in unconstrained images assuming non-face regions can be fully masked out . along with recent breakthroughs in deep learning , we demonstrate that pixel-level facial segmentation is possible in real-time by repurposing convolutional neural networks designed originally for general semantic segmentation . we develop an efficient architecture based on a two-stream deconvolution network with complementary characteristics , and introduce carefully designed training samples and data augmentation strategies for improved segmentation accuracy and robustness . we adopt a state-of-the-art regression-based facial tracking framework with segmented face images as training , and demonstrate accurate and uninterrupted facial performance capture in the presence of extreme occlusion and even side views . furthermore , the resulting segmentation can be directly used to composite partial 3d face models on the input images and enable seamless facial manipulation tasks , such as virtual make-up or face replacement .", "topics": ["supervised learning", "pixel"]}
{"title": "end to end learning for self-driving cars", "abstract": "we trained a convolutional neural network ( cnn ) to map raw pixels from a single front-facing camera directly to steering commands . this end-to-end approach proved surprisingly powerful . with minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways . it also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads . the system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal . we never explicitly trained it to detect , for example , the outline of roads . compared to explicit decomposition of the problem , such as lane marking detection , path planning , and control , our end-to-end system optimizes all processing steps simultaneously . we argue that this will eventually lead to better performance and smaller systems . better performance will result because the internal components self-optimize to maximize overall system performance , instead of optimizing human-selected intermediate criteria , e.g . , lane detection . such criteria understandably are selected for ease of human interpretation which does n't automatically guarantee maximum system performance . smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps . we used an nvidia devbox and torch 7 for training and an nvidia drive ( tm ) px self-driving car computer also running torch 7 for determining where to drive . the system operates at 30 frames per second ( fps ) .", "topics": ["test set", "end-to-end principle"]}
{"title": "packing and padding : coupled multi-index for accurate image retrieval", "abstract": "in bag-of-words ( bow ) based image retrieval , the sift visual word has a low discriminative power , so false positive matches occur prevalently . apart from the information loss during quantization , another cause is that the sift feature only describes the local gradient distribution . to address this problem , this paper proposes a coupled multi-index ( c-mi ) framework to perform feature fusion at indexing level . basically , complementary features are coupled into a multi-dimensional inverted index . each dimension of c-mi corresponds to one kind of feature , and the retrieval process votes for images similar in both sift and other feature spaces . specifically , we exploit the fusion of local color feature into c-mi . while the precision of visual match is greatly enhanced , we adopt multiple assignment to improve recall . the joint cooperation of sift and color features significantly reduces the impact of false positive matches . extensive experiments on several benchmark datasets demonstrate that c-mi improves the retrieval accuracy significantly , while consuming only half of the query time compared to the baseline . importantly , we show that c-mi is well complementary to many prior techniques . assembling these methods , we have obtained an map of 85.8 % and n-s score of 3.85 on holidays and ukbench datasets , respectively , which compare favorably with the state-of-the-arts .", "topics": ["baseline ( configuration management )", "gradient"]}
{"title": "a novel low-complexity framework in ultra-wideband imaging for breast cancer detection", "abstract": "in this research work , a novel framework is pro- posed as an efficient successor to traditional imaging methods for breast cancer detection in order to decrease the computational complexity . in this framework , the breast is devided into seg- ments in an iterative process and in each iteration , the one having the most probability of containing tumor with lowest possible resolution is selected by using suitable decision metrics . after finding the smallest tumor-containing segment , the resolution is increased in the detected tumor-containing segment , leaving the other parts of the breast image with low resolution . our framework is applied on the most common used beamforming techniques , such as delay and sum ( das ) and delay multiply and sum ( dmas ) and according to simulation results , our framework can decrease the computational complexity significantly for both das and dmas without imposing any degradation on accuracy of basic algorithms . the amount of complexity reduction can be determined manually or automatically based on two proposed methods that are described in this framework .", "topics": ["computational complexity theory", "simulation"]}
{"title": "learning binary or real-valued time-series via spike-timing dependent plasticity", "abstract": "a dynamic boltzmann machine ( dybm ) has been proposed as a model of a spiking neural network , and its learning rule of maximizing the log-likelihood of given time-series has been shown to exhibit key properties of spike-timing dependent plasticity ( stdp ) , which had been postulated and experimentally confirmed in the field of neuroscience as a learning rule that refines the hebbian rule . here , we relax some of the constraints in the dybm in a way that it becomes more suitable for computation and learning . we show that learning the dybm can be considered as logistic regression for binary-valued time-series . we also show how the dybm can learn real-valued data in the form of a gaussian dybm and discuss its relation to the vector autoregressive ( var ) model . the gaussian dybm extends the var by using additional explanatory variables , which correspond to the eligibility traces of the dybm and capture long term dependency of the time-series . numerical experiments show that the gaussian dybm significantly improves the predictive accuracy over var .", "topics": ["time series", "numerical analysis"]}
{"title": "benchmarking deep reinforcement learning for continuous control", "abstract": "recently , researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning . some notable examples include training agents to play atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs . however , it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark . in this work , we present a benchmark suite of continuous control tasks , including classic tasks like cart-pole swing-up , tasks with very high state and action dimensionality such as 3d humanoid locomotion , tasks with partial observations , and tasks with hierarchical structure . we report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms . both the benchmark and reference implementations are released at https : //github.com/rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers .", "topics": ["reinforcement learning", "pixel"]}
{"title": "the fast haar wavelet transform for signal & image processing", "abstract": "a method for the design of fast haar wavelet for signal processing and image processing has been proposed . in the proposed work , the analysis bank and synthesis bank of haar wavelet is modified by using polyphase structure . finally , the fast haar wavelet was designed and it satisfies alias free and perfect reconstruction condition . computational time and computational complexity is reduced in fast haar wavelet transform .", "topics": ["image processing", "computational complexity theory"]}
{"title": "a theoretical framework for context-sensitive temporal probability model construction with application to plan projection", "abstract": "we define a context-sensitive temporal probability logic for representing classes of discrete-time temporal bayesian networks . context constraints allow inference to be focused on only the relevant portions of the probabilistic knowledge . we provide a declarative semantics for our language . we present a bayesian network construction algorithm whose generated networks give sound and complete answers to queries . we use related concepts in logic programming to justify our approach . we have implemented a bayesian network construction algorithm for a subset of the theory and demonstrate it 's application to the problem of evaluating the effectiveness of treatments for acute cardiac conditions .", "topics": ["bayesian network"]}
{"title": "dynamic filter networks", "abstract": "in a traditional convolutional layer , the learned filters stay fixed after training . in contrast , we introduce a new framework , the dynamic filter network , where filters are generated dynamically conditioned on an input . we show that this architecture is a powerful one , with increased flexibility thanks to its adaptive nature , yet without an excessive increase in the number of model parameters . a wide variety of filtering operations can be learned this way , including local spatial transformations , but also others like selective ( de ) blurring or adaptive feature extraction . moreover , multiple such layers can be combined , e.g . in a recurrent architecture . we demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction , and reach state-of-the-art performance on the moving mnist dataset with a much smaller model . by visualizing the learned filters , we illustrate that the network has picked up flow information by only looking at unlabelled training data . this suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way , like optical flow and depth estimation .", "topics": ["test set", "feature extraction"]}
{"title": "a laplacian framework for option discovery in reinforcement learning", "abstract": "representation learning and option discovery are two of the biggest challenges in reinforcement learning ( rl ) . proto-value functions ( pvfs ) are a well-known approach for representation learning in mdps . in this paper we address the option discovery problem by showing how pvfs implicitly define options . we do it by introducing eigenpurposes , intrinsic reward functions derived from the learned representations . the options discovered from eigenpurposes traverse the principal directions of the state space . they are useful for multiple tasks because they are discovered without taking the environment 's rewards into consideration . moreover , different options act at different time scales , making them helpful for exploration . we demonstrate features of eigenpurposes in traditional tabular domains as well as in atari 2600 games .", "topics": ["feature learning", "reinforcement learning"]}
{"title": "contribution of case based reasoning ( cbr ) in the exploitation of return of experience . application to accident scenarii in railroad transport", "abstract": "the study is from a base of accident scenarii in rail transport ( feedback ) in order to develop a tool to share build and sustain knowledge and safety and secondly to exploit the knowledge stored to prevent the reproduction of accidents / incidents . this tool should ultimately lead to the proposal of prevention and protection measures to minimize the risk level of a new transport system and thus to improve safety . the approach to achieving this goal largely depends on the use of artificial intelligence techniques and rarely the use of a method of automatic learning in order to develop a feasibility model of a software tool based on case based reasoning ( cbr ) to exploit stored knowledge in order to create know-how that can help stimulate domain experts in the task of analysis , evaluation and certification of a new system .", "topics": ["artificial intelligence"]}
{"title": "behavior and path planning for the coalition of cognitive robots in smart relocation tasks", "abstract": "in this paper we outline the approach of solving special type of navigation tasks for robotic systems , when a coalition of robots ( agents ) acts in the 2d environment , which can be modified by the actions , and share the same goal location . the latter is originally unreachable for some members of the coalition , but the common task still can be accomplished as the agents can assist each other ( e.g . by modifying the environment ) . we call such tasks smart relocation tasks ( as the can not be solved by pure path planning methods ) and study spatial and behavior interaction of robots while solving them . we use cognitive approach and introduce semiotic knowledge representation - sign world model which underlines behavioral planning methodology . planning is viewed as a recursive search process in the hierarchical state-space induced by sings with path planning signs reside on the lowest level . reaching this level triggers path planning which is accomplished by state of the art grid-based planners focused on producing smooth paths ( e.g . lian ) and thus indirectly guarantying feasibility of that paths against agent 's dynamic constraints .", "topics": ["robot"]}
{"title": "talking about the moving image : a declarative model for image schema based embodied perception grounding and language generation", "abstract": "we present a general theory and corresponding declarative model for the embodied grounding and natural language based analytical summarisation of dynamic visuo-spatial imagery . the declarative model -- -ecompassing spatio-linguistic abstractions , image schemas , and a spatio-temporal feature based language generator -- - is modularly implemented within constraint logic programming ( clp ) . the implemented model is such that primitives of the theory , e.g . , pertaining to space and motion , image schemata , are available as first-class objects with `deep semantics ' suited for inference and query . we demonstrate the model with select examples broadly motivated by areas such as film , design , geography , smart environments where analytical natural language based externalisations of the moving image are central from the viewpoint of human interaction , evidence-based qualitative analysis , and sensemaking . keywords : moving image , visual semantics and embodiment , visuo-spatial cognition and computation , cognitive vision , computational models of narrative , declarative spatial reasoning", "topics": ["natural language"]}
{"title": "mean field multi-agent reinforcement learning", "abstract": "existing multi-agent reinforcement learning methods are limited typically to a small number of agents . when the agent number increases largely , the learning becomes intractable due to the curse of the dimensionality and the exponential growth of user interactions . in this paper , we present mean field reinforcement learning where the interactions within the population of agents are approximated by those between a single agent and the average effect from the overall population or neighboring agents ; the interplay between the two entities is mutually reinforced : the learning of the individual agent 's optimal policy depends on the dynamics of the population , while the dynamics of the population change according to the collective patterns of the individual policies . we develop practical mean field q-learning and mean field actor-critic algorithms and analyze the convergence of the solution . experiments on resource allocation , ising model estimation , and battle game tasks verify the learning effectiveness of our mean field approaches in handling many-agent interactions in population .", "topics": ["reinforcement learning", "time complexity"]}
{"title": "person re-identification by deep joint learning of multi-loss classification", "abstract": "existing person re-identification ( re-id ) methods rely mostly on either localised or global feature representation alone . this ignores their joint benefit and mutual complementary effects . in this work , we show the advantages of jointly learning local and global features in a convolutional neural network ( cnn ) by aiming to discover correlated local and global features in different context . specifically , we formulate a method for joint learning of local and global feature selection losses designed to optimise person re-id when using only generic matching metrics such as the l2 distance . we design a novel cnn architecture for jointly learning multi-loss ( jlml ) of local and global discriminative feature optimisation subject concurrently to the same re-id labelled information . extensive comparative evaluations demonstrate the advantages of this new jlml model for person re-id over a wide range of state-of-the-art re-id methods on five benchmarks ( viper , grid , cuhk01 , cuhk03 , market-1501 ) .", "topics": ["mathematical optimization"]}
{"title": "automatic rule extraction from long short term memory networks", "abstract": "although deep learning models have proven effective at solving problems in natural language processing , the mechanism by which they come to their conclusions is often unclear . as a result , these models are generally treated as black boxes , yielding no insight of the underlying learned patterns . in this paper we consider long short term memory networks ( lstms ) and demonstrate a new approach for tracking the importance of a given input to the lstm for a given output . by identifying consistently important patterns of words , we are able to distill state of the art lstms on sentiment analysis and question answering into a set of representative phrases . this representation is then quantitatively validated by using the extracted phrases to construct a simple , rule-based classifier which approximates the output of the lstm .", "topics": ["natural language processing"]}
{"title": "acronym-meaning extraction from corpora using multi-tape weighted finite-state machines", "abstract": "the automatic extraction of acronyms and their meaning from corpora is an important sub-task of text mining . it can be seen as a special case of string alignment , where a text chunk is aligned with an acronym . alternative alignments have different cost , and ideally the least costly one should give the correct meaning of the acronym . we show how this approach can be implemented by means of a 3-tape weighted finite-state machine ( 3-wfsm ) which reads a text chunk on tape 1 and an acronym on tape 2 , and generates all alternative alignments on tape 3 . the 3-wfsm can be automatically generated from a simple regular expression . no additional algorithms are required at any stage . our 3-wfsm has a size of 27 states and 64 transitions , and finds the best analysis of an acronym in a few milliseconds .", "topics": ["text corpus"]}
{"title": "action-conditional video prediction using deep networks in atari games", "abstract": "motivated by vision-based reinforcement learning ( rl ) problems , in particular atari games from the recent benchmark aracade learning environment ( ale ) , we consider spatio-temporal prediction problems where future ( image- ) frames are dependent on control variables or actions as well as previous frames . while not composed of natural scenes , frames in atari games are high-dimensional in size , can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly , can involve entry and departure of objects , and can involve deep partial observability . we propose and evaluate two deep neural network architectures that consist of encoding , action-conditional transformation , and decoding layers based on convolutional neural networks and recurrent neural networks . experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games . to the best of our knowledge , this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs .", "topics": ["recurrent neural network", "reinforcement learning"]}
{"title": "linguistically motivated vocabulary reduction for neural machine translation from turkish to english", "abstract": "the necessity of using a fixed-size word vocabulary in order to control the model complexity in state-of-the-art neural machine translation ( nmt ) systems is an important bottleneck on performance , especially for morphologically rich languages . conventional methods that aim to overcome this problem by using sub-word or character-level representations solely rely on statistics and disregard the linguistic properties of words , which leads to interruptions in the word structure and causes semantic and syntactic losses . in this paper , we propose a new vocabulary reduction method for nmt , which can reduce the vocabulary of a given input corpus at any rate while also considering the morphological properties of the language . our method is based on unsupervised morphology learning and can be , in principle , used for pre-processing any language pair . we also present an alternative word segmentation method based on supervised morphological analysis , which aids us in measuring the accuracy of our model . we evaluate our method in turkish-to-english nmt task where the input language is morphologically rich and agglutinative . we analyze different representation methods in terms of translation accuracy as well as the semantic and syntactic properties of the generated output . our method obtains a significant improvement of 2.3 bleu points over the conventional vocabulary reduction technique , showing that it can provide better accuracy in open vocabulary translation of morphologically rich languages .", "topics": ["machine translation"]}
{"title": "learning diagnostic policies from examples by systematic search", "abstract": "a diagnostic policy specifies what test to perform next , based on the results of previous tests , and when to stop and make a diagnosis . cost-sensitive diagnostic policies perform tradeoffs between ( a ) the cost of tests and ( b ) the cost of misdiagnoses . an optimal diagnostic policy minimizes the expected total cost . we formalize this diagnosis process as a markov decision process ( mdp ) . we investigate two types of algorithms for solving this mdp : systematic search based on ao* algorithm and greedy search ( particularly the value of information method ) . we investigate the issue of learning the mdp probabilities from examples , but only as they are relevant to the search for good policies . we do not learn nor assume a bayesian network for the diagnosis process . regularizers are developed to control overfitting and speed up the search . this research is the first that integrates overfitting prevention into systematic search . the paper has two contributions : it discusses the factors that make systematic search feasible for diagnosis , and it shows experimentally , on benchmark data sets , that systematic search methods produce better diagnostic policies than greedy methods .", "topics": ["bayesian network", "markov chain"]}
{"title": "dictionary-learning-based reconstruction method for electron tomography", "abstract": "electron tomography usually suffers from so called missing wedge artifacts caused by limited tilt angle range . an equally sloped tomography ( est ) acquisition scheme ( which should be called the linogram sampling scheme ) was recently applied to achieve 2.4-angstrom resolution . on the other hand , a compressive sensing-inspired reconstruction algorithm , known as adaptive dictionary based statistical iterative reconstruction ( adsir ) , has been reported for x-ray computed tomography . in this paper , we evaluate the est , adsir and an ordered-subset simultaneous algebraic reconstruction technique ( os-sart ) , and compare the es and equally angled ( ea ) data acquisition modes . our results show that os-sart is comparable to est , and the adsir outperforms est and os-sart . furthermore , the equally sloped projection data acquisition mode has no advantage over the conventional equally angled mode in the context .", "topics": ["sampling ( signal processing )", "dictionary"]}
{"title": "multiscale sequence modeling with a learned dictionary", "abstract": "we propose a generalization of neural network sequence models . instead of predicting one symbol at a time , our multi-scale model makes predictions over multiple , potentially overlapping multi-symbol tokens . a variation of the byte-pair encoding ( bpe ) compression algorithm is used to learn the dictionary of tokens that the model is trained with . when applied to language modelling , our model has the flexibility of character-level models while maintaining many of the performance benefits of word-level models . our experiments show that this model performs better than a regular lstm on language modeling tasks , especially for smaller models .", "topics": ["dictionary"]}
{"title": "multilingual visual sentiment concept matching", "abstract": "the impact of culture in visual emotion perception has recently captured the attention of multimedia research . in this study , we pro- vide powerful computational linguistics tools to explore , retrieve and browse a dataset of 16k multilingual affective visual concepts and 7.3m flickr images . first , we design an effective crowdsourc- ing experiment to collect human judgements of sentiment connected to the visual concepts . we then use word embeddings to repre- sent these concepts in a low dimensional vector space , allowing us to expand the meaning around concepts , and thus enabling insight about commonalities and differences among different languages . we compare a variety of concept representations through a novel evaluation task based on the notion of visual semantic relatedness . based on these representations , we design clustering schemes to group multilingual visual concepts , and evaluate them with novel metrics based on the crowdsourced sentiment annotations as well as visual semantic relatedness . the proposed clustering framework enables us to analyze the full multilingual dataset in-depth and also show an application on a facial data subset , exploring cultural in- sights of portrait-related affective visual concepts .", "topics": ["cluster analysis"]}
{"title": "a brief review of data mining application involving protein sequence classification", "abstract": "data mining techniques have been used by researchers for analyzing protein sequences . in protein analysis , especially in protein sequence classification , selection of feature is most important . popular protein sequence classification techniques involve extraction of specific features from the sequences . researchers apply some well-known classification techniques like neural networks , genetic algorithm , fuzzy artmap , rough set classifier etc for accurate classification . this paper presents a review is with three different classification models such as neural network model , fuzzy artmap model and rough set classifier model . a new technique for classifying protein sequences have been proposed in the end . the proposed technique tries to reduce the computational overheads encountered by earlier approaches and increase the accuracy of classification .", "topics": ["data mining"]}
{"title": "on the usability of deep networks for object-based image analysis", "abstract": "as computer vision before , remote sensing has been radically changed by the introduction of convolution neural networks . land cover use , object detection and scene understanding in aerial images rely more and more on deep learning to achieve new state-of-the-art results . recent architectures such as fully convolutional networks ( long et al . , 2015 ) can even produce pixel level annotations for semantic mapping . in this work , we show how to use such deep networks to detect , segment and classify different varieties of wheeled vehicles in aerial images from the isprs potsdam dataset . this allows us to tackle object detection and classification on a complex dataset made up of visually similar classes , and to demonstrate the relevance of such a subclass modeling approach . especially , we want to show that deep learning is also suitable for object-oriented analysis of earth observation data . first , we train a fcn variant on the isprs potsdam dataset and show how the learnt semantic maps can be used to extract precise segmentation of vehicles , which allow us studying the repartition of vehicles in the city . second , we train a cnn to perform vehicle classification on the vedai ( razakarivony and jurie , 2016 ) dataset , and transfer its knowledge to classify candidate segmented vehicles on the potsdam dataset .", "topics": ["object detection", "computer vision"]}
{"title": "cuni system for wmt16 automatic post-editing and multimodal translation tasks", "abstract": "neural sequence to sequence learning recently became a very promising paradigm in machine translation , achieving competitive results with statistical phrase-based systems . in this system description paper , we attempt to utilize several recently published methods used for neural sequential learning in order to build systems for wmt 2016 shared tasks of automatic post-editing and multimodal machine translation .", "topics": ["machine translation"]}
{"title": "sparse representation based augmented multinomial logistic extreme learning machine with weighted composite features for spectral spatial hyperspectral image classification", "abstract": "although extreme learning machine ( elm ) has been successfully applied to a number of pattern recognition problems , it fails to pro-vide sufficient good results in hyperspectral image ( hsi ) classification due to two main drawbacks . the first is due to the random weights and bias of elm , which may lead to ill-posed problems . the second is the lack of spatial information for classification . to tackle these two problems , in this paper , we propose a new framework for elm based spectral-spatial classification of hsi , where probabilistic modelling with sparse representation and weighted composite features ( wcf ) are employed respectively to derive the op-timized output weights and extract spatial features . first , the elm is represented as a concave logarithmic likelihood function under statistical modelling using the maximum a posteriori ( map ) . second , the sparse representation is applied to the laplacian prior to effi-ciently determine a logarithmic posterior with a unique maximum in order to solve the ill-posed problem of elm . the variable splitting and the augmented lagrangian are subsequently used to further reduce the computation complexity of the proposed algorithm and it has been proven a more efficient method for speed improvement . third , the spatial information is extracted using the weighted compo-site features ( wcfs ) to construct the spectral-spatial classification framework . in addition , the lower bound of the proposed method is derived by a rigorous mathematical proof . experimental results on two publicly available hsi data sets demonstrate that the proposed methodology outperforms elm and a number of state-of-the-art approaches .", "topics": ["computation", "sparse matrix"]}
{"title": "sparse principal component analysis via rotation and truncation", "abstract": "sparse principal component analysis ( sparse pca ) aims at finding a sparse basis to improve the interpretability over the dense basis of pca , meanwhile the sparse basis should cover the data subspace as much as possible . in contrast to most of existing work which deal with the problem by adding some sparsity penalties on various objectives of pca , in this paper , we propose a new method spcart , whose motivation is to find a rotation matrix and a sparse basis such that the sparse basis approximates the basis of pca after the rotation . the algorithm of spcart consists of three alternating steps : rotate pca basis , truncate small entries , and update the rotation matrix . its performance bounds are also given . spcart is efficient , with each iteration scaling linearly with the data dimension . it is easy to choose parameters in spcart , due to its explicit physical explanations . besides , we give a unified view to several existing sparse pca methods and discuss the connection with spcart . some ideas in spcart are extended to gpower , a popular sparse pca algorithm , to overcome its drawback . experimental results demonstrate that spcart achieves the state-of-the-art performance . it also achieves a good tradeoff among various criteria , including sparsity , explained variance , orthogonality , balance of sparsity among loadings , and computational speed .", "topics": ["sparse matrix", "iteration"]}
{"title": "a greedy approach to adapting the trace parameter for temporal difference learning", "abstract": "one of the main obstacles to broad application of reinforcement learning methods is the parameter sensitivity of our core learning algorithms . in many large-scale applications , online computation and function approximation represent key strategies in scaling up reinforcement learning algorithms . in this setting , we have effective and reasonably well understood algorithms for adapting the learning-rate parameter , online during learning . such meta-learning approaches can improve robustness of learning and enable specialization to current task , improving learning speed . for temporal-difference learning algorithms which we study here , there is yet another parameter , $ \\lambda $ , that similarly impacts learning speed and stability in practice . unfortunately , unlike the learning-rate parameter , $ \\lambda $ parametrizes the objective function that temporal-difference methods optimize . different choices of $ \\lambda $ produce different fixed-point solutions , and thus adapting $ \\lambda $ online and characterizing the optimization is substantially more complex than adapting the learning-rate parameter . there are no meta-learning method for $ \\lambda $ that can achieve ( 1 ) incremental updating , ( 2 ) compatibility with function approximation , and ( 3 ) maintain stability of learning under both on and off-policy sampling . in this paper we contribute a novel objective function for optimizing $ \\lambda $ as a function of state rather than time . we derive a new incremental , linear complexity $ \\lambda $ -adaption algorithm that does not require offline batch updating or access to a model of the world , and present a suite of experiments illustrating the practicality of our new algorithm in three different settings . taken together , our contributions represent a concrete step towards black-box application of temporal-difference learning methods in real world problems .", "topics": ["sampling ( signal processing )", "optimization problem"]}
{"title": "stagewise learning for sparse clustering of discretely-valued data", "abstract": "the performance of em in learning mixtures of product distributions often depends on the initialization . this can be problematic in crowdsourcing and other applications , e.g . when a small number of 'experts ' are diluted by a large number of noisy , unreliable participants . we develop a new em algorithm that is driven by these experts . in a manner that differs from other approaches , we start from a single mixture class . the algorithm then develops the set of 'experts ' in a stagewise fashion based on a mutual information criterion . at each stage em operates on this subset of the players , effectively regularizing the e rather than the m step . experiments show that stagewise em outperforms other initialization techniques for crowdsourcing and neurosciences applications , and can guide a full em to results comparable to those obtained knowing the exact distribution .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "left ventricle segmentation in cardiac mr images using fully convolutional network", "abstract": "medical image analysis , especially segmenting a specific organ , has an important role in developing clinical decision support systems . in cardiac magnetic resonance ( mr ) imaging , segmenting the left and right ventricles helps physicians diagnose different heart abnormalities . there are challenges for this task , including the intensity and shape similarity between left ventricle and other organs , inaccurate boundaries and presence of noise in most of the images . in this paper we propose an automated method for segmenting the left ventricle in cardiac mr images . we first automatically extract the region of interest , and then employ it as an input of a fully convolutional network . we train the network accurately despite the small number of left ventricle pixels in comparison with the whole image . thresholding on the output map of the fully convolutional network and selection of regions based on their roundness are performed in our proposed post-processing phase . the dice score of our method reaches 87.24 % by applying this algorithm on the york dataset of heart images .", "topics": ["pixel"]}
{"title": "structured probabilistic pruning for convolutional neural network acceleration", "abstract": "although deep convolutional neural network ( cnn ) has shown better performance in various computer vision tasks , its application is restricted by a significant increase in storage and computation . among cnn simplification techniques , parameter pruning is a promising approach which aims at reducing the number of weights of various layers without intensively reducing the original accuracy . in this paper , we propose a novel progressive parameter pruning method , named structured probabilistic pruning ( spp ) , which effectively prunes weights of convolutional layers in a probabilistic manner . specifically , unlike existing deterministic pruning approaches , where unimportant weights are permanently eliminated , spp introduces a pruning probability for each weight , and pruning is guided by sampling from the pruning probabilities . a mechanism is designed to increase and decrease pruning probabilities based on importance criteria for the training process . experiments show that , with 4x speedup , spp can accelerate alexnet with only 0.3 % loss of top-5 accuracy and vgg-16 with 0.8 % loss of top-5 accuracy in imagenet classification . moreover , spp can be directly applied to accelerate multi-branch cnn networks , such as resnet , without specific adaptations . our 2x speedup resnet-50 only suffers 0.8 % loss of top-5 accuracy on imagenet . we further prove the effectiveness of our method on transfer learning task on flower-102 dataset with alexnet .", "topics": ["iteration", "computation"]}
{"title": "query-focused multi-document summarization : combining a novel topic model with graph-based semi-supervised learning", "abstract": "graph-based semi-supervised learning has proven to be an effective approach for query-focused multi-document summarization . the problem of previous semi-supervised learning is that sentences are ranked without considering the higher level information beyond sentence level . researches on general summarization illustrated that the addition of topic level can effectively improve the summary quality . inspired by previous researches , we propose a two-layer ( i.e . sentence layer and topic layer ) graph-based semi-supervised learning approach . at the same time , we propose a novel topic model which makes full use of the dependence between sentences and words . experimental results on duc and tac data sets demonstrate the effectiveness of our proposed approach .", "topics": ["supervised learning"]}
{"title": "learning end-to-end multimodal sensor policies for autonomous navigation", "abstract": "multisensory polices are known to enhance both state estimation and target tracking . however , in the space of end-to-end sensorimotor control , this multi-sensor outlook has received limited attention . moreover , systematic ways to make policies robust to partial sensor failure are not well explored . in this work , we propose a specific customization of dropout , called \\textit { sensor dropout } , to improve multisensory policy robustness and handle partial failure in the sensor-set . we also introduce an additional auxiliary loss on the policy network in order to reduce variance in the band of potential multi- and uni-sensory policies to reduce jerks during policy switching triggered by an abrupt sensor failure or deactivation/activation . finally , through the visualization of gradients , we show that the learned policies are conditioned on the same latent states representation despite having diverse observations spaces - a hallmark of true sensor-fusion . simulation results of the multisensory policy , as visualized in torcs racing game , can be seen here : https : //youtu.be/qak2lcxjnzc .", "topics": ["simulation", "matrix regularization"]}
{"title": "direct load control of thermostatically controlled loads based on sparse observations using deep reinforcement learning", "abstract": "this paper considers a demand response agent that must find a near-optimal sequence of decisions based on sparse observations of its environment . extracting a relevant set of features from these observations is a challenging task and may require substantial domain knowledge . one way to tackle this problem is to store sequences of past observations and actions in the state vector , making it high dimensional , and apply techniques from deep learning . this paper investigates the capabilities of different deep learning techniques , such as convolutional neural networks and recurrent neural networks , to extract relevant features for finding near-optimal policies for a residential heating system and electric water heater that are hindered by sparse observations . our simulation results indicate that in this specific scenario , feeding sequences of time-series to an lstm network , which is a specific type of recurrent neural network , achieved a higher performance than stacking these time-series in the input of a convolutional neural network or deep neural network .", "topics": ["recurrent neural network", "time series"]}
{"title": "when waiting is not an option : learning options with a deliberation cost", "abstract": "recent work has shown that temporally extended actions ( options ) can be learned fully end-to-end as opposed to being specified in advance . while the problem of `` how '' to learn options is increasingly well understood , the question of `` what '' good options should be has remained elusive . we formulate our answer to what `` good '' options should be in the bounded rationality framework ( simon , 1957 ) through the notion of deliberation cost . we then derive practical gradient-based learning algorithms to implement this objective . our results in the arcade learning environment ( ale ) show increased performance and interpretability .", "topics": ["mathematical optimization", "time complexity"]}
{"title": "statistical augmentation of a chinese machine-readable dictionary", "abstract": "we describe a method of using statistically-collected chinese character groups from a corpus to augment a chinese dictionary . the method is particularly useful for extracting domain-specific and regional words not readily available in machine-readable dictionaries . output was evaluated both using human evaluators and against a previously available dictionary . we also evaluated performance improvement in automatic chinese tokenization . results show that our method outputs legitimate words , acronymic constructions , idioms , names and titles , as well as technical compounds , many of which were lacking from the original dictionary .", "topics": ["text corpus", "dictionary"]}
{"title": "sparse boltzmann machines with structure learning as applied to text analysis", "abstract": "we are interested in exploring the possibility and benefits of structure learning for deep models . as the first step , this paper investigates the matter for restricted boltzmann machines ( rbms ) . we conduct the study with replicated softmax , a variant of rbms for unsupervised text analysis . we present a method for learning what we call sparse boltzmann machines , where each hidden unit is connected to a subset of the visible units instead of all of them . empirical results show that the method yields models with significantly improved model fit and interpretability as compared with rbms where each hidden unit is connected to all visible units .", "topics": ["sparse matrix"]}
{"title": "temporal convolutional networks for action segmentation and detection", "abstract": "the ability to identify and temporally segment fine-grained human actions throughout a video is crucial for robotics , surveillance , education , and beyond . typical approaches decouple this problem by first extracting local spatiotemporal features from video frames and then feeding them into a temporal classifier that captures high-level temporal patterns . we introduce a new class of temporal models , which we call temporal convolutional networks ( tcns ) , that use a hierarchy of temporal convolutions to perform fine-grained action segmentation or detection . our encoder-decoder tcn uses pooling and upsampling to efficiently capture long-range temporal patterns whereas our dilated tcn uses dilated convolutions . we show that tcns are capable of capturing action compositions , segment durations , and long-range dependencies , and are over a magnitude faster to train than competing lstm-based recurrent neural networks . we apply these models to three challenging fine-grained datasets and show large improvements over the state of the art .", "topics": ["recurrent neural network", "high- and low-level"]}
{"title": "prosody based co-analysis for continuous recognition of coverbal gestures", "abstract": "although speech and gesture recognition has been studied extensively , all the successful attempts of combining them in the unified framework were semantically motivated , e.g . , keyword-gesture cooccurrence . such formulations inherited the complexity of natural language processing . this paper presents a bayesian formulation that uses a phenomenon of gesture and speech articulation for improving accuracy of automatic recognition of continuous coverbal gestures . the prosodic features from the speech signal were coanalyzed with the visual signal to learn the prior probability of co-occurrence of the prominent spoken segments with the particular kinematical phases of gestures . it was found that the above co-analysis helps in detecting and disambiguating visually small gestures , which subsequently improves the rate of continuous gesture recognition . the efficacy of the proposed approach was demonstrated on a large database collected from the weather channel broadcast . this formulation opens new avenues for bottom-up frameworks of multimodal integration .", "topics": ["natural language processing", "natural language"]}
{"title": "multi-objective contextual bandit problem with similarity information", "abstract": "in this paper we propose the multi-objective contextual bandit problem with similarity information . this problem extends the classical contextual bandit problem with similarity information by introducing multiple and possibly conflicting objectives . since the best arm in each objective can be different given the context , learning the best arm based on a single objective can jeopardize the rewards obtained from the other objectives . in order to evaluate the performance of the learner in this setup , we use a performance metric called the contextual pareto regret . essentially , the contextual pareto regret is the sum of the distances of the arms chosen by the learner to the context dependent pareto front . for this problem , we develop a new online learning algorithm called pareto contextual zooming ( pcz ) , which exploits the idea of contextual zooming to learn the arms that are close to the pareto front for each observed context by adaptively partitioning the joint context-arm set according to the observed rewards and locations of the context-arm pairs selected in the past . then , we prove that pcz achieves $ \\tilde o ( t^ { ( 1+d_p ) / ( 2+d_p ) } ) $ pareto regret where $ d_p $ is the pareto zooming dimension that depends on the size of the set of near-optimal context-arm pairs . moreover , we show that this regret bound is nearly optimal by providing an almost matching $ \\omega ( t^ { ( 1+d_p ) / ( 2+d_p ) } ) $ lower bound .", "topics": ["regret ( decision theory )", "artificial intelligence"]}
{"title": "structured label inference for visual understanding", "abstract": "visual data such as images and videos contain a rich source of structured semantic labels as well as a wide range of interacting components . visual content could be assigned with fine-grained labels describing major components , coarse-grained labels depicting high level abstractions , or a set of labels revealing attributes . such categorization over different , interacting layers of labels evinces the potential for a graph-based encoding of label information . in this paper , we exploit this rich structure for performing graph-based inference in label space for a number of tasks : multi-label image and video classification and action detection in untrimmed videos . we consider the use of the bidirectional inference neural network ( binn ) and structured inference neural network ( sinn ) for performing graph-based inference in label space and propose a long short-term memory ( lstm ) based extension for exploiting activity progression on untrimmed videos . the methods were evaluated on ( i ) the animal with attributes ( awa ) , scene understanding ( sun ) and nus-wide datasets for multi-label image classification , ( ii ) the first two releases of the youtube-8m large scale dataset for multi-label video classification , and ( iii ) the thumos'14 and multithumos video datasets for action detection . our results demonstrate the effectiveness of structured label inference in these challenging tasks , achieving significant improvements against baselines .", "topics": ["computer vision"]}
{"title": "deep reinforcement learning for unsupervised video summarization with diversity-representativeness reward", "abstract": "video summarization aims to facilitate large-scale video browsing by producing short , concise summaries that are diverse and representative of original videos . in this paper , we formulate video summarization as a sequential decision-making process and develop a deep summarization network ( dsn ) to summarize videos . dsn predicts for each video frame a probability , which indicates how likely a frame is selected , and then takes actions based on the probability distributions to select frames , forming video summaries . to train our dsn , we propose an end-to-end , reinforcement learning-based framework , where we design a novel reward function that jointly accounts for diversity and representativeness of generated summaries and does not rely on labels or user interactions at all . during training , the reward function judges how diverse and representative the generated summaries are , while dsn strives for earning higher rewards by learning to produce more diverse and more representative summaries . since labels are not required , our method can be fully unsupervised . extensive experiments on two benchmark datasets show that our unsupervised method not only outperforms other state-of-the-art unsupervised methods , but also is comparable to or even superior than most of published supervised approaches .", "topics": ["unsupervised learning", "reinforcement learning"]}
{"title": "asynchronous decentralized parallel stochastic gradient descent", "abstract": "most commonly used distributed machine learning systems are either synchronous or centralized asynchronous . synchronous algorithms like allreduce-sgd perform poorly in a heterogeneous environment , while asynchronous algorithms using a parameter server suffer from 1 ) communication bottleneck at parameter servers when workers are many , and 2 ) significantly worse convergence when the traffic to parameter server is congested . can we design an algorithm that is robust in a heterogeneous environment , while being communication efficient and maintaining the best-possible convergence rate ? in this paper , we propose an asynchronous decentralized stochastic gradient decent algorithm ( ad-psgd ) satisfying all above expectations . our theoretical analysis shows ad-psgd converges at the optimal $ o ( 1/\\sqrt { k } ) $ rate as sgd and has linear speedup w.r.t . number of workers . empirically , ad-psgd outperforms the best of decentralized parallel sgd ( d-psgd ) , asynchronous parallel sgd ( a-psgd ) , and standard data parallel sgd ( allreduce-sgd ) , often by orders of magnitude in a heterogeneous environment . when training resnet-50 on imagenet with up to 128 gpus , ad-psgd converges ( w.r.t epochs ) similarly to the allreduce-sgd , but each epoch can be up to 4-8x faster than its synchronous counterparts in a network-sharing hpc environment . to the best of our knowledge , ad-psgd is the first asynchronous algorithm that achieves a similar epoch-wise convergence rate as allreduce-sgd , at an over 100-gpu scale .", "topics": ["computational complexity theory", "gradient descent"]}
{"title": "adaptive graph convolutional neural networks", "abstract": "graph convolutional neural networks ( graph cnns ) are generalizations of classical cnns to handle graph data such as molecular data , point could and social networks . current filters in graph cnns are built for fixed and shared graph structure . however , for most real data , the graph structures varies in both size and connectivity . the paper proposes a generalized and flexible graph cnn taking data of arbitrary graph structure as input . in that way a task-driven adaptive graph is learned for each graph data while training . to efficiently learn the graph , a distance metric learning is proposed . extensive experiments on nine graph-structured datasets have demonstrated the superior performance improvement on both convergence speed and predictive accuracy .", "topics": ["neural networks"]}
{"title": "syntactic topic models", "abstract": "the syntactic topic model ( stm ) is a bayesian nonparametric model of language that discovers latent distributions of words ( topics ) that are both semantically and syntactically coherent . the stm models dependency parsed corpora where sentences are grouped into documents . it assumes that each word is drawn from a latent topic chosen by combining document-level features and the local syntactic context . each document has a distribution over latent topics , as in topic models , which provides the semantic consistency . each element in the dependency parse tree also has a distribution over the topics of its children , as in latent-state syntax models , which provides the syntactic consistency . these distributions are convolved so that the topic of each word is likely under both its document and syntactic context . we derive a fast posterior inference algorithm based on variational methods . we report qualitative and quantitative studies on both synthetic data and hand-parsed documents . we show that the stm is a more predictive model of language than current models based only on syntax or only on topics .", "topics": ["calculus of variations", "approximation algorithm"]}
{"title": "using wikipedia to boost svd recommender systems", "abstract": "singular value decomposition ( svd ) has been used successfully in recent years in the area of recommender systems . in this paper we present how this model can be extended to consider both user ratings and information from wikipedia . by mapping items to wikipedia pages and quantifying their similarity , we are able to use this information in order to improve recommendation accuracy , especially when the sparsity is high . another advantage of the proposed approach is the fact that it can be easily integrated into any other svd implementation , regardless of additional parameters that may have been added to it . preliminary experimental results on the movielens dataset are encouraging .", "topics": ["sparse matrix"]}
{"title": "phrase-based image captioning", "abstract": "generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing . in this paper , we present a simple model that is able to generate descriptive sentences given a sample image . this model has a strong focus on the syntax of the descriptions . we train a purely bilinear model that learns a metric between an image representation ( generated from a previously trained convolutional neural network ) and phrases that are used to described them . the system is then able to infer phrases from a given image sample . based on caption syntax statistics , we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred . our approach , which is considerably simpler than state-of-the-art models , achieves comparable results in two popular datasets for the task : flickr30k and the recently proposed microsoft coco .", "topics": ["natural language processing", "computer vision"]}
{"title": "learning max-margin tree predictors", "abstract": "structured prediction is a powerful framework for coping with joint prediction of interacting outputs . a central difficulty in using this framework is that often the correct label dependence structure is unknown . at the same time , we would like to avoid an overly complex structure that will lead to intractable prediction . in this work we address the challenge of learning tree structured predictive models that achieve high accuracy while at the same time facilitate efficient ( linear time ) inference . we start by proving that this task is in general np-hard , and then suggest an approximate alternative . briefly , our crank approach relies on a novel circuit-rank regularizer that penalizes non-tree structures and that can be optimized using a cccp procedure . we demonstrate the effectiveness of our approach on several domains and show that , despite the relative simplicity of the structure , prediction accuracy is competitive with a fully connected model that is computationally costly at prediction time .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "herding as a learning system with edge-of-chaos dynamics", "abstract": "herding defines a deterministic dynamical system at the edge of chaos . it generates a sequence of model states and parameters by alternating parameter perturbations with state maximizations , where the sequence of states can be interpreted as `` samples '' from an associated mrf model . herding differs from maximum likelihood estimation in that the sequence of parameters does not converge to a fixed point and differs from an mcmc posterior sampling approach in that the sequence of states is generated deterministically . herding may be interpreted as a '' perturb and map '' method where the parameter perturbations are generated using a deterministic nonlinear dynamical system rather than randomly from a gumbel distribution . this chapter studies the distinct statistical characteristics of the herding algorithm and shows that the fast convergence rate of the controlled moments may be attributed to edge of chaos dynamics . the herding algorithm can also be generalized to models with latent variables and to a discriminative learning setting . the perceptron cycling theorem ensures that the fast moment matching property is preserved in the more general framework .", "topics": ["sampling ( signal processing )", "nonlinear system"]}
{"title": "consistently estimating markov chains with noisy aggregate data", "abstract": "we address the problem of estimating the parameters of a time-homogeneous markov chain given only noisy , aggregate data . this arises when a population of individuals behave independently according to a markov chain , but individual sample paths can not be observed due to limitations of the observation process or the need to protect privacy . instead , only population-level counts of the number of individuals in each state at each time step are available . when these counts are exact , a conditional least squares ( cls ) estimator is known to be consistent and asymptotically normal . we initiate the study of method of moments estimators for this problem to handle the more realistic case when observations are additionally corrupted by noise . we show that cls can be interpreted as a simple `` plug-in '' method of moments estimator . however , when observations are noisy , it is not consistent because it fails to account for additional variance introduced by the noise . we develop a new , simpler method of moments estimator that bypasses this problem and is consistent under noisy observations .", "topics": ["markov chain"]}
{"title": "two step cca : a new spectral method for estimating vector models of words", "abstract": "unlabeled data is often used to learn representations which can be used to supplement baseline features in a supervised learner . for example , for text applications where the words lie in a very high dimensional space ( the size of the vocabulary ) , one can learn a low rank `` dictionary '' by an eigen-decomposition of the word co-occurrence matrix ( e.g . using pca or cca ) . in this paper , we present a new spectral method based on cca to learn an eigenword dictionary . our improved procedure computes two set of ccas , the first one between the left and right contexts of the given word and the second one between the projections resulting from this cca and the word itself . we prove theoretically that this two-step procedure has lower sample complexity than the simple single step procedure and also illustrate the empirical efficacy of our approach and the richness of representations learned by our two step cca ( tscca ) procedure on the tasks of pos tagging and sentiment classification .", "topics": ["baseline ( configuration management )", "supervised learning"]}
{"title": "image denoising through bivariate shrinkage function in framelet domain", "abstract": "denoising of coefficients in a sparse domain ( e.g . wavelet ) has been researched extensively because of its simplicity and effectiveness . literature mainly has focused on designing the best global threshold . however , this paper proposes a new denoising method using bivariate shrinkage function in framelet domain . in the proposed method , maximum aposteriori probability is used for estimate of the denoised coefficient and non-gaussian bivariate function is applied to model the statistics of framelet coefficients . for every framelet coefficient , there is a corresponding threshold depending on the local statistics of framelet coefficients . experimental results show that using bivariate shrinkage function in framelet domain yields significantly superior image quality and higher psnr than some well-known denoising methods .", "topics": ["noise reduction", "sparse matrix"]}
{"title": "submodular function maximization for group elevator scheduling", "abstract": "we propose a novel approach for group elevator scheduling by formulating it as the maximization of submodular function under a matroid constraint . in particular , we propose to model the total waiting time of passengers using a quadratic boolean function . the unary and pairwise terms in the function denote the waiting time for single and pairwise allocation of passengers to elevators , respectively . we show that this objective function is submodular . the matroid constraints ensure that every passenger is allocated to exactly one elevator . we use a greedy algorithm to maximize the submodular objective function , and derive provable guarantees on the optimality of the solution . we tested our algorithm using elevate 8 , a commercial-grade elevator simulator that allows simulation with a wide range of elevator settings . we achieve significant improvement over the existing algorithms .", "topics": ["optimization problem", "loss function"]}
{"title": "opportunities & challenges in automatic speech recognition", "abstract": "automatic speech recognition enables a wide range of current and emerging applications such as automatic transcription , multimedia content analysis , and natural human-computer interfaces . this paper provides a glimpse of the opportunities and challenges that parallelism provides for automatic speech recognition and related application research from the point of view of speech researchers . the increasing parallelism in computing platforms opens three major possibilities for speech recognition systems : improving recognition accuracy in non-ideal , everyday noisy environments ; increasing recognition throughput in batch processing of speech data ; and reducing recognition latency in realtime usage scenarios . this paper describes technical challenges , approaches taken , and possible directions for future research to guide the design of efficient parallel software and hardware infrastructures .", "topics": ["speech recognition"]}
{"title": "sliced wasserstein kernels for probability distributions", "abstract": "optimal transport distances , otherwise known as wasserstein distances , have recently drawn ample attention in computer vision and machine learning as a powerful discrepancy measure for probability distributions . the recent developments on alternative formulations of the optimal transport have allowed for faster solutions to the problem and has revamped its practical applications in machine learning . in this paper , we exploit the widely used kernel methods and provide a family of provably positive definite kernels based on the sliced wasserstein distance and demonstrate the benefits of these kernels in a variety of learning tasks . our work provides a new perspective on the application of optimal transport flavored distances through kernel methods in machine learning tasks .", "topics": ["computer vision"]}
{"title": "stability of multi-task kernel regression algorithms", "abstract": "we study the stability properties of nonlinear multi-task regression in reproducing hilbert spaces with operator-valued kernels . such kernels , a.k.a . multi-task kernels , are appropriate for learning prob- lems with nonscalar outputs like multi-task learning and structured out- put prediction . we show that multi-task kernel regression algorithms are uniformly stable in the general case of infinite-dimensional output spaces . we then derive under mild assumption on the kernel generaliza- tion bounds of such algorithms , and we show their consistency even with non hilbert-schmidt operator-valued kernels . we demonstrate how to apply the results to various multi-task kernel regression methods such as vector-valued svr and functional ridge regression .", "topics": ["kernel ( operating system )", "nonlinear system"]}
{"title": "multiple instance fuzzy inference neural networks", "abstract": "fuzzy logic is a powerful tool to model knowledge uncertainty , measurements imprecision , and vagueness . however , there is another type of vagueness that arises when data have multiple forms of expression that fuzzy logic does not address quite well . this is the case for multiple instance learning problems ( mil ) . in mil , an object is represented by a collection of instances , called a bag . a bag is labeled negative if all of its instances are negative , and positive if at least one of its instances is positive . positive bags encode ambiguity since the instances themselves are not labeled . in this paper , we introduce fuzzy inference systems and neural networks designed to handle bags of instances as input and capable of learning from ambiguously labeled data . first , we introduce the multiple instance sugeno style fuzzy inference ( mi-sugeno ) that extends the standard sugeno style inference to handle reasoning with multiple instances . second , we use mi-sugeno to define and develop multiple instance adaptive neuro fuzzy inference system ( mi-anfis ) . we expand the architecture of the standard anfis to allow reasoning with bags and derive a learning algorithm using backpropagation to identify the premise and consequent parameters of the network . the proposed inference system is tested and validated using synthetic and benchmark datasets suitable for mil problems . we also apply the proposed mi-anfis to fuse the output of multiple discrimination algorithms for the purpose of landmine detection using ground penetrating radar .", "topics": ["synthetic data", "neural networks"]}
{"title": "an approach for weakly-supervised deep information retrieval", "abstract": "recent developments in neural information retrieval models have been promising , but a problem remains : human relevance judgments are expensive to produce , while neural models require a considerable amount of training data . in an attempt to fill this gap , we present an approach that -- -given a weak training set of pseudo-queries , documents , relevance information -- -filters the data to produce effective positive and negative query-document pairs . this allows large corpora to be used as neural ir model training data , while eliminating training examples that do not transfer well to relevance scoring . the filters include unsupervised ranking heuristics and a novel measure of interaction similarity . we evaluate our approach using a news corpus with article headlines acting as pseudo-queries and article content as documents , with implicit relevance between an article 's headline and its content . by using our approach to train state-of-the-art neural ir models and comparing to established baselines , we find that training data generated by our approach can lead to good results on a benchmark test collection .", "topics": ["test set", "text corpus"]}
{"title": "a human-grounded evaluation benchmark for local explanations of machine learning", "abstract": "in order for people to be able to trust and take advantage of the results of advanced machine learning and artificial intelligence solutions for real decision making , people need to be able to understand the machine rationale for given output . research in explain artificial intelligence ( xai ) addresses the aim , but there is a need for evaluation of human relevance and understandability of explanations . our work contributes a novel methodology for evaluating the quality or human interpretability of explanations for machine learning models . we present an evaluation benchmark for instance explanations from text and image classifiers . the explanation meta-data in this benchmark is generated from user annotations of image and text samples . we describe the benchmark and demonstrate its utility by a quantitative evaluation on explanations generated from a recent machine learning algorithm . this research demonstrates how human-grounded evaluation could be used as a measure to qualify local machine-learning explanations .", "topics": ["relevance", "artificial intelligence"]}
{"title": "classification with noisy labels by importance reweighting", "abstract": "in this paper , we study a classification problem in which sample labels are randomly corrupted . in this scenario , there is an unobservable sample with noise-free labels . however , before being observed , the true labels are independently flipped with a probability $ \\rho\\in [ 0,0.5 ) $ , and the random label noise can be class-conditional . here , we address two fundamental problems raised by this scenario . the first is how to best use the abundant surrogate loss functions designed for the traditional classification problem when there is label noise . we prove that any surrogate loss function can be used for classification with noisy labels by using importance reweighting , with consistency assurance that the label noise does not ultimately hinder the search for the optimal classifier of the noise-free sample . the other is the open problem of how to obtain the noise rate $ \\rho $ . we show that the rate is upper bounded by the conditional probability $ p ( y|x ) $ of the noisy sample . consequently , the rate can be estimated , because the upper bound can be easily reached in classification problems . experimental results on synthetic and real datasets confirm the efficiency of our methods .", "topics": ["statistical classification", "synthetic data"]}
{"title": "surrogate search as a way to combat harmful effects of ill-behaved evaluation functions", "abstract": "recently , several researchers have found that cost-based satisficing search with a* often runs into problems . although some `` work arounds '' have been proposed to ameliorate the problem , there has been little concerted effort to pinpoint its origin . in this paper , we argue that the origins of this problem can be traced back to the fact that most planners that try to optimize cost also use cost-based evaluation functions ( i.e . , f ( n ) is a cost estimate ) . we show that cost-based evaluation functions become ill-behaved whenever there is a wide variance in action costs ; something that is all too common in planning domains . the general solution to this malady is what we call a surrogatesearch , where a surrogate evaluation function that does n't directly track the cost objective , and is resistant to cost-variance , is used . we will discuss some compelling choices for surrogate evaluation functions that are based on size rather that cost . of particular practical interest is a cost-sensitive version of size-based evaluation function -- where the heuristic estimates the size of cheap paths , as it provides attractive quality vs. speed tradeoffs", "topics": ["heuristic"]}
{"title": "exact and inexact subsampled newton methods for optimization", "abstract": "the paper studies the solution of stochastic optimization problems in which approximations to the gradient and hessian are obtained through subsampling . we first consider newton-like methods that employ these approximations and discuss how to coordinate the accuracy in the gradient and hessian to yield a superlinear rate of convergence in expectation . the second part of the paper analyzes an inexact newton method that solves linear systems approximately using the conjugate gradient ( cg ) method , and that samples the hessian and not the gradient ( the gradient is assumed to be exact ) . we provide a complexity analysis for this method based on the properties of the cg iteration and the quality of the hessian approximation , and compare it with a method that employs a stochastic gradient iteration instead of the cg method . we report preliminary numerical results that illustrate the performance of inexact subsampled newton methods on machine learning applications based on logistic regression .", "topics": ["numerical analysis", "iteration"]}
{"title": "statistical guarantees for the em algorithm : from population to sample-based analysis", "abstract": "we develop a general framework for proving rigorous guarantees on the performance of the em algorithm and a variant known as gradient em . our analysis is divided into two parts : a treatment of these algorithms at the population level ( in the limit of infinite data ) , followed by results that apply to updates based on a finite set of samples . first , we characterize the domain of attraction of any global maximizer of the population likelihood . this characterization is based on a novel view of the em updates as a perturbed form of likelihood ascent , or in parallel , of the gradient em updates as a perturbed form of standard gradient ascent . leveraging this characterization , we then provide non-asymptotic guarantees on the em and gradient em algorithms when applied to a finite set of samples . we develop consequences of our general theory for three canonical examples of incomplete-data problems : mixture of gaussians , mixture of regressions , and linear regression with covariates missing completely at random . in each case , our theory guarantees that with a suitable initialization , a relatively small number of em ( or gradient em ) steps will yield ( with high probability ) an estimate that is within statistical error of the mle . we provide simulations to confirm this theoretically predicted behavior .", "topics": ["gradient descent", "simulation"]}
{"title": "when is clustering perturbation robust ?", "abstract": "clustering is a fundamental data mining tool that aims to divide data into groups of similar items . generally , intuition about clustering reflects the ideal case -- exact data sets endowed with flawless dissimilarity between individual instances . in practice however , these cases are in the minority , and clustering applications are typically characterized by noisy data sets with approximate pairwise dissimilarities . as such , the efficacy of clustering methods in practical applications necessitates robustness to perturbations . in this paper , we perform a formal analysis of perturbation robustness , revealing that the extent to which algorithms can exhibit this desirable characteristic is inherently limited , and identifying the types of structures that allow popular clustering paradigms to discover meaningful clusters in spite of faulty data .", "topics": ["cluster analysis", "data mining"]}
{"title": "a cellular automaton based controller for a ms. pac-man agent", "abstract": "video games can be used as an excellent test bed for artificial intelligence ( ai ) techniques . they are challenging and non-deterministic , this makes it very difficult to write strong ai players . an example of such a video game is ms . pac-man . in this paper we will outline some of the previous techniques used to build ai controllers for ms. pac-man as well as presenting a new and novel solution . my technique utilises a cellular automaton ( ca ) to build a representation of the environment at each time step of the game . the basis of the representation is a 2-d grid of cells . each cell has a state and a set of rules which determine whether or not that cell will dominate ( i.e . pass its state value onto ) adjacent cells at each update . once a certain number of update iterations have been completed , the ca represents the state of the environment in the game , the goals and the dangers . at this point , ms. pac-man will decide her next move based only on her adjacent cells , that is to say , she has no knowledge of the state of the environment as a whole , she will simply follow the strongest path . this technique shows promise and allows the controller to achieve high scores in a live game , the behaviour it exhibits is interesting and complex . moreover , this behaviour is produced by using very simple rules which are applied many times to each cell in the grid . simple local interactions with complex global results are truly achieved .", "topics": ["interaction", "iteration"]}
{"title": "parallels of human language in the behavior of bottlenose dolphins", "abstract": "a short review of similarities between dolphins and humans with the help of quantitative linguistics and information theory .", "topics": ["parsing"]}
{"title": "lifeworld analysis", "abstract": "we argue that the analysis of agent/environment interactions should be extended to include the conventions and invariants maintained by agents throughout their activity . we refer to this thicker notion of environment as a lifeworld and present a partial set of formal tools for describing structures of lifeworlds and the ways in which they computationally simplify activity . as one specific example , we apply the tools to the analysis of the toast system and show how versions of the system with very different control structures in fact implement a common control structure together with different conventions for encoding task state in the positions or states of objects in the environment .", "topics": ["interaction"]}
{"title": "comparing multi-target trackers on different force unit levels", "abstract": "consider the problem of tracking a set of moving targets . apart from the tracking result , it is often important to know where the tracking fails , either to steer sensors to that part of the state-space , or to inform a human operator about the status and quality of the obtained information . an intuitive quality measure is the correlation between two tracking results based on uncorrelated observations . in the case of bayesian trackers such a correlation measure could be the kullback-leibler difference . we focus on a scenario with a large number of military units moving in some terrain . the units are observed by several types of sensors and `` meta-sensors '' with force aggregation capabilities . the sensors register units of different size . two separate multi-target probability hypothesis density ( phd ) particle filters are used to track some type of units ( e.g . , companies ) and their sub-units ( e.g . , platoons ) , respectively , based on observations of units of those sizes . each observation is used in one filter only . although the state-space may well be the same in both filters , the posterior phd distributions are not directly comparable -- one unit might correspond to three or four spatially distributed sub-units . therefore , we introduce a mapping function between distributions for different unit size , based on doctrine knowledge of unit configuration . the mapped distributions can now be compared -- locally or globally -- using some measure , which gives the correlation between two phd distributions in a bounded volume of the state-space . to locate areas where the tracking fails , a discretized quality map of the state-space can be generated by applying the measure locally to different parts of the space .", "topics": ["sensor"]}
{"title": "on the use of lee 's protocol for speckle-reducing techniques", "abstract": "this paper presents two new map ( maximum a posteriori ) filters for speckle noise reduction and a monte carlo procedure for the assessment of their performance . in order to quantitatively evaluate the results obtained using these new filters , with respect to classical ones , a monte carlo extension of lee 's protocol is proposed . this extension of the protocol shows that its original version leads to inconsistencies that hamper its use as a general procedure for filter assessment . some solutions for these inconsistencies are proposed , and a consistent comparison of speckle-reducing filters is provided .", "topics": ["image processing", "noise reduction"]}
{"title": "locally orderless registration", "abstract": "image registration is an important tool for medical image analysis and is used to bring images into the same reference frame by warping the coordinate field of one image , such that some similarity measure is minimized . we study similarity in image registration in the context of locally orderless images ( loi ) , which is the natural way to study density estimates and reveals the 3 fundamental scales : the measurement scale , the intensity scale , and the integration scale . this paper has three main contributions : firstly , we rephrase a large set of popular similarity measures into a common framework , which we refer to as locally orderless registration , and which makes full use of the features of local histograms . secondly , we extend the theoretical understanding of the local histograms . thirdly , we use our framework to compare two state-of-the-art intensity density estimators for image registration : the parzen window ( pw ) and the generalized partial volume ( gpv ) , and we demonstrate their differences on a popular similarity measure , normalized mutual information ( nmi ) . we conclude , that complicated similarity measures such as nmi may be evaluated almost as fast as simple measures such as sum of squared distances ( ssd ) regardless of the choice of pw and gpv . also , gpv is an asymmetric measure , and pw is our preferred choice .", "topics": ["gradient", "gradient"]}
{"title": "autogenic training with natural language processing modules : a recent tool for certain neuro cognitive studies", "abstract": "learning to respond to voice-text input involves the subject 's ability in understanding the phonetic and text based contents and his/her ability to communicate based on his/her experience . the neuro-cognitive facility of the subject has to support two important domains in order to make the learning process complete . in many cases , though the understanding is complete , the response is partial . this is one valid reason why we need to support the information from the subject with scalable techniques such as natural language processing ( nlp ) for abstraction of the contents from the output . this paper explores the feasibility of using nlp modules interlaced with neural networks to perform the required task in autogenic training related to medical applications .", "topics": ["natural language processing", "neural networks"]}
{"title": "hybrid cnn and dictionary-based models for scene recognition and domain adaptation", "abstract": "convolutional neural network ( cnn ) has achieved state-of-the-art performance in many different visual tasks . learned from a large-scale training dataset , cnn features are much more discriminative and accurate than the hand-crafted features . moreover , cnn features are also transferable among different domains . on the other hand , traditional dictionarybased features ( such as bow and spm ) contain much more local discriminative and structural information , which is implicitly embedded in the images . to further improve the performance , in this paper , we propose to combine cnn with dictionarybased models for scene recognition and visual domain adaptation . specifically , based on the well-tuned cnn models ( e.g . , alexnet and vgg net ) , two dictionary-based representations are further constructed , namely mid-level local representation ( mlr ) and convolutional fisher vector representation ( cfv ) . in mlr , an efficient two-stage clustering method , i.e . , weighted spatial and feature space spectral clustering on the parts of a single image followed by clustering all representative parts of all images , is used to generate a class-mixture or a classspecific part dictionary . after that , the part dictionary is used to operate with the multi-scale image inputs for generating midlevel representation . in cfv , a multi-scale and scale-proportional gmm training strategy is utilized to generate fisher vectors based on the last convolutional layer of cnn . by integrating the complementary information of mlr , cfv and the cnn features of the fully connected layer , the state-of-the-art performance can be achieved on scene recognition and domain adaptation problems . an interested finding is that our proposed hybrid representation ( from vgg net trained on imagenet ) is also complementary with googlenet and/or vgg-11 ( trained on place205 ) greatly .", "topics": ["test set", "feature vector"]}
{"title": "learning features by watching objects move", "abstract": "this paper presents a novel yet intuitive approach to unsupervised feature learning . inspired by the human visual system , we explore whether low-level motion-based grouping cues can be used to learn an effective visual representation . specifically , we use unsupervised motion-based segmentation on videos to obtain segments , which we use as 'pseudo ground truth ' to train a convolutional network to segment objects from a single frame . given the extensive evidence that motion plays a key role in the development of the human visual system , we hope that this straightforward approach to unsupervised learning will be more effective than cleverly designed 'pretext ' tasks studied in the literature . indeed , our extensive experiments show that this is the case . when used for transfer learning on object detection , our representation significantly outperforms previous unsupervised approaches across multiple settings , especially when training data for the target task is scarce .", "topics": ["feature learning", "test set"]}
{"title": "towards social pattern characterization in egocentric photo-streams", "abstract": "following the increasingly popular trend of social interaction analysis in egocentric vision , this manuscript presents a comprehensive study for automatic social pattern characterization of a wearable photo-camera user , by relying on the visual analysis of egocentric photo-streams . the proposed framework consists of three major steps . the first step is to detect social interactions of the user where the impact of several social signals on the task is explored . the detected social events are inspected in the second step for categorization into different social meetings . these two steps act at event-level where each potential social event is modeled as a multi-dimensional time-series , whose dimensions correspond to a set of relevant features for each task , and lstm is employed to classify the time-series . the last step of the framework is to characterize social patterns , which is essentially to infer the diversity and frequency of the social relations of the user through discovery of recurrences of the same people across the whole set of social events of the user . experimental evaluation over a dataset acquired by 9 users demonstrates promising results on the task of social pattern characterization from egocentric photo-streams .", "topics": ["time series", "interaction"]}
{"title": "dissecting adam : the sign , magnitude and variance of stochastic gradients", "abstract": "the adam optimizer is exceedingly popular in the deep learning community . often it works very well , sometimes it does n't . why ? we interpret adam as a combination of two aspects : for each weight , the update direction is determined by the sign of stochastic gradients , whereas the update magnitude is determined by an estimate of their relative variance . we disentangle these two aspects and analyze them in isolation , gaining insight into the mechanisms underlying adam . this analysis also extends recent results on adverse effects of adam on generalization , isolating the sign aspect as the problematic one . transferring the variance adaptation to sgd gives rise to a novel method , completing the practitioner 's toolbox for problems where adam fails .", "topics": ["approximation algorithm", "gradient"]}
{"title": "joint blind motion deblurring and depth estimation of light field", "abstract": "removing camera motion blur from a single light field is a challenging task since it is highly ill-posed inverse problem . the problem becomes even worse when blur kernel varies spatially due to scene depth variation and high-order camera motion . in this paper , we propose a novel algorithm to estimate all blur model variables jointly , including latent sub-aperture image , camera motion , and scene depth from the blurred 4d light field . exploiting multi-view nature of a light field relieves the inverse property of the optimization by utilizing strong depth cues and multi-view blur observation . the proposed joint estimation achieves high quality light field deblurring and depth estimation simultaneously under arbitrary 6-dof camera motion and unconstrained scene depth . intensive experiment on real and synthetic blurred light field confirms that the proposed algorithm outperforms the state-of-the-art light field deblurring and depth estimation methods .", "topics": ["synthetic data"]}
{"title": "philosophy in the face of artificial intelligence", "abstract": "in this article , i discuss how the ai community views concerns about the emergence of superintelligent ai and related philosophical issues .", "topics": ["artificial intelligence", "autonomous car"]}
{"title": "annealed generative adversarial networks", "abstract": "we introduce a novel framework for adversarial training where the target distribution is annealed between the uniform distribution and the data distribution . we posited a conjecture that learning under continuous annealing in the nonparametric regime is stable irrespective of the divergence measures in the objective function and proposed an algorithm , dubbed { \\ss } -gan , in corollary . in this framework , the fact that the initial support of the generative network is the whole ambient space combined with annealing are key to balancing the minimax game . in our experiments on synthetic data , mnist , and celeba , { \\ss } -gan with a fixed annealing schedule was stable and did not suffer from mode collapse .", "topics": ["loss function"]}
{"title": "greenhouse : a zero-positive machine learning system for time-series anomaly detection", "abstract": "this short paper describes our ongoing research on greenhouse - a zero-positive machine learning system for time-series anomaly detection .", "topics": ["time series"]}
{"title": "most correlated arms identification", "abstract": "we study the problem of finding the most mutually correlated arms among many arms . we show that adaptive arms sampling strategies can have significant advantages over the non-adaptive uniform sampling strategy . our proposed algorithms rely on a novel correlation estimator . the use of this accurate estimator allows us to get improved results for a wide range of problem instances .", "topics": ["sampling ( signal processing )"]}
{"title": "supervised hashing using graph cuts and boosted decision trees", "abstract": "embedding image features into a binary hamming space can improve both the speed and accuracy of large-scale query-by-example image retrieval systems . supervised hashing aims to map the original features to compact binary codes in a manner which preserves the label-based similarities of the original data . most existing approaches apply a single form of hash function , and an optimization process which is typically deeply coupled to this specific form . this tight coupling restricts the flexibility of those methods , and can result in complex optimization problems that are difficult to solve . in this work we proffer a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions . the proposed framework allows a number of existing approaches to hashing to be placed in context , and simplifies the development of new problem-specific hashing methods . our framework decomposes the into two steps : binary code ( hash bits ) learning , and hash function learning . the first step can typically be formulated as a binary quadratic problem , and the second step can be accomplished by training standard binary classifiers . for solving large-scale binary code inference , we show how to ensure that the binary quadratic problems are submodular such that an efficient graph cut approach can be used . to achieve efficiency as well as efficacy on large-scale high-dimensional data , we propose to use boosted decision trees as the hash functions , which are nonlinear , highly descriptive , and very fast to train and evaluate . experiments demonstrate that our proposed method significantly outperforms most state-of-the-art methods , especially on high-dimensional data .", "topics": ["nonlinear system", "loss function"]}
{"title": "online learning for classification of low-rank representation features and its applications in audio segment classification", "abstract": "in this paper , a novel framework based on trace norm minimization for audio segment is proposed . in this framework , both the feature extraction and classification are obtained by solving corresponding convex optimization problem with trace norm regularization . for feature extraction , robust principle component analysis ( robust pca ) via minimization a combination of the nuclear norm and the $ \\ell_1 $ -norm is used to extract low-rank features which are robust to white noise and gross corruption for audio segments . these low-rank features are fed to a linear classifier where the weight and bias are learned by solving similar trace norm constrained problems . for this classifier , most methods find the weight and bias in batch-mode learning , which makes them inefficient for large-scale problems . in this paper , we propose an online framework using accelerated proximal gradient method . this framework has a main advantage in memory cost . in addition , as a result of the regularization formulation of matrix classification , the lipschitz constant was given explicitly , and hence the step size estimation of general proximal gradient method was omitted in our approach . experiments on real data sets for laugh/non-laugh and applause/non-applause classification indicate that this novel framework is effective and noise robust .", "topics": ["optimization problem", "feature extraction"]}
{"title": "familia : an open-source toolkit for industrial topic modeling", "abstract": "familia is an open-source toolkit for pragmatic topic modeling in industry . familia abstracts the utilities of topic modeling in industry as two paradigms : semantic representation and semantic matching . efficient implementations of the two paradigms are made publicly available for the first time . furthermore , we provide off-the-shelf topic models trained on large-scale industrial corpora , including latent dirichlet allocation ( lda ) , sentencelda and topical word embedding ( twe ) . we further describe typical applications which are successfully powered by topic modeling , in order to ease the confusions and difficulties of software engineers during topic model selection and utilization .", "topics": ["text corpus"]}
{"title": "character-level neural translation for multilingual media monitoring in the summa project", "abstract": "the paper steps outside the comfort-zone of the traditional nlp tasks like automatic speech recognition ( asr ) and machine translation ( mt ) to addresses two novel problems arising in the automated multilingual news monitoring : segmentation of the tv and radio program asr transcripts into individual stories , and clustering of the individual stories coming from various sources and languages into storylines . storyline clustering of stories covering the same events is an essential task for inquisitorial media monitoring . we address these two problems jointly by engaging the low-dimensional semantic representation capabilities of the sequence to sequence neural translation models . to enable joint multi-task learning for multilingual neural translation of morphologically rich languages we replace the attention mechanism with the sliding-window mechanism and operate the sequence to sequence neural translation model on the character-level rather than on the word-level . the story segmentation and storyline clustering problem is tackled by examining the low-dimensional vectors produced as a side-product of the neural translation process . the results of this paper describe a novel approach to the automatic story segmentation and storyline clustering problem .", "topics": ["natural language processing", "cluster analysis"]}
{"title": "2d-3d fully convolutional neural networks for cardiac mr segmentation", "abstract": "in this paper , we develop a 2d and 3d segmentation pipelines for fully automated cardiac mr image segmentation using deep convolutional neural networks ( cnn ) . our models are trained end-to-end from scratch using the acd challenge 2017 dataset comprising of 100 studies , each containing cardiac mr images in end diastole and end systole phase . we show that both our segmentation models achieve near state-of-the-art performance scores in terms of distance metrics and have convincing accuracy in terms of clinical parameters . a comparative analysis is provided by introducing a novel dice loss function and its combination with cross entropy loss . by exploring different network structures and comprehensive experiments , we discuss several key insights to obtain optimal model performance , which also is central to the theme of this challenge .", "topics": ["image segmentation", "loss function"]}
{"title": "sceneflowfields : dense interpolation of sparse scene flow correspondences", "abstract": "while most scene flow methods use either variational optimization or a strong rigid motion assumption , we show for the first time that scene flow can also be estimated by dense interpolation of sparse matches . to this end , we find sparse matches across two stereo image pairs that are detected without any prior regularization and perform dense interpolation preserving geometric and motion boundaries by using edge information . a few iterations of variational energy minimization are performed to refine our results , which are thoroughly evaluated on the kitti benchmark and additionally compared to state-of-the-art on mpi sintel . for application in an automotive context , we further show that an optional ego-motion model helps to boost performance and blends smoothly into our approach to produce a segmentation of the scene into static and dynamic parts .", "topics": ["calculus of variations", "matrix regularization"]}
{"title": "mcgan : mean and covariance feature matching gan", "abstract": "we introduce new families of integral probability metrics ( ipm ) for training generative adversarial networks ( gan ) . our ipms are based on matching statistics of distributions embedded in a finite dimensional feature space . mean and covariance feature matching ipms allow for stable training of gans , which we will call mcgan . mcgan minimizes a meaningful loss between distributions .", "topics": ["feature vector"]}
{"title": "deep learning with lung segmentation and bone shadow exclusion techniques for chest x-ray analysis of lung cancer", "abstract": "the recent progress of computing , machine learning , and especially deep learning , for image recognition brings a meaningful effect for automatic detection of various diseases from chest x-ray images ( cxrs ) . here efficiency of lung segmentation and bone shadow exclusion techniques is demonstrated for analysis of 2d cxrs by deep learning approach to help radiologists identify suspicious lesions and nodules in lung cancer patients . training and validation was performed on the original jsrt dataset ( dataset # 01 ) , bse-jsrt dataset , i.e . the same jsrt dataset , but without clavicle and rib shadows ( dataset # 02 ) , original jsrt dataset after segmentation ( dataset # 03 ) , and bse-jsrt dataset after segmentation ( dataset # 04 ) . the results demonstrate the high efficiency and usefulness of the considered pre-processing techniques in the simplified configuration even . the pre-processed dataset without bones ( dataset # 02 ) demonstrates the much better accuracy and loss results in comparison to the other pre-processed datasets after lung segmentation ( datasets # 02 and # 03 ) .", "topics": ["computer vision"]}
{"title": "zero-shot learning through cross-modal transfer", "abstract": "this work introduces a model that can recognize objects in images even if no training data is available for the objects . the only necessary knowledge about the unseen categories comes from unsupervised large text corpora . in our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like . most previous zero-shot learning models can only differentiate between unseen classes . in contrast , our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes . this is achieved by first using outlier detection in the semantic space and then two separate recognition models . furthermore , our model does not require any manually defined semantic features for either words or images .", "topics": ["test set", "text corpus"]}
{"title": "a geometric approach to mapping bitext correspondence", "abstract": "the first step in most corpus-based multilingual nlp work is to construct a detailed map of the correspondence between a text and its translation . several automatic methods for this task have been proposed in recent years . yet even the best of these methods can err by several typeset pages . the smooth injective map recognizer ( simr ) is a new bitext mapping algorithm . simr 's errors are smaller than those of the previous front-runner by more than a factor of 4 . its robustness has enabled new commercial-quality applications . the greedy nature of the algorithm makes it independent of memory resources . unlike other bitext mapping algorithms , simr allows crossing correspondences to account for word order differences . its output can be converted quickly and easily into a sentence alignment . simr 's output has been used to align over 200 megabytes of the canadian hansards for publication by the linguistic data consortium .", "topics": ["test set", "natural language processing"]}
{"title": "analytical shape determination of fiber-like objects with virtual image correlation", "abstract": "this paper reports a method allowing for the determination of the shape of deformed fiber-like objects . compared to existing methods , it provides analytical results including the local slope and curvature which are of first importance , for instance , in beam mechanics . the presented vic ( virtual image correlation ) method consists in looking for the best correlation between the image of the fiber-like object and a virtual beam image , using an algorithm close to the digital image correlation method developed in experimental solid mechanics . the computation only involves the part of the image in the vicinity of the fiber : the method is thus insensitive to the picture background and the computational cost remains low . two examples are reported : the first proves the precision of the method , the second its ability to identify a complex shape with multiple loops .", "topics": ["computation"]}
{"title": "adequacy of the gradient-descent method for classifier evasion attacks", "abstract": "despite the wide use of machine learning in adversarial settings including computer security , recent studies have demonstrated vulnerabilities to evasion attacks -- -carefully crafted adversarial samples that closely resemble legitimate instances , but cause misclassification . in this paper , we examine the adequacy of the leading approach to generating adversarial samples -- -the gradient descent approach . in particular ( 1 ) we perform extensive experiments on three datasets , mnist , usps and spambase , in order to analyse the effectiveness of the gradient-descent method against non-linear support vector machines , and conclude that carefully reduced kernel smoothness can significantly increase robustness to the attack ; ( 2 ) we demonstrate that separated inter-class support vectors lead to more secure models , and propose a quantity similar to margin that can efficiently predict potential susceptibility to gradient-descent attacks , before the attack is launched ; and ( 3 ) we design a new adversarial sample construction algorithm based on optimising the multiplicative ratio of class decision functions .", "topics": ["support vector machine", "nonlinear system"]}
{"title": "snect : scalable network constrained tucker decomposition for integrative multi-platform data analysis", "abstract": "motivation : how do we integratively analyze large-scale multi-platform genomic data that are high dimensional and sparse ? furthermore , how can we incorporate prior knowledge , such as the association between genes , in the analysis systematically ? method : to solve this problem , we propose a scalable network constrained tucker decomposition method we call snect . snect adopts parallel stochastic gradient descent approach on the proposed parallelizable network constrained optimization function . snect decomposition is applied to tensor constructed from large scale multi-platform multi-cohort cancer data , pancan12 , constrained on a network built from pathwaycommons database . results : the decomposed factor matrices are applied to stratify cancers , to search for top-k similar patients , and to illustrate how the matrices can be used for personalized interpretation . in the stratification test , combined twelve-cohort data is clustered to form thirteen subclasses . the thirteen subclasses have a high correlation to tissue of origin in addition to other interesting observations , such as clear separation of ov cancers to two groups , and high clinical correlation within subclusters formed in cohorts brca and ucec . in the top-k search , a new patient 's genomic profile is generated and searched against existing patients based on the factor matrices . the similarity of the top-k patient to the query is high for 23 clinical features , including estrogen/progesterone receptor statuses of brca patients with average precision value ranges from 0.72 to 0.86 and from 0.68 to 0.86 , respectively . we also provide an illustration of how the factor matrices can be used for interpretable personalized analysis of each patient .", "topics": ["sparse matrix"]}
{"title": "a deep belief network based machine learning system for risky host detection", "abstract": "to assure cyber security of an enterprise , typically siem ( security information and event management ) system is in place to normalize security event from different preventive technologies and flag alerts . analysts in the security operation center ( soc ) investigate the alerts to decide if it is truly malicious or not . however , generally the number of alerts is overwhelming with majority of them being false positive and exceeding the soc 's capacity to handle all alerts . there is a great need to reduce the false positive rate as much as possible . while most previous research focused on network intrusion detection , we focus on risk detection and propose an intelligent deep belief network machine learning system . the system leverages alert information , various security logs and analysts ' investigation results in a real enterprise environment to flag hosts that have high likelihood of being compromised . text mining and graph based method are used to generate targets and create features for machine learning . in the experiment , deep belief network is compared with other machine learning algorithms , including multi-layer neural network , random forest , support vector machine and logistic regression . results on real enterprise data indicate that the deep belief network machine learning system performs better than other algorithms for our problem and is six times more effective than current rule-based system . we also implement the whole system from data collection , label creation , feature engineering to host score generation in a real enterprise production environment .", "topics": ["bayesian network"]}
{"title": "towards imperceptible and robust adversarial example attacks against neural networks", "abstract": "machine learning systems based on deep neural networks , being able to produce state-of-the-art results on various perception tasks , have gained mainstream adoption in many applications . however , they are shown to be vulnerable to adversarial example attack , which generates malicious output by adding slight perturbations to the input . previous adversarial example crafting methods , however , use simple metrics to evaluate the distances between the original examples and the adversarial ones , which could be easily detected by human eyes . in addition , these attacks are often not robust due to the inevitable noises and deviation in the physical world . in this work , we present a new adversarial example attack crafting method , which takes the human perceptual system into consideration and maximizes the noise tolerance of the crafted adversarial example . experimental results demonstrate the efficacy of the proposed technique .", "topics": ["neural networks"]}
{"title": "human activity learning and segmentation using partially hidden discriminative models", "abstract": "learning and understanding the typical patterns in the daily activities and routines of people from low-level sensory data is an important problem in many application domains such as building smart environments , or providing intelligent assistance . traditional approaches to this problem typically rely on supervised learning and generative models such as the hidden markov models and its extensions . while activity data can be readily acquired from pervasive sensors , e.g . in smart environments , providing manual labels to support supervised training is often extremely expensive . in this paper , we propose a new approach based on semi-supervised training of partially hidden discriminative models such as the conditional random field ( crf ) and the maximum entropy markov model ( memm ) . we show that these models allow us to incorporate both labeled and unlabeled data for learning , and at the same time , provide us with the flexibility and accuracy of the discriminative framework . our experimental results in the video surveillance domain illustrate that these models can perform better than their generative counterpart , the partially hidden markov model , even when a substantial amount of labels are unavailable .", "topics": ["supervised learning", "high- and low-level"]}
{"title": "automatic synonym discovery with knowledge bases", "abstract": "recognizing entity synonyms from text has become a crucial task in many entity-leveraging applications . however , discovering entity synonyms from domain-specific text corpora ( e.g . , news articles , scientific papers ) is rather challenging . current systems take an entity name string as input to find out other names that are synonymous , ignoring the fact that often times a name string can refer to multiple entities ( e.g . , `` apple '' could refer to both apple inc and the fruit apple ) . moreover , most existing methods require training data manually created by domain experts to construct supervised-learning systems . in this paper , we study the problem of automatic synonym discovery with knowledge bases , that is , identifying synonyms for knowledge base entities in a given domain-specific corpus . the manually-curated synonyms for each entity stored in a knowledge base not only form a set of name strings to disambiguate the meaning for each other , but also can serve as `` distant '' supervision to help determine important features for the task . we propose a novel framework , called dpe , to integrate two kinds of mutually-complementing signals for synonym discovery , i.e . , distributional features based on corpus-level statistics and textual patterns based on local contexts . in particular , dpe jointly optimizes the two kinds of signals in conjunction with distant supervision , so that they can mutually enhance each other in the training stage . at the inference stage , both signals will be utilized to discover synonyms for the given entities . experimental results prove the effectiveness of the proposed framework .", "topics": ["test set", "supervised learning"]}
{"title": "a simple practical accelerated method for finite sums", "abstract": "we describe a novel optimization method for finite sums ( such as empirical risk minimization problems ) building on the recently introduced saga method . our method achieves an accelerated convergence rate on strongly convex smooth problems . our method has only one parameter ( a step size ) , and is radically simpler than other accelerated methods for finite sums . additionally it can be applied when the terms are non-smooth , yielding a method applicable in many areas where operator splitting methods would traditionally be applied .", "topics": ["gradient descent"]}
{"title": "learning to order facts for discourse planning in natural language generation", "abstract": "this paper presents a machine learning approach to discourse planning in natural language generation . more specifically , we address the problem of learning the most natural ordering of facts in discourse plans for a specific domain . we discuss our methodology and how it was instantiated using two different machine learning algorithms . a quantitative evaluation performed in the domain of museum exhibit descriptions indicates that our approach performs significantly better than manually constructed ordering rules . being retrainable , the resulting planners can be ported easily to other similar domains , without requiring language technology expertise .", "topics": ["natural language"]}
{"title": "image semantic transformation : faster , lighter and stronger", "abstract": "we propose image-semantic-transformation-reconstruction-circle ( istrc ) model , a novel and powerful method using facenet 's euclidean latent space to understand the images . as the name suggests , istrc construct the circle , able to perfectly reconstruct images . one powerful euclidean latent space embedded in istrc is facenet 's last layer with the power of distinguishing and understanding images . our model will reconstruct the images and manipulate euclidean latent vectors to achieve semantic transformations and semantic images arthimetic calculations . in this paper , we show that istrc performs 10 high-level semantic transformations like `` male and female '' , '' add smile '' , '' open mouth '' , `` deduct beard or add mustache '' , `` bigger/smaller nose '' , `` make older and younger '' , `` bigger lips '' , `` bigger eyes '' , `` bigger/smaller mouths '' and `` more attractive '' . it just takes 3 hours ( gtx 1080 ) to train the models of 10 semantic transformations .", "topics": ["high- and low-level"]}
{"title": "equivariance through parameter-sharing", "abstract": "we propose to study equivariance in deep neural networks through parameter symmetries . in particular , given a group $ \\mathcal { g } $ that acts discretely on the input and output of a standard neural network layer $ \\phi_ { w } : \\re^ { m } \\to \\re^ { n } $ , we show that $ \\phi_ { w } $ is equivariant with respect to $ \\mathcal { g } $ -action iff $ \\mathcal { g } $ explains the symmetries of the network parameters $ w $ . inspired by this observation , we then propose two parameter-sharing schemes to induce the desirable symmetry on $ w $ . our procedures for tying the parameters achieve $ \\mathcal { g } $ -equivariance and , under some conditions on the action of $ \\mathcal { g } $ , they guarantee sensitivity to all other permutation groups outside $ \\mathcal { g } $ .", "topics": ["test set", "graphical model"]}
{"title": "brain architecture : a design for natural computation", "abstract": "fifty years ago , john von neumann compared the architecture of the brain with that of computers that he invented and which is still in use today . in those days , the organisation of computers was based on concepts of brain organisation . here , we give an update on current results on the global organisation of neural systems . for neural systems , we outline how the spatial and topological architecture of neuronal and cortical networks facilitates robustness against failures , fast processing , and balanced network activation . finally , we discuss mechanisms of self-organization for such architectures . after all , the organization of the brain might again inspire computer architecture .", "topics": ["computation"]}
{"title": "gans for biological image synthesis", "abstract": "in this paper , we propose a novel application of generative adversarial networks ( gan ) to the synthesis of cells imaged by fluorescence microscopy . compared to natural images , cells tend to have a simpler and more geometric global structure that facilitates image generation . however , the correlation between the spatial pattern of different fluorescent proteins reflects important biological functions , and synthesized images have to capture these relationships to be relevant for biological applications . we adapt gans to the task at hand and propose new models with casual dependencies between image channels that can generate multi-channel images , which would be impossible to obtain experimentally . we evaluate our approach using two independent techniques and compare it against sensible baselines . finally , we demonstrate that by interpolating across the latent space we can mimic the known changes in protein localization that occur through time during the cell cycle , allowing us to predict temporal evolution from static images .", "topics": ["baseline ( configuration management )"]}
{"title": "optimizing gotools ' search heuristics using genetic algorithms", "abstract": "gotools is a program which solves life & death problems in the game of go . this paper describes experiments using a genetic algorithm to optimize heuristic weights used by gotools ' tree-search . the complete set of heuristic weights is composed of different subgroups , each of which can be optimized with a suitable fitness function . as a useful side product , an mpi interface for freepascal was implemented to allow the use of a parallelized fitness function running on a beowulf cluster . the aim of this exercise is to optimize the current version of gotools , and to make tools available in preparation of an extension of gotools for solving open boundary life & death problems , which will introduce more heuristic parameters to be fine tuned .", "topics": ["heuristic"]}
{"title": "privlogit : efficient privacy-preserving logistic regression by tailoring numerical optimizers", "abstract": "safeguarding privacy in machine learning is highly desirable , especially in collaborative studies across many organizations . privacy-preserving distributed machine learning ( based on cryptography ) is popular to solve the problem . however , existing cryptographic protocols still incur excess computational overhead . here , we make a novel observation that this is partially due to naive adoption of mainstream numerical optimization ( e.g . , newton method ) and failing to tailor for secure computing . this work presents a contrasting perspective : customizing numerical optimization specifically for secure settings . we propose a seemingly less-favorable optimization method that can in fact significantly accelerate privacy-preserving logistic regression . leveraging this new method , we propose two new secure protocols for conducting logistic regression in a privacy-preserving and distributed manner . extensive theoretical and empirical evaluations prove the competitive performance of our two secure proposals while without compromising accuracy or privacy : with speedup up to 2.3x and 8.1x , respectively , over state-of-the-art ; and even faster as data scales up . such drastic speedup is on top of and in addition to performance improvements from existing ( and future ) state-of-the-art cryptography . our work provides a new way towards efficient and practical privacy-preserving logistic regression for large-scale studies which are common for modern science .", "topics": ["mathematical optimization", "numerical analysis"]}
{"title": "segmentation of breast regions in mammogram based on density : a review", "abstract": "the focus of this paper is to review approaches for segmentation of breast regions in mammograms according to breast density . studies based on density have been undertaken because of the relationship between breast cancer and density . breast cancer usually occurs in the fibroglandular area of breast tissue , which appears bright on mammograms and is described as breast density . most of the studies are focused on the classification methods for glandular tissue detection . others highlighted on the segmentation methods for fibroglandular tissue , while few researchers performed segmentation of the breast anatomical regions based on density . there have also been works on the segmentation of other specific parts of breast regions such as either detection of nipple position , skin-air interface or pectoral muscles . the problems on the evaluation performance of the segmentation results in relation to ground truth are also discussed in this paper .", "topics": ["ground truth"]}
{"title": "graph-sparse logistic regression", "abstract": "we introduce graph-sparse logistic regression , a new algorithm for classification for the case in which the support should be sparse but connected on a graph . we val- idate this algorithm against synthetic data and benchmark it against l1-regularized logistic regression . we then explore our technique in the bioinformatics context of proteomics data on the interactome graph . we make all our experimental code public and provide gslr as an open source package .", "topics": ["synthetic data", "sparse matrix"]}
{"title": "stream clipper : scalable submodular maximization on stream", "abstract": "we propose a streaming submodular maximization algorithm `` stream clipper '' that performs as well as the offline greedy algorithm on document/video summarization in practice . it adds elements from a stream either to a solution set $ s $ or to an extra buffer $ b $ based on two adaptive thresholds , and improves $ s $ by a final greedy step that starts from $ s $ adding elements from $ b $ . during this process , swapping elements out of $ s $ can occur if doing so yields improvements . the thresholds adapt based on if current memory utilization exceeds a budget , e.g . , it increases the lower threshold , and removes from the buffer $ b $ elements below the new lower threshold . we show that , while our approximation factor in the worst case is $ 1/2 $ ( like in previous work , and corresponding to the tight bound ) , we show that there are data-dependent conditions where our bound falls within the range $ [ 1/2 , 1-1/e ] $ . in news and video summarization experiments , the algorithm consistently outperforms other streaming methods , and , while using significantly less computation and memory , performs similarly to the offline greedy algorithm .", "topics": ["approximation", "computation"]}
{"title": "language identification with confidence limits", "abstract": "a statistical classification algorithm and its application to language identification from noisy input are described . the main innovation is to compute confidence limits on the classification , so that the algorithm terminates when enough evidence to make a clear decision has been made , and so avoiding problems with categories that have similar characteristics . a second application , to genre identification , is briefly examined . the results show that some of the problems of other language identification techniques can be avoided , and illustrate a more important point : that a statistical language process can be used to provide feedback about its own success rate .", "topics": ["statistical classification"]}
{"title": "the reversible residual network : backpropagation without storing activations", "abstract": "deep residual networks ( resnets ) have significantly pushed forward the state-of-the-art on image classification , increasing in performance as networks grow both deeper and wider . however , memory consumption becomes a bottleneck , as one needs to store the activations in order to calculate gradients using backpropagation . we present the reversible residual network ( revnet ) , a variant of resnets where each layer 's activations can be reconstructed exactly from the next layer 's . therefore , the activations for most layers need not be stored in memory during backpropagation . we demonstrate the effectiveness of revnets on cifar-10 , cifar-100 , and imagenet , establishing nearly identical classification accuracy to equally-sized resnets , even though the activation storage requirements are independent of depth .", "topics": ["computer vision"]}
{"title": "proceedings of the third conference on uncertainty in artificial intelligence ( 1987 )", "abstract": "this is the proceedings of the third conference on uncertainty in artificial intelligence , which was held in seattle , wa , july 10-12 , 1987", "topics": ["artificial intelligence"]}
{"title": "causality on longitudinal data : stable specification search in constrained structural equation modeling", "abstract": "a typical problem in causal modeling is the instability of model structure learning , i.e . , small changes in finite data can result in completely different optimal models . the present work introduces a novel causal modeling algorithm for longitudinal data , that is robust for finite samples based on recent advances in stability selection using subsampling and selection algorithms . our approach uses exploratory search but allows incorporation of prior knowledge , e.g . , the absence of a particular causal relationship between two specific variables . we represent causal relationships using structural equation models . models are scored along two objectives : the model fit and the model complexity . since both objectives are often conflicting we apply a multi-objective evolutionary algorithm to search for pareto optimal models . to handle the instability of small finite data samples , we repeatedly subsample the data and select those substructures ( from the optimal models ) that are both stable and parsimonious . these substructures can be visualized through a causal graph . our more exploratory approach achieves at least comparable performance as , but often a significant improvement over state-of-the-art alternative approaches on a simulated data set with a known ground truth . we also present the results of our method on three real-world longitudinal data sets on chronic fatigue syndrome , alzheimer disease , and chronic kidney disease . the findings obtained with our approach are generally in line with results from more hypothesis-driven analyses in earlier studies and suggest some novel relationships that deserve further research .", "topics": ["simulation", "ground truth"]}
{"title": "proximal alternating direction network : a globally converged deep unrolling framework", "abstract": "deep learning models have gained great success in many real-world applications . however , most existing networks are typically designed in heuristic manners , thus lack of rigorous mathematical principles and derivations . several recent studies build deep structures by unrolling a particular optimization model that involves task information . unfortunately , due to the dynamic nature of network parameters , their resultant deep propagation networks do \\emph { not } possess the nice convergence property as the original optimization scheme does . this paper provides a novel proximal unrolling framework to establish deep models by integrating experimentally verified network architectures and rich cues of the tasks . more importantly , we \\emph { prove in theory } that 1 ) the propagation generated by our unrolled deep model globally converges to a critical-point of a given variational energy , and 2 ) the proposed framework is still able to learn priors from training data to generate a convergent propagation even when task information is only partially available . indeed , these theoretical results are the best we can ask for , unless stronger assumptions are enforced . extensive experiments on various real-world applications verify the theoretical convergence and demonstrate the effectiveness of designed deep models .", "topics": ["test set", "calculus of variations"]}
{"title": "spatiotemporal networks for video emotion recognition", "abstract": "our experiment adapts several popular deep learning methods as well as some traditional methods on the problem of video emotion recognition . in our experiment , we use the cnn-lstm architecture for visual information extraction and classification and utilize traditional methods such as for audio feature classification . for multimodal fusion , we use the traditional support vector machine . our experiment yields a good result on the afew 6.0 dataset .", "topics": ["time series"]}
{"title": "speedread : a fast named entity recognition pipeline", "abstract": "online content analysis employs algorithmic methods to identify entities in unstructured text . both machine learning and knowledge-base approaches lie at the foundation of contemporary named entities extraction systems . however , the progress in deploying these approaches on web-scale has been been hampered by the computational cost of nlp over massive text corpora . we present speedread ( sr ) , a named entity recognition pipeline that runs at least 10 times faster than stanford nlp pipeline . this pipeline consists of a high performance penn treebank- compliant tokenizer , close to state-of-art part-of-speech ( pos ) tagger and knowledge-based named entity recognizer .", "topics": ["natural language processing", "entity"]}
{"title": "an integrated system for 3d gaze recovery and semantic analysis of human attention", "abstract": "this work describes a computer vision system that enables pervasive mapping and monitoring of human attention . the key contribution is that our methodology enables full 3d recovery of the gaze pointer , human view frustum and associated human centered measurements directly into an automatically computed 3d model in real-time . we apply rgb-d slam and descriptor matching methodologies for the 3d modeling , localization and fully automated annotation of rois ( regions of interest ) within the acquired 3d model . this innovative methodology will open new avenues for attention studies in real world environments , bringing new potential into automated processing for human factors technologies .", "topics": ["computer vision"]}
{"title": "neural network learning of optimal kalman prediction and control", "abstract": "although there are many neural network ( nn ) algorithms for prediction and for control , and although methods for optimal estimation ( including filtering and prediction ) and for optimal control in linear systems were provided by kalman in 1960 ( with nonlinear extensions since then ) , there has been , to my knowledge , no nn algorithm that learns either kalman prediction or kalman control ( apart from the special case of stationary control ) . here we show how optimal kalman prediction and control ( kpc ) , as well as system identification , can be learned and executed by a recurrent neural network composed of linear-response nodes , using as input only a stream of noisy measurement data . the requirements of kpc appear to impose significant constraints on the allowed nn circuitry and signal flows . the nn architecture implied by these constraints bears certain resemblances to the local-circuit architecture of mammalian cerebral cortex . we discuss these resemblances , as well as caveats that limit our current ability to draw inferences for biological function . it has been suggested that the local cortical circuit ( lcc ) architecture may perform core functions ( as yet unknown ) that underlie sensory , motor , and other cortical processing . it is reasonable to conjecture that such functions may include prediction , the estimation or inference of missing or noisy sensory data , and the goal-driven generation of control signals . the resemblances found between the kpc nn architecture and that of the lcc are consistent with this conjecture .", "topics": ["recurrent neural network", "nonlinear system"]}
{"title": "dense associative memory for pattern recognition", "abstract": "a model of associative memory is studied , which stores and reliably retrieves many more patterns than the number of neurons in the network . we propose a simple duality between this dense associative memory and neural networks commonly used in deep learning . on the associative memory side of this duality , a family of models that smoothly interpolates between two limiting cases can be constructed . one limit is referred to as the feature-matching mode of pattern recognition , and the other one as the prototype regime . on the deep learning side of the duality , this family corresponds to feedforward neural networks with one hidden layer and various activation functions , which transmit the activities of the visible neurons to the hidden layer . this family of activation functions includes logistics , rectified linear units , and rectified polynomials of higher degrees . the proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used in deep learning . the utility of the dense memories is illustrated for two test cases : the logical gate xor and the recognition of handwritten digits from the mnist data set .", "topics": ["mnist database", "polynomial"]}
{"title": "spectral methods meet em : a provably optimal algorithm for crowdsourcing", "abstract": "crowdsourcing is a popular paradigm for effectively collecting labels at low cost . the dawid-skene estimator has been widely used for inferring the true labels from the noisy labels provided by non-expert crowdsourcing workers . however , since the estimator maximizes a non-convex log-likelihood function , it is hard to theoretically justify its performance . in this paper , we propose a two-stage efficient algorithm for multi-class crowd labeling problems . the first stage uses the spectral method to obtain an initial estimate of parameters . then the second stage refines the estimation by optimizing the objective function of the dawid-skene estimator via the em algorithm . we show that our algorithm achieves the optimal convergence rate up to a logarithmic factor . we conduct extensive experiments on synthetic and real datasets . experimental results demonstrate that the proposed algorithm is comparable to the most accurate empirical approach , while outperforming several other recently proposed methods .", "topics": ["synthetic data", "loss function"]}
{"title": "transformation of basic probability assignments to probabilities based on a new entropy measure", "abstract": "dempster-shafer evidence theory is an efficient mathematical tool to deal with uncertain information . in that theory , basic probability assignment ( bpa ) is the basic element for the expression and inference of uncertainty . decision-making based on bpa is still an open issue in dempster-shafer evidence theory . in this paper , a novel approach of transforming basic probability assignments to probabilities is proposed based on deng entropy which is a new measure for the uncertainty of bpa . the principle of the proposed method is to minimize the difference of uncertainties involving in the given bpa and obtained probability distribution . numerical examples are given to show the proposed approach .", "topics": ["numerical analysis"]}
{"title": "automated machine learning on big data using stochastic algorithm tuning", "abstract": "we introduce a means of automating machine learning ( ml ) for big data tasks , by performing scalable stochastic bayesian optimisation of ml algorithm parameters and hyper-parameters . more often than not , the critical tuning of ml algorithm parameters has relied on domain expertise from experts , along with laborious hand-tuning , brute search or lengthy sampling runs . against this background , bayesian optimisation is finding increasing use in automating parameter tuning , making ml algorithms accessible even to non-experts . however , the state of the art in bayesian optimisation is incapable of scaling to the large number of evaluations of algorithm performance required to fit realistic models to complex , big data . we here describe a stochastic , sparse , bayesian optimisation strategy to solve this problem , using many thousands of noisy evaluations of algorithm performance on subsets of data in order to effectively train algorithms for big data . we provide a comprehensive benchmarking of possible sparsification strategies for bayesian optimisation , concluding that a nystrom approximation offers the best scaling and performance for real tasks . our proposed algorithm demonstrates substantial improvement over the state of the art in tuning the parameters of a gaussian process time series prediction task on real , big data .", "topics": ["sampling ( signal processing )", "mathematical optimization"]}
{"title": "an incremental linear-time learning algorithm for the optimum-path forest classifier", "abstract": "we present a classification method with incremental capabilities based on the optimum-path forest classifier ( opf ) . the opf considers instances as nodes of a fully-connected training graph , arc weights represent distances between two feature vectors . our algorithm includes new instances in an opf in linear-time , while keeping similar accuracies when compared with the original quadratic-time model .", "topics": ["feature vector", "time complexity"]}
{"title": "hybrid dialog state tracker with asr features", "abstract": "this paper presents a hybrid dialog state tracker enhanced by trainable spoken language understanding ( slu ) for slot-filling dialog systems . our architecture is inspired by previously proposed neural-network-based belief-tracking systems . in addition , we extended some parts of our modular architecture with differentiable rules to allow end-to-end training . we hypothesize that these rules allow our tracker to generalize better than pure machine-learning based systems . for evaluation , we used the dialog state tracking challenge ( dstc ) 2 dataset - a popular belief tracking testbed with dialogs from restaurant information system . to our knowledge , our hybrid tracker sets a new state-of-the-art result in three out of four categories within the dstc2 .", "topics": ["speech recognition", "end-to-end principle"]}
{"title": "multiple kernel sparse representations for supervised and unsupervised learning", "abstract": "in complex visual recognition tasks it is typical to adopt multiple descriptors , that describe different aspects of the images , for obtaining an improved recognition performance . descriptors that have diverse forms can be fused into a unified feature space in a principled manner using kernel methods . sparse models that generalize well to the test data can be learned in the unified kernel space , and appropriate constraints can be incorporated for application in supervised and unsupervised learning . in this paper , we propose to perform sparse coding and dictionary learning in the multiple kernel space , where the weights of the ensemble kernel are tuned based on graph-embedding principles such that class discrimination is maximized . in our proposed algorithm , dictionaries are inferred using multiple levels of 1-d subspace clustering in the kernel space , and the sparse codes are obtained using a simple levelwise pursuit scheme . empirical results for object recognition and image clustering show that our algorithm outperforms existing sparse coding based approaches , and compares favorably to other state-of-the-art methods .", "topics": ["kernel ( operating system )", "supervised learning"]}
{"title": "generalized collective inference with symmetric clique potentials", "abstract": "collective graphical models exploit inter-instance associative dependence to output more accurate labelings . however existing models support very limited kind of associativity which restricts accuracy gains . this paper makes two major contributions . first , we propose a general collective inference framework that biases data instances to agree on a set of { \\em properties } of their labelings . agreement is encouraged through symmetric clique potentials . we show that rich properties leads to bigger gains , and present a systematic inference procedure for a large class of such properties . the procedure performs message passing on the cluster graph , where property-aware messages are computed with cluster specific algorithms . this provides an inference-only solution for domain adaptation . our experiments on bibliographic information extraction illustrate significant test error reduction over unseen domains . our second major contribution consists of algorithms for computing outgoing messages from clique clusters with symmetric clique potentials . our algorithms are exact for arbitrary symmetric potentials on binary labels and for max-like and majority-like potentials on multiple labels . for majority potentials , we also provide an efficient lagrangian relaxation based algorithm that compares favorably with the exact algorithm . we present a 13/15-approximation algorithm for the np-hard potts potential , with runtime sub-quadratic in the clique size . in contrast , the best known previous guarantee for graphs with potts potentials is only 1/2 . we empirically show that our method for potts potentials is an order of magnitude faster than the best alternatives , and our lagrangian relaxation based algorithm for majority potentials beats the best applicable heuristic -- icm .", "topics": ["graphical model", "approximation algorithm"]}
{"title": "neuroner : an easy-to-use program for named-entity recognition based on neural networks", "abstract": "named-entity recognition ( ner ) aims at identifying entities of interest in a text . artificial neural networks ( anns ) have recently been shown to outperform existing ner systems . however , anns remain challenging to use for non-expert users . in this paper , we present neuroner , an easy-to-use named-entity recognition tool based on anns . users can annotate entities using a graphical web-based user interface ( brat ) : the annotations are then used to train an ann , which in turn predict entities ' locations and categories in new texts . neuroner makes this annotation-training-prediction flow smooth and accessible to anyone .", "topics": ["entity"]}
{"title": "a dual sparse decomposition method for image denoising", "abstract": "this article addresses the image denoising problem in the situations of strong noise . we propose a dual sparse decomposition method . this method makes a sub-dictionary decomposition on the over-complete dictionary in the sparse decomposition . the sub-dictionary decomposition makes use of a novel criterion based on the occurrence frequency of atoms of the over-complete dictionary over the data set . the experimental results demonstrate that the dual-sparse-decomposition method surpasses state-of-art denoising performance in terms of both peak-signal-to-noise ratio and structural-similarity-index-metric , and also at subjective visual quality .", "topics": ["noise reduction", "sparse matrix"]}
{"title": "effective xml representation for spoken language in organisations", "abstract": "spoken language can be used to provide insights into organisational processes , unfortunately transcription and coding stages are very time consuming and expensive . the concept of partial transcription and coding is proposed in which spoken language is indexed prior to any subsequent processing . the functional linguistic theory of texture is used to describe the effects of partial transcription on observational records . the standard used to encode transcript context and metadata is called chat , but a previous xml schema developed to implement it contains design assumptions that make it difficult to support partial transcription for example . this paper describes a more effective xml schema that overcomes many of these problems and is intended for use in applications that support the rapid development of spoken language deliverables .", "topics": ["natural language processing", "entity"]}
{"title": "yara parser : a fast and accurate dependency parser", "abstract": "dependency parsers are among the most crucial tools in natural language processing as they have many important applications in downstream tasks such as information retrieval , machine translation and knowledge acquisition . we introduce the yara parser , a fast and accurate open-source dependency parser based on the arc-eager algorithm and beam search . it achieves an unlabeled accuracy of 93.32 on the standard wsj test set which ranks it among the top dependency parsers . at its fastest , yara can parse about 4000 sentences per second when in greedy mode ( 1 beam ) . when optimizing for accuracy ( using 64 beams and brown cluster features ) , yara can parse 45 sentences per second . the parser can be trained on any syntactic dependency treebank and different options are provided in order to make it more flexible and tunable for specific tasks . it is released with the apache version 2.0 license and can be used for both commercial and academic purposes . the parser can be found at https : //github.com/yahoo/yaraparser .", "topics": ["test set", "natural language processing"]}
{"title": "clustering on multi-layer graphs via subspace analysis on grassmann manifolds", "abstract": "relationships between entities in datasets are often of multiple nature , like geographical distance , social relationships , or common interests among people in a social network , for example . this information can naturally be modeled by a set of weighted and undirected graphs that form a global multilayer graph , where the common vertex set represents the entities and the edges on different layers capture the similarities of the entities in term of the different modalities . in this paper , we address the problem of analyzing multi-layer graphs and propose methods for clustering the vertices by efficiently merging the information provided by the multiple modalities . to this end , we propose to combine the characteristics of individual graph layers using tools from subspace analysis on a grassmann manifold . the resulting combination can then be viewed as a low dimensional representation of the original data which preserves the most important information from diverse relationships between entities . we use this information in new clustering methods and test our algorithm on several synthetic and real world datasets where we demonstrate superior or competitive performances compared to baseline and state-of-the-art techniques . our generic framework further extends to numerous analysis and learning problems that involve different types of information on graphs .", "topics": ["baseline ( configuration management )", "cluster analysis"]}
{"title": "characterization of gradient dominance and regularity conditions for neural networks", "abstract": "the past decade has witnessed a successful application of deep learning to solving many challenging problems in machine learning and artificial intelligence . however , the loss functions of deep neural networks ( especially nonlinear networks ) are still far from being well understood from a theoretical aspect . in this paper , we enrich the current understanding of the landscape of the square loss functions for three types of neural networks . specifically , when the parameter matrices are square , we provide an explicit characterization of the global minimizers for linear networks , linear residual networks , and nonlinear networks with one hidden layer . then , we establish two quadratic types of landscape properties for the square loss of these neural networks , i.e . , the gradient dominance condition within the neighborhood of their full rank global minimizers , and the regularity condition along certain directions and within the neighborhood of their global minimizers . these two landscape properties are desirable for the optimization around the global minimizers of the loss function for these neural networks .", "topics": ["neural networks", "nonlinear system"]}
{"title": "on hashing-based approaches to approximate dnf-counting", "abstract": "propositional model counting is a fundamental problem in artificial intelligence with a wide variety of applications , such as probabilistic inference , decision making under uncertainty , and probabilistic databases . consequently , the problem is of theoretical as well as practical interest . when the constraints are expressed as dnf formulas , monte carlo-based techniques have been shown to provide a fully polynomial randomized approximation scheme ( fpras ) . for cnf constraints , hashing-based approximation techniques have been demonstrated to be highly successful . furthermore , it was shown that hashing-based techniques also yield an fpras for dnf counting without usage of monte carlo sampling . our analysis , however , shows that the proposed hashing-based approach to dnf counting provides poor time complexity compared to the monte carlo-based dnf counting techniques . given the success of hashing-based techniques for cnf constraints , it is natural to ask : can hashing-based techniques provide an efficient fpras for dnf counting ? in this paper , we provide a positive answer to this question . to this end , we introduce two novel algorithmic techniques : \\emph { symbolic hashing } and \\emph { stochastic cell counting } , along with a new hash family of \\emph { row-echelon hash functions } . these innovations allow us to design a hashing-based fpras for dnf counting of similar complexity ( up to polylog factors ) as that of prior works . furthermore , we expect these techniques to have potential applications beyond dnf counting .", "topics": ["time complexity", "approximation"]}
{"title": "capturing long-range contextual dependencies with memory-enhanced conditional random fields", "abstract": "despite successful applications across a broad range of nlp tasks , conditional random fields ( `` crfs '' ) , in particular the linear-chain variant , are only able to model local features . while this has important benefits in terms of inference tractability , it limits the ability of the model to capture long-range dependencies between items . attempts to extend crfs to capture long-range dependencies have largely come at the cost of computational complexity and approximate inference . in this work , we propose an extension to crfs by integrating external memory , taking inspiration from memory networks , thereby allowing crfs to incorporate information far beyond neighbouring steps . experiments across two tasks show substantial improvements over strong crf and lstm baselines .", "topics": ["baseline ( configuration management )", "natural language processing"]}
{"title": "process discovery using inductive miner and decomposition", "abstract": "this report presents a submission to the process discovery contest . the contest is dedicated to the assessment of tools and techniques that discover business process models from event logs . the objective is to compare the efficiency of techniques to discover process models that provide a proper balance between `` overfitting '' and `` underfitting '' . in the context of the process discovery contest , process discovery is turned into a classification task with a training set and a test set ; where a process model needs to decide whether traces are fitting or not . in this report , we first show how we use two discovery techniques , namely : inductive miner and decomposition , to discover process models from the training set using prom tool . second , we show how we use replay results to 1 ) check the rediscoverability of models , and to 2 ) classify unseen traces ( in test logs ) as fitting or not . then , we discuss the classification results of validation logs , the complexity of discovered models , and their impact on the selection of models for submission . the report ends with the pictures of the submitted process models .", "topics": ["test set"]}
{"title": "shadow estimation method for `` the episolar constraint : monocular shape from shadow correspondence ''", "abstract": "recovering shadows is an important step for many vision algorithms . current approaches that work with time-lapse sequences are limited to simple thresholding heuristics . we show these approaches only work with very careful tuning of parameters , and do not work well for long-term time-lapse sequences taken over the span of many months . we introduce a parameter-free expectation maximization approach which simultaneously estimates shadows , albedo , surface normals , and skylight . this approach is more accurate than previous methods , works over both very short and very long sequences , and is robust to the effects of nonlinear camera response . finally , we demonstrate that the shadow masks derived through this algorithm substantially improve the performance of sun-based photometric stereo compared to earlier shadow mask estimation .", "topics": ["nonlinear system", "heuristic"]}
{"title": "robust artificial neural networks and outlier detection . technical report", "abstract": "large outliers break down linear and nonlinear regression models . robust regression methods allow one to filter out the outliers when building a model . by replacing the traditional least squares criterion with the least trimmed squares criterion , in which half of data is treated as potential outliers , one can fit accurate regression models to strongly contaminated data . high-breakdown methods have become very well established in linear regression , but have started being applied for non-linear regression only recently . in this work , we examine the problem of fitting artificial neural networks to contaminated data using least trimmed squares criterion . we introduce a penalized least trimmed squares criterion which prevents unnecessary removal of valid data . training of anns leads to a challenging non-smooth global optimization problem . we compare the efficiency of several derivative-free optimization methods in solving it , and show that our approach identifies the outliers correctly when anns are used for nonlinear regression .", "topics": ["optimization problem", "nonlinear system"]}
{"title": "the gtr-model : a universal framework for quantum-like measurements", "abstract": "we present a very general geometrico-dynamical description of physical or more abstract entities , called the 'general tension-reduction ' ( gtr ) model , where not only states , but also measurement-interactions can be represented , and the associated outcome probabilities calculated . underlying the model is the hypothesis that indeterminism manifests as a consequence of unavoidable fluctuations in the experimental context , in accordance with the 'hidden-measurements interpretation ' of quantum mechanics . when the structure of the state space is hilbertian , and measurements are of the 'universal ' kind , i.e . , are the result of an average over all possible ways of selecting an outcome , the gtr-model provides the same predictions of the born rule , and therefore provides a natural completed version of quantum mechanics . however , when the structure of the state space is non-hilbertian and/or not all possible ways of selecting an outcome are available to be actualized , the predictions of the model generally differ from the quantum ones , especially when sequential measurements are considered . some paradigmatic examples will be discussed , taken from physics and human cognition . particular attention will be given to some known psychological effects , like question order effects and response replicability , which we show are able to generate non-hilbertian statistics . we also suggest a realistic interpretation of the gtr-model , when applied to human cognition and decision , which we think could become the generally adopted interpretative framework in quantum cognition research .", "topics": ["interaction", "entity"]}
{"title": "neuronal circuit policies", "abstract": "we propose an effective way to create interpretable control agents , by re-purposing the function of a biological neural circuit model , to govern simulated and real world reinforcement learning ( rl ) test-beds . we model the tap-withdrawal ( tw ) neural circuit of the nematode , c. elegans , a circuit responsible for the worm 's reflexive response to external mechanical touch stimulations , and learn its synaptic and neuronal parameters as a policy for controlling basic rl tasks . we also autonomously park a real rover robot on a pre-defined trajectory , by deploying such neuronal circuit policies learned in a simulated environment . for reconfiguration of the purpose of the tw neural circuit , we adopt a search-based rl algorithm . we show that our neuronal policies perform as good as deep neural network policies with the advantage of realizing interpretable dynamics at the cell level .", "topics": ["reinforcement learning", "simulation"]}
{"title": "a deep learning model integrating fcnns and crfs for brain tumor segmentation", "abstract": "accurate and reliable brain tumor segmentation is a critical component in cancer diagnosis , treatment planning , and treatment outcome evaluation . build upon successful deep learning techniques , a novel brain tumor segmentation method is developed by integrating fully convolutional neural networks ( fcnns ) and conditional random fields ( crfs ) in a unified framework to obtain segmentation results with appearance and spatial consistency . we train a deep learning based segmentation model using 2d image patches and image slices in following steps : 1 ) training fcnns using image patches ; 2 ) training crfs as recurrent neural networks ( crf-rnn ) using image slices with parameters of fcnns fixed ; and 3 ) fine-tuning the fcnns and the crf-rnn using image slices . particularly , we train 3 segmentation models using 2d image patches and slices obtained in axial , coronal and sagittal views respectively , and combine them to segment brain tumors using a voting based fusion strategy . our method could segment brain images slice-by-slice , much faster than those based on image patches . we have evaluated our method based on imaging data provided by the multimodal brain tumor image segmentation challenge ( brats ) 2013 , brats 2015 and brats 2016 . the experimental results have demonstrated that our method could build a segmentation model with flair , t1c , and t2 scans and achieve competitive performance as those built with flair , t1 , t1c , and t2 scans .", "topics": ["image segmentation", "recurrent neural network"]}
{"title": "gated end-to-end memory networks", "abstract": "machine reading using differentiable reasoning models has recently shown remarkable progress . in this context , end-to-end trainable memory networks , memn2n , have demonstrated promising performance on simple natural language based reasoning tasks such as factual reasoning and basic deduction . however , other tasks , namely multi-fact question-answering , positional reasoning or dialog related tasks , remain challenging particularly due to the necessity of more complex interactions between the memory and controller modules composing this family of models . in this paper , we introduce a novel end-to-end memory access regulation mechanism inspired by the current progress on the connection short-cutting principle in the field of computer vision . concretely , we develop a gated end-to-end trainable memory network architecture , gmemn2n . from the machine learning perspective , this new capability is learned in an end-to-end fashion without the use of any additional supervision signal which is , as far as our knowledge goes , the first of its kind . our experiments show significant improvements on the most challenging tasks in the 20 babi dataset , without the use of any domain knowledge . then , we show improvements on the dialog babi tasks including the real human-bot conversion-based dialog state tracking challenge ( dstc-2 ) dataset . on these two datasets , our model sets the new state of the art .", "topics": ["computer vision", "natural language"]}
{"title": "few-shot learning through an information retrieval lens", "abstract": "few-shot learning refers to understanding new concepts from only a few examples . we propose an information retrieval-inspired approach for this problem that is motivated by the increased importance of maximally leveraging all the available information in this low-data regime . we define a training objective that aims to extract as much information as possible from each training batch by effectively optimizing over all relative orderings of the batch points simultaneously . in particular , we view each batch point as a `query ' that ranks the remaining ones based on its predicted relevance to them and we define a model within the framework of structured prediction to optimize mean average precision over these rankings . our method achieves impressive results on the standard few-shot classification benchmarks while is also capable of few-shot retrieval .", "topics": ["relevance"]}
{"title": "approximation algorithms for k-modes clustering", "abstract": "in this paper , we study clustering with respect to the k-modes objective function , a natural formulation of clustering for categorical data . one of the main contributions of this paper is to establish the connection between k-modes and k-median , i.e . , the optimum of k-median is at most twice the optimum of k-modes for the same categorical data clustering problem . based on this observation , we derive a deterministic algorithm that achieves an approximation factor of 2 . furthermore , we prove that the distance measure in k-modes defines a metric . hence , we are able to extend existing approximation algorithms for metric k-median to k-modes . empirical results verify the superiority of our method .", "topics": ["approximation algorithm", "cluster analysis"]}
{"title": "using causal discovery to track information flow in spatio-temporal data - a testbed and experimental results using advection-diffusion simulations", "abstract": "causal discovery algorithms based on probabilistic graphical models have emerged in geoscience applications for the identification and visualization of dynamical processes . the key idea is to learn the structure of a graphical model from observed spatio-temporal data , which indicates information flow , thus pathways of interactions , in the observed physical system . studying those pathways allows geoscientists to learn subtle details about the underlying dynamical mechanisms governing our planet . initial studies using this approach on real-world atmospheric data have shown great potential for scientific discovery . however , in these initial studies no ground truth was available , so that the resulting graphs have been evaluated only by whether a domain expert thinks they seemed physically plausible . this paper seeks to fill this gap . we develop a testbed that emulates two dynamical processes dominant in many geoscience applications , namely advection and diffusion , in a 2d grid . then we apply the causal discovery based information tracking algorithms to the simulation data to study how well the algorithms work for different scenarios and to gain a better understanding of the physical meaning of the graph results , in particular of instantaneous connections . we make all data sets used in this study available to the community as a benchmark . keywords : information flow , graphical model , structure learning , causal discovery , geoscience .", "topics": ["graphical model", "simulation"]}
{"title": "one-class collective anomaly detection based on long short-term memory recurrent neural networks", "abstract": "intrusion detection for computer network systems has been becoming one of the most critical tasks for network administrators today . it has an important role for organizations , governments and our society due to the valuable resources hosted on computer networks . traditional misuse detection strategies are unable to detect new and unknown intrusion types . in contrast , anomaly detection in network security aims to distinguish between illegal or malicious events and normal behavior of network systems . anomaly detection can be considered as a classification problem where it builds models of normal network behavior , of which it uses to detect new patterns that significantly deviate from the model . most of the current approaches on anomaly detection is based on the learning of normal behavior and anomalous actions . they do not include memory that is they do not take into account previous events classify new ones . in this paper , we propose a one class collective anomaly detection model based on neural network learning . normally a long short term memory recurrent neural network ( lstm rnn ) is trained only on normal data , and it is capable of predicting several time steps ahead of an input . in our approach , a lstm rnn is trained on normal time series data before performing a prediction for each time step . instead of considering each time-step separately , the observation of prediction errors from a certain number of time-steps is now proposed as a new idea for detecting collective anomalies . the prediction errors of a certain number of the latest time-steps above a threshold will indicate a collective anomaly . the model is evaluated on a time series version of the kdd 1999 dataset . the experiments demonstrate that the proposed model is capable to detect collective anomaly efficiently", "topics": ["data mining", "time series"]}
{"title": "fishing for exactness", "abstract": "statistical methods for automatically identifying dependent word pairs ( i.e . dependent bigrams ) in a corpus of natural language text have traditionally been performed using asymptotic tests of significance . this paper suggests that fisher 's exact test is a more appropriate test due to the skewed and sparse data samples typical of this problem . both theoretical and experimental comparisons between fisher 's exact test and a variety of asymptotic tests ( the t-test , pearson 's chi-square test , and likelihood-ratio chi-square test ) are presented . these comparisons show that fisher 's exact test is more reliable in identifying dependent word pairs . the usefulness of fisher 's exact test extends to other problems in statistical natural language processing as skewed and sparse data appears to be the rule in natural language . the experiment presented in this paper was performed using proc freq of the sas system .", "topics": ["natural language processing", "natural language"]}
{"title": "weight-based variable ordering in the context of high-level consistencies", "abstract": "dom/wdeg is one of the best performing heuristics for dynamic variable ordering in backtrack search [ boussemart et al . , 2004 ] . as originally defined , this heuristic increments the weight of the constraint that causes a domain wipeout ( i.e . , a dead-end ) when enforcing arc consistency during search . `` the process of weighting constraints with dom/wdeg is not defined when more than one constraint lead to a domain wipeout [ vion et al . , 2011 ] . '' in this paper , we investigate how weights should be updated in the context of two high-level consistencies , namely , singleton ( poac ) and relational consistencies ( rnic ) . we propose , analyze , and empirically evaluate several strategies for updating the weights . we statistically compare the proposed strategies and conclude with our recommendations .", "topics": ["high- and low-level", "heuristic"]}
{"title": "efficient regression in metric spaces via approximate lipschitz extension", "abstract": "we present a framework for performing efficient regression in general metric spaces . roughly speaking , our regressor predicts the value at a new point by computing a lipschitz extension -- - the smoothest function consistent with the observed data -- - after performing structural risk minimization to avoid overfitting . we obtain finite-sample risk bounds with minimal structural and noise assumptions , and a natural speed-precision tradeoff . the offline ( learning ) and online ( prediction ) stages can be solved by convex programming , but this naive approach has runtime complexity $ o ( n^3 ) $ , which is prohibitive for large datasets . we design instead a regression algorithm whose speed and generalization performance depend on the intrinsic dimension of the data , to which the algorithm adapts . while our main innovation is algorithmic , the statistical results may also be of independent interest .", "topics": ["approximation algorithm"]}
{"title": "evolution of sustained foraging in 3d environments with physics", "abstract": "artificially evolving foraging behavior in simulated legged animals has proved to be a notoriously difficult task . here , we co-evolve the morphology and controller for virtual organisms in a three-dimensional physically realistic environment to produce goal-directed legged locomotion . we show that following and reaching multiple food sources can evolve de novo , by evaluating each organism on multiple food sources placed on a basic pattern that is gradually randomized across generations . we devised a strategy of evolutionary `` staging '' , where the best organism from a set of evolutionary experiments using a particular fitness function is used to seed a new set , with a fitness function that is progressively altered to better challenge organisms as evolution improves them . we find that an organism 's efficiency at reaching the first food source does not predict its ability at finding subsequent ones because foraging efficiency crucially depends on the position of the last food source reached , an effect illustrated by `` foraging maps '' that capture the organism 's controller state , body position , and orientation . our best evolved foragers are able to reach multiple food sources over 90 % of the time on average , a behavior that is key to any biologically realistic simulation where a self-sustaining population has to survive by collecting food sources in three-dimensional , physical environments .", "topics": ["simulation", "map"]}
{"title": "artificial error generation with machine translation and syntactic patterns", "abstract": "shortage of available training data is holding back progress in the area of automated error detection . this paper investigates two alternative methods for artificially generating writing errors , in order to create additional resources . we propose treating error generation as a machine translation task , where grammatically correct text is translated to contain errors . in addition , we explore a system for extracting textual patterns from an annotated corpus , which can then be used to insert errors into grammatically correct sentences . our experiments show that the inclusion of artificially generated errors significantly improves error detection accuracy on both fce and conll 2014 datasets .", "topics": ["test set", "machine translation"]}
{"title": "goal conflict in designing an autonomous artificial system", "abstract": "research on human self-regulation has shown that people hold many goals simultaneously and have complex self-regulation mechanisms to deal with this goal conflict . artificial autonomous systems may also need to find ways to cope with conflicting goals . indeed , the intricate interplay among different goals may be critical to the design as well as long-term safety and stability of artificial autonomous systems . i discuss some of the critical features of the human self-regulation system and how it might be applied to an artificial system . furthermore , the implications of goal conflict for the reliability and stability of artificial autonomous systems and ensuring their alignment with human goals and ethics is examined .", "topics": ["autonomous car"]}
{"title": "robust object tracking based on self-adaptive search area", "abstract": "discriminative correlation filter ( dcf ) based trackers have recently achieved excellent performance with great computational efficiency . however , dcf based trackers suffer boundary effects , which result in the unstable performance in challenging situations exhibiting fast motion . in this paper , we propose a novel method to mitigate this side-effect in dcf based trackers . we change the search area according to the prediction of target motion . when the object moves fast , broad search area could alleviate boundary effects and reserve the probability of locating the object . when the object moves slowly , narrow search area could prevent the effect of useless background information and improve computational efficiency to attain real-time performance . this strategy can impressively soothe boundary effects in situations exhibiting fast motion and motion blur , and it can be used in almost all dcf based trackers . the experiments on otb benchmark show that the proposed framework improves the performance compared with the baseline trackers .", "topics": ["baseline ( configuration management )"]}
{"title": "object detection can be improved using human-derived contextual expectations", "abstract": "each object in the world occurs in a specific context : cars are seen on highways but not in forests . contextual information is generally thought to facilitate computation by constraining locations to search . but can knowing context yield tangible benefits in object detection ? for it to do so , scene context needs to be learned independently from target features . however this is impossible in traditional object detection where classifiers are trained on images containing both target features and surrounding coarse scene features . in contrast , we humans have the opportunity to learn context and target features separately , such as when we see highways without cars . here we show for the first time that human-derived scene expectations can be used to improve object detection performance in machines . to measure these expectations , we asked human subjects to indicate the scale , location and likelihood at which targets may occur on scenes containing no targets . humans showed highly systematic expectations that we could accurately predict using scene features . we then augmented state-of-the-art object detectors ( based on deep neural networks ) with these human-derived expectations on novel scenes to produce a significant ( 1-3 % ) improvement in detecting cars and people in scenes . this improvement was due to low-confidence detector matches being correctly relabeled as targets when they occurred in likely scenes .", "topics": ["object detection"]}
{"title": "bounded recursive self-improvement", "abstract": "we have designed a machine that becomes increasingly better at behaving in underspecified circumstances , in a goal-directed way , on the job , by modeling itself and its environment as experience accumulates . based on principles of autocatalysis , endogeny , and reflectivity , the work provides an architectural blueprint for constructing systems with high levels of operational autonomy in underspecified circumstances , starting from a small seed . through value-driven dynamic priority scheduling controlling the parallel execution of a vast number of reasoning threads , the system achieves recursive self-improvement after it leaves the lab , within the boundaries imposed by its designers . a prototype system has been implemented and demonstrated to learn a complex real-world task , real-time multimodal dialogue with humans , by on-line observation . our work presents solutions to several challenges that must be solved for achieving artificial general intelligence .", "topics": ["autonomous car"]}
{"title": "vanishing point detection with convolutional neural networks", "abstract": "inspired by the finding that vanishing point ( road tangent ) guides driver 's gaze , in our previous work we showed that vanishing point attracts gaze during free viewing of natural scenes as well as in visual search ( borji et al . , journal of vision 2016 ) . we have also introduced improved saliency models using vanishing point detectors ( feng et al . , wacv 2016 ) . here , we aim to predict vanishing points in naturalistic environments by training convolutional neural networks in an end-to-end manner over a large set of road images downloaded from youtube with vanishing points annotated . results demonstrate effectiveness of our approach compared to classic approaches of vanishing point detection in the literature .", "topics": ["end-to-end principle", "autonomous car"]}
{"title": "feature tracking cardiac magnetic resonance via deep learning and spline optimization", "abstract": "feature tracking cardiac magnetic resonance ( cmr ) has recently emerged as an area of interest for quantification of regional cardiac function from balanced , steady state free precession ( ssfp ) cine sequences . however , currently available techniques lack full automation , limiting reproducibility . we propose a fully automated technique whereby a cmr image sequence is first segmented with a deep , fully convolutional neural network ( cnn ) architecture , and quadratic basis splines are fitted simultaneously across all cardiac frames using least squares optimization . experiments are performed using data from 42 patients with hypertrophic cardiomyopathy ( hcm ) and 21 healthy control subjects . in terms of segmentation , we compared state-of-the-art cnn frameworks , u-net and dilated convolution architectures , with and without temporal context , using cross validation with three folds . performance relative to expert manual segmentation was similar across all networks : pixel accuracy was ~97 % , intersection-over-union ( iou ) across all classes was ~87 % , and iou across foreground classes only was ~85 % . endocardial left ventricular circumferential strain calculated from the proposed pipeline was significantly different in control and disease subjects ( -25.3 % vs -29.1 % , p = 0.006 ) , in agreement with the current clinical literature .", "topics": ["convolution", "pixel"]}
{"title": "human pose estimation from rgb input using synthetic training data", "abstract": "we address the problem of estimating the pose of humans using rgb image input . more specifically , we are using a random forest classifier to classify pixels into joint-based body part categories , much similar to the famous kinect pose estimator [ 11 ] , [ 12 ] . however , we are using pure rgb input , i.e . no depth . since the random forest requires a large number of training examples , we are using computer graphics generated , synthetic training data . in addition , we assume that we have access to a large number of real images with bounding box labels , extracted for example by a pedestrian detector or a tracking system . we propose a new objective function for random forest training that uses the weakly labeled data from the target domain to encourage the learner to select features that generalize from the synthetic source domain to the real target domain . we demonstrate on a publicly available dataset [ 6 ] that the proposed objective function yields a classifier that significantly outperforms a baseline classifier trained using the standard entropy objective [ 10 ] .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "image sampling with quasicrystals", "abstract": "we investigate the use of quasicrystals in image sampling . quasicrystals produce space-filling , non-periodic point sets that are uniformly discrete and relatively dense , thereby ensuring the sample sites are evenly spread out throughout the sampled image . their self-similar structure can be attractive for creating sampling patterns endowed with a decorative symmetry . we present a brief general overview of the algebraic theory of cut-and-project quasicrystals based on the geometry of the golden ratio . to assess the practical utility of quasicrystal sampling , we evaluate the visual effects of a variety of non-adaptive image sampling strategies on photorealistic image reconstruction and non-photorealistic image rendering used in multiresolution image representations . for computer visualization of point sets used in image sampling , we introduce a mosaic rendering technique .", "topics": ["sampling ( signal processing )"]}
{"title": "learning to read chest x-rays : recurrent neural cascade model for automated image annotation", "abstract": "despite the recent advances in automatically describing image contents , their applications have been mostly limited to image caption datasets containing natural images ( e.g . , flickr 30k , mscoco ) . in this paper , we present a deep learning model to efficiently detect a disease from an image and annotate its contexts ( e.g . , location , severity and the affected organs ) . we employ a publicly available radiology dataset of chest x-rays and their reports , and use its image annotations to mine disease names to train convolutional neural networks ( cnns ) . in doing so , we adopt various regularization techniques to circumvent the large normal-vs-diseased cases bias . recurrent neural networks ( rnns ) are then trained to describe the contexts of a detected disease , based on the deep cnn features . moreover , we introduce a novel approach to use the weights of the already trained pair of cnn/rnn on the domain-specific image/text dataset , to infer the joint image/text contexts for composite image labeling . significantly improved image annotation results are demonstrated using the recurrent neural cascade model by taking the joint image/text contexts into account .", "topics": ["recurrent neural network", "matrix regularization"]}
{"title": "thompson sampling for dynamic pricing", "abstract": "in this paper we apply active learning algorithms for dynamic pricing in a prominent e-commerce website . dynamic pricing involves changing the price of items on a regular basis , and uses the feedback from the pricing decisions to update prices of the items . most popular approaches to dynamic pricing use a passive learning approach , where the algorithm uses historical data to learn various parameters of the pricing problem , and uses the updated parameters to generate a new set of prices . we show that one can use active learning algorithms such as thompson sampling to more efficiently learn the underlying parameters in a pricing problem . we apply our algorithms to a real e-commerce system and show that the algorithms indeed improve revenue compared to pricing algorithms that use passive learning .", "topics": ["sampling ( signal processing )"]}
{"title": "a new approach for super resolution by using web images and fft based image registration", "abstract": "preserving accuracy is a challenging issue in super resolution images . in this paper , we propose a new fft based image registration algorithm and a sparse based super resolution algorithm to improve the accuracy of super resolution image . given a low resolution image , our approach initially extracts the local descriptors from the input and then the local descriptors from the whole correlated images using the sift algorithm . once this is completed , it will compare the local descriptors on the basis of a threshold value . the retrieved images could be having different focal length , illumination , inclination and size . to overcome the above differences of the retrieved images , we propose a new fft based image registration algorithm . after the registration stage , we apply a sparse based super resolution on the images for recreating images with better resolution compared to the input . based on the pssnr calculation and ssim comparison , we can see that the new methodology creates a better image than the traditional methods .", "topics": ["sparse matrix"]}
{"title": "a permutation-based model for crowd labeling : optimal estimation and robustness", "abstract": "the aggregation and denoising of crowd labeled data is a task that has gained increased significance with the advent of crowdsourcing platforms and massive datasets . in this paper , we propose a permutation-based model for crowd labeled data that is a significant generalization of the common dawid-skene model , and introduce a new error metric by which to compare different estimators . working in a high-dimensional non-asymptotic framework that allows both the number of workers and tasks to scale , we derive optimal rates of convergence for the permutation-based model . we show that the permutation-based model offers significant robustness in estimation due to its richness , while surprisingly incurring only a small additional statistical penalty as compared to the dawid-skene model . finally , we propose a computationally-efficient method , called the obi-wan estimator , that is uniformly optimal over a class intermediate between the permutation-based and the dawid-skene models , and is uniformly consistent over the entire permutation-based model class . in contrast , the guarantees for estimators available in prior literature are sub-optimal over the original dawid-skene model .", "topics": ["noise reduction"]}
{"title": "an image based technique for enhancement of underwater images", "abstract": "the underwater images usually suffers from non-uniform lighting , low contrast , blur and diminished colors . in this paper , we proposed an image based preprocessing technique to enhance the quality of the underwater images . the proposed technique comprises a combination of four filters such as homomorphic filtering , wavelet denoising , bilateral filter and contrast equalization . these filters are applied sequentially on degraded underwater images . the literature survey reveals that image based preprocessing algorithms uses standard filter techniques with various combinations . for smoothing the image , the image based preprocessing algorithms uses the anisotropic filter . the main drawback of the anisotropic filter is that iterative in nature and computation time is high compared to bilateral filter . in the proposed technique , in addition to other three filters , we employ a bilateral filter for smoothing the image . the experimentation is carried out in two stages . in the first stage , we have conducted various experiments on captured images and estimated optimal parameters for bilateral filter . similarly , optimal filter bank and optimal wavelet shrinkage function are estimated for wavelet denoising . in the second stage , we conducted the experiments using estimated optimal parameters , optimal filter bank and optimal wavelet shrinkage function for evaluating the proposed technique . we evaluated the technique using quantitative based criteria such as a gradient magnitude histogram and peak signal to noise ratio ( psnr ) . further , the results are qualitatively evaluated based on edge detection results . the proposed technique enhances the quality of the underwater images and can be employed prior to apply computer vision techniques .", "topics": ["noise reduction", "time complexity"]}
{"title": "performance and convergence of multi-user online learning", "abstract": "we study the problem of allocating multiple users to a set of wireless channels in a decentralized manner when the channel quali- ties are time-varying and unknown to the users , and accessing the same channel by multiple users leads to reduced quality due to interference . in such a setting the users not only need to learn the inherent channel quality and at the same time the best allocations of users to channels so as to maximize the social welfare . assuming that the users adopt a certain online learning algorithm , we investigate under what conditions the socially optimal allocation is achievable . in particular we examine the effect of different levels of knowledge the users may have and the amount of communications and cooperation . the general conclusion is that when the cooperation of users decreases and the uncertainty about channel payoffs increases it becomes harder to achieve the socially opti- mal allocation .", "topics": ["regret ( decision theory )", "mathematical optimization"]}
{"title": "training better cnns requires to rethink relu", "abstract": "with the rapid development of deep convolutional neural networks ( dcnns ) , numerous works focus on designing better network architectures ( i.e . , alexnet , vgg , inception , resnet and densenet etc . ) . nevertheless , all these networks have the same characteristic : each convolutional layer is followed by an activation layer , a rectified linear unit ( relu ) layer is the most used among them . in this work , we argue that the paired module with 1:1 convolution and relu ratio is not the best choice since it may result in poor generalization ability . thus , we try to investigate the more suitable convolution and relu ratio for exploring the better network architectures . specifically , inspired by leaky relu , we focus on adopting the proportional module with n : m ( n $ > $ m ) convolution and relu ratio to design the better networks . from the perspective of ensemble learning , leaky relu can be considered as an ensemble of networks with different convolution and relu ratio . we find that the proportional module with n : m ( n $ > $ m ) convolution and relu ratio can help networks acquire the better performance , through the analysis of a simple leaky relu model . by utilizing the proportional module with n : m ( n $ > $ m ) convolution and relu ratio , many popular networks can form more rich representations in models , since the n : m ( n $ > $ m ) proportional module can utilize information more effectively . furthermore , we apply this module in diverse dcnn models to explore whether is the n : m ( n $ > $ m ) convolution and relu ratio indeed more effective . from our experimental results , we can find that such a simple yet effective method achieves better performance in different benchmarks with various network architectures and the experimental results verify that the superiority of the proportional module .", "topics": ["neural networks", "computer vision"]}
{"title": "dropneuron : simplifying the structure of deep neural networks", "abstract": "deep learning using multi-layer neural networks ( nns ) architecture manifests superb power in modern machine learning systems . the trained deep neural networks ( dnns ) are typically large . the question we would like to address is whether it is possible to simplify the nn during training process to achieve a reasonable performance within an acceptable computational time . we presented a novel approach of optimising a deep neural network through regularisation of net- work architecture . we proposed regularisers which support a simple mechanism of dropping neurons during a network training process . the method supports the construction of a simpler deep neural networks with compatible performance with its simplified version . as a proof of concept , we evaluate the proposed method with examples including sparse linear regression , deep autoencoder and convolutional neural network . the valuations demonstrate excellent performance . the code for this work can be found in http : //www.github.com/panweihit/dropneuron", "topics": ["neural networks", "time complexity"]}
{"title": "scan : learning abstract hierarchical compositional visual concepts", "abstract": "the natural world is infinitely diverse , yet this diversity arises from a relatively small set of coherent properties and rules , such as the laws of physics or chemistry . we conjecture that biological intelligent systems are able to survive within their diverse environments by discovering the regularities that arise from these rules primarily through unsupervised experiences , and representing this knowledge as abstract concepts . such representations possess useful properties of compositionality and hierarchical organisation , which allow intelligent agents to recombine a finite set of conceptual building blocks into an exponentially large set of useful new concepts . this paper describes scan ( symbol-concept association network ) , a new framework for learning such concepts in the visual domain . we first use the previously published beta-vae ( higgins et al . , 2017a ) architecture to learn a disentangled representation of the latent structure of the visual world , before training scan to extract abstract concepts grounded in such disentangled visual primitives through fast symbol association . our approach requires very few pairings between symbols and images and makes no assumptions about the choice of symbol representations . once trained , scan is capable of multimodal bi-directional inference , generating a diverse set of image samples from symbolic descriptions and vice versa . it also allows for traversal and manipulation of the implicit hierarchy of compositional visual concepts through symbolic instructions and learnt logical recombination operations . such manipulations enable scan to invent and learn novel visual concepts through recombination of the few learnt concepts .", "topics": ["unsupervised learning"]}
{"title": "understanding trajectory behavior : a motion pattern approach", "abstract": "mining the underlying patterns in gigantic and complex data is of great importance to data analysts . in this paper , we propose a motion pattern approach to mine frequent behaviors in trajectory data . motion patterns , defined by a set of highly similar flow vector groups in a spatial locality , have been shown to be very effective in extracting dominant motion behaviors in video sequences . inspired by applications and properties of motion patterns , we have designed a framework that successfully solves the general task of trajectory clustering . our proposed algorithm consists of four phases : flow vector computation , motion component extraction , motion component 's reachability set creation , and motion pattern formation . for the first phase , we break down trajectories into flow vectors that indicate instantaneous movements . in the second phase , via a kmeans clustering approach , we create motion components by clustering the flow vectors with respect to their location and velocity . next , we create motion components ' reachability set in terms of spatial proximity and motion similarity . finally , for the fourth phase , we cluster motion components using agglomerative clustering with the weighted jaccard distance between the motion components ' signatures , a set created using path reachability . we have evaluated the effectiveness of our proposed method in an extensive set of experiments on diverse datasets . further , we have shown how our proposed method handles difficulties in the general task of trajectory clustering that challenge the existing state-of-the-art methods .", "topics": ["cluster analysis", "computation"]}
{"title": "a framework for intelligent multi agent system based neural network classification model", "abstract": "tintelligent multi agent systems have great potentials to use in different purposes and research areas . one of the important issues to apply intelligent multi agent systems in real world and virtual environment is to develop a framework that support machine learning model to reflect the whole complexity of the real world . in this paper , we proposed a framework of intelligent agent based neural network classification model to solve the problem of gap between two applicable flows of intelligent multi agent technology and learning model from real environment . we consider the new supervised multilayers feed forward neural network ( smffnn ) model as an intelligent classification for learning model in the framework . the framework earns the information from the respective environment and its behavior can be recognized by the weights . therefore , the smffnn model that lies in the framework will give more benefits in finding the suitable information and the real weights from the environment which result for better recognition . the framework is applicable to different domains successfully and for the potential case study , the clinical organization and its domain is considered for the proposed framework", "topics": ["neural networks"]}
{"title": "built-in foreground/background prior for weakly-supervised semantic segmentation", "abstract": "pixel-level annotations are expensive and time consuming to obtain . hence , weak supervision using only image tags could have a significant impact in semantic segmentation . recently , cnn-based methods have proposed to fine-tune pre-trained networks using image tags . without additional information , this leads to poor localization accuracy . this problem , however , was alleviated by making use of objectness priors to generate foreground/background masks . unfortunately these priors either require training pixel-level annotations/bounding boxes , or still yield inaccurate object boundaries . here , we propose a novel method to extract markedly more accurate masks from the pre-trained network itself , forgoing external objectness modules . this is accomplished using the activations of the higher-level convolutional layers , smoothed by a dense crf . we demonstrate that our method , based on these masks and a weakly-supervised loss , outperforms the state-of-the-art tag-based weakly-supervised semantic segmentation techniques . furthermore , we introduce a new form of inexpensive weak supervision yielding an additional accuracy boost .", "topics": ["pixel"]}
{"title": "variance based moving k-means algorithm", "abstract": "clustering is a useful data exploratory method with its wide applicability in multiple fields . however , data clustering greatly relies on initialization of cluster centers that can result in large intra-cluster variance and dead centers , therefore leading to sub-optimal solutions . this paper proposes a novel variance based version of the conventional moving k-means ( mkm ) algorithm called variance based moving k-means ( vmkm ) that can partition data into optimal homogeneous clusters , irrespective of cluster initialization . the algorithm utilizes a novel distance metric and a unique data element selection criteria to transfer the selected elements between clusters to achieve low intra-cluster variance and subsequently avoid dead centers . quantitative and qualitative comparison with various clustering techniques is performed on four datasets selected from image processing , bioinformatics , remote sensing and the stock market respectively . an extensive analysis highlights the superior performance of the proposed method over other techniques .", "topics": ["cluster analysis", "image processing"]}
{"title": "learning to search on manifolds for 3d pose estimation of articulated objects", "abstract": "this paper focuses on the challenging problem of 3d pose estimation of a diverse spectrum of articulated objects from single depth images . a novel structured prediction approach is considered , where 3d poses are represented as skeletal models that naturally operate on manifolds . given an input depth image , the problem of predicting the most proper articulation of underlying skeletal model is thus formulated as sequentially searching for the optimal skeletal configuration . this is subsequently addressed by convolutional neural nets trained end-to-end to render sequential prediction of the joint locations as regressing a set of tangent vectors of the underlying manifolds . our approach is examined on various articulated objects including human hand , mouse , and fish benchmark datasets . empirically it is shown to deliver highly competitive performance with respect to the state-of-the-arts , while operating in real-time ( over 30 fps ) .", "topics": ["end-to-end principle"]}
{"title": "descriptor ensemble : an unsupervised approach to descriptor fusion in the homography space", "abstract": "with the aim to improve the performance of feature matching , we present an unsupervised approach to fuse various local descriptors in the space of homographies . inspired by the observation that the homographies of correct feature correspondences vary smoothly along the spatial domain , our approach stands on the unsupervised nature of feature matching , and can select a good descriptor for matching each feature point . specifically , the homography space serves as the common domain , in which a correspondence obtained by any descriptor is considered as a point , for integrating various heterogeneous descriptors . both geometric coherence and spatial continuity among correspondences are considered via computing their geodesic distances in the space . in this way , mutual verification across different descriptors is allowed , and correct correspondences will be highlighted with a high degree of consistency ( i.e . , short geodesic distances here ) . it follows that one-class svm can be applied to identifying these correct correspondences , and boosts the performance of feature matching . the proposed approach is comprehensively compared with the state-of-the-art approaches , and evaluated on four benchmarks of image matching . the promising results manifest its effectiveness .", "topics": ["unsupervised learning"]}
{"title": "the neural network pushdown automaton : model , stack and learning simulations", "abstract": "in order for neural networks to learn complex languages or grammars , they must have sufficient computational power or resources to recognize or generate such languages . though many approaches have been discussed , one ob- vious approach to enhancing the processing power of a recurrent neural network is to couple it with an external stack memory - in effect creating a neural network pushdown automata ( nnpda ) . this paper discusses in detail this nnpda - its construction , how it can be trained and how useful symbolic information can be extracted from the trained network . in order to couple the external stack to the neural network , an optimization method is developed which uses an error function that connects the learning of the state automaton of the neural network to the learning of the operation of the external stack . to minimize the error function using gradient descent learning , an analog stack is designed such that the action and storage of information in the stack are continuous . one interpretation of a continuous stack is the probabilistic storage of and action on data . after training on sample strings of an unknown source grammar , a quantization procedure extracts from the analog stack and neural network a discrete pushdown automata ( pda ) . simulations show that in learning deterministic context-free grammars - the balanced parenthesis language , 1*n0*n , and the deterministic palindrome - the extracted pda is correct in the sense that it can correctly recognize unseen strings of arbitrary length . in addition , the extracted pdas can be shown to be identical or equivalent to the pdas of the source grammars which were used to generate the training strings .", "topics": ["recurrent neural network", "simulation"]}
{"title": "lstm-based predictions for proactive information retrieval", "abstract": "we describe a method for proactive information retrieval targeted at retrieving relevant information during a writing task . in our method , the current task and the needs of the user are estimated , and the potential next steps are unobtrusively predicted based on the user 's past actions . we focus on the task of writing , in which the user is coalescing previously collected information into a text . our proactive system automatically recommends the user relevant background information . the proposed system incorporates text input prediction using a long short-term memory ( lstm ) network . we present simulations , which show that the system is able to reach higher precision values in an exploratory search setting compared to both a baseline and a comparison system .", "topics": ["baseline ( configuration management )", "simulation"]}
{"title": "rotting bandits", "abstract": "the multi-armed bandits ( mab ) framework highlights the tension between acquiring new knowledge ( exploration ) and leveraging available knowledge ( exploitation ) . in the classical mab problem , a decision maker must choose an arm at each time step , upon which she receives a reward . the decision maker 's objective is to maximize her cumulative expected reward over the time horizon . the mab problem has been studied extensively , specifically under the assumption of the arms ' rewards distributions being stationary , or quasi-stationary , over time . we consider a variant of the mab framework , which we termed rotting bandits , where each arm 's expected reward decays as a function of the number of times it has been pulled . we are motivated by many real-world scenarios such as online advertising , content recommendation , crowdsourcing , and more . we present algorithms , accompanied by simulations , and derive theoretical guarantees .", "topics": ["simulation"]}
{"title": "the conditional lucas & kanade algorithm", "abstract": "the lucas & kanade ( lk ) algorithm is the method of choice for efficient dense image and object alignment . the approach is efficient as it attempts to model the connection between appearance and geometric displacement through a linear relationship that assumes independence across pixel coordinates . a drawback of the approach , however , is its generative nature . specifically , its performance is tightly coupled with how well the linear model can synthesize appearance from geometric displacement , even though the alignment task itself is associated with the inverse problem . in this paper , we present a new approach , referred to as the conditional lk algorithm , which : ( i ) directly learns linear models that predict geometric displacement as a function of appearance , and ( ii ) employs a novel strategy for ensuring that the generative pixel independence assumption can still be taken advantage of . we demonstrate that our approach exhibits superior performance to classical generative forms of the lk algorithm . furthermore , we demonstrate its comparable performance to state-of-the-art methods such as the supervised descent method with substantially less training examples , as well as the unique ability to `` swap '' geometric warp functions without having to retrain from scratch . finally , from a theoretical perspective , our approach hints at possible redundancies that exist in current state-of-the-art methods for alignment that could be leveraged in vision systems of the future .", "topics": ["pixel"]}
{"title": "recovering articulated object models from 3d range data", "abstract": "we address the problem of unsupervised learning of complex articulated object models from 3d range data . we describe an algorithm whose input is a set of meshes corresponding to different configurations of an articulated object . the algorithm automatically recovers a decomposition of the object into approximately rigid parts , the location of the parts in the different object instances , and the articulated object skeleton linking the parts . our algorithm first registers allthe meshes using an unsupervised non-rigid technique described in a companion paper . it then segments the meshes using a graphical model that captures the spatial contiguity of parts . the segmentation is done using the em algorithm , iterating between finding a decomposition of the object into rigid parts , and finding the location of the parts in the object instances . although the graphical model is densely connected , the object decomposition step can be performed optimally and efficiently , allowing us to identify a large number of object parts while avoiding local maxima . we demonstrate the algorithm on real world datasets , recovering a 15-part articulated model of a human puppet from just 7 different puppet configurations , as well as a 4 part model of a fiexing arm where significant non-rigid deformation was present .", "topics": ["graphical model", "mathematical optimization"]}
{"title": "robust probabilistic predictive syntactic processing", "abstract": "this thesis presents a broad-coverage probabilistic top-down parser , and its application to the problem of language modeling for speech recognition . the parser builds fully connected derivations incrementally , in a single pass from left-to-right across the string . we argue that the parsing approach that we have adopted is well-motivated from a psycholinguistic perspective , as a model that captures probabilistic dependencies between lexical items , as part of the process of building connected syntactic structures . the basic parser and conditional probability models are presented , and empirical results are provided for its parsing accuracy on both newspaper text and spontaneous telephone conversations . modifications to the probability model are presented that lead to improved performance . a new language model which uses the output of the parser is then defined . perplexity and word error rate reduction are demonstrated over trigram models , even when the trigram is trained on significantly more data . interpolation on a word-by-word basis with a trigram model yields additional improvements .", "topics": ["parsing", "speech recognition"]}
{"title": "yuanfudao at semeval-2018 task 11 : three-way attention and relational knowledge for commonsense machine comprehension", "abstract": "this paper describes our system for semeval-2018 task 11 : machine comprehension using commonsense knowledge . we use three-way attentive networks ( trian ) to model interactions between the passage , question and answers . to incorporate commonsense knowledge , we augment input with relation embedding from the graph of general knowledge conceptnet ( speer et al . , 2017 ) . as a result , our system achieves 2nd place with 83.95 % accuracy on the official test data . code is publicly available at https : //github.com/intfloat/commonsense-rc", "topics": ["interaction"]}
{"title": "detecting and grouping identical objects for region proposal and classification", "abstract": "often multiple instances of an object occur in the same scene , for example in a warehouse . unsupervised multi-instance object discovery algorithms are able to detect and identify such objects . we use such an algorithm to provide object proposals to a convolutional neural network ( cnn ) based classifier . this results in fewer regions to evaluate , compared to traditional region proposal algorithms . additionally , it enables using the joint probability of multiple instances of an object , resulting in improved classification accuracy . the proposed technique can also split a single class into multiple sub-classes corresponding to the different object types , enabling hierarchical classification .", "topics": ["unsupervised learning"]}
{"title": "deep temporal linear encoding networks", "abstract": "the cnn-encoding of features from entire videos for the representation of human actions has rarely been addressed . instead , cnn work has focused on approaches to fuse spatial and temporal networks , but these were typically limited to processing shorter sequences . we present a new video representation , called temporal linear encoding ( tle ) and embedded inside of cnns as a new layer , which captures the appearance and motion throughout entire videos . it encodes this aggregated information into a robust video feature representation , via end-to-end learning . advantages of tles are : ( a ) they encode the entire video into a compact feature representation , learning the semantics and a discriminative feature space ; ( b ) they are applicable to all kinds of networks like 2d and 3d cnns for video classification ; and ( c ) they model feature interactions in a more expressive way and without loss of information . we conduct experiments on two challenging human action datasets : hmdb51 and ucf101 . the experiments show that tle outperforms current state-of-the-art methods on both datasets .", "topics": ["feature vector", "end-to-end principle"]}
{"title": "how to scale distributed deep learning ?", "abstract": "training time on large datasets for deep neural networks is the principal workflow bottleneck in a number of important applications of deep learning , such as object classification and detection in automatic driver assistance systems ( adas ) . to minimize training time , the training of a deep neural network must be scaled beyond a single machine to as many machines as possible by distributing the optimization method used for training . while a number of approaches have been proposed for distributed stochastic gradient descent ( sgd ) , at the current time synchronous approaches to distributed sgd appear to be showing the greatest performance at large scale . synchronous scaling of sgd suffers from the need to synchronize all processors on each gradient step and is not resilient in the face of failing or lagging processors . in asynchronous approaches using parameter servers , training is slowed by contention to the parameter server . in this paper we compare the convergence of synchronous and asynchronous sgd for training a modern resnet network architecture on the imagenet classification problem . we also propose an asynchronous method , gossiping sgd , that aims to retain the positive features of both systems by replacing the all-reduce collective operation of synchronous training with a gossip aggregation algorithm . we find , perhaps counterintuitively , that asynchronous sgd , including both elastic averaging and gossiping , converges faster at fewer nodes ( up to about 32 nodes ) , whereas synchronous sgd scales better to more nodes ( up to about 100 nodes ) .", "topics": ["gradient descent", "gradient"]}
{"title": "on the differential privacy of bayesian inference", "abstract": "we study how to communicate findings of bayesian inference to third parties , while preserving the strong guarantee of differential privacy . our main contributions are four different algorithms for private bayesian inference on proba-bilistic graphical models . these include two mechanisms for adding noise to the bayesian updates , either directly to the posterior parameters , or to their fourier transform so as to preserve update consistency . we also utilise a recently introduced posterior sampling mechanism , for which we prove bounds for the specific but general case of discrete bayesian networks ; and we introduce a maximum-a-posteriori private mechanism . our analysis includes utility and privacy bounds , with a novel focus on the influence of graph structure on privacy . worked examples and experiments with bayesian na { \\ '' i } ve bayes and bayesian linear regression illustrate the application of our mechanisms .", "topics": ["graphical model", "bayesian network"]}
{"title": "oriented response networks", "abstract": "deep convolution neural networks ( dcnns ) are capable of learning unprecedentedly effective image representations . however , their ability in handling significant local and global image rotations remains limited . in this paper , we propose active rotating filters ( arfs ) that actively rotate during convolution and produce feature maps with location and orientation explicitly encoded . an arf acts as a virtual filter bank containing the filter itself and its multiple unmaterialised rotated versions . during back-propagation , an arf is collectively updated using errors from all its rotated versions . dcnns using arfs , referred to as oriented response networks ( orns ) , can produce within-class rotation-invariant deep features while maintaining inter-class discrimination for classification tasks . the oriented response produced by orns can also be used for image and object orientation estimation tasks . over multiple state-of-the-art dcnn architectures , such as vgg , resnet , and stn , we consistently observe that replacing regular filters with the proposed arfs leads to significant reduction in the number of network parameters and improvement in classification performance . we report the best results on several commonly used benchmarks .", "topics": ["convolution", "map"]}
{"title": "neural machine translation via binary code prediction", "abstract": "in this paper , we propose a new method for calculating the output layer in neural machine translation systems . the method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case . in addition , we also introduce two advanced approaches to improve the robustness of the proposed model : using error-correcting codes and combining softmax and binary codes . experiments on two english-japanese bidirectional translation tasks show proposed models achieve bleu scores that approach the softmax , while reducing memory usage to the order of less than 1/10 and improving decoding speed on cpus by x5 to x10 .", "topics": ["time complexity", "machine translation"]}
{"title": "large-scale optimization algorithms for sparse conditional gaussian graphical models", "abstract": "this paper addresses the problem of scalable optimization for l1-regularized conditional gaussian graphical models . conditional gaussian graphical models generalize the well-known gaussian graphical models to conditional distributions to model the output network influenced by conditioning input variables . while highly scalable optimization methods exist for sparse gaussian graphical model estimation , state-of-the-art methods for conditional gaussian graphical models are not efficient enough and more importantly , fail due to memory constraints for very large problems . in this paper , we propose a new optimization procedure based on a newton method that efficiently iterates over two sub-problems , leading to drastic improvement in computation time compared to the previous methods . we then extend our method to scale to large problems under memory constraints , using block coordinate descent to limit memory usage while achieving fast convergence . using synthetic and genomic data , we show that our methods can solve one million dimensional problems to high accuracy in a little over a day on a single machine .", "topics": ["graphical model", "mathematical optimization"]}
{"title": "track and transfer : watching videos to simulate strong human supervision for weakly-supervised object detection", "abstract": "the status quo approach to training object detectors requires expensive bounding box annotations . our framework takes a markedly different direction : we transfer tracked object boxes from weakly-labeled videos to weakly-labeled images to automatically generate pseudo ground-truth boxes , which replace manually annotated bounding boxes . we first mine discriminative regions in the weakly-labeled image collection that frequently/rarely appear in the positive/negative images . we then match those regions to videos and retrieve the corresponding tracked object boxes . finally , we design a hough transform algorithm to vote for the best box to serve as the pseudo gt for each image , and use them to train an object detector . together , these lead to state-of-the-art weakly-supervised detection results on the pascal 2007 and 2010 datasets .", "topics": ["object detection", "ground truth"]}
{"title": "ucb algorithm for exponential distributions", "abstract": "we introduce in this paper a new algorithm for multi-armed bandit ( mab ) problems . a machine learning paradigm popular within cognitive network related topics ( e.g . , spectrum sensing and allocation ) . we focus on the case where the rewards are exponentially distributed , which is common when dealing with rayleigh fading channels . this strategy , named multiplicative upper confidence bound ( mucb ) , associates a utility index to every available arm , and then selects the arm with the highest index . for every arm , the associated index is equal to the product of a multiplicative factor by the sample mean of the rewards collected by this arm . we show that the mucb policy has a low complexity and is order optimal .", "topics": ["time complexity", "coefficient"]}
{"title": "end-to-end neural coreference resolution", "abstract": "we introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector . the key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each . the model computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism . it is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions . experiments demonstrate state-of-the-art performance , with a gain of 1.5 f1 on the ontonotes benchmark and by 3.1 f1 using a 5-model ensemble , despite the fact that this is the first approach to be successfully trained with no external resources .", "topics": ["parsing", "end-to-end principle"]}
{"title": "dictionary and image recovery from incomplete and random measurements", "abstract": "this paper tackles algorithmic and theoretical aspects of dictionary learning from incomplete and random block-wise image measurements and the performance of the adaptive dictionary for sparse image recovery . this problem is related to blind compressed sensing in which the sparsifying dictionary or basis is viewed as an unknown variable and subject to estimation during sparse recovery . however , unlike existing guarantees for a successful blind compressed sensing , our results do not rely on additional structural constraints on the learned dictionary or the measured signal . in particular , we rely on the spatial diversity of compressive measurements to guarantee that the solution is unique with a high probability . moreover , our distinguishing goal is to measure and reduce the estimation error with respect to the ideal dictionary that is based on the complete image . using recent results from random matrix theory , we show that applying a slightly modified dictionary learning algorithm over compressive measurements results in accurate estimation of the ideal dictionary for large-scale images . empirically , we experiment with both space-invariant and space-varying sensing matrices and demonstrate the critical role of spatial diversity in measurements . simulation results confirm that the presented algorithm outperforms the typical non-adaptive sparse recovery based on offline-learned universal dictionaries .", "topics": ["simulation", "sparse matrix"]}
{"title": "prediction by compression", "abstract": "it is well known that text compression can be achieved by predicting the next symbol in the stream of text data based on the history seen up to the current symbol . the better the prediction the more skewed the conditional probability distribution of the next symbol and the shorter the codeword that needs to be assigned to represent this next symbol . what about the opposite direction ? suppose we have a black box that can compress text stream . can it be used to predict the next symbol in the stream ? we introduce a criterion based on the length of the compressed data and use it to predict the next symbol . we examine empirically the prediction error rate and its dependency on some compression parameters .", "topics": ["text corpus", "eisenstein 's criterion"]}
{"title": "weakly supervised learning of affordances", "abstract": "localizing functional regions of objects or affordances is an important aspect of scene understanding . in this work , we cast the problem of affordance segmentation as that of semantic image segmentation . in order to explore various levels of supervision , we introduce a pixel-annotated affordance dataset of 3090 images containing 9916 object instances with rich contextual information in terms of human-object interactions . we use a deep convolutional neural network within an expectation maximization framework to take advantage of weakly labeled data like image level annotations or keypoint annotations . we show that a further reduction in supervision is possible with a minimal loss in performance when human pose is used as context .", "topics": ["image segmentation", "supervised learning"]}
{"title": "icnet for real-time semantic segmentation on high-resolution images", "abstract": "we focus on the challenging task of realtime semantic segmentation in this paper . it finds many practical applications and yet is with fundamental difficulty of reducing a large portion of computation for pixel-wise label inference . we propose an compressed-pspnet-based image cascade network ( icnet ) that incorporates multi-resolution branches under proper label guidance to address this challenge . we provide in-depth analysis of our framework and introduce the cascade feature fusion to quickly achieve high-quality segmentation . our system yields realtime inference on a single gpu card with decent quality results evaluated on challenging cityscapes dataset .", "topics": ["computation", "pixel"]}
{"title": "interactive cost configuration over decision diagrams", "abstract": "in many ai domains such as product configuration , a user should interactively specify a solution that must satisfy a set of constraints . in such scenarios , offline compilation of feasible solutions into a tractable representation is an important approach to delivering efficient backtrack-free user interaction online . in particular , binary decision diagrams ( bdds ) have been successfully used as a compilation target for product and service configuration . in this paper we discuss how to extend bdd-based configuration to scenarios involving cost functions which express user preferences . we first show that an efficient , robust and easy to implement extension is possible if the cost function is additive , and feasible solutions are represented using multi-valued decision diagrams ( mdds ) . we also discuss the effect on mdd size if the cost function is non-additive or if it is encoded explicitly into mdd . we then discuss interactive configuration in the presence of multiple cost functions . we prove that even in its simplest form , multiple-cost configuration is np-hard in the input mdd . however , for solving two-cost configuration we develop a pseudo-polynomial scheme and a fully polynomial approximation scheme . the applicability of our approach is demonstrated through experiments over real-world configuration models and product-catalogue datasets . response times are generally within a fraction of a second even for very large instances .", "topics": ["loss function", "polynomial"]}
{"title": "learning phrase representations using rnn encoder-decoder for statistical machine translation", "abstract": "in this paper , we propose a novel neural network model called rnn encoder-decoder that consists of two recurrent neural networks ( rnn ) . one rnn encodes a sequence of symbols into a fixed-length vector representation , and the other decodes the representation into another sequence of symbols . the encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence . the performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the rnn encoder-decoder as an additional feature in the existing log-linear model . qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .", "topics": ["recurrent neural network", "machine translation"]}
{"title": "dynamic decision process modeling and relation-line handling in distributed cooperative modeling system", "abstract": "the distributed cooperative modeling system ( dcms ) solves complex decision problems involving a lot of participants with different viewpoints by network based distributed modeling and multi-template aggregation . this thesis aims at extending the system with support for dynamic decision making process . first , the thesis presents a discussion of characteristics and optimal policy finding markov decision process as well as a brief introduction to dynamic bayesian decision network , which is inherently equal to mdp . after that , discussion and implementation of prediction in markov process for both discrete and continuous random variable are given , as well as several different kinds of correlation analysis among multiple indices which could help decision-makers to realize the interaction of indices and design appropriate policy . appending history data of macau industry , as the foundation of extending dcms , is introduced . additional works include rearrangement of graphical class hierarchy in dcms , which in turn allows convenient implementation of curve relation-line , which makes template modeling clearer and friendlier .", "topics": ["markov chain"]}
{"title": "track , then decide : category-agnostic vision-based multi-object tracking", "abstract": "the most common paradigm for vision-based multi-object tracking is tracking-by-detection , due to the availability of reliable detectors for several important object categories such as cars and pedestrians . however , future mobile systems will need a capability to cope with rich human-made environments , in which obtaining detectors for every possible object category would be infeasible . in this paper , we propose a model-free multi-object tracking approach that uses a category-agnostic image segmentation method to track objects . we present an efficient segmentation mask-based tracker which associates pixel-precise masks reported by the segmentation . our approach can utilize semantic information whenever it is available for classifying objects at the track level , while retaining the capability to track generic unknown objects in the absence of such information . we demonstrate experimentally that our approach achieves performance comparable to state-of-the-art tracking-by-detection methods for popular object categories such as cars and pedestrians . additionally , we show that the proposed method can discover and robustly track a large variety of other objects .", "topics": ["image segmentation", "computer vision"]}
{"title": "budget-aware activity detection with a recurrent policy network", "abstract": "in this paper , we address the challenging problem of effi- cient temporal activity detection in untrimmed long videos . while most recent work has focused and advanced the de- tection accuracy , the inference time can take seconds to minutes in processing one video , which is computationally prohibitive for many applications with tight runtime con- straints . this motivates our proposed budget-aware frame- work , which learns to perform activity detection by intel- ligently selecting a small subset of frames according to a specified time or computational budget . we formulate this problem as a markov decision process , and adopt a recurrent network to model a policy for the frame selec- tion . to train the network , we employ a recurrent policy gradient approach to approximate the gradient of the non- decomposable and non-differentiable objective defined in our problem . in the extensive experiments on two bench- mark datasets , we achieve competitive detection accuracy , and more importantly , our approach is able to substantially reduce computational time and detect multiple activities in only 348ms for each untrimmed long video of thumos14 and activitynet .", "topics": ["computational complexity theory", "time complexity"]}
{"title": "fomtrace : interactive video segmentation by image graphs and fuzzy object models", "abstract": "common users have changed from mere consumers to active producers of multimedia data content . video editing plays an important role in this scenario , calling for simple segmentation tools that can handle fast-moving and deformable video objects with possible occlusions , color similarities with the background , among other challenges . we present an interactive video segmentation method , named fomtrace , which addresses the problem in an effective and efficient way . from a user-provided object mask in a first frame , the method performs semi-automatic video segmentation on a spatiotemporal superpixel-graph , and then estimates a fuzzy object model ( fom ) , which refines segmentation of the second frame by constraining delineation on a pixel-graph within a region where the object 's boundary is expected to be . the user can correct/accept the refined object mask in the second frame , which is then similarly used to improve the spatiotemporal video segmentation of the remaining frames . both steps are repeated alternately , within interactive response times , until the segmentation refinement of the final frame is accepted by the user . extensive experiments demonstrate fomtrace 's ability for tracing objects in comparison with state-of-the-art approaches for interactive video segmentation , supervised , and unsupervised object tracking .", "topics": ["image segmentation", "pixel"]}
{"title": "concept formation and dynamics of repeated inference in deep generative models", "abstract": "deep generative models are reported to be useful in broad applications including image generation . repeated inference between data space and latent space in these models can denoise cluttered images and improve the quality of inferred results . however , previous studies only qualitatively evaluated image outputs in data space , and the mechanism behind the inference has not been investigated . the purpose of the current study is to numerically analyze changes in activity patterns of neurons in the latent space of a deep generative model called a `` variational auto-encoder '' ( vae ) . what kinds of inference dynamics the vae demonstrates when noise is added to the input data are identified . the vae embeds a dataset with clear cluster structures in the latent space and the center of each cluster of multiple correlated data points ( memories ) is referred as the concept . our study demonstrated that transient dynamics of inference first approaches a concept , and then moves close to a memory . moreover , the vae revealed that the inference dynamics approaches a more abstract concept to the extent that the uncertainty of input data increases due to noise . it was demonstrated that by increasing the number of the latent variables , the trend of the inference dynamics to approach a concept can be enhanced , and the generalization ability of the vae can be improved .", "topics": ["generative model", "calculus of variations"]}
{"title": "multi-task video captioning with video and entailment generation", "abstract": "video captioning , the task of describing the content of a video , has seen some promising improvements in recent years with sequence-to-sequence models , but accurately learning the temporal and logical dynamics involved in the task still remains a challenge , especially given the lack of sufficient annotated data . we improve video captioning by sharing knowledge with two related directed-generation tasks : a temporally-directed unsupervised video prediction task to learn richer context-aware video encoder representations , and a logically-directed language entailment generation task to learn better video-entailed caption decoder representations . for this , we present a many-to-many multi-task learning model that shares parameters across the encoders and decoders of the three tasks . we achieve significant improvements and the new state-of-the-art on several standard video captioning datasets using diverse automatic and human evaluations . we also show mutual multi-task improvements on the entailment generation task .", "topics": ["encoder"]}
{"title": "beyond owl 2 ql in obda : rewritings and approximations ( extended version )", "abstract": "ontology-based data access ( obda ) is a novel paradigm facilitating access to relational data , realized by linking data sources to an ontology by means of declarative mappings . dl-lite_r , which is the logic underpinning the w3c ontology language owl 2 ql and the current language of choice for obda , has been designed with the goal of delegating query answering to the underlying database engine , and thus is restricted in expressive power . e.g . , it does not allow one to express disjunctive information , and any form of recursion on the data . the aim of this paper is to overcome these limitations of dl-lite_r , and extend obda to more expressive ontology languages , while still leveraging the underlying relational technology for query answering . we achieve this by relying on two well-known mechanisms , namely conservative rewriting and approximation , but significantly extend their practical impact by bringing into the picture the mapping , an essential component of obda . specifically , we develop techniques to rewrite obda specifications with an expressive ontology to `` equivalent '' ones with a dl-lite_r ontology , if possible , and to approximate them otherwise . we do so by exploiting the high expressive power of the mapping layer to capture part of the domain semantics of rich ontology languages . we have implemented our techniques in the prototype system ontoprox , making use of the state-of-the-art obda system ontop and the query answering system clipper , and we have shown their feasibility and effectiveness with experiments on synthetic and real-world data .", "topics": ["approximation algorithm", "synthetic data"]}
{"title": "detection-aided liver lesion segmentation using deep learning", "abstract": "a fully automatic technique for segmenting the liver and localizing its unhealthy tissues is a convenient tool in order to diagnose hepatic diseases and assess the response to the according treatments . in this work we propose a method to segment the liver and its lesions from computed tomography ( ct ) scans using convolutional neural networks ( cnns ) , that have proven good results in a variety of computer vision tasks , including medical imaging . the network that segments the lesions consists of a cascaded architecture , which first focuses on the region of the liver in order to segment the lesions on it . moreover , we train a detector to localize the lesions , and mask the results of the segmentation network with the positive detections . the segmentation architecture is based on driu , a fully convolutional network ( fcn ) with side outputs that work on feature maps of different resolutions , to finally benefit from the multi-scale information learned by different stages of the network . the main contribution of this work is the use of a detector to localize the lesions , which we show to be beneficial to remove false positives triggered by the segmentation network . source code and models are available at https : //imatge-upc.github.io/liverseg-2017-nipsws/ .", "topics": ["neural networks", "computer vision"]}
{"title": "learning to segment object proposals via recursive neural networks", "abstract": "to avoid the exhaustive search over locations and scales , current state-of-the-art object detection systems usually involve a crucial component generating a batch of candidate object proposals from images . in this paper , we present a simple yet effective approach for segmenting object proposals via a deep architecture of recursive neural networks ( rnns ) , which hierarchically groups regions for detecting object candidates over scales . unlike traditional methods that mainly adopt fixed similarity measures for merging regions or finding object proposals , our approach adaptively learns the region merging similarity and the objectness measure during the process of hierarchical region grouping . specifically , guided by a structured loss , the rnn model jointly optimizes the cross-region similarity metric with the region merging process as well as the objectness prediction . during inference of the object proposal generation , we introduce randomness into the greedy search to cope with the ambiguity of grouping regions . extensive experiments on standard benchmarks , e.g . , pascal voc and imagenet , suggest that our approach is capable of producing object proposals with high recall while well preserving the object boundaries and outperforms other existing methods in both accuracy and efficiency .", "topics": ["object detection", "neural networks"]}
{"title": "nonparametric latent tree graphical models : inference , estimation , and structure learning", "abstract": "tree structured graphical models are powerful at expressing long range or hierarchical dependency among many variables , and have been widely applied in different areas of computer science and statistics . however , existing methods for parameter estimation , inference , and structure learning mainly rely on the gaussian or discrete assumptions , which are restrictive under many applications . in this paper , we propose new nonparametric methods based on reproducing kernel hilbert space embeddings of distributions that can recover the latent tree structures , estimate the parameters , and perform inference for high dimensional continuous and non-gaussian variables . the usefulness of the proposed methods are illustrated by thorough numerical results .", "topics": ["graphical model", "numerical analysis"]}
{"title": "a transformational characterization of markov equivalence for directed acyclic graphs with latent variables", "abstract": "different directed acyclic graphs ( dags ) may be markov equivalent in the sense that they entail the same conditional independence relations among the observed variables . chickering ( 1995 ) provided a transformational characterization of markov equivalence for dags ( with no latent variables ) , which is useful in deriving properties shared by markov equivalent dags , and , with certain generalization , is needed to prove the asymptotic correctness of a search procedure over markov equivalence classes , known as the ges algorithm . for dag models with latent variables , maximal ancestral graphs ( mags ) provide a neat representation that facilitates model search . however , no transformational characterization -- analogous to chickering 's -- of markov equivalent mags is yet available . this paper establishes such a characterization for directed mags , which we expect will have similar uses as it does for dags .", "topics": ["markov chain"]}
{"title": "efficient sparse group feature selection via nonconvex optimization", "abstract": "sparse feature selection has been demonstrated to be effective in handling high-dimensional data . while promising , most of the existing works use convex methods , which may be suboptimal in terms of the accuracy of feature selection and parameter estimation . in this paper , we expand a nonconvex paradigm to sparse group feature selection , which is motivated by applications that require identifying the underlying group structure and performing feature selection simultaneously . the main contributions of this article are twofold : ( 1 ) statistically , we introduce a nonconvex sparse group feature selection model which can reconstruct the oracle estimator . therefore , consistent feature selection and parameter estimation can be achieved ; ( 2 ) computationally , we propose an efficient algorithm that is applicable to large-scale problems . numerical results suggest that the proposed nonconvex method compares favorably against its competitors on synthetic data and real-world applications , thus achieving desired goal of delivering high performance .", "topics": ["numerical analysis", "synthetic data"]}
{"title": "finding consensus bayesian network structures", "abstract": "suppose that multiple experts ( or learning algorithms ) provide us with alternative bayesian network ( bn ) structures over a domain , and that we are interested in combining them into a single consensus bn structure . specifically , we are interested in that the consensus bn structure only represents independences all the given bn structures agree upon and that it has as few parameters associated as possible . in this paper , we prove that there may exist several non-equivalent consensus bn structures and that finding one of them is np-hard . thus , we decide to resort to heuristics to find an approximated consensus bn structure . in this paper , we consider the heuristic proposed in \\citep { matzkevichandabramson1992 , matzkevichandabramson1993a , matzkevichandabramson1993b } . this heuristic builds upon two algorithms , called methods a and b , for efficiently deriving the minimal directed independence map of a bn structure relative to a given node ordering . methods a and b are claimed to be correct although no proof is provided ( a proof is just sketched ) . in this paper , we show that methods a and b are not correct and propose a correction of them .", "topics": ["bayesian network", "heuristic"]}
{"title": "neural networks classifier for data selection in statistical machine translation", "abstract": "we address the data selection problem in statistical machine translation ( smt ) as a classification task . the new data selection method is based on a neural network classifier . we present a new method description and empirical results proving that our data selection method provides better translation quality , compared to a state-of-the-art method ( i.e . , cross entropy ) . moreover , the empirical results reported are coherent across different language pairs .", "topics": ["machine translation", "neural networks"]}
{"title": "automatic knee osteoarthritis diagnosis from plain radiographs : a deep learning-based approach", "abstract": "knee osteoarthritis ( oa ) is the most common musculoskeletal disorder . oa diagnosis is currently conducted by assessing symptoms and evaluating plain radiographs , but this process suffers from subjectivity . in this study , we present a new transparent computer-aided diagnosis method based on the deep siamese convolutional neural network to automatically score knee oa severity according to the kellgren-lawrence grading scale . we trained our method using the data solely from the multicenter osteoarthritis study and validated it on randomly selected 3,000 subjects ( 5,960 knees ) from osteoarthritis initiative dataset . our method yielded a quadratic kappa coefficient of 0.83 and average multiclass accuracy of 66.71\\ % compared to the annotations given by a committee of clinical experts . here , we also report a radiological oa diagnosis area under the roc curve of 0.93 . we also present attention maps -- given as a class probability distribution -- highlighting the radiological features affecting the network decision . this information makes the decision process transparent for the practitioner , which builds better trust toward automatic methods . we believe that our model is useful for clinical decision making and for oa research ; therefore , we openly release our training codes and the data set created in this study .", "topics": ["map"]}
{"title": "dictionary learning for deblurring and digital zoom", "abstract": "this paper proposes a novel approach to image deblurring and digital zooming using sparse local models of image appearance . these models , where small image patches are represented as linear combinations of a few elements drawn from some large set ( dictionary ) of candidates , have proven well adapted to several image restoration tasks . a key to their success has been to learn dictionaries adapted to the reconstruction of small image patches . in contrast , recent works have proposed instead to learn dictionaries which are not only adapted to data reconstruction , but also tuned for a specific task . we introduce here such an approach to deblurring and digital zoom , using pairs of blurry/sharp ( or low-/high-resolution ) images for training , as well as an effective stochastic gradient algorithm for solving the corresponding optimization task . although this learning problem is not convex , once the dictionaries have been learned , the sharp/high-resolution image can be recovered via convex optimization at test time . experiments with synthetic and real data demonstrate the effectiveness of the proposed approach , leading to state-of-the-art performance for non-blind image deblurring and digital zoom .", "topics": ["synthetic data", "sparse matrix"]}
{"title": "convex learning of multiple tasks and their structure", "abstract": "reducing the amount of human supervision is a key problem in machine learning and a natural approach is that of exploiting the relations ( structure ) among different tasks . this is the idea at the core of multi-task learning . in this context a fundamental question is how to incorporate the tasks structure in the learning problem.we tackle this question by studying a general computational framework that allows to encode a-priori knowledge of the tasks structure in the form of a convex penalty ; in this setting a variety of previously proposed methods can be recovered as special cases , including linear and non-linear approaches . within this framework , we show that tasks and their structure can be efficiently learned considering a convex optimization problem that can be approached by means of block coordinate methods such as alternating minimization and for which we prove convergence to the global minimum .", "topics": ["optimization problem", "nonlinear system"]}
{"title": "a batchwise monotone algorithm for dictionary learning", "abstract": "we propose a batchwise monotone algorithm for dictionary learning . unlike the state-of-the-art dictionary learning algorithms which impose sparsity constraints on a sample-by-sample basis , we instead treat the samples as a batch , and impose the sparsity constraint on the whole . the benefit of batchwise optimization is that the non-zeros can be better allocated across the samples , leading to a better approximation of the whole . to accomplish this , we propose procedures to switch non-zeros in both rows and columns in the support of the coefficient matrix to reduce the reconstruction error . we prove in the proposed support switching procedure the objective of the algorithm , i.e . , the reconstruction error , decreases monotonically and converges . furthermore , we introduce a block orthogonal matching pursuit algorithm that also operates on sample batches to provide a warm start . experiments on both natural image patches and uci data sets show that the proposed algorithm produces a better approximation with the same sparsity levels compared to the state-of-the-art algorithms .", "topics": ["sparse matrix", "dictionary"]}
{"title": "sliding window approach based text binarisation from complex textual images", "abstract": "text binarisation process classifies individual pixels as text or background in the textual images . binarization is necessary to bridge the gap between localization and recognition by ocr . this paper presents sliding window method to binarise text from textual images with textured background . suitable preprocessing techniques are applied first to increase the contrast of the image and blur the background noises due to textured background . then edges are detected by iterative thresholding . subsequently formed edge boxes are analyzed to remove unwanted edges due to complex background and binarised by sliding window approach based character size uniformity check algorithm . the proposed method has been applied on localized region from heterogeneous textual images and compared with otsu , niblack methods and shown encouraging performance of the proposed method .", "topics": ["pixel"]}
{"title": "convergence and error bounds for universal prediction of nonbinary sequences", "abstract": "solomonoff 's uncomputable universal prediction scheme $ \\xi $ allows to predict the next symbol $ x_k $ of a sequence $ x_1 ... x_ { k-1 } $ for any turing computable , but otherwise unknown , probabilistic environment $ \\mu $ . this scheme will be generalized to arbitrary environmental classes , which , among others , allows the construction of computable universal prediction schemes $ \\xi $ . convergence of $ \\xi $ to $ \\mu $ in a conditional mean squared sense and with $ \\mu $ probability 1 is proven . it is shown that the average number of prediction errors made by the universal $ \\xi $ scheme rapidly converges to those made by the best possible informed $ \\mu $ scheme . the schemes , theorems and proofs are given for general finite alphabet , which results in additional complications as compared to the binary case . several extensions of the presented theory and results are outlined . they include general loss functions and bounds , games of chance , infinite alphabet , partial and delayed prediction , classification , and more active systems .", "topics": ["loss function"]}
{"title": "deep learning multi-view representation for face recognition", "abstract": "various factors , such as identities , views ( poses ) , and illuminations , are coupled in face images . disentangling the identity and view representations is a major challenge in face recognition . existing face recognition systems either use handcrafted features or learn features discriminatively to improve recognition accuracy . this is different from the behavior of human brain . intriguingly , even without accessing 3d data , human not only can recognize face identity , but can also imagine face images of a person under different viewpoints given a single 2d image , making face perception in the brain robust to view changes . in this sense , human brain has learned and encoded 3d face models from 2d images . to take into account this instinct , this paper proposes a novel deep neural net , named multi-view perceptron ( mvp ) , which can untangle the identity and view features , and infer a full spectrum of multi-view images in the meanwhile , given a single 2d face image . the identity features of mvp achieve superior performance on the multipie dataset . mvp is also capable to interpolate and predict images under viewpoints that are unobserved in the training data .", "topics": ["test set"]}
{"title": "discovering containment : from infants to machines", "abstract": "current artificial learning systems can recognize thousands of visual categories , or play go at a champion '' s level , but can not explain infants learning , in particular the ability to learn complex concepts without guidance , in a specific order . a notable example is the category of 'containers ' and the notion of containment , one of the earliest spatial relations to be learned , starting already at 2.5 months , and preceding other common relations ( e.g . , support ) . such spontaneous unsupervised learning stands in contrast with current highly successful computational models , which learn in a supervised manner , that is , by using large data sets of labeled examples . how can meaningful concepts be learned without guidance , and what determines the trajectory of infant learning , making some notions appear consistently earlier than others ?", "topics": ["unsupervised learning", "artificial intelligence"]}
{"title": "seqface : make full use of sequence information for face recognition", "abstract": "deep convolutional neural networks ( cnns ) have greatly improved the face recognition ( fr ) performance in recent years . almost all cnns in fr are trained on the carefully labeled datasets containing plenty of identities . however , such high-quality datasets are very expensive to collect , which restricts many researchers to achieve state-of-the-art performance . in this paper , we propose a framework , called seqface , for learning discriminative face features . besides a traditional identity training dataset , the designed seqface can train cnns by using an additional dataset which includes a large number of face sequences collected from videos . moreover , the label smoothing regularization ( lsr ) and a new proposed discriminative sequence agent ( dsa ) loss are employed to enhance discrimination power of deep face features via making full use of the sequence data . our method achieves excellent performance on labeled faces in the wild ( lfw ) , youtube faces ( ytf ) , only with a single resnet . the code and models are publicly available on-line ( https : //github.com/huangyangyu/seqface ) .", "topics": ["matrix regularization"]}
{"title": "robust structure from motion in the presence of outliers and missing data", "abstract": "structure from motion is an import theme in computer vision . although great progress has been made both in theory and applications , most of the algorithms only work for static scenes and rigid objects . in recent years , structure and motion recovery of non-rigid objects and dynamic scenes have received a lot of attention . in this paper , the state-of-the-art techniques for structure and motion factorization of non-rigid objects are reviewed and discussed . first , an introduction of the structure from motion problem is presented , followed by a general formulation of non-rigid structure from motion . second , an augmented affined factorization framework , by using homogeneous representation , is presented to solve the registration issue in the presence of outlying and missing data . third , based on the observation that the reprojection residuals of outliers are significantly larger than those of inliers , a robust factorization strategy with outlier rejection is proposed by means of the reprojection residuals , followed by some comparative experimental evaluations . finally , some future research topics in non-rigid structure from motion are discussed .", "topics": ["synthetic data"]}
{"title": "large-scale shape retrieval with sparse 3d convolutional neural networks", "abstract": "in this paper we present results of performance evaluation of s3dcnn - a sparse 3d convolutional neural network - on a large-scale 3d shape benchmark modelnet40 , and measure how it is impacted by voxel resolution of input shape . we demonstrate comparable classification and retrieval performance to state-of-the-art models , but with much less computational costs in training and inference phases . we also notice that benefits of higher input resolution can be limited by an ability of a neural network to generalize high level features .", "topics": ["sparse matrix"]}
{"title": "synthesis-based robust low resolution face recognition", "abstract": "recognition of low resolution face images is a challenging problem in many practical face recognition systems . methods have been proposed in the face recognition literature for the problem which assume that the probe is low resolution , but a high resolution gallery is available for recognition . these attempts have been aimed at modifying the probe image such that the resultant image provides better discrimination . we formulate the problem differently by leveraging the information available in the high resolution gallery image and propose a dictionary learning approach for classifying the low-resolution probe image . an important feature of our algorithm is that it can handle resolution change along with illumination variations . furthermore , we also kernelize the algorithm to handle non-linearity in data and present a joint dictionary learning technique for robust recognition at low resolutions . the effectiveness of the proposed method is demonstrated using standard datasets and a challenging outdoor face dataset . it is shown that our method is efficient and can perform significantly better than many competitive low resolution face recognition algorithms .", "topics": ["nonlinear system", "sparse matrix"]}
{"title": "a model-based approach to predicting predator-prey & friend-foe relationships in ant colonies", "abstract": "understanding predator-prey relationships among insects is a challenging task in the domain of insect-colony research . this is due to several factors involved , such as determining whether a particular behavior is the result of a predator-prey interaction , a friend-foe interaction or another kind of interaction . in this paper , we analyze a series of predator-prey and friend-foe interactions in two colonies of carpenter ants to better understand and predict such behavior . using the data gathered , we have also come up with a preliminary model for predicting such behavior under the specific conditions the experiment was conducted in . in this paper , we present the results of our data analysis as well as an overview of the processes involved .", "topics": ["interaction"]}
{"title": "design and classification of dynamic multi-objective optimization problems", "abstract": "in this work we provide a formal model for the different time-dependent components that can appear in dynamic multi-objective optimization problems , along with a classification of these components . four main classes are identified , corresponding to the influence of the parameters , objective functions , previous states of the dynamic system and , last , environment changes , which in turn lead to online optimization problems . for illustration purposes , examples are provided for each class identified - by no means standing as the most representative ones or exhaustive in scope .", "topics": ["mathematical optimization"]}
{"title": "scalable bayesian optimization using deep neural networks", "abstract": "bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations . it relies on querying a distribution over functions defined by a relatively cheap surrogate model . an accurate model for this distribution over functions is critical to the effectiveness of the approach , and is typically fit using gaussian processes ( gps ) . however , since gps scale cubically with the number of observations , it has been challenging to handle objectives whose optimization requires many evaluations , and as such , massively parallelizing the optimization . in this work , we explore the use of neural networks as an alternative to gps to model distributions over functions . we show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art gp-based approaches , but scales linearly with the number of data rather than cubically . this allows us to achieve a previously intractable degree of parallelism , which we apply to large scale hyperparameter optimization , rapidly finding competitive models on benchmark object recognition tasks using convolutional networks , and image caption generation using neural language models .", "topics": ["neural networks"]}
{"title": "adobe-mit submission to the dstc 4 spoken language understanding pilot task", "abstract": "the dialog state tracking challenge 4 ( dstc 4 ) proposes several pilot tasks . in this paper , we focus on the spoken language understanding pilot task , which consists of tagging a given utterance with speech acts and semantic slots . we compare different classifiers : the best system obtains 0.52 and 0.67 f1-scores on the test set for speech act recognition for the tourist and the guide respectively , and 0.52 f1-score for semantic tagging for both the guide and the tourist .", "topics": ["test set", "statistical classification"]}
{"title": "an ensemble of machine learning and anti-learning methods for predicting tumour patient survival rates", "abstract": "this paper primarily addresses a dataset relating to cellular , chemical and physical conditions of patients gathered at the time they are operated upon to remove colorectal tumours . this data provides a unique insight into the biochemical and immunological status of patients at the point of tumour removal along with information about tumour classification and post-operative survival . the relationship between severity of tumour , based on tnm staging , and survival is still unclear for patients with tnm stage 2 and 3 tumours . we ask whether it is possible to predict survival rate more accurately using a selection of machine learning techniques applied to subsets of data to gain a deeper understanding of the relationships between a patient 's biochemical markers and survival . we use a range of feature selection and single classification techniques to predict the 5 year survival rate of tnm stage 2 and 3 patients which initially produces less than ideal results . the performance of each model individually is then compared with subsets of the data where agreement is reached for multiple models . this novel method of selective ensembling demonstrates that significant improvements in model accuracy on an unseen test set can be achieved for patients where agreement between models is achieved . finally we point at a possible method to identify whether a patients prognosis can be accurately predicted or not .", "topics": ["test set"]}
{"title": "very strict selectional restrictions", "abstract": "we discuss the characteristics and behaviour of two parallel classes of verbs in two romance languages , french and portuguese . examples of these verbs are port . abater [ gado ] and fr . abattre [ b\\'etail ] , both meaning `` slaughter [ cattle ] '' . in both languages , the definition of the class of verbs includes several features : - they have only one essential complement , which is a direct object . - the nominal distribution of the complement is very limited , i.e . , few nouns can be selected as head nouns of the complement . however , this selection is not restricted to a single noun , as would be the case for verbal idioms such as fr . monter la garde `` mount guard '' . - we excluded from the class constructions which are reductions of more complex constructions , e.g . port . afinar [ instrumento ] com `` tune [ instrument ] with '' .", "topics": ["natural language processing"]}
{"title": "order matters : distributional properties of speech to young children bootstraps learning of semantic representations", "abstract": "some researchers claim that language acquisition is critically dependent on experiencing linguistic input in order of increasing complexity . we set out to test this hypothesis using a simple recurrent neural network ( srn ) trained to predict word sequences in childes , a 5-million-word corpus of speech directed to children . first , we demonstrated that age-ordered childes exhibits a gradual increase in linguistic complexity . next , we compared the performance of two groups of srns trained on childes which had either been age-ordered or not . specifically , we assessed learning of grammatical and semantic structure and showed that training on age-ordered input facilitates learning of semantic , but not of sequential structure . we found that this advantage is eliminated when the models were trained on input with utterance boundary information removed .", "topics": ["recurrent neural network"]}
{"title": "modeling transportation routines using hybrid dynamic mixed networks", "abstract": "this paper describes a general framework called hybrid dynamic mixed networks ( hdmns ) which are hybrid dynamic bayesian networks that allow representation of discrete deterministic information in the form of constraints . we propose approximate inference algorithms that integrate and adjust well known algorithmic principles such as generalized belief propagation , rao-blackwellised particle filtering and constraint propagation to address the complexity of modeling and reasoning in hdmns . we use this framework to model a person 's travel activity over time and to predict destination and routes given the current location . we present a preliminary empirical evaluation demonstrating the effectiveness of our modeling framework and algorithms using several variants of the activity model .", "topics": ["approximation algorithm", "bayesian network"]}
{"title": "combining recurrent and convolutional neural networks for relation classification", "abstract": "this paper investigates two different neural architectures for the task of relation classification : convolutional neural networks and recurrent neural networks . for both models , we demonstrate the effect of different architectural choices . we present a new context representation for convolutional neural networks for relation classification ( extended middle context ) . furthermore , we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization . finally , we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results . our neural models achieve state-of-the-art results on the semeval 2010 relation classification task .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "a benchmark and evaluation of non-rigid structure from motion", "abstract": "non-rigid structure from motion ( nrsfm ) , is a long standing and central problem in computer vision , allowing us to obtain 3d information from multiple images when the scene is dynamic . a main issue regarding the further development of this important computer vision topic , is the lack of high quality data sets . we here address this issue by presenting of data set compiled for this purpose , which is made publicly available , and considerably larger than previous state of the art . to validate the applicability of this data set , and provide and investigation into the state of the art of nrsfm , including potential directions forward , we here present a benchmark and a scrupulous evaluation using this data set . this benchmark evaluates 16 different methods with available code , which we argue reasonably spans the state of the art in nrsfm . we also hope , that the presented and public data set and evaluation , will provide benchmark tools for further development in this field .", "topics": ["computer vision"]}
{"title": "a hebbian/anti-hebbian neural network for linear subspace learning : a derivation from multidimensional scaling of streaming data", "abstract": "neural network models of early sensory processing typically reduce the dimensionality of streaming input data . such networks learn the principal subspace , in the sense of principal component analysis ( pca ) , by adjusting synaptic weights according to activity-dependent learning rules . when derived from a principled cost function these rules are nonlocal and hence biologically implausible . at the same time , biologically plausible local rules have been postulated rather than derived from a principled cost function . here , to bridge this gap , we derive a biologically plausible network for subspace learning on streaming data by minimizing a principled cost function . in a departure from previous work , where cost was quantified by the representation , or reconstruction , error , we adopt a multidimensional scaling ( mds ) cost function for streaming data . the resulting algorithm relies only on biologically plausible hebbian and anti-hebbian local learning rules . in a stochastic setting , synaptic weights converge to a stationary state which projects the input data onto the principal subspace . if the data are generated by a nonstationary distribution , the network can track the principal subspace . thus , our result makes a step towards an algorithmic theory of neural computation .", "topics": ["loss function", "computation"]}
{"title": "estimation of linear , non-gaussian causal models in the presence of confounding latent variables", "abstract": "the estimation of linear causal models ( also known as structural equation models ) from data is a well-known problem which has received much attention in the past . most previous work has , however , made an explicit or implicit assumption of gaussianity , limiting the identifiability of the models . we have recently shown ( shimizu et al , 2005 ; hoyer et al , 2006 ) that for non-gaussian distributions the full causal model can be estimated in the no hidden variables case . in this contribution , we discuss the estimation of the model when confounding latent variables are present . although in this case uniqueness is no longer guaranteed , there is at most a finite set of models which can fit the data . we develop an algorithm for estimating this set , and describe numerical simulations which confirm the theoretical arguments and demonstrate the practical viability of the approach . full matlab code is provided for all simulations .", "topics": ["numerical analysis", "simulation"]}
{"title": "challenges in persian electronic text analysis", "abstract": "farsi , also known as persian , is the official language of iran and tajikistan and one of the two main languages spoken in afghanistan . farsi enjoys a unified arabic script as its writing system . in this paper we briefly introduce the writing standards of farsi and highlight problems one would face when analyzing farsi electronic texts , especially during development of farsi corpora regarding to transcription and encoding of farsi e-texts . the pointes mentioned may sounds easy but they are crucial when developing and processing written corpora of farsi .", "topics": ["text corpus"]}
{"title": "a case study in complexity estimation : towards parallel branch-and-bound over graphical models", "abstract": "we study the problem of complexity estimation in the context of parallelizing an advanced branch and bound-type algorithm over graphical models . the algorithm 's pruning power makes load balancing , one crucial element of every distributed system , very challenging . we propose using a statistical regression model to identify and tackle disproportionally complex parallel subproblems , the cause of load imbalance , ahead of time . the proposed model is evaluated and analyzed on various levels and shown to yield robust predictions . we then demonstrate its effectiveness for load balancing in practice .", "topics": ["graphical model"]}
{"title": "global convergence of the ( 1+1 ) evolution strategy", "abstract": "we establish global convergence of the ( 1+1 ) -es algorithm , i.e . , convergence to a critical point independent of the initial state . the analysis is based on two ingredients . we establish a sufficient decrease condition for elitist , rank-based evolutionary algorithms , formulated for an essentially monotonically transformed variant of the objective function . this tool is of general value , and it is therefore formulated for general search spaces . to make it applicable to the ( 1+1 ) -es , we show that the algorithm state is found infinitely often in a regime where step size and success rate are simultaneously bounded away from zero , with full probability . the main result is proven by combining both statements . under minimal technical preconditions , the theorem ensures that the sequence of iterates has a limit point that can not be improved in the limit of vanishing step size , a generalization of the notion of critical points of smooth functions . importantly , our analysis reflects the actual dynamics of the algorithm and hence supports our understanding of its mechanisms , in particular success-based step size control . we apply the theorem to the analysis of the optimization behavior of the ( 1+1 ) -es on various problems ranging from the smooth ( non-convex ) cases over different types of saddle points and ridge functions to discontinuous and extremely rugged problems .", "topics": ["optimization problem", "loss function"]}
{"title": "a data mining approach to the diagnosis of tuberculosis by cascading clustering and classification", "abstract": "in this paper , a methodology for the automated detection and classification of tuberculosis ( tb ) is presented . tuberculosis is a disease caused by mycobacterium which spreads through the air and attacks low immune bodies easily . our methodology is based on clustering and classification that classifies tb into two categories , pulmonary tuberculosis ( ptb ) and retroviral ptb ( rptb ) that is those with human immunodeficiency virus ( hiv ) infection . initially k-means clustering is used to group the tb data into two clusters and assigns classes to clusters . subsequently multiple different classification algorithms are trained on the result set to build the final classifier model based on k-fold cross validation method . this methodology is evaluated using 700 raw tb data obtained from a city hospital . the best obtained accuracy was 98.7 % from support vector machine ( svm ) compared to other classifiers . the proposed approach helps doctors in their diagnosis decisions and also in their treatment planning procedures for different categories .", "topics": ["cluster analysis", "data mining"]}
{"title": "estimating well-performing bayesian networks using bernoulli mixtures", "abstract": "a novel method for estimating bayesian network ( bn ) parameters from data is presented which provides improved performance on test data . previous research has shown the value of representing conditional probability distributions ( cpds ) via neural networks ( neal 1992 ) , noisy-or gates ( neal 1992 , diez 1993 ) and decision trees ( friedman and goldszmidt 1996 ) .the bernoulli mixture network ( bmn ) explicitly represents the cpds of discrete bn nodes as mixtures of local distributions , each having a different set of parents.this increases the space of possible structures which can be considered , enabling the cpds to have finer-grained dependencies.the resulting estimation procedure induces a modelthat is better able to emulate the underlying interactions occurring in the data than conventional conditional bernoulli network models.the results for artificially generated data indicate that overfitting is best reduced by restricting the complexity of candidate mixture substructures local to each node . furthermore , mixtures of very simple substructures can perform almost as well as more complex ones.the bmn is also applied to data collected from an online adventure game with an application to keyhole plan recognition . the results show that the bmn-based model brings a dramatic improvement in performance over a conventional bn model .", "topics": ["interaction", "bayesian network"]}
{"title": "texture classification in extreme scale variations using ganet", "abstract": "research in texture recognition often concentrates on recognizing textures with intraclass variations such as illumination , rotation , viewpoint and small scale changes . in contrast , in real-world applications a change in scale can have a dramatic impact on texture appearance , to the point of changing completely from one texture category to another . as a result , texture variations due to changes in scale are amongst the hardest to handle . in this work we conduct the first study of classifying textures with extreme variations in scale . to address this issue , we first propose and then reduce scale proposals on the basis of dominant texture patterns . motivated by the challenges posed by this problem , we propose a new ganet network where we use a genetic algorithm to change the units in the hidden layers during network training , in order to promote the learning of more informative semantic texture patterns . finally , we adopt a fvcnn ( fisher vector pooling of a convolutional neural network filter bank ) feature encoder for global texture representation . because extreme scale variations are not necessarily present in most standard texture databases , to support the proposed extreme-scale aspects of texture understanding we are developing a new dataset , the extreme scale variation textures ( esvat ) , to test the performance of our framework . it is demonstrated that the proposed framework significantly outperforms gold-standard texture features by more than 10 % on esvat . we also test the performance of our proposed approach on the kthtips2b and os datasets and a further dataset synthetically derived from forrest , showing superior performance compared to the state of the art .", "topics": ["computer vision", "database"]}
{"title": "efficient and invariant convolutional neural networks for dense prediction", "abstract": "convolutional neural networks have shown great success on feature extraction from raw input data such as images . although convolutional neural networks are invariant to translations on the inputs , they are not invariant to other transformations , including rotation and flip . recent attempts have been made to incorporate more invariance in image recognition applications , but they are not applicable to dense prediction tasks , such as image segmentation . in this paper , we propose a set of methods based on kernel rotation and flip to enable rotation and flip invariance in convolutional neural networks . the kernel rotation can be achieved on kernels of 3 $ \\times $ 3 , while kernel flip can be applied on kernels of any size . by rotating in eight or four angles , the convolutional layers could produce the corresponding number of feature maps based on eight or four different kernels . by using flip , the convolution layer can produce three feature maps . by combining produced feature maps using maxout , the resource requirement could be significantly reduced while still retain the invariance properties . experimental results demonstrate that the proposed methods can achieve various invariance at reasonable resource requirements in terms of both memory and time .", "topics": ["kernel ( operating system )", "feature extraction"]}
{"title": "a multiagent urban traffic simulation", "abstract": "we built a multiagent simulation of urban traffic to model both ordinary traffic and emergency or crisis mode traffic . this simulation first builds a modeled road network based on detailed geographical information . on this network , the simulation creates two populations of agents : the transporters and the mobiles . transporters embody the roads themselves ; they are utilitarian and meant to handle the low level realism of the simulation . mobile agents embody the vehicles that circulate on the network . they have one or several destinations they try to reach using initially their beliefs of the structure of the network ( length of the edges , speed limits , number of lanes etc . ) . nonetheless , when confronted to a dynamic , emergent prone environment ( other vehicles , unexpectedly closed ways or lanes , traffic jams etc . ) , the rather reactive agent will activate more cognitive modules to adapt its beliefs , desires and intentions . it may change its destination ( s ) , change the tactics used to reach the destination ( favoring less used roads , following other agents , using general headings ) , etc . we describe our current validation of our model and the next planned improvements , both in validation and in functionalities .", "topics": ["high- and low-level", "simulation"]}
{"title": "a deep learning algorithm for one-step contour aware nuclei segmentation of histopathological images", "abstract": "this paper addresses the task of nuclei segmentation in high-resolution histopathological images . we propose an auto- matic end-to-end deep neural network algorithm for segmenta- tion of individual nuclei . a nucleus-boundary model is introduced to predict nuclei and their boundaries simultaneously using a fully convolutional neural network . given a color normalized image , the model directly outputs an estimated nuclei map and a boundary map . a simple , fast and parameter-free post-processing procedure is performed on the estimated nuclei map to produce the final segmented nuclei . an overlapped patch extraction and assembling method is also designed for seamless prediction of nuclei in large whole-slide images . we also show the effectiveness of data augmentation methods for nuclei segmentation task . our experiments showed our method outperforms prior state-of-the- art methods . moreover , it is efficient that one 1000x1000 image can be segmented in less than 5 seconds . this makes it possible to precisely segment the whole-slide image in acceptable time", "topics": ["end-to-end principle"]}
{"title": "stein variational adaptive importance sampling", "abstract": "we propose a novel adaptive importance sampling algorithm which incorporates stein variational gradient decent algorithm ( svgd ) with importance sampling ( is ) . our algorithm leverages the nonparametric transforms in svgd to iteratively decrease the kl divergence between our importance proposal and the target distribution . the advantages of this algorithm are twofold : first , our algorithm turns svgd into a standard is algorithm , allowing us to use standard diagnostic and analytic tools of is to evaluate and interpret the results ; second , we do not restrict the choice of our importance proposal to predefined distribution families like traditional ( adaptive ) is methods . empirical experiments demonstrate that our algorithm performs well on evaluating partition functions of restricted boltzmann machines and testing likelihood of variational auto-encoders .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "proximal algorithms in statistics and machine learning", "abstract": "in this paper we develop proximal methods for statistical learning . proximal point algorithms are useful in statistics and machine learning for obtaining optimization solutions for composite functions . our approach exploits closed-form solutions of proximal operators and envelope representations based on the moreau , forward-backward , douglas-rachford and half-quadratic envelopes . envelope representations lead to novel proximal algorithms for statistical optimisation of composite objective functions which include both non-smooth and non-convex objectives . we illustrate our methodology with regularized logistic and poisson regression and non-convex bridge penalties with a fused lasso norm . we provide a discussion of convergence of non-descent algorithms with acceleration and for non-convex functions . finally , we provide directions for future research .", "topics": ["mathematical optimization"]}
{"title": "pixel-variant local homography for fisheye stereo rectification minimizing resampling distortion", "abstract": "large field-of-view fisheye lens cameras have attracted more and more researchers ' attention in the field of robotics . however , there does not exist a convenient off-the-shelf stereo rectification approach which can be applied directly to fisheye stereo rig . one obvious drawback of existing methods is that the resampling distortion ( which is defined as the loss of pixels due to under-sampling and the creation of new pixels due to over-sampling during rectification process ) is severe if we want to obtain a rectification with epipolar line ( not epipolar circle ) constraint . to overcome this weakness , we propose a novel pixel-wise local homography technique for stereo rectification . first , we prove that there indeed exist enough degrees of freedom to apply pixel-wise local homography for stereo rectification . then we present a method to exploit these freedoms and the solution via an optimization framework . finally , the robustness and effectiveness of the proposed method have been verified on real fisheye lens images . the rectification results show that the proposed approach can effectively reduce the resampling distortion in comparison with existing methods while satisfying the epipolar line constraint . by employing the proposed method , dense stereo matching and 3d reconstruction for fisheye lens camera become as easy as perspective lens cameras .", "topics": ["sampling ( signal processing )", "pixel"]}
{"title": "selecting the selection", "abstract": "modern saturation-based automated theorem provers typically implement the superposition calculus for reasoning about first-order logic with or without equality . practical implementations of this calculus use a variety of literal selections and term orderings to tame the growth of the search space and help steer proof search . this paper introduces the notion of lookahead selection that estimates ( looks ahead ) the effect on the search space of selecting a literal . there is also a case made for the use of incomplete selection functions that attempt to restrict the search space instead of satisfying some completeness criteria . experimental evaluation in the \\vampire\\ theorem prover shows that both lookahead selection and incomplete selection significantly contribute to solving hard problems unsolvable by other methods .", "topics": ["parsing"]}
{"title": "bagan : data augmentation with balancing gan", "abstract": "image classification datasets are often imbalanced , characteristic that negatively affects the accuracy of deeplearning classifiers . in this work we propose balancing gans ( bagans ) as an augmentation tool to restore balance in imbalanced datasets . this is challenging because the few minority-class images may not be enough to train a gan . we overcome this issue by including during training all available images of majority and minority classes . the generative model learns useful features from majority classes and uses these to generate images for minority classes . we apply class-conditioning in the latent space to drive the generation process towards a target class . additionally , we couple gans with autoencoding techniques to reduce the risk of collapsing toward the generation of few foolish examples . we compare the proposed methodology with state-of-the-art gans and demonstrate that bagan generates images of superior quality when trained with an imbalanced dataset .", "topics": ["generative model", "autoencoder"]}
{"title": "fusing deep convolutional networks for large scale visual concept classification", "abstract": "deep learning architectures are showing great promise in various computer vision domains including image classification , object detection , event detection and action recognition . in this study , we investigate various aspects of convolutional neural networks ( cnns ) from the big data perspective . we analyze recent studies and different network architectures both in terms of running time and accuracy . we present extensive empirical information along with best practices for big data practitioners . using these best practices we propose efficient fusion mechanisms both for single and multiple network models . we present state-of-the art results on benchmark datasets while keeping computational costs at a lower level . another contribution of our paper is that these state-of-the-art results can be reached without using extensive data augmentation techniques .", "topics": ["object detection", "time complexity"]}
{"title": "learned in translation : contextualized word vectors", "abstract": "computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like imagenet . natural language processing ( nlp ) typically sees initialization of only the lowest layer of deep models with pretrained word vectors . in this paper , we use a deep lstm encoder from an attentional sequence-to-sequence model trained for machine translation ( mt ) to contextualize word vectors . we show that adding these context vectors ( cove ) improves performance over using only unsupervised word and character vectors on a wide variety of common nlp tasks : sentiment analysis ( sst , imdb ) , question classification ( trec ) , entailment ( snli ) , and question answering ( squad ) . for fine-grained sentiment analysis and entailment , cove improves performance of our baseline models to the state of the art .", "topics": ["baseline ( configuration management )", "natural language processing"]}
{"title": "a multi-stage supply chain network optimization using genetic algorithms", "abstract": "in today 's global business market place , individual firms no longer compete as independent entities with unique brand names but as integral part of supply chain links . key to success of any business is satisfying customer 's demands on time which may result in cost reductions and increase in service level . in supply chain networks decisions are made with uncertainty about product 's demands , costs , prices , lead times , quality in a competitive and collaborative environment . if poor decisions are made , they may lead to excess inventories that are costly or to insufficient inventory that can not meet customer 's demands . in this work we developed a bi-objective model that minimizes system wide costs of the supply chain and delays on delivery of products to distribution centers for a three echelon supply chain . picking a set of pareto front for multi-objective optimization problems require robust and efficient methods that can search an entire space . we used evolutionary algorithms to find the set of pareto fronts which have proved to be effective in finding the entire set of pareto fronts .", "topics": ["entity"]}
{"title": "coevolve : a joint point process model for information diffusion and network co-evolution", "abstract": "information diffusion in online social networks is affected by the underlying network topology , but it also has the power to change it . online users are constantly creating new links when exposed to new information sources , and in turn these links are alternating the way information spreads . however , these two highly intertwined stochastic processes , information diffusion and network evolution , have been predominantly studied separately , ignoring their co-evolutionary dynamics . we propose a temporal point process model , coevolve , for such joint dynamics , allowing the intensity of one process to be modulated by that of the other . this model allows us to efficiently simulate interleaved diffusion and network events , and generate traces obeying common diffusion and network patterns observed in real-world networks . furthermore , we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces . we experimented with both synthetic data and data gathered from twitter , and show that our model provides a good fit to the data as well as more accurate predictions than alternatives .", "topics": ["synthetic data"]}
{"title": "filtered fictitious play for perturbed observation potential games and decentralised pomdps", "abstract": "potential games and decentralised partially observable mdps ( dec-pomdps ) are two commonly used models of multi-agent interaction , for static optimisation and sequential decisionmaking settings , respectively . in this paper we introduce filtered fictitious play for solving repeated potential games in which each player 's observations of others ' actions are perturbed by random noise , and use this algorithm to construct an online learning method for solving dec-pomdps . specifically , we prove that noise in observations prevents standard fictitious play from converging to nash equilibrium in potential games , which also makes fictitious play impractical for solving dec-pomdps . to combat this , we derive filtered fictitious play , and provide conditions under which it converges to a nash equilibrium in potential games with noisy observations . we then use filtered fictitious play to construct a solver for dec-pomdps , and demonstrate our new algorithm 's performance in a box pushing problem . our results show that we consistently outperform the state-of-the-art dec-pomdp solver by an average of 100 % across the range of noise in the observation function .", "topics": ["mathematical optimization"]}
{"title": "object detection from video tubelets with convolutional neural networks", "abstract": "deep convolution neural networks ( cnns ) have shown impressive performance in various vision tasks such as image classification , object detection and semantic segmentation . for object detection , particularly in still images , the performance has been significantly increased last year thanks to powerful deep networks ( e.g . googlenet ) and detection frameworks ( e.g . regions with cnn features ( r-cnn ) ) . the lately introduced imagenet task on object detection from video ( vid ) brings the object detection task into the video domain , in which objects ' locations at each frame are required to be annotated with bounding boxes . in this work , we introduce a complete framework for the vid task based on still-image object detection and general object tracking . their relations and contributions in the vid task are thoroughly studied and evaluated . in addition , a temporal convolution network is proposed to incorporate temporal information to regularize the detection results and shows its effectiveness for the task .", "topics": ["object detection", "computer vision"]}
{"title": "solution of system of linear equations - a neuro-fuzzy approach", "abstract": "neuro-fuzzy modeling has been applied in a wide variety of fields such as decision making , engineering and management sciences etc . in particular , applications of this modeling technique in decision making by involving complex systems of linear algebraic equations have remarkable significance . in this paper , we present polak-ribiere conjugate gradient based neural network with fuzzy rules to solve system of simultaneous linear algebraic equations . this is achieved using fuzzy backpropagation learning rule . the implementation results show that the proposed neuro-fuzzy network yields effective solutions for exactly determined , underdetermined and over-determined systems of linear equations . this fact is demonstrated by the computational complexity analysis of the neuro-fuzzy algorithm . the proposed algorithm is simulated effectively using matlab software . to the best of our knowledge this is the first work of the systems of linear algebraic equations using neuro-fuzzy modeling .", "topics": ["computational complexity theory", "simulation"]}
{"title": "video scene parsing with predictive feature learning", "abstract": "in this work , we address the challenging video scene parsing problem by developing effective representation learning methods given limited parsing annotations . in particular , we contribute two novel methods that constitute a unified parsing framework . ( 1 ) \\textbf { predictive feature learning } } from nearly unlimited unlabeled video data . different from existing methods learning features from single frame parsing , we learn spatiotemporal discriminative features by enforcing a parsing network to predict future frames and their parsing maps ( if available ) given only historical frames . in this way , the network can effectively learn to capture video dynamics and temporal context , which are critical clues for video scene parsing , without requiring extra manual annotations . ( 2 ) \\textbf { prediction steering parsing } } architecture that effectively adapts the learned spatiotemporal features to scene parsing tasks and provides strong guidance for any off-the-shelf parsing model to achieve better video scene parsing performance . extensive experiments over two challenging datasets , cityscapes and camvid , have demonstrated the effectiveness of our methods by showing significant improvement over well-established baselines .", "topics": ["feature learning", "test set"]}
{"title": "overview of the nlpcc 2017 shared task : chinese news headline categorization", "abstract": "in this paper , we give an overview for the shared task at the ccf conference on natural language processing \\ & chinese computing ( nlpcc 2017 ) : chinese news headline categorization . the dataset of this shared task consists 18 classes , 12,000 short texts along with corresponded labels for each class . the dataset and example code can be accessed at https : //github.com/fudannlp/nlpcc2017_news_headline_categorization .", "topics": ["natural language processing"]}
{"title": "expert and non-expert opinion about technological unemployment", "abstract": "there is significant concern that technological advances , especially in robotics and artificial intelligence ( ai ) , could lead to high levels of unemployment in the coming decades . studies have estimated that around half of all current jobs are at risk of automation . to look into this issue in more depth , we surveyed experts in robotics and ai about the risk , and compared their views with those of non-experts . whilst the experts predicted a significant number of occupations were at risk of automation in the next two decades , they were more cautious than people outside the field in predicting occupations at risk . their predictions were consistent with their estimates for when computers might be expected to reach human level performance across a wide range of skills . these estimates were typically decades later than those of the non-experts . technological barriers may therefore provide society with more time to prepare for an automated future than the public fear . in addition , public expectations may need to be dampened about the speed of progress to be expected in robotics and ai .", "topics": ["artificial intelligence"]}
{"title": "combining residual networks with lstms for lipreading", "abstract": "we propose an end-to-end deep learning architecture for word-level visual speech recognition . the system is a combination of spatiotemporal convolutional , residual and bidirectional long short-term memory networks . we train and evaluate it on the lipreading in-the-wild benchmark , a challenging database of 500-size target-words consisting of 1.28sec video excerpts from bbc tv broadcasts . the proposed network attains word accuracy equal to 83.0 , yielding 6.8 absolute improvement over the current state-of-the-art , without using information about word boundaries during training or testing .", "topics": ["speech recognition", "end-to-end principle"]}
{"title": "a computational analysis of collective discourse", "abstract": "this paper is focused on the computational analysis of collective discourse , a collective behavior seen in non-expert content contributions in online social media . we collect and analyze a wide range of real-world collective discourse datasets from movie user reviews to microblogs and news headlines to scientific citations . we show that all these datasets exhibit diversity of perspective , a property seen in other collective systems and a criterion in wise crowds . our experiments also confirm that the network of different perspective co-occurrences exhibits the small-world property with high clustering of different perspectives . finally , we show that non-expert contributions in collective discourse can be used to answer simple questions that are otherwise hard to answer .", "topics": ["cluster analysis", "natural language processing"]}
{"title": "learning large-scale topological maps using sum-product networks", "abstract": "in order to perform complex actions in human environments , an autonomous robot needs the ability to understand the environment , that is , to gather and maintain spatial knowledge . topological map is commonly used for representing large scale , global maps such as floor plans . although much work has been done in topological map extraction , we have found little previous work on the problem of learning the topological map using a probabilistic model . learning a topological map means learning the structure of the large-scale space and dependency between places , for example , how the evidence of a group of places influence the attributes of other places . this is an important step towards planning complex actions in the environment . in this thesis , we consider the problem of using probabilistic deep learning model to learn the topological map , which is essentially a sparse undirected graph where nodes represent places annotated with their semantic attributes ( e.g . place category ) . we propose to use a novel probabilistic deep model , sum-product networks ( spns ) , due to their unique properties . we present two methods for learning topological maps using spns : the place grid method and the template-based method . we contribute an algorithm that builds spns for graphs using template models . our experiments evaluate the ability of our models to enable robots to infer semantic attributes and detect maps with novel semantic attribute arrangements . our results demonstrate their understanding of the topological map structure and spatial relations between places .", "topics": ["map", "sparse matrix"]}
{"title": "end-to-end waveform utterance enhancement for direct evaluation metrics optimization by fully convolutional neural networks", "abstract": "speech enhancement model is used to map a noisy speech to a clean speech . in the training stage , an objective function is often adopted to optimize the model parameters . however , in most studies , there is an inconsistency between the model optimization criterion and the evaluation criterion on the enhanced speech . for example , in measuring speech intelligibility , most of the evaluation metric is based on a short-time objective intelligibility ( stoi ) measure , while the frame based minimum mean square error ( mmse ) between estimated and clean speech is widely used in optimizing the model . due to the inconsistency , there is no guarantee that the trained model can provide optimal performance in applications . in this study , we propose an end-to-end utterance-based speech enhancement framework using fully convolutional neural networks ( fcn ) to reduce the gap between the model optimization and evaluation criterion . because of the utterance-based optimization , temporal correlation information of long speech segments , or even at the entire utterance level , can be considered when perception-based objective functions are used for the direct optimization . as an example , we implement the proposed fcn enhancement framework to optimize the stoi measure . experimental results show that the stoi of test speech is better than conventional mmse-optimized speech due to the consistency between the training and evaluation target . moreover , by integrating the stoi in model optimization , the intelligibility of human subjects and automatic speech recognition ( asr ) system on the enhanced speech is also substantially improved compared to those generated by the mmse criterion .", "topics": ["mathematical optimization", "loss function"]}
{"title": "improving testsuites via instrumentation", "abstract": "this paper explores the usefulness of a technique from software engineering , namely code instrumentation , for the development of large-scale natural language grammars . information about the usage of grammar rules in test sentences is used to detect untested rules , redundant test sentences , and likely causes of overgeneration . results show that less than half of a large-coverage grammar for german is actually tested by two large testsuites , and that 10-30 % of testing time is redundant . the methodology applied can be seen as a re-use of grammar writing knowledge for testsuite compilation .", "topics": ["machine translation", "natural language"]}
{"title": "armdn : associative and recurrent mixture density networks for eretail demand forecasting", "abstract": "accurate demand forecasts can help on-line retail organizations better plan their supply-chain processes . the challenge , however , is the large number of associative factors that result in large , non-stationary shifts in demand , which traditional time series and regression approaches fail to model . in this paper , we propose a neural network architecture called ar-mdn , that simultaneously models associative factors , time-series trends and the variance in the demand . we first identify several causal features and use a combination of feature embeddings , mlp and lstm to represent them . we then model the output density as a learned mixture of gaussian distributions . the ar-mdn can be trained end-to-end without the need for additional supervision . we experiment on a dataset of an year 's worth of data over tens-of-thousands of products from flipkart . the proposed architecture yields a significant improvement in forecasting accuracy when compared with existing alternatives .", "topics": ["time series", "end-to-end principle"]}
{"title": "a general theory for training learning machine", "abstract": "though the deep learning is pushing the machine learning to a new stage , basic theories of machine learning are still limited . the principle of learning , the role of the a prior knowledge , the role of neuron bias , and the basis for choosing neural transfer function and cost function , etc . , are still far from clear . in this paper , we present a general theoretical framework for machine learning . we classify the prior knowledge into common and problem-dependent parts , and consider that the aim of learning is to maximally incorporate them . the principle we suggested for maximizing the former is the design risk minimization principle , while the neural transfer function , the cost function , as well as pretreatment of samples , are endowed with the role for maximizing the latter . the role of the neuron bias is explained from a different angle . we develop a monte carlo algorithm to establish the input-output responses , and we control the input-output sensitivity of a learning machine by controlling that of individual neurons . applications of function approaching and smoothing , pattern recognition and classification , are provided to illustrate how to train general learning machines based on our theory and algorithm . our method may in addition induce new applications , such as the transductive inference .", "topics": ["loss function"]}
{"title": "removing biases from trainable mt metrics by using self-training", "abstract": "most trainable machine translation ( mt ) metrics train their weights on human judgments of state-of-the-art mt systems outputs . this makes trainable metrics biases in many ways . one of them is preferring longer translations . these biased metrics when used for tuning are evaluating different types of translations -- n-best lists of translations with very diverse quality . systems tuned with these metrics tend to produce overly long translations that are preferred by the metric but not by humans . this is usually solved by manually tweaking metric 's weights to equally value recall and precision . our solution is more general : ( 1 ) it does not address only the recall bias but also all other biases that might be present in the data and ( 2 ) it does not require any knowledge of the types of features used which is useful in cases when manual tuning of metric 's weights is not possible . this is accomplished by self-training on unlabeled n-best lists by using metric that was initially trained on standard human judgments . one way of looking at this is as domain adaptation from the domain of state-of-the-art mt translations to diverse n-best list translations .", "topics": ["machine translation"]}
{"title": "unsupervised topic modeling approaches to decision summarization in spoken meetings", "abstract": "we present a token-level decision summarization framework that utilizes the latent topic structures of utterances to identify `` summary-worthy '' words . concretely , a series of unsupervised topic models is explored and experimental results show that fine-grained topic models , which discover topics at the utterance-level rather than the document-level , can better identify the gist of the decision-making process . moreover , our proposed token-level summarization approach , which is able to remove redundancies within utterances , outperforms existing utterance ranking based summarization methods . finally , context information is also investigated to add additional relevant information to the summary .", "topics": ["unsupervised learning"]}
{"title": "adaptive-rate sparse signal reconstruction with application in compressive background subtraction", "abstract": "we propose and analyze an online algorithm for reconstructing a sequence of signals from a limited number of linear measurements . the signals are assumed sparse , with unknown support , and evolve over time according to a generic nonlinear dynamical model . our algorithm , based on recent theoretical results for $ \\ell_1 $ - $ \\ell_1 $ minimization , is recursive and computes the number of measurements to be taken at each time on-the-fly . as an example , we apply the algorithm to compressive video background subtraction , a problem that can be stated as follows : given a set of measurements of a sequence of images with a static background , simultaneously reconstruct each image while separating its foreground from the background . the performance of our method is illustrated on sequences of real images : we observe that it allows a dramatic reduction in the number of measurements with respect to state-of-the-art compressive background subtraction schemes .", "topics": ["nonlinear system", "sparse matrix"]}
{"title": "a novel data pre-processing method for multi-dimensional and non-uniform data", "abstract": "we are in the era of data analytics and data science which is on full bloom . there is abundance of all kinds of data for example biometrics based data , satellite images data , chip-seq data , social network data , sensor based data etc . from a variety of sources . this data abundance is the result of the fact that storage cost is getting cheaper day by day , so people as well as almost all business or scientific organizations are storing more and more data . most of the real data is multi-dimensional , non-uniform , and big in size , such that it requires a unique pre-processing before analyzing it . in order to make data useful for any kind of analysis , pre-processing is a very important step . this paper presents a unique and novel pre-processing method for multi-dimensional and non-uniform data with the aim of making it uniform and reduced in size without losing much of its value . we have chosen biometric signature data to demonstrate the proposed method as it qualifies for the attributes of being multi-dimensional , non-uniform and big in size . biometric signature data does not only captures the structural characteristics of a signature but also its behavioral characteristics that are captured using a dynamic signature capture device . these features like pen pressure , pen tilt angle , time taken to sign a document when collected in real-time turn out to be of varying dimensions . this feature data set along with the structural data needs to be pre-processed in order to use it to train a machine learning based model for signature verification purposes . we demonstrate the success of the proposed method over other methods using experimental results for biometric signature data but the same can be implemented for any other data with similar properties from a different domain .", "topics": ["image processing"]}
{"title": "a tutorial on modeling and inference in undirected graphical models for hyperspectral image analysis", "abstract": "undirected graphical models have been successfully used to jointly model the spatial and the spectral dependencies in earth observing hyperspectral images . they produce less noisy , smooth , and spatially coherent land cover maps and give top accuracies on many datasets . moreover , they can easily be combined with other state-of-the-art approaches , such as deep learning . this has made them an essential tool for remote sensing researchers and practitioners . however , graphical models have not been easily accessible to the larger remote sensing community as they are not discussed in standard remote sensing textbooks and not included in the popular remote sensing software and toolboxes . in this tutorial , we provide a theoretical introduction to markov random fields and conditional random fields based spatial-spectral classification for land cover mapping along with a detailed step-by-step practical guide on applying these methods using freely available software . furthermore , the discussed methods are benchmarked on four public hyperspectral datasets for a fair comparison among themselves and easy comparison with the vast number of methods in literature which use the same datasets . the source code necessary to reproduce all the results in the paper is published on-line to make it easier for the readers to apply these techniques to different remote sensing problems .", "topics": ["graphical model", "map"]}
{"title": "regularized em algorithms : a unified framework and statistical guarantees", "abstract": "latent variable models are a fundamental modeling tool in machine learning applications , but they present significant computational and analytical challenges . the popular em algorithm and its variants , is a much used algorithmic tool ; yet our rigorous understanding of its performance is highly incomplete . recently , work in balakrishnan et al . ( 2014 ) has demonstrated that for an important class of problems , em exhibits linear local convergence . in the high-dimensional setting , however , the m-step may not be well defined . we address precisely this setting through a unified treatment using regularization . while regularization for high-dimensional problems is by now well understood , the iterative em algorithm requires a careful balancing of making progress towards the solution while identifying the right structure ( e.g . , sparsity or low-rank ) . in particular , regularizing the m-step using the state-of-the-art high-dimensional prescriptions ( e.g . , wainwright ( 2014 ) ) is not guaranteed to provide this balance . our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors . we specialize our general framework to sparse gaussian mixture models , high-dimensional mixed regression , and regression with missing variables , obtaining statistical guarantees for each of these examples .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "web page categorization using artificial neural networks", "abstract": "web page categorization is one of the challenging tasks in the world of ever increasing web technologies . there are many ways of categorization of web pages based on different approach and features . this paper proposes a new dimension in the way of categorization of web pages using artificial neural network ( ann ) through extracting the features automatically . here eight major categories of web pages have been selected for categorization ; these are business & economy , education , government , entertainment , sports , news & media , job search , and science . the whole process of the proposed system is done in three successive stages . in the first stage , the features are automatically extracted through analyzing the source of the web pages . the second stage includes fixing the input values of the neural network ; all the values remain between 0 and 1 . the variations in those values affect the output . finally the third stage determines the class of a certain web page out of eight predefined classes . this stage is done using back propagation algorithm of artificial neural network . the proposed concept will facilitate web mining , retrievals of information from the web and also the search engines .", "topics": ["value ( ethics )", "neural networks"]}
{"title": "iterative thresholded bi-histogram equalization for medical image enhancement", "abstract": "enhancement of human vision to get an insight to information content is of vital importance . the traditional histogram equalization methods have been suffering from amplified contrast with the addition of artifacts and a surprising unnatural visibility of the processed images . in order to overcome these drawbacks , this paper proposes interative , mean , and multi-threshold selection criterion with plateau limits , which consist of histogram segmentation , clipping and transformation modules . the histogram partition consists of multiple thresholding processes that divide the histogram into two parts , whereas the clipping process nicely enhances the contrast by having a check on the rate of enhancement that could be tuned . histogram equalization to each segmented sub-histogram provides the output image with preserved brightness and enhanced contrast . results of the present study showed that the proposed method efficiently handles the noise amplification . further , it also preserves the brightness by retaining natural look of targeted image .", "topics": ["eisenstein 's criterion"]}
{"title": "design of the artificial : lessons from the biological roots of general intelligence", "abstract": "our desire and fascination with intelligent machines dates back to the antiquity 's mythical automaton talos , aristotle 's mode of mechanical thought ( syllogism ) and heron of alexandria 's mechanical machines and automata . however , the quest for artificial general intelligence ( agi ) is troubled with repeated failures of strategies and approaches throughout the history . this decade has seen a shift in interest towards bio-inspired software and hardware , with the assumption that such mimicry entails intelligence . though these steps are fruitful in certain directions and have advanced automation , their singular design focus renders them highly inefficient in achieving agi . which set of requirements have to be met in the design of agi ? what are the limits in the design of the artificial ? here , a careful examination of computation in biological systems hints that evolutionary tinkering of contextual processing of information enabled by a hierarchical architecture is the key to build agi .", "topics": ["computation"]}
{"title": "natasha 2 : faster non-convex optimization than sgd", "abstract": "we design a stochastic algorithm to train any smooth neural network to $ \\varepsilon $ -approximate local minima , using $ o ( \\varepsilon^ { -3.25 } ) $ backpropagations . the best result was essentially $ o ( \\varepsilon^ { -4 } ) $ by sgd . more broadly , it finds $ \\varepsilon $ -approximate local minima of any smooth nonconvex function in rate $ o ( \\varepsilon^ { -3.25 } ) $ , with only oracle access to stochastic gradients .", "topics": ["matrix regularization", "gradient"]}
{"title": "global bandits with holder continuity", "abstract": "standard multi-armed bandit ( mab ) problems assume that the arms are independent . however , in many application scenarios , the information obtained by playing an arm provides information about the remainder of the arms . hence , in such applications , this informativeness can and should be exploited to enable faster convergence to the optimal solution . in this paper , we introduce and formalize the global mab ( gmab ) , in which arms are globally informative through a global parameter , i.e . , choosing an arm reveals information about all the arms . we propose a greedy policy for the gmab which always selects the arm with the highest estimated expected reward , and prove that it achieves bounded parameter-dependent regret . hence , this policy selects suboptimal arms only finitely many times , and after a finite number of initial time steps , the optimal arm is selected in all of the remaining time steps with probability one . in addition , we also study how the informativeness of the arms about each other 's rewards affects the speed of learning . specifically , we prove that the parameter-free ( worst-case ) regret is sublinear in time , and decreases with the informativeness of the arms . we also prove a sublinear in time bayesian risk bound for the gmab which reduces to the well-known bayesian risk bound for linearly parameterized bandits when the arms are fully informative . gmabs have applications ranging from drug and treatment discovery to dynamic pricing .", "topics": ["regret ( decision theory )", "optimization problem"]}
{"title": "an investigation of newton-sketch and subsampled newton methods", "abstract": "the concepts of sketching and subsampling have recently received much attention by the optimization and statistics communities . in this paper , we study newton-sketch and subsampled newton ( ssn ) methods for the finite-sum optimization problem . we consider practical versions of the two methods in which the newton equations are solved approximately using the conjugate gradient ( cg ) method or a stochastic gradient iteration . we establish new complexity results for the ssn-cg method that exploit the spectral properties of cg . controlled numerical experiments compare the relative strengths of newton-sketch and ssn methods and show that for many finite-sum problems , they are far more efficient than svrg , a popular first-order method .", "topics": ["optimization problem", "numerical analysis"]}
{"title": "a network-based end-to-end trainable task-oriented dialogue system", "abstract": "teaching machines to accomplish tasks by conversing naturally with humans is challenging . currently , developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting , or acquiring costly labelled datasets to solve a statistical learning problem for each component . in this work we introduce a neural network-based text-in , text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined wizard-of-oz framework . this approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand . the results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain .", "topics": ["end-to-end principle"]}
{"title": "weakly supervised object detection with pointwise mutual information", "abstract": "in this work a novel approach for weakly supervised object detection that incorporates pointwise mutual information is presented . a fully convolutional neural network architecture is applied in which the network learns one filter per object class . the resulting feature map indicates the location of objects in an image , yielding an intuitive representation of a class activation map . while traditionally such networks are learned by a softmax or binary logistic regression ( sigmoid cross-entropy loss ) , a learning approach based on a cosine loss is introduced . a pointwise mutual information layer is incorporated in the network in order to project predictions and ground truth presence labels in a non-categorical embedding space . thus , the cosine loss can be employed in this non-categorical representation . besides integrating image level annotations , it is shown how to integrate point-wise annotations using a spatial pyramid pooling layer . the approach is evaluated on the voc2012 dataset for classification , point localization and weakly supervised bounding box localization . it is shown that the combination of pointwise mutual information and a cosine loss eases the learning process and thus improves the accuracy . the integration of coarse point-wise localizations further improves the results at minimal annotation costs .", "topics": ["object detection", "ground truth"]}
{"title": "robust photometric stereo via dictionary learning", "abstract": "photometric stereo is a method that seeks to reconstruct the normal vectors of an object from a set of images of the object illuminated under different light sources . while effective in some situations , classical photometric stereo relies on a diffuse surface model that can not handle objects with complex reflectance patterns , and it is sensitive to non-idealities in the images . in this work , we propose a novel approach to photometric stereo that relies on dictionary learning to produce robust normal vector reconstructions . specifically , we develop three formulations for applying dictionary learning to photometric stereo . we propose a preprocessing step that utilizes dictionary learning to denoise the images . we also present a model that applies dictionary learning to regularize and reconstruct the normal vectors from the images under the classic lambertian reflectance model . finally , we generalize the latter model to explicitly model non-lambertian objects . we investigate all three approaches through extensive experimentation on synthetic and real benchmark datasets and observe state-of-the-art performance compared to existing robust photometric stereo methods .", "topics": ["synthetic data", "noise reduction"]}
{"title": "duality between subgradient and conditional gradient methods", "abstract": "given a convex optimization problem and its dual , there are many possible first-order algorithms . in this paper , we show the equivalence between mirror descent algorithms and algorithms generalizing the conditional gradient method . this is done through convex duality , and implies notably that for certain problems , such as for supervised machine learning problems with non-smooth losses or problems regularized by non-smooth regularizers , the primal subgradient method and the dual conditional gradient method are formally equivalent . the dual interpretation leads to a form of line search for mirror descent , as well as guarantees of convergence for primal-dual certificates .", "topics": ["optimization problem", "supervised learning"]}
{"title": "differentially private variational dropout", "abstract": "deep neural networks with their large number of parameters are highly flexible learning systems . the high flexibility in such networks brings with some serious problems such as overfitting , and regularization is used to address this problem . a currently popular and effective regularization technique for controlling the overfitting is dropout . often , large data collections required for neural networks contain sensitive information such as the medical histories of patients , and the privacy of the training data should be protected . in this paper , we modify the recently proposed variational dropout technique which provided an elegant bayesian interpretation to dropout , and show that the intrinsic noise in the variational dropout can be exploited to obtain a degree of differential privacy . the iterative nature of training neural networks presents a challenge for privacy-preserving estimation since multiple iterations increase the amount of noise added . we overcome this by using a relaxed notion of differential privacy , called concentrated differential privacy , which provides tighter estimates on the overall privacy loss . we demonstrate the accuracy of our privacy-preserving variational dropout algorithm on benchmark datasets .", "topics": ["test set", "calculus of variations"]}
{"title": "quantized neural networks : training neural networks with low precision weights and activations", "abstract": "we introduce a method to train quantized neural networks ( qnns ) -- - neural networks with extremely low precision ( e.g . , 1-bit ) weights and activations , at run-time . at train-time the quantized weights and activations are used for computing the parameter gradients . during the forward pass , qnns drastically reduce memory size and accesses , and replace most arithmetic operations with bit-wise operations . as a result , power consumption is expected to be drastically reduced . we trained qnns over the mnist , cifar-10 , svhn and imagenet datasets . the resulting qnns achieve prediction accuracy comparable to their 32-bit counterparts . for example , our quantized version of alexnet with 1-bit weights and 2-bit activations achieves $ 51\\ % $ top-1 accuracy . moreover , we quantize the parameter gradients to 6-bits as well which enables gradients computation using only bit-wise operation . quantized recurrent neural networks were tested over the penn treebank dataset , and achieved comparable accuracy as their 32-bit counterparts using only 4-bits . last but not least , we programmed a binary matrix multiplication gpu kernel with which it is possible to run our mnist qnn 7 times faster than with an unoptimized gpu kernel , without suffering any loss in classification accuracy . the qnn code is available online .", "topics": ["kernel ( operating system )", "recurrent neural network"]}
{"title": "a semantics and complete algorithm for subsumption in the classic description logic", "abstract": "this paper analyzes the correctness of the subsumption algorithm used in classic , a description logic-based knowledge representation system that is being used in practical applications . in order to deal efficiently with individuals in classic descriptions , the developers have had to use an algorithm that is incomplete with respect to the standard , model-theoretic semantics for description logics . we provide a variant semantics for descriptions with respect to which the current implementation is complete , and which can be independently motivated . the soundness and completeness of the polynomial-time subsumption algorithm is established using description graphs , which are an abstracted version of the implementation structures used in classic , and are of independent interest .", "topics": ["time complexity", "polynomial"]}
{"title": "distributed heuristic forward search for multi-agent systems", "abstract": "this paper describes a number of distributed forward search algorithms for solving multi-agent planning problems . we introduce a distributed formulation of non-optimal forward search , as well as an optimal version , mad-a* . our algorithms exploit the structure of multi-agent problems to not only distribute the work efficiently among different agents , but also to remove symmetries and reduce the overall workload . the algorithms ensure that private information is not shared among agents , yet computation is still efficient -- outperforming current state-of-the-art distributed planners , and in some cases even centralized search -- despite the fact that each agent has access only to partial information .", "topics": ["computation", "heuristic"]}
{"title": "inverse reinforcement learning under noisy observations", "abstract": "we consider the problem of performing inverse reinforcement learning when the trajectory of the expert is not perfectly observed by the learner . instead , a noisy continuous-time observation of the trajectory is provided to the learner . this problem exhibits wide-ranging applications and the specific application we consider here is the scenario in which the learner seeks to penetrate a perimeter patrolled by a robot . the learner 's field of view is limited due to which it can not observe the patroller 's complete trajectory . instead , we allow the learner to listen to the expert 's movement sound , which it can also use to estimate the expert 's state and action using an observation model . we treat the expert 's state and action as hidden data and present an algorithm based on expectation maximization and maximum entropy principle to solve the non-linear , non-convex problem . related work considers discrete-time observations and an observation model that does not include actions . in contrast , our technique takes expectations over both state and action of the expert , enabling learning even in the presence of extreme noise and broader applications .", "topics": ["nonlinear system", "reinforcement learning"]}
{"title": "the statistical methods of pixel-based image fusion techniques", "abstract": "there are many image fusion methods that can be used to produce high-resolution mutlispectral images from a high-resolution panchromatic ( pan ) image and low-resolution multispectral ( ms ) of remote sensed images . this paper attempts to undertake the study of image fusion techniques with different statistical techniques for image fusion as local mean matching ( lmm ) , local mean and variance matching ( lmvm ) , regression variable substitution ( rvs ) , local correlation modeling ( lcm ) and they are compared with one another so as to choose the best technique , that can be applied on multi-resolution satellite images . this paper also devotes to concentrate on the analytical techniques for evaluating the quality of image fusion ( f ) by using various methods including standard deviation ( sd ) , entropy ( en ) , correlation coefficient ( cc ) , signal-to noise ratio ( snr ) , normalization root mean square error ( nrmse ) and deviation index ( di ) to estimate the quality and degree of information improvement of a fused image quantitatively .", "topics": ["pixel", "coefficient"]}
{"title": "piecewise flat embedding for image segmentation", "abstract": "we propose a new nonlinear embedding -- piecewise flat embedding ( pfe ) -- for image segmentation . based on the theory of sparse signal recovery , piecewise flat embedding attempts to recover a piecewise constant image representation with sparse region boundaries and sparse cluster value scattering . the resultant piecewise flat embedding exhibits interesting properties such as suppressing slowly varying signals , and offers an image representation with higher region identifiability which is desirable for image segmentation or high-level semantic analysis tasks . we formulate our embedding as a variant of the laplacian eigenmap embedding with an $ l_ { 1 , p } ( 0 < p\\leq1 ) $ regularization term to promote sparse solutions . first , we devise a two-stage numerical algorithm based on bregman iterations to compute $ l_ { 1,1 } $ -regularized piecewise flat embeddings . we further generalize this algorithm through iterative reweighting to solve the general $ l_ { 1 , p } $ -regularized problem . to demonstrate its efficacy , we integrate pfe into two existing image segmentation frameworks , segmentation based on clustering and hierarchical segmentation based on contour detection . experiments on four major benchmark datasets , bsds500 , msrc , stanford background dataset , and pascal context , show that segmentation algorithms incorporating our embedding achieve significantly improved results .", "topics": ["cluster analysis", "image segmentation"]}
{"title": "convex relaxations of bregman divergence clustering", "abstract": "although many convex relaxations of clustering have been proposed in the past decade , current formulations remain restricted to spherical gaussian or discriminative models and are susceptible to imbalanced clusters . to address these shortcomings , we propose a new class of convex relaxations that can be flexibly applied to more general forms of bregman divergence clustering . by basing these new formulations on normalized equivalence relations we retain additional control on relaxation quality , which allows improvement in clustering quality . we furthermore develop optimization methods that improve scalability by exploiting recent implicit matrix norm methods . in practice , we find that the new formulations are able to efficiently produce tighter clusterings that improve the accuracy of state of the art methods .", "topics": ["cluster analysis", "scalability"]}
{"title": "ese : efficient speech recognition engine with sparse lstm on fpga", "abstract": "long short-term memory ( lstm ) is widely used in speech recognition . in order to achieve higher prediction accuracy , machine learning scientists have built larger and larger models . such large model is both computation intensive and memory intensive . deploying such bulky model results in high power consumption and leads to high total cost of ownership ( tco ) of a data center . in order to speedup the prediction and make it energy efficient , we first propose a load-balance-aware pruning method that can compress the lstm model size by 20x ( 10x from pruning and 2x from quantization ) with negligible loss of the prediction accuracy . the pruned model is friendly for parallel processing . next , we propose scheduler that encodes and partitions the compressed model to each pe for parallelism , and schedule the complicated lstm data flow . finally , we design the hardware architecture , named efficient speech recognition engine ( ese ) that works directly on the compressed model . implemented on xilinx xcku060 fpga running at 200mhz , ese has a performance of 282 gops working directly on the compressed lstm network , corresponding to 2.52 tops on the uncompressed one , and processes a full lstm for speech recognition with a power dissipation of 41 watts . evaluated on the lstm for speech recognition benchmark , ese is 43x and 3x faster than core i7 5930k cpu and pascal titan x gpu implementations . it achieves 40x and 11.5x higher energy efficiency compared with the cpu and gpu respectively .", "topics": ["computation", "speech recognition"]}
{"title": "exploring the use of shatter for allsat through ramsey-type problems", "abstract": "in the context of sat solvers , shatter is a popular tool for symmetry breaking on cnf formulas . nevertheless , little has been said about its use in the context of allsat problems : problems where we are interested in listing all the models of a boolean formula . allsat has gained much popularity in recent years due to its many applications in domains like model checking , data mining , etc . one example of a particularly transparent application of allsat to other fields of computer science is computational ramsey theory . in this paper we study the effect of incorporating shatter to the workflow of using boolean formulas to generate all possible edge colorings of a graph avoiding prescribed monochromatic subgraphs . generating complete sets of colorings is an important building block in computational ramsey theory . we identify two drawbacks in the na\\ '' ive use of shatter to break the symmetries of boolean formulas encoding ramsey-type problems for graphs : a `` blow-up '' in the number of models and the generation of incomplete sets of colorings . the issues presented in this work are not intended to discourage the use of shatter as a preprocessing tool for allsat problems in combinatorial computing but to help researchers properly use this tool by avoiding these potential pitfalls . to this end , we provide strategies and additional tools to cope with the negative effects of using shatter for allsat . while the specific application addressed in this paper is that of ramsey-type problems , the analysis we carry out applies to many other areas in which highly-symmetrical boolean formulas arise and we wish to find all of their models .", "topics": ["data mining"]}
{"title": "non-uniform blind deblurring with a spatially-adaptive sparse prior", "abstract": "typical blur from camera shake often deviates from the standard uniform convolutional script , in part because of problematic rotations which create greater blurring away from some unknown center point . consequently , successful blind deconvolution requires the estimation of a spatially-varying or non-uniform blur operator . using ideas from bayesian inference and convex analysis , this paper derives a non-uniform blind deblurring algorithm with several desirable , yet previously-unexplored attributes . the underlying objective function includes a spatially adaptive penalty which couples the latent sharp image , non-uniform blur operator , and noise level together . this coupling allows the penalty to automatically adjust its shape based on the estimated degree of local blur and image structure such that regions with large blur or few prominent edges are discounted . remaining regions with modest blur and revealing edges therefore dominate the overall estimation process without explicitly incorporating structure-selection heuristics . the algorithm can be implemented using a majorization-minimization strategy that is virtually parameter free . detailed theoretical analysis and empirical validation on real images serve to validate the proposed method .", "topics": ["loss function", "sparse matrix"]}
{"title": "m-fish karyotyping - a new approach based on watershed transform", "abstract": "karyotyping is a process in which chromosomes in a dividing cell are properly stained , identified and displayed in a standard format , which helps geneticist to study and diagnose genetic factors behind various genetic diseases and for studying cancer . m-fish ( multiplex fluorescent in-situ hybridization ) provides color karyotyping . in this paper , an automated method for m-fish chromosome segmentation based on watershed transform followed by naive bayes classification of each region using the features , mean and standard deviation , is presented . also , a post processing step is added to re-classify the small chromosome segments to the neighboring larger segment for reducing the chances of misclassification . the approach provided improved accuracy when compared to the pixel-by-pixel approach . the approach was tested on 40 images from the dataset and achieved an accuracy of 84.21 % .", "topics": ["pixel"]}
{"title": "construction of a bilingual dictionary intermediated by a third language", "abstract": "when using a third language to construct a bilingual dictionary , it is necessary to discriminate equivalencies from inappropriate words derived as a result of ambiguity in the third language . we propose a method to treat this by utilizing the structures of dictionaries to measure the nearness of the meanings of words . the resulting dictionary is a word-to-word bilingual dictionary of nouns and can be used to refine the entries and equivalencies in published bilingual dictionaries .", "topics": ["dictionary"]}
{"title": "reliable attribute-based object recognition using high predictive value classifiers", "abstract": "we consider the problem of object recognition in 3d using an ensemble of attribute-based classifiers . we propose two new concepts to improve classification in practical situations , and show their implementation in an approach implemented for recognition from point-cloud data . first , the viewing conditions can have a strong influence on classification performance . we study the impact of the distance between the camera and the object and propose an approach to fuse multiple attribute classifiers , which incorporates distance into the decision making . second , lack of representative training samples often makes it difficult to learn the optimal threshold value for best positive and negative detection rate . we address this issue , by setting in our attribute classifiers instead of just one threshold value , two threshold values to distinguish a positive , a negative and an uncertainty class , and we prove the theoretical correctness of this approach . empirical studies demonstrate the effectiveness and feasibility of the proposed concepts .", "topics": ["value ( ethics )"]}
{"title": "provable algorithms for inference in topic models", "abstract": "recently , there has been considerable progress on designing algorithms with provable guarantees -- typically using linear algebraic methods -- for parameter learning in latent variable models . but designing provable algorithms for inference has proven to be more challenging . here we take a first step towards provable inference in topic models . we leverage a property of topic models that enables us to construct simple linear estimators for the unknown topic proportions that have small variance , and consequently can work with short documents . our estimators also correspond to finding an estimate around which the posterior is well-concentrated . we show lower bounds that for shorter documents it can be information theoretically impossible to find the hidden topics . finally , we give empirical results that demonstrate that our algorithm works on realistic topic models . it yields good solutions on synthetic data and runs in time comparable to a { \\em single } iteration of gibbs sampling .", "topics": ["sampling ( signal processing )", "synthetic data"]}
{"title": "adaptive feature ranking for unsupervised transfer learning", "abstract": "transfer learning is concerned with the application of knowledge gained from solving a problem to a different but related problem domain . in this paper , we propose a method and efficient algorithm for ranking and selecting representations from a restricted boltzmann machine trained on a source domain to be transferred onto a target domain . experiments carried out using the mnist , icdar and ticc image datasets show that the proposed adaptive feature ranking and transfer learning method offers statistically significant improvements on the training of rbms . our method is general in that the knowledge chosen by the ranking function does not depend on its relation to any specific target domain , and it works with unsupervised learning and knowledge-based transfer .", "topics": ["unsupervised learning", "mnist database"]}
{"title": "multi-oriented scene text detection via corner localization and region segmentation", "abstract": "previous deep learning based state-of-the-art scene text detection methods can be roughly classified into two categories . the first category treats scene text as a type of general objects and follows general object detection paradigm to localize scene text by regressing the text box locations , but troubled by the arbitrary-orientation and large aspect ratios of scene text . the second one segments text regions directly , but mostly needs complex post processing . in this paper , we present a method that combines the ideas of the two types of methods while avoiding their shortcomings . we propose to detect scene text by localizing corner points of text bounding boxes and segmenting text regions in relative positions . in inference stage , candidate boxes are generated by sampling and grouping corner points , which are further scored by segmentation maps and suppressed by nms . compared with previous methods , our method can handle long oriented text naturally and does n't need complex post processing . the experiments on icdar2013 , icdar2015 , msra-td500 , mlt and coco-text demonstrate that the proposed algorithm achieves better or comparable results in both accuracy and efficiency . based on vgg16 , it achieves an f-measure of 84.3 % on icdar2015 and 81.5 % on msra-td500 .", "topics": ["sampling ( signal processing )", "object detection"]}
{"title": "an experiment on the connection between the dls ' family dl < forallpizero > and the real world", "abstract": "this paper describes the analysis of a selected testbed of semantic web ontologies , by a sparql query , which determines those ontologies that can be related to the description logic dl < forallpizero > , introduced in [ 4 ] and studied in [ 9 ] . we will see that a reasonable number of them is expressible within such computationally efficient language . we expect that , in a long-term view , a temporalization of description logics , and consequently , of owl ( 2 ) , can open new perspectives for the inclusion in this language of a greater number of ontologies of the testbed and , hopefully , of the `` real world '' .", "topics": ["computational complexity theory"]}
{"title": "robust ensemble clustering using probability trajectories", "abstract": "although many successful ensemble clustering approaches have been developed in recent years , there are still two limitations to most of the existing approaches . first , they mostly overlook the issue of uncertain links , which may mislead the overall consensus process . second , they generally lack the ability to incorporate global information to refine the local links . to address these two limitations , in this paper , we propose a novel ensemble clustering approach based on sparse graph representation and probability trajectory analysis . in particular , we present the elite neighbor selection strategy to identify the uncertain links by locally adaptive thresholds and build a sparse graph with a small number of probably reliable links . we argue that a small number of probably reliable links can lead to significantly better consensus results than using all graph links regardless of their reliability . the random walk process driven by a new transition probability matrix is utilized to explore the global information in the graph . we derive a novel and dense similarity measure from the sparse graph by analyzing the probability trajectories of the random walkers , based on which two consensus functions are further proposed . experimental results on multiple real-world datasets demonstrate the effectiveness and efficiency of our approach .", "topics": ["cluster analysis", "sparse matrix"]}
{"title": "an efficient secure multimodal biometric fusion using palmprint and face image", "abstract": "biometrics based personal identification is regarded as an effective method for automatically recognizing , with a high confidence a person 's identity . a multimodal biometric systems consolidate the evidence presented by multiple biometric sources and typically better recognition performance compare to system based on a single biometric modality . this paper proposes an authentication method for a multimodal biometric system identification using two traits i.e . face and palmprint . the proposed system is designed for application where the training data contains a face and palmprint . integrating the palmprint and face features increases robustness of the person authentication . the final decision is made by fusion at matching score level architecture in which features vectors are created independently for query measures and are then compared to the enrolment template , which are stored during database preparation . multimodal biometric system is developed through fusion of face and palmprint recognition .", "topics": ["test set"]}
{"title": "identifying independence in relational models", "abstract": "the rules of d-separation provide a framework for deriving conditional independence facts from model structure . however , this theory only applies to simple directed graphical models . we introduce relational d-separation , a theory for deriving conditional independence in relational models . we provide a sound , complete , and computationally efficient method for relational d-separation , and we present empirical results that demonstrate effectiveness .", "topics": ["graphical model", "computational complexity theory"]}
{"title": "enhancing twitter data analysis with simple semantic filtering : example in tracking influenza-like illnesses", "abstract": "systems that exploit publicly available user generated content such as twitter messages have been successful in tracking seasonal influenza . we developed a novel filtering method for influenza-like-illnesses ( ili ) -related messages using 587 million messages from twitter micro-blogs . we first filtered messages based on syndrome keywords from the biocaster ontology , an extant knowledge model of laymen 's terms . we then filtered the messages according to semantic features such as negation , hashtags , emoticons , humor and geography . the data covered 36 weeks for the us 2009 influenza season from 30th august 2009 to 8th may 2010 . results showed that our system achieved the highest pearson correlation coefficient of 98.46 % ( p-value < 2.2e-16 ) , an improvement of 3.98 % over the previous state-of-the-art method . the results indicate that simple nlp-based enhancements to existing approaches to mine twitter data can increase the value of this inexpensive resource .", "topics": ["natural language processing", "coefficient"]}
{"title": "analysis of approximate stochastic gradient using quadratic constraints and sequential semidefinite programs", "abstract": "we present convergence rate analysis for the approximate stochastic gradient method , where individual gradient updates are corrupted by computation errors . we develop stochastic quadratic constraints to formulate a small linear matrix inequality ( lmi ) whose feasible set characterizes convergence properties of the approximate stochastic gradient . based on this lmi condition , we develop a sequential minimization approach to analyze the intricate trade-offs that couple stepsize selection , convergence rate , optimization accuracy , and robustness to gradient inaccuracy . we also analytically solve this lmi condition and obtain theoretical formulas that quantify the convergence properties of the approximate stochastic gradient under various assumptions on the loss functions .", "topics": ["approximation algorithm", "loss function"]}
{"title": "sparsifying neural network connections for face recognition", "abstract": "this paper proposes to learn high-performance deep convnets with sparse neural connections , referred to as sparse convnets , for face recognition . the sparse convnets are learned in an iterative way , each time one additional layer is sparsified and the entire model is re-trained given the initial weights learned in previous iterations . one important finding is that directly training the sparse convnet from scratch failed to find good solutions for face recognition , while using a previously learned denser model to properly initialize a sparser model is critical to continue learning effective features for face recognition . this paper also proposes a new neural correlation-based weight selection criterion and empirically verifies its effectiveness in selecting informative connections from previously learned models in each iteration . when taking a moderately sparse structure ( 26 % -76 % of weights in the dense model ) , the proposed sparse convnet model significantly improves the face recognition performance of the previous state-of-the-art deepid2+ models given the same training data , while it keeps the performance of the baseline model with only 12 % of the original parameters .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "maximum classifier discrepancy for unsupervised domain adaptation", "abstract": "in this work , we present a method for unsupervised domain adaptation ( uda ) , where we aim to transfer knowledge from a label-rich domain ( i.e . , a source domain ) to an unlabeled domain ( i.e . , a target domain ) . many adversarial learning methods have been proposed for this task . these methods train domain classifier networks ( i.e . , a discriminator ) to discriminate distinguish the features as either a source or target and train a feature generator network to mimic the discriminator.however , the domain classifier only tries to distinguish the features as a source or target and thus does not consider task-specific decision boundaries between classes . therefore , a trained generator can generate ambiguous features near class boundaries . to solve the problem , we propose a new approach that attempts to align distributions of source and target by utilizing the task-specific decision boundaries . we propose to utilize task-specific classifiers as discriminators that try to detect target samples that are far from the support of the source . a feature generator learns to generate target features inside the support to fool the classifiers . since the generator uses feedback from task-specific classifiers , it avoids generating target features near class boundaries . our method outperforms other methods on several datasets of image classification and semantic segmentation .", "topics": ["computer vision"]}
{"title": "prosodic features from large corpora of child-directed speech as predictors of the age of acquisition of words", "abstract": "the impressive ability of children to acquire language is a widely studied phenomenon , and the factors influencing the pace and patterns of word learning remains a subject of active research . although many models predicting the age of acquisition of words have been proposed , little emphasis has been directed to the raw input children achieve . in this work we present a comparatively large-scale multi-modal corpus of prosody-text aligned child directed speech . our corpus contains automatically extracted word-level prosodic features , and we investigate the utility of this information as predictors of age of acquisition . we show that prosody features boost predictive power in a regularized regression , and demonstrate their utility in the context of a multi-modal factorized language models trained and tested on child-directed speech .", "topics": ["text corpus"]}
{"title": "learning bayesian networks : the combination of knowledge and statistical data", "abstract": "we describe algorithms for learning bayesian networks from a combination of user knowledge and statistical data . the algorithms have two components : a scoring metric and a search procedure . the scoring metric takes a network structure , statistical data , and a user 's prior knowledge , and returns a score proportional to the posterior probability of the network structure given the data . the search procedure generates networks for evaluation by the scoring metric . our contributions are threefold . first , we identify two important properties of metrics , which we call event equivalence and parameter modularity . these properties have been mostly ignored , but when combined , greatly simplify the encoding of a user 's prior knowledge . in particular , a user can express her knowledge-for the most part-as a single prior bayesian network for the domain . second , we describe local search and annealing algorithms to be used in conjunction with scoring metrics . in the special case where each node has at most one parent , we show that heuristic search can be replaced with a polynomial algorithm to identify the networks with the highest score . third , we describe a methodology for evaluating bayesian-network learning algorithms . we apply this approach to a comparison of metrics and search procedures .", "topics": ["simulation", "bayesian network"]}
{"title": "online learning in decentralized multiuser resource sharing problems", "abstract": "in this paper , we consider the general scenario of resource sharing in a decentralized system when the resource rewards/qualities are time-varying and unknown to the users , and using the same resource by multiple users leads to reduced quality due to resource sharing . firstly , we consider a user-independent reward model with no communication between the users , where a user gets feedback about the congestion level in the resource it uses . secondly , we consider user-specific rewards and allow costly communication between the users . the users have a cooperative goal of achieving the highest system utility . there are multiple obstacles in achieving this goal such as the decentralized nature of the system , unknown resource qualities , communication , computation and switching costs . we propose distributed learning algorithms with logarithmic regret with respect to the optimal allocation . our logarithmic regret result holds under both i.i.d . and markovian reward models , as well as under communication , computation and switching costs .", "topics": ["regret ( decision theory )", "mathematical optimization"]}
{"title": "expected policy gradients", "abstract": "we propose expected policy gradients ( epg ) , which unify stochastic policy gradients ( spg ) and deterministic policy gradients ( dpg ) for reinforcement learning . inspired by expected sarsa , epg integrates across the action when estimating the gradient , instead of relying only on the action in the sampled trajectory . we establish a new general policy gradient theorem , of which the stochastic and deterministic policy gradient theorems are special cases . we also prove that epg reduces the variance of the gradient estimates without requiring deterministic policies and , for the gaussian case , with no computational overhead . finally , we show that it is optimal in a certain sense to explore with a gaussian policy such that the covariance is proportional to the exponential of the scaled hessian of the critic with respect to the actions . we present empirical results confirming that this new form of exploration substantially outperforms dpg with the ornstein-uhlenbeck heuristic in four challenging mujoco domains .", "topics": ["reinforcement learning", "gradient descent"]}
{"title": "large scale , large margin classification using indefinite similarity measures", "abstract": "despite the success of the popular kernelized support vector machines , they have two major limitations : they are restricted to positive semi-definite ( psd ) kernels , and their training complexity scales at least quadratically with the size of the data . many natural measures of similarity between pairs of samples are not psd e.g . invariant kernels , and those that are implicitly or explicitly defined by latent variable models . in this paper , we investigate scalable approaches for using indefinite similarity measures in large margin frameworks . in particular we show that a normalization of similarity to a subset of the data points constitutes a representation suitable for linear classifiers . the result is a classifier which is competitive to kernelized svm in terms of accuracy , despite having better training and test time complexities . experimental results demonstrate that on cifar-10 dataset , the model equipped with similarity measures invariant to rigid and non-rigid deformations , can be made more than 5 times sparser while being more accurate than kernelized svm using rbf kernels .", "topics": ["support vector machine", "scalability"]}
{"title": "properties of the least squares temporal difference learning algorithm", "abstract": "this paper presents four different ways of looking at the well-known least squares temporal differences ( lstd ) algorithm for computing the value function of a markov reward process , each of them leading to different insights : the operator-theory approach via the galerkin method , the statistical approach via instrumental variables , the linear dynamical system view as well as the limit of the td iteration . we also give a geometric view of the algorithm as an oblique projection . furthermore , there is an extensive comparison of the optimization problem solved by lstd as compared to bellman residual minimization ( brm ) . we then review several schemes for the regularization of the lstd solution . we then proceed to treat the modification of lstd for the case of episodic markov reward processes .", "topics": ["optimization problem", "value ( ethics )"]}
{"title": "representation of texts as complex networks : a mesoscopic approach", "abstract": "statistical techniques that analyze texts , referred to as text analytics , have departed from the use of simple word count statistics towards a new paradigm . text mining now hinges on a more sophisticated set of methods , including the representations in terms of complex networks . while well-established word-adjacency ( co-occurrence ) methods successfully grasp syntactical features of written texts , they are unable to represent important aspects of textual data , such as its topical structure , i.e . the sequence of subjects developing at a mesoscopic level along the text . such aspects are often overlooked by current methodologies . in order to grasp the mesoscopic characteristics of semantical content in written texts , we devised a network model which is able to analyze documents in a multi-scale fashion . in the proposed model , a limited amount of adjacent paragraphs are represented as nodes , which are connected whenever they share a minimum semantical content . to illustrate the capabilities of our model , we present , as a case example , a qualitative analysis of `` alice 's adventures in wonderland '' . we show that the mesoscopic structure of a document , modeled as a network , reveals many semantic traits of texts . such an approach paves the way to a myriad of semantic-based applications . in addition , our approach is illustrated in a machine learning context , in which texts are classified among real texts and randomized instances .", "topics": ["text corpus"]}
{"title": "supervised and unsupervised transfer learning for question answering", "abstract": "although transfer learning has been shown to be successful for tasks like object and speech recognition , its applicability to question answering ( qa ) has yet to be well-studied . in this paper , we conduct extensive experiments to investigate the transferability of knowledge learned from a source qa dataset to a target dataset using two qa models . the performance of both models on a toefl listening comprehension test ( tseng et al . , 2016 ) and mctest ( richardson et al . , 2013 ) is significantly improved via a simple transfer learning technique from movieqa ( tapaswi et al . , 2016 ) . in particular , one of the models achieves the state-of-the-art on all target datasets ; for the toefl listening comprehension test , it outperforms the previous best model by 7 % . finally , we show that transfer learning is helpful even in unsupervised scenarios when correct answers for target qa dataset examples are not available .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "benchmarking quantum hardware for training of fully visible boltzmann machines", "abstract": "quantum annealing ( qa ) is a hardware-based heuristic optimization and sampling method applicable to discrete undirected graphical models . while similar to simulated annealing , qa relies on quantum , rather than thermal , effects to explore complex search spaces . for many classes of problems , qa is known to offer computational advantages over simulated annealing . here we report on the ability of recent qa hardware to accelerate training of fully visible boltzmann machines . we characterize the sampling distribution of qa hardware , and show that in many cases , the quantum distributions differ significantly from classical boltzmann distributions . in spite of this difference , training ( which seeks to match data and model statistics ) using standard classical gradient updates is still effective . we investigate the use of qa for seeding markov chains as an alternative to contrastive divergence ( cd ) and persistent contrastive divergence ( pcd ) . using $ k=50 $ gibbs steps , we show that for problems with high-energy barriers between modes , qa-based seeds can improve upon chains with cd and pcd initializations . for these hard problems , qa gradient estimates are more accurate , and allow for faster learning . furthermore , and interestingly , even the case of raw qa samples ( that is , $ k=0 $ ) achieved similar improvements . we argue that this relates to the fact that we are training a quantum rather than classical boltzmann distribution in this case . the learned parameters give rise to hardware qa distributions closely approximating classical boltzmann distributions that are hard to train with cd/pcd .", "topics": ["sampling ( signal processing )", "graphical model"]}
{"title": "sense tagging : semantic tagging with a lexicon", "abstract": "sense tagging , the automatic assignment of the appropriate sense from some lexicon to each of the words in a text , is a specialised instance of the general problem of semantic tagging by category or type . we discuss which recent word sense disambiguation algorithms are appropriate for sense tagging . it is our belief that sense tagging can be carried out effectively by combining several simple , independent , methods and we include the design of such a tagger . a prototype of this system has been implemented , correctly tagging 86 % of polysemous word tokens in a small test set , providing evidence that our hypothesis is correct .", "topics": ["test set"]}
{"title": "clothing and people - a social signal processing perspective", "abstract": "in our society and century , clothing is not anymore used only as a means for body protection . our paper builds upon the evidence , studied within the social sciences , that clothing brings a clear communicative message in terms of social signals , influencing the impression and behaviour of others towards a person . in fact , clothing correlates with personality traits , both in terms of self-assessment and assessments that unacquainted people give to an individual . the consequences of these facts are important : the influence of clothing on the decision making of individuals has been investigated in the literature , showing that it represents a discriminative factor to differentiate among diverse groups of people . unfortunately , this has been observed after cumbersome and expensive manual annotations , on very restricted populations , limiting the scope of the resulting claims . with this position paper , we want to sketch the main steps of the very first systematic analysis , driven by social signal processing techniques , of the relationship between clothing and social signals , both sent and perceived . thanks to human parsing technologies , which exhibit high robustness owing to deep learning architectures , we are now capable to isolate visual patterns characterising a large types of garments . these algorithms will be used to capture statistical relations on a large corpus of evidence to confirm the sociological findings and to go beyond the state of the art .", "topics": ["parsing", "text corpus"]}
{"title": "rosoclingo : a ros package for asp-based robot control", "abstract": "knowledge representation and reasoning capacities are vital to cognitive robotics because they provide higher level cognitive functions for reasoning about actions , environments , goals , perception , etc . although answer set programming ( asp ) is well suited for modelling such functions , there was so far no seamless way to use asp in a robotic environment . we address this shortcoming and show how a recently developed reactive asp system can be harnessed to provide appropriate reasoning capacities within a robotic system . to be more precise , we furnish a package integrating the reactive asp solver oclingo with the popular open-source robotic middleware ros . the resulting system , rosoclingo , provides a generic way by which an asp program can be used to control the behaviour of a robot and to respond to the results of the robot 's actions .", "topics": ["robot"]}
{"title": "the biodynamo project", "abstract": "computer simulations have become a very powerful tool for scientific research . given the vast complexity that comes with many open scientific questions , a purely analytical or experimental approach is often not viable . for example , biological systems ( such as the human brain ) comprise an extremely complex organization and heterogeneous interactions across different spatial and temporal scales . in order to facilitate research on such problems , the biodynamo project ( \\url { https : //biodynamo.web.cern.ch/ } ) aims at a general platform for computer simulations for biological research . since the scientific investigations require extensive computer resources , this platform should be executable on hybrid cloud computing systems , allowing for the efficient use of state-of-the-art computing technology . this paper describes challenges during the early stages of the software development process . in particular , we describe issues regarding the implementation and the highly interdisciplinary as well as international nature of the collaboration . moreover , we explain the methodologies , the approach , and the lessons learnt by the team during these first stages .", "topics": ["simulation", "interaction"]}
{"title": "interactive video object segmentation in the wild", "abstract": "in this paper we present our system for human-in-the-loop video object segmentation . the backbone of our system is a method for one-shot video object segmentation . while fast , this method requires an accurate pixel-level segmentation of one ( or several ) frames as input . as manually annotating such a segmentation is impractical , we propose a deep interactive image segmentation method , that can accurately segment objects with only a handful of clicks . on the grabcut dataset , our method obtains 90 % iou with just 3.8 clicks on average , setting the new state of the art . furthermore , as our method iteratively refines an initial segmentation , it can effectively correct frames where the video object segmentation fails , thus allowing users to quickly obtain high quality results even on challenging sequences . finally , we investigate usage patterns and give insights in how many steps users take to annotate frames , what kind of corrections they provide , etc . , thus giving important insights for further improving interactive video segmentation .", "topics": ["image segmentation", "pixel"]}
{"title": "study of a robust algorithm applied in the optimal position tuning for the camera lens in automated visual inspection systems", "abstract": "this paper present the mathematical fundaments and experimental study of an algorithm used to find the optimal position for the camera lens to obtain a maximum of details . this information can be further applied to a appropriate system to automatically correct this position . the algorithm is based on the evaluation of a so called resolution function who calculates the maximum of gradient in a certain zone of the image . the paper also presents alternative forms of the function , results of measurements and set up a set of practical rules for the right application of the algorithm .", "topics": ["gradient"]}
{"title": "correlation-based construction of neighborhood and edge features", "abstract": "motivated by an abstract notion of low-level edge detector filters , we propose a simple method of unsupervised feature construction based on pairwise statistics of features . in the first step , we construct neighborhoods of features by regrouping features that correlate . then we use these subsets as filters to produce new neighborhood features . next , we connect neighborhood features that correlate , and construct edge features by subtracting the correlated neighborhood features of each other . to validate the usefulness of the constructed features , we ran adaboost.mh on four multi-class classification problems . our most significant result is a test error of 0.94 % on mnist with an algorithm which is essentially free of any image-specific priors . on cifar-10 our method is suboptimal compared to today 's best deep learning techniques , nevertheless , we show that the proposed method outperforms not only boosting on the raw pixels , but also boosting on haar filters .", "topics": ["feature vector", "mnist database"]}
{"title": "malaria likelihood prediction by effectively surveying households using deep reinforcement learning", "abstract": "we build a deep reinforcement learning ( rl ) agent that can predict the likelihood of an individual testing positive for malaria by asking questions about their household . the rl agent learns to determine which survey question to ask next and when to stop to make a prediction about their likelihood of malaria based on their responses hitherto . the agent incurs a small penalty for each question asked , and a large reward/penalty for making the correct/wrong prediction ; it thus has to learn to balance the length of the survey with the accuracy of its final predictions . our rl agent is a deep q-network that learns a policy directly from the responses to the questions , with an action defined for each possible survey question and for each possible prediction class . we focus on kenya , where malaria is a massive health burden , and train the rl agent on a dataset of 6481 households from the kenya malaria indicator survey 2015 . to investigate the importance of having survey questions be adaptive to responses , we compare our rl agent to a supervised learning ( sl ) baseline that fixes its set of survey questions a priori . we evaluate on prediction accuracy and on the number of survey questions asked on a holdout set and find that the rl agent is able to predict with 80 % accuracy , using only 2.5 questions on average . in addition , the rl agent learns to survey adaptively to responses and is able to match the sl baseline in prediction accuracy while significantly reducing survey length .", "topics": ["baseline ( configuration management )", "supervised learning"]}
{"title": "semi-supervised generation with cluster-aware generative models", "abstract": "deep generative models trained with large amounts of unlabelled data have proven to be powerful within the domain of unsupervised learning . many real life data sets contain a small amount of labelled data points , that are typically disregarded when training generative models . we propose the cluster-aware generative model , that uses unlabelled information to infer a latent representation that models the natural clustering of the data , and additional labelled data points to refine this clustering . the generative performances of the model significantly improve when labelled information is exploited , obtaining a log-likelihood of -79.38 nats on permutation invariant mnist , while also achieving competitive semi-supervised classification accuracies . the model can also be trained fully unsupervised , and still improve the log-likelihood performance with respect to related methods .", "topics": ["generative model", "supervised learning"]}
{"title": "domain adaptation for neural networks by parameter augmentation", "abstract": "we propose a simple domain adaptation method for neural networks in a supervised setting . supervised domain adaptation is a way of improving the generalization performance on the target domain by using the source domain dataset , assuming that both of the datasets are labeled . recently , recurrent neural networks have been shown to be successful on a variety of nlp tasks such as caption generation ; however , the existing domain adaptation techniques are limited to ( 1 ) tune the model parameters by the target dataset after the training by the source dataset , or ( 2 ) design the network to have dual output , one for the source domain and the other for the target domain . reformulating the idea of the domain adaptation technique proposed by daume ( 2007 ) , we propose a simple domain adaptation method , which can be applied to neural networks trained with a cross-entropy loss . on captioning datasets , we show performance improvements over other domain adaptation methods .", "topics": ["natural language processing", "recurrent neural network"]}
{"title": "robust regression via hard thresholding", "abstract": "we study the problem of robust least squares regression ( rlsr ) where several response variables can be adversarially corrupted . more specifically , for a data matrix x \\in r^ { p x n } and an underlying model w* , the response vector is generated as y = x'w* + b where b \\in r^n is the corruption vector supported over at most c.n coordinates . existing exact recovery results for rlsr focus solely on l1-penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of x . in this work , we study a simple hard-thresholding algorithm called torrent which , under mild conditions on x , can recover w* exactly even if b corrupts the response variables in an adversarial manner , i.e . both the support and entries of b are selected adversarially after observing x and w* . our results hold under deterministic assumptions which are satisfied if x is sampled from any sub-gaussian distribution . finally unlike existing results that apply only to a fixed w* , generated independently of x , our results are universal and hold for any w* \\in r^p . next , we propose gradient descent-based extensions of torrent that can scale efficiently to large scale problems , such as high dimensional sparse recovery and prove similar recovery guarantees for these extensions . empirically we find torrent , and more so its extensions , offering significantly faster recovery than the state-of-the-art l1 solvers . for instance , even on moderate-sized datasets ( with p = 50k ) with around 40 % corrupted responses , a variant of our proposed method called torrent-hyb is more than 20x faster than the best l1 solver .", "topics": ["sparse matrix", "gradient descent"]}
{"title": "light field stitching for extended synthetic aperture", "abstract": "through capturing spatial and angular radiance distribution , light field cameras introduce new capabilities that are not possible with conventional cameras . so far in the light field imaging literature , the focus has been on the theory and applications of single light field capture . by combining multiple light fields , it is possible to obtain new capabilities and enhancements , and even exceed physical limitations , such as spatial resolution and aperture size of the imaging device . in this paper , we present an algorithm to register and stitch multiple light fields . we utilize the regularity of the spatial and angular sampling in light field data , and extend some techniques developed for stereo vision systems to light field data . such an extension is not straightforward for a micro-lens array ( mla ) based light field camera due to extremely small baseline and low spatial resolution . by merging multiple light fields captured by an mla based camera , we obtain larger synthetic aperture , which results in improvements in light field capabilities , such as increased depth estimation range/accuracy and wider perspective shift range .", "topics": ["sampling ( signal processing )", "baseline ( configuration management )"]}
{"title": "gene ontology ( go ) prediction using machine learning methods", "abstract": "we applied machine learning to predict whether a gene is involved in axon regeneration . we extracted 31 features from different databases and trained five machine learning models . our optimal model , a random forest classifier with 50 submodels , yielded a test score of 85.71 % , which is 4.1 % higher than the baseline score . we concluded that our models have some predictive capability . similar methodology and features could be applied to predict other gene ontology ( go ) terms .", "topics": ["baseline ( configuration management )", "database"]}
{"title": "morphological constraints for phrase pivot statistical machine translation", "abstract": "the lack of parallel data for many language pairs is an important challenge to statistical machine translation ( smt ) . one common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages . although pivoting is a robust technique , it introduces some low quality translations especially when a poor morphology language is used as the pivot between rich morphology languages . in this paper , we examine the use of synchronous morphology constraint features to improve the quality of phrase pivot smt . we compare hand-crafted constraints to those learned from limited parallel data between source and target languages . the learned morphology constraints are based on projected align- ments between the source and target phrases in the pivot phrase table . we show positive results on hebrew-arabic smt ( pivoting on english ) . we get 1.5 bleu points over a phrase pivot baseline and 0.8 bleu points over a system combination baseline with a direct model built from parallel data .", "topics": ["baseline ( configuration management )", "machine translation"]}
{"title": "a bayesian approach to discovering truth from conflicting sources for data integration", "abstract": "in practical data integration systems , it is common for the data sources being integrated to provide conflicting information about the same entity . consequently , a major challenge for data integration is to derive the most complete and accurate integrated records from diverse and sometimes conflicting sources . we term this challenge the truth finding problem . we observe that some sources are generally more reliable than others , and therefore a good model of source quality is the key to solving the truth finding problem . in this work , we propose a probabilistic graphical model that can automatically infer true records and source quality without any supervision . in contrast to previous methods , our principled approach leverages a generative process of two types of errors ( false positive and false negative ) by modeling two different aspects of source quality . in so doing , ours is also the first approach designed to merge multi-valued attribute types . our method is scalable , due to an efficient sampling-based inference algorithm that needs very few iterations in practice and enjoys linear time complexity , with an even faster incremental variant . experiments on two real world datasets show that our new method outperforms existing state-of-the-art approaches to the truth finding problem .", "topics": ["graphical model", "time complexity"]}
{"title": "speeding up the binary gaussian process classification", "abstract": "gaussian processes ( gp ) are attractive building blocks for many probabilistic models . their drawbacks , however , are the rapidly increasing inference time and memory requirement alongside increasing data . the problem can be alleviated with compactly supported ( cs ) covariance functions , which produce sparse covariance matrices that are fast in computations and cheap to store . cs functions have previously been used in gp regression but here the focus is in a classification problem . this brings new challenges since the posterior inference has to be done approximately . we utilize the expectation propagation algorithm and show how its standard implementation has to be modified to obtain computational benefits from the sparse covariance matrices . we study four cs covariance functions and show that they may lead to substantial speed up in the inference time compared to globally supported functions .", "topics": ["sparse matrix", "computation"]}
{"title": "between theory and practice : guidelines for an optimization scheme with genetic algorithms - part i : single-objective continuous global optimization", "abstract": "the rapid advances in the field of optimization methods in many pure and applied science pose the difficulty of keeping track of the developments as well as selecting an appropriate technique that best suits the problem in-hand . from a practitioner point of view is rightful to wander `` which optimization method is the best for my problem ? '' . looking at the optimization process as a `` system '' of intercon- nected parts , in this paper are collected some ideas about how to tackle an optimization problem using a class of tools from evolutionary computations called genetic algorithms . despite the number of optimization techniques available nowadays the author of this paper thinks that genetic algorithms still play a central role for their versatility , robustness , theoretical framework and simplicity of use . the paper can be considered a `` collection of tips '' ( from literature and personal experience ) for the non-computer-scientist that has to deal with optimization problems both in the science and engineering practice . no original methods or algorithms are proposed .", "topics": ["mathematical optimization", "optimization problem"]}
{"title": "on the definition of a general learning system with user-defined operators", "abstract": "in this paper , we push forward the idea of machine learning systems whose operators can be modified and fine-tuned for each problem . this allows us to propose a learning paradigm where users can write ( or adapt ) their operators , according to the problem , data representation and the way the information should be navigated . to achieve this goal , data instances , background knowledge , rules , programs and operators are all written in the same functional language , erlang . since changing operators affect how the search space needs to be explored , heuristics are learnt as a result of a decision process based on reinforcement learning where each action is defined as a choice of operator and rule . as a result , the architecture can be seen as a 'system for writing machine learning systems ' or to explore new operators where the policy reuse ( as a kind of transfer learning ) is allowed . states and actions are represented in a q matrix which is actually a table , from which a supervised model is learnt . this makes it possible to have a more flexible mapping between old and new problems , since we work with an abstraction of rules and actions . we include some examples sharing reuse and the application of the system gerl to iq problems . in order to evaluate gerl , we will test it against some structured problems : a selection of iq test tasks and some experiments on some structured prediction problems ( list patterns ) .", "topics": ["reinforcement learning", "heuristic"]}
{"title": "analyzing and improving statistical language models for speech recognition", "abstract": "in many current speech recognizers , a statistical language model is used to indicate how likely it is that a certain word will be spoken next , given the words recognized so far . how can statistical language models be improved so that more complex speech recognition tasks can be tackled ? since the knowledge of the weaknesses of any theory often makes improving the theory easier , the central idea of this thesis is to analyze the weaknesses of existing statistical language models in order to subsequently improve them . to that end , we formally define a weakness of a statistical language model in terms of the logarithm of the total probability , ltp , a term closely related to the standard perplexity measure used to evaluate statistical language models . we apply our definition of a weakness to a frequently used statistical language model , called a bi-pos model . this results , for example , in a new modeling of unknown words which improves the performance of the model by 14 % to 21 % . moreover , one of the identified weaknesses has prompted the development of our generalized n-pos language model , which is also outlined in this thesis . it can incorporate linguistic knowledge even if it extends over many words and this is not feasible in a traditional n-pos model . this leads to a discussion of whatknowledge should be added to statistical language models in general and we give criteria for selecting potentially useful knowledge . these results show the usefulness of both our definition of a weakness and of performing an analysis of weaknesses of statistical language models in general .", "topics": ["speech recognition"]}
{"title": "event-triggered learning for resource-efficient networked control", "abstract": "common event-triggered state estimation ( etse ) algorithms save communication in networked control systems by predicting agents ' behavior , and transmitting updates only when the predictions deviate significantly . the effectiveness in reducing communication thus heavily depends on the quality of the dynamics models used to predict the agents ' states or measurements . event-triggered learning is proposed herein as a novel concept to further reduce communication : whenever poor communication performance is detected , an identification experiment is triggered and an improved prediction model learned from data . effective learning triggers are obtained by comparing the actual communication rate with the one that is expected based on the current model . by analyzing statistical properties of the inter-communication times and leveraging powerful convergence results , the proposed trigger is proven to limit learning experiments to the necessary instants . numerical and physical experiments demonstrate that event-triggered learning improves robustness toward changing environments and yields lower communication rates than common etse .", "topics": ["numerical analysis"]}
{"title": "3d facial expression reconstruction using cascaded regression", "abstract": "this paper proposes a novel model fitting algorithm for 3d facial expression reconstruction from a single image . face expression reconstruction from a single image is a challenging task in computer vision . most state-of-the-art methods fit the input image to a 3d morphable model ( 3dmm ) . these methods need to solve a stochastic problem and can not deal with expression and pose variations . to solve this problem , we adopt a 3d face expression model and use a combined feature which is robust to scale , rotation and different lighting conditions . the proposed method applies a cascaded regression framework to estimate parameters for the 3dmm . 2d landmarks are detected and used to initialize the 3d shape and mapping matrices . in each iteration , residues between the current 3dmm parameters and the ground truth are estimated and then used to update the 3d shapes . the mapping matrices are also calculated based on the updated shapes and 2d landmarks . hog features of the local patches and displacements between 3d landmark projections and 2d landmarks are exploited . compared with existing methods , the proposed method is robust to expression and pose changes and can reconstruct higher fidelity 3d face shape .", "topics": ["computer vision", "iteration"]}
{"title": "mindx : denoising mixed impulse poisson-gaussian noise using proximal algorithms", "abstract": "we present a novel algorithm for blind denoising of images corrupted by mixed impulse , poisson , and gaussian noises . the algorithm starts by applying the anscombe variance-stabilizing transformation to convert the poisson into white gaussian noise . then it applies a combinatorial optimization technique to denoise the mixed impulse gaussian noise using proximal algorithms . the result is then processed by the inverse anscombe transform . we compare our algorithm to state of the art methods on standard images , and show its superior performance in various noise conditions .", "topics": ["noise reduction"]}
{"title": "effective scaling registration approach by imposing the emphasis on the scale factor", "abstract": "this paper proposes an effective approach for the scaling registration of $ m $ -d point sets . different from the rigid transformation , the scaling registration can not be formulated into the common least square function due to the ill-posed problem caused by the scale factor . therefore , this paper designs a novel objective function for the scaling registration problem . the appearance of this objective function is a rational fraction , where the numerator item is the least square error and the denominator item is the square of the scale factor . by imposing the emphasis on scale factor , the ill-posed problem can be avoided in the scaling registration . subsequently , the new objective function can be solved by the proposed scaling iterative closest point ( icp ) algorithm , which can obtain the optimal scaling transformation . for the practical applications , the scaling icp algorithm is further extended to align partially overlapping point sets . finally , the proposed approach is tested on public data sets and applied to merging grid maps of different resolutions . experimental results demonstrate its superiority over previous approaches on efficiency and robustness .", "topics": ["optimization problem", "loss function"]}
{"title": "biologically inspired feedforward supervised learning for deep self-organizing map networks", "abstract": "in this study , we propose a novel deep neural network and its supervised learning method that uses a feedforward supervisory signal . the method is inspired by the human visual system and performs human-like association-based learning without any backward error propagation . the feedforward supervisory signal that produces the correct result is preceded by the target signal and associates its confirmed label with the classification result of the target signal . it effectively uses a large amount of information from the feedforward signal , and forms a continuous and rich learning representation . the method is validated using visual recognition tasks on the mnist handwritten dataset .", "topics": ["feature learning", "supervised learning"]}
{"title": "arguments for the effectiveness of human problem solving", "abstract": "the question of how humans solve problem has been addressed extensively . however , the direct study of the effectiveness of this process seems to be overlooked . in this paper , we address the issue of the effectiveness of human problem solving : we analyze where this effectiveness comes from and what cognitive mechanisms or heuristics are involved . our results are based on the optimal probabilistic problem solving strategy that appeared in solomonoff paper on general problem solving system . we provide arguments that a certain set of cognitive mechanisms or heuristics drive human problem solving in the similar manner as the optimal solomonoff strategy . the results presented in this paper can serve both cognitive psychology in better understanding of human problem solving processes as well as artificial intelligence in designing more human-like agents .", "topics": ["artificial intelligence", "artificial intelligence"]}
{"title": "releaf : an algorithm for learning and exploiting relevance", "abstract": "recommender systems , medical diagnosis , network security , etc . , require on-going learning and decision-making in real time . these -- and many others -- represent perfect examples of the opportunities and difficulties presented by big data : the available information often arrives from a variety of sources and has diverse features so that learning from all the sources may be valuable but integrating what is learned is subject to the curse of dimensionality . this paper develops and analyzes algorithms that allow efficient learning and decision-making while avoiding the curse of dimensionality . we formalize the information available to the learner/decision-maker at a particular time as a context vector which the learner should consider when taking actions . in general the context vector is very high dimensional , but in many settings , the most relevant information is embedded into only a few relevant dimensions . if these relevant dimensions were known in advance , the problem would be simple -- but they are not . moreover , the relevant dimensions may be different for different actions . our algorithm learns the relevant dimensions for each action , and makes decisions based in what it has learned . formally , we build on the structure of a contextual multi-armed bandit by adding and exploiting a relevance relation . we prove a general regret bound for our algorithm whose time order depends only on the maximum number of relevant dimensions among all the actions , which in the special case where the relevance relation is single-valued ( a function ) , reduces to $ \\tilde { o } ( t^ { 2 ( \\sqrt { 2 } -1 ) } ) $ ; in the absence of a relevance relation , the best known contextual bandit algorithms achieve regret $ \\tilde { o } ( t^ { ( d+1 ) / ( d+2 ) } ) $ , where $ d $ is the full dimension of the context vector .", "topics": ["regret ( decision theory )", "relevance"]}
{"title": "pca-based missing information imputation for real-time crash likelihood prediction under imbalanced data", "abstract": "the real-time crash likelihood prediction has been an important research topic . various classifiers , such as support vector machine ( svm ) and tree-based boosting algorithms , have been proposed in traffic safety studies . however , few research focuses on the missing data imputation in real-time crash likelihood prediction , although missing values are commonly observed due to breakdown of sensors or external interference . besides , classifying imbalanced data is also a difficult problem in real-time crash likelihood prediction , since it is hard to distinguish crash-prone cases from non-crash cases which compose the majority of the observed samples . in this paper , principal component analysis ( pca ) based approaches , including ls-pca , ppca , and vbpca , are employed for imputing missing values , while two kinds of solutions are developed to solve the problem in imbalanced data . the results show that ppca and vbpca not only outperform ls-pca and other imputation methods ( including mean imputation and k-means clustering imputation ) , in terms of the root mean square error ( rmse ) , but also help the classifiers achieve better predictive performance . the two solutions , i.e . , cost-sensitive learning and synthetic minority oversampling technique ( smote ) , help improve the sensitivity by adjusting the classifiers to pay more attention to the minority class .", "topics": ["cluster analysis", "support vector machine"]}
{"title": "weakly-supervised spatial context networks", "abstract": "we explore the power of spatial context as a self-supervisory signal for learning visual representations . in particular , we propose spatial context networks that learn to predict a representation of one image patch from another image patch , within the same image , conditioned on their real-valued relative spatial offset . unlike auto-encoders , that aim to encode and reconstruct original image patches , our network aims to encode and reconstruct intermediate representations of the spatially offset patches . as such , the network learns a spatially conditioned contextual representation . by testing performance with various patch selection mechanisms we show that focusing on object-centric patches is important , and that using object proposal as a patch selection mechanism leads to the highest improvement in performance . further , unlike auto-encoders , context encoders [ 21 ] , or other forms of unsupervised feature learning , we illustrate that contextual supervision ( with pre-trained model initialization ) can improve on existing pre-trained model performance . we build our spatial context networks on top of standard vgg_19 and cnn_m architectures and , among other things , show that we can achieve improvements ( with no additional explicit supervision ) over the original imagenet pre-trained vgg_19 and cnn_m models in object categorization and detection on voc2007 .", "topics": ["feature learning", "encoder"]}
{"title": "d2ke : from distance to kernel and embedding", "abstract": "for many machine learning problem settings , particularly with structured inputs such as sequences or sets of objects , a distance measure between inputs can be specified more naturally than a feature representation . however , most standard machine models are designed for inputs with a vector feature representation . in this work , we consider the estimation of a function $ f : \\mathcal { x } \\rightarrow \\r $ based solely on a dissimilarity measure $ d : \\mathcal { x } \\times\\mathcal { x } \\rightarrow \\r $ between inputs . in particular , we propose a general framework to derive a family of \\emph { positive definite kernels } from a given dissimilarity measure , which subsumes the widely-used \\emph { representative-set method } as a special case , and relates to the well-known \\emph { distance substitution kernel } in a limiting case . we show that functions in the corresponding reproducing kernel hilbert space ( rkhs ) are lipschitz-continuous w.r.t . the given distance metric . we provide a tractable algorithm to estimate a function from this rkhs , and show that it enjoys better generalizability than nearest-neighbor estimates . our approach draws from the literature of random features , but instead of deriving feature maps from an existing kernel , we construct novel kernels from a random feature map , that we specify given the distance measure . we conduct classification experiments with such disparate domains as strings , time series , and sets of vectors , where our proposed framework compares favorably to existing distance-based learning methods such as $ k $ -nearest-neighbors , distance-substitution kernels , pseudo-euclidean embedding , and the representative-set method .", "topics": ["kernel ( operating system )", "time series"]}
{"title": "a formal model of dictionary structure and content", "abstract": "we show that a general model of lexical information conforms to an abstract model that reflects the hierarchy of information found in a typical dictionary entry . we show that this model can be mapped into a well-formed xml document , and how the xsl transformation language can be used to implement a semantics defined over the abstract model to enable extraction and manipulation of the information in any format .", "topics": ["dictionary"]}
{"title": "convolutional neural network-based place recognition", "abstract": "recently convolutional neural networks ( cnns ) have been shown to achieve state-of-the-art performance on various classification tasks . in this paper , we present for the first time a place recognition technique based on cnn models , by combining the powerful features learnt by cnns with a spatial and sequential filter . applying the system to a 70 km benchmark place recognition dataset we achieve a 75 % increase in recall at 100 % precision , significantly outperforming all previous state of the art techniques . we also conduct a comprehensive performance comparison of the utility of features from all 21 layers for place recognition , both for the benchmark dataset and for a second dataset with more significant viewpoint changes .", "topics": ["neural networks"]}
{"title": "building a linguistic corpus from bee dance data", "abstract": "this paper discusses the problems and possibility of collecting bee dance data in a linguistic \\textit { corpus } and use linguistic instruments such as zipf 's law and entropy statistics to decide on the question whether the dance carries information of any kind . we describe this against the historical background of attempts to analyse non-human communication systems .", "topics": ["text corpus"]}
{"title": "one-to-many face recognition with bilinear cnns", "abstract": "the recent explosive growth in convolutional neural network ( cnn ) research has produced a variety of new architectures for deep learning . one intriguing new architecture is the bilinear cnn ( b-cnn ) , which has shown dramatic performance gains on certain fine-grained recognition problems [ 15 ] . we apply this new cnn to the challenging new face recognition benchmark , the iarpa janus benchmark a ( ijb-a ) [ 12 ] . it features faces from a large number of identities in challenging real-world conditions . because the face images were not identified automatically using a computerized face detection system , it does not have the bias inherent in such a database . we demonstrate the performance of the b-cnn model beginning from an alexnet-style network pre-trained on imagenet . we then show results for fine-tuning using a moderate-sized and public external database , facescrub [ 17 ] . we also present results with additional fine-tuning on the limited training data provided by the protocol . in each case , the fine-tuned bilinear model shows substantial improvements over the standard cnn . finally , we demonstrate how a standard cnn pre-trained on a large face database , the recently released vgg-face model [ 20 ] , can be converted into a b-cnn without any additional feature training . this b-cnn improves upon the cnn performance on the ijb-a benchmark , achieving 89.5 % rank-1 recall .", "topics": ["test set", "database"]}
{"title": "regression-based hypergraph learning for image clustering and classification", "abstract": "inspired by the recently remarkable successes of sparse representation ( sr ) , collaborative representation ( cr ) and sparse graph , we present a novel hypergraph model named regression-based hypergraph ( rh ) which utilizes the regression models to construct the high quality hypergraphs . moreover , we plug rh into two conventional hypergraph learning frameworks , namely hypergraph spectral clustering and hypergraph transduction , to present regression-based hypergraph spectral clustering ( rhsc ) and regression-based hypergraph transduction ( rht ) models for addressing the image clustering and classification issues . sparse representation and collaborative representation are employed to instantiate two rh instances and their rhsc and rht algorithms . the experimental results on six popular image databases demonstrate that the proposed rh learning algorithms achieve promising image clustering and classification performances , and also validate that rh can inherit the desirable properties from both hypergraph models and regression models .", "topics": ["cluster analysis", "sparse matrix"]}
{"title": "fast color quantization using weighted sort-means clustering", "abstract": "color quantization is an important operation with numerous applications in graphics and image processing . most quantization methods are essentially based on data clustering algorithms . however , despite its popularity as a general purpose clustering algorithm , k-means has not received much respect in the color quantization literature because of its high computational requirements and sensitivity to initialization . in this paper , a fast color quantization method based on k-means is presented . the method involves several modifications to the conventional ( batch ) k-means algorithm including data reduction , sample weighting , and the use of triangle inequality to speed up the nearest neighbor search . experiments on a diverse set of images demonstrate that , with the proposed modifications , k-means becomes very competitive with state-of-the-art color quantization methods in terms of both effectiveness and efficiency .", "topics": ["image processing", "cluster analysis"]}
{"title": "compact representations of extended causal models", "abstract": "judea pearl was the first to propose a definition of actual causation using causal models . a number of authors have suggested that an adequate account of actual causation must appeal not only to causal structure , but also to considerations of normality . in earlier work , we provided a definition of actual causation using extended causal models , which include information about both causal structure and normality . extended causal models are potentially very complex . in this paper , we show how it is possible to achieve a compact representation of extended causal models .", "topics": ["causality"]}
{"title": "a logic-based approach to generatively defined discriminative modeling", "abstract": "conditional random fields ( crfs ) are usually specified by graphical models but in this paper we propose to use probabilistic logic programs and specify them generatively . our intension is first to provide a unified approach to crfs for complex modeling through the use of a turing complete language and second to offer a convenient way of realizing generative-discriminative pairs in machine learning to compare generative and discriminative models and choose the best model . we implemented our approach as the d-prism language by modifying prism , a logic-based probabilistic modeling language for generative modeling , while exploiting its dynamic programming mechanism for efficient probability computation . we tested d-prism with logistic regression , a linear-chain crf and a crf-cfg and empirically confirmed their excellent discriminative performance compared to their generative counterparts , i.e.\\ naive bayes , an hmm and a pcfg . we also introduced new crf models , crf-bncs and crf-lcgs . they are crf versions of bayesian network classifiers and probabilistic left-corner grammars respectively and easily implementable in d-prism . we empirically showed that they outperform their generative counterparts as expected .", "topics": ["generative model", "graphical model"]}
{"title": "man [ and woman ] vs. machine : a case study in base noun phrase learning", "abstract": "a great deal of work has been done demonstrating the ability of machine learning algorithms to automatically extract linguistic knowledge from annotated corpora . very little work has gone into quantifying the difference in ability at this task between a person and a machine . this paper is a first step in that direction .", "topics": ["text corpus"]}
{"title": "neural network model for path-planning of robotic rover systems", "abstract": "today , robotics is an auspicious and fast-growing branch of technology that involves the manufacturing , design , and maintenance of robot machines that can operate in an autonomous fashion and can be used in a wide variety of applications including space exploration , weaponry , household , and transportation . more particularly , in space applications , a common type of robots has been of widespread use in the recent years . it is called planetary rover which is a robot vehicle that moves across the surface of a planet and conducts detailed geological studies pertaining to the properties of the landing cosmic environment . however , rovers are always impeded by obstacles along the traveling path which can destabilize the rover 's body and prevent it from reaching its goal destination . this paper proposes an ann model that allows rover systems to carry out autonomous path-planning to successfully navigate through challenging planetary terrains and follow their goal location while avoiding dangerous obstacles . the proposed ann is a multilayer network made out of three layers : an input , a hidden , and an output layer . the network is trained in offline mode using back-propagation supervised learning algorithm . a software-simulated rover was experimented and it revealed that it was able to follow the safest trajectory despite existing obstacles . as future work , the proposed ann is to be parallelized so as to speed-up the execution time of the training process .", "topics": ["supervised learning", "simulation"]}
{"title": "statistical noise analysis in sense parallel mri", "abstract": "a complete first and second order statistical characterization of noise in sense reconstructed data is proposed . sense acquisitions have usually been modeled as rician distributed , since the data reconstruction takes place into the spatial domain , where gaussian noise is assumed . however , this model just holds for the first order statistics and obviates other effects induced by coils correlations and the reconstruction interpolation . those effects are properly taken into account in this study , in order to fully justify a final sense noise model . as a result , some interesting features of the reconstructed image arise : ( 1 ) there is a strong correlation between adjacent lines . ( 2 ) the resulting distribution is non-stationary and therefore the variance of noise will vary from point to point across the image . closed equations for the calculation of the variance of noise and the correlation coefficient between lines are proposed . the proposed model is totally compatible with g-factor formulations .", "topics": ["coefficient"]}
{"title": "a multi-instance deep neural network classifier : application to higgs boson cp measurement", "abstract": "we investigate properties of a classifier applied to the measurements of the cp state of the higgs boson in $ h\\rightarrow\\tau\\tau $ decays . the problem is framed as binary classifier applied to individual instances . then the prior knowledge that the instances belong to the same class is used to define the multi-instance classifier . its final score is calculated as multiplication of single instance scores for a given series of instances . in the paper we discuss properties of such classifier , notably its dependence on the number of instances in the series . this classifier exhibits very strong random dependence on the number of epochs used for training and requires careful tuning of the classification threshold . we derive formula for this optimal threshold .", "topics": ["statistical classification"]}
{"title": "recent advances in neural program synthesis", "abstract": "in recent years , deep learning has made tremendous progress in a number of fields that were previously out of reach for artificial intelligence . the successes in these problems has led researchers to consider the possibilities for intelligent systems to tackle a problem that humans have only recently themselves considered : program synthesis . this challenge is unlike others such as object recognition and speech translation , since its abstract nature and demand for rigor make it difficult even for human minds to attempt . while it is still far from being solved or even competitive with most existing methods , neural program synthesis is a rapidly growing discipline which holds great promise if completely realized . in this paper , we start with exploring the problem statement and challenges of program synthesis . then , we examine the fascinating evolution of program induction models , along with how they have succeeded , failed and been reimagined since . finally , we conclude with a contrastive look at program synthesis and future research recommendations for the field .", "topics": ["artificial intelligence"]}
{"title": "efficient gradient-based inference through transformations between bayes nets and neural nets", "abstract": "hierarchical bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models . we show that either of these types of models can often be transformed into an instance of the other , by switching between centered and differentiable non-centered parameterizations of the latent variables . the choice of parameterization greatly influences the efficiency of gradient-based posterior inference ; we show that they are often complementary to eachother , we clarify when each parameterization is preferred and show how inference can be made robust . in the non-centered form , a simple monte carlo estimator of the marginal likelihood can be used for learning the parameters . theoretical results are supported by experiments .", "topics": ["bayesian network", "gradient"]}
{"title": "stochastic inference with deterministic spiking neurons", "abstract": "the seemingly stochastic transient dynamics of neocortical circuits observed in vivo have been hypothesized to represent a signature of ongoing stochastic inference . in vitro neurons , on the other hand , exhibit a highly deterministic response to various types of stimulation . we show that an ensemble of deterministic leaky integrate-and-fire neurons embedded in a spiking noisy environment can attain the correct firing statistics in order to sample from a well-defined target distribution . we provide an analytical derivation of the activation function on the single cell level ; for recurrent networks , we examine convergence towards stationarity in computer simulations and demonstrate sample-based bayesian inference in a mixed graphical model . this establishes a rigorous link between deterministic neuron models and functional stochastic dynamics on the network level .", "topics": ["graphical model", "simulation"]}
{"title": "fast and accurate time series classification with weasel", "abstract": "time series ( ts ) occur in many scientific and commercial applications , ranging from earth surveillance to industry automation to the smart grids . an important type of ts analysis is classification , which can , for instance , improve energy load forecasting in smart grids by detecting the types of electronic devices based on their energy consumption profiles recorded by automatic sensors . such sensor-driven applications are very often characterized by ( a ) very long ts and ( b ) very large ts datasets needing classification . however , current methods to time series classification ( tsc ) can not cope with such data volumes at acceptable accuracy ; they are either scalable but offer only inferior classification quality , or they achieve state-of-the-art classification quality but can not scale to large data volumes . in this paper , we present weasel ( word extraction for time series classification ) , a novel tsc method which is both scalable and accurate . like other state-of-the-art tsc methods , weasel transforms time series into feature vectors , using a sliding-window approach , which are then analyzed through a machine learning classifier . the novelty of weasel lies in its specific method for deriving features , resulting in a much smaller yet much more discriminative feature set . on the popular ucr benchmark of 85 ts datasets , weasel is more accurate than the best current non-ensemble algorithms at orders-of-magnitude lower classification and training times , and it is almost as accurate as ensemble classifiers , whose computational complexity makes them inapplicable even for mid-size datasets . the outstanding robustness of weasel is also confirmed by experiments on two real smart grid datasets , where it out-of-the-box achieves almost the same accuracy as highly tuned , domain-specific methods .", "topics": ["time series", "computational complexity theory"]}
{"title": "review and evaluation of feature selection algorithms in synthetic problems", "abstract": "the main purpose of feature subset selection is to find a reduced subset of attributes from a data set described by a feature set . the task of a feature selection algorithm ( fsa ) is to provide with a computational solution motivated by a certain definition of relevance or by a reliable evaluation measure . in this paper several fundamental algorithms are studied to assess their performance in a controlled experimental scenario . a measure to evaluate fsas is devised that computes the degree of matching between the output given by a fsa and the known optimal solutions . an extensive experimental study on synthetic problems is carried out to assess the behaviour of the algorithms in terms of solution accuracy and size as a function of the relevance , irrelevance , redundancy and size of the data samples . the controlled experimental conditions facilitate the derivation of better-supported and meaningful conclusions .", "topics": ["synthetic data", "relevance"]}
{"title": "label-dependencies aware recurrent neural networks", "abstract": "in the last few years , recurrent neural networks ( rnns ) have proved effective on several nlp tasks . despite such great success , their ability to model \\emph { sequence labeling } is still limited . this lead research toward solutions where rnns are combined with models which already proved effective in this domain , such as crfs . in this work we propose a solution far simpler but very effective : an evolution of the simple jordan rnn , where labels are re-injected as input into the network , and converted into embeddings , in the same way as words . we compare this rnn variant to all the other rnn models , elman and jordan rnn , lstm and gru , on two well-known tasks of spoken language understanding ( slu ) . thanks to label embeddings and their combination at the hidden layer , the proposed variant , which uses more parameters than elman and jordan rnns , but far fewer than lstm and gru , is more effective than other rnns , but also outperforms sophisticated crf models .", "topics": ["recurrent neural network", "natural language processing"]}
{"title": "a study of sindhi related and arabic script adapted languages recognition", "abstract": "a large number of publications are available for the optical character recognition ( ocr ) . significant researches , as well as articles are present for the latin , chinese and japanese scripts . arabic script is also one of mature script from ocr perspective . the adaptive languages which share arabic script or its extended characters ; still lacking the ocrs for their language . in this paper we present the efforts of researchers on arabic and its related and adapted languages . this survey is organized in different sections , in which introduction is followed by properties of sindhi language . ocr process techniques and methods used by various researchers are presented . the last section is dedicated for future work and conclusion is also discussed .", "topics": ["image processing", "statistical classification"]}
{"title": "annealing gaussian into relu : a new sampling strategy for leaky-relu rbm", "abstract": "restricted boltzmann machine ( rbm ) is a bipartite graphical model that is used as the building block in energy-based deep generative models . due to numerical stability and quantifiability of the likelihood , rbm is commonly used with bernoulli units . here , we consider an alternative member of exponential family rbm with leaky rectified linear units -- called leaky rbm . we first study the joint and marginal distributions of leaky rbm under different leakiness , which provides us important insights by connecting the leaky rbm model and truncated gaussian distributions . the connection leads us to a simple yet efficient method for sampling from this model , where the basic idea is to anneal the leakiness rather than the energy ; -- i.e . , start from a fully gaussian/linear unit and gradually decrease the leakiness over iterations . this serves as an alternative to the annealing of the temperature parameter and enables numerical estimation of the likelihood that are more efficient and more accurate than the commonly used annealed importance sampling ( ais ) . we further demonstrate that the proposed sampling algorithm enjoys faster mixing property than contrastive divergence algorithm , which benefits the training without any additional computational cost .", "topics": ["sampling ( signal processing )", "graphical model"]}
{"title": "hilbert space embeddings of predictive state representations", "abstract": "predictive state representations ( psrs ) are an expressive class of models for controlled stochastic processes . psrs represent state as a set of predictions of future observable events . because psrs are defined entirely in terms of observable data , statistically consistent estimates of psr parameters can be learned efficiently by manipulating moments of observed training data . most learning algorithms for psrs have assumed that actions and observations are finite with low cardinality . in this paper , we generalize psrs to infinite sets of observations and actions , using the recent concept of hilbert space embeddings of distributions . the essence is to represent the state as a nonparametric conditional embedding operator in a reproducing kernel hilbert space ( rkhs ) and leverage recent work in kernel methods to estimate , predict , and update the representation . we show that these hilbert space embeddings of psrs are able to gracefully handle continuous actions and observations , and that our learned models outperform competing system identification algorithms on several prediction benchmarks .", "topics": ["test set"]}
{"title": "segmentation of bleeding regions in wireless capsule endoscopy images an approach for inside capsule video summarization", "abstract": "wireless capsule endoscopy ( wce ) is an effective means of diagnosis of gastrointestinal disorders . detection of informative scenes by wce could reduce the length of transmitted videos and can help with the diagnosis . in this paper we propose a simple and efficient method for segmentation of the bleeding regions in wce captured images . suitable color channels are selected and classified by a multi-layer perceptron ( mlp ) structure . the mlp structure is quantized such that the implementation does not require multiplications . the proposed method is tested by simulation on wce bleeding image dataset . the proposed structure is designed considering hardware resource constrains that exist in wce systems .", "topics": ["simulation"]}
{"title": "using artificial intelligence to identify state secrets", "abstract": "whether officials can be trusted to protect national security information has become a matter of great public controversy , reigniting a long-standing debate about the scope and nature of official secrecy . the declassification of millions of electronic records has made it possible to analyze these issues with greater rigor and precision . using machine-learning methods , we examined nearly a million state department cables from the 1970s to identify features of records that are more likely to be classified , such as international negotiations , military operations , and high-level communications . even with incomplete data , algorithms can use such features to identify 90 % of classified cables with < 11 % false positives . but our results also show that there are longstanding problems in the identification of sensitive information . error analysis reveals many examples of both overclassification and underclassification . this indicates both the need for research on inter-coder reliability among officials as to what constitutes classified material and the opportunity to develop recommender systems to better manage both classification and declassification .", "topics": ["high- and low-level", "artificial intelligence"]}
{"title": "efficient human computation", "abstract": "collecting large labeled data sets is a laborious and expensive task , whose scaling up requires division of the labeling workload between many teachers . when the number of classes is large , miscorrespondences between the labels given by the different teachers are likely to occur , which , in the extreme case , may reach total inconsistency . in this paper we describe how globally consistent labels can be obtained , despite the absence of teacher coordination , and discuss the possible efficiency of this process in terms of human labor . we define a notion of label efficiency , measuring the ratio between the number of globally consistent labels obtained and the number of labels provided by distributed teachers . we show that the efficiency depends critically on the ratio alpha between the number of data instances seen by a single teacher , and the number of classes . we suggest several algorithms for the distributed labeling problem , and analyze their efficiency as a function of alpha . in addition , we provide an upper bound on label efficiency for the case of completely uncoordinated teachers , and show that efficiency approaches 0 as the ratio between the number of labels each teacher provides and the number of classes drops ( i.e . alpha goes to 0 ) .", "topics": ["computation"]}
{"title": "unsupervised deep domain adaptation for pedestrian detection", "abstract": "this paper addresses the problem of unsupervised domain adaptation on the task of pedestrian detection in crowded scenes . first , we utilize an iterative algorithm to iteratively select and auto-annotate positive pedestrian samples with high confidence as the training samples for the target domain . meanwhile , we also reuse negative samples from the source domain to compensate for the imbalance between the amount of positive samples and negative samples . second , based on the deep network we also design an unsupervised regularizer to mitigate influence from data noise . more specifically , we transform the last fully connected layer into two sub-layers - an element-wise multiply layer and a sum layer , and add the unsupervised regularizer to further improve the domain adaptation accuracy . in experiments for pedestrian detection , the proposed method boosts the recall value by nearly 30 % while the precision stays almost the same . furthermore , we perform our method on standard domain adaptation benchmarks on both supervised and unsupervised settings and also achieve state-of-the-art results .", "topics": ["unsupervised learning"]}
{"title": "heterogeneous knowledge transfer in video emotion recognition , attribution and summarization", "abstract": "emotion is a key element in user-generated videos . however , it is difficult to understand emotions conveyed in such videos due to the complex and unstructured nature of user-generated content and the sparsity of video frames expressing emotion . in this paper , for the first time , we study the problem of transferring knowledge from heterogeneous external sources , including image and textual data , to facilitate three related tasks in understanding video emotion : emotion recognition , emotion attribution and emotion-oriented summarization . specifically , our framework ( 1 ) learns a video encoding from an auxiliary emotional image dataset in order to improve supervised video emotion recognition , and ( 2 ) transfers knowledge from an auxiliary textual corpora for zero-shot recognition of emotion classes unseen during training . the proposed technique for knowledge transfer facilitates novel applications of emotion attribution and emotion-oriented summarization . a comprehensive set of experiments on multiple datasets demonstrate the effectiveness of our framework .", "topics": ["text corpus", "sparse matrix"]}
{"title": "convolutional oriented boundaries : from image segmentation to high-level tasks", "abstract": "we present convolutional oriented boundaries ( cob ) , which produces multiscale oriented contours and region hierarchies starting from generic image classification convolutional neural networks ( cnns ) . cob is computationally efficient , because it requires a single cnn forward pass for multi-scale contour detection and it uses a novel sparse boundary representation for hierarchical segmentation ; it gives a significant leap in performance over the state-of-the-art , and it generalizes very well to unseen categories and datasets . particularly , we show that learning to estimate not only contour strength but also orientation provides more accurate results . we perform extensive experiments for low-level applications on bsds , pascal context , pascal segmentation , and nyud to evaluate boundary detection performance , showing that cob provides state-of-the-art contours and region hierarchies in all datasets . we also evaluate cob on high-level tasks when coupled with multiple pipelines for object proposals , semantic contours , semantic segmentation , and object detection on ms-coco , sbd , and pascal ; showing that cob also improves the results for all tasks .", "topics": ["image segmentation", "object detection"]}
{"title": "perception , attention , and resources : a decision-theoretic approach to graphics rendering", "abstract": "we describe work to control graphics rendering under limited computational resources by taking a decision-theoretic perspective on perceptual costs and computational savings of approximations . the work extends earlier work on the control of rendering by introducing methods and models for computing the expected cost associated with degradations of scene components . the expected cost is computed by considering the perceptual cost of degradations and a probability distribution over the attentional focus of viewers . we review the critical literature describing findings on visual search and attention , discuss the implications of the findings , and introduce models of expected perceptual cost . finally , we discuss policies that harness information about the expected cost of scene components .", "topics": ["approximation"]}
{"title": "robust multilingual named entity recognition with shallow semi-supervised features", "abstract": "we present a multilingual named entity recognition approach based on a robust and general set of features across languages and datasets . our system combines shallow local information with clustering semi-supervised features induced on large amounts of unlabeled text . understanding via empirical experimentation how to effectively combine various types of clustering features allows us to seamlessly export our system to other datasets and languages . the result is a simple but highly competitive system which obtains state of the art results across five languages and twelve datasets . the results are reported on standard shared task evaluation data such as conll for english , spanish and dutch . furthermore , and despite the lack of linguistically motivated features , we also report best results for languages such as basque and german . in addition , we demonstrate that our method also obtains very competitive results even when the amount of supervised data is cut by half , alleviating the dependency on manually annotated data . finally , the results show that our emphasis on clustering features is crucial to develop robust out-of-domain models . the system and models are freely available to facilitate its use and guarantee the reproducibility of results .", "topics": ["cluster analysis", "supervised learning"]}
{"title": "a hierarchical dirichlet process model with multiple levels of clustering for human eeg seizure modeling", "abstract": "driven by the multi-level structure of human intracranial electroencephalogram ( ieeg ) recordings of epileptic seizures , we introduce a new variant of a hierarchical dirichlet process -- -the multi-level clustering hierarchical dirichlet process ( mlc-hdp ) -- -that simultaneously clusters datasets on multiple levels . our seizure dataset contains brain activity recorded in typically more than a hundred individual channels for each seizure of each patient . the mlc-hdp model clusters over channels-types , seizure-types , and patient-types simultaneously . we describe this model and its implementation in detail . we also present the results of a simulation study comparing the mlc-hdp to a similar model , the nested dirichlet process and finally demonstrate the mlc-hdp 's use in modeling seizures across multiple patients . we find the mlc-hdp 's clustering to be comparable to independent human physician clusterings . to our knowledge , the mlc-hdp model is the first in the epilepsy literature capable of clustering seizures within and between patients .", "topics": ["cluster analysis", "simulation"]}
{"title": "hash2vec , feature hashing for word embeddings", "abstract": "in this paper we propose the application of feature hashing to create word embeddings for natural language processing . feature hashing has been used successfully to create document vectors in related tasks like document classification . in this work we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data . the results show that this algorithm , that does not need training , is able to capture the semantic meaning of words . we compare the results against glove showing that they are similar . as far as we know this is the first application of feature hashing to the word embeddings problem and the results indicate this is a scalable technique with practical results for nlp applications .", "topics": ["natural language processing", "time complexity"]}
{"title": "towards cognitive-and-immersive systems : experiments in a shared ( or common ) blockworld framework", "abstract": "as computational power has continued to increase , and sensors have become more accurate , the corresponding advent of systems that are cognitive-and-immersive ( cai ) has come to pass . cai systems fall squarely into the intersection of ai with hci/hri : such systems interact with and assist the human agents that enter them , in no small part because such systems are infused with ai able to understand and reason about these humans and their beliefs , goals , and plans . we herein explain our approach to engineering cai systems . we emphasize the capacity of a cai system to develop and reason over a `` theory of the mind '' of its humans partners . this capacity means that the ai in question has a sophisticated model of the beliefs , knowledge , goals , desires , emotions , etc . of these humans . to accomplish this engineering , a formal framework of very high expressivity is needed . in our case , this framework is a \\textit { cognitive event calculus } , a partciular kind of quantified multi-modal logic , and a matching high-expressivity planner . to explain , advance , and to a degree validate our approach , we show that a calculus of this type can enable a cai system to understand a psychologically tricky scenario couched in what we call the \\textit { cognitive blockworld framework } ( cbf ) . cbf includes machinery able to represent and plan over not merely blocks and actions , but also agents and their mental attitudes about other agents .", "topics": ["sensor"]}
{"title": "geometric insights into support vector machine behavior using the kkt conditions", "abstract": "the support vector machine ( svm ) is a powerful and widely used classification algorithm . its performance is well known to be impacted by a tuning parameter which is frequently selected by cross-validation . this paper uses the karush-kuhn-tucker conditions to provide rigorous mathematical proof for new insights into the behavior of svm in the large and small tuning parameter regimes . these insights provide perhaps unexpected relationships between svm and naive bayes and maximal data piling directions . we explore how characteristics of the training data affect the behavior of svm in many cases including : balanced vs. unbalanced classes , low vs. high dimension , separable vs. non-separable data . these results present a simple explanation of svm 's behavior as a function of the tuning parameter . we also elaborate on the geometry of complete data piling directions in high dimensional space . the results proved in this paper suggest important implications for tuning svm with cross-validation .", "topics": ["test set", "support vector machine"]}
{"title": "curriculum learning of visual attribute clusters for multi-task classification", "abstract": "visual attributes , from simple objects ( e.g . , backpacks , hats ) to soft-biometrics ( e.g . , gender , height , clothing ) have proven to be a powerful representational approach for many applications such as image description and human identification . in this paper , we introduce a novel method to combine the advantages of both multi-task and curriculum learning in a visual attribute classification framework . individual tasks are grouped after performing hierarchical clustering based on their correlation . the clusters of tasks are learned in a curriculum learning setup by transferring knowledge between clusters . the learning process within each cluster is performed in a multi-task classification setup . by leveraging the acquired knowledge , we speed-up the process and improve performance . we demonstrate the effectiveness of our method via ablation studies and a detailed analysis of the covariates , on a variety of publicly available datasets of humans standing with their full-body visible . extensive experimentation has proven that the proposed approach boosts the performance by 4 % to 10 % .", "topics": ["cluster analysis"]}
{"title": "efficient online inference for infinite evolutionary cluster models with applications to latent social event discovery", "abstract": "the recurrent chinese restaurant process ( rcrp ) is a powerful statistical method for modeling evolving clusters in large scale social media data . with the rcrp , one can allow both the number of clusters and the cluster parameters in a model to change over time . however , application of the rcrp has largely been limited due to the non-conjugacy between the cluster evolutionary priors and the multinomial likelihood . this non-conjugacy makes inference di cult and restricts the scalability of models which use the rcrp , leading to the rcrp being applied only in simple problems , such as those that can be approximated by a single gaussian emission . in this paper , we provide a novel solution for the non-conjugacy issues for the rcrp and an example of how to leverage our solution for one speci c problem - the social event discovery problem . by utilizing sequential monte carlo methods in inference , our approach can be massively paralleled and is highly scalable , to the extent it can work on tens of millions of documents . we are able to generate high quality topical and location distributions of the clusters that can be directly interpreted as real social events , and our experimental results suggest that the approaches proposed achieve much better predictive performance than techniques reported in prior work . we also demonstrate how the techniques we develop can be used in a much more general ways toward similar problems .", "topics": ["scalability"]}
{"title": "computing posterior probabilities of structural features in bayesian networks", "abstract": "we study the problem of learning bayesian network structures from data . koivisto and sood ( 2004 ) and koivisto ( 2006 ) presented algorithms that can compute the exact marginal posterior probability of a subnetwork , e.g . , a single edge , in o ( n2n ) time and the posterior probabilities for all n ( n-1 ) potential edges in o ( n2n ) total time , assuming that the number of parents per node or the indegree is bounded by a constant . one main drawback of their algorithms is the requirement of a special structure prior that is non uniform and does not respect markov equivalence . in this paper , we develop an algorithm that can compute the exact posterior probability of a subnetwork in o ( 3n ) time and the posterior probabilities for all n ( n-1 ) potential edges in o ( n3n ) total time . our algorithm also assumes a bounded indegree but allows general structure priors . we demonstrate the applicability of the algorithm on several data sets with up to 20 variables .", "topics": ["bayesian network", "markov chain"]}
{"title": "a frequency-domain encoding for neuroevolution", "abstract": "neuroevolution has yet to scale up to complex reinforcement learning tasks that require large networks . networks with many inputs ( e.g . raw video ) imply a very high dimensional search space if encoded directly . indirect methods use a more compact genotype representation that is transformed into networks of potentially arbitrary size . in this paper , we present an indirect method where networks are encoded by a set of fourier coefficients which are transformed into network weight matrices via an inverse fourier-type transform . because there often exist network solutions whose weight matrices contain regularity ( i.e . adjacent weights are correlated ) , the number of coefficients required to represent these networks in the frequency domain is much smaller than the number of weights ( in the same way that natural images can be compressed by ignore high-frequency components ) . this `` compressed '' encoding is compared to the direct approach where search is conducted in the weight space on the high-dimensional octopus arm task . the results show that representing networks in the frequency domain can reduce the search-space dimensionality by as much as two orders of magnitude , both accelerating convergence and yielding more general solutions .", "topics": ["reinforcement learning", "coefficient"]}
{"title": "optimizing interactive systems with data-driven objectives", "abstract": "effective optimization is essential for interactive systems to provide a satisfactory user experience . however , it is often challenging to find an objective to optimize for . generally , such objectives are manually crafted and rarely capture complex user needs accurately . conversely , we propose an approach that infers the objective directly from observed user interactions . these inferences can be made regardless of prior knowledge and across different types of user behavior . then we introduce : interactive system optimizer ( iso ) , a novel algorithm that uses these inferred objectives for optimization . our main contribution is a new general principled approach to optimizing interactive systems using data-driven objectives . we demonstrate the high effectiveness of iso over several gridworld simulations .", "topics": ["interaction", "simulation"]}
{"title": "variational neural machine translation", "abstract": "models of neural machine translation are often from a discriminative family of encoderdecoders that learn a conditional distribution of a target sentence given a source sentence . in this paper , we propose a variational model to learn this conditional distribution for neural machine translation : a variational encoderdecoder model that can be trained end-to-end . different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone , the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations . in order to perform efficient posterior inference and large-scale training , we build a neural posterior approximator conditioned on both the source and the target sides , and equip it with a reparameterization technique to estimate the variational lower bound . experiments on both chinese-english and english- german translation tasks show that the proposed variational neural machine translation achieves significant improvements over the vanilla neural machine translation baselines .", "topics": ["baseline ( configuration management )", "calculus of variations"]}
{"title": "exploiting deep features for remote sensing image retrieval : a systematic investigation", "abstract": "remote sensing ( rs ) image retrieval based on visual content is of great significance for geological information mining . over the past two decades , a large amount of research on this task has been carried out , which mainly focuses on the following three core issues of image retrieval : visual feature , similarity metric and relevance feedback . along with the advance of these issues , the technology of rs image retrieval has been developed comparatively mature . however , due to the complexity and multiformity of high-resolution remote sensing ( hrrs ) images , there is still room for improvement in the current methods on hrrs data retrieval . in this paper , we analyze the three key aspects of retrieval and provide a comprehensive review on content-based rs image retrieval methods . furthermore , for the goal to advance the state-of-the-art in hrrs image retrieval , we focus on the visual feature aspect and delve how to use powerful deep representations in this task . we conduct systematic investigation on evaluating factors that may affect the performance of deep features . by optimizing each factor , we acquire remarkable retrieval results on publicly available hrrs datasets . finally , we explain the experimental phenomenon in detail and draw instructive conclusions according to our analysis . our work can serve as a guiding role for the research of content-based rs image retrieval .", "topics": ["data mining"]}
{"title": "sar image despeckling based on nonlocal similarity sparse decomposition", "abstract": "this letter presents a method of synthetic aperture radar ( sar ) image despeckling aimed to preserve the detail information while suppressing speckle noise . this method combines the nonlocal self-similarity partition and a proposed modified sparse decomposition . the nonlocal partition method groups a series of structure-similarity data sets . each data set has a good sparsity for learning an over-complete dictionary in sparse representation . in the sparse decomposition , we propose a novel method to identify principal atoms from over-complete dictionary to form a principal dictionary . despeckling is performed on each data set over the principal dictionary with principal atoms . experimental results demonstrate that the proposed method can achieve high performances in terms of both speckle noise reduction and structure details preservation .", "topics": ["noise reduction", "synthetic data"]}
{"title": "sparse partial least squares for on-line variable selection in multivariate data streams", "abstract": "in this paper we propose a computationally efficient algorithm for on-line variable selection in multivariate regression problems involving high dimensional data streams . the algorithm recursively extracts all the latent factors of a partial least squares solution and selects the most important variables for each factor . this is achieved by means of only one sparse singular value decomposition which can be efficiently updated on-line and in an adaptive fashion . simulation results based on artificial data streams demonstrate that the algorithm is able to select important variables in dynamic settings where the correlation structure among the observed streams is governed by a few hidden components and the importance of each variable changes over time . we also report on an application of our algorithm to a multivariate version of the `` enhanced index tracking '' problem using financial data streams . the application consists of performing on-line asset allocation with the objective of overperforming two benchmark indices simultaneously .", "topics": ["computational complexity theory", "simulation"]}
{"title": "improvement of automatic hemorrhages detection methods using shapes recognition", "abstract": "diabetic retinopathy is a medical condition where the retina is damaged because fluid leaks from blood vessels into the retina . the presence of hemorrhages in the retina is the earliest symptom of diabetic retinopathy . the number and shape of hemorrhages is used to indicate the severity of the disease . early automated hemorrhage detection can help reduce the incidence of blindness . this paper introduced new method depending on the hemorrhage shape to detect the dot hemorrhage ( dh ) , its number , and size at early stage , this can be achieved by reducing the retinal image details . detection and recognize the dh by following three sequential steps , removing the fovea , removing the vasculature and recognize dh by determining the circularity for all the objects in the image , finally determine the shape factor which is related to dh recognition , this stage strengthens the recognition process . the proposed method recognizes and separates all the dh .", "topics": ["speech recognition"]}
{"title": "english conversational telephone speech recognition by humans and machines", "abstract": "one of the most difficult speech recognition tasks is accurate recognition of human to human communication . advances in deep learning over the last few years have produced major speech recognition improvements on the representative switchboard conversational corpus . word error rates that just a few years ago were 14 % have dropped to 8.0 % , then 6.6 % and most recently 5.8 % , and are now believed to be within striking range of human performance . this then raises two issues - what is human performance , and how far down can we still drive speech recognition error rates ? a recent paper by microsoft suggests that we have already achieved human performance . in trying to verify this statement , we performed an independent set of human performance measurements on two conversational tasks and found that human performance may be considerably better than what was earlier reported , giving the community a significantly harder goal to achieve . we also report on our own efforts in this area , presenting a set of acoustic and language modeling techniques that lowered the word error rate of our own english conversational telephone lvcsr system to the level of 5.5 % /10.3 % on the switchboard/callhome subsets of the hub5 2000 evaluation , which - at least at the writing of this paper - is a new performance milestone ( albeit not at what we measure to be human performance ! ) . on the acoustic side , we use a score fusion of three models : one lstm with multiple feature inputs , a second lstm trained with speaker-adversarial multi-task learning and a third residual net ( resnet ) with 25 convolutional layers and time-dilated convolutions . on the language modeling side , we use word and character lstms and convolutional wavenet-style language models .", "topics": ["speech recognition"]}
{"title": "control of the correlation of spontaneous neuron activity in biological and noise-activated cmos artificial neural microcircuits", "abstract": "there are several indications that brain is organized not on a basis of individual unreliable neurons , but on a micro-circuital scale providing lego blocks employed to create complex architectures . at such an intermediate scale , the firing activity in the microcircuits is governed by collective effects emerging by the background noise soliciting spontaneous firing , the degree of mutual connections between the neurons , and the topology of the connections . we compare spontaneous firing activity of small populations of neurons adhering to an engineered scaffold with simulations of biologically plausible cmos artificial neuron populations whose spontaneous activity is ignited by tailored background noise . we provide a full set of flexible and low-power consuming silicon blocks including neurons , excitatory and inhibitory synapses , and both white and pink noise generators for spontaneous firing activation . we achieve a comparable degree of correlation of the firing activity of the biological neurons by controlling the kind and the number of connection among the silicon neurons . the correlation between groups of neurons , organized as a ring of four distinct populations connected by the equivalent of interneurons , is triggered more effectively by adding multiple synapses to the connections than increasing the number of independent point-to-point connections . the comparison between the biological and the artificial systems suggests that a considerable number of synapses is active also in biological populations adhering to engineered scaffolds .", "topics": ["simulation"]}
{"title": "causality-aided falsification", "abstract": "falsification is drawing attention in quality assurance of heterogeneous systems whose complexities are beyond most verification techniques ' scalability . in this paper we introduce the idea of causality aid in falsification : by providing a falsification solver -- that relies on stochastic optimization of a certain cost function -- with suitable causal information expressed by a bayesian network , search for a falsifying input value can be efficient . our experiment results show the idea 's viability .", "topics": ["bayesian network", "scalability"]}
{"title": "analyzing business process anomalies using autoencoders", "abstract": "businesses are naturally interested in detecting anomalies in their internal processes , because these can be indicators for fraud and inefficiencies . within the domain of business intelligence , classic anomaly detection is not very frequently researched . in this paper , we propose a method , using autoencoders , for detecting and analyzing anomalies occurring in the execution of a business process . our method does not rely on any prior knowledge about the process and can be trained on a noisy dataset already containing the anomalies . we demonstrate its effectiveness by evaluating it on 700 different datasets and testing its performance against three state-of-the-art anomaly detection methods . this paper is an extension of our previous work from 2016 [ 30 ] . compared to the original publication we have further refined the approach in terms of performance and conducted an elaborate evaluation on more sophisticated datasets including real-life event logs from the business process intelligence challenges of 2012 and 2017 . in our experiments our approach reached an f1 score of 0.87 , whereas the best unaltered state-of-the-art approach reached an f1 score of 0.72 . furthermore , our approach can be used to analyze the detected anomalies in terms of which event within one execution of the process causes the anomaly .", "topics": ["autoencoder"]}
{"title": "fortia-fbk at semeval-2017 task 5 : bullish or bearish ? inferring sentiment towards brands from financial news headlines", "abstract": "in this paper , we describe a methodology to infer bullish or bearish sentiment towards companies/brands . more specifically , our approach leverages affective lexica and word embeddings in combination with convolutional neural networks to infer the sentiment of financial news headlines towards a target company . such architecture was used and evaluated in the context of the semeval 2017 challenge ( task 5 , subtask 2 ) , in which it obtained the best performance .", "topics": ["loss function", "matrix regularization"]}
{"title": "dynamic knowledge capitalization through annotation among economic intelligence actors in a collaborative environment", "abstract": "the shift from industrial economy to knowledge economy in today 's world has revolutionalized strategic planning in organizations as well as their problem solving approaches . the point of focus today is knowledge and service production with more emphasis been laid on knowledge capital . many organizations are investing on tools that facilitate knowledge sharing among their employees and they are as well promoting and encouraging collaboration among their staff in order to build the organization 's knowledge capital with the ultimate goal of creating a lasting competitive advantage for their organizations . one of the current leading approaches used for solving organization 's decision problem is the economic intelligence ( ei ) approach which involves interactions among various actors called ei actors . these actors collaborate to ensure the overall success of the decision problem solving process . in the course of the collaboration , the actors express knowledge which could be capitalized for future reuse . in this paper , we propose in the first place , an annotation model for knowledge elicitation among ei actors . because of the need to build a knowledge capital , we also propose a dynamic knowledge capitalisation approach for managing knowledge produced by the actors . finally , the need to manage the interactions and the interdependencies among collaborating ei actors , led to our third proposition which constitute an awareness mechanism for group work management .", "topics": ["interaction", "artificial intelligence"]}
{"title": "empirical evaluation of gated recurrent neural networks on sequence modeling", "abstract": "in this paper we compare different types of recurrent units in recurrent neural networks ( rnns ) . especially , we focus on more sophisticated units that implement a gating mechanism , such as a long short-term memory ( lstm ) unit and a recently proposed gated recurrent unit ( gru ) . we evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling . our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units . also , we found gru to be comparable to lstm .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "reluplex : an efficient smt solver for verifying deep neural networks", "abstract": "deep neural networks have emerged as a widely used and effective means for tackling complex , real-world problems . however , a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior . we present a novel , scalable , and efficient technique for verifying properties of deep neural networks ( or providing counter-examples ) . the technique is based on the simplex method , extended to handle the non-convex rectified linear unit ( relu ) activation function , which is a crucial ingredient in many modern neural networks . the verification procedure tackles neural networks as a whole , without making any simplifying assumptions . we evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft ( acas xu ) . results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods .", "topics": ["neural networks", "scalability"]}
{"title": "exploratory gradient boosting for reinforcement learning in complex domains", "abstract": "high-dimensional observations and complex real-world dynamics present major challenges in reinforcement learning for both function approximation and exploration . we address both of these challenges with two complementary techniques : first , we develop a gradient-boosting style , non-parametric function approximator for learning on $ q $ -function residuals . and second , we propose an exploration strategy inspired by the principles of state abstraction and information acquisition under uncertainty . we demonstrate the empirical effectiveness of these techniques , first , as a preliminary check , on two standard tasks ( blackjack and $ n $ -chain ) , and then on two much larger and more realistic tasks with high-dimensional observation spaces . specifically , we introduce two benchmarks built within the game minecraft where the observations are pixel arrays of the agent 's visual field . a combination of our two algorithmic techniques performs competitively on the standard reinforcement-learning tasks while consistently and substantially outperforming baselines on the two tasks with high-dimensional observation spaces . the new function approximator , exploration strategy , and evaluation benchmarks are each of independent interest in the pursuit of reinforcement-learning methods that scale to real-world domains .", "topics": ["reinforcement learning", "gradient descent"]}
{"title": "a novel framework to expedite systematic reviews by automatically building information extraction training corpora", "abstract": "a systematic review identifies and collates various clinical studies and compares data elements and results in order to provide an evidence based answer for a particular clinical question . the process is manual and involves lot of time . a tool to automate this process is lacking . the aim of this work is to develop a framework using natural language processing and machine learning to build information extraction algorithms to identify data elements in a new primary publication , without having to go through the expensive task of manual annotation to build gold standards for each data element type . the system is developed in two stages . initially , it uses information contained in existing systematic reviews to identify the sentences from the pdf files of the included references that contain specific data elements of interest using a modified jaccard similarity measure . these sentences have been treated as labeled data.a support vector machine ( svm ) classifier is trained on this labeled data to extract data elements of interests from a new article . we conducted experiments on cochrane database systematic reviews related to congestive heart failure using inclusion criteria as an example data element . the empirical results show that the proposed system automatically identifies sentences containing the data element of interest with a high recall ( 93.75 % ) and reasonable precision ( 27.05 % - which means the reviewers have to read only 3.7 sentences on average ) . the empirical results suggest that the tool is retrieving valuable information from the reference articles , even when it is time-consuming to identify them manually . thus we hope that the tool will be useful for automatic data extraction from biomedical research publications . the future scope of this work is to generalize this information framework for all types of systematic reviews .", "topics": ["natural language processing", "support vector machine"]}
{"title": "social and business intelligence analysis using pso", "abstract": "the goal of this paper is to elaborate swarm intelligence for business intelligence decision making and the business rules management improvement . .the swarm optimization , which is highly influenced by the behavior of creature , performs in group . the spatial data is defined as data that is represented by 2d or 3d images . sql server supports only 2d images till now . as we know that location is an essential part of any organizational data as well as business data enterprises maintain customer address lists , own property , ship goods from and to warehouses , manage transport flows among their workforce , and perform many other activities . by means to say a lot of spatial data is used and processed by enterprises , organizations and other bodies in order to make the things more visible and self descriptive . from the experiments , we found that pso is can facilitate the intelligence in social and business behavior .", "topics": ["neural networks"]}
{"title": "entropy and belief networks", "abstract": "the product expansion of conditional probabilities for belief nets is not maximum entropy . this appears to deny a desirable kind of assurance for the model . however , a kind of guarantee that is almost as strong as maximum entropy can be derived . surprisingly , a variant model also exhibits the guarantee , and for many cases obtains a higher performance score than the product expansion .", "topics": ["bayesian network"]}
{"title": "kernelized locality-sensitive hashing for semi-supervised agglomerative clustering", "abstract": "large scale agglomerative clustering is hindered by computational burdens . we propose a novel scheme where exact inter-instance distance calculation is replaced by the hamming distance between kernelized locality-sensitive hashing ( klsh ) hashed values . this results in a method that drastically decreases computation time . additionally , we take advantage of certain labeled data points via distance metric learning to achieve a competitive precision and recall comparing to k-means but in much less computation time .", "topics": ["cluster analysis", "time complexity"]}
{"title": "the asymptotic performance of linear echo state neural networks", "abstract": "in this article , a study of the mean-square error ( mse ) performance of linear echo-state neural networks is performed , both for training and testing tasks . considering the realistic setting of noise present at the network nodes , we derive deterministic equivalents for the aforementioned mse in the limit where the number of input data $ t $ and network size $ n $ both grow large . specializing then the network connectivity matrix to specific random settings , we further obtain simple formulas that provide new insights on the performance of such networks .", "topics": ["neural networks"]}
{"title": "efficient transductive online learning via randomized rounding", "abstract": "most traditional online learning algorithms are based on variants of mirror descent or follow-the-leader . in this paper , we present an online algorithm based on a completely different approach , tailored for transductive settings , which combines `` random playout '' and randomized rounding of loss subgradients . as an application of our approach , we present the first computationally efficient online algorithm for collaborative filtering with trace-norm constrained matrices . as a second application , we solve an open question linking batch learning and transductive online learning", "topics": ["computational complexity theory"]}
{"title": "co-occurrence of the benford-like and zipf laws arising from the texts representing human and artificial languages", "abstract": "we demonstrate that large texts , representing human ( english , russian , ukrainian ) and artificial ( c++ , java ) languages , display quantitative patterns characterized by the benford-like and zipf laws . the frequency of a word following the zipf law is inversely proportional to its rank , whereas the total numbers of a certain word appearing in the text generate the uneven benford-like distribution of leading numbers . excluding the most popular words essentially improves the correlation of actual textual data with the zipfian distribution , whereas the benford distribution of leading numbers ( arising from the overall amount of a certain word ) is insensitive to the same elimination procedure . the calculated values of the moduli of slopes of double logarithmical plots for artificial languages ( c++ , java ) are markedly larger than those for human ones .", "topics": ["text corpus"]}
{"title": "learning word embeddings from speech", "abstract": "in this paper , we propose a novel deep neural network architecture , sequence-to-sequence audio2vec , for unsupervised learning of fixed-length vector representations of audio segments excised from a speech corpus , where the vectors contain semantic information pertaining to the segments , and are close to other vectors in the embedding space if their corresponding segments are semantically similar . the design of the proposed model is based on the rnn encoder-decoder framework , and borrows the methodology of continuous skip-grams for training . the learned vector representations are evaluated on 13 widely used word similarity benchmarks , and achieved competitive results to that of glove . the biggest advantage of the proposed model is its capability of extracting semantic information of audio segments taken directly from raw speech , without relying on any other modalities such as text or images , which are challenging and expensive to collect and annotate .", "topics": ["unsupervised learning", "encoder"]}
{"title": "scalable nonparametric bayesian inference on point processes with gaussian processes", "abstract": "in this paper we propose the first non-parametric bayesian model using gaussian processes to make inference on poisson point processes without resorting to gridding the domain or to introducing latent thinning points . unlike competing models that scale cubically and have a squared memory requirement in the number of data points , our model has a linear complexity and memory requirement . we propose an mcmc sampler and show that our model is faster , more accurate and generates less correlated samples than competing models on both synthetic and real-life data . finally , we show that our model easily handles data sizes not considered thus far by alternate approaches .", "topics": ["sampling ( signal processing )", "synthetic data"]}
{"title": "regret lower bounds and extended upper confidence bounds policies in stochastic multi-armed bandit problem", "abstract": "this paper is devoted to regret lower bounds in the classical model of stochastic multi-armed bandit . a well-known result of lai and robbins , which has then been extended by burnetas and katehakis , has established the presence of a logarithmic bound for all consistent policies . we relax the notion of consistence , and exhibit a generalisation of the logarithmic bound . we also show the non existence of logarithmic bound in the general case of hannan consistency . to get these results , we study variants of popular upper confidence bounds ( ucb ) policies . as a by-product , we prove that it is impossible to design an adaptive policy that would select the best of two algorithms by taking advantage of the properties of the environment .", "topics": ["regret ( decision theory )"]}
{"title": "pushing the limits of paraphrastic sentence embeddings with millions of machine translations", "abstract": "we extend the work of wieting et al . ( 2017 ) , back-translating a large parallel corpus to produce a dataset of more than 51 million english-english sentential paraphrase pairs in a dataset we call paranmt-50m . we find this corpus to be cover many domains and styles of text , in addition to being rich in paraphrases with different sentence structure , and we release it to the community . to show its utility , we use it to train paraphrastic sentence embeddings using only minor changes to the framework of wieting et al . ( 2016b ) . the resulting embeddings outperform all supervised systems on every semeval semantic textual similarity ( sts ) competition , and are a significant improvement in capturing paraphrastic similarity over all other sentence embeddings we also show that our embeddings perform competitively on general sentence embedding tasks . we release the corpus , pretrained sentence embeddings , and code to generate them . we believe the corpus can be a valuable resource for automatic paraphrase generation and can provide a rich source of semantic information to improve downstream natural language understanding tasks .", "topics": ["natural language", "text corpus"]}
{"title": "enabling sparse winograd convolution by native pruning", "abstract": "sparse methods and the use of winograd convolutions are two orthogonal approaches , each of which significantly accelerates convolution computations in modern cnns . sparse winograd merges these two and thus has the potential to offer a combined performance benefit . nevertheless , training convolution layers so that the resulting winograd kernels are sparse has not hitherto been very successful . by introducing a winograd layer in place of a standard convolution layer , we can learn and prune winograd coefficients `` natively '' and obtain sparsity level beyond 90 % with only 0.1 % accuracy loss with alexnet on imagenet dataset . furthermore , we present a sparse winograd convolution algorithm and implementation that exploits the sparsity , achieving up to 31.7 effective tflop/s in 32-bit precision on a latest intel xeon cpu , which corresponds to a 5.4x speedup over a state-of-the-art dense convolution implementation .", "topics": ["sparse matrix", "convolution"]}
{"title": "on clustering network-valued data", "abstract": "community detection , which focuses on clustering nodes or detecting communities in ( mostly ) a single network , is a problem of considerable practical interest and has received a great deal of attention in the research community . while being able to cluster within a network is important , there are emerging needs to be able to cluster multiple networks . this is largely motivated by the routine collection of network data that are generated from potentially different populations . these networks may or may not have node correspondence . when node correspondence is present , we cluster networks by summarizing a network by its graphon estimate , whereas when node correspondence is not present , we propose a novel solution for clustering such networks by associating a computationally feasible feature vector to each network based on trace of powers of the adjacency matrix . we illustrate our methods using both simulated and real data sets , and theoretical justifications are provided in terms of consistency .", "topics": ["cluster analysis"]}
{"title": "multimodal recurrent neural networks with information transfer layers for indoor scene labeling", "abstract": "this paper proposes a new method called multimodal rnns for rgb-d scene semantic segmentation . it is optimized to classify image pixels given two input sources : rgb color channels and depth maps . it simultaneously performs training of two recurrent neural networks ( rnns ) that are crossly connected through information transfer layers , which are learnt to adaptively extract relevant cross-modality features . each rnn model learns its representations from its own previous hidden states and transferred patterns from the other rnns previous hidden states ; thus , both model-specific and crossmodality features are retained . we exploit the structure of quad-directional 2d-rnns to model the short and long range contextual information in the 2d input image . we carefully designed various baselines to efficiently examine our proposed model structure . we test our multimodal rnns method on popular rgb-d benchmarks and show how it outperforms previous methods significantly and achieves competitive results with other state-of-the-art works .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "from word embeddings to item recommendation", "abstract": "social network platforms can use the data produced by their users to serve them better . one of the services these platforms provide is recommendation service . recommendation systems can predict the future preferences of users using their past preferences . in the recommendation systems literature there are various techniques , such as neighborhood based methods , machine-learning based methods and matrix-factorization based methods . in this work , a set of well known methods from natural language processing domain , namely word2vec , is applied to recommendation systems domain . unlike previous works that use word2vec for recommendation , this work uses non-textual features , the check-ins , and it recommends venues to visit/check-in to the target users . for the experiments , a foursquare check-in dataset is used . the results show that use of continuous vector space representations of items modeled by techniques of word2vec is promising for making recommendations .", "topics": ["natural language processing"]}
{"title": "conditional plausibility measures and bayesian networks", "abstract": "a general notion of algebraic conditional plausibility measures is defined . probability measures , ranking functions , possibility measures , and ( under the appropriate definitions ) sets of probability measures can all be viewed as defining algebraic conditional plausibility measures . it is shown that the technology of bayesian networks can be applied to algebraic conditional plausibility measures .", "topics": ["bayesian network"]}
{"title": "compact convolutional neural networks for classification of asynchronous steady-state visual evoked potentials", "abstract": "steady-state visual evoked potentials ( ssveps ) are neural oscillations from the parietal and occipital regions of the brain that are evoked from flickering visual stimuli . ssveps are robust signals measurable in the electroencephalogram ( eeg ) and are commonly used in brain-computer interfaces ( bcis ) . however , methods for high-accuracy decoding of ssveps usually require hand-crafted approaches that leverage domain-specific knowledge of the stimulus signals , such as specific temporal frequencies in the visual stimuli and their relative spatial arrangement . when this knowledge is unavailable , such as when ssvep signals are acquired asynchronously , such approaches tend to fail . in this paper , we show how a compact convolutional neural network ( compact-cnn ) , which only requires raw eeg signals for automatic feature extraction , can be used to decode signals from a 12-class ssvep dataset without the need for any domain-specific knowledge or calibration data . we report across subject mean accuracy of approximately 80 % ( chance being 8.3 % ) and show this is substantially better than current state-of-the-art hand-crafted approaches using canonical correlation analysis ( cca ) and combined-cca . furthermore , we analyze our compact-cnn to examine the underlying feature representation , discovering that the deep learner extracts additional phase and amplitude related features associated with the structure of the dataset . we discuss how our compact-cnn shows promise for bci applications that allow users to freely gaze/attend to any stimulus at any time ( e.g . , asynchronous bci ) as well as provides a method for analyzing ssvep signals in a way that might augment our understanding about the basic processing in the visual cortex .", "topics": ["feature extraction"]}
{"title": "lifting object detection datasets into 3d", "abstract": "while data has certainly taken the center stage in computer vision in recent years , it can still be difficult to obtain in certain scenarios . in particular , acquiring ground truth 3d shapes of objects pictured in 2d images remains a challenging feat and this has hampered progress in recognition-based object reconstruction from a single image . here we propose to bypass previous solutions such as 3d scanning or manual design , that scale poorly , and instead populate object category detection datasets semi-automatically with dense , per-object 3d reconstructions , bootstrapped from : ( i ) class labels , ( ii ) ground truth figure-ground segmentations and ( iii ) a small set of keypoint annotations . our proposed algorithm first estimates camera viewpoint using rigid structure-from-motion and then reconstructs object shapes by optimizing over visual hull proposals guided by loose within-class shape similarity assumptions . the visual hull sampling process attempts to intersect an object 's projection cone with the cones of minimal subsets of other similar objects among those pictured from certain vantage points . we show that our method is able to produce convincing per-object 3d reconstructions and to accurately estimate cameras viewpoints on one of the most challenging existing object-category detection datasets , pascal voc . we hope that our results will re-stimulate interest on joint object recognition and 3d reconstruction from a single image .", "topics": ["object detection", "computer vision"]}
{"title": "online classification with complex metrics", "abstract": "we present a framework and analysis of consistent binary classification for complex and non-decomposable performance metrics such as the f-measure and the jaccard measure . the proposed framework is general , as it applies to both batch and online learning , and to both linear and non-linear models . our work follows recent results showing that the bayes optimal classifier for many complex metrics is given by a thresholding of the conditional probability of the positive class . this manuscript extends this thresholding characterization -- showing that the utility is strictly locally quasi-concave with respect to the threshold for a wide range of models and performance metrics . this , in turn , motivates simple normalized gradient ascent updates for threshold estimation . we present a finite-sample regret analysis for the resulting procedure . in particular , the risk for the batch case converges to the bayes risk at the same rate as that of the underlying conditional probability estimation , and the risk of proposed online algorithm converges at a rate that depends on the conditional probability estimation risk . for instance , in the special case where the conditional probability model is logistic regression , our procedure achieves $ o ( \\frac { 1 } { \\sqrt { n } } ) $ sample complexity , both for batch and online training . empirical evaluation shows that the proposed algorithms out-perform alternatives in practice , with comparable or better prediction performance and reduced run time for various metrics and datasets .", "topics": ["regret ( decision theory )", "nonlinear system"]}
{"title": "from dependency to causality : a machine learning approach", "abstract": "the relationship between statistical dependency and causality lies at the heart of all statistical approaches to causal inference . recent results in the chalearn cause-effect pair challenge have shown that causal directionality can be inferred with good accuracy also in markov indistinguishable configurations thanks to data driven approaches . this paper proposes a supervised machine learning approach to infer the existence of a directed causal link between two variables in multivariate settings with $ n > 2 $ variables . the approach relies on the asymmetry of some conditional ( in ) dependence relations between the members of the markov blankets of two variables causally connected . our results show that supervised learning methods may be successfully used to extract causal information on the basis of asymmetric statistical descriptors also for $ n > 2 $ variate distributions .", "topics": ["supervised learning", "causality"]}
{"title": "calibration of an articulated camera system with scale factor estimation", "abstract": "multiple camera systems ( mcs ) have been widely used in many vision applications and attracted much attention recently . there are two principle types of mcs , one is the rigid multiple camera system ( rmcs ) ; the other is the articulated camera system ( acs ) . in a rmcs , the relative poses ( relative 3-d position and orientation ) between the cameras are invariant . while , in an acs , the cameras are articulated through movable joints , the relative pose between them may change . therefore , through calibration of an acs we want to find not only the relative poses between the cameras but also the positions of the joints in the acs . in this paper , we developed calibration algorithms for the acs using a simple constraint : the joint is fixed relative to the cameras connected with it during the transformations of the acs . when the transformations of the cameras in an acs can be estimated relative to the same coordinate system , the positions of the joints in the acs can be calculated by solving linear equations . however , in a non-overlapping view acs , only the ego-transformations of the cameras and can be estimated . we proposed a two-steps method to deal with this problem . in both methods , the acs is assumed to have performed general transformations in a static environment . the efficiency and robustness of the proposed methods are tested by simulation and real experiments . in the real experiment , the intrinsic and extrinsic parameters of the acs are obtained simultaneously by our calibration procedure using the same image sequences , no extra data capturing step is required . the corresponding trajectory is recovered and illustrated using the calibration results of the acs . since the estimated translations of different cameras in an acs may scaled by different scale factors , a scale factor estimation algorithm is also proposed . to our knowledge , we are the first to study the calibration of acs .", "topics": ["approximation algorithm", "simulation"]}
{"title": "developing an icu scoring system with interaction terms using a genetic algorithm", "abstract": "icu mortality scoring systems attempt to predict patient mortality using predictive models with various clinical predictors . examples of such systems are apache , saps and mpm . however , most such scoring systems do not actively look for and include interaction terms , despite physicians intuitively taking such interactions into account when making a diagnosis . one barrier to including such terms in predictive models is the difficulty of using most variable selection methods in high-dimensional datasets . a genetic algorithm framework for variable selection with logistic regression models is used to search for two-way interaction terms in a clinical dataset of adult icu patients , with separate models being built for each category of diagnosis upon admittance to the icu . the models had good discrimination across all categories , with a weighted average auc of 0.84 ( > 0.90 for several categories ) and the genetic algorithm was able to find several significant interaction terms , which may be able to provide greater insight into mortality prediction for health practitioners . the ga selected models had improved performance against stepwise selection and random forest models , and provides greater flexibility in terms of variable selection by being able to optimize over any modeler-defined model performance metric instead of a specific variable importance metric .", "topics": ["interaction"]}
{"title": "a density compensation-based path computing model for measuring semantic similarity", "abstract": "the shortest path between two concepts in a taxonomic ontology is commonly used to represent the semantic distance between concepts in the edge-based semantic similarity measures . in the past , the edge counting is considered to be the default method for the path computation , which is simple , intuitive and has low computational complexity . however , a large lexical taxonomy of such as wordnet has the irregular densities of links between concepts due to its broad domain but . the edge counting-based path computation is powerless for this non-uniformity problem . in this paper , we advocate that the path computation is able to be separated from the edge-based similarity measures and form various general computing models . therefore , in order to solve the problem of non-uniformity of concept density in a large taxonomic ontology , we propose a new path computing model based on the compensation of local area density of concepts , which is equal to the number of direct hyponyms of the subsumers of concepts in their shortest path . this path model considers the local area density of concepts as an extension of the edge-based path and converts the local area density divided by their depth into the compensation for edge-based path with an adjustable parameter , which idea has been proven to be consistent with the information theory . this model is a general path computing model and can be applied in various edge-based similarity algorithms . the experiment results show that the proposed path model improves the average correlation between edge-based measures with human judgments on miller and charles benchmark from less than 0.8 to more than 0.85 , and has a big advantage in efficiency than information content ( ic ) computation in a dynamic ontology , thereby successfully solving the non-uniformity problem of taxonomic ontology .", "topics": ["computational complexity theory", "computation"]}
{"title": "mind the gap : a well log data analysis", "abstract": "the main task in oil and gas exploration is to gain an understanding of the distribution and nature of rocks and fluids in the subsurface . well logs are records of petro-physical data acquired along a borehole , providing direct information about what is in the subsurface . the data collected by logging wells can have significant economic consequences , due to the costs inherent to drilling wells , and the potential return of oil deposits . in this paper , we describe preliminary work aimed at building a general framework for well log prediction . first , we perform a descriptive and exploratory analysis of the gaps in the neutron porosity logs of more than a thousand wells in the north sea . then , we generate artificial gaps in the neutron logs that reflect the statistics collected before . finally , we compare artificial neural networks , random forests , and three algorithms of linear regression in the prediction of missing gaps on a well-by-well basis .", "topics": ["neural networks"]}
{"title": "minimax classifier for uncertain costs", "abstract": "many studies on the cost-sensitive learning assumed that a unique cost matrix is known for a problem . however , this assumption may not hold for many real-world problems . for example , a classifier might need to be applied in several circumstances , each of which associates with a different cost matrix . or , different human experts have different opinions about the costs for a given problem . motivated by these facts , this study aims to seek the minimax classifier over multiple cost matrices . in summary , we theoretically proved that , no matter how many cost matrices are involved , the minimax problem can be tackled by solving a number of standard cost-sensitive problems and sub-problems that involve only two cost matrices . as a result , a general framework for achieving minimax classifier over multiple cost matrices is suggested and justified by preliminary empirical studies .", "topics": ["statistical classification"]}
{"title": "domain adaptation for sequence labeling using hidden markov models", "abstract": "most natural language processing systems based on machine learning are not robust to domain shift . for example , a state-of-the-art syntactic dependency parser trained on wall street journal sentences has an absolute drop in performance of more than ten points when tested on textual data from the web . an efficient solution to make these methods more robust to domain shift is to first learn a word representation using large amounts of unlabeled data from both domains , and then use this representation as features in a supervised learning algorithm . in this paper , we propose to use hidden markov models to learn word representations for part-of-speech tagging . in particular , we study the influence of using data from the source , the target or both domains to learn the representation and the different ways to represent words using an hmm .", "topics": ["natural language processing", "supervised learning"]}
{"title": "improving opinion-target extraction with character-level word embeddings", "abstract": "fine-grained sentiment analysis is receiving increasing attention in recent years . extracting opinion target expressions ( ote ) in reviews is often an important step in fine-grained , aspect-based sentiment analysis . retrieving this information from user-generated text , however , can be difficult . customer reviews , for instance , are prone to contain misspelled words and are difficult to process due to their domain-specific language . in this work , we investigate whether character-level models can improve the performance for the identification of opinion target expressions . we integrate information about the character structure of a word into a sequence labeling system using character-level word embeddings and show their positive impact on the system 's performance . specifically , we obtain an increase by 3.3 points f1-score with respect to our baseline model . in further experiments , we reveal encoded character patterns of the learned embeddings and give a nuanced view of the performance differences of both models .", "topics": ["baseline ( configuration management )"]}
{"title": "a classifier-free ensemble selection method based on data diversity in random subspaces", "abstract": "the ensemble of classifiers ( eoc ) has been shown to be effective in improving the performance of single classifiers by combining their outputs , and one of the most important properties involved in the selection of the best eoc from a pool of classifiers is considered to be classifier diversity . in general , classifier diversity does not occur randomly , but is generated systematically by various ensemble creation methods . by using diverse data subsets to train classifiers , these methods can create diverse classifiers for the eoc . in this work , we propose a scheme to measure data diversity directly from random subspaces , and explore the possibility of using it to select the best data subsets for the construction of the eoc . our scheme is the first ensemble selection method to be presented in the literature based on the concept of data diversity . its main advantage over the traditional framework ( ensemble creation then selection ) is that it obviates the need for classifier training prior to ensemble selection . a single genetic algorithm ( ga ) and a multi-objective genetic algorithm ( moga ) were evaluated to search for the best solutions for the classifier-free ensemble selection . in both cases , objective functions based on different clustering diversity measures were implemented and tested . all the results obtained with the proposed classifier-free ensemble selection method were compared with the traditional classifier-based ensemble selection using mean classifier error ( me ) and majority voting error ( mve ) . the applicability of the method is tested on uci machine learning problems and nist sd19 handwritten numerals .", "topics": ["cluster analysis"]}
{"title": "deepchess : end-to-end deep neural network for automatic learning in chess", "abstract": "we present an end-to-end learning method for chess , relying on deep neural networks . without any a priori knowledge , in particular without any knowledge regarding the rules of chess , a deep neural network is trained using a combination of unsupervised pretraining and supervised training . the unsupervised training extracts high level features from a given position , and the supervised training learns to compare two chess positions and select the more favorable one . the training relies entirely on datasets of several million chess games , and no further domain specific knowledge is incorporated . the experiments show that the resulting neural network ( referred to as deepchess ) is on a par with state-of-the-art chess playing programs , which have been developed through many years of manual feature selection and tuning . deepchess is the first end-to-end machine learning-based method that results in a grandmaster-level chess playing performance .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "vector symbolic architectures answer jackendoff 's challenges for cognitive neuroscience", "abstract": "jackendoff ( 2002 ) posed four challenges that linguistic combinatoriality and rules of language present to theories of brain function . the essence of these problems is the question of how to neurally instantiate the rapid construction and transformation of the compositional structures that are typically taken to be the domain of symbolic processing . he contended that typical connectionist approaches fail to meet these challenges and that the dialogue between linguistic theory and cognitive neuroscience will be relatively unproductive until the importance of these problems is widely recognised and the challenges answered by some technical innovation in connectionist modelling . this paper claims that a little-known family of connectionist models ( vector symbolic architectures ) are able to meet jackendoff 's challenges .", "topics": ["support vector machine"]}
{"title": "improving deep learning by inverse square root linear units ( isrlus )", "abstract": "we introduce the `` inverse square root linear unit '' ( isrlu ) to speed up learning in deep neural networks . isrlu has better performance than elu but has many of the same benefits . isrlu and elu have similar curves and characteristics . both have negative values , allowing them to push mean unit activation closer to zero , and bring the normal gradient closer to the unit natural gradient , ensuring a noise-robust deactivation state , lessening the over fitting risk . the significant performance advantage of isrlu on traditional cpus also carry over to more efficient hw implementations on hw/sw codesign for cnns/rnns . in experiments with tensorflow , isrlu leads to faster learning and better generalization than relu on cnns . this work also suggests a computationally efficient variant called the `` inverse square root unit '' ( isru ) which can be used for rnns . many rnns use either long short-term memory ( lstm ) and gated recurrent units ( gru ) which are implemented with tanh and sigmoid activation functions . isru has less com- putational complexity but still has a similar curve to tanh and sigmoid .", "topics": ["computational complexity theory", "gradient"]}
{"title": "multi-sensor prognostics using an unsupervised health index based on lstm encoder-decoder", "abstract": "many approaches for estimation of remaining useful life ( rul ) of a machine , using its operational sensor data , make assumptions about how a system degrades or a fault evolves , e.g . , exponential degradation . however , in many domains degradation may not follow a pattern . we propose a long short term memory based encoder-decoder ( lstm-ed ) scheme to obtain an unsupervised health index ( hi ) for a system using multi-sensor time-series data . lstm-ed is trained to reconstruct the time-series corresponding to healthy state of a system . the reconstruction error is used to compute hi which is then used for rul estimation . we evaluate our approach on publicly available turbofan engine and milling machine datasets . we also present results on a real-world industry dataset from a pulverizer mill where we find significant correlation between lstm-ed based hi and maintenance costs .", "topics": ["encoder"]}
{"title": "opposition based electromagnetismlike for global optimization", "abstract": "electromagnetismlike optimization ( emo ) is a global optimization algorithm , particularly well suited to solve problems featuring nonlinear and multimodal cost functions . emo employs searcher agents that emulate a population of charged particles which interact to each other according to electromagnetisms laws of attraction and repulsion . however , emo usually requires a large number of iterations for a local search procedure ; any reduction or cancelling over such number , critically perturb other issues such as convergence , exploration , population diversity and accuracy . this paper presents an enhanced emo algorithm called obemo , which employs the opposition-based learning ( obl ) approach to accelerate the global convergence speed . obl is a machine intelligence strategy which considers the current candidate solution and its opposite value at the same time , achieving a faster exploration of the search space . the proposed obemo method significantly reduces the required computational effort yet avoiding any detriment to the good search capabilities of the original emo algorithm . experiments are conducted over a comprehensive set of benchmark functions , showing that obemo obtains promising performance for most of the discussed test problems .", "topics": ["nonlinear system", "iteration"]}
{"title": "learning bayesian networks with incomplete data by augmentation", "abstract": "we present new algorithms for learning bayesian networks from data with missing values using a data augmentation approach . an exact bayesian network learning algorithm is obtained by recasting the problem into a standard bayesian network learning problem without missing data . to the best of our knowledge , this is the first exact algorithm for this problem . as expected , the exact algorithm does not scale to large domains . we build on the exact method to create an approximate algorithm using a hill-climbing technique . this algorithm scales to large domains so long as a suitable standard structure learning method for complete data is available . we perform a wide range of experiments to demonstrate the benefits of learning bayesian networks with such new approach .", "topics": ["approximation algorithm", "bayesian network"]}
{"title": "some theoretical properties of gans", "abstract": "generative adversarial networks ( gans ) are a class of generative algorithms that have been shown to produce state-of-the art samples , especially in the domain of image creation . the fundamental principle of gans is to approximate the unknown distribution of a given data set by optimizing an objective function through an adversarial game between a family of generators and a family of discriminators . in this paper , we offer a better theoretical understanding of gans by analyzing some of their mathematical and statistical properties . we study the deep connection between the adversarial principle underlying gans and the jensen-shannon divergence , together with some optimality characteristics of the problem . an analysis of the role of the discriminator family via approximation arguments is also provided . in addition , taking a statistical point of view , we study the large sample properties of the estimated distribution and prove in particular a central limit theorem . some of our results are illustrated with simulated examples .", "topics": ["approximation algorithm", "loss function"]}
{"title": "modeling compositionality with multiplicative recurrent neural networks", "abstract": "we present the multiplicative recurrent neural network as a general model for compositional meaning in language , and evaluate it on the task of fine-grained sentiment analysis . we establish a connection to the previously investigated matrix-space models for compositionality , and show they are special cases of the multiplicative recurrent net . our experiments show that these models perform comparably or better than elman-type additive recurrent neural networks and outperform matrix-space models on a standard fine-grained sentiment analysis corpus . furthermore , they yield comparable results to structural deep models on the recently published stanford sentiment treebank without the need for generating parse trees .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "machine teaching for bayesian learners in the exponential family", "abstract": "what if there is a teacher who knows the learning goal and wants to design good training data for a machine learner ? we propose an optimal teaching framework aimed at learners who employ bayesian models . our framework is expressed as an optimization problem over teaching examples that balance the future loss of the learner and the effort of the teacher . this optimization problem is in general hard . in the case where the learner employs conjugate exponential family models , we present an approximate algorithm for finding the optimal teaching set . our algorithm optimizes the aggregate sufficient statistics , then unpacks them into actual teaching examples . we give several examples to illustrate our framework .", "topics": ["test set", "approximation algorithm"]}
{"title": "geometry aware mappings for high dimensional sparse factors", "abstract": "while matrix factorisation models are ubiquitous in large scale recommendation and search , real time application of such models requires inner product computations over an intractably large set of item factors . in this manuscript we present a novel framework that uses the inverted index representation to exploit structural properties of sparse vectors to significantly reduce the run time computational cost of factorisation models . we develop techniques that use geometry aware permutation maps on a tessellated unit sphere to obtain high dimensional sparse embeddings for latent factors with sparsity patterns related to angular closeness of the original latent factors . we also design several efficient and deterministic realisations within this framework and demonstrate with experiments that our techniques lead to faster run time operation with minimal loss of accuracy .", "topics": ["computational complexity theory", "sparse matrix"]}
{"title": "data fusion of objects using techniques such as laser scanning , structured light and photogrammetry for cultural heritage applications", "abstract": "in this paper we present a semi-automatic 2d-3d local registration pipeline capable of coloring 3d models obtained from 3d scanners by using uncalibrated images . the proposed pipeline exploits the structure from motion ( sfm ) technique in order to reconstruct a sparse representation of the 3d object and obtain the camera parameters from image feature matches . we then coarsely register the reconstructed 3d model to the scanned one through the scale iterative closest point ( sicp ) algorithm . sicp provides the global scale , rotation and translation parameters , using minimal manual user intervention . in the final processing stage , a local registration refinement algorithm optimizes the color projection of the aligned photos on the 3d object removing the blurring/ghosting artefacts introduced due to small inaccuracies during the registration . the proposed pipeline is capable of handling real world cases with a range of characteristics from objects with low level geometric features to complex ones .", "topics": ["high- and low-level", "sparse matrix"]}
{"title": "c3a : a cognitive collaborative control architecture for an intelligent wheelchair", "abstract": "retention of residual skills for persons who partially lose their cognitive or physical ability is of utmost importance . research is focused on developing systems that provide need-based assistance for retention of such residual skills . this paper describes a novel cognitive collaborative control architecture c3a , designed to address the challenges of developing need- based assistance for wheelchair navigation . organization of c3a is detailed and results from simulation of the proposed architecture is presented . for simulation of our proposed architecture , we have used ros ( robot operating system ) as a control framework and a 3d robotic simulator called usarsim ( unified system for automation and robot simulation ) .", "topics": ["simulation", "robot"]}
{"title": "neural morphological tagging from characters for morphologically rich languages", "abstract": "this paper investigates neural character-based morphological tagging for languages with complex morphology and large tag sets . we systematically explore a variety of neural architectures ( dnn , cnn , cnnhighway , lstm , blstm ) to obtain character-based word vectors combined with bidirectional lstms to model across-word context in an end-to-end setting . we explore supplementary use of word-based vectors trained on large amounts of unlabeled data . our experiments for morphological tagging suggest that for `` simple '' model configurations , the choice of the network architecture ( cnn vs. cnnhighway vs. lstm vs. blstm ) or the augmentation with pre-trained word embeddings can be important and clearly impact the accuracy . increasing the model capacity by adding depth , for example , and carefully optimizing the neural networks can lead to substantial improvements , and the differences in accuracy ( but not training time ) become much smaller or even negligible . overall , our best morphological taggers for german and czech outperform the best results reported in the literature by a large margin .", "topics": ["end-to-end principle"]}
{"title": "binary generative adversarial networks for image retrieval", "abstract": "the most striking successes in image retrieval using deep hashing have mostly involved discriminative models , which require labels . in this paper , we use binary generative adversarial networks ( bgan ) to embed images to binary codes in an unsupervised way . by restricting the input noise variable of generative adversarial networks ( gan ) to be binary and conditioned on the features of each input image , bgan can simultaneously learn a binary representation per image , and generate an image plausibly similar to the original one . in the proposed framework , we address two main problems : 1 ) how to directly generate binary codes without relaxation ? 2 ) how to equip the binary representation with the ability of accurate image retrieval ? we resolve these problems by proposing new sign-activation strategy and a loss function steering the learning process , which consists of new models for adversarial loss , a content loss , and a neighborhood structure loss . experimental results on standard datasets ( cifar-10 , nuswide , and flickr ) demonstrate that our bgan significantly outperforms existing hashing methods by up to 107\\ % in terms of~map ( see table tab.res.map.comp ) our anonymous code is available at : https : //github.com/htconquer/bgan .", "topics": ["loss function"]}
{"title": "predicting complete 3d models of indoor scenes", "abstract": "one major goal of vision is to infer physical models of objects , surfaces , and their layout from sensors . in this paper , we aim to interpret indoor scenes from one rgbd image . our representation encodes the layout of walls , which must conform to a manhattan structure but is otherwise flexible , and the layout and extent of objects , modeled with cad-like 3d shapes . we represent both the visible and occluded portions of the scene , producing a complete 3d parse . such a scene interpretation is useful for robotics and visual reasoning , but difficult to produce due to the well-known challenge of segmentation , the high degree of occlusion , and the diversity of objects in indoor scene . we take a data-driven approach , generating sets of potential object regions , matching to regions in training images , and transferring and aligning associated 3d models while encouraging fit to observations and overall consistency . we demonstrate encouraging results on the nyu v2 dataset and highlight a variety of interesting directions for future work .", "topics": ["parsing", "sensor"]}
{"title": "neutral evolution and turnover over centuries of english word popularity", "abstract": "here we test neutral models against the evolution of english word frequency and vocabulary at the population scale , as recorded in annual word frequencies from three centuries of english language books . against these data , we test both static and dynamic predictions of two neutral models , including the relation between corpus size and vocabulary size , frequency distributions , and turnover within those frequency distributions . although a commonly used neutral model fails to replicate all these emergent properties at once , we find that modified two-stage neutral model does replicate the static and dynamic properties of the corpus data . this two-stage model is meant to represent a relatively small corpus ( population ) of english books , analogous to a `canon ' , sampled by an exponentially increasing corpus of books in the wider population of authors . more broadly , this mode -- a smaller neutral model within a larger neutral model -- could represent more broadly those situations where mass attention is focused on a small subset of the cultural variants .", "topics": ["text corpus"]}
{"title": "real-time bidding by reinforcement learning in display advertising", "abstract": "the majority of online display ads are served through real-time bidding ( rtb ) -- - each ad display impression is auctioned off in real-time when it is just being generated from a user visit . to place an ad automatically and optimally , it is critical for advertisers to devise a learning algorithm to cleverly bid an ad impression in real-time . most previous works consider the bid decision as a static optimization problem of either treating the value of each impression independently or setting a bid price to each segment of ad volume . however , the bidding for a given ad campaign would repeatedly happen during its life span before the budget runs out . as such , each bid is strategically correlated by the constrained budget and the overall effectiveness of the campaign ( e.g . , the rewards from generated clicks ) , which is only observed after the campaign has completed . thus , it is of great interest to devise an optimal bidding strategy sequentially so that the campaign budget can be dynamically allocated across all the available impressions on the basis of both the immediate and future rewards . in this paper , we formulate the bid decision process as a reinforcement learning problem , where the state space is represented by the auction information and the campaign 's real-time parameters , while an action is the bid price to set . by modeling the state transition via auction competition , we build a markov decision process framework for learning the optimal bidding policy to optimize the advertising performance in the dynamic real-time bidding environment . furthermore , the scalability problem from the large real-world auction volume and campaign budget is well handled by state value approximation using neural networks .", "topics": ["value ( ethics )", "optimization problem"]}
{"title": "attend refine repeat : active box proposal generation via in-out localization", "abstract": "the problem of computing category agnostic bounding box proposals is utilized as a core component in many computer vision tasks and thus has lately attracted a lot of attention . in this work we propose a new approach to tackle this problem that is based on an active strategy for generating box proposals that starts from a set of seed boxes , which are uniformly distributed on the image , and then progressively moves its attention on the promising image areas where it is more likely to discover well localized bounding box proposals . we call our approach attractionet and a core component of it is a cnn-based category agnostic object location refinement module that is capable of yielding accurate and robust bounding box predictions regardless of the object category . we extensively evaluate our attractionet approach on several image datasets ( i.e . coco , pascal , imagenet detection and nyu-depth v2 datasets ) reporting on all of them state-of-the-art results that surpass the previous work in the field by a significant margin and also providing strong empirical evidence that our approach is capable to generalize to unseen categories . furthermore , we evaluate our attractionet proposals in the context of the object detection task using a vgg16-net based detector and the achieved detection performance on coco manages to significantly surpass all other vgg16-net based detectors while even being competitive with a heavily tuned resnet-101 based detector . code as well as box proposals computed for several datasets are available at : : https : //github.com/gidariss/attractionet .", "topics": ["test set", "computer vision"]}
{"title": "multisegmentation through wavelets : comparing the efficacy of daubechies vs coiflets", "abstract": "in this paper , we carry out a comparative study of the efficacy of wavelets belonging to daubechies and coiflet family in achieving image segmentation through a fast statistical algorithm.the fact that wavelets belonging to daubechies family optimally capture the polynomial trends and those of coiflet family satisfy mini-max condition , makes this comparison interesting . in the context of the present algorithm , it is found that the performance of coiflet wavelets is better , as compared to daubechies wavelet .", "topics": ["image segmentation", "polynomial"]}
{"title": "nested dictionary learning for hierarchical organization of imagery and text", "abstract": "a tree-based dictionary learning model is developed for joint analysis of imagery and associated text . the dictionary learning may be applied directly to the imagery from patches , or to general feature vectors extracted from patches or superpixels ( using any existing method for image feature extraction ) . each image is associated with a path through the tree ( from root to a leaf ) , and each of the multiple patches in a given image is associated with one node in that path . nodes near the tree root are shared between multiple paths , representing image characteristics that are common among different types of images . moving toward the leaves , nodes become specialized , representing details in image classes . if available , words ( text ) are also jointly modeled , with a path-dependent probability over words . the tree structure is inferred via a nested dirichlet process , and a retrospective stick-breaking sampler is used to infer the tree depth and width .", "topics": ["sampling ( signal processing )", "feature extraction"]}
{"title": "quantum artificial life in an ibm quantum computer", "abstract": "we present the first experimental realization of a quantum artificial life algorithm in a quantum computer . the quantum biomimetic protocol encodes tailored quantum behaviors belonging to living systems , namely , self-replication , mutation , interaction between individuals , and death , into the cloud quantum computer ibm ibmqx4 . in this experiment , entanglement spreads throughout generations of individuals , where genuine quantum information features are inherited through genealogical networks . as a pioneering proof-of-principle , experimental data fits the ideal model with accuracy . thereafter , these and other models of quantum artificial life , for which no classical device may predict its quantum supremacy evolution , can be further explored in novel generations of quantum computers . quantum biomimetics , quantum machine learning , and quantum artificial intelligence will move forward hand in hand through more elaborate levels of quantum complexity .", "topics": ["artificial intelligence"]}
{"title": "the amu-uedin submission to the wmt16 news translation task : attention-based nmt models as feature functions in phrase-based smt", "abstract": "this paper describes the amu-uedin submissions to the wmt 2016 shared task on news translation . we explore methods of decode-time integration of attention-based neural translation models with phrase-based statistical machine translation . efficient batch-algorithms for gpu-querying are proposed and implemented . for english-russian , our system stays behind the state-of-the-art pure neural models in terms of bleu . among restricted systems , manual evaluation places it in the first cluster tied with the pure neural model . for the russian-english task , our submission achieves the top bleu result , outperforming the best pure neural system by 1.1 bleu points and our own phrase-based baseline by 1.6 bleu . after manual evaluation , this system is the best restricted system in its own cluster . in follow-up experiments we improve results by additional 0.8 bleu .", "topics": ["baseline ( configuration management )", "machine translation"]}
{"title": "graph convolutional networks for classification with a structured label space", "abstract": "it is a usual practice to ignore any structural information underlying classes in multi-class classification . in this paper , we propose a graph convolutional network ( gcn ) augmented neural network classifier to exploit a known , underlying graph structure of labels . the proposed approach resembles an ( approximate ) inference procedure in , for instance , a conditional random field ( crf ) . we evaluate the proposed approach on document classification and object recognition and report both accuracies and graph-theoretic metrics that correspond to the consistency of the model 's prediction . the experiment results reveal that the proposed model outperforms a baseline method which ignores the graph structures of a label space in terms of graph-theoretic metrics .", "topics": ["baseline ( configuration management )", "approximation algorithm"]}
{"title": "optimization of real , hermitian quadratic forms : real , complex hopfield-amari neural network", "abstract": "in this research paper , the problem of optimization of quadratic forms associated with the dynamics of hopfield-amari neural network is considered . an elegant ( and short ) proof of the states at which local/global minima of quadratic form are attained is provided . a theorem associated with local/global minimization of quadratic energy function using the hopfield-amari neural network is discussed . the results are generalized to a `` complex hopfield neural network '' dynamics over the complex hypercube ( using a `` complex signum function '' ) . it is also reasoned through two theorems that there is no loss of generality in assuming the threshold vector to be a zero vector in the case of real as well as a `` complex hopfield neural network '' . some structured quadratic forms like toeplitz form and complex toeplitz form are discussed .", "topics": ["mathematical optimization"]}
{"title": "euclidean upgrade from a minimal number of segments", "abstract": "in this paper , we propose an algebraic approach to upgrade a projective reconstruction to a euclidean one , and aim at computing the rectifying homography from a minimal number of 9 segments of known length . constraints are derived from these segments which yield a set of polynomial equations that we solve by means of gr\\ '' obner bases . we explain how a solver for such a system of equations can be constructed from simplified template data . moreover , we present experiments that demonstrate that the given problem can be solved in this way .", "topics": ["polynomial"]}
{"title": "merf : morphology-based entity and relational entity extraction framework for arabic", "abstract": "rule-based techniques and tools to extract entities and relational entities from documents allow users to specify desired entities using natural language questions , finite state automata , regular expressions , structured query language statements , or proprietary scripts . these techniques and tools require expertise in linguistics and programming and lack support of arabic morphological analysis which is key to process arabic text . in this work , we present merf ; a morphology-based entity and relational entity extraction framework for arabic text . merf provides a user-friendly interface where the user , with basic knowledge of linguistic features and regular expressions , defines tag types and interactively associates them with regular expressions defined over boolean formulae . boolean formulae range over matches of arabic morphological features , and synonymity features . users define user defined relations with tuples of subexpression matches and can associate code actions with subexpressions . merf computes feature matches , regular expression matches , and constructs entities and relational entities from user defined relations . we evaluated our work with several case studies and compared with existing application-specific techniques . the results show that merf requires shorter development time and effort compared to existing techniques and produces reasonably accurate results within a reasonable overhead in run time .", "topics": ["natural language", "entity"]}
{"title": "structure and parameter learning for causal independence and causal interaction models", "abstract": "this paper discusses causal independence models and a generalization of these models called causal interaction models . causal interaction models are models that have independent mechanisms where a mechanism can have several causes . in addition to introducing several particular types of causal interaction models , we show how we can apply the bayesian approach to learning causal interaction models obtaining approximate posterior distributions for the models and obtain map and ml estimates for the parameters . we illustrate the approach with a simulation study of learning model posteriors .", "topics": ["approximation algorithm", "simulation"]}
{"title": "understanding and diagnosing visual tracking systems", "abstract": "several benchmark datasets for visual tracking research have been proposed in recent years . despite their usefulness , whether they are sufficient for understanding and diagnosing the strengths and weaknesses of different trackers remains questionable . to address this issue , we propose a framework by breaking a tracker down into five constituent parts , namely , motion model , feature extractor , observation model , model updater , and ensemble post-processor . we then conduct ablative experiments on each component to study how it affects the overall result . surprisingly , our findings are discrepant with some common beliefs in the visual tracking research community . we find that the feature extractor plays the most important role in a tracker . on the other hand , although the observation model is the focus of many studies , we find that it often brings no significant improvement . moreover , the motion model and model updater contain many details that could affect the result . also , the ensemble post-processor can improve the result substantially when the constituent trackers have high diversity . based on our findings , we put together some very elementary building blocks to give a basic tracker which is competitive in performance to the state-of-the-art trackers . we believe our framework can provide a solid baseline when conducting controlled experiments for visual tracking research .", "topics": ["baseline ( configuration management )"]}
{"title": "variational dual-tree framework for large-scale transition matrix approximation", "abstract": "in recent years , non-parametric methods utilizing random walks on graphs have been used to solve a wide range of machine learning problems , but in their simplest form they do not scale well due to the quadratic complexity . in this paper , a new dual-tree based variational approach for approximating the transition matrix and efficiently performing the random walk is proposed . the approach exploits a connection between kernel density estimation , mixture modeling , and random walk on graphs in an optimization of the transition matrix for the data graph that ties together edge transitions probabilities that are similar . compared to the de facto standard approximation method based on k-nearestneighbors , we demonstrate order of magnitudes speedup without sacrificing accuracy for label propagation tasks on benchmark data sets in semi-supervised learning .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "using apache lucene to search vector of locally aggregated descriptors", "abstract": "surrogate text representation ( str ) is a profitable solution to efficient similarity search on metric space using conventional text search engines , such as apache lucene . this technique is based on comparing the permutations of some reference objects in place of the original metric distance . however , the achilles heel of str approach is the need to reorder the result set of the search according to the metric distance . this forces to use a support database to store the original objects , which requires efficient random i/o on a fast secondary memory ( such as flash-based storages ) . in this paper , we propose to extend the surrogate text representation to specifically address a class of visual metric objects known as vector of locally aggregated descriptors ( vlad ) . this approach is based on representing the individual sub-vectors forming the vlad vector with the str , providing a finer representation of the vector and enabling us to get rid of the reordering phase . the experiments on a publicly available dataset show that the extended str outperforms the baseline str achieving satisfactory performance near to the one obtained with the original vlad vectors .", "topics": ["baseline ( configuration management )", "database"]}
{"title": "numerical sensitivity and efficiency in the treatment of epistemic and aleatory uncertainty", "abstract": "the treatment of both aleatory and epistemic uncertainty by recent methods often requires an high computational effort . in this abstract , we propose a numerical sampling method allowing to lighten the computational burden of treating the information by means of so-called fuzzy random variables .", "topics": ["sampling ( signal processing )", "numerical analysis"]}
{"title": "the design and experimental analysis of algorithms for temporal reasoning", "abstract": "many applications -- from planning and scheduling to problems in molecular biology -- rely heavily on a temporal reasoning component . in this paper , we discuss the design and empirical analysis of algorithms for a temporal reasoning system based on allen 's influential interval-based framework for representing temporal information . at the core of the system are algorithms for determining whether the temporal information is consistent , and , if so , finding one or more scenarios that are consistent with the temporal information . two important algorithms for these tasks are a path consistency algorithm and a backtracking algorithm . for the path consistency algorithm , we develop techniques that can result in up to a ten-fold speedup over an already highly optimized implementation . for the backtracking algorithm , we develop variable and value ordering heuristics that are shown empirically to dramatically improve the performance of the algorithm . as well , we show that a previously suggested reformulation of the backtracking search problem can reduce the time and space requirements of the backtracking search . taken together , the techniques we develop allow a temporal reasoning component to solve problems that are of practical size .", "topics": ["heuristic"]}
{"title": "improved linear embeddings via lagrange duality", "abstract": "near isometric orthogonal embeddings to lower dimensions are a fundamental tool in data science and machine learning . in this paper , we present the construction of such embeddings that minimizes the maximum distortion for a given set of points . we formulate the problem as a non convex constrained optimization problem . we first construct a primal relaxation and then use the theory of lagrange duality to create dual relaxation . we also suggest a polynomial time algorithm based on the theory of convex optimization to solve the dual relaxation provably . we provide a theoretical upper bound on the approximation guarantees for our algorithm , which depends only on the spectral properties of the dataset . we experimentally demonstrate the superiority of our algorithm compared to baselines in terms of the scalability and the ability to achieve lower distortion .", "topics": ["optimization problem", "time complexity"]}
{"title": "lstm : a search space odyssey", "abstract": "several variants of the long short-term memory ( lstm ) architecture for recurrent neural networks have been proposed since its inception in 1995 . in recent years , these networks have become the state-of-the-art models for a variety of machine learning problems . this has led to a renewed interest in understanding the role and utility of various computational components of typical lstm variants . in this paper , we present the first large-scale analysis of eight lstm variants on three representative tasks : speech recognition , handwriting recognition , and polyphonic music modeling . the hyperparameters of all lstm variants for each task were optimized separately using random search , and their importance was assessed using the powerful fanova framework . in total , we summarize the results of 5400 experimental runs ( $ \\approx 15 $ years of cpu time ) , which makes our study the largest of its kind on lstm networks . our results show that none of the variants can improve upon the standard lstm architecture significantly , and demonstrate the forget gate and the output activation function to be its most critical components . we further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment .", "topics": ["recurrent neural network", "speech recognition"]}
{"title": "optimal change point detection in gaussian processes", "abstract": "we study the problem of detecting a change in the mean of one-dimensional gaussian process data . this problem is investigated in the setting of increasing domain ( customarily employed in time series analysis ) and in the setting of fixed domain ( typically arising in spatial data analysis ) . we propose a detection method based on the generalized likelihood ratio test ( glrt ) , and show that our method achieves nearly asymptotically optimal rate in the minimax sense , in both settings . the salient feature of the proposed method is that it exploits in an efficient way the data dependence captured by the gaussian process covariance structure . when the covariance is not known , we propose the plug-in glrt method and derive conditions under which the method remains asymptotically near optimal . by contrast , the standard cusum method , which does not account for the covariance structure , is shown to be asymptotically optimal only in the increasing domain . our algorithms and accompanying theory are applicable to a wide variety of covariance structures , including the matern class , the powered exponential class , and others . the plug-in glrt method is shown to perform well for maximum likelihood estimators with a dense covariance matrix .", "topics": ["time series", "time complexity"]}
{"title": "max-pooling dropout for regularization of convolutional neural networks", "abstract": "recently , dropout has seen increasing use in deep learning . for deep convolutional neural networks , dropout is known to work well in fully-connected layers . however , its effect in pooling layers is still not clear . this paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time . in light of this insight , we advocate employing our proposed probabilistic weighted pooling , instead of commonly used max-pooling , to act as model averaging at test time . empirical evidence validates the superiority of probabilistic weighted pooling . we also compare max-pooling dropout and stochastic pooling , both of which introduce stochasticity based on multinomial distributions at pooling stage .", "topics": ["matrix regularization"]}
{"title": "neural signatures for licence plate re-identification", "abstract": "the problem of vehicle licence plate re-identification is generally considered as a one-shot image retrieval problem . the objective of this task is to learn a feature representation ( called a `` signature '' ) for licence plates . incoming licence plate images are converted to signatures and matched to a previously collected template database through a distance measure . then , the input image is recognized as the template whose signature is `` nearest '' to the input signature . the template database is restricted to contain only a single signature per unique licence plate for our problem . we measure the performance of deep convolutional net-based features adapted from face recognition on this task . in addition , we also test a hybrid approach combining the fisher vector with a neural network-based embedding called `` f2nn '' trained with the triplet loss function . we find that the hybrid approach performs comparably while providing computational benefits . the signature generated by the hybrid approach also shows higher generalizability to datasets more dissimilar to the training corpus .", "topics": ["loss function"]}
{"title": "riemannian dictionary learning and sparse coding for positive definite matrices", "abstract": "data encoded as symmetric positive definite ( spd ) matrices frequently arise in many areas of computer vision and machine learning . while these matrices form an open subset of the euclidean space of symmetric matrices , viewing them through the lens of non-euclidean riemannian geometry often turns out to be better suited in capturing several desirable data properties . however , formulating classical machine learning algorithms within such a geometry is often non-trivial and computationally expensive . inspired by the great success of dictionary learning and sparse coding for vector-valued data , our goal in this paper is to represent data in the form of spd matrices as sparse conic combinations of spd atoms from a learned dictionary via a riemannian geometric approach . to that end , we formulate a novel riemannian optimization objective for dictionary learning and sparse coding in which the representation loss is characterized via the affine invariant riemannian metric . we also present a computationally simple algorithm for optimizing our model . experiments on several computer vision datasets demonstrate superior classification and retrieval performance using our approach when compared to sparse coding via alternative non-riemannian formulations .", "topics": ["mathematical optimization", "computer vision"]}
{"title": "predicting citywide crowd flows using deep spatio-temporal residual networks", "abstract": "forecasting the flow of crowds is of great importance to traffic management and public safety , and very challenging as it is affected by many complex factors , including spatial dependencies ( nearby and distant ) , temporal dependencies ( closeness , period , trend ) , and external conditions ( e.g . , weather and events ) . we propose a deep-learning-based approach , called st-resnet , to collectively forecast two types of crowd flows ( i.e . inflow and outflow ) in each and every region of a city . we design an end-to-end structure of st-resnet based on unique properties of spatio-temporal data . more specifically , we employ the residual neural network framework to model the temporal closeness , period , and trend properties of crowd traffic . for each property , we design a branch of residual convolutional units , each of which models the spatial properties of crowd traffic . st-resnet learns to dynamically aggregate the output of the three residual neural networks based on data , assigning different weights to different branches and regions . the aggregation is further combined with external factors , such as weather and day of the week , to predict the final traffic of crowds in each and every region . we have developed a real-time system based on microsoft azure cloud , called urbanflow , providing the crowd flow monitoring and forecasting in guiyang city of china . in addition , we present an extensive experimental evaluation using two types of crowd flows in beijing and new york city ( nyc ) , where st-resnet outperforms nine well-known baselines .", "topics": ["baseline ( configuration management )", "end-to-end principle"]}
{"title": "the off-switch game", "abstract": "it is clear that one of the primary tools we can use to mitigate the potential risk from a misbehaving ai system is the ability to turn the system off . as the capabilities of ai systems improve , it is important to ensure that such systems do not adopt subgoals that prevent a human from switching them off . this is a challenge because many formulations of rational agents create strong incentives for self-preservation . this is not caused by a built-in instinct , but because a rational agent will maximize expected utility and can not achieve whatever objective it has been given if it is dead . our goal is to study the incentives an agent has to allow itself to be switched off . we analyze a simple game between a human h and a robot r , where h can press r 's off switch but r can disable the off switch . a traditional agent takes its reward function for granted : we show that such agents have an incentive to disable the off switch , except in the special case where h is perfectly rational . our key insight is that for r to want to preserve its off switch , it needs to be uncertain about the utility associated with the outcome , and to treat h 's actions as important observations about that utility . ( r also has no incentive to switch itself off in this setting . ) we conclude that giving machines an appropriate level of uncertainty about their objectives leads to safer designs , and we argue that this setting is a useful generalization of the classical ai paradigm of rational agents .", "topics": ["reinforcement learning"]}
{"title": "statistical exponential families : a digest with flash cards", "abstract": "this document describes concisely the ubiquitous class of exponential family distributions met in statistics . the first part recalls definitions and summarizes main properties and duality with bregman divergences ( all proofs are skipped ) . the second part lists decompositions and related formula of common exponential family distributions . we recall the fisher-rao-riemannian geometries and the dual affine connection information geometries of statistical manifolds . it is intended to maintain and update this document and catalog by adding new distribution items .", "topics": ["time complexity"]}
{"title": "anveshak - a groundtruth generation tool for foreground regions of document images", "abstract": "we propose a graphical user interface based groundtruth generation tool in this paper . here , annotation of an input document image is done based on the foreground pixels . foreground pixels are grouped together with user interaction to form labeling units . these units are then labeled by the user with the user defined labels . the output produced by the tool is an image with an xml file containing its metadata information . this annotated data can be further used in different applications of document image analysis .", "topics": ["pixel"]}
{"title": "user dependent features in online signature verification", "abstract": "in this paper , we propose a novel approach for verification of on-line signatures based on user dependent feature selection and symbolic representation . unlike other signature verification methods , which work with same features for all users , the proposed approach introduces the concept of user dependent features . it exploits the typicality of each and every user to select different features for different users . initially all possible features are extracted for all users and a method of feature selection is employed for selecting user dependent features . the selected features are clustered using fuzzy c means algorithm . in order to preserve the intra-class variation within each user , we recommend to represent each cluster in the form of an interval valued symbolic feature vector . a method of signature verification based on the proposed cluster based symbolic representation is also presented . extensive experimentations are conducted on mcyt-100 user ( db1 ) and mcyt-330 user ( db2 ) online signature data sets to demonstrate the effectiveness of the proposed novel approach .", "topics": ["feature vector"]}
{"title": "institutionally distributed deep learning networks", "abstract": "deep learning has become a promising approach for automated medical diagnoses . when medical data samples are limited , collaboration among multiple institutions is necessary to achieve high algorithm performance . however , sharing patient data often has limitations due to technical , legal , or ethical concerns . in such cases , sharing a deep learning model is a more attractive alternative . the best method of performing such a task is unclear , however . in this study , we simulate the dissemination of learning deep learning network models across four institutions using various heuristics and compare the results with a deep learning model trained on centrally hosted patient data . the heuristics investigated include ensembling single institution models , single weight transfer , and cyclical weight transfer . we evaluated these approaches for image classification in three independent image collections ( retinal fundus photos , mammography , and imagenet ) . we find that cyclical weight transfer resulted in a performance ( testing accuracy = 77.3 % ) that was closest to that of centrally hosted patient data ( testing accuracy = 78.7 % ) . we also found that there is an improvement in the performance of cyclical weight transfer heuristic with high frequency of weight transfer .", "topics": ["computer vision", "simulation"]}
{"title": "manifold alignment determination : finding correspondences across different data views", "abstract": "we present manifold alignment determination ( mad ) , an algorithm for learning alignments between data points from multiple views or modalities . the approach is capable of learning correspondences between views as well as correspondences between individual data-points . the proposed method requires only a few aligned examples from which it is capable to recover a global alignment through a probabilistic model . the strong , yet flexible regularization provided by the generative model is sufficient to align the views . we provide experiments on both synthetic and real data to highlight the benefit of the proposed approach .", "topics": ["generative model", "synthetic data"]}
{"title": "network dissection : quantifying interpretability of deep visual representations", "abstract": "we propose a general framework called network dissection for quantifying the interpretability of latent representations of cnns by evaluating the alignment between individual hidden units and a set of semantic concepts . given any cnn model , the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer . the units with semantics are given labels across a range of objects , parts , scenes , textures , materials , and colors . we use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units , then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks . we further analyze the effect of training iterations , compare networks trained with different initializations , examine the impact of network depth and width , and measure the effect of dropout and batch normalization on the interpretability of deep visual representations . we demonstrate that the proposed method can shed light on characteristics of cnn models and training methods that go beyond measurements of their discriminative power .", "topics": ["iteration"]}
{"title": "causal discovery using proxy variables", "abstract": "discovering causal relations is fundamental to reasoning and intelligence . in particular , observational causal discovery algorithms estimate the cause-effect relation between two random entities $ x $ and $ y $ , given $ n $ samples from $ p ( x , y ) $ . in this paper , we develop a framework to estimate the cause-effect relation between two static entities $ x $ and $ y $ : for instance , an art masterpiece $ x $ and its fraudulent copy $ y $ . to this end , we introduce the notion of proxy variables , which allow the construction of a pair of random entities $ ( a , b ) $ from the pair of static entities $ ( x , y ) $ . then , estimating the cause-effect relation between $ a $ and $ b $ using an observational causal discovery algorithm leads to an estimation of the cause-effect relation between $ x $ and $ y $ . for example , our framework detects the causal relation between unprocessed photographs and their modifications , and orders in time a set of shuffled frames from a video . as our main case study , we introduce a human-elicited dataset of 10,000 pairs of casually-linked pairs of words from natural language . our methods discover 75 % of these causal relations . finally , we discuss the role of proxy variables in machine learning , as a general tool to incorporate static knowledge into prediction tasks .", "topics": ["entity", "causality"]}
{"title": "online learning under delayed feedback", "abstract": "online learning with delayed feedback has received increasing attention recently due to its several applications in distributed , web-based learning problems . in this paper we provide a systematic study of the topic , and analyze the effect of delay on the regret of online learning algorithms . somewhat surprisingly , it turns out that delay increases the regret in a multiplicative way in adversarial problems , and in an additive way in stochastic problems . we give meta-algorithms that transform , in a black-box fashion , algorithms developed for the non-delayed case into ones that can handle the presence of delays in the feedback loop . modifications of the well-known ucb algorithm are also developed for the bandit problem with delayed feedback , with the advantage over the meta-algorithms that they can be implemented with lower complexity .", "topics": ["regret ( decision theory )"]}
{"title": "the rapidly changing landscape of conversational agents", "abstract": "conversational agents have become ubiquitous , ranging from goal-oriented systems for helping with reservations to chit-chat models found in modern virtual assistants . in this survey paper , we explore this fascinating field . we look at some of the pioneering work that defined the field and gradually move to the current state-of-the-art models . we look at statistical , neural , generative adversarial network based and reinforcement learning based approaches and how they evolved . along the way we discuss various challenges that the field faces , lack of context in utterances , not having a good quantitative metric to compare models , lack of trust in agents because they do not have a consistent persona etc . we structure this paper in a way that answers these pertinent questions and discusses competing approaches to solve them .", "topics": ["reinforcement learning", "relevance"]}
{"title": "pattern-based context-free grammars for machine translation", "abstract": "this paper proposes the use of `` pattern-based '' context-free grammars as a basis for building machine translation ( mt ) systems , which are now being adopted as personal tools by a broad range of users in the cyberspace society . we discuss major requirements for such tools , including easy customization for diverse domains , the efficiency of the translation algorithm , and scalability ( incremental improvement in translation quality through user interaction ) , and describe how our approach meets these requirements .", "topics": ["machine translation", "scalability"]}
{"title": "using k-nearest neighbors to construct cancelable minutiae templates", "abstract": "fingerprint is widely used in a variety of applications . security measures have to be taken to protect the privacy of fingerprint data . cancelable biometrics is proposed as an effective mechanism of using and protecting biometrics . in this paper we propose a new method of constructing cancelable fingerprint template by combining real template with synthetic template . specifically , each user is given one synthetic minutia template generated with random number generator . every minutia point from the real template is individually thrown into the synthetic template , from which its k-nearest neighbors are found . the verification template is constructed by combining an arbitrary set of the k-nearest neighbors . to prove the validity of the scheme , testing is carried out on three databases . the results show that the constructed templates satisfy the requirements of cancelable biometrics .", "topics": ["synthetic data", "database"]}
{"title": "generalized nonconvex nonsmooth low-rank minimization", "abstract": "as surrogate functions of $ l_0 $ -norm , many nonconvex penalty functions have been proposed to enhance the sparse vector recovery . it is easy to extend these nonconvex penalty functions on singular values of a matrix to enhance low-rank matrix recovery . however , different from convex optimization , solving the nonconvex low-rank minimization problem is much more challenging than the nonconvex sparse minimization problem . we observe that all the existing nonconvex penalty functions are concave and monotonically increasing on $ [ 0 , \\infty ) $ . thus their gradients are decreasing functions . based on this property , we propose an iteratively reweighted nuclear norm ( irnn ) algorithm to solve the nonconvex nonsmooth low-rank minimization problem . irnn iteratively solves a weighted singular value thresholding ( wsvt ) problem . by setting the weight vector as the gradient of the concave penalty function , the wsvt problem has a closed form solution . in theory , we prove that irnn decreases the objective function value monotonically , and any limit point is a stationary point . extensive experiments on both synthetic data and real images demonstrate that irnn enhances the low-rank matrix recovery compared with state-of-the-art convex algorithms .", "topics": ["optimization problem", "loss function"]}
{"title": "differentially private empirical risk minimization revisited : faster and more general", "abstract": "in this paper we study the differentially private empirical risk minimization ( erm ) problem in different settings . for smooth ( strongly ) convex loss function with or without ( non ) -smooth regularization , we give algorithms that achieve either optimal or near optimal utility bounds with less gradient complexity compared with previous work . for erm with smooth convex loss function in high-dimensional ( $ p\\gg n $ ) setting , we give an algorithm which achieves the upper bound with less gradient complexity than previous ones . at last , we generalize the expected excess empirical risk from convex loss functions to non-convex ones satisfying the polyak-lojasiewicz condition and give a tighter upper bound on the utility than the one in \\cite { ijcai2017-548 } .", "topics": ["loss function", "matrix regularization"]}
{"title": "a metric on the space of finite sets of trajectories for evaluation of multi-target tracking algorithms", "abstract": "in this paper , we propose a metric on the space of finite sets of trajectories for assessing multi-target tracking algorithms in a mathematically sound way . the metric can be used , e.g . , to compare estimates from algorithms with the ground truth . it includes intuitive costs associated to localization , missed and false targets and track switches . the metric computation is based on multi-dimensional assignments , which is an np hard problem . therefore , we also propose a lower bound for the metric , which is also a metric for sets of trajectories and is computable in polynomial time using linear programming ( lp ) . the lp metric can be implemented using alternating direction method of multipliers such that the complexity scales linearly with the length of the trajectories .", "topics": ["time complexity", "computation"]}
{"title": "bandits meet computer architecture : designing a smartly-allocated cache", "abstract": "in many embedded systems , such as imaging sys- tems , the system has a single designated purpose , and same threads are executed repeatedly . profiling thread behavior , allows the system to allocate each thread its resources in a way that improves overall system performance . we study an online resource al- locationproblem , wherearesourcemanagersimulta- neously allocates resources ( exploration ) , learns the impact on the different consumers ( learning ) and im- proves allocation towards optimal performance ( ex- ploitation ) . we build on the rich framework of multi- armed bandits and present online and offline algo- rithms . through extensive experiments with both synthetic data and real-world cache allocation to threads we show the merits and properties of our al- gorithms", "topics": ["synthetic data"]}
{"title": "nested junction trees", "abstract": "the efficiency of inference in both the hugin and , most notably , the shafer-shenoy architectures can be improved by exploiting the independence relations induced by the incoming messages of a clique . that is , the message to be sent from a clique can be computed via a factorization of the clique potential in the form of a junction tree . in this paper we show that by exploiting such nested junction trees in the computation of messages both space and time costs of the conventional propagation methods may be reduced . the paper presents a structured way of exploiting the nested junction trees technique to achieve such reductions . the usefulness of the method is emphasized through a thorough empirical evaluation involving ten large real-world bayesian networks and the hugin inference algorithm .", "topics": ["computation", "bayesian network"]}
{"title": "raiders of the lost architecture : kernels for bayesian optimization in conditional parameter spaces", "abstract": "in practical bayesian optimization , we must often search over structures with differing numbers of parameters . for instance , we may wish to search over neural network architectures with an unknown number of layers . to relate performance data gathered for different architectures , we define a new kernel for conditional parameter spaces that explicitly includes information about which parameters are relevant in a given structure . we show that this kernel improves model quality and bayesian optimization results over several simpler baseline kernels .", "topics": ["kernel ( operating system )", "baseline ( configuration management )"]}
{"title": "from logical to distributional models", "abstract": "the paper relates two variants of semantic models for natural language , logical functional models and compositional distributional vector space models , by transferring the logic and reasoning from the logical to the distributional models . the geometrical operations of quantum logic are reformulated as algebraic operations on vectors . a map from functional models to vector space models makes it possible to compare the meaning of sentences word by word .", "topics": ["natural language"]}
{"title": "image retrieval based on binary signature ang s-kgraph", "abstract": "in this paper , we introduce an optimum approach for querying similar images on large digital-image databases . our work is based on rbir ( region-based image retrieval ) method which uses multiple regions as the key to retrieval images . this method significantly improves the accuracy of queries . however , this also increases the cost of computing . to reduce this expensive computational cost , we implement binary signature encoder which maps an image to its identification in binary . in order to fasten the lookup , binary signatures of images are classified by the help of s-kgraph . finally , our work is evaluated on corel 's images .", "topics": ["map", "database"]}
{"title": "identifying metaphor hierarchies in a corpus analysis of finance articles", "abstract": "using a corpus of over 17,000 financial news reports ( involving over 10m words ) , we perform an analysis of the argument-distributions of the up- and down-verbs used to describe movements of indices , stocks , and shares . using measures of the overlap in the argument distributions of these verbs and k-means clustering of their distributions , we advance evidence for the proposal that the metaphors referred to by these verbs are organised into hierarchical structures of superordinate and subordinate groups .", "topics": ["cluster analysis", "text corpus"]}
{"title": "predicting the behavior of interacting humans by fusing data from multiple sources", "abstract": "multi-fidelity methods combine inexpensive low-fidelity simulations with costly but high-fidelity simulations to produce an accurate model of a system of interest at minimal cost . they have proven useful in modeling physical systems and have been applied to engineering problems such as wing-design optimization . during human-in-the-loop experimentation , it has become increasingly common to use online platforms , like mechanical turk , to run low-fidelity experiments to gather human performance data in an efficient manner . one concern with these experiments is that the results obtained from the online environment generalize poorly to the actual domain of interest . to address this limitation , we extend traditional multi-fidelity approaches to allow us to combine fewer data points from high-fidelity human-in-the-loop experiments with plentiful but less accurate data from low-fidelity experiments to produce accurate models of how humans interact . we present both model-based and model-free methods , and summarize the predictive performance of each method under different conditions .", "topics": ["simulation"]}
{"title": "dictionary learning from incomplete data", "abstract": "this paper extends the recently proposed and theoretically justified iterative thresholding and $ k $ residual means algorithm itkrm to learning dicionaries from incomplete/masked training data ( itkrmm ) . it further adapts the algorithm to the presence of a low rank component in the data and provides a strategy for recovering this low rank component again from incomplete data . several synthetic experiments show the advantages of incorporating information about the corruption into the algorithm . finally , image inpainting is considered as application example , which demonstrates the superior performance of itkrmm in terms of speed at similar or better reconstruction quality compared to its closest dictionary learning counterpart .", "topics": ["dictionary"]}
{"title": "scalable bayesian modelling of paired symbols", "abstract": "we present a novel , scalable and bayesian approach to modelling the occurrence of pairs of symbols ( i , j ) drawn from a large vocabulary . observed pairs are assumed to be generated by a simple popularity based selection process followed by censoring using a preference function . by basing inference on the well-founded principle of variational bounding , and using new site-independent bounds , we show how a scalable inference procedure can be obtained for large data sets . state of the art results are presented on real-world movie viewing data .", "topics": ["calculus of variations", "scalability"]}
{"title": "accelerating competitive learning graph quantization", "abstract": "vector quantization ( vq ) is a lossy data compression technique from signal processing for which simple competitive learning is one standard method to quantize patterns from the input space . extending competitive learning vq to the domain of graphs results in competitive learning for quantizing input graphs . in this contribution , we propose an accelerated version of competitive learning graph quantization ( gq ) without trading computational time against solution quality . for this , we lift graphs locally to vectors in order to avoid unnecessary calculations of intractable graph distances . in doing so , the accelerated version of competitive learning gq gradually turns locally into a competitive learning vq with increasing number of iterations . empirical results show a significant speedup by maintaining a comparable solution quality .", "topics": ["iteration"]}
{"title": "fast end-to-end trainable guided filter", "abstract": "image processing and pixel-wise dense prediction have been advanced by harnessing the capabilities of deep learning . one central issue of deep learning is the limited capacity to handle joint upsampling . we present a deep learning building block for joint upsampling , namely guided filtering layer . this layer aims at efficiently generating the high-resolution output given the corresponding low-resolution one and a high-resolution guidance map . the proposed layer is composed of a guided filter , which is reformulated as a fully differentiable block . to this end , we show that a guided filter can be expressed as a group of spatial varying linear transformation matrices . this layer could be integrated with the convolutional neural networks ( cnns ) and jointly optimized through end-to-end training . to further take advantage of end-to-end training , we plug in a trainable transformation function that generates task-specific guidance maps . by integrating the cnns and the proposed layer , we form deep guided filtering networks . the proposed networks are evaluated on five advanced image processing tasks . experiments on mit-adobe fivek dataset demonstrate that the proposed approach runs 10-100 times faster and achieves the state-of-the-art performance . we also show that the proposed guided filtering layer helps to improve the performance of multiple pixel-wise dense prediction tasks . the code is available at https : //github.com/wuhuikai/deepguidedfilter .", "topics": ["image processing", "map"]}
{"title": "efficient multi-person pose estimation with provable guarantees", "abstract": "multi-person pose estimation ( mppe ) in natural images is key to the meaningful use of visual data in many fields including movement science , security , and rehabilitation . in this paper we tackle mppe with a bottom-up approach , starting with candidate detections of body parts from a convolutional neural network ( cnn ) and grouping them into people . we formulate the grouping of body part detections into people as a minimum-weight set packing ( mwsp ) problem where the set of potential people is the power set of body part detections . we model the quality of a hypothesis of a person which is a set in the mwsp by an augmented tree-structured markov random field where variables correspond to body-parts and their state-spaces correspond to the power set of the detections for that part . we describe a novel algorithm that combines efficiency with provable bounds on this mwsp problem . we employ an implicit column generation strategy where the pricing problem is formulated as a dynamic program . to efficiently solve this dynamic program we exploit the problem structure utilizing a nested bender 's decomposition ( nbd ) exact inference strategy which we speed up by recycling bender 's rows between calls to the pricing problem . we test our approach on the mpii-multiperson dataset , showing that our approach obtains comparable results with the state-of-the-art algorithm for joint node labeling and grouping problems , and that nbd achieves considerable speed-ups relative to a naive dynamic programming approach . typical algorithms that solve joint node labeling and grouping problems use heuristics and thus can not obtain proofs of optimality . our approach , in contrast , proves that for over 99 percent of problem instances we find the globally optimal solution and otherwise provide upper/lower bounds .", "topics": ["optimization problem", "heuristic"]}
{"title": "high efficiency compression for object detection", "abstract": "image and video compression has traditionally been tailored to human vision . however , modern applications such as visual analytics and surveillance rely on computers seeing and analyzing the images before ( or instead of ) humans . for these applications , it is important to adjust compression to computer vision . in this paper we present a bit allocation and rate control strategy that is tailored to object detection . using the initial convolutional layers of a state-of-the-art object detector , we create an importance map that can guide bit allocation to areas that are important for object detection . the proposed method enables bit rate savings of 7 % or more compared to default hevc , at the equivalent object detection rate .", "topics": ["object detection", "computer vision"]}
{"title": "using echo state networks for cryptography", "abstract": "echo state networks are simple recurrent neural networks that are easy to implement and train . despite their simplicity , they show a form of memory and can predict or regenerate sequences of data . we make use of this property to realize a novel neural cryptography scheme . the key idea is to assume that alice and bob share a copy of an echo state network . if alice trains her copy to memorize a message , she can communicate the trained part of the network to bob who plugs it into his copy to regenerate the message . considering a byte-level representation of in- and output , the technique applies to arbitrary types of data ( texts , images , audio files , etc . ) and practical experiments reveal it to satisfy the fundamental cryptographic properties of diffusion and confusion .", "topics": ["recurrent neural network"]}
{"title": "deep manifold-to-manifold transforming network for action recognition", "abstract": "symmetric positive definite ( spd ) matrices ( e.g . , covariances , graph laplacians , etc . ) are widely used to model the relationship of spatial or temporal domain . nevertheless , spd matrices are theoretically embedded on riemannian manifolds . in this paper , we propose an end-to-end deep manifold-to-manifold transforming network ( dmt-net ) which can make spd matrices flow from one riemannian manifold to another more discriminative one . to learn discriminative spd features characterizing both spatial and temporal dependencies , we specifically develop three novel layers on manifolds : ( i ) the local spd convolutional layer , ( ii ) the non-linear spd activation layer , and ( iii ) the riemannian-preserved recursive layer . the spd property is preserved through all layers without any requirement of singular value decomposition ( svd ) , which is often used in the existing methods with expensive computation cost . furthermore , a diagonalizing spd layer is designed to efficiently calculate the final metric for the classification task . to evaluate our proposed method , we conduct extensive experiments on the task of action recognition , where input signals are popularly modeled as spd matrices . the experimental results demonstrate that our dmt-net is much more competitive over state-of-the-art .", "topics": ["computation", "convolution"]}
{"title": "using sentence-level lstm language models for script inference", "abstract": "there is a small but growing body of research on statistical scripts , models of event sequences that allow probabilistic inference of implicit events from documents . these systems operate on structured verb-argument events produced by an nlp pipeline . we compare these systems with recent recurrent neural net models that directly operate on raw tokens to predict sentences , finding the latter to be roughly comparable to the former in terms of predicting missing events in documents .", "topics": ["natural language processing"]}
{"title": "automatic identification of altlexes using monolingual parallel corpora", "abstract": "the automatic identification of discourse relations is still a challenging task in natural language processing . discourse connectives , such as `` since '' or `` but '' , are the most informative cues to identify explicit relations ; however discourse parsers typically use a closed inventory of such connectives . as a result , discourse relations signaled by markers outside these inventories ( i.e . altlexes ) are not detected as effectively . in this paper , we propose a novel method to leverage parallel corpora in text simplification and lexical resources to automatically identify alternative lexicalizations that signal discourse relation . when applied to the simple wikipedia and newsela corpora along with wordnet and the ppdb , the method allowed the automatic discovery of 91 altlexes .", "topics": ["natural language processing", "text corpus"]}
{"title": "bandits with delayed , aggregated anonymous feedback", "abstract": "we study a variant of the stochastic $ k $ -armed bandit problem , which we call `` bandits with delayed , aggregated anonymous feedback '' . in this problem , when the player pulls an arm , a reward is generated , however it is not immediately observed . instead , at the end of each round the player observes only the sum of a number of previously generated rewards which happen to arrive in the given round . the rewards are stochastically delayed and due to the aggregated nature of the observations , the information of which arm led to a particular reward is lost . the question is what is the cost of the information loss due to this delayed , aggregated anonymous feedback ? previous works have studied bandits with stochastic , non-anonymous delays and found that the regret increases only by an additive factor relating to the expected delay . in this paper , we show that this additive regret increase can be maintained in the harder delayed , aggregated anonymous feedback setting when the expected delay ( or a bound on it ) is known . we provide an algorithm that matches the worst case regret of the non-anonymous problem exactly when the delays are bounded , and up to logarithmic factors or an additive variance term , for unbounded delays .", "topics": ["regret ( decision theory )"]}
{"title": "pan-tilt camera and pir sensor fusion based moving object detection for mobile security robots", "abstract": "one of fundamental issues for security robots is to detect and track people in the surroundings . the main problems of this task are real-time constraints , a changing background , varying illumination conditions and a non-rigid shape of the person to be tracked . in this paper , we propose a solution for tracking with a pan-tilt camera and a passive infrared range ( pir ) sensor to detect the moving object based on consecutive frame difference . the proposed method is excellent in real-time performance because it requires only a little memory and computation . experiment results show that this method can detect the moving object such as human efficiently and accurately in non-stationary and complex indoor environment .", "topics": ["object detection", "computation"]}
{"title": "particle metropolis-hastings using gradient and hessian information", "abstract": "particle metropolis-hastings ( pmh ) allows for bayesian parameter inference in nonlinear state space models by combining markov chain monte carlo ( mcmc ) and particle filtering . the latter is used to estimate the intractable likelihood . in its original formulation , pmh makes use of a marginal mcmc proposal for the parameters , typically a gaussian random walk . however , this can lead to a poor exploration of the parameter space and an inefficient use of the generated particles . we propose a number of alternative versions of pmh that incorporate gradient and hessian information about the posterior into the proposal . this information is more or less obtained as a byproduct of the likelihood estimation . indeed , we show how to estimate the required information using a fixed-lag particle smoother , with a computational cost growing linearly in the number of particles . we conclude that the proposed methods can : ( i ) decrease the length of the burn-in phase , ( ii ) increase the mixing of the markov chain at the stationary phase , and ( iii ) make the proposal distribution scale invariant which simplifies tuning .", "topics": ["nonlinear system", "gradient"]}
{"title": "making early predictions of the accuracy of machine learning applications", "abstract": "the accuracy of machine learning systems is a widely studied research topic . established techniques such as cross-validation predict the accuracy on unseen data of the classifier produced by applying a given learning method to a given training data set . however , they do not predict whether incurring the cost of obtaining more data and undergoing further training will lead to higher accuracy . in this paper we investigate techniques for making such early predictions . we note that when a machine learning algorithm is presented with a training set the classifier produced , and hence its error , will depend on the characteristics of the algorithm , on training set 's size , and also on its specific composition . in particular we hypothesise that if a number of classifiers are produced , and their observed error is decomposed into bias and variance terms , then although these components may behave differently , their behaviour may be predictable . we test our hypothesis by building models that , given a measurement taken from the classifier created from a limited number of samples , predict the values that would be measured from the classifier produced when the full data set is presented . we create separate models for bias , variance and total error . our models are built from the results of applying ten different machine learning algorithms to a range of data sets , and tested with `` unseen '' algorithms and datasets . we analyse the results for various numbers of initial training samples , and total dataset sizes . results show that our predictions are very highly correlated with the values observed after undertaking the extra training . finally we consider the more complex case where an ensemble of heterogeneous classifiers is trained , and show how we can accurately estimate an upper bound on the accuracy achievable after further training .", "topics": ["test set", "statistical classification"]}
{"title": "embedding projector : interactive visualization and interpretation of embeddings", "abstract": "embeddings are ubiquitous in machine learning , appearing in recommender systems , nlp , and many other applications . researchers and developers often need to explore the properties of a specific embedding , and one way to analyze embeddings is to visualize them . we present the embedding projector , a tool for interactive visualization and interpretation of embeddings .", "topics": ["natural language processing"]}
{"title": "3d shape retrieval via irrelevance filtering and similarity ranking ( if/sr )", "abstract": "a novel solution for the content-based 3d shape retrieval problem using an unsupervised clustering approach , which does not need any label information of 3d shapes , is presented in this work . the proposed shape retrieval system consists of two modules in cascade : the irrelevance filtering ( if ) module and the similarity ranking ( sr ) module . the if module attempts to cluster gallery shapes that are similar to each other by examining global and local features simultaneously . however , shapes that are close in the local feature space can be distant in the global feature space , and vice versa . to resolve this issue , we propose a joint cost function that strikes a balance between two distances . irrelevant samples that are close in the local feature space but distant in the global feature space can be removed in this stage . the remaining gallery samples are ranked in the sr module using the local feature . the superior performance of the proposed if/sr method is demonstrated by extensive experiments conducted on the popular shrec12 dataset .", "topics": ["feature vector", "cluster analysis"]}
{"title": "topic modeling using distributed word embeddings", "abstract": "we propose a new algorithm for topic modeling , vec2topic , that identifies the main topics in a corpus using semantic information captured via high-dimensional distributed word embeddings . our technique is unsupervised and generates a list of topics ranked with respect to importance . we find that it works better than existing topic modeling techniques such as latent dirichlet allocation for identifying key topics in user-generated content , such as emails , chats , etc . , where topics are diffused across the corpus . we also find that vec2topic works equally well for non-user generated content , such as papers , reports , etc . , and for small corpora such as a single-document .", "topics": ["unsupervised learning", "text corpus"]}
{"title": "a fully data-driven method to identify ( correlated ) changes in diachronic corpora", "abstract": "in this paper , a method for measuring synchronic corpus ( dis- ) similarity put forward by kilgarriff ( 2001 ) is adapted and extended to identify trends and correlated changes in diachronic text data , using the corpus of historical american english ( davies 2010a ) and the google ngram corpora ( michel et al . 2010a ) . this paper shows that this fully data-driven method , which extracts word types that have undergone the most pronounced change in frequency in a given period of time , is computationally very cheap and that it allows interpretations of diachronic trends that are both intuitively plausible and motivated from the perspective of information theory . furthermore , it demonstrates that the method is able to identify correlated linguistic changes and diachronic shifts that can be linked to historical events . finally , it can help to improve diachronic pos tagging and complement existing nlp approaches . this indicates that the approach can facilitate an improved understanding of diachronic processes in language change .", "topics": ["natural language processing", "text corpus"]}
{"title": "on the theory of variance reduction for stochastic gradient monte carlo", "abstract": "we provide convergence guarantees in wasserstein distance for a variety of variance-reduction methods : saga langevin diffusion , svrg langevin diffusion and control-variate underdamped langevin diffusion . we analyze these methods under a uniform set of assumptions on the log-posterior distribution , assuming it to be smooth , strongly convex and hessian lipschitz . this is achieved by a new proof technique combining ideas from finite-sum optimization and the analysis of sampling methods . our sharp theoretical bounds allow us to identify regimes of interest where each method performs better than the others . our theory is verified with experiments on real-world and synthetic datasets .", "topics": ["sampling ( signal processing )", "synthetic data"]}
{"title": "k-plane regression", "abstract": "in this paper , we present a novel algorithm for piecewise linear regression which can learn continuous as well as discontinuous piecewise linear functions . the main idea is to repeatedly partition the data and learn a liner model in in each partition . while a simple algorithm incorporating this idea does not work well , an interesting modification results in a good algorithm . the proposed algorithm is similar in spirit to $ k $ -means clustering algorithm . we show that our algorithm can also be viewed as an em algorithm for maximum likelihood estimation of parameters under a reasonable probability model . we empirically demonstrate the effectiveness of our approach by comparing its performance with the state of art regression learning algorithms on some real world datasets .", "topics": ["cluster analysis"]}
{"title": "conjugate-computation variational inference : converting variational inference in non-conjugate models to inferences in conjugate models", "abstract": "variational inference is computationally challenging in models that contain both conjugate and non-conjugate terms . methods specifically designed for conjugate models , even though computationally efficient , find it difficult to deal with non-conjugate terms . on the other hand , stochastic-gradient methods can handle the non-conjugate terms but they usually ignore the conjugate structure of the model which might result in slow convergence . in this paper , we propose a new algorithm called conjugate-computation variational inference ( cvi ) which brings the best of the two worlds together -- it uses conjugate computations for the conjugate terms and employs stochastic gradients for the rest . we derive this algorithm by using a stochastic mirror-descent method in the mean-parameter space , and then expressing each gradient step as a variational inference in a conjugate model . we demonstrate our algorithm 's applicability to a large class of models and establish its convergence . our experimental results show that our method converges much faster than the methods that ignore the conjugate structure of the model .", "topics": ["calculus of variations", "computational complexity theory"]}
{"title": "a two-stage architecture for stock price forecasting by combining som and fuzzy-svm", "abstract": "this paper proposed a model to predict the stock price based on combining self-organizing map ( som ) and fuzzy-support vector machines ( f-svm ) . extraction of fuzzy rules from raw data based on the combining of statistical machine learning models is base of this proposed approach . in the proposed model , som is used as a clustering algorithm to partition the whole input space into the several disjoint regions . for each partition , a set of fuzzy rules is extracted based on a f-svm combining model . then fuzzy rules sets are used to predict the test data using fuzzy inference algorithms . the performance of the proposed approach is compared with other models using four data sets", "topics": ["cluster analysis", "support vector machine"]}
{"title": "message passing for quantified boolean formulas", "abstract": "we introduce two types of message passing algorithms for quantified boolean formulas ( qbf ) . the first type is a message passing based heuristics that can prove unsatisfiability of the qbf by assigning the universal variables in such a way that the remaining formula is unsatisfiable . in the second type , we use message passing to guide branching heuristics of a davis-putnam logemann-loveland ( dpll ) complete solver . numerical experiments show that on random qbfs our branching heuristics gives robust exponential efficiency gain with respect to the state-of-art solvers . we also manage to solve some previously unsolved benchmarks from the qbflib library . apart from this our study sheds light on using message passing in small systems and as subroutines in complete solvers .", "topics": ["time complexity", "numerical analysis"]}
{"title": "enter the matrix : a virtual world approach to safely interruptable autonomous systems", "abstract": "robots and autonomous systems that operate around humans will likely always rely on kill switches that stop their execution and allow them to be remote-controlled for the safety of humans or to prevent damage to the system . it is theoretically possible for an autonomous system with sufficient sensor and effector capability and using reinforcement learning to learn that the kill switch deprives it of long-term reward and learn to act to disable the switch or otherwise prevent a human operator from using the switch . this is referred to as the big red button problem . we present a technique which prevents a reinforcement learning agent from learning to disable the big red button . our technique interrupts the agent or robot by placing it in a virtual simulation where it continues to receive reward . we illustrate our technique in a simple grid world environment .", "topics": ["reinforcement learning", "simulation"]}
{"title": "evolving latent space model for dynamic networks", "abstract": "networks observed in the real world like social networks , collaboration networks etc . , exhibit temporal dynamics , i.e . nodes and edges appear and/or disappear over time . in this paper , we propose a generative , latent space based , statistical model for such networks ( called dynamic networks ) . we consider the case where the number of nodes is fixed , but the presence of edges can vary over time . our model allows the number of communities in the network to be different at different time steps . we use a neural network based methodology to perform approximate inference in the proposed model and its simplified version . experiments done on synthetic and real-world networks for the task of community detection and link prediction demonstrate the utility and effectiveness of our model as compared to other similar existing approaches . to the best of our knowledge , this is the first work that integrates statistical modeling of dynamic networks with deep learning for community detection and link prediction .", "topics": ["approximation algorithm", "synthetic data"]}
{"title": "multistage hybrid arabic/indian numeral ocr system", "abstract": "the use of ocr in postal services is not yet universal and there are still many countries that process mail sorting manually . automated arabic/indian numeral optical character recognition ( ocr ) systems for postal services are being used in some countries , but still there are errors during the mail sorting process , thus causing a reduction in efficiency . the need to investigate fast and efficient recognition algorithms/systems is important so as to correctly read the postal codes from mail addresses and to eliminate any errors during the mail sorting stage . the objective of this study is to recognize printed numerical postal codes from mail addresses . the proposed system is a multistage hybrid system which consists of three different feature extraction methods , i.e . , binary , zoning , and fuzzy features , and three different classifiers , i.e . , hamming nets , euclidean distance , and fuzzy neural network classifiers . the proposed system , systematically compares the performance of each of these methods , and ensures that the numerals are recognized correctly . comprehensive results provide a very high recognition rate , outperforming the other known developed methods in literature .", "topics": ["feature extraction", "numerical analysis"]}
{"title": "semi-supervised cross-entropy clustering with information bottleneck constraint", "abstract": "in this paper , we propose a semi-supervised clustering method , cec-ib , that models data with a set of gaussian distributions and that retrieves clusters based on a partial labeling provided by the user ( partition-level side information ) . by combining the ideas from cross-entropy clustering ( cec ) with those from the information bottleneck method ( ib ) , our method trades between three conflicting goals : the accuracy with which the data set is modeled , the simplicity of the model , and the consistency of the clustering with side information . experiments demonstrate that cec-ib has a performance comparable to gaussian mixture models ( gmm ) in a classical semi-supervised scenario , but is faster , more robust to noisy labels , automatically determines the optimal number of clusters , and performs well when not all classes are present in the side information . moreover , in contrast to other semi-supervised models , it can be successfully applied in discovering natural subgroups if the partition-level side information is derived from the top levels of a hierarchical clustering .", "topics": ["cluster analysis"]}
{"title": "effective listings of function stop words for twitter", "abstract": "many words in documents recur very frequently but are essentially meaningless as they are used to join words together in a sentence . it is commonly understood that stop words do not contribute to the context or content of textual documents . due to their high frequency of occurrence , their presence in text mining presents an obstacle to the understanding of the content in the documents . to eliminate the bias effects , most text mining software or approaches make use of stop words list to identify and remove those words . however , the development of such top words list is difficult and inconsistent between textual sources . this problem is further aggravated by sources such as twitter which are highly repetitive or similar in nature . in this paper , we will be examining the original work using term frequency , inverse document frequency and term adjacency for developing a stop words list for the twitter data source . we propose a new technique using combinatorial values as an alternative measure to effectively list out stop words .", "topics": ["value ( ethics )"]}
{"title": "learning via social awareness : improving sketch representations with facial feedback", "abstract": "in the quest towards general artificial intelligence ( ai ) , researchers have explored developing loss functions that act as intrinsic motivators in the absence of external rewards . this paper argues that such research has overlooked an important and useful intrinsic motivator : social interaction . we posit that making an ai agent aware of implicit social feedback from humans can allow for faster learning of more generalizable and useful representations , and could potentially impact ai safety . we collect social feedback in the form of facial expression reactions to samples from sketch rnn , an lstm-based variational autoencoder ( vae ) designed to produce sketch drawings . we use a latent constraints gan ( lc-gan ) to learn from the facial feedback of a small group of viewers , and then show in an independent evaluation with 76 users that this model produced sketches that lead to significantly more positive facial expressions . thus , we establish that implicit social feedback can improve the output of a deep learning model .", "topics": ["loss function", "artificial intelligence"]}
{"title": "a corpus-based investigation of definite description use", "abstract": "we present the results of a study of definite descriptions use in written texts aimed at assessing the feasibility of annotating corpora with information about definite description interpretation . we ran two experiments , in which subjects were asked to classify the uses of definite descriptions in a corpus of 33 newspaper articles , containing a total of 1412 definite descriptions . we measured the agreement among annotators about the classes assigned to definite descriptions , as well as the agreement about the antecedent assigned to those definites that the annotators classified as being related to an antecedent in the text . the most interesting result of this study from a corpus annotation perspective was the rather low agreement ( k=0.63 ) that we obtained using versions of hawkins ' and prince 's classification schemes ; better results ( k=0.76 ) were obtained using the simplified scheme proposed by fraurud that includes only two classes , first-mention and subsequent-mention . the agreement about antecedents was also not complete . these findings raise questions concerning the strategy of evaluating systems for definite description interpretation by comparing their results with a standardized annotation . from a linguistic point of view , the most interesting observations were the great number of discourse-new definites in our corpus ( in one of our experiments , about 50 % of the definites in the collection were classified as discourse-new , 30 % as anaphoric , and 18 % as associative/bridging ) and the presence of definites which did not seem to require a complete disambiguation .", "topics": ["text corpus"]}
{"title": "neural text generation : a practical guide", "abstract": "deep learning methods have recently achieved great empirical success on machine translation , dialogue response generation , summarization , and other text generation tasks . at a high level , the technique has been to train end-to-end neural network models consisting of an encoder model to produce a hidden representation of the source text , followed by a decoder model to generate the target . while such models have significantly fewer pieces than earlier systems , significant tuning is still required to achieve good performance . for text generation models in particular , the decoder can behave in undesired ways , such as by generating truncated or repetitive outputs , outputting bland and generic responses , or in some cases producing ungrammatical gibberish . this paper is intended as a practical guide for resolving such undesired behavior in text generation models , with the aim of helping enable real-world applications .", "topics": ["machine translation", "end-to-end principle"]}
{"title": "cm-gans : cross-modal generative adversarial networks for common representation learning", "abstract": "it is known that the inconsistent distribution and representation of different modalities , such as image and text , cause the heterogeneity gap that makes it challenging to correlate such heterogeneous data . generative adversarial networks ( gans ) have shown its strong ability of modeling data distribution and learning discriminative representation , existing gans-based works mainly focus on generative problem to generate new data . we have different goal , aim to correlate heterogeneous data , by utilizing the power of gans to model cross-modal joint distribution . thus , we propose cross-modal gans to learn discriminative common representation for bridging heterogeneity gap . the main contributions are : ( 1 ) cross-modal gans architecture is proposed to model joint distribution over data of different modalities . the inter-modality and intra-modality correlation can be explored simultaneously in generative and discriminative models . both of them beat each other to promote cross-modal correlation learning . ( 2 ) cross-modal convolutional autoencoders with weight-sharing constraint are proposed to form generative model . they can not only exploit cross-modal correlation for learning common representation , but also preserve reconstruction information for capturing semantic consistency within each modality . ( 3 ) cross-modal adversarial mechanism is proposed , which utilizes two kinds of discriminative models to simultaneously conduct intra-modality and inter-modality discrimination . they can mutually boost to make common representation more discriminative by adversarial training process . to the best of our knowledge , our proposed cm-gans approach is the first to utilize gans to perform cross-modal common representation learning . experiments are conducted to verify the performance of our proposed approach on cross-modal retrieval paradigm , compared with 10 methods on 3 cross-modal datasets .", "topics": ["generative model", "feature learning"]}
{"title": "freeze-thaw bayesian optimization", "abstract": "in this paper we develop a dynamic form of bayesian optimization for machine learning models with the goal of rapidly finding good hyperparameter settings . our method uses the partial information gained during the training of a machine learning model in order to decide whether to pause training and start a new model , or resume the training of a previously-considered model . we specifically tailor our method to machine learning problems by developing a novel positive-definite covariance kernel to capture a variety of training curves . furthermore , we develop a gaussian process prior that scales gracefully with additional temporal observations . finally , we provide an information-theoretic framework to automate the decision process . experiments on several common machine learning models show that our approach is extremely effective in practice .", "topics": ["test set", "optimization problem"]}
{"title": "subword and crossword units for ctc acoustic models", "abstract": "this paper proposes a novel approach to create an unit set for ctc based speech recognition systems . by using byte pair encoding we learn an unit set of an arbitrary size on a given training text . in contrast to using characters or words as units this allows us to find a good trade-off between the size of our unit set and the available training data . we evaluate both crossword units , that may span multiple word , and subword units . by combining this approach with decoding methods using a separate language model we are able to achieve state of the art results for grapheme based ctc systems .", "topics": ["test set", "speech recognition"]}
{"title": "feature pyramid networks for object detection", "abstract": "feature pyramids are a basic component in recognition systems for detecting objects at different scales . but recent deep learning object detectors have avoided pyramid representations , in part because they are compute and memory intensive . in this paper , we exploit the inherent multi-scale , pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost . a top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales . this architecture , called a feature pyramid network ( fpn ) , shows significant improvement as a generic feature extractor in several applications . using fpn in a basic faster r-cnn system , our method achieves state-of-the-art single-model results on the coco detection benchmark without bells and whistles , surpassing all existing single-model entries including those from the coco 2016 challenge winners . in addition , our method can run at 5 fps on a gpu and thus is a practical and accurate solution to multi-scale object detection . code will be made publicly available .", "topics": ["high- and low-level", "object detection"]}
{"title": "accurate automatic segmentation of retina layers with emphasis on first layer", "abstract": "quantification of intra-retinal boundaries in optical coherence tomography ( oct ) is a crucial task for studying and diagnosing neurological and ocular diseases . since manual segmentation of layers is usually a time consuming task and relay on user , a lot of attempts done to do it automatically and without interference of user . although for extracting all layers usually same procedure is applied but finding the first layer is usually more difficult due to vanishing it in some region specially close to fobia . to have a general software , beside using common methods like applying shortest path algorithm on global gradient of image , some extra steps are used here to confine search area for dijstra algorithm especially for the second layer . results demonstrates high accuracy in segmenting all present layers , especially the first one that is important for diagnosing issue .", "topics": ["gradient descent", "gradient"]}
{"title": "hydraplus-net : attentive deep features for pedestrian analysis", "abstract": "pedestrian analysis plays a vital role in intelligent video surveillance and is a key component for security-centric computer vision systems . despite that the convolutional neural networks are remarkable in learning discriminative features from images , the learning of comprehensive features of pedestrians for fine-grained tasks remains an open problem . in this study , we propose a new attention-based deep neural network , named as hydraplus-net ( hp-net ) , that multi-directionally feeds the multi-level attention maps to different feature layers . the attentive deep features learned from the proposed hp-net bring unique advantages : ( 1 ) the model is capable of capturing multiple attentions from low-level to semantic-level , and ( 2 ) it explores the multi-scale selectiveness of attentive features to enrich the final feature representations for a pedestrian image . we demonstrate the effectiveness and generality of the proposed hp-net for pedestrian analysis on two tasks , i.e . pedestrian attribute recognition and person re-identification . intensive experimental results have been provided to prove that the hp-net outperforms the state-of-the-art methods on various datasets .", "topics": ["high- and low-level", "computer vision"]}
{"title": "entropy and syntropy in the context of five-valued logics", "abstract": "this paper presents a five-valued representation of bifuzzy sets . this representation is related to a five-valued logic that uses the following values : true , false , inconsistent , incomplete and ambiguous . in the framework of five-valued representation , formulae for similarity , entropy and syntropy of bifuzzy sets are constructed .", "topics": ["value ( ethics )"]}
{"title": "multimedia semantic integrity assessment using joint embedding of images and text", "abstract": "real world multimedia data is often composed of multiple modalities such as an image or a video with associated text ( e.g . captions , user comments , etc . ) and metadata . such multimodal data packages are prone to manipulations , where a subset of these modalities can be altered to misrepresent or repurpose data packages , with possible malicious intent . it is , therefore , important to develop methods to assess or verify the integrity of these multimedia packages . using computer vision and natural language processing methods to directly compare the image ( or video ) and the associated caption to verify the integrity of a media package is only possible for a limited set of objects and scenes . in this paper , we present a novel deep learning-based approach for assessing the semantic integrity of multimedia packages containing images and captions , using a reference set of multimedia packages . we construct a joint embedding of images and captions with deep multimodal representation learning on the reference dataset in a framework that also provides image-caption consistency scores ( iccss ) . the integrity of query media packages is assessed as the inlierness of the query iccss with respect to the reference dataset . we present the multimodal information manipulation dataset ( maim ) , a new dataset of media packages from flickr , which we make available to the research community . we use both the newly created dataset as well as flickr30k and ms coco datasets to quantitatively evaluate our proposed approach . the reference dataset does not contain unmanipulated versions of tampered query packages . our method is able to achieve f1 scores of 0.75 , 0.89 and 0.94 on maim , flickr30k and ms coco , respectively , for detecting semantically incoherent media packages .", "topics": ["feature learning", "natural language processing"]}
{"title": "event representations with tensor-based compositions", "abstract": "robust and flexible event representations are important to many core areas in language understanding . scripts were proposed early on as a way of representing sequences of events for such understanding , and has recently attracted renewed attention . however , obtaining effective representations for modeling script-like event sequences is challenging . it requires representations that can capture event-level and scenario-level semantics . we propose a new tensor-based composition method for creating event representations . the method captures more subtle semantic interactions between an event and its entities and yields representations that are effective at multiple event-related tasks . with the continuous representations , we also devise a simple schema generation method which produces better schemas compared to a prior discrete representation based method . our analysis shows that the tensors capture distinct usages of a predicate even when there are only subtle differences in their surface realizations .", "topics": ["interaction", "entity"]}
{"title": "clustering based approach extracting collocations", "abstract": "the following study presents a collocation extraction approach based on clustering technique . this study uses a combination of several classical measures which cover all aspects of a given corpus then it suggests separating bigrams found in the corpus in several disjoint groups according to the probability of presence of collocations . this will allow excluding groups where the presence of collocations is very unlikely and thus reducing in a meaningful way the search space .", "topics": ["cluster analysis", "natural language processing"]}
{"title": "semi automatic color segmentation of document pages", "abstract": "-this paper presents a semi automatic method used to segment color documents into different uniform color plans . the practical application is dedicated to administrative documents segmentation . in these documents , like in many other cases , color has a semantic meaning : it is then possible to identify some specific regions like manual annotations , rubber stamps or colored highlighting . a first step of user-controlled learning of the desired color plans is made on few sample documents . an automatic process can then be performed on the much bigger set as a batch . our experiments show very interesting results in with a very competitive processing time .", "topics": ["cluster analysis", "image segmentation"]}
{"title": "large margin multiclass gaussian classification with differential privacy", "abstract": "as increasing amounts of sensitive personal information is aggregated into data repositories , it has become important to develop mechanisms for processing the data without revealing information about individual data instances . the differential privacy model provides a framework for the development and theoretical analysis of such mechanisms . in this paper , we propose an algorithm for learning a discriminatively trained multi-class gaussian classifier that satisfies differential privacy using a large margin loss function with a perturbed regularization term . we present a theoretical upper bound on the excess risk of the classifier introduced by the perturbation .", "topics": ["loss function", "matrix regularization"]}
{"title": "a kernel for hierarchical parameter spaces", "abstract": "we define a family of kernels for mixed continuous/discrete hierarchical parameter spaces and show that they are positive definite .", "topics": ["kernel ( operating system )"]}
{"title": "a random block-coordinate douglas-rachford splitting method with low computational complexity for binary logistic regression", "abstract": "in this paper , we propose a new optimization algorithm for sparse logistic regression based on a stochastic version of the douglas-rachford splitting method . our algorithm sweeps the training set by randomly selecting a mini-batch of data at each iteration , and it allows us to update the variables in a block coordinate manner . our approach leverages the proximity operator of the logistic loss , which is expressed with the generalized lambert w function . experiments carried out on standard datasets demonstrate the efficiency of our approach w.r.t . stochastic gradient-like methods .", "topics": ["computational complexity theory", "iteration"]}
{"title": "material editing using a physically based rendering network", "abstract": "the ability to edit materials of objects in images is desirable by many content creators . however , this is an extremely challenging task as it requires to disentangle intrinsic physical properties of an image . we propose an end-to-end network architecture that replicates the forward image formation process to accomplish this task . specifically , given a single image , the network first predicts intrinsic properties , i.e . shape , illumination , and material , which are then provided to a rendering layer . this layer performs in-network image synthesis , thereby enabling the network to understand the physics behind the image formation process . the proposed rendering layer is fully differentiable , supports both diffuse and specular materials , and thus can be applicable in a variety of problem settings . we demonstrate a rich set of visually plausible material editing examples and provide an extensive comparative study .", "topics": ["end-to-end principle"]}
{"title": "symmetry breaking in neuroevolution : a technical report", "abstract": "artificial neural networks ( ann ) comprise important symmetry properties , which can influence the performance of monte carlo methods in neuroevolution . the problem of the symmetries is also known as the competing conventions problem or simply as the permutation problem . in the literature , symmetries are mainly addressed in genetic algoritm based approaches . however , investigations in this direction based on other evolutionary algorithms ( ea ) are rare or missing . furthermore , there are different and contradictionary reports on the efficacy of symmetry breaking . by using a novel viewpoint , we offer a possible explanation for this issue . as a result , we show that a strategy which is invariant to the global optimum can only be successfull on certain problems , whereas it must fail to improve the global convergence on others . we introduce the \\emph { minimum global optimum proximity } principle as a generalized and adaptive strategy to symmetry breaking , which depends on the location of the global optimum . we apply the proposed principle to differential evolution ( de ) and covariance matrix adaptation evolution strategies ( cma-es ) , which are two popular and conceptually different global optimization methods . using a wide range of feedforward ann problems , we experimentally illustrate significant improvements in the global search efficiency by the proposed symmetry breaking technique .", "topics": ["mathematical optimization", "neural networks"]}
{"title": "a class-incremental learning method based on one class support vector machine", "abstract": "a method based on one class support vector machine ( ocsvm ) is proposed for class incremental learning . several ocsvm models divide the input space into several parts . then , the 1vs1 classifiers are constructed for the confuse part by using the support vectors . during the class incremental learning process , the ocsvm of the new class is trained at first . then the support vectors of the old classes and the support vectors of the new class are reused to train 1vs1 classifiers for the confuse part . in order to bring more information to the certain support vectors , the support vectors are at the boundary of the distribution of samples as much as possible when the ocsvm is built . compared with the traditional methods , the proposed method retains the original model and thus reduces memory consumption and training time cost . various experiments on different datasets also verify the efficiency of the proposed method .", "topics": ["support vector machine", "support vector machine"]}
{"title": "a gaussian scale space approach for exudates detection , classification and severity prediction", "abstract": "in the context of computer aided diagnosis system for diabetic retinopathy , we present a novel method for detection of exudates and their classification for disease severity prediction . the method is based on gaussian scale space based interest map and mathematical morphology . it makes use of support vector machine for classification and location information of the optic disc and the macula region for severity prediction . it can efficiently handle luminance variation and it is suitable for varied sized exudates . the method has been probed in publicly available diaretdb1v2 and e-ophthaex databases . for exudate detection the proposed method achieved a sensitivity of 96.54 % and prediction of 98.35 % in diaretdb1v2 database .", "topics": ["support vector machine", "database"]}
{"title": "quantum annealing for variational bayes inference", "abstract": "this paper presents studies on a deterministic annealing algorithm based on quantum annealing for variational bayes ( qavb ) inference , which can be seen as an extension of the simulated annealing for variational bayes ( savb ) inference . qavb is as easy as savb to implement . experiments revealed qavb finds a better local optimum than savb in terms of the variational free energy in latent dirichlet allocation ( lda ) .", "topics": ["calculus of variations", "simulation"]}
{"title": "preference completion from partial rankings", "abstract": "we propose a novel and efficient algorithm for the collaborative preference completion problem , which involves jointly estimating individualized rankings for a set of entities over a shared set of items , based on a limited number of observed affinity values . our approach exploits the observation that while preferences are often recorded as numerical scores , the predictive quantity of interest is the underlying rankings . thus , attempts to closely match the recorded scores may lead to overfitting and impair generalization performance . instead , we propose an estimator that directly fits the underlying preference order , combined with nuclear norm constraints to encourage low -- rank parameters . besides ( approximate ) correctness of the ranking order , the proposed estimator makes no generative assumption on the numerical scores of the observations . one consequence is that the proposed estimator can fit any consistent partial ranking over a subset of the items represented as a directed acyclic graph ( dag ) , generalizing standard techniques that can only fit preference scores . despite this generality , for supervision representing total or blockwise total orders , the computational complexity of our algorithm is within a $ \\log $ factor of the standard algorithms for nuclear norm regularization based estimates for matrix completion . we further show promising empirical results for a novel and challenging application of collaboratively ranking of the associations between brain -- regions and cognitive neuroscience terms .", "topics": ["approximation algorithm", "computational complexity theory"]}
{"title": "regularized laplacian estimation and fast eigenvector approximation", "abstract": "recently , mahoney and orecchia demonstrated that popular diffusion-based procedures to compute a quick \\emph { approximation } to the first nontrivial eigenvector of a data graph laplacian \\emph { exactly } solve certain regularized semi-definite programs ( sdps ) . in this paper , we extend that result by providing a statistical interpretation of their approximation procedure . our interpretation will be analogous to the manner in which $ \\ell_2 $ -regularized or $ \\ell_1 $ -regularized $ \\ell_2 $ -regression ( often called ridge regression and lasso regression , respectively ) can be interpreted in terms of a gaussian prior or a laplace prior , respectively , on the coefficient vector of the regression problem . our framework will imply that the solutions to the mahoney-orecchia regularized sdp can be interpreted as regularized estimates of the pseudoinverse of the graph laplacian . conversely , it will imply that the solution to this regularized estimation problem can be computed very quickly by running , e.g . , the fast diffusion-based pagerank procedure for computing an approximation to the first nontrivial eigenvector of the graph laplacian . empirical results are also provided to illustrate the manner in which approximate eigenvector computation \\emph { implicitly } performs statistical regularization , relative to running the corresponding exact algorithm .", "topics": ["approximation algorithm", "matrix regularization"]}
{"title": "a comprehensive study on the applications of machine learning for diagnosis of cancer", "abstract": "collectively , lung cancer , breast cancer and melanoma was diagnosed in over 535,340 people out of which , 209,400 deaths were reported [ 13 ] . it is estimated that over 600,000 people will be diagnosed with these forms of cancer in 2015 . most of the deaths from lung cancer , breast cancer and melanoma result due to late detection . all of these cancers , if detected early , are 100 % curable . in this study , we develop and evaluate algorithms to diagnose breast cancer , melanoma , and lung cancer . in the first part of the study , we employed a normalised gradient descent and an artificial neural network to diagnose breast cancer with an overall accuracy of 91 % and 95 % respectively . in the second part of the study , an artificial neural network coupled with image processing and analysis algorithms was employed to achieve an overall accuracy of 93 % a naive mobile based application that allowed people to take diagnostic tests on their phones was developed . finally , a support vector machine algorithm incorporating image processing and image analysis algorithms was developed to diagnose lung cancer with an accuracy of 94 % . all of the aforementioned systems had very low false positive and false negative rates . we are developing an online network that incorporates all of these systems and allows people to collaborate globally .", "topics": ["image processing", "support vector machine"]}
{"title": "detecting motifs in system call sequences", "abstract": "the search for patterns or motifs in data represents an area of key interest to many researchers . in this paper we present the motif tracking algorithm , a novel immune inspired pattern identification tool that is able to identify unknown motifs which repeat within time series data . the power of the algorithm is derived from its use of a small number of parameters with minimal assumptions . the algorithm searches from a completely neutral perspective that is independent of the data being analysed , and the underlying motifs . in this paper the motif tracking algorithm is applied to the search for patterns within sequences of low level system calls between the linux kernel and the operating system 's user space . the mta is able to compress data found in large system call data sets to a limited number of motifs which summarise that data . the motifs provide a resource from which a profile of executed processes can be built . the potential for these profiles and new implications for security research are highlighted . a higher level call system language for measuring similarity between patterns of such calls is also suggested .", "topics": ["time series", "high- and low-level"]}
{"title": "toward implicit sample noise modeling : deviation-driven matrix factorization", "abstract": "the objective function of a matrix factorization model usually aims to minimize the average of a regression error contributed by each element . however , given the existence of stochastic noises , the implicit deviations of sample data from their true values are almost surely diverse , which makes each data point not equally suitable for fitting a model . in this case , simply averaging the cost among data in the objective function is not ideal . intuitively we would like to emphasize more on the reliable instances ( i.e . , those contain smaller noise ) while training a model . motivated by such observation , we derive our formula from a theoretical framework for optimal weighting under heteroscedastic noise distribution . specifically , by modeling and learning the deviation of data , we design a novel matrix factorization model . our model has two advantages . first , it jointly learns the deviation and conducts dynamic reweighting of instances , allowing the model to converge to a better solution . second , during learning the deviated instances are assigned lower weights , which leads to faster convergence since the model does not need to overfit the noise . the experiments are conducted in clean recommendation and noisy sensor datasets to test the effectiveness of the model in various scenarios . the results show that our model outperforms the state-of-the-art factorization and deep learning models in both accuracy and efficiency .", "topics": ["value ( ethics )", "optimization problem"]}
{"title": "deep generative model for joint alignment and word representation", "abstract": "this work exploits translation data as a source of semantically relevant learning signal for models of word representation . in particular , we exploit equivalence through translation as a form of distributed context and jointly learn how to embed and align with a deep generative model . our embedalign model embeds words in their complete observed context and learns by marginalisation of latent lexical alignments . besides , it embeds words as posterior probability densities , rather than point estimates , which allows us to compare words in context using a measure of overlap between distributions ( e.g . kl divergence ) . we investigate our model 's performance on a range of lexical semantics tasks achieving competitive results on several standard benchmarks including natural language inference , paraphrasing , and text similarity .", "topics": ["generative model", "natural language"]}
{"title": "operations for learning with graphical models", "abstract": "this paper is a multidisciplinary review of empirical , statistical learning from a graphical model perspective . well-known examples of graphical models include bayesian networks , directed graphs representing a markov chain , and undirected networks representing a markov field . these graphical models are extended to model data analysis and empirical learning using the notation of plates . graphical operations for simplifying and manipulating a problem are provided including decomposition , differentiation , and the manipulation of probability models from the exponential family . two standard algorithm schemas for learning are reviewed in a graphical framework : gibbs sampling and the expectation maximization algorithm . using these operations and schemas , some popular algorithms can be synthesized from their graphical specification . this includes versions of linear regression , techniques for feed-forward networks , and learning gaussian and discrete bayesian networks from data . the paper concludes by sketching some implications for data analysis and summarizing how some popular algorithms fall within the framework presented . the main original contributions here are the decomposition techniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms .", "topics": ["graphical model", "time complexity"]}
{"title": "uncovering locally discriminative structure for feature analysis", "abstract": "manifold structure learning is often used to exploit geometric information among data in semi-supervised feature learning algorithms . in this paper , we find that local discriminative information is also of importance for semi-supervised feature learning . we propose a method that utilizes both the manifold structure of data and local discriminant information . specifically , we define a local clique for each data point . the k-nearest neighbors ( knn ) is used to determine the structural information within each clique . we then employ a variant of fisher criterion model to each clique for local discriminant evaluation and sum all cliques as global integration into the framework . in this way , local discriminant information is embedded . labels are also utilized to minimize distances between data from the same class . in addition , we use the kernel method to extend our proposed model and facilitate feature learning in a high-dimensional space after feature mapping . experimental results show that our method is superior to all other compared methods over a number of datasets .", "topics": ["feature learning", "eisenstein 's criterion"]}
{"title": "supervised dimensionality reduction via distance correlation maximization", "abstract": "in our work , we propose a novel formulation for supervised dimensionality reduction based on a nonlinear dependency criterion called statistical distance correlation , szekely et . al . ( 2007 ) . we propose an objective which is free of distributional assumptions on regression variables and regression model assumptions . our proposed formulation is based on learning a low-dimensional feature representation $ \\mathbf { z } $ , which maximizes the squared sum of distance correlations between low dimensional features $ \\mathbf { z } $ and response $ y $ , and also between features $ \\mathbf { z } $ and covariates $ \\mathbf { x } $ . we propose a novel algorithm to optimize our proposed objective using the generalized minimization maximizaiton method of \\parizi et . al . ( 2015 ) . we show superior empirical results on multiple datasets proving the effectiveness of our proposed approach over several relevant state-of-the-art supervised dimensionality reduction methods .", "topics": ["nonlinear system", "eisenstein 's criterion"]}
{"title": "finite sample prediction and recovery bounds for ordinal embedding", "abstract": "the goal of ordinal embedding is to represent items as points in a low-dimensional euclidean space given a set of constraints in the form of distance comparisons like `` item $ i $ is closer to item $ j $ than item $ k $ '' . ordinal constraints like this often come from human judgments . to account for errors and variation in judgments , we consider the noisy situation in which the given constraints are independently corrupted by reversing the correct constraint with some probability . this paper makes several new contributions to this problem . first , we derive prediction error bounds for ordinal embedding with noise by exploiting the fact that the rank of a distance matrix of points in $ \\mathbb { r } ^d $ is at most $ d+2 $ . these bounds characterize how well a learned embedding predicts new comparative judgments . second , we investigate the special case of a known noise model and study the maximum likelihood estimator . third , knowledge of the noise model enables us to relate prediction errors to embedding accuracy . this relationship is highly non-trivial since we show that the linear map corresponding to distance comparisons is non-invertible , but there exists a nonlinear map that is invertible . fourth , two new algorithms for ordinal embedding are proposed and evaluated in experiments .", "topics": ["nonlinear system"]}
{"title": "geniepath : graph neural networks with adaptive receptive paths", "abstract": "we present , geniepath , a scalable approach for learning adaptive receptive fields of neural networks defined on permutation invariant graph data . in geniepath , we propose an adaptive path layer consists of two functions designed for breadth and depth exploration respectively , where the former learns the importance of different sized neighborhoods , while the latter extracts and filters signals aggregated from neighbors of different hops away . our method works both in transductive and inductive settings , and extensive experiments compared with state-of-the-art methods show that our approaches are useful especially on large graph .", "topics": ["scalability"]}
{"title": "a component based heuristic search method with evolutionary eliminations", "abstract": "nurse rostering is a complex scheduling problem that affects hospital personnel on a daily basis all over the world . this paper presents a new component-based approach with evolutionary eliminations , for a nurse scheduling problem arising at a major uk hospital . the main idea behind this technique is to decompose a schedule into its components ( i.e . the allocated shift pattern of each nurse ) , and then to implement two evolutionary elimination strategies mimicking natural selection and natural mutation process on these components respectively to iteratively deliver better schedules . the worthiness of all components in the schedule has to be continuously demonstrated in order for them to remain there . this demonstration employs an evaluation function which evaluates how well each component contributes towards the final objective . two elimination steps are then applied : the first elimination eliminates a number of components that are deemed not worthy to stay in the current schedule ; the second elimination may also throw out , with a low level of probability , some worthy components . the eliminated components are replenished with new ones using a set of constructive heuristics using local optimality criteria . computational results using 52 data instances demonstrate the applicability of the proposed approach in solving real-world problems .", "topics": ["high- and low-level", "heuristic"]}
{"title": "solutions of quadratic first-order odes applied to computer vision problems", "abstract": "the article proves the existence of a maximum of two possible solutions to the initial value problem composed by the planar-perspective equation and an initial condition . this initial value problem has a geometric interpretation . solutions are curves than pass trough the initial condition which is a point of the plane .", "topics": ["nonlinear system", "loss function"]}
{"title": "scale-recurrent network for deep image deblurring", "abstract": "in single image deblurring , the `` coarse-to-fine '' scheme , i.e . gradually restoring the sharp image on different resolutions in a pyramid , is very successful in both traditional optimization-based methods and recent neural-network-based approaches . in this paper , we investigate this strategy and propose a scale-recurrent network ( srn-deblurnet ) for this deblurring task . compared with the many recent learning-based approaches in [ 25 ] , it has a simpler network structure , a smaller number of parameters and is easier to train . we evaluate our method on large-scale deblurring datasets with complex motion . results show that our method can produce better quality results than state-of-the-arts , both quantitatively and qualitatively .", "topics": ["recurrent neural network"]}
{"title": "computing circumscriptive databases by integer programming : revisited ( extended abstract )", "abstract": "in this paper , we consider a method of computing minimal models in circumscription using integer programming in propositional logic and first-order logic with domain closure axioms and unique name axioms . this kind of treatment is very important since this enable to apply various technique developed in operations research to nonmonotonic reasoning . nerode et al . ( 1995 ) are the first to propose a method of computing circumscription using integer programming . they claimed their method was correct for circumscription with fixed predicate , but we show that their method does not correctly reflect their claim . we show a correct method of computing all the minimal models not only with fixed predicates but also with varied predicates and we extend our method to compute prioritized circumscription as well .", "topics": ["database"]}
{"title": "learning a multi-view stereo machine", "abstract": "we present a learnt system for multi-view stereopsis . in contrast to recent learning based methods for 3d reconstruction , we leverage the underlying 3d geometry of the problem through feature projection and unprojection along viewing rays . by formulating these operations in a differentiable manner , we are able to learn the system end-to-end for the task of metric 3d reconstruction . end-to-end learning allows us to jointly reason about shape priors while conforming geometric constraints , enabling reconstruction from much fewer images ( even a single image ) than required by classical approaches as well as completion of unseen surfaces . we thoroughly evaluate our approach on the shapenet dataset and demonstrate the benefits over classical approaches as well as recent learning based methods .", "topics": ["end-to-end principle"]}
{"title": "varying k-lipschitz constraint for generative adversarial networks", "abstract": "generative adversarial networks ( gans ) are powerful generative models , but suffer from training instability . the recent proposed wasserstein gan with gradient penalty ( wgan-gp ) makes progress toward stable training . gradient penalty acts as the role of enforcing a lipschitz constraint . further investigation on gradient penalty shows that gradient penalty may impose restriction on the capacity of discriminator . as a replacement , we introduce varying k-lipschitz constraint . proposed varying k-lipschitz constraint witness better image quality and significantly improved training speed when testing on gan architecture . besides , we introduce an effective convergence measure , which correlates well with image quality .", "topics": ["gradient", "gradient"]}
{"title": "random polyhedral scenes : an image generator for active vision system experiments", "abstract": "we present a polyhedral scene generator system which creates a random scene based on a few user parameters , renders the scene from random view points and creates a dataset containing the renderings and corresponding annotation files . we hope that this generator will enable research on how a program could parse a scene if it had multiple viewpoints to consider . for ambiguous scenes , typically people move their head or change their position to see the scene from different angles as well as seeing how it changes while they move ; this research field is called active perception . the random scene generator presented is designed to support research in this field by generating images of scenes with known complexity characteristics and with verifiable properties with respect to the distribution of features across a population . thus , it is well-suited for research in active perception without the requirement of a live 3d environment and mobile sensing agent , including comparative performance evaluations . the system is publicly available at https : //polyhedral.eecs.yorku.ca .", "topics": ["computer vision", "parsing"]}
{"title": "sparseness meets deepness : 3d human pose estimation from monocular video", "abstract": "this paper addresses the challenge of 3d full-body human pose estimation from a monocular image sequence . here , two cases are considered : ( i ) the image locations of the human joints are provided and ( ii ) the image locations of joints are unknown . in the former case , a novel approach is introduced that integrates a sparsity-driven 3d geometric prior and temporal smoothness . in the latter case , the former case is extended by treating the image locations of the joints as latent variables . a deep fully convolutional network is trained to predict the uncertainty maps of the 2d joint locations . the 3d pose estimates are realized via an expectation-maximization algorithm over the entire sequence , where it is shown that the 2d joint location uncertainties can be conveniently marginalized out during inference . empirical evaluation on the human3.6m dataset shows that the proposed approaches achieve greater 3d pose estimation accuracy over state-of-the-art baselines . further , the proposed approach outperforms a publicly available 2d pose estimation baseline on the challenging pennaction dataset .", "topics": ["baseline ( configuration management )", "map"]}
{"title": "iterative deepening branch and bound", "abstract": "in tree search problem the best-first search algorithm needs too much of space . to remove such drawbacks of these algorithms the ida* was developed which is both space and time cost efficient . but again ida* can give an optimal solution for real valued problems like flow shop scheduling , travelling salesman and 0/1 knapsack due to their real valued cost estimates . thus further modifications are done on it and the iterative deepening branch and bound search algorithms is developed which meets the requirements . we have tried using this algorithm for the flow shop scheduling problem and have found that it is quite effective .", "topics": ["optimization problem"]}
{"title": "multi-fold gabor , pca and ica filter convolution descriptor for face recognition", "abstract": "this paper devises a new means of filter diversification , dubbed multi-fold filter convolution ( m-ffc ) , for face recognition . on the assumption that m-ffc receives single-scale gabor filters of varying orientations as input , these filters are self-cross convolved by m-fold to instantiate a filter offspring set . the m-ffc flexibility also permits cross convolution amongst gabor filters and other filter banks of profoundly dissimilar traits , e.g . , principal component analysis ( pca ) filters , and independent component analysis ( ica ) filters . the 2-ffc of gabor , pca and ica filters thus yields three offspring sets : ( 1 ) gabor filters solely , ( 2 ) gabor-pca filters , and ( 3 ) gabor-ica filters , to render the learning-free and the learning-based 2-ffc descriptors . to facilitate a sensible gabor filter selection for m-ffc , the 40 multi-scale , multi-orientation gabor filters are condensed into 8 elementary filters . aside from that , an average histogram pooling operator is employed to leverage the 2-ffc histogram features , prior to the final whitening pca compression . the empirical results substantiate that the 2-ffc descriptors prevail over , or on par with , other face descriptors on both identification and verification tasks .", "topics": ["convolution"]}
{"title": "learning to learn by gradient descent by gradient descent", "abstract": "the move from hand-designed features to learned features in machine learning has been wildly successful . in spite of this , optimization algorithms are still designed by hand . in this paper we show how the design of an optimization algorithm can be cast as a learning problem , allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way . our learned algorithms , implemented by lstms , outperform generic , hand-designed competitors on the tasks for which they are trained , and also generalize well to new tasks with similar structure . we demonstrate this on a number of tasks , including simple convex problems , training neural networks , and styling images with neural art .", "topics": ["gradient descent", "gradient"]}
{"title": "generating multilingual personalized descriptions of museum exhibits - the m-piro project", "abstract": "this paper provides an overall presentation of the m-piro project . m-piro is developing technology that will allow museums to generate automatically textual or spoken descriptions of exhibits for collections available over the web or in virtual reality environments . the descriptions are generated in several languages from information in a language-independent database and small fragments of text , and they can be tailored according to the backgrounds of the users , their ages , and their previous interaction with the system . an authoring tool allows museum curators to update the system 's database and to control the language and content of the resulting descriptions . although the project is still in progress , a web-based demonstrator that supports english , greek and italian is already available , and it is used throughout the paper to highlight the capabilities of the emerging technology .", "topics": ["database"]}
{"title": "dcan : deep contour-aware networks for accurate gland segmentation", "abstract": "the morphology of glands has been used routinely by pathologists to assess the malignancy degree of adenocarcinomas . accurate segmentation of glands from histology images is a crucial step to obtain reliable morphological statistics for quantitative diagnosis . in this paper , we proposed an efficient deep contour-aware network ( dcan ) to solve this challenging problem under a unified multi-task learning framework . in the proposed network , multi-level contextual features from the hierarchical architecture are explored with auxiliary supervision for accurate gland segmentation . when incorporated with multi-task regularization during the training , the discriminative capability of intermediate features can be further improved . moreover , our network can not only output accurate probability maps of glands , but also depict clear contours simultaneously for separating clustered objects , which further boosts the gland segmentation performance . this unified framework can be efficient when applied to large-scale histopathological data without resorting to additional steps to generate contours based on low-level cues for post-separating . our method won the 2015 miccai gland segmentation challenge out of 13 competitive teams , surpassing all the other methods by a significant margin .", "topics": ["high- and low-level", "matrix regularization"]}
{"title": "robot vision architecture for autonomous clothes manipulation", "abstract": "this paper presents a novel robot vision architecture for perceiving generic 3d clothes configurations . our architecture is hierarchically structured , starting from low-level curvatures , across mid-level geometric shapes \\ & topology descriptions ; and finally approaching high-level semantic surface structure descriptions . we demonstrate our robot vision architecture in a customised dual-arm industrial robot with our self-designed , off-the-self stereo vision system , carrying out autonomous grasping and dual-arm flattening . it is worth noting that the proposed dual-arm flattening approach is unique among the state-of-the-art robot autonomous system , which is the major contribution of this paper . the experimental results show that the proposed dual-arm flattening using stereo vision system remarkably outperforms the single-arm flattening and widely-cited kinect-based sensing system for dexterous manipulation tasks . in addition , the proposed grasping approach achieves satisfactory performance on grasping various kind of garments , verifying the capability of proposed visual perception architecture to be adapted to more than one clothing manipulation tasks .", "topics": ["high- and low-level", "autonomous car"]}
{"title": "beating the world 's best at super smash bros. with deep reinforcement learning", "abstract": "there has been a recent explosion in the capabilities of game-playing artificial intelligence . many classes of rl tasks , from atari games to motor control to board games , are now solvable by fairly generic algorithms , based on deep learning , that learn to play from experience with minimal knowledge of the specific domain of interest . in this work , we will investigate the performance of these methods on super smash bros. melee ( ssbm ) , a popular console fighting game . the ssbm environment has complex dynamics and partial observability , making it challenging for human and machine alike . the multi-player aspect poses an additional challenge , as the vast majority of recent advances in rl have focused on single-agent environments . nonetheless , we will show that it is possible to train agents that are competitive against and even surpass human professionals , a new result for the multi-player video game setting .", "topics": ["reinforcement learning", "artificial intelligence"]}
{"title": "deep learning-based food calorie estimation method in dietary assessment", "abstract": "obesity treatment requires obese patients to record all food intakes per day . computer vision has been introduced to estimate calories from food images . in order to increase accuracy of detection and reduce the error of volume estimation in food calorie estimation , we present our calorie estimation method in this paper . to estimate calorie of food , a top view and side view is needed . faster r-cnn is used to detect the food and calibration object . grabcut algorithm is used to get each food 's contour . then the volume is estimated with the food and corresponding object . finally we estimate each food 's calorie . and the experiment results show our estimation method is effective .", "topics": ["computer vision"]}
{"title": "adapting a general parser to a sublanguage", "abstract": "in this paper , we propose a method to adapt a general parser ( link parser ) to sublanguages , focusing on the parsing of texts in biology . our main proposal is the use of terminology ( identication and analysis of terms ) in order to reduce the complexity of the text to be parsed . several other strategies are explored and finally combined among which text normalization , lexicon and morpho-guessing module extensions and grammar rules adaptation . we compare the parsing results before and after these adaptations .", "topics": ["parsing"]}
{"title": "bilevel approaches for learning of variational imaging models", "abstract": "we review some recent learning approaches in variational imaging , based on bilevel optimisation , and emphasize the importance of their treatment in function space . the paper covers both analytical and numerical techniques . analytically , we include results on the existence and structure of minimisers , as well as optimality conditions for their characterisation . based on this information , newton type methods are studied for the solution of the problems at hand , combining them with sampling techniques in case of large databases . the computational verification of the developed techniques is extensively documented , covering instances with different type of regularisers , several noise models , spatially dependent weights and large image databases .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "pyramidbox : a context-assisted single shot face detector", "abstract": "face detection has been well studied for many years and one of the remaining challenges is to detect small , blurred and partially occluded faces in uncontrolled environment . this paper proposes a novel context-assisted single shot face detector , named pyramidbox , to handle the hard face detection problem . observing the importance of the context , we improve the utilization of contextual information in the following three aspects . first , we design a novel contextual anchor to supervise high-level contextual feature learning by a semi-supervised method , which we call it pyramidanchors . second , we propose the low-level feature pyramid network to combine adequate high-level contextual semantic feature and low-level facial feature together , which also allows the pyramidbox to predict faces of all scales in a single shot . third , we introduce a context-sensitive structure to increase the capacity of prediction network to improve the final accuracy of output . in addition , we use the method of data-anchor-sampling to augment the training samples across different scales , which increases the diversities of training data for smaller faces . by exploiting the value of context , pyramidbox achieves superior performance among the state-of-the-art on the two common face detection benchmarks , fddb and wider face .", "topics": ["sampling ( signal processing )", "feature learning"]}
{"title": "cross-lingual distillation for text classification", "abstract": "cross-lingual text classification ( cltc ) is the task of classifying documents written in different languages into the same taxonomy of categories . this paper presents a novel approach to cltc that builds on model distillation , which adapts and extends a framework originally proposed for model compression . using soft probabilistic predictions for the documents in a label-rich language as the ( induced ) supervisory labels in a parallel corpus of documents , we train classifiers successfully for new languages in which labeled training data are not available . an adversarial feature adaptation technique is also applied during the model training to reduce distribution mismatch . we conducted experiments on two benchmark cltc datasets , treating english as the source language and german , french , japan and chinese as the unlabeled target languages . the proposed approach had the advantageous or comparable performance of the other state-of-art methods .", "topics": ["test set"]}
{"title": "fixed-form variational posterior approximation through stochastic linear regression", "abstract": "we propose a general algorithm for approximating nonstandard bayesian posterior distributions . the algorithm minimizes the kullback-leibler divergence of an approximating distribution to the intractable posterior distribution . our method can be used to approximate any posterior distribution , provided that it is given in closed form up to the proportionality constant . the approximation can be any distribution in the exponential family or any mixture of such distributions , which means that it can be made arbitrarily precise . several examples illustrate the speed and accuracy of our approximation method in practice .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "improving content marketing processes with the approaches by artificial intelligence", "abstract": "content marketing is todays one of the most remarkable approaches in the context of marketing processes of companies . value of this kind of marketing has improved in time , thanks to the latest developments regarding to computer and communication technologies . nowadays , especially social media based platforms have a great importance on enabling companies to design multimedia oriented , interactive content . but on the other hand , there is still something more to do for improved content marketing approaches . in this context , objective of this study is to focus on intelligent content marketing , which can be done by using artificial intelligence . artificial intelligence is todays one of the most remarkable research fields and it can be used easily as multidisciplinary . so , this study has aimed to discuss about its potential on improving content marketing . in detail , the study has enabled readers to improve their awareness about the intersection point of content marketing and artificial intelligence . furthermore , the authors have introduced some example models of intelligent content marketing , which can be achieved by using current web technologies and artificial intelligence techniques .", "topics": ["artificial intelligence"]}
{"title": "cache-based document-level neural machine translation", "abstract": "sentences in a well-formed text are connected to each other via various links to form the cohesive structure of the text . current neural machine translation ( nmt ) systems translate a text in a conventional sentence-by-sentence fashion , ignoring such cross-sentence links and dependencies . this may lead to generate an incohesive and incoherent target text for a cohesive and coherent source text . in order to handle this issue , we propose a cache-based approach to document-level neural machine translation by capturing contextual information either from recently translated sentences or the entire document . particularly , we explore two types of caches : a dynamic cache , which stores words from the best translation hypotheses of preceding sentences , and a topic cache , which maintains a set of target-side topical words that are semantically related to the document to be translated . on this basis , we build a new layer to score target words in these two caches with a cache-based neural model . here the estimated probabilities from the cache-based neural model are combined with nmt probabilities into the final word prediction probabilities via a gating mechanism . finally , the proposed cache-based neural model is trained jointly with a state-of-the-art neural machine translation system in an end-to-end manner . on several nist chinese-english translation tasks , our experiments demonstrate that the proposed cache-based model achieves substantial improvements over several state-of-the-art smt and nmt baselines .", "topics": ["machine translation", "end-to-end principle"]}
{"title": "uncovering temporal context for video question and answering", "abstract": "in this work , we introduce video question answering in temporal domain to infer the past , describe the present and predict the future . we present an encoder-decoder approach using recurrent neural networks to learn temporal structures of videos and introduce a dual-channel ranking loss to answer multiple-choice questions . we explore approaches for finer understanding of video content using question form of `` fill-in-the-blank '' , and managed to collect 109,895 video clips with duration over 1,000 hours from tacos , mpii-md , medtest 14 datasets , while the corresponding 390,744 questions are generated from annotations . extensive experiments demonstrate that our approach significantly outperforms the compared baselines .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "a survey of recent advances in cnn-based single image crowd counting and density estimation", "abstract": "estimating count and density maps from crowd images has a wide range of applications such as video surveillance , traffic monitoring , public safety and urban planning . in addition , techniques developed for crowd counting can be applied to related tasks in other fields of study such as cell microscopy , vehicle counting and environmental survey . the task of crowd counting and density map estimation is riddled with many challenges such as occlusions , non-uniform density , intra-scene and inter-scene variations in scale and perspective . nevertheless , over the last few years , crowd count analysis has evolved from earlier methods that are often limited to small variations in crowd density and scales to the current state-of-the-art methods that have developed the ability to perform successfully on a wide range of scenarios . the success of crowd counting methods in the recent years can be largely attributed to deep learning and publications of challenging datasets . in this paper , we provide a comprehensive survey of recent convolutional neural network ( cnn ) based approaches that have demonstrated significant improvements over earlier methods that rely largely on hand-crafted representations . first , we briefly review the pioneering methods that use hand-crafted representations and then we delve in detail into the deep learning-based approaches and recently published datasets . furthermore , we discuss the merits and drawbacks of existing cnn-based approaches and identify promising avenues of research in this rapidly evolving field .", "topics": ["numerical analysis", "map"]}
{"title": "an approach to solve linear equations using a time-variant adaptation based hybrid evolutionary algorithm", "abstract": "for small number of equations , systems of linear ( and sometimes nonlinear ) equations can be solved by simple classical techniques . however , for large number of systems of linear ( or nonlinear ) equations , solutions using classical method become arduous . on the other hand evolutionary algorithms have mostly been used to solve various optimization and learning problems . recently , hybridization of evolutionary algorithm with classical gauss-seidel based successive over relaxation ( sor ) method has successfully been used to solve large number of linear equations ; where a uniform adaptation ( ua ) technique of relaxation factor is used . in this paper , a new hybrid algorithm is proposed in which a time-variant adaptation ( tva ) technique of relaxation factor is used instead of uniform adaptation technique to solve large number of linear equations . the convergence theorems of the proposed algorithms are proved theoretically . and the performance of the proposed tva-based algorithm is compared with the ua-based hybrid algorithm in the experimental domain . the proposed algorithm outperforms the hybrid one in terms of efficiency .", "topics": ["nonlinear system"]}
{"title": "diagnostic prediction using discomfort drawings with ibtm", "abstract": "in this paper , we explore the possibility to apply machine learning to make diagnostic predictions using discomfort drawings . a discomfort drawing is an intuitive way for patients to express discomfort and pain related symptoms . these drawings have proven to be an effective method to collect patient data and make diagnostic decisions in real-life practice . a dataset from real-world patient cases is collected for which medical experts provide diagnostic labels . next , we use a factorized multimodal topic model , inter-battery topic model ( ibtm ) , to train a system that can make diagnostic predictions given an unseen discomfort drawing . the number of output diagnostic labels is determined by using mean-shift clustering on the discomfort drawing . experimental results show reasonable predictions of diagnostic labels given an unseen discomfort drawing . additionally , we generate synthetic discomfort drawings with ibtm given a diagnostic label , which results in typical cases of symptoms . the positive result indicates a significant potential of machine learning to be used for parts of the pain diagnostic process and to be a decision support system for physicians and other health care personnel .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "proceedings of the sixteenth conference on uncertainty in artificial intelligence ( 2000 )", "abstract": "this is the proceedings of the sixteenth conference on uncertainty in artificial intelligence , which was held in san francisco , ca , june 30 - july 3 , 2000", "topics": ["artificial intelligence"]}
{"title": "simultaneous tensor completion and denoising by noise inequality constrained convex optimization", "abstract": "tensor completion is a technique of filling missing elements of the incomplete data tensors . it being actively studied based on the convex optimization scheme such as nuclear-norm minimization . when given data tensors include some noises , the nuclear-norm minimization problem is usually converted to the nuclear-norm `regularization ' problem which simultaneously minimize penalty and error terms with some trade-off parameter . however , the good value of trade-off is not easily determined because of the difference of two units and the data dependence . in the sense of trade-off tuning , the noisy tensor completion problem with the `noise inequality constraint ' is better choice than the `regularization ' because the good noise threshold can be easily bounded with noise standard deviation . in this study , we tackle to solve the convex tensor completion problems with two types of noise inequality constraints : gaussian and laplace distributions . the contributions of this study are follows : ( 1 ) new tensor completion and denoising models using tensor total variation and nuclear-norm are proposed which can be characterized as a generalization/extension of many past matrix and tensor completion models , ( 2 ) proximal mappings for noise inequalities are derived which are analytically computable with low computational complexity , ( 3 ) convex optimization algorithm is proposed based on primal-dual splitting framework , ( 4 ) new step-size adaptation method is proposed to accelerate the optimization , and ( 5 ) extensive experiments demonstrated the advantages of the proposed method for visual data retrieval such as for color images , movies , and 3d-volumetric data .", "topics": ["computational complexity theory", "noise reduction"]}
{"title": "weakly supervised patchnets : describing and aggregating local patches for scene recognition", "abstract": "traditional feature encoding scheme ( e.g . , fisher vector ) with local descriptors ( e.g . , sift ) and recent convolutional neural networks ( cnns ) are two classes of successful methods for image recognition . in this paper , we propose a hybrid representation , which leverages the discriminative capacity of cnns and the simplicity of descriptor encoding schema for image recognition , with a focus on scene recognition . to this end , we make three main contributions from the following aspects . first , we propose a patch-level and end-to-end architecture to model the appearance of local patches , called { \\em patchnet } . patchnet is essentially a customized network trained in a weakly supervised manner , which uses the image-level supervision to guide the patch-level feature extraction . second , we present a hybrid visual representation , called { \\em vsad } , by utilizing the robust feature representations of patchnet to describe local patches and exploiting the semantic probabilities of patchnet to aggregate these local patches into a global representation . third , based on the proposed vsad representation , we propose a new state-of-the-art scene recognition approach , which achieves an excellent performance on two standard benchmarks : mit indoor67 ( 86.2\\ % ) and sun397 ( 73.0\\ % ) .", "topics": ["feature extraction", "computer vision"]}
{"title": "contrast enhancement of brightness-distorted images by improved adaptive gamma correction", "abstract": "as an efficient image contrast enhancement ( ce ) tool , adaptive gamma correction ( agc ) was previously proposed by relating gamma parameter with cumulative distribution function ( cdf ) of the pixel gray levels within an image . acg deals well with most dimmed images , but fails for globally bright images and the dimmed images with local bright regions . such two categories of brightness-distorted images are universal in real scenarios , such as improper exposure and white object regions . in order to attenuate such deficiencies , here we propose an improved agc algorithm . the novel strategy of negative images is used to realize ce of the bright images , and the gamma correction modulated by truncated cdf is employed to enhance the dimmed ones . as such , local over-enhancement and structure distortion can be alleviated . both qualitative and quantitative experimental results show that our proposed method yields consistently good ce results .", "topics": ["pixel"]}
{"title": "data cleaning for xml electronic dictionaries via statistical anomaly detection", "abstract": "many important forms of data are stored digitally in xml format . errors can occur in the textual content of the data in the fields of the xml . fixing these errors manually is time-consuming and expensive , especially for large amounts of data . there is increasing interest in the research , development , and use of automated techniques for assisting with data cleaning . electronic dictionaries are an important form of data frequently stored in xml format that frequently have errors introduced through a mixture of manual typographical entry errors and optical character recognition errors . in this paper we describe methods for flagging statistical anomalies as likely errors in electronic dictionaries stored in xml format . we describe six systems based on different sources of information . the systems detect errors using various signals in the data including uncommon characters , text length , character-based language models , word-based language models , tied-field length ratios , and tied-field transliteration models . four of the systems detect errors based on expectations automatically inferred from content within elements of a single field type . we call these single-field systems . two of the systems detect errors based on correspondence expectations automatically inferred from content within elements of multiple related field types . we call these tied-field systems . for each system , we provide an intuitive analysis of the type of error that it is successful at detecting . finally , we describe two larger-scale evaluations using crowdsourcing with amazon 's mechanical turk platform and using the annotations of a domain expert . the evaluations consistently show that the systems are useful for improving the efficiency with which errors in xml electronic dictionaries can be detected .", "topics": ["dictionary"]}
{"title": "structured dictionary learning for classification", "abstract": "sparsity driven signal processing has gained tremendous popularity in the last decade . at its core , the assumption is that the signal of interest is sparse with respect to either a fixed transformation or a signal dependent dictionary . to better capture the data characteristics , various dictionary learning methods have been proposed for both reconstruction and classification tasks . for classification particularly , most approaches proposed so far have focused on designing explicit constraints on the sparse code to improve classification accuracy while simply adopting $ l_0 $ -norm or $ l_1 $ -norm for sparsity regularization . motivated by the success of structured sparsity in the area of compressed sensing , we propose a structured dictionary learning framework ( structdl ) that incorporates the structure information on both group and task levels in the learning process . its benefits are two-fold : ( i ) the label consistency between dictionary atoms and training data are implicitly enforced ; and ( ii ) the classification performance is more robust in the cases of a small dictionary size or limited training data than other techniques . using the subspace model , we derive the conditions for structdl to guarantee the performance and show theoretically that structdl is superior to $ l_0 $ -norm or $ l_1 $ -norm regularized dictionary learning for classification . extensive experiments have been performed on both synthetic simulations and real world applications , such as face recognition and object classification , to demonstrate the validity of the proposed dl framework .", "topics": ["test set", "synthetic data"]}
{"title": "exploiting diversity for natural language parsing", "abstract": "the popularity of applying machine learning methods to computational linguistics problems has produced a large supply of trainable natural language processing systems . most problems of interest have an array of off-the-shelf products or downloadable code implementing solutions using various techniques . where these solutions are developed independently , it is observed that their errors tend to be independently distributed . this thesis is concerned with approaches for capitalizing on this situation in a sample problem domain , penn treebank-style parsing . the machine learning community provides techniques for combining outputs of classifiers , but parser output is more structured and interdependent than classifications . to address this discrepancy , two novel strategies for combining parsers are used : learning to control a switch between parsers and constructing a hybrid parse from multiple parsers ' outputs . off-the-shelf parsers are not developed with an intention to perform well in a collaborative ensemble . two techniques are presented for producing an ensemble of parsers that collaborate . all of the ensemble members are created using the same underlying parser induction algorithm , and the method for producing complementary parsers is only loosely constrained by that chosen algorithm .", "topics": ["natural language processing", "natural language"]}
{"title": "deep visual attention prediction", "abstract": "in this work , we aim to predict human eye fixation with view-free scenes based on an end-to-end deep learning architecture . although convolutional neural networks ( cnns ) have made substantial improvement on human attention prediction , it is still needed to improve cnn based attention models by efficiently leveraging multi-scale features . our visual attention network is proposed to capture hierarchical saliency information from deep , coarse layers with global saliency information to shallow , fine layers with local saliency response . our model is based on a skip-layer network structure , which predicts human attention from multiple convolutional layers with various reception fields . final saliency prediction is achieved via the cooperation of those global and local predictions . our model is learned in a deep supervision manner , where supervision is directly fed into multi-level layers , instead of previous approaches of providing supervision only at the output layer and propagating this supervision back to earlier layers . our model thus incorporates multi-level saliency predictions within a single network , which significantly decreases the redundancy of previous approaches of learning multiple network streams with different input scales . extensive experimental analysis on various challenging benchmark datasets demonstrate our method yields state-of-the-art performance with competitive inference time .", "topics": ["end-to-end principle"]}
{"title": "graph learning from data under structural and laplacian constraints", "abstract": "graphs are fundamental mathematical structures used in various fields to represent data , signals and processes . in this paper , we propose a novel framework for learning/estimating graphs from data . the proposed framework includes ( i ) formulation of various graph learning problems , ( ii ) their probabilistic interpretations and ( iii ) associated algorithms . specifically , graph learning problems are posed as estimation of graph laplacian matrices from some observed data under given structural constraints ( e.g . , graph connectivity and sparsity level ) . from a probabilistic perspective , the problems of interest correspond to maximum a posteriori ( map ) parameter estimation of gaussian-markov random field ( gmrf ) models , whose precision ( inverse covariance ) is a graph laplacian matrix . for the proposed graph learning problems , specialized algorithms are developed by incorporating the graph laplacian and structural constraints . the experimental results demonstrate that the proposed algorithms outperform the current state-of-the-art methods in terms of accuracy and computational efficiency .", "topics": ["sparse matrix"]}
{"title": "deep embedding forest : forest-based serving with deep embedding features", "abstract": "deep neural networks ( dnn ) have demonstrated superior ability to extract high level embedding vectors from low level features . despite the success , the serving time is still the bottleneck due to expensive run-time computation of multiple layers of dense matrices . gpgpu , fpga , or asic-based serving systems require additional hardware that are not in the mainstream design of most commercial applications . in contrast , tree or forest-based models are widely adopted because of low serving cost , but heavily depend on carefully engineered features . this work proposes a deep embedding forest model that benefits from the best of both worlds . the model consists of a number of embedding layers and a forest/tree layer . the former maps high dimensional ( hundreds of thousands to millions ) and heterogeneous low-level features to the lower dimensional ( thousands ) vectors , and the latter ensures fast serving . built on top of a representative dnn model called deep crossing , and two forest/tree-based models including xgboost and lightgbm , a two-step deep embedding forest algorithm is demonstrated to achieve on-par or slightly better performance as compared with the dnn counterpart , with only a fraction of serving time on conventional hardware . after comparing with a joint optimization algorithm called partial fuzzification , also proposed in this paper , it is concluded that the two-step deep embedding forest has achieved near optimal performance . experiments based on large scale data sets ( up to 1 billion samples ) from a major sponsored search engine proves the efficacy of the proposed model .", "topics": ["high- and low-level", "neural networks"]}
{"title": "universal memcomputing machines", "abstract": "we introduce the notion of universal memcomputing machines ( umms ) : a class of brain-inspired general-purpose computing machines based on systems with memory , whereby processing and storing of information occur on the same physical location . we analytically prove that the memory properties of umms endow them with universal computing power - they are turing-complete - , intrinsic parallelism , functional polymorphism , and information overhead , namely their collective states can support exponential data compression directly in memory . we also demonstrate that a umm has the same computational power as a non-deterministic turing machine , namely it can solve np -- complete problems in polynomial time . however , by virtue of its information overhead , a umm needs only an amount of memory cells ( memprocessors ) that grows polynomially with the problem size . as an example we provide the polynomial-time solution of the subset-sum problem and a simple hardware implementation of the same . even though these results do not prove the statement np=p within the turing paradigm , the practical realization of these umms would represent a paradigm shift from present von neumann architectures bringing us closer to brain-like neural computation .", "topics": ["time complexity", "support vector machine"]}
{"title": "learning constructive primitives for online level generation and real-time content adaptation in super mario bros", "abstract": "procedural content generation ( pcg ) is of great interest to game design and development as it generates game content automatically . motivated by the recent learning-based pcg framework and other existing pcg works , we propose an alternative approach to online content generation and adaptation in super mario bros ( smb ) . unlike most of existing works in smb , our approach exploits the synergy between rule-based and learning-based methods to produce constructive primitives , quality yet controllable game segments in smb . as a result , a complete quality game level can be generated online by integrating relevant constructive primitives via controllable parameters regarding geometrical features and procedure-level properties . also the adaptive content can be generated in real time by dynamically selecting proper constructive primitives via an adaptation criterion , e.g . , dynamic difficulty adjustment ( dda ) . our approach is of several favorable properties in terms of content quality assurance , generation efficiency and controllability . extensive simulation results demonstrate that the proposed approach can generate controllable yet quality game levels online and adaptable content for dda in real time .", "topics": ["simulation", "eisenstein 's criterion"]}
{"title": "ranking via robust binary classification and parallel parameter estimation in large-scale data", "abstract": "we propose robirank , a ranking algorithm that is motivated by observing a close connection between evaluation metrics for learning to rank and loss functions for robust classification . the algorithm shows a very competitive performance on standard benchmark datasets against other representative algorithms in the literature . on the other hand , in large scale problems where explicit feature vectors and scores are not given , our algorithm can be efficiently parallelized across a large number of machines ; for a task that requires 386,133 x 49,824,519 pairwise interactions between items to be ranked , our algorithm finds solutions that are of dramatically higher quality than that can be found by a state-of-the-art competitor algorithm , given the same amount of wall-clock time for computation .", "topics": ["loss function", "computation"]}
{"title": "symmetry learning for function approximation in reinforcement learning", "abstract": "in this paper we explore methods to exploit symmetries for ensuring sample efficiency in reinforcement learning ( rl ) , this problem deserves ever increasing attention with the recent advances in the use of deep networks for complex rl tasks which require large amount of training data . we introduce a novel method to detect symmetries using reward trails observed during episodic experience and prove its completeness . we also provide a framework to incorporate the discovered symmetries for functional approximation . finally we show that the use of potential based reward shaping is especially effective for our symmetry exploitation mechanism . experiments on various classical problems show that our method improves the learning performance significantly by utilizing symmetry information .", "topics": ["test set", "reinforcement learning"]}
{"title": "general problem solving with category theory", "abstract": "this paper proposes a formal cognitive framework for problem solving based on category theory . we introduce cognitive categories , which are categories with exactly one morphism between any two objects . objects in these categories are interpreted as states and morphisms as transformations between states . moreover , cognitive problems are reduced to the specification of two objects in a cognitive category : an outset ( i.e . the current state of the system ) and a goal ( i.e . the desired state ) . cognitive systems transform the target system by means of generators and evaluators . generators realize cognitive operations over a system by grouping morphisms , whilst evaluators group objects as a way to generalize outsets and goals to partially defined states . meta-cognition emerges when the whole cognitive system is self-referenced as sub-states in the cognitive category , whilst learning must always be considered as a meta-cognitive process to maintain consistency . several examples grounded in basic ai methods are provided as well .", "topics": ["artificial intelligence"]}
{"title": "a correspondence between random neural networks and statistical field theory", "abstract": "a number of recent papers have provided evidence that practical design questions about neural networks may be tackled theoretically by studying the behavior of random networks . however , until now the tools available for analyzing random neural networks have been relatively ad-hoc . in this work , we show that the distribution of pre-activations in random neural networks can be exactly mapped onto lattice models in statistical physics . we argue that several previous investigations of stochastic networks actually studied a particular factorial approximation to the full lattice model . for random linear networks and random rectified linear networks we show that the corresponding lattice models in the wide network limit may be systematically approximated by a gaussian distribution with covariance between the layers of the network . in each case , the approximate distribution can be diagonalized by fourier transformation . we show that this approximation accurately describes the results of numerical simulations of wide random neural networks . finally , we demonstrate that in each case the large scale behavior of the random networks can be approximated by an effective field theory .", "topics": ["neural networks", "numerical analysis"]}
{"title": "comparison of brain networks with unknown correspondences", "abstract": "graph theory has drawn a lot of attention in the field of neuroscience during the last decade , mainly due to the abundance of tools that it provides to explore the interactions of elements in a complex network like the brain . the local and global organization of a brain network can shed light on mechanisms of complex cognitive functions , while disruptions within the network can be linked to neurodevelopmental disorders . in this effort , the construction of a representative brain network for each individual is critical for further analysis . additionally , graph comparison is an essential step for inference and classification analyses on brain graphs . in this work we explore a method based on graph edit distance for evaluating graph similarity , when correspondences between network elements are unknown due to different underlying subdivisions of the brain . we test this method on 30 unrelated subjects as well as 40 twin pairs and show that this method can accurately reflect the higher similarity between two related networks compared to unrelated ones , while identifying node correspondences .", "topics": ["interaction"]}
{"title": "modeling attention in panoramic video : a deep reinforcement learning approach", "abstract": "panoramic video provides immersive and interactive experience by enabling humans to control the field of view ( fov ) through head movement ( hm ) . thus , hm plays a key role in modeling human attention on panoramic video . this paper establishes a database collecting subjects ' hm positions on panoramic video sequences . from this database , we find that the hm data are highly consistent across subjects . furthermore , we find that deep reinforcement learning ( drl ) can be applied to predict hm positions , via maximizing the reward of imitating human hm scanpaths through the agent 's actions . based on our findings , we propose a drl based hm prediction ( dhp ) approach with offline and online versions , called offline-dhp and online-dhp . in offline-dhp , multiple drl workflows are run to determine potential hm positions at each panoramic frame . then , a heat map of the potential hm positions , named the hm map , is generated as the output of offline-dhp . in online-dhp , the next hm position of one subject is estimated given the currently observed hm position , which is achieved by developing a drl algorithm upon the learned offline-dhp model . finally , the experimental results validate that our approach is effective in offline and online prediction of hm positions for panoramic video , and that the learned offline-dhp model can improve the performance of online-dhp .", "topics": ["reinforcement learning", "database"]}
{"title": "neural machine translation and sequence-to-sequence models : a tutorial", "abstract": "this tutorial introduces a new and powerful set of techniques variously called `` neural machine translation '' or `` neural sequence-to-sequence models '' . these techniques have been used in a number of tasks regarding the handling of human language , and can be a powerful tool in the toolbox of anyone who wants to model sequential data of some sort . the tutorial assumes that the reader knows the basics of math and programming , but does not assume any particular experience with neural networks or natural language processing . it attempts to explain the intuition behind the various methods covered , then delves into them with enough mathematical detail to understand them concretely , and culiminates with a suggestion for an implementation exercise , where readers can test that they understood the content in practice .", "topics": ["natural language processing", "machine translation"]}
{"title": "narrowing the modeling gap : a cluster-ranking approach to coreference resolution", "abstract": "traditional learning-based coreference resolvers operate by training the mention-pair model for determining whether two mentions are coreferent or not . though conceptually simple and easy to understand , the mention-pair model is linguistically rather unappealing and lags far behind the heuristic-based coreference models proposed in the pre-statistical nlp era in terms of sophistication . two independent lines of recent research have attempted to improve the mention-pair model , one by acquiring the mention-ranking model to rank preceding mentions for a given anaphor , and the other by training the entity-mention model to determine whether a preceding cluster is coreferent with a given mention . we propose a cluster-ranking approach to coreference resolution , which combines the strengths of the mention-ranking model and the entity-mention model , and is therefore theoretically more appealing than both of these models . in addition , we seek to improve cluster rankers via two extensions : ( 1 ) lexicalization and ( 2 ) incorporating knowledge of anaphoricity by jointly modeling anaphoricity determination and coreference resolution . experimental results on the ace data sets demonstrate the superior performance of cluster rankers to competing approaches as well as the effectiveness of our two extensions .", "topics": ["natural language processing", "heuristic"]}
{"title": "connotation frames : a data-driven investigation", "abstract": "through a particular choice of a predicate ( e.g . , `` x violated y '' ) , a writer can subtly connote a range of implied sentiments and presupposed facts about the entities x and y : ( 1 ) writer 's perspective : projecting x as an `` antagonist '' and y as a `` victim '' , ( 2 ) entities ' perspective : y probably dislikes x , ( 3 ) effect : something bad happened to y , ( 4 ) value : y is something valuable , and ( 5 ) mental state : y is distressed by the event . we introduce connotation frames as a representation formalism to organize these rich dimensions of connotation using typed relations . first , we investigate the feasibility of obtaining connotative labels through crowdsourcing experiments . we then present models for predicting the connotation frames of verb predicates based on their distributional word representations and the interplay between different types of connotative relations . empirical results confirm that connotation frames can be induced from various data sources that reflect how people use language and give rise to the connotative meanings . we conclude with analytical results that show the potential use of connotation frames for analyzing subtle biases in online news media .", "topics": ["entity"]}
{"title": "frame-semantic parsing with softmax-margin segmental rnns and a syntactic scaffold", "abstract": "we present a new , efficient frame-semantic parser that labels semantic arguments to framenet predicates . built using an extension to the segmental rnn that emphasizes recall , our basic system achieves competitive performance without any calls to a syntactic parser . we then introduce a method that uses phrase-syntactic annotations from the penn treebank during training only , through a multitask objective ; no parsing is required at training or test time . this `` syntactic scaffold '' offers a cheaper alternative to traditional syntactic pipelining , and achieves state-of-the-art performance .", "topics": ["parsing"]}
{"title": "speech repairs , intonational boundaries and discourse markers : modeling speakers ' utterances in spoken dialog", "abstract": "in this thesis , we present a statistical language model for resolving speech repairs , intonational boundaries and discourse markers . rather than finding the best word interpretation for an acoustic signal , we redefine the speech recognition problem to so that it also identifies the pos tags , discourse markers , speech repairs and intonational phrase endings ( a major cue in determining utterance units ) . adding these extra elements to the speech recognition problem actually allows it to better predict the words involved , since we are able to make use of the predictions of boundary tones , discourse markers and speech repairs to better account for what word will occur next . furthermore , we can take advantage of acoustic information , such as silence information , which tends to co-occur with speech repairs and intonational phrase endings , that current language models can only regard as noise in the acoustic signal . the output of this language model is a much fuller account of the speaker 's turn , with part-of-speech assigned to each word , intonation phrase endings and discourse markers identified , and speech repairs detected and corrected . in fact , the identification of the intonational phrase endings , discourse markers , and resolution of the speech repairs allows the speech recognizer to model the speaker 's utterances , rather than simply the words involved , and thus it can return a more meaningful analysis of the speaker 's turn for later processing .", "topics": ["natural language", "speech recognition"]}
{"title": "optimal parameter selection for unsupervised neural network using genetic algorithm", "abstract": "k-means fast learning artificial neural network ( k-flann ) is an unsupervised neural network requires two parameters : tolerance and vigilance . best clustering results are feasible only by finest parameters specified to the neural network . selecting optimal values for these parameters is a major problem . to solve this issue , genetic algorithm ( ga ) is used to determine optimal parameters of k-flann for finding groups in multidimensional data . k-flann is a simple topological network , in which output nodes grows dynamically during the clustering process on receiving input patterns . original k-flann is enhanced to select winner unit out of the matched nodes so that stable clusters are formed with in a less number of epochs . the experimental results show that the ga is efficient in finding optimal values of parameters from the large search space and is tested using artificial and synthetic data sets .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "loyalty in online communities", "abstract": "loyalty is an essential component of multi-community engagement . when users have the choice to engage with a variety of different communities , they often become loyal to just one , focusing on that community at the expense of others . however , it is unclear how loyalty is manifested in user behavior , or whether loyalty is encouraged by certain community characteristics . in this paper we operationalize loyalty as a user-community relation : users loyal to a community consistently prefer it over all others ; loyal communities retain their loyal users over time . by exploring this relation using a large dataset of discussion communities from reddit , we reveal that loyalty is manifested in remarkably consistent behaviors across a wide spectrum of communities . loyal users employ language that signals collective identity and engage with more esoteric , less popular content , indicating they may play a curational role in surfacing new material . loyal communities have denser user-user interaction networks and lower rates of triadic closure , suggesting that community-level loyalty is associated with more cohesive interactions and less fragmentation into subgroups . we exploit these general patterns to predict future rates of loyalty . our results show that a user 's propensity to become loyal is apparent from their first interactions with a community , suggesting that some users are intrinsically loyal from the very beginning .", "topics": ["interaction"]}
{"title": "on affinity measures for artificial immune system movie recommenders", "abstract": "we combine artificial immune systems 'ais ' , technology with collaborative filtering 'cf ' and use it to build a movie recommendation system . we already know that artificial immune systems work well as movie recommenders from previous work by cayzer and aickelin 3 , 4 , 5 . here our aim is to investigate the effect of different affinity measure algorithms for the ais . two different affinity measures , kendalls tau and weighted kappa , are used to calculate the correlation coefficients for the movie recommender . we compare the results with those published previously and show that weighted kappa is more suitable than others for movie problems . we also show that ais are generally robust movie recommenders and that , as long as a suitable affinity measure is chosen , results are good .", "topics": ["coefficient"]}
{"title": "learning type-driven tensor-based meaning representations", "abstract": "this paper investigates the learning of 3rd-order tensors representing the semantics of transitive verbs . the meaning representations are part of a type-driven tensor-based semantic framework , from the newly emerging field of compositional distributional semantics . standard techniques from the neural networks literature are used to learn the tensors , which are tested on a selectional preference-style task with a simple 2-dimensional sentence space . promising results are obtained against a competitive corpus-based baseline . we argue that extending this work beyond transitive verbs , and to higher-dimensional sentence spaces , is an interesting and challenging problem for the machine learning community to consider .", "topics": ["baseline ( configuration management )"]}
{"title": "inverted bilingual topic models for lexicon extraction from non-parallel data", "abstract": "topic models have been successfully applied in lexicon extraction . however , most previous methods are limited to document-aligned data . in this paper , we try to address two challenges of applying topic models to lexicon extraction in non-parallel data : 1 ) hard to model the word relationship and 2 ) noisy seed dictionary . to solve these two challenges , we propose two new bilingual topic models to better capture the semantic information of each word while discriminating the multiple translations in a noisy seed dictionary . we extend the scope of topic models by inverting the roles of `` word '' and `` document '' . in addition , to solve the problem of noise in seed dictionary , we incorporate the probability of translation selection in our models . moreover , we also propose an effective measure to evaluate the similarity of words in different languages and select the optimal translation pairs . experimental results using real world data demonstrate the utility and efficacy of the proposed models .", "topics": ["dictionary"]}
{"title": "segdensenet : iris segmentation for pre and post cataract surgery", "abstract": "cataract is caused due to various factors such as age , trauma , genetics , smoking and substance consumption , and radiation . it is one of the major common ophthalmic diseases worldwide which can potentially affect iris-based biometric systems . india , which hosts the largest biometrics project in the world , has about 8 million people undergoing cataract surgery annually . while existing research shows that cataract does not have a major impact on iris recognition , our observations suggest that the iris segmentation approaches are not well equipped to handle cataract or post cataract surgery cases . therefore , failure in iris segmentation affects the overall recognition performance . this paper presents an efficient iris segmentation algorithm with variations due to cataract and post cataract surgery . the proposed algorithm , termed as segdensenet , is a deep learning algorithm based on densenets . the experiments on the iiitd cataract database show that improving the segmentation enhances the identification by up to 25 % across different sensors and matchers .", "topics": ["sensor"]}
{"title": "the digital restoration of da vinci 's sketches", "abstract": "a sketch , found in one of leonardo da vinci 's notebooks and covered by the written notes of this genius , has been recently restored . the restoration reveals a possible self-portrait of the artist , drawn when he was young . here , we discuss the discovery of this self-portrait and the procedure used for restoration . actually , this is a restoration performed on the digital image of the sketch , a procedure that can easily extended and applied to ancient documents for studies of art and palaeography .", "topics": ["image processing", "iteration"]}
{"title": "deep local binary patterns", "abstract": "local binary pattern ( lbp ) is a traditional descriptor for texture analysis that gained attention in the last decade . being robust to several properties such as invariance to illumination translation and scaling , lbps achieved state-of-the-art results in several applications . however , lbps are not able to capture high-level features from the image , merely encoding features with low abstraction levels . in this work , we propose deep lbp , which borrow ideas from the deep learning community to improve lbp expressiveness . by using parametrized data-driven lbp , we enable successive applications of the lbp operators with increasing abstraction levels . we validate the relevance of the proposed idea in several datasets from a wide range of applications . deep lbp improved the performance of traditional and multiscale lbp in all cases .", "topics": ["high- and low-level", "relevance"]}
{"title": "sparsity , variance and curvature in multi-armed bandits", "abstract": "in ( online ) learning theory the concepts of sparsity , variance and curvature are well-understood and are routinely used to obtain refined regret and generalization bounds . in this paper we further our understanding of these concepts in the more challenging limited feedback scenario . we consider the adversarial multi-armed bandit and linear bandit settings and solve several open problems pertaining to the existence of algorithms with favorable regret bounds under the following assumptions : ( i ) sparsity of the individual losses , ( ii ) small variation of the loss sequence , and ( iii ) curvature of the action set . specifically we show that ( i ) for $ s $ -sparse losses one can obtain $ \\tilde { o } ( \\sqrt { s t } ) $ -regret ( solving an open problem by kwon and perchet ) , ( ii ) for loss sequences with variation bounded by $ q $ one can obtain $ \\tilde { o } ( \\sqrt { q } ) $ -regret ( solving an open problem by kale and hazan ) , and ( iii ) for linear bandit on an $ \\ell_p^n $ ball one can obtain $ \\tilde { o } ( \\sqrt { n t } ) $ -regret for $ p \\in [ 1,2 ] $ and one has $ \\tilde { \\omega } ( n \\sqrt { t } ) $ -regret for $ p > 2 $ ( solving an open problem by bubeck , cesa-bianchi and kakade ) . a key new insight to obtain these results is to use regularizers satisfying more refined conditions than general self-concordance", "topics": ["regret ( decision theory )", "sparse matrix"]}
{"title": "transa : an adaptive approach for knowledge graph embedding", "abstract": "knowledge representation is a major topic in ai , and many studies attempt to represent entities and relations of knowledge base in a continuous vector space . among these attempts , translation-based methods build entity and relation vectors by minimizing the translation loss from a head entity to a tail one . in spite of the success of these methods , translation-based methods also suffer from the oversimplified loss metric , and are not competitive enough to model various and complex entities/relations in knowledge bases . to address this issue , we propose \\textbf { transa } , an adaptive metric approach for embedding , utilizing the metric learning ideas to provide a more flexible embedding method . experiments are conducted on the benchmark datasets and our proposed method makes significant and consistent improvements over the state-of-the-art baselines .", "topics": ["baseline ( configuration management )", "entity"]}
{"title": "deep spatial feature reconstruction for partial person re-identification : alignment-free approach", "abstract": "partial person re-identification ( re-id ) is a challenging problem , where only some partial observations ( images ) of persons are available for matching . however , few studies have offered a flexible solution of how to identify an arbitrary patch of a person image . in this paper , we propose a fast and accurate matching method to address this problem . the proposed method leverages fully convolutional network ( fcn ) to generate certain-sized spatial feature maps such that pixel-level features are consistent . to match a pair of person images of different sizes , hence , a novel method called deep spatial feature reconstruction ( dsr ) is further developed to avoid explicit alignment . specifically , dsr exploits the reconstructing error from popular dictionary learning models to calculate the similarity between different spatial feature maps . in that way , we expect that the proposed fcn can decrease the similarity of coupled images from different persons and increase that of coupled images from the same person . experimental results on two partial person datasets demonstrate the efficiency and effectiveness of the proposed method in comparison with several state-of-the-art partial person re-id approaches . additionally , it achieves competitive results on a benchmark person dataset market1501 with the rank-1 accuracy being 83.58 % .", "topics": ["map", "pixel"]}
{"title": "graphrnn : a deep generative model for graphs", "abstract": "modeling and generating graphs is fundamental for studying networks in biology , engineering , and social sciences . however , modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique , high-dimensional nature of graphs and the complex , non-local dependencies that exist between edges in a given graph . here we propose graphrnn , a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure . graphrnn learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations , conditioned on the graph structure generated so far . in order to quantitatively evaluate the performance of graphrnn , we introduce a benchmark suite of datasets , baselines and novel evaluation metrics based on maximum mean discrepancy , which measure distances between sets of graphs . our experiments show that graphrnn significantly outperforms all baselines , learning to generate diverse graphs that match the structural characteristics of a target set , while also scaling to graphs 50 times larger than previous deep models .", "topics": ["sampling ( signal processing )", "generative model"]}
{"title": "generative moment matching networks", "abstract": "we consider the problem of learning deep generative models from data . we formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron , as in the recently proposed generative adversarial networks ( goodfellow et al . , 2014 ) . training a generative adversarial network , however , requires careful optimization of a difficult minimax program . instead , we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy ( mmd ) , which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model , and can be trained by backpropagation . we further boost the performance of this approach by combining our generative network with an auto-encoder network , using mmd to learn to generate codes that can then be decoded to produce samples . we show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on mnist and the toronto face database .", "topics": ["baseline ( configuration management )", "generative model"]}
{"title": "dynamic deep neural networks : optimizing accuracy-efficiency trade-offs by selective execution", "abstract": "we introduce dynamic deep neural networks ( d2nn ) , a new type of feed-forward deep neural network that allows selective execution . given an input , only a subset of d2nn neurons are executed , and the particular subset is determined by the d2nn itself . by pruning unnecessary computation depending on input , d2nns provide a way to improve computational efficiency . to achieve dynamic selective execution , a d2nn augments a feed-forward deep neural network ( directed acyclic graph of differentiable modules ) with controller modules . each controller module is a sub-network whose output is a decision that controls whether other modules can execute . a d2nn is trained end to end . both regular and controller modules in a d2nn are learnable and are jointly trained to optimize both accuracy and efficiency . such training is achieved by integrating backpropagation with reinforcement learning . with extensive experiments of various d2nn architectures on image classification tasks , we demonstrate that d2nns are general and flexible , and can effectively optimize accuracy-efficiency trade-offs .", "topics": ["neural networks", "reinforcement learning"]}
{"title": "medical text classification using convolutional neural networks", "abstract": "we present an approach to automatically classify clinical text at a sentence level . we are using deep convolutional neural networks to represent complex features . we train the network on a dataset providing a broad categorization of health information . through a detailed evaluation , we demonstrate that our method outperforms several approaches widely used in natural language processing tasks by about 15 % .", "topics": ["natural language processing", "neural networks"]}
{"title": "variational methods for normal integration", "abstract": "the need for an efficient method of integration of a dense normal field is inspired by several computer vision tasks , such as shape-from-shading , photometric stereo , deflectometry , etc . inspired by edge-preserving methods from image processing , we study in this paper several variational approaches for normal integration , with a focus on non-rectangular domains , free boundary and depth discontinuities . we first introduce a new discretization for quadratic integration , which is designed to ensure both fast recovery and the ability to handle non-rectangular domains with a free boundary . yet , with this solver , discontinuous surfaces can be handled only if the scene is first segmented into pieces without discontinuity . hence , we then discuss several discontinuity-preserving strategies . those inspired , respectively , by the mumford-shah segmentation method and by anisotropic diffusion , are shown to be the most effective for recovering discontinuities .", "topics": ["calculus of variations", "image processing"]}
{"title": "resilience : a criterion for learning in the presence of arbitrary outliers", "abstract": "we introduce a criterion , resilience , which allows properties of a dataset ( such as its mean or best low rank approximation ) to be robustly computed , even in the presence of a large fraction of arbitrary additional data . resilience is a weaker condition than most other properties considered so far in the literature , and yet enables robust estimation in a broader variety of settings . we provide new information-theoretic results on robust distribution learning , robust estimation of stochastic block models , and robust mean estimation under bounded $ k $ th moments . we also provide new algorithmic results on robust distribution learning , as well as robust mean estimation in $ \\ell_p $ -norms . among our proof techniques is a method for pruning a high-dimensional distribution with bounded $ 1 $ st moments to a stable `` core '' with bounded $ 2 $ nd moments , which may be of independent interest .", "topics": ["eisenstein 's criterion"]}
{"title": "compensating interpolation distortion by using new optimized modular method", "abstract": "a modular method was suggested before to recover a band limited signal from the sample and hold and linearly interpolated ( or , in general , an nth-order-hold ) version of the regular samples . in this paper a novel approach for compensating the distortion of any interpolation based on modular method has been proposed . in this method the performance of the modular method is optimized by adding only some simply calculated coefficients . this approach causes drastic improvement in terms of signal-to-noise ratios with fewer modules compared to the classical modular method . simulation results clearly confirm the improvement of the proposed method and also its superior robustness against additive noise .", "topics": ["simulation", "coefficient"]}
{"title": "markov blanket ranking using kernel-based conditional dependence measures", "abstract": "developing feature selection algorithms that move beyond a pure correlational to a more causal analysis of observational data is an important problem in the sciences . several algorithms attempt to do so by discovering the markov blanket of a target , but they all contain a forward selection step which variables must pass in order to be included in the conditioning set . as a result , these algorithms may not consider all possible conditional multivariate combinations . we improve on this limitation by proposing a backward elimination method that uses a kernel-based conditional dependence measure to identify the markov blanket in a fully multivariate fashion . the algorithm is easy to implement and compares favorably to other methods on synthetic and real datasets .", "topics": ["synthetic data", "markov chain"]}
{"title": "computational models : bottom-up and top-down aspects", "abstract": "computational models of visual attention have become popular over the past decade , we believe primarily for two reasons : first , models make testable predictions that can be explored by experimentalists as well as theoreticians , second , models have practical and technological applications of interest to the applied science and engineering communities . in this chapter , we take a critical look at recent attention modeling efforts . we focus on { \\em computational models of attention } as defined by tsotsos \\ & rothenstein \\shortcite { tsotsos_rothenstein11 } : models which can process any visual stimulus ( typically , an image or video clip ) , which can possibly also be given some task definition , and which make predictions that can be compared to human or animal behavioral or physiological responses elicited by the same stimulus and task . thus , we here place less emphasis on abstract models , phenomenological models , purely data-driven fitting or extrapolation models , or models specifically designed for a single task or for a restricted class of stimuli . for theoretical models , we refer the reader to a number of previous reviews that address attention theories and models more generally \\cite { itti_koch01nrn , paletta_etal05 , frintrop_etal10 , rothenstein_tsotsos08 , gottlieb_balan10 , toet11 , borji_itti12pami } .", "topics": ["approximation", "computation"]}
{"title": "automated identification of drug-drug interactions in pediatric congestive heart failure patients", "abstract": "congestive heart failure , or chf , is a serious medical condition that can result in fluid buildup in the body as a result of a weak heart . when the heart ca n't pump enough blood to efficiently deliver nutrients and oxygen to the body , kidney function may be impaired , resulting in fluid retention . chf patients require a broad drug regimen to maintain the delicate system balance , particularly between their heart and kidneys . these drugs include ace inhibitors and beta blockers to control blood pressure , anticoagulants to prevent blood clots , and diuretics to reduce fluid overload . many of these drugs may interact , and potential effects of these interactions must be weighed against their benefits . for this project , we consider a set of 44 drugs identified as specifically relevant for treating chf by pediatric cardiologists at lucile packard children 's hospital . this list was generated as part of our current work at the lpch heart center . the goal of this project is to identify and evaluate potentially harmful drug-drug interactions ( ddis ) within pediatric patients with congestive heart failure . this identification will be done autonomously , so that it may continuously update by evaluating newly published literature .", "topics": ["interaction"]}
{"title": "parsimonious topic models with salient word discovery", "abstract": "we propose a parsimonious topic model for text corpora . in related models such as latent dirichlet allocation ( lda ) , all words are modeled topic-specifically , even though many words occur with similar frequencies across different topics . our modeling determines salient words for each topic , which have topic-specific probabilities , with the rest explained by a universal shared model . further , in lda all topics are in principle present in every document . by contrast our model gives sparse topic representation , determining the ( small ) subset of relevant topics for each document . we derive a bayesian information criterion ( bic ) , balancing model complexity and goodness of fit . here , interestingly , we identify an effective sample size and corresponding penalty specific to each parameter type in our model . we minimize bic to jointly determine our entire model -- the topic-specific words , document-specific topics , all model parameter values , { \\it and } the total number of topics -- in a wholly unsupervised fashion . results on three text corpora and an image dataset show that our model achieves higher test set likelihood and better agreement with ground-truth class labels , compared to lda and to a model designed to incorporate sparsity .", "topics": ["test set", "value ( ethics )"]}
{"title": "deep perceptual mapping for thermal to visible face recognition", "abstract": "cross modal face matching between the thermal and visible spectrum is a much de- sired capability for night-time surveillance and security applications . due to a very large modality gap , thermal-to-visible face recognition is one of the most challenging face matching problem . in this paper , we present an approach to bridge this modality gap by a significant margin . our approach captures the highly non-linear relationship be- tween the two modalities by using a deep neural network . our model attempts to learn a non-linear mapping from visible to thermal spectrum while preserving the identity in- formation . we show substantive performance improvement on a difficult thermal-visible face dataset . the presented approach improves the state-of-the-art by more than 10 % in terms of rank-1 identification and bridge the drop in performance due to the modality gap by more than 40 % .", "topics": ["nonlinear system"]}
{"title": "gradient-enhanced kriging for high-dimensional problems", "abstract": "surrogate models provide a low computational cost alternative to evaluating expensive functions . the construction of accurate surrogate models with large numbers of independent variables is currently prohibitive because it requires a large number of function evaluations . gradient-enhanced kriging has the potential to reduce the number of function evaluations for the desired accuracy when efficient gradient computation , such as an adjoint method , is available . however , current gradient-enhanced kriging methods do not scale well with the number of sampling points due to the rapid growth in the size of the correlation matrix where new information is added for each sampling point in each direction of the design space . they do not scale well with the number of independent variables either due to the increase in the number of hyperparameters that needs to be estimated . to address this issue , we develop a new gradient-enhanced surrogate model approach that drastically reduced the number of hyperparameters through the use of the partial-least squares method that maintains accuracy . in addition , this method is able to control the size of the correlation matrix by adding only relevant points defined through the information provided by the partial-least squares method . to validate our method , we compare the global accuracy of the proposed method with conventional kriging surrogate models on two analytic functions with up to 100 dimensions , as well as engineering problems of varied complexity with up to 15 dimensions . we show that the proposed method requires fewer sampling points than conventional methods to obtain the desired accuracy , or provides more accuracy for a fixed budget of sampling points . in some cases , we get over 3 times more accurate models than a bench of surrogate models from the literature , and also over 3200 times faster than standard gradient-enhanced kriging models .", "topics": ["sampling ( signal processing )", "computation"]}
{"title": "going deeper in spiking neural networks : vgg and residual architectures", "abstract": "over the past few years , spiking neural networks ( snns ) have become popular as a possible pathway to enable low-power event-driven neuromorphic hardware . however , their application in machine learning have largely been limited to very shallow neural network architectures for simple problems . in this paper , we propose a novel algorithmic technique for generating an snn with a deep architecture , and demonstrate its effectiveness on complex visual recognition problems such as cifar-10 and imagenet . our technique applies to both vgg and residual network architectures , with significantly better accuracy than the state-of-the-art . finally , we present analysis of the sparse event-driven computations to demonstrate reduced hardware overhead when operating in the spiking domain .", "topics": ["neural networks", "computation"]}
{"title": "towards a better understanding of the local attractor in particle swarm optimization : speed and solution quality", "abstract": "particle swarm optimization ( pso ) is a popular nature-inspired meta-heuristic for solving continuous optimization problems . although this technique is widely used , the understanding of the mechanisms that make swarms so successful is still limited . we present the first substantial experimental investigation of the influence of the local attractor on the quality of exploration and exploitation . we compare in detail classical pso with the social-only variant where local attractors are ignored . to measure the exploration capabilities , we determine how frequently both variants return results in the neighborhood of the global optimum . we measure the quality of exploitation by considering only function values from runs that reached a search point sufficiently close to the global optimum and then comparing in how many digits such values still deviate from the global minimum value . it turns out that the local attractor significantly improves the exploration , but sometimes reduces the quality of the exploitation . as a compromise , we propose and evaluate a hybrid pso which switches off its local attractors at a certain point in time . the effects mentioned can also be observed by measuring the potential of the swarm .", "topics": ["value ( ethics )", "heuristic"]}
{"title": "learning discrete distributions from untrusted batches", "abstract": "we consider the problem of learning a discrete distribution in the presence of an $ \\epsilon $ fraction of malicious data sources . specifically , we consider the setting where there is some underlying distribution , $ p $ , and each data source provides a batch of $ \\ge k $ samples , with the guarantee that at least a $ ( 1-\\epsilon ) $ fraction of the sources draw their samples from a distribution with total variation distance at most $ \\eta $ from $ p $ . we make no assumptions on the data provided by the remaining $ \\epsilon $ fraction of sources -- this data can even be chosen as an adversarial function of the $ ( 1-\\epsilon ) $ fraction of `` good '' batches . we provide two algorithms : one with runtime exponential in the support size , $ n $ , but polynomial in $ k $ , $ 1/\\epsilon $ and $ 1/\\eta $ that takes $ o ( ( n+k ) /\\epsilon^2 ) $ batches and recovers $ p $ to error $ o ( \\eta+\\epsilon/\\sqrt { k } ) $ . this recovery accuracy is information theoretically optimal , to constant factors , even given an infinite number of data sources . our second algorithm applies to the $ \\eta = 0 $ setting and also achieves an $ o ( \\epsilon/\\sqrt { k } ) $ recover guarantee , though it runs in $ \\mathrm { poly } ( ( nk ) ^k ) $ time . this second algorithm , which approximates a certain tensor via a rank-1 tensor minimizing $ \\ell_1 $ distance , is surprising in light of the hardness of many low-rank tensor approximation problems , and may be of independent interest .", "topics": ["time complexity", "polynomial"]}
{"title": "contrast enhancement estimation for digital image forensics", "abstract": "inconsistency in contrast enhancement can be used to expose image forgeries . in this work , we describe a new method to estimate contrast enhancement from a single image . our method takes advantage of the nature of contrast enhancement as a mapping between pixel values , and the distinct characteristics it introduces to the image pixel histogram . our method recovers the original pixel histogram and the contrast enhancement simultaneously from a single image with an iterative algorithm . unlike previous methods , our method is robust in the presence of additive noise perturbations that are used to hide the traces of contrast enhancement . furthermore , we also develop an e effective method to to detect image regions undergone contrast enhancement transformations that are different from the rest of the image , and use this method to detect composite images . we perform extensive experimental evaluations to demonstrate the efficacy and efficiency of our method method .", "topics": ["pixel"]}
{"title": "non-quadratic convex regularized reconstruction of mr images from spiral acquisitions", "abstract": "combining fast mr acquisition sequences and high resolution imaging is a major issue in dynamic imaging . reducing the acquisition time can be achieved by using non-cartesian and sparse acquisitions . the reconstruction of mr images from these measurements is generally carried out using gridding that interpolates the missing data to obtain a dense cartesian k-space filling . the mr image is then reconstructed using a conventional fast fourier transform . the estimation of the missing data unavoidably introduces artifacts in the image that remain difficult to quantify . a general reconstruction method is proposed to take into account these limitations . it can be applied to any sampling trajectory in k-space , cartesian or not , and specifically takes into account the exact location of the measured data , without making any interpolation of the missing data in k-space . information about the expected characteristics of the imaged object is introduced to preserve the spatial resolution and improve the signal to noise ratio in a regularization framework . the reconstructed image is obtained by minimizing a non-quadratic convex objective function . an original rewriting of this criterion is shown to strongly improve the reconstruction efficiency . results on simulated data and on a real spiral acquisition are presented and discussed .", "topics": ["sampling ( signal processing )", "simulation"]}
{"title": "pcanet-ii : when pcanet meets the second order pooling", "abstract": "pcanet , as one noticeable shallow network , employs the histogram representation for feature pooling . however , there are three main problems about this kind of pooling method . first , the histogram-based pooling method binarizes the feature maps and leads to inevitable discriminative information loss . second , it is difficult to effectively combine other visual cues into a compact representation , because the simple concatenation of various visual cues leads to feature representation inefficiency . third , the dimensionality of histogram-based output grows exponentially with the number of feature maps used . in order to overcome these problems , we propose a novel shallow network model , named as pcanet-ii . compared with the histogram-based output , the second order pooling not only provides more discriminative information by preserving both the magnitude and sign of convolutional responses , but also dramatically reduces the size of output features . thus we combine the second order statistical pooling method with the shallow network , i.e . , pcanet . moreover , it is easy to combine other discriminative and robust cues by using the second order pooling . so we introduce the binary feature difference encoding scheme into our pcanet-ii to further improve robustness . experiments demonstrate the effectiveness and robustness of our proposed pcanet-ii method .", "topics": ["map"]}
{"title": "deep class-wise hashing : semantics-preserving hashing via class-wise loss", "abstract": "deep supervised hashing has emerged as an influential solution to large-scale semantic image retrieval problems in computer vision . in the light of recent progress , convolutional neural network based hashing methods typically seek pair-wise or triplet labels to conduct the similarity preserving learning . however , complex semantic concepts of visual contents are hard to capture by similar/dissimilar labels , which limits the retrieval performance . generally , pair-wise or triplet losses not only suffer from expensive training costs but also lack in extracting sufficient semantic information . in this regard , we propose a novel deep supervised hashing model to learn more compact class-level similarity preserving binary codes . our deep learning based model is motivated by deep metric learning that directly takes semantic labels as supervised information in training and generates corresponding discriminant hashing code . specifically , a novel cubic constraint loss function based on gaussian distribution is proposed , which preserves semantic variations while penalizes the overlap part of different classes in the embedding space . to address the discrete optimization problem introduced by binary codes , a two-step optimization strategy is proposed to provide efficient training and avoid the problem of gradient vanishing . extensive experiments on four large-scale benchmark databases show that our model can achieve the state-of-the-art retrieval performance . moreover , when training samples are limited , our method surpasses other supervised deep hashing methods with non-negligible margins .", "topics": ["optimization problem", "computer vision"]}
{"title": "stochastic recursive gradient algorithm for nonconvex optimization", "abstract": "in this paper , we study and analyze the mini-batch version of stochastic recursive gradient algorithm ( sarah ) , a method employing the stochastic recursive gradient , for solving empirical loss minimization for the case of nonconvex losses . we provide a sublinear convergence rate ( to stationary points ) for general nonconvex functions and a linear convergence rate for gradient dominated functions , both of which have some advantages compared to other modern stochastic gradient algorithms for nonconvex losses .", "topics": ["gradient"]}
{"title": "information dropout : learning optimal representations through noisy computation", "abstract": "the cross-entropy loss commonly used in deep learning is closely related to the defining properties of optimal representations , but does not enforce some of the key properties . we show that this can be solved by adding a regularization term , which is in turn related to injecting multiplicative noise in the activations of a deep neural network , a special case of which is the common practice of dropout . we show that our regularized loss function can be efficiently minimized using information dropout , a generalization of dropout rooted in information theoretic principles that automatically adapts to the data and can better exploit architectures of limited capacity . when the task is the reconstruction of the input , we show that our loss function yields a variational autoencoder as a special case , thus providing a link between representation learning , information theory and variational inference . finally , we prove that we can promote the creation of disentangled representations simply by enforcing a factorized prior , a fact that has been observed empirically in recent work . our experiments validate the theoretical intuitions behind our method , and we find that information dropout achieves a comparable or better generalization performance than binary dropout , especially on smaller models , since it can automatically adapt the noise to the structure of the network , as well as to the test sample .", "topics": ["feature learning", "calculus of variations"]}
{"title": "exploiting strong convexity from data with primal-dual first-order algorithms", "abstract": "we consider empirical risk minimization of linear predictors with convex loss functions . such problems can be reformulated as convex-concave saddle point problems , and thus are well suitable for primal-dual first-order algorithms . however , primal-dual algorithms often require explicit strongly convex regularization in order to obtain fast linear convergence , and the required dual proximal mapping may not admit closed-form or efficient solution . in this paper , we develop both batch and randomized primal-dual algorithms that can exploit strong convexity from data adaptively and are capable of achieving linear convergence even without regularization . we also present dual-free variants of the adaptive primal-dual algorithms that do not require computing the dual proximal mapping , which are especially suitable for logistic regression .", "topics": ["matrix regularization", "loss function"]}
{"title": "a pursuit of temporal accuracy in general activity detection", "abstract": "detecting activities in untrimmed videos is an important but challenging task . the performance of existing methods remains unsatisfactory , e.g . , they often meet difficulties in locating the beginning and end of a long complex action . in this paper , we propose a generic framework that can accurately detect a wide variety of activities from untrimmed videos . our first contribution is a novel proposal scheme that can efficiently generate candidates with accurate temporal boundaries . the other contribution is a cascaded classification pipeline that explicitly distinguishes between relevance and completeness of a candidate instance . on two challenging temporal activity detection datasets , thumos14 and activitynet , the proposed framework significantly outperforms the existing state-of-the-art methods , demonstrating superior accuracy and strong adaptivity in handling activities with various temporal structures .", "topics": ["relevance"]}
{"title": "linearized kernel dictionary learning", "abstract": "in this paper we present a new approach of incorporating kernels into dictionary learning . the kernel k-svd algorithm ( kksvd ) , which has been introduced recently , shows an improvement in classification performance , with relation to its linear counterpart k-svd . however , this algorithm requires the storage and handling of a very large kernel matrix , which leads to high computational cost , while also limiting its use to setups with small number of training examples . we address these problems by combining two ideas : first we approximate the kernel matrix using a cleverly sampled subset of its columns using the nystr\\ '' { o } m method ; secondly , as we wish to avoid using this matrix altogether , we decompose it by svd to form new `` virtual samples , '' on which any linear dictionary learning can be employed . our method , termed `` linearized kernel dictionary learning '' ( lkdl ) can be seamlessly applied as a pre-processing stage on top of any efficient off-the-shelf dictionary learning scheme , effectively `` kernelizing '' it . we demonstrate the effectiveness of our method on several tasks of both supervised and unsupervised classification and show the efficiency of the proposed scheme , its easy integration and performance boosting properties .", "topics": ["kernel ( operating system )", "approximation algorithm"]}
{"title": "sparse coding and dictionary learning for symmetric positive definite matrices : a kernel approach", "abstract": "recent advances suggest that a wide range of computer vision problems can be addressed more appropriately by considering non-euclidean geometry . this paper tackles the problem of sparse coding and dictionary learning in the space of symmetric positive definite matrices , which form a riemannian manifold . with the aid of the recently introduced stein kernel ( related to a symmetric version of bregman matrix divergence ) , we propose to perform sparse coding by embedding riemannian manifolds into reproducing kernel hilbert spaces . this leads to a convex and kernel version of the lasso problem , which can be solved efficiently . we furthermore propose an algorithm for learning a riemannian dictionary ( used for sparse coding ) , closely tied to the stein kernel . experiments on several classification tasks ( face recognition , texture classification , person re-identification ) show that the proposed sparse coding approach achieves notable improvements in discrimination accuracy , in comparison to state-of-the-art methods such as tensor sparse coding , riemannian locality preserving projection , and symmetry-driven accumulation of local features .", "topics": ["kernel ( operating system )", "computer vision"]}
{"title": "minimal dependency translation : a framework for computer-assisted translation for under-resourced languages", "abstract": "this paper introduces minimal dependency translation ( mdt ) , an ongoing project to develop a rule-based framework for the creation of rudimentary bilingual lexicon-grammars for machine translation and computer-assisted translation into and out of under-resourced languages as well as initial steps towards an implementation of mdt for english-to-amharic translation . the basic units in mdt , called groups , are headed multi-item sequences . in addition to wordforms , groups may contain lexemes , syntactic-semantic categories , and grammatical features . each group is associated with one or more translations , each of which is a group in a target language . during translation , constraint satisfaction is used to select a set of source-language groups for the input sentence and to sequence the words in the associated target-language groups .", "topics": ["machine translation"]}
{"title": "simple search algorithms on semantic networks learned from language use", "abstract": "recent empirical and modeling research has focused on the semantic fluency task because it is informative about semantic memory . an interesting interplay arises between the richness of representations in semantic memory and the complexity of algorithms required to process it . it has remained an open question whether representations of words and their relations learned from language use can enable a simple search algorithm to mimic the observed behavior in the fluency task . here we show that it is plausible to learn rich representations from naturalistic data for which a very simple search algorithm ( a random walk ) can replicate the human patterns . we suggest that explicitly structuring knowledge about words into a semantic network plays a crucial role in modeling human behavior in memory search and retrieval ; moreover , this is the case across a range of semantic information sources .", "topics": ["computational complexity theory"]}
{"title": "qcri machine translation systems for iwslt 16", "abstract": "this paper describes qcri 's machine translation systems for the iwslt 2016 evaluation campaign . we participated in the arabic- > english and english- > arabic tracks . we built both phrase-based and neural machine translation models , in an effort to probe whether the newly emerged nmt framework surpasses the traditional phrase-based systems in arabic-english language pairs . we trained a very strong phrase-based system including , a big language model , the operation sequence model , neural network joint model and class-based models along with different domain adaptation techniques such as mml filtering , mixture modeling and using fine tuning over nnjm model . however , a neural mt system , trained by stacking data from different genres through fine-tuning , and applying ensemble over 8 models , beat our very strong phrase-based system by a significant 2 bleu points margin in arabic- > english direction . we did not obtain similar gains in the other direction but were still able to outperform the phrase-based system . we also applied system combination on phrase-based and nmt outputs .", "topics": ["machine translation"]}
{"title": "deep learning bank distress from news and numerical financial data", "abstract": "in this paper we focus our attention on the exploitation of the information contained in financial news to enhance the performance of a classifier of bank distress . such information should be analyzed and inserted into the predictive model in the most efficient way and this task deals with all the issues related to text analysis and specifically analysis of news media . among the different models proposed for such purpose , we investigate one of the possible deep learning approaches , based on a doc2vec representation of the textual data , a kind of neural network able to map the sequential and symbolic text input onto a reduced latent semantic space . afterwards , a second supervised neural network is trained combining news data with standard financial figures to classify banks whether in distressed or tranquil states , based on a small set of known distress events . then the final aim is not only the improvement of the predictive performance of the classifier but also to assess the importance of news data in the classification process . does news data really bring more useful information not contained in standard financial variables ? our results seem to confirm such hypothesis .", "topics": ["numerical analysis", "text corpus"]}
{"title": "3d human pose estimation in the wild by adversarial learning", "abstract": "recently , remarkable advances have been achieved in 3d human pose estimation from monocular images because of the powerful deep convolutional neural networks ( dcnns ) . despite their success on large-scale datasets collected in the constrained lab environment , it is difficult to obtain the 3d pose annotations for in-the-wild images . therefore , 3d human pose estimation in the wild is still a challenge . in this paper , we propose an adversarial learning framework , which distills the 3d human pose structures learned from the fully annotated dataset to in-the-wild images with only 2d pose annotations . instead of defining hard-coded rules to constrain the pose estimation results , we design a novel multi-source discriminator to distinguish the predicted 3d poses from the ground-truth , which helps to enforce the pose estimator to generate anthropometrically valid poses even with images in the wild . we also observe that a carefully designed information source for the discriminator is essential to boost the performance . thus , we design a geometric descriptor , which computes the pairwise relative locations and distances between body joints , as a new information source for the discriminator . the efficacy of our adversarial learning framework with the new geometric descriptor has been demonstrated through extensive experiments on widely used public benchmarks . our approach significantly improves the performance compared with previous state-of-the-art approaches .", "topics": ["neural networks"]}
{"title": "a framework for evaluating 6-dof object trackers", "abstract": "we present a challenging and realistic novel dataset for evaluating 6-dof object tracking algorithms . existing datasets show serious limitations -- -notably , unrealistic synthetic data , or real data with large fiducial markers -- -preventing the community from obtaining an accurate picture of the state-of-the-art . our key contribution is a novel pipeline for acquiring accurate ground truth poses of real objects w.r.t a kinect v2 sensor by using a commercial motion capture system . a total of 100 calibrated sequences of real objects are acquired in three different scenarios to evaluate the performance of trackers in various scenarios : stability , robustness to occlusion and accuracy during challenging interactions between a person and the object . we conduct an extensive study of a deep 6-dof tracking architecture and determine a set of optimal parameters . we enhance the architecture and the training methodology to train a 6-dof tracker that can robustly generalize to objects never seen during training , and demonstrate favorable performance compared to previous approaches trained specifically on the objects to track .", "topics": ["synthetic data", "interaction"]}
{"title": "spectrum estimation from samples", "abstract": "we consider the problem of approximating the set of eigenvalues of the covariance matrix of a multivariate distribution ( equivalently , the problem of approximating the `` population spectrum '' ) , given access to samples drawn from the distribution . the eigenvalues of the covariance of a distribution contain basic information about the distribution , including the presence or lack of structure in the distribution , the effective dimensionality of the distribution , and the applicability of higher-level machine learning and multivariate statistical tools . we consider this fundamental recovery problem in the regime where the number of samples is comparable , or even sublinear in the dimensionality of the distribution in question . first , we propose a theoretically optimal and computationally efficient algorithm for recovering the moments of the eigenvalues of the population covariance matrix . we then leverage this accurate moment recovery , via a wasserstein distance argument , to show that the vector of eigenvalues can be accurately recovered . we provide finite -- sample bounds on the expected error of the recovered eigenvalues , which imply that our estimator is asymptotically consistent as the dimensionality of the distribution and sample size tend towards infinity , even in the sublinear sample regime where the ratio of the sample size to the dimensionality tends to zero . in addition to our theoretical results , we show that our approach performs well in practice for a broad range of distributions and sample sizes .", "topics": ["approximation algorithm", "computational complexity theory"]}
{"title": "chi2tex semi-automatic translation from chiwriter to latex", "abstract": "semi-automatic translation of math-filled book from obsolete chiwriter format to latex . is it possible ? idea of criterion whether to use automatic or hand mode for translation . illustrations .", "topics": ["machine translation", "eisenstein 's criterion"]}
{"title": "stochastic zeroth-order optimization in high dimensions", "abstract": "we consider the problem of optimizing a high-dimensional convex function using stochastic zeroth-order queries . under sparsity assumptions on the gradients or function values , we present two algorithms : a successive component/feature selection algorithm and a noisy mirror descent algorithm using lasso gradient estimates , and show that both algorithms have convergence rates that de- pend only logarithmically on the ambient dimension of the problem . empirical results confirm our theoretical findings and show that the algorithms we design outperform classical zeroth-order optimization methods in the high-dimensional setting .", "topics": ["mathematical optimization", "sparse matrix"]}
{"title": "anomaly detection in clutter using spectrally enhanced ladar", "abstract": "discrete return ( dr ) laser detection and ranging ( ladar ) systems provide a series of echoes that reflect from objects in a scene . these can be first , last or multi-echo returns . in contrast , full-waveform ( fw ) -ladar systems measure the intensity of light reflected from objects continuously over a period of time . in a camouflaged scenario , e.g . , objects hidden behind dense foliage , a fw-ladar penetrates such foliage and returns a sequence of echoes including buried faint echoes . the aim of this paper is to learn local-patterns of co-occurring echoes characterised by their measured spectra . a deviation from such patterns defines an abnormal event in a forest/tree depth profile . as far as the authors know , neither dr or fw-ladar , along with several spectral measurements , has not been applied to anomaly detection . this work presents an algorithm that allows detection of spectral and temporal anomalies in fw-multi spectral ladar ( fw-msl ) data samples . an anomaly is defined as a full waveform temporal and spectral signature that does not conform to a prior expectation , represented using a learnt subspace ( dictionary ) and set of coefficients that capture co-occurring local-patterns using an overlapping temporal window . a modified optimization scheme is proposed for subspace learning based on stochastic approximations . the objective function is augmented with a discriminative term that represents the subspace 's separability properties and supports anomaly characterisation . the algorithm detects several man-made objects and anomalous spectra hidden in a dense clutter of vegetation and also allows tree species classification .", "topics": ["loss function", "approximation"]}
{"title": "analysis of japanese compound nouns using collocational information", "abstract": "analyzing compound nouns is one of the crucial issues for natural language processing systems , in particular for those systems that aim at a wide coverage of domains . in this paper , we propose a method to analyze structures of japanese compound nouns by using both word collocations statistics and a thesaurus . an experiment is conducted with 160,000 word collocations to analyze compound nouns of with an average length of 4.9 characters . the accuracy of this method is about 80 % .", "topics": ["natural language processing", "text corpus"]}
{"title": "quality expectations of machine translation", "abstract": "machine translation ( mt ) is being deployed for a range of use-cases by millions of people on a daily basis . there should , therefore , be no doubt as to the utility of mt . however , not everyone is convinced that mt can be useful , especially as a productivity enhancer for human translators . in this chapter , i address this issue , describing how mt is currently deployed , how its output is evaluated and how this could be enhanced , especially as mt quality itself improves . central to these issues is the acceptance that there is no longer a single 'gold standard ' measure of quality , such that the situation in which mt is deployed needs to be borne in mind , especially with respect to the expected 'shelf-life ' of the translation itself .", "topics": ["machine translation"]}
{"title": "supervised learning with quantum-inspired tensor networks", "abstract": "tensor networks are efficient representations of high-dimensional tensors which have been very successful for physics and mathematics applications . we demonstrate how algorithms for optimizing such networks can be adapted to supervised learning tasks by using matrix product states ( tensor trains ) to parameterize models for classifying images . for the mnist data set we obtain less than 1 % test set classification error . we discuss how the tensor network form imparts additional structure to the learned model and suggest a possible generative interpretation .", "topics": ["test set", "supervised learning"]}
{"title": "blind image deblurring by spectral properties of convolution operators", "abstract": "in this paper , we study the problem of recovering a sharp version of a given blurry image when the blur kernel is unknown . previous methods often introduce an image-independent regularizer ( such as gaussian or sparse priors ) on the desired blur kernel . we shall show that the blurry image itself encodes rich information about the blur kernel . such information can be found through analyzing and comparing how the spectrum of an image as a convolution operator changes before and after blurring . our analysis leads to an effective convex regularizer on the blur kernel which depends only on the given blurry image . we show that the minimizer of this regularizer guarantees to give good approximation to the blur kernel if the original image is sharp enough . by combining this powerful regularizer with conventional image deblurring techniques , we show how we could significantly improve the deblurring results through simulations and experiments on real images . in addition , our analysis and experiments help explaining a widely accepted doctrine ; that is , the edges are good features for deblurring .", "topics": ["kernel ( operating system )", "sparse matrix"]}
{"title": "word-alignment-based segment-level machine translation evaluation using word embeddings", "abstract": "one of the most important problems in machine translation ( mt ) evaluation is to evaluate the similarity between translation hypotheses with different surface forms from the reference , especially at the segment level . we propose to use word embeddings to perform word alignment for segment-level mt evaluation . we performed experiments with three types of alignment methods using word embeddings . we evaluated our proposed methods with various translation datasets . experimental results show that our proposed methods outperform previous word embeddings-based methods .", "topics": ["machine translation"]}
{"title": "effective deep learning training for single-image super-resolution in endomicroscopy exploiting video-registration-based reconstruction", "abstract": "purpose : probe-based confocal laser endomicroscopy ( pcle ) is a recent imaging modality that allows performing in vivo optical biopsies . the design of pcle hardware , and its reliance on an optical fibre bundle , fundamentally limits the image quality with a few tens of thousands fibres , each acting as the equivalent of a single-pixel detector , assembled into a single fibre bundle . video-registration techniques can be used to estimate high-resolution ( hr ) images by exploiting the temporal information contained in a sequence of low-resolution ( lr ) images . however , the alignment of lr frames , required for the fusion , is computationally demanding and prone to artefacts . methods : in this work , we propose a novel synthetic data generation approach to train exemplar-based deep neural networks ( dnns ) . hr pcle images with enhanced quality are recovered by the models trained on pairs of estimated hr images ( generated by the video-registration algorithm ) and realistic synthetic lr images . performance of three different state-of-the-art dnns techniques were analysed on a smart atlas database of 8806 images from 238 pcle video sequences . the results were validated through an extensive image quality assessment ( iqa ) that takes into account different quality scores , including a mean opinion score ( mos ) . results : results indicate that the proposed solution produces an effective improvement in the quality of the obtained reconstructed image . conclusion : the proposed training strategy and associated dnns allows us to perform convincing super-resolution of pcle images .", "topics": ["synthetic data", "pixel"]}
{"title": "dialogue act sequence labeling using hierarchical encoder with crf", "abstract": "dialogue act recognition associate dialogue acts ( i.e . , semantic labels ) to utterances in a conversation . the problem of associating semantic labels to utterances can be treated as a sequence labeling problem . in this work , we build a hierarchical recurrent neural network using bidirectional lstm as a base unit and the conditional random field ( crf ) as the top layer to classify each utterance into its corresponding dialogue act . the hierarchical network learns representations at multiple levels , i.e . , word level , utterance level , and conversation level . the conversation level representations are input to the crf layer , which takes into account not only all previous utterances but also their dialogue acts , thus modeling the dependency among both , labels and utterances , an important consideration of natural dialogue . we validate our approach on two different benchmark data sets , switchboard and meeting recorder dialogue act , and show performance improvement over the state-of-the-art methods by $ 2.2\\ % $ and $ 4.1\\ % $ absolute points , respectively . it is worth noting that the inter-annotator agreement on switchboard data set is $ 84\\ % $ , and our method is able to achieve the accuracy of about $ 79\\ % $ despite being trained on the noisy data .", "topics": ["recurrent neural network"]}
{"title": "tweet2vec : character-based distributed representations for social media", "abstract": "text from social media provides a set of challenges that can cause traditional nlp approaches to fail . informal language , spelling errors , abbreviations , and special characters are all commonplace in these posts , leading to a prohibitively large vocabulary size for word-level approaches . we propose a character composition model , tweet2vec , which finds vector-space representations of whole tweets by learning complex , non-local dependencies in character sequences . the proposed model outperforms a word-level baseline at predicting user-annotated hashtags associated with the posts , doing significantly better when the input contains many out-of-vocabulary words or unusual character sequences . our tweet2vec encoder is publicly available .", "topics": ["baseline ( configuration management )", "natural language processing"]}
{"title": "interpolating conditional density trees", "abstract": "joint distributions over many variables are frequently modeled by decomposing them into products of simpler , lower-dimensional conditional distributions , such as in sparsely connected bayesian networks . however , automatically learning such models can be very computationally expensive when there are many datapoints and many continuous variables with complex nonlinear relationships , particularly when no good ways of decomposing the joint distribution are known a priori . in such situations , previous research has generally focused on the use of discretization techniques in which each continuous variable has a single discretization that is used throughout the entire network . \\ in this paper , we present and compare a wide variety of tree-based algorithms for learning and evaluating conditional density estimates over continuous variables . these trees can be thought of as discretizations that vary according to the particular interactions being modeled ; however , the density within a given leaf of the tree need not be assumed constant , and we show that such nonuniform leaf densities lead to more accurate density estimation . we have developed bayesian network structure-learning algorithms that employ these tree-based conditional density representations , and we show that they can be used to practically learn complex joint probability models over dozens of continuous variables from thousands of datapoints . we focus on finding models that are simultaneously accurate , fast to learn , and fast to evaluate once they are learned .", "topics": ["nonlinear system", "interaction"]}
{"title": "polynet : a pursuit of structural diversity in very deep networks", "abstract": "a number of studies have shown that increasing the depth or width of convolutional networks is a rewarding approach to improve the performance of image recognition . in our study , however , we observed difficulties along both directions . on one hand , the pursuit for very deep networks is met with a diminishing return and increased training difficulty ; on the other hand , widening a network would result in a quadratic growth in both computational cost and memory demand . these difficulties motivate us to explore structural diversity in designing deep networks , a new dimension beyond just depth and width . specifically , we present a new family of modules , namely the polyinception , which can be flexibly inserted in isolation or in a composition as replacements of different parts of a network . choosing polyinception modules with the guidance of architectural efficiency can improve the expressive power while preserving comparable computational cost . the very deep polynet , designed following this direction , demonstrates substantial improvements over the state-of-the-art on the ilsvrc 2012 benchmark . compared to inception-resnet-v2 , it reduces the top-5 validation error on single crops from 4.9 % to 4.25 % , and that on multi-crops from 3.7 % to 3.45 % .", "topics": ["computer vision"]}
{"title": "cross-paced representation learning with partial curricula for sketch-based image retrieval", "abstract": "in this paper we address the problem of learning robust cross-domain representations for sketch-based image retrieval ( sbir ) . while most sbir approaches focus on extracting low- and mid-level descriptors for direct feature matching , recent works have shown the benefit of learning coupled feature representations to describe data from two related sources . however , cross-domain representation learning methods are typically cast into non-convex minimization problems that are difficult to optimize , leading to unsatisfactory performance . inspired by self-paced learning , a learning methodology designed to overcome convergence issues related to local optima by exploiting the samples in a meaningful order ( i.e . easy to hard ) , we introduce the cross-paced partial curriculum learning ( cppcl ) framework . compared with existing self-paced learning methods which only consider a single modality and can not deal with prior knowledge , cppcl is specifically designed to assess the learning pace by jointly handling data from dual sources and modality-specific prior information provided in the form of partial curricula . additionally , thanks to the learned dictionaries , we demonstrate that the proposed cppcl embeds robust coupled representations for sbir . our approach is extensively evaluated on four publicly available datasets ( i.e . cufs , flickr15k , queenmary sbir and tu-berlin extension datasets ) , showing superior performance over competing sbir methods .", "topics": ["feature learning", "dictionary"]}
{"title": "latent composite likelihood learning for the structured canonical correlation model", "abstract": "latent variable models are used to estimate variables of interest quantities which are observable only up to some measurement error . in many studies , such variables are known but not precisely quantifiable ( such as `` job satisfaction '' in social sciences and marketing , `` analytical ability '' in educational testing , or `` inflation '' in economics ) . this leads to the development of measurement instruments to record noisy indirect evidence for such unobserved variables such as surveys , tests and price indexes . in such problems , there are postulated latent variables and a given measurement model . at the same time , other unantecipated latent variables can add further unmeasured confounding to the observed variables . the problem is how to deal with unantecipated latents variables . in this paper , we provide a method loosely inspired by canonical correlation that makes use of background information concerning the `` known '' latent variables . given a partially specified structure , it provides a structure learning approach to detect `` unknown unknowns , '' the confounding effect of potentially infinitely many other latent variables . this is done without explicitly modeling such extra latent factors . because of the special structure of the problem , we are able to exploit a new variation of composite likelihood fitting to efficiently learn this structure . validation is provided with experiments in synthetic data and the analysis of a large survey done with a sample of over 100,000 staff members of the national health service of the united kingdom .", "topics": ["synthetic data"]}
{"title": "distributed decision trees", "abstract": "recently proposed budding tree is a decision tree algorithm in which every node is part internal node and part leaf . this allows representing every decision tree in a continuous parameter space , and therefore a budding tree can be jointly trained with backpropagation , like a neural network . even though this continuity allows it to be used in hierarchical representation learning , the learned representations are local : activation makes a soft selection among all root-to-leaf paths in a tree . in this work we extend the budding tree and propose the distributed tree where the children use different and independent splits and hence multiple paths in a tree can be traversed at the same time . this ability to combine multiple paths gives the power of a distributed representation , as in a traditional perceptron layer . we show that distributed trees perform comparably or better than budding and traditional hard trees on classification and regression tasks .", "topics": ["feature learning"]}
{"title": "deep learning of appearance models for online object tracking", "abstract": "this paper introduces a novel deep learning based approach for vision based single target tracking . we address this problem by proposing a network architecture which takes the input video frames and directly computes the tracking score for any candidate target location by estimating the probability distributions of the positive and negative examples . this is achieved by combining a deep convolutional neural network with a bayesian loss layer in a unified framework . in order to deal with the limited number of positive training examples , the network is pre-trained offline for a generic image feature representation and then is fine-tuned in multiple steps . an online fine-tuning step is carried out at every frame to learn the appearance of the target . we adopt a two-stage iterative algorithm to adaptively update the network parameters and maintain a probability density for target/non-target regions . the tracker has been tested on the standard tracking benchmark and the results indicate that the proposed solution achieves state-of-the-art tracking results .", "topics": ["iteration"]}
{"title": "iterated ontology revision by reinterpretation", "abstract": "iterated applications of belief change operators are essential for different scenarios such as that of ontology evolution where new information is not presented at once but only in piecemeal fashion within a sequence . i discuss iterated applications of so called reinterpretation operators that trace conflicts between ontologies back to the ambiguous of symbols and that provide conflict resolution strategies with bridging axioms . the discussion centers on adaptations of the classical iteration postulates according to darwiche and pearl . the main result of the paper is that reinterpretation operators fulfill the postulates for sequences containing only atomic triggers . for complex triggers , a fulfillment is not guaranteed and indeed there are different reasons for the different postulates why they should not be fulfilled in the particular scenario of ontology revision with well developed ontologies .", "topics": ["iteration"]}
{"title": "attentional network for visual object detection", "abstract": "we propose augmenting deep neural networks with an attention mechanism for the visual object detection task . as perceiving a scene , humans have the capability of multiple fixation points , each attended to scene content at different locations and scales . however , such a mechanism is missing in the current state-of-the-art visual object detection methods . inspired by the human vision system , we propose a novel deep network architecture that imitates this attention mechanism . as detecting objects in an image , the network adaptively places a sequence of glimpses of different shapes at different locations in the image . evidences of the presence of an object and its location are extracted from these glimpses , which are then fused for estimating the object class and bounding box coordinates . due to lacks of ground truth annotations of the visual attention mechanism , we train our network using a reinforcement learning algorithm with policy gradients . experiment results on standard object detection benchmarks show that the proposed network consistently outperforms the baseline networks that does not model the attention mechanism .", "topics": ["baseline ( configuration management )", "object detection"]}
{"title": "helping ai to play hearthstone : aaia'17 data mining challenge", "abstract": "this paper summarizes the aaia'17 data mining challenge : helping ai to play hearthstone which was held between march 23 , and may 15 , 2017 at the knowledge pit platform . we briefly describe the scope and background of this competition in the context of a more general project related to the development of an ai engine for video games , called grail . we also discuss the outcomes of this challenge and demonstrate how predictive models for the assessment of player 's winning chances can be utilized in a construction of an intelligent agent for playing hearthstone . finally , we show a few selected machine learning approaches for modeling state and action values in hearthstone . we provide evaluation for a few promising solutions that may be used to create more advanced types of agents , especially in conjunction with monte carlo tree search algorithms .", "topics": ["data mining"]}
{"title": "prototypical recurrent unit", "abstract": "despite the great successes of deep learning , the effectiveness of deep neural networks has not been understood at any theoretical depth . this work is motivated by the thrust of developing a deeper understanding of recurrent neural networks , particularly lstm/gru-like networks . as the highly complex structure of the recurrent unit in lstm and gru networks makes them difficult to analyze , our methodology in this research theme is to construct an alternative recurrent unit that is as simple as possible and yet also captures the key components of lstm/gru recurrent units . such a unit can then be used for the study of recurrent networks and its structural simplicity may allow easier analysis . towards that goal , we take a system-theoretic perspective to design a new recurrent unit , which we call the prototypical recurrent unit ( pru ) . not only having minimal complexity , pru is demonstrated experimentally to have comparable performance to gru and lstm unit . this establishes pru networks as a prototype for future study of lstm/gru-like recurrent networks . this paper also studies the memorization abilities of lstm , gru and pru networks , motivated by the folk belief that such networks possess long-term memory . for this purpose , we design a simple and controllable task , called `` memorization problem '' , where the networks are trained to memorize certain targeted information . we show that the memorization performance of all three networks depends on the amount of targeted information , the amount of `` interfering '' information , and the state space dimension of the recurrent unit . experiments are also performed for another controllable task , the adding problem , and similar conclusions are obtained .", "topics": ["recurrent neural network", "nonlinear system"]}
{"title": "social trust prediction via max-norm constrained 1-bit matrix completion", "abstract": "social trust prediction addresses the significant problem of exploring interactions among users in social networks . naturally , this problem can be formulated in the matrix completion framework , with each entry indicating the trustness or distrustness . however , there are two challenges for the social trust problem : 1 ) the observed data are with sign ( 1-bit ) measurements ; 2 ) they are typically sampled non-uniformly . most of the previous matrix completion methods do not well handle the two issues . motivated by the recent progress of max-norm , we propose to solve the problem with a 1-bit max-norm constrained formulation . since max-norm is not easy to optimize , we utilize a reformulation of max-norm which facilitates an efficient projected gradient decent algorithm . we demonstrate the superiority of our formulation on two benchmark datasets .", "topics": ["interaction", "gradient"]}
{"title": "a survey on hough transform , theory , techniques and applications", "abstract": "for more than half a century , the hough transform is ever-expanding for new frontiers . thousands of research papers and numerous applications have evolved over the decades . carrying out an all-inclusive survey is hardly possible and enormously space-demanding . what we care about here is emphasizing some of the most crucial milestones of the transform . we describe its variations elaborating on the basic ones such as the line and circle hough transforms . the high demand for storage and computation time is clarified with different solution approaches . since most uses of the transform take place on binary images , we have been concerned with the work done directly on gray or color images . the myriad applications of the standard transform and its variations have been classified highlighting the up-to-date and the unconventional ones . due to its merits such as noise-immunity and expandability , the transform has an excellent history , and a bright future as well .", "topics": ["time complexity", "computation"]}
{"title": "heuristics and parse ranking", "abstract": "there are currently two philosophies for building grammars and parsers -- statistically induced grammars and wide-coverage grammars . one way to combine the strengths of both approaches is to have a wide-coverage grammar with a heuristic component which is domain independent but whose contribution is tuned to particular domains . in this paper , we discuss a three-stage approach to disambiguation in the context of a lexicalized grammar , using a variety of domain independent heuristic techniques . we present a training algorithm which uses hand-bracketed treebank parses to set the weights of these heuristics . we compare the performance of our grammar against the performance of the ibm statistical grammar , using both untrained and trained weights for the heuristics .", "topics": ["parsing", "heuristic"]}
{"title": "cost-sensitive tree of classifiers", "abstract": "recently , machine learning algorithms have successfully entered large-scale real-world industrial applications ( e.g . search engines and email spam filters ) . here , the cpu cost during test time must be budgeted and accounted for . in this paper , we address the challenge of balancing the test-time cost and the classifier accuracy in a principled fashion . the test-time cost of a classifier is often dominated by the computation required for feature extraction-which can vary drastically across eatures . we decrease this extraction time by constructing a tree of classifiers , through which test inputs traverse along individual paths . each path extracts different features and is optimized for a specific sub-partition of the input space . by only computing features for inputs that benefit from them the most , our cost sensitive tree of classifiers can match the high accuracies of the current state-of-the-art at a small fraction of the computational cost .", "topics": ["feature extraction", "computation"]}
{"title": "protecting sensory data against sensitive inferences", "abstract": "there is growing concern about how personal data are used when users grant applications direct access to the sensors of their mobile devices . in fact , high resolution temporal data generated by motion sensors reflect directly the activities of a user and indirectly physical and demographic attributes . in this paper , we propose a feature learning architecture for mobile devices that provides flexible and negotiable privacy-preserving sensor data transmission by appropriately transforming raw sensor data . the objective is to move from the current binary setting of granting or not permission to an application , toward a model that allows users to grant each application permission over a limited range of inferences according to the provided services . the internal structure of each component of the proposed architecture can be flexibly changed and the trade-off between privacy and utility can be negotiated between the constraints of the user and the underlying application . we validated the proposed architecture in an activity recognition application using two real-world datasets , with the objective of recognizing an activity without disclosing gender as an example of private information . results show that the proposed framework maintains the usefulness of the transformed data for activity recognition , with an average loss of only around three percentage points , while reducing the possibility of gender classification to around 50\\ % , the target random guess , from more than 90\\ % when using raw sensor data . we also present and distribute motionsense , a new dataset for activity and attribute recognition collected from motion sensors .", "topics": ["feature learning", "sensor"]}
{"title": "progressively diffused networks for semantic image segmentation", "abstract": "this paper introduces progressively diffused networks ( pdns ) for unifying multi-scale context modeling with deep feature learning , by taking semantic image segmentation as an exemplar application . prior neural networks , such as resnet , tend to enhance representational power by increasing the depth of architectures and driving the training objective across layers . however , we argue that spatial dependencies in different layers , which generally represent the rich contexts among data elements , are also critical to building deep and discriminative representations . to this end , our pdns enables to progressively broadcast information over the learned feature maps by inserting a stack of information diffusion layers , each of which exploits multi-dimensional convolutional lstms ( long-short-term memory structures ) . in each lstm unit , a special type of atrous filters are designed to capture the short range and long range dependencies from various neighbors to a certain site of the feature map and pass the accumulated information to the next layer . from the extensive experiments on semantic image segmentation benchmarks ( e.g . , imagenet parsing , pascal voc2012 and pascal-part ) , our framework demonstrates the effectiveness to substantially improve the performances over the popular existing neural network models , and achieves state-of-the-art on imagenet parsing for large scale semantic segmentation .", "topics": ["feature learning", "image segmentation"]}
{"title": "para-active learning", "abstract": "training examples are not all equally informative . active learning strategies leverage this observation in order to massively reduce the number of examples that need to be labeled . we leverage the same observation to build a generic strategy for parallelizing learning algorithms . this strategy is effective because the search for informative examples is highly parallelizable and because we show that its performance does not deteriorate when the sifting process relies on a slightly outdated model . parallel active learning is particularly attractive to train nonlinear models with non-linear representations because there are few practical parallel learning algorithms for such models . we report preliminary experiments using both kernel svms and sgd-trained neural networks .", "topics": ["nonlinear system"]}
{"title": "direct mapping hidden excited state interaction patterns from ab initio dynamics and its implications on force field development", "abstract": "the excited states of polyatomic systems are rather complex , and often exhibit meta-stable dynamical behaviors . static analysis of reaction pathway often fails to sufficiently characterize excited state motions due to their highly non-equilibrium nature . here , we proposed a time series guided clustering algorithm to generate most relevant meta-stable patterns directly from ab initio dynamic trajectories . based on the knowledge of these meta-stable patterns , we suggested an interpolation scheme with only a concrete and finite set of known patterns to accurately predict the ground and excited state properties of the entire dynamics trajectories . as illustrated with the example of sinapic acids , the estimation error for both ground and excited state is very close , which indicates one could predict the ground and excited state molecular properties with similar accuracy . these results may provide us some insights to construct an excited state force field with compatible energy terms as traditional ones .", "topics": ["cluster analysis", "time series"]}
{"title": "asynchronous byzantine machine learning", "abstract": "asynchronous distributed machine learning solutions have proven very effective so far , but always assuming perfectly functioning workers . in practice , some of the workers can however exhibit byzantine behavior , caused by hardware failures , software bugs , corrupt data , or even malicious attacks . we introduce \\emph { kardam } , the first distributed asynchronous stochastic gradient descent ( sgd ) algorithm that copes with byzantine workers . kardam consists of two complementary components : a filtering and a dampening component . the first is scalar-based and ensures resilience against $ \\frac { 1 } { 3 } $ byzantine workers . essentially , this filter leverages the lipschitzness of cost functions and acts as a self-stabilizer against byzantine workers that would attempt to corrupt the progress of sgd . the dampening component bounds the convergence rate by adjusting to stale information through a generic gradient weighting scheme . we prove that kardam guarantees almost sure convergence in the presence of asynchrony and byzantine behavior , and we derive its convergence rate . we evaluate kardam on the cifar-100 and emnist datasets and measure its overhead with respect to non byzantine-resilient solutions . we empirically show that kardam does not introduce additional noise to the learning procedure but does induce a slowdown ( the cost of byzantine resilience ) that we both theoretically and empirically show to be less than $ f/n $ , where $ f $ is the number of byzantine failures tolerated and $ n $ the total number of workers . interestingly , we also empirically observe that the dampening component is interesting in its own right for it enables to build an sgd algorithm that outperforms alternative staleness-aware asynchronous competitors in environments with honest workers .", "topics": ["gradient descent", "gradient"]}
{"title": "temporal information extraction for question answering using syntactic dependencies in an lstm-based architecture", "abstract": "in this paper , we propose to use a set of simple , uniform in architecture lstm-based models to recover different kinds of temporal relations from text . using the shortest dependency path between entities as input , the same architecture is used to extract intra-sentence , cross-sentence , and document creation time relations . a `` double-checking '' technique reverses entity pairs in classification , boosting the recall of positive cases and reducing misclassifications between opposite classes . an efficient pruning algorithm resolves conflicts globally . evaluated on qa-tempeval ( semeval2015 task 5 ) , our proposed technique outperforms state-of-the-art methods by a large margin .", "topics": ["entity"]}
{"title": "awareness improves problem-solving performance", "abstract": "the brain 's self-monitoring of activities , including internal activities -- a functionality that we refer to as awareness -- has been suggested as a key element of consciousness . here we investigate whether the presence of an inner-eye-like process ( monitor ) that supervises the activities of a number of subsystems ( operative agents ) engaged in the solution of a problem can improve the problem-solving efficiency of the system . the problem is to find the global maximum of a nk fitness landscape and the performance is measured by the time required to find that maximum . the operative agents explore blindly the fitness landscape and the monitor provides them with feedback on the quality ( fitness ) of the proposed solutions . this feedback is then used by the operative agents to bias their searches towards the fittest regions of the landscape . we find that a weak feedback between the monitor and the operative agents improves the performance of the system , regardless of the difficulty of the problem , which is gauged by the number of local maxima in the landscape . for easy problems ( i.e . , landscapes without local maxima ) , the performance improves monotonically as the feedback strength increases , but for difficult problems , there is an optimal value of the feedback strength beyond which the system performance degrades very rapidly .", "topics": ["optimization problem"]}
{"title": "deep reinforcement learning for dexterous manipulation with concept networks", "abstract": "deep reinforcement learning yields great results for a large array of problems , but models are generally retrained anew for each new problem to be solved . prior learning and knowledge are difficult to incorporate when training new models , requiring increasingly longer training as problems become more complex . this is especially problematic for problems with sparse rewards . we provide a solution to these problems by introducing concept network reinforcement learning ( cnrl ) , a framework which allows us to decompose problems using a multi-level hierarchy . concepts in a concept network are reusable , and flexible enough to encapsulate feature extractors , skills , or other concept networks . with this hierarchical learning approach , deep reinforcement learning can be used to solve complex tasks in a modular way , through problem decomposition . we demonstrate the strength of cnrl by training a model to grasp a rectangular prism and precisely stack it on top of a cube using a gripper on a kinova jaco arm , simulated in mujoco . our experiments show that our use of hierarchy results in a 45x reduction in environment interactions compared to the state-of-the-art on this task .", "topics": ["reinforcement learning", "simulation"]}
{"title": "couplenet : coupling global structure with local parts for object detection", "abstract": "the region-based convolutional neural network ( cnn ) detectors such as faster r-cnn or r-fcn have already shown promising results for object detection by combining the region proposal subnetwork and the classification subnetwork together . although r-fcn has achieved higher detection speed while keeping the detection performance , the global structure information is ignored by the position-sensitive score maps . to fully explore the local and global properties , in this paper , we propose a novel fully convolutional network , named as couplenet , to couple the global structure with local parts for object detection . specifically , the object proposals obtained by the region proposal network ( rpn ) are fed into the the coupling module which consists of two branches . one branch adopts the position-sensitive roi ( psroi ) pooling to capture the local part information of the object , while the other employs the roi pooling to encode the global and context information . next , we design different coupling strategies and normalization ways to make full use of the complementary advantages between the global and local branches . extensive experiments demonstrate the effectiveness of our approach . we achieve state-of-the-art results on all three challenging datasets , i.e . a map of 82.7 % on voc07 , 80.4 % on voc12 , and 34.4 % on coco . codes will be made publicly available .", "topics": ["object detection", "map"]}
{"title": "incorporating external knowledge to answer open-domain visual questions with dynamic memory networks", "abstract": "visual question answering ( vqa ) has attracted much attention since it offers insight into the relationships between the multi-modal analysis of images and natural language . most of the current algorithms are incapable of answering open-domain questions that require to perform reasoning beyond the image contents . to address this issue , we propose a novel framework which endows the model capabilities in answering more complex questions by leveraging massive external knowledge with dynamic memory networks . specifically , the questions along with the corresponding images trigger a process to retrieve the relevant information in external knowledge bases , which are embedded into a continuous vector space by preserving the entity-relation structures . afterwards , we employ dynamic memory networks to attend to the large body of facts in the knowledge graph and images , and then perform reasoning over these facts to generate corresponding answers . extensive experiments demonstrate that our model not only achieves the state-of-the-art performance in the visual question answering task , but can also answer open-domain questions effectively by leveraging the external knowledge .", "topics": ["natural language"]}
{"title": "codeco : a grammar notation for controlled natural language in predictive editors", "abstract": "existing grammar frameworks do not work out particularly well for controlled natural languages ( cnl ) , especially if they are to be used in predictive editors . i introduce in this paper a new grammar notation , called codeco , which is designed specifically for cnls and predictive editors . two different parsers have been implemented and a large subset of attempto controlled english ( ace ) has been represented in codeco . the results show that codeco is practical , adequate and efficient .", "topics": ["natural language", "parsing"]}
{"title": "a probabilistic optimum-path forest classifier for binary classification problems", "abstract": "probabilistic-driven classification techniques extend the role of traditional approaches that output labels ( usually integer numbers ) only . such techniques are more fruitful when dealing with problems where one is not interested in recognition/identification only , but also into monitoring the behavior of consumers and/or machines , for instance . therefore , by means of probability estimates , one can take decisions to work better in a number of scenarios . in this paper , we propose a probabilistic-based optimum path forest ( opf ) classifier to handle with binary classification problems , and we show it can be more accurate than naive opf in a number of datasets . in addition to being just more accurate or not , probabilistic opf turns to be another useful tool to the scientific community .", "topics": ["speech recognition"]}
{"title": "optimal auctions through deep learning", "abstract": "designing an auction that maximizes expected revenue is an intricate task . indeed , as of today -- despite major efforts and impressive progress over the past few years -- only the single-item case is fully understood . in this work , we initiate the exploration of the use of tools from deep learning on this topic . the design objective is revenue optimal , dominant-strategy incentive compatible auctions . we show that multi-layer neural networks can learn almost-optimal auctions for settings for which there are analytical solutions , such as myerson 's auction for a single item , manelli and vincent 's mechanism for a single bidder with additive preferences over two items , or yao 's auction for two additive bidders with binary support distributions and multiple items , even if no prior knowledge about the form of optimal auctions is encoded in the network and the only feedback during training is revenue and regret . we further show how characterization results , even rather implicit ones such as rochet 's characterization through induced utilities and their gradients , can be leveraged to obtain more precise fits to the optimal design . we conclude by demonstrating the potential of deep learning for deriving optimal auctions with high revenue for poorly understood problems .", "topics": ["regret ( decision theory )", "autonomous car"]}
{"title": "differentiable pooling for hierarchical feature learning", "abstract": "we introduce a parametric form of pooling , based on a gaussian , which can be optimized alongside the features in a single global objective function . by contrast , existing pooling schemes are based on heuristics ( e.g . local maximum ) and have no clear link to the cost function of the model . furthermore , the variables of the gaussian explicitly store location information , distinct from the appearance captured by the features , thus providing a what/where decomposition of the input signal . although the differentiable pooling scheme can be incorporated in a wide range of hierarchical models , we demonstrate it in the context of a deconvolutional network model ( zeiler et al . iccv 2011 ) . we also explore a number of secondary issues within this model and present detailed experiments on mnist digits .", "topics": ["feature learning", "loss function"]}
{"title": "dynamic boltzmann machines for second order moments and generalized gaussian distributions", "abstract": "dynamic boltzmann machine ( dybm ) has been shown highly efficient to predict time-series data . gaussian dybm is a dybm that assumes the predicted data is generated by a gaussian distribution whose first-order moment ( mean ) dynamically changes over time but its second-order moment ( variance ) is fixed . however , in many financial applications , the assumption is quite limiting in two aspects . first , even when the data follows a gaussian distribution , its variance may change over time . such variance is also related to important temporal economic indicators such as the market volatility . second , financial time-series data often requires learning datasets generated by the generalized gaussian distribution with an additional shape parameter that is important to approximate heavy-tailed distributions . addressing those aspects , we show how to extend dybm that results in significant performance improvement in predicting financial time-series data .", "topics": ["time series", "approximation algorithm"]}
{"title": "image super-resolution via sparse bayesian modeling of natural images", "abstract": "image super-resolution ( sr ) is one of the long-standing and active topics in image processing community . a large body of works for image super resolution formulate the problem with bayesian modeling techniques and then obtain its maximum-a-posteriori ( map ) solution , which actually boils down to a regularized regression task over separable regularization term . although straightforward , this approach can not exploit the full potential offered by the probabilistic modeling , as only the posterior mode is sought . also , the separable property of the regularization term can not capture any correlations between the sparse coefficients , which sacrifices much on its modeling accuracy . we propose a bayesian image sr algorithm via sparse modeling of natural images . the sparsity property of the latent high resolution image is exploited by introducing latent variables into the high-order markov random field ( mrf ) which capture the content adaptive variance by pixel-wise adaptation . the high-resolution image is estimated via empirical bayesian estimation scheme , which is substantially faster than our previous approach based on markov chain monte carlo sampling [ 1 ] . it is shown that the actual cost function for the proposed approach actually incorporates a non-factorial regularization term over the sparse coefficients . experimental results indicate that the proposed method can generate competitive or better results than \\emph { state-of-the-art } sr algorithms .", "topics": ["sampling ( signal processing )", "image processing"]}
{"title": "stochastic variance reduced multiplicative update for nonnegative matrix factorization", "abstract": "nonnegative matrix factorization ( nmf ) , a dimensionality reduction and factor analysis method , is a special case in which factor matrices have low-rank nonnegative constraints . considering the stochastic learning in nmf , we specifically address the multiplicative update ( mu ) rule , which is the most popular , but which has slow convergence property . this present paper introduces on the stochastic mu rule a variance-reduced technique of stochastic gradient . numerical comparisons suggest that our proposed algorithms robustly outperform state-of-the-art algorithms across different synthetic and real-world datasets .", "topics": ["numerical analysis", "synthetic data"]}
{"title": "towards neural network-based reasoning", "abstract": "we propose neural reasoner , a framework for neural network-based reasoning over natural language sentences . given a question , neural reasoner can infer over multiple supporting facts and find an answer to the question in specific forms . neural reasoner has 1 ) a specific interaction-pooling mechanism , allowing it to examine multiple facts , and 2 ) a deep architecture , allowing it to model the complicated logical relations in reasoning tasks . assuming no particular structure exists in the question and facts , neural reasoner is able to accommodate different types of reasoning and different forms of language expressions . despite the model complexity , neural reasoner can still be trained effectively in an end-to-end manner . our empirical studies show that neural reasoner can outperform existing neural reasoning systems with remarkable margins on two difficult artificial tasks ( positional reasoning and path finding ) proposed in [ 8 ] . for example , it improves the accuracy on path finding ( 10k ) from 33.4 % [ 6 ] to over 98 % .", "topics": ["natural language", "end-to-end principle"]}
{"title": "a deep architecture for semantic matching with multiple positional sentence representations", "abstract": "matching natural language sentences is central for many applications such as information retrieval and question answering . existing deep models rely on a single sentence representation or multiple granularity representations for matching . however , such methods can not well capture the contextualized local information in the matching process . to tackle this problem , we present a new deep architecture to match two sentences with multiple positional sentence representations . specifically , each positional sentence representation is a sentence representation at this position , generated by a bidirectional long short term memory ( bi-lstm ) . the matching score is finally produced by aggregating interactions between these different positional sentence representations , through $ k $ -max pooling and a multi-layer perceptron . our model has several advantages : ( 1 ) by using bi-lstm , rich context of the whole sentence is leveraged to capture the contextualized local information in each positional sentence representation ; ( 2 ) by matching with multiple positional sentence representations , it is flexible to aggregate different important contextualized local information in a sentence to support the matching ; ( 3 ) experiments on different tasks such as question answering and sentence completion demonstrate the superiority of our model .", "topics": ["natural language"]}
{"title": "how to elicit many probabilities", "abstract": "in building bayesian belief networks , the elicitation of all probabilities required can be a major obstacle . we learned the extent of this often-cited observation in the construction of the probabilistic part of a complex influence diagram in the field of cancer treatment . based upon our negative experiences with existing methods , we designed a new method for probability elicitation from domain experts . the method combines various ideas , among which are the ideas of transcribing probabilities and of using a scale with both numerical and verbal anchors for marking assessments . in the construction of the probabilistic part of our influence diagram , the method proved to allow for the elicitation of many probabilities in little time .", "topics": ["numerical analysis", "bayesian network"]}
{"title": "improvement of pso algorithm by memory based gradient search - application in inventory management", "abstract": "advanced inventory management in complex supply chains requires effective and robust nonlinear optimization due to the stochastic nature of supply and demand variations . application of estimated gradients can boost up the convergence of particle swarm optimization ( pso ) algorithm but classical gradient calculation can not be applied to stochastic and uncertain systems . in these situations monte-carlo ( mc ) simulation can be applied to determine the gradient . we developed a memory based algorithm where instead of generating and evaluating new simulated samples the stored and shared former function evaluations of the particles are sampled to estimate the gradients by local weighted least squares regression . the performance of the resulted regional gradient-based pso is verified by several benchmark problems and in a complex application example where optimal reorder points of a supply chain are determined .", "topics": ["nonlinear system", "simulation"]}
{"title": "monocular object instance segmentation and depth ordering with cnns", "abstract": "in this paper we tackle the problem of instance-level segmentation and depth ordering from a single monocular image . towards this goal , we take advantage of convolutional neural nets and train them to directly predict instance-level segmentations where the instance id encodes the depth ordering within image patches . to provide a coherent single explanation of an image we develop a markov random field which takes as input the predictions of convolutional neural nets applied at overlapping patches of different resolutions , as well as the output of a connected component algorithm . it aims to predict accurate instance-level segmentation and depth ordering . we demonstrate the effectiveness of our approach on the challenging kitti benchmark and show good performance on both tasks .", "topics": ["markov chain"]}
{"title": "strong baselines for simple question answering over knowledge graphs with and without neural networks", "abstract": "we examine the problem of question answering over knowledge graphs , focusing on simple questions that can be answered by the lookup of a single fact . adopting a straightforward decomposition of the problem into entity detection , entity linking , relation prediction , and evidence combination , we explore simple yet strong baselines . on the simplequestions dataset , we find that baseline lstms and grus plus a few heuristics yield accuracies that approach the state of the art , and techniques that do not use neural networks also perform reasonably well . these results show that gains from sophisticated deep learning techniques proposed in the literature are quite modest and that some previous models exhibit unnecessary complexity .", "topics": ["baseline ( configuration management )", "neural networks"]}
{"title": "deep learning for answer sentence selection", "abstract": "answer sentence selection is the task of identifying sentences that contain the answer to a given question . this is an important problem in its own right as well as in the larger context of open domain question answering . we propose a novel approach to solving this task via means of distributed representations , and learn to match questions with answers by considering their semantic encoding . this contrasts prior work on this task , which typically relies on classifiers with large numbers of hand-crafted syntactic and semantic features and various external resources . our approach does not require any feature engineering nor does it involve specialist linguistic data , making this model easily applicable to a wide range of domains and languages . experimental results on a standard benchmark dataset from trec demonstrate that -- -despite its simplicity -- -our model matches state of the art performance on the answer sentence selection task .", "topics": ["statistical classification"]}
{"title": "improving graph convolutional networks with non-parametric activation functions", "abstract": "graph neural networks ( gnns ) are a class of neural networks that allow to efficiently perform inference on data that is associated to a graph structure , such as , e.g . , citation networks or knowledge graphs . while several variants of gnns have been proposed , they only consider simple nonlinear activation functions in their layers , such as rectifiers or squashing functions . in this paper , we investigate the use of graph convolutional networks ( gcns ) when combined with more complex activation functions , able to adapt from the training data . more specifically , we extend the recently proposed kernel activation function , a non-parametric model which can be implemented easily , can be regularized with standard $ \\ell_p $ -norms techniques , and is smooth over its entire domain . our experimental evaluation shows that the proposed architecture can significantly improve over its baseline , while similar improvements can not be obtained by simply increasing the depth or size of the original gcn .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "enhancing person re-identification in a self-trained subspace", "abstract": "despite the promising progress made in recent years , person re-identification ( re-id ) remains a challenging task due to the complex variations in human appearances from different camera views . for this challenging problem , a large variety of algorithms have been developed in the fully-supervised setting , requiring access to a large amount of labeled training data . however , the main bottleneck for fully-supervised re-id is the limited availability of labeled training samples . to address this problem , in this paper , we propose a self-trained subspace learning paradigm for person re-id which effectively utilizes both labeled and unlabeled data to learn a discriminative subspace where person images across disjoint camera views can be easily matched . the proposed approach first constructs pseudo pairwise relationships among unlabeled persons using the k-nearest neighbors algorithm . then , with the pseudo pairwise relationships , the unlabeled samples can be easily combined with the labeled samples to learn a discriminative projection by solving an eigenvalue problem . in addition , we refine the pseudo pairwise relationships iteratively , which further improves the learning performance . a multi-kernel embedding strategy is also incorporated into the proposed approach to cope with the non-linearity in person 's appearance and explore the complementation of multiple kernels . in this way , the performance of person re-id can be greatly enhanced when training data are insufficient . experimental results on six widely-used datasets demonstrate the effectiveness of our approach and its performance can be comparable to the reported results of most state-of-the-art fully-supervised methods while using much fewer labeled data .", "topics": ["test set", "supervised learning"]}
{"title": "boost phrase-level polarity labelling with review-level sentiment classification", "abstract": "sentiment analysis on user reviews helps to keep track of user reactions towards products , and make advices to users about what to buy . state-of-the-art review-level sentiment classification techniques could give pretty good precisions of above 90 % . however , current phrase-level sentiment analysis approaches might only give sentiment polarity labelling precisions of around 70 % ~80 % , which is far from satisfaction and restricts its application in many practical tasks . in this paper , we focus on the problem of phrase-level sentiment polarity labelling and attempt to bridge the gap between phrase-level and review-level sentiment analysis . we investigate the inconsistency between the numerical star ratings and the sentiment orientation of textual user reviews . although they have long been treated as identical , which serves as a basic assumption in previous work , we find that this assumption is not necessarily true . we further propose to leverage the results of review-level sentiment classification to boost the performance of phrase-level polarity labelling using a novel constrained convex optimization framework . besides , the framework is capable of integrating various kinds of information sources and heuristics , while giving the global optimal solution due to its convexity . experimental results on both english and chinese reviews show that our framework achieves high labelling precisions of up to 89 % , which is a significant improvement from current approaches .", "topics": ["numerical analysis", "optimization problem"]}
{"title": "information density as a factor for variation in the embedding of relative clauses", "abstract": "in german , relative clauses can be positioned in-situ or extraposed . a potential factor for the variation might be information density . in this study , this hypothesis is tested with a corpus of 17th century german funeral sermons . for each referent in the relative clauses and their matrix clauses , the attention state was determined ( first calculation ) . in a second calculation , for each word the surprisal values were determined , using a bi-gram language model . in a third calculation , the surprisal values were accommodated as to whether it is the first occurrence of the word in question or not . all three calculations pointed in the same direction : with in-situ relative clauses , the rate of new referents was lower and the average surprisal values were lower , especially the accommodated surprisal values , than with extraposed relative clauses . this indicated that in-formation density is a factor governing the choice between in-situ and extraposed relative clauses . the study also sheds light on the intrinsic relation-ship between the information theoretic concept of information density and in-formation structural concepts such as givenness which are used under a more linguistic perspective .", "topics": ["value ( ethics )", "text corpus"]}
{"title": "six challenges for neural machine translation", "abstract": "we explore six challenges for neural machine translation : domain mismatch , amount of training data , rare words , long sentences , word alignment , and beam search . we show both deficiencies and improvements over the quality of phrase-based statistical machine translation .", "topics": ["test set", "machine translation"]}
{"title": "a bayesian method for joint clustering of vectorial data and network data", "abstract": "we present a new model-based integrative method for clustering objects given both vectorial data , which describes the feature of each object , and network data , which indicates the similarity of connected objects . the proposed general model is able to cluster the two types of data simultaneously within one integrative probabilistic model , while traditional methods can only handle one data type or depend on transforming one data type to another . bayesian inference of the clustering is conducted based on a markov chain monte carlo algorithm . a special case of the general model combining the gaussian mixture model and the stochastic block model is extensively studied . we used both synthetic data and real data to evaluate this new method and compare it with alternative methods . the results show that our simultaneous clustering method performs much better . this improvement is due to the power of the model-based probabilistic approach for efficiently integrating information .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "object detection using keygraphs", "abstract": "we propose a new framework for object detection based on a generalization of the keypoint correspondence framework . this framework is based on replacing keypoints by keygraphs , i.e . isomorph directed graphs whose vertices are keypoints , in order to explore relative and structural information . unlike similar works in the literature , we deal directly with graphs in the entire pipeline : we search for graph correspondences instead of searching for individual point correspondences and then building graph correspondences from them afterwards . we also estimate the pose from graph correspondences instead of falling back to point correspondences through a voting table . the contributions of this paper are the proposed framework and an implementation that properly handles its inherent issues of loss of locality and combinatorial explosion , showing its viability for real-time applications . in particular , we introduce the novel concept of keytuples to solve a running time issue . the accuracy of the implementation is shown by results of over 800 experiments with a well-known database of images . the speed is illustrated by real-time tracking with two different cameras in ordinary hardware .", "topics": ["time complexity", "object detection"]}
{"title": "dugma : dynamic uncertainty-based gaussian mixture alignment", "abstract": "registering accurately point clouds from a cheap low-resolution sensor is a challenging task . existing rigid registration methods failed to use the physical 3d uncertainty distribution of each point from a real sensor in the dynamic alignment process mainly because the uncertainty model for a point is static and invariant and it is hard to describe the change of these physical uncertainty models in the registration process . additionally , the existing gaussian mixture alignment architecture can not be efficiently implement these dynamic changes . this paper proposes a simple architecture combining error estimation from sample covariances and dual dynamic global probability alignment using the convolution of uncertainty-based gaussian mixture models ( gmm ) from point clouds . firstly , we propose an efficient way to describe the change of each 3d uncertainty model , which represents the structure of the point cloud much better . unlike the invariant gmm ( representing a fixed point cloud ) in traditional gaussian mixture alignment , we use two uncertainty-based gmms that change and interact with each other in each iteration . in order to have a wider basin of convergence than other local algorithms , we design a more robust energy function by convolving efficiently the two gmms over the whole 3d space . tens of thousands of trials have been conducted on hundreds of models from multiple datasets to demonstrate the proposed method 's superior performance compared with the current state-of-the-art methods . the new dataset and code is available from https : //github.com/canpu999", "topics": ["mathematical optimization", "iteration"]}
{"title": "an image dataset of text patches in everyday scenes", "abstract": "this paper describes a dataset containing small images of text from everyday scenes . the purpose of the dataset is to support the development of new automated systems that can detect and analyze text . although much research has been devoted to text detection and recognition in scanned documents , relatively little attention has been given to text detection in other types of images , such as photographs that are posted on social-media sites . this new dataset , known as coco-text-patch , contains approximately 354,000 small images that are each labeled as `` text '' or `` non-text '' . this dataset particularly addresses the problem of text verification , which is an essential stage in the end-to-end text detection and recognition pipeline . in order to evaluate the utility of this dataset , it has been used to train two deep convolution neural networks to distinguish text from non-text . one network is inspired by the googlenet architecture , and the second one is based on caffenet . accuracy levels of 90.2 % and 90.9 % were obtained using the two networks , respectively . all of the images , source code , and deep-learning trained models described in this paper will be publicly available", "topics": ["end-to-end principle", "convolution"]}
{"title": "bin completion algorithms for multicontainer packing , knapsack , and covering problems", "abstract": "many combinatorial optimization problems such as the bin packing and multiple knapsack problems involve assigning a set of discrete objects to multiple containers . these problems can be used to model task and resource allocation problems in multi-agent systems and distributed systms , and can also be found as subproblems of scheduling problems . we propose bin completion , a branch-and-bound strategy for one-dimensional , multicontainer packing problems . bin completion combines a bin-oriented search space with a powerful dominance criterion that enables us to prune much of the space . the performance of the basic bin completion framework can be enhanced by using a number of extensions , including nogood-based pruning techniques that allow further exploitation of the dominance criterion . bin completion is applied to four problems : multiple knapsack , bin covering , min-cost covering , and bin packing . we show that our bin completion algorithms yield new , state-of-the-art results for the multiple knapsack , bin covering , and min-cost covering problems , outperforming previous algorithms by several orders of magnitude with respect to runtime on some classes of hard , random problem instances . for the bin packing problem , we demonstrate significant improvements compared to most previous results , but show that bin completion is not competitive with current state-of-the-art cutting-stock based approaches .", "topics": ["eisenstein 's criterion"]}
{"title": "recurrent neural networks to correct satellite image classification maps", "abstract": "while initially devised for image categorization , convolutional neural networks ( cnns ) are being increasingly used for the pixelwise semantic labeling of images . however , the proper nature of the most common cnn architectures makes them good at recognizing but poor at localizing objects precisely . this problem is magnified in the context of aerial and satellite image labeling , where a spatially fine object outlining is of paramount importance . different iterative enhancement algorithms have been presented in the literature to progressively improve the coarse cnn outputs , seeking to sharpen object boundaries around real image edges . however , one must carefully design , choose and tune such algorithms . instead , our goal is to directly learn the iterative process itself . for this , we formulate a generic iterative enhancement process inspired from partial differential equations , and observe that it can be expressed as a recurrent neural network ( rnn ) . consequently , we train such a network from manually labeled data for our enhancement task . in a series of experiments we show that our rnn effectively learns an iterative process that significantly improves the quality of satellite image classification maps .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "bridging cognitive programs and machine learning", "abstract": "while great advances are made in pattern recognition and machine learning , the successes of such fields remain restricted to narrow applications and seem to break down when training data is scarce , a shift in domain occurs , or when intelligent reasoning is required for rapid adaptation to new environments . in this work , we list several of the shortcomings of modern machine-learning solutions , specifically in the contexts of computer vision and in reinforcement learning and suggest directions to explore in order to try to ameliorate these weaknesses .", "topics": ["test set", "reinforcement learning"]}
{"title": "learning discriminative relational features for sequence labeling", "abstract": "discovering relational structure between input features in sequence labeling models has shown to improve their accuracy in several problem settings . however , the search space of relational features is exponential in the number of basic input features . consequently , approaches that learn relational features , tend to follow a greedy search strategy . in this paper , we study the possibility of optimally learning and applying discriminative relational features for sequence labeling . for learning features derived from inputs at a particular sequence position , we propose a hierarchical kernels-based approach ( referred to as hierarchical kernel learning for structured output spaces - structhkl ) . this approach optimally and efficiently explores the hierarchical structure of the feature space for problems with structured output spaces such as sequence labeling . since the structhkl approach has limitations in learning complex relational features derived from inputs at relative positions , we propose two solutions to learn relational features namely , ( i ) enumerating simple component features of complex relational features and discovering their compositions using structhkl and ( ii ) leveraging relational kernels , that compute the similarity between instances implicitly , in the sequence labeling problem . we perform extensive empirical evaluation on publicly available datasets and record our observations on settings in which certain approaches are effective .", "topics": ["feature vector", "mathematical optimization"]}
{"title": "learning diverse image colorization", "abstract": "colorization is an ambiguous problem , with multiple viable colorizations for a single grey-level image . however , previous methods only produce the single most probable colorization . our goal is to model the diversity intrinsic to the problem of colorization and produce multiple colorizations that display long-scale spatial co-ordination . we learn a low dimensional embedding of color fields using a variational autoencoder ( vae ) . we construct loss terms for the vae decoder that avoid blurry outputs and take into account the uneven distribution of pixel colors . finally , we build a conditional model for the multi-modal distribution between grey-level image and the color field embeddings . samples from this conditional model result in diverse colorization . we demonstrate that our method obtains better diverse colorizations than a standard conditional variational autoencoder ( cvae ) model , as well as a recently proposed conditional generative adversarial network ( cgan ) .", "topics": ["calculus of variations", "autoencoder"]}
{"title": "root13 : spotting hypernyms , co-hyponyms and randoms", "abstract": "in this paper , we describe root13 , a supervised system for the classification of hypernyms , co-hyponyms and random words . the system relies on a random forest algorithm and 13 unsupervised corpus-based features . we evaluate it with a 10-fold cross validation on 9,600 pairs , equally distributed among the three classes and involving several parts-of-speech ( i.e . adjectives , nouns and verbs ) . when all the classes are present , root13 achieves an f1 score of 88.3 % , against a baseline of 57.6 % ( vector cosine ) . when the classification is binary , root13 achieves the following results : hypernyms-co-hyponyms ( 93.4 % vs. 60.2 % ) , hypernymsrandom ( 92.3 % vs. 65.5 % ) and co-hyponyms-random ( 97.3 % vs. 81.5 % ) . our results are competitive with stateof-the-art models .", "topics": ["baseline ( configuration management )", "supervised learning"]}
{"title": "causal transportability of experiments on controllable subsets of variables : z-transportability", "abstract": "we introduce z-transportability , the problem of estimating the causal effect of a set of variables x on another set of variables y in a target domain from experiments on any subset of controllable variables z where z is an arbitrary subset of observable variables v in a source domain . z-transportability generalizes z-identifiability , the problem of estimating in a given domain the causal effect of x on y from surrogate experiments on a set of variables z such that z is disjoint from x ; . z-transportability also generalizes transportability which requires that the causal effect of x on y in the target domain be estimable from experiments on any subset of all observable variables in the source domain . we first generalize z-identifiability to allow cases where z is not necessarily disjoint from x . then , we establish a necessary and sufficient condition for z-transportability in terms of generalized z-identifiability and transportability . we provide a correct and complete algorithm that determines whether a causal effect is z-transportable ; and if it is , produces a transport formula , that is , a recipe for estimating the causal effect of x on y in the target domain using information elicited from the results of experimental manipulations of z in the source domain and observational data from the target domain . our results also show that do-calculus is complete for z-transportability .", "topics": ["causality"]}
{"title": "video captioning with transferred semantic attributes", "abstract": "automatically generating natural language descriptions of videos plays a fundamental challenge for computer vision community . most recent progress in this problem has been achieved through employing 2-d and/or 3-d convolutional neural networks ( cnn ) to encode video content and recurrent neural networks ( rnn ) to decode a sentence . in this paper , we present long short-term memory with transferred semantic attributes ( lstm-tsa ) -- -a novel deep architecture that incorporates the transferred semantic attributes learnt from images and videos into the cnn plus rnn framework , by training them in an end-to-end manner . the design of lstm-tsa is highly inspired by the facts that 1 ) semantic attributes play a significant contribution to captioning , and 2 ) images and videos carry complementary semantics and thus can reinforce each other for captioning . to boost video captioning , we propose a novel transfer unit to model the mutually correlated attributes learnt from images and videos . extensive experiments are conducted on three public datasets , i.e . , msvd , m-vad and mpii-md . our proposed lstm-tsa achieves to-date the best published performance in sentence generation on msvd : 52.8 % and 74.0 % in terms of bleu @ 4 and cider-d. superior results when compared to state-of-the-art methods are also reported on m-vad and mpii-md .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "a markov chain theory approach to characterizing the minimax optimality of stochastic gradient descent ( for least squares )", "abstract": "this work provides a simplified proof of the statistical minimax optimality of ( iterate averaged ) stochastic gradient descent ( sgd ) , for the special case of least squares . this result is obtained by analyzing sgd as a stochastic process and by sharply characterizing the stationary covariance matrix of this process . the finite rate optimality characterization captures the constant factors and addresses model mis-specification .", "topics": ["gradient descent", "gradient"]}
{"title": "variance-reduced and projection-free stochastic optimization", "abstract": "the frank-wolfe optimization algorithm has recently regained popularity for machine learning applications due to its projection-free property and its ability to handle structured constraints . however , in the stochastic learning setting , it is still relatively understudied compared to the gradient descent counterpart . in this work , leveraging a recent variance reduction technique , we propose two stochastic frank-wolfe variants which substantially improve previous results in terms of the number of stochastic gradient evaluations needed to achieve $ 1-\\epsilon $ accuracy . for example , we improve from $ o ( \\frac { 1 } { \\epsilon } ) $ to $ o ( \\ln\\frac { 1 } { \\epsilon } ) $ if the objective function is smooth and strongly convex , and from $ o ( \\frac { 1 } { \\epsilon^2 } ) $ to $ o ( \\frac { 1 } { \\epsilon^ { 1.5 } } ) $ if the objective function is smooth and lipschitz . the theoretical improvement is also observed in experiments on real-world datasets for a multiclass classification application .", "topics": ["loss function", "gradient descent"]}
{"title": "incremental learning of 3d-dct compact representations for robust visual tracking", "abstract": "visual tracking usually requires an object appearance model that is robust to changing illumination , pose and other factors encountered in video . in this paper , we construct an appearance model using the 3d discrete cosine transform ( 3d-dct ) . the 3d-dct is based on a set of cosine basis functions , which are determined by the dimensions of the 3d signal and thus independent of the input video data . in addition , the 3d-dct can generate a compact energy spectrum whose high-frequency coefficients are sparse if the appearance samples are similar . by discarding these high-frequency coefficients , we simultaneously obtain a compact 3d-dct based object representation and a signal reconstruction-based similarity measure ( reflecting the information loss from signal reconstruction ) . to efficiently update the object representation , we propose an incremental 3d-dct algorithm , which decomposes the 3d-dct into successive operations of the 2d discrete cosine transform ( 2d-dct ) and 1d discrete cosine transform ( 1d-dct ) on the input video data .", "topics": ["computational complexity theory", "sparse matrix"]}
{"title": "an efficient algorithm for surface generation", "abstract": "a method is given that `` inverts '' a logic grammar and displays it from the point of view of the logical form , rather than from that of the word string . lr-compiling techniques are used to allow a recursive-descent generation algorithm to perform `` functor merging '' much in the same way as an lr parser performs prefix merging . this is an improvement on the semantic-head-driven generator that results in a much smaller search space . the amount of semantic lookahead can be varied , and appropriate tradeoff points between table size and resulting nondeterminism can be found automatically .", "topics": ["natural language processing", "time complexity"]}
{"title": "a stochastic gradient method with an exponential convergence rate for finite training sets", "abstract": "we propose a new stochastic gradient method for optimizing the sum of a finite set of smooth functions , where the sum is strongly convex . while standard stochastic gradient methods converge at sublinear rates for this problem , the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence rate . in a machine learning context , numerical experiments indicate that the new algorithm can dramatically outperform standard algorithms , both in terms of optimizing the training error and reducing the test error quickly .", "topics": ["numerical analysis", "gradient"]}
{"title": "integral policy iterations for reinforcement learning problems in continuous time and space", "abstract": "policy iteration ( pi ) is a recursive process of policy evaluation and improvement to solve an optimal decision-making , e.g . , reinforcement learning ( rl ) or optimal control problem and has served as the fundamental to develop rl methods . motivated by integral pi ( ipi ) schemes in optimal control and rl methods in continuous time and space ( cts ) , this paper proposes on-policy ipi to solve the general rl problem in cts , with its environment modeled by an ordinary differential equation ( ode ) . in such continuous domain , we also propose four off-policy ipi methods -- -two are the ideal pi forms that use advantage and q-functions , respectively , and the other two are natural extensions of the existing off-policy ipi schemes to our general rl framework . compared to the ipi methods in optimal control , the proposed ipi schemes can be applied to more general situations and do not require an initial stabilizing policy to run ; they are also strongly relevant to the rl algorithms in cts such as advantage updating , q-learning , and value-gradient based ( vgb ) greedy policy improvement . our on-policy ipi is basically model-based but can be made partially model-free ; each off-policy method is also either partially or completely model-free . the mathematical properties of the ipi methods -- -admissibility , monotone improvement , and convergence towards the optimal solution -- -are all rigorously proven , together with the equivalence of on- and off-policy ipi . finally , the ipi methods are simulated with an inverted-pendulum model to support the theory and verify the performance .", "topics": ["optimization problem", "reinforcement learning"]}
{"title": "a new kernel-based approach to system identification with quantized output data", "abstract": "in this paper we introduce a novel method for linear system identification with quantized output data . we model the impulse response as a zero-mean gaussian process whose covariance ( kernel ) is given by the recently proposed stable spline kernel , which encodes information on regularity and exponential stability . this serves as a starting point to cast our system identification problem into a bayesian framework . we employ markov chain monte carlo methods to provide an estimate of the system . in particular , we design two methods based on the so-called gibbs sampler that allow also to estimate the kernel hyperparameters by marginal likelihood maximization via the expectation-maximization method . numerical simulations show the effectiveness of the proposed scheme , as compared to the state-of-the-art kernel-based methods when these are employed in system identification with quantized data .", "topics": ["kernel ( operating system )", "sampling ( signal processing )"]}
{"title": "identifying reasoning patterns in games", "abstract": "we present an algorithm that identifies the reasoning patterns of agents in a game , by iteratively examining the graph structure of its multi-agent influence diagram ( maid ) representation . if the decision of an agent participates in no reasoning patterns , then we can effectively ignore that decision for the purpose of calculating a nash equilibrium for the game . in some cases , this can lead to exponential time savings in the process of equilibrium calculation . moreover , our algorithm can be used to enumerate the reasoning patterns in a game , which can be useful for constructing more effective computerized agents interacting with humans .", "topics": ["time complexity"]}
{"title": "stability of phase retrievable frames", "abstract": "in this paper we study the property of phase retrievability by redundant sysems of vectors under perturbations of the frame set . specifically we show that if a set $ \\fc $ of $ m $ vectors in the complex hilbert space of dimension n allows for vector reconstruction from magnitudes of its coefficients , then there is a perturbation bound $ \\rho $ so that any frame set within $ \\rho $ from $ \\fc $ has the same property . in particular this proves the recent construction in \\cite { bh13 } is stable under perturbations . by the same token we reduce the critical cardinality conjectured in \\cite { bcmn13a } to proving a stability result for non phase-retrievable frames .", "topics": ["coefficient"]}
{"title": "interacting conceptual spaces i : grammatical composition of concepts", "abstract": "the categorical compositional approach to meaning has been successfully applied in natural language processing , outperforming other models in mainstream empirical language processing tasks . we show how this approach can be generalized to conceptual space models of cognition . in order to do this , first we introduce the category of convex relations as a new setting for categorical compositional semantics , emphasizing the convex structure important to conceptual space applications . we then show how to construct conceptual spaces for various types such as nouns , adjectives and verbs . finally we show by means of examples how concepts can be systematically combined to establish the meanings of composite phrases from the meanings of their constituent parts . this provides the mathematical underpinnings of a new compositional approach to cognition .", "topics": ["natural language processing"]}
{"title": "data augmentation for brain-computer interfaces : analysis on event-related potentials data", "abstract": "on image data , data augmentation is becoming less relevant due to the large amount of available training data and regularization techniques . common approaches are moving windows ( cropping ) , scaling , affine distortions , random noise , and elastic deformations . for electroencephalographic data , the lack of sufficient training data is still a major issue . we suggest and evaluate different approaches to generate augmented data using temporal and spatial/rotational distortions . our results on the perception of rare stimuli ( p300 data ) and movement prediction ( mrcp data ) show that these approaches are feasible and can significantly increase the performance of signal processing chains for brain-computer interfaces by 1 % to 6 % .", "topics": ["test set", "matrix regularization"]}
{"title": "model selection for topic models via spectral decomposition", "abstract": "topic models have achieved significant successes in analyzing large-scale text corpus . in practical applications , we are always confronted with the challenge of model selection , i.e . , how to appropriately set the number of topics . following recent advances in topic model inference via tensor decomposition , we make a first attempt to provide theoretical analysis on model selection in latent dirichlet allocation . under mild conditions , we derive the upper bound and lower bound on the number of topics given a text collection of finite size . experimental results demonstrate that our bounds are accurate and tight . furthermore , using gaussian mixture model as an example , we show that our methodology can be easily generalized to model selection analysis for other latent models .", "topics": ["synthetic data"]}
{"title": "evidence absorption and propagation through evidence reversals", "abstract": "the arc reversal/node reduction approach to probabilistic inference is extended to include the case of instantiated evidence by an operation called `` evidence reversal . '' this not only provides a technique for computing posterior joint distributions on general belief networks , but also provides insight into the methods of pearl [ 1986b ] and lauritzen and spiegelhalter [ 1988 ] . although it is well understood that the latter two algorithms are closely related , in fact all three algorithms are identical whenever the belief network is a forest .", "topics": ["bayesian network"]}
{"title": "active learning of custering with side information using $ \\eps $ -smooth relative regret approximations", "abstract": "clustering is considered a non-supervised learning setting , in which the goal is to partition a collection of data points into disjoint clusters . often a bound $ k $ on the number of clusters is given or assumed by the practitioner . many versions of this problem have been defined , most notably $ k $ -means and $ k $ -median . an underlying problem with the unsupervised nature of clustering it that of determining a similarity function . one approach for alleviating this difficulty is known as clustering with side information , alternatively , semi-supervised clustering . here , the practitioner incorporates side information in the form of `` must be clustered '' or `` must be separated '' labels for data point pairs . each such piece of information comes at a `` query cost '' ( often involving human response solicitation ) . the collection of labels is then incorporated in the usual clustering algorithm as either strict or as soft constraints , possibly adding a pairwise constraint penalty function to the chosen clustering objective . our work is mostly related to clustering with side information . we ask how to choose the pairs of data points . our analysis gives rise to a method provably better than simply choosing them uniformly at random . roughly speaking , we show that the distribution must be biased so as more weight is placed on pairs incident to elements in smaller clusters in some optimal solution . of course we do not know the optimal solution , hence we do n't know the bias . using the recently introduced method of $ \\eps $ -smooth relative regret approximations of ailon , begleiter and ezra , we can show an iterative process that improves both the clustering and the bias in tandem . the process provably converges to the optimal solution faster ( in terms of query cost ) than an algorithm selecting pairs uniformly .", "topics": ["regret ( decision theory )", "cluster analysis"]}
{"title": "selective classification for deep neural networks", "abstract": "selective classification techniques ( also known as reject option ) have not yet been considered in the context of deep neural networks ( dnns ) . these techniques can potentially significantly improve dnns prediction performance by trading-off coverage . in this paper we propose a method to construct a selective classifier given a trained neural network . our method allows a user to set a desired risk level . at test time , the classifier rejects instances as needed , to grant the desired risk ( with high probability ) . empirical results over cifar and imagenet convincingly demonstrate the viability of our method , which opens up possibilities to operate dnns in mission-critical applications . for example , using our method an unprecedented 2 % error in top-5 imagenet classification can be guaranteed with probability 99.9 % , and almost 60 % test coverage .", "topics": ["neural networks"]}
{"title": "efficient visual coding : from retina to v2", "abstract": "the human visual system has a hierarchical structure consisting of layers of processing , such as the retina , v1 , v2 , etc . understanding the functional roles of these visual processing layers would help to integrate the psychophysiological and neurophysiological models into a consistent theory of human vision , and would also provide insights to computer vision research . one classical theory of the early visual pathway hypothesizes that it serves to capture the statistical structure of the visual inputs by efficiently coding the visual information in its outputs . until recently , most computational models following this theory have focused upon explaining the receptive field properties of one or two visual layers . recent work in deep networks has eliminated this concern , however , there is till the retinal layer to consider . here we improve on a previously-described hierarchical model recursive ica ( rica ) [ 1 ] which starts with pca , followed by a layer of sparse coding or ica , followed by a component-wise nonlinearity derived from considerations of the variable distributions expected by ica . this process is then repeated . in this work , we improve on this model by using a new version of sparse pca ( spca ) , which results in biologically-plausible receptive fields for both the spca and ica/sparse coding . when applied to natural image patches , our model learns visual features exhibiting the receptive field properties of retinal ganglion cells/lateral geniculate nucleus ( lgn ) cells , v1 simple cells , v1 complex cells , and v2 cells . our work provides predictions for experimental neuroscience studies . for example , our result suggests that a previous neurophysiological study improperly discarded some of their recorded neurons ; we predict that their discarded neurons capture the shape contour of objects .", "topics": ["nonlinear system", "computer vision"]}
{"title": "learning cross-modal deep representations for robust pedestrian detection", "abstract": "this paper presents a novel method for detecting pedestrians under adverse illumination conditions . our approach relies on a novel cross-modality learning framework and it is based on two main phases . first , given a multimodal dataset , a deep convolutional network is employed to learn a non-linear mapping , modeling the relations between rgb and thermal data . then , the learned feature representations are transferred to a second deep network , which receives as input an rgb image and outputs the detection results . in this way , features which are both discriminative and robust to bad illumination conditions are learned . importantly , at test time , only the second pipeline is considered and no thermal data are required . our extensive evaluation demonstrates that the proposed approach outperforms the state-of- the-art on the challenging kaist multispectral pedestrian dataset and it is competitive with previous methods on the popular caltech dataset .", "topics": ["nonlinear system"]}
{"title": "mixing representation levels : the hybrid approach to automatic text generation", "abstract": "natural language generation systems ( nlg ) map non-linguistic representations into strings of words through a number of steps using intermediate representations of various levels of abstraction . template based systems , by contrast , tend to use only one representation level , i.e . fixed strings , which are combined , possibly in a sophisticated way , to generate the final text . in some circumstances , it may be profitable to combine nlg and template based techniques . the issue of combining generation techniques can be seen in more abstract terms as the issue of mixing levels of representation of different degrees of linguistic abstraction . this paper aims at defining a reference architecture for systems using mixed representations . we argue that mixed representations can be used without abandoning a linguistically grounded approach to language generation .", "topics": ["natural language"]}
{"title": "binary classifier calibration : non-parametric approach", "abstract": "accurate calibration of probabilistic predictive models learned is critical for many practical prediction and decision-making tasks . there are two main categories of methods for building calibrated classifiers . one approach is to develop methods for learning probabilistic models that are well-calibrated , ab initio . the other approach is to use some post-processing methods for transforming the output of a classifier to be well calibrated , as for example histogram binning , platt scaling , and isotonic regression . one advantage of the post-processing approach is that it can be applied to any existing probabilistic classification model that was constructed using any machine-learning method . in this paper , we first introduce two measures for evaluating how well a classifier is calibrated . we prove three theorems showing that using a simple histogram binning post-processing method , it is possible to make a classifier be well calibrated while retaining its discrimination capability . also , by casting the histogram binning method as a density-based non-parametric binary classifier , we can extend it using two simple non-parametric density estimation methods . we demonstrate the performance of the proposed calibration methods on synthetic and real datasets . experimental results show that the proposed methods either outperform or are comparable to existing calibration methods .", "topics": ["statistical classification", "synthetic data"]}
{"title": "supervised logeuclidean metric learning for symmetric positive definite matrices", "abstract": "metric learning has been shown to be highly effective to improve the performance of nearest neighbor classification . in this paper , we address the problem of metric learning for symmetric positive definite ( spd ) matrices such as covariance matrices , which arise in many real-world applications . naively using standard mahalanobis metric learning methods under the euclidean geometry for spd matrices is not appropriate , because the difference of spd matrices can be a non-spd matrix and thus the obtained solution can be uninterpretable . to cope with this problem , we propose to use a properly parameterized logeuclidean distance and optimize the metric with respect to kernel-target alignment , which is a supervised criterion for kernel learning . then the resulting non-trivial optimization problem is solved by utilizing the riemannian geometry . finally , we experimentally demonstrate the usefulness of our logeuclidean metric learning algorithm on real-world classification tasks for eeg signals and texture patches .", "topics": ["optimization problem", "eisenstein 's criterion"]}
{"title": "rule-based spanish morphological analyzer built from spell checking lexicon", "abstract": "preprocessing tools for automated text analysis have become more widely available in major languages , but non-english tools are often still limited in their functionality . when working with spanish-language text , researchers can easily find tools for tokenization and stemming , but may not have the means to extract more complex word features like verb tense or mood . yet spanish is a morphologically rich language in which such features are often identifiable from word form . conjugation rules are consistent , but many special verbs and nouns take on different rules . while building a complete dictionary of known words and their morphological rules would be labor intensive , resources to do so already exist , in spell checkers designed to generate valid forms of known words . this paper introduces a set of tools for spanish-language morphological analysis , built using the coes spell checking tools , to label person , mood , tense , gender and number , derive a word 's root noun or verb infinitive , and convert verbs to their nominal form .", "topics": ["dictionary"]}
{"title": "automatically extracting action graphs from materials science synthesis procedures", "abstract": "computational synthesis planning approaches have achieved recent success in organic chemistry , where tabulated synthesis procedures are readily available for supervised learning . the syntheses of inorganic materials , however , exist primarily as natural language narratives contained within scientific journal articles . this synthesis information must first be extracted from the text in order to enable analogous synthesis planning methods for inorganic materials . in this work , we present a system for automatically extracting structured representations of synthesis procedures from the texts of materials science journal articles that describe explicit , experimental syntheses of inorganic compounds . we define the structured representation as a set of linked events made up of extracted scientific entities and evaluate two unsupervised approaches for extracting these structures on expert-annotated articles : a strong heuristic baseline and a generative model of procedural text . we also evaluate a variety of supervised models for extracting scientific entities . our results provide insight into the nature of the data and directions for further work in this exciting new area of research .", "topics": ["baseline ( configuration management )", "generative model"]}
{"title": "online multi-task learning with hard constraints", "abstract": "we discuss multi-task online learning when a decision maker has to deal simultaneously with m tasks . the tasks are related , which is modeled by imposing that the m-tuple of actions taken by the decision maker needs to satisfy certain constraints . we give natural examples of such restrictions and then discuss a general class of tractable constraints , for which we introduce computationally efficient ways of selecting actions , essentially by reducing to an on-line shortest path problem . we briefly discuss `` tracking '' and `` bandit '' versions of the problem and extend the model in various ways , including non-additive global losses and uncountably infinite sets of tasks .", "topics": ["computational complexity theory"]}
{"title": "sapfocs : a metaheuristic based approach to part family formation problems in group technology", "abstract": "this article deals with part family formation problem which is believed to be moderately complicated to be solved in polynomial time in the vicinity of group technology ( gt ) . in the past literature researchers investigated that the part family formation techniques are principally based on production flow analysis ( pfa ) which usually considers operational requirements , sequences and time . part coding analysis ( pca ) is merely considered in gt which is believed to be the proficient method to identify the part families . pca classifies parts by allotting them to different families based on their resemblances in : ( 1 ) design characteristics such as shape and size , and/or ( 2 ) manufacturing characteristics ( machining requirements ) . a novel approach based on simulated annealing namely sapfocs is adopted in this study to develop effective part families exploiting the pca technique . thereafter taguchi 's orthogonal design method is employed to solve the critical issues on the subject of parameters selection for the proposed metaheuristic algorithm . the adopted technique is therefore tested on 5 different datasets of size 5 { \\times } 9 to 27 { \\times } 9 and the obtained results are compared with c-linkage clustering technique . the experimental results reported that the proposed metaheuristic algorithm is extremely effective in terms of the quality of the solution obtained and has outperformed c-linkage algorithm in most instances .", "topics": ["cluster analysis", "time complexity"]}
{"title": "online cluster validity indices for streaming data", "abstract": "cluster analysis is used to explore structure in unlabeled data sets in a wide range of applications . an important part of cluster analysis is validating the quality of computationally obtained clusters . a large number of different internal indices have been developed for validation in the offline setting . however , this concept has not been extended to the online setting . a key challenge is to find an efficient incremental formulation of an index that can capture both cohesion and separation of the clusters over potentially infinite data streams . in this paper , we develop two online versions ( with and without forgetting factors ) of the xie-beni and davies-bouldin internal validity indices , and analyze their characteristics , using two streaming clustering algorithms ( sk-means and online ellipsoidal clustering ) , and illustrate their use in monitoring evolving clusters in streaming data . we also show that incremental cluster validity indices are capable of sending a distress signal to online monitors when evolving clusters go awry . our numerical examples indicate that the incremental xie-beni index with forgetting factor is superior to the other three indices tested .", "topics": ["cluster analysis", "data mining"]}
{"title": "core sampling framework for pixel classification", "abstract": "the intermediate map responses of a convolutional neural network ( cnn ) contain information about an image that can be used to extract contextual knowledge about it . in this paper , we present a core sampling framework that is able to use these activation maps from several layers as features to another neural network using transfer learning to provide an understanding of an input image . our framework creates a representation that combines features from the test data and the contextual knowledge gained from the responses of a pretrained network , processes it and feeds it to a separate deep belief network . we use this representation to extract more information from an image at the pixel level , hence gaining understanding of the whole image . we experimentally demonstrate the usefulness of our framework using a pretrained vgg-16 model to perform segmentation on the baeri dataset of synthetic aperture radar ( sar ) imagery and the camvid dataset .", "topics": ["sampling ( signal processing )", "map"]}
{"title": "emotion recognition from speech with recurrent neural networks", "abstract": "in this paper the task of emotion recognition from speech is considered . proposed approach uses deep recurrent neural network trained on a sequence of acoustic features calculated over small speech intervals . at the same time special probabilistic-nature ctc loss function allows to consider long utterances containing both emotional and unemotional parts . the effectiveness of such an approach is shown in two ways . first one is the comparison with recent advances in this field . while second way implies measuring human performance on the same task , which also was done by authors .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "residual connections encourage iterative inference", "abstract": "residual networks ( resnets ) have become a prominent architecture in deep learning . however , a comprehensive understanding of resnets is still a topic of ongoing research . a recent view argues that resnets perform iterative refinement of features . we attempt to further expose properties of this aspect . to this end , we study resnets both analytically and empirically . we formalize the notion of iterative refinement in resnets by showing that residual connections naturally encourage features of residual blocks to move along the negative gradient of loss as we go from one block to the next . in addition , our empirical analysis suggests that resnets are able to perform both representation learning and iterative refinement . in general , a resnet block tends to concentrate representation learning behavior in the first few layers while higher layers perform iterative refinement of features . finally we observe that sharing residual layers naively leads to representation explosion and counterintuitively , overfitting , and we show that simple existing strategies can help alleviating this problem .", "topics": ["feature learning", "iteration"]}
{"title": "large scale biomedical texts classification : a knn and an esa-based approaches", "abstract": "with the large and increasing volume of textual data , automated methods for identifying significant topics to classify textual documents have received a growing interest . while many efforts have been made in this direction , it still remains a real challenge . moreover , the issue is even more complex as full texts are not always freely available . then , using only partial information to annotate these documents is promising but remains a very ambitious issue . methodswe propose two classification methods : a k-nearest neighbours ( knn ) -based approach and an explicit semantic analysis ( esa ) -based approach . although the knn-based approach is widely used in text classification , it needs to be improved to perform well in this specific classification problem which deals with partial information . compared to existing knn-based methods , our method uses classical machine learning ( ml ) algorithms for ranking the labels . additional features are also investigated in order to improve the classifiers ' performance . in addition , the combination of several learning algorithms with various techniques for fixing the number of relevant topics is performed . on the other hand , esa seems promising for this classification task as it yielded interesting results in related issues , such as semantic relatedness computation between texts and text classification . unlike existing works , which use esa for enriching the bag-of-words approach with additional knowledge-based features , our esa-based method builds a standalone classifier . furthermore , we investigate if the results of this method could be useful as a complementary feature of our knn-based approach.resultsexperimental evaluations performed on large standard annotated datasets , provided by the bioasq organizers , show that the knn-based method with the random forest learning algorithm achieves good performances compared with the current state-of-the-art methods , reaching a competitive f-measure of 0.55 % while the esa-based approach surprisingly yielded reserved results.conclusionswe have proposed simple classification methods suitable to annotate textual documents using only partial information . they are therefore adequate for large multi-label classification and particularly in the biomedical domain . thus , our work contributes to the extraction of relevant information from unstructured documents in order to facilitate their automated processing . consequently , it could be used for various purposes , including document indexing , information retrieval , etc .", "topics": ["statistical classification", "text corpus"]}
{"title": "neural networks architecture evaluation in a quantum computer", "abstract": "in this work , we propose a quantum algorithm to evaluate neural networks architectures named quantum neural network architecture evaluation ( qnnae ) . the proposed algorithm is based on a quantum associative memory and the learning algorithm for artificial neural networks . unlike conventional algorithms for evaluating neural network architectures , qnnae does not depend on initialization of weights . the proposed algorithm has a binary output and results in 0 with probability proportional to the performance of the network . and its computational cost is equal to the computational cost to train a neural network .", "topics": ["neural networks"]}
{"title": "the price of anarchy in auctions", "abstract": "this survey outlines a general and modular theory for proving approximation guarantees for equilibria of auctions in complex settings . this theory complements traditional economic techniques , which generally focus on exact and optimal solutions and are accordingly limited to relatively stylized settings . we highlight three user-friendly analytical tools : smoothness-type inequalities , which immediately yield approximation guarantees for many auction formats of interest in the special case of complete information and deterministic strategies ; extension theorems , which extend such guarantees to randomized strategies , no-regret learning outcomes , and incomplete-information settings ; and composition theorems , which extend such guarantees from simpler to more complex auctions . combining these tools yields tight worst-case approximation guarantees for the equilibria of many widely-used auction formats .", "topics": ["regret ( decision theory )"]}
{"title": "cross temporal recurrent networks for ranking question answer pairs", "abstract": "temporal gates play a significant role in modern recurrent-based neural encoders , enabling fine-grained control over recursive compositional operations over time . in recurrent models such as the long short-term memory ( lstm ) , temporal gates control the amount of information retained or discarded over time , not only playing an important role in influencing the learned representations but also serving as a protection against vanishing gradients . this paper explores the idea of learning temporal gates for sequence pairs ( question and answer ) , jointly influencing the learned representations in a pairwise manner . in our approach , temporal gates are learned via 1d convolutional layers and then subsequently cross applied across question and answer for joint learning . empirically , we show that this conceptually simple sharing of temporal gates can lead to competitive performance across multiple benchmarks . intuitively , what our network achieves can be interpreted as learning representations of question and answer pairs that are aware of what each other is remembering or forgetting , i.e . , pairwise temporal gating . via extensive experiments , we show that our proposed model achieves state-of-the-art performance on two community-based qa datasets and competitive performance on one factoid-based qa dataset .", "topics": ["encoder"]}
{"title": "attempto controlled english ( ace )", "abstract": "attempto controlled english ( ace ) allows domain specialists to interactively formulate requirements specifications in domain concepts . ace can be accurately and efficiently processed by a computer , but is expressive enough to allow natural usage . the attempto system translates specification texts in ace into discourse representation structures and optionally into prolog . translated specification texts are incrementally added to a knowledge base . this knowledge base can be queried in ace for verification , and it can be executed for simulation , prototyping and validation of the specification .", "topics": ["simulation"]}
{"title": "ahugin : a system creating adaptive causal probabilistic networks", "abstract": "the paper describes ahugin , a tool for creating adaptive systems . ahugin is an extension of the hugin shell , and is based on the methods reported by spiegelhalter and lauritzen ( 1990a ) . the adaptive systems resulting from ahugin are able to adjust the c011ditional probabilities in the model . a short analysis of the adaptation task is given and the features of ahugin are described . finally a session with experiments is reported and the results are discussed .", "topics": ["causality"]}
{"title": "use of dempster-shafer conflict metric to detect interpretation inconsistency", "abstract": "a model of the world built from sensor data may be incorrect even if the sensors are functioning correctly . possible causes include the use of inappropriate sensors ( e.g . a laser looking through glass walls ) , sensor inaccuracies accumulate ( e.g . localization errors ) , the a priori models are wrong , or the internal representation does not match the world ( e.g . a static occupancy grid used with dynamically moving objects ) . we are interested in the case where the constructed model of the world is flawed , but there is no access to the ground truth that would allow the system to see the discrepancy , such as a robot entering an unknown environment . this paper considers the problem of determining when something is wrong using only the sensor data used to construct the world model . it proposes 11 interpretation inconsistency indicators based on the dempster-shafer conflict metric , con , and evaluates these indicators according to three criteria : ability to distinguish true inconsistency from sensor noise ( classification ) , estimate the magnitude of discrepancies ( estimation ) , and determine the source ( s ) ( if any ) of sensing problems in the environment ( isolation ) . the evaluation is conducted using data from a mobile robot with sonar and laser range sensors navigating indoor environments under controlled conditions . the evaluation shows that the gambino indicator performed best in terms of estimation ( at best 0.77 correlation ) , isolation , and classification of the sensing situation as degraded ( 7 % false negative rate ) or normal ( 0 % false positive rate ) .", "topics": ["ground truth", "sensor"]}
{"title": "representing and reasoning with probabilistic knowledge : a bayesian approach", "abstract": "pagoda ( probabilistic autonomous goal-directed agent ) is a model for autonomous learning in probabilistic domains [ desjardins , 1992 ] that incorporates innovative techniques for using the agent 's existing knowledge to guide and constrain the learning process and for representing , reasoning with , and learning probabilistic knowledge . this paper describes the probabilistic representation and inference mechanism used in pagoda . pagoda forms theories about the effects of its actions and the world state on the environment over time . these theories are represented as conditional probability distributions . a restriction is imposed on the structure of the theories that allows the inference mechanism to find a unique predicted distribution for any action and world state description . these restricted theories are called uniquely predictive theories . the inference mechanism , probability combination using independence ( pci ) , uses minimal independence assumptions to combine the probabilities in a theory to make probabilistic predictions .", "topics": ["autonomous car"]}
{"title": "building a regular decision boundary with deep networks", "abstract": "in this work , we build a generic architecture of convolutional neural networks to discover empirical properties of neural networks . our first contribution is to introduce a state-of-the-art framework that depends upon few hyper parameters and to study the network when we vary them . it has no max pooling , no biases , only 13 layers , is purely convolutional and yields up to 95.4 % and 79.6 % accuracy respectively on cifar10 and cifar100 . we show that the nonlinearity of a deep network does not need to be continuous , non expansive or point-wise , to achieve good performance . we show that increasing the width of our network permits being competitive with very deep networks . our second contribution is an analysis of the contraction and separation properties of this network . indeed , a 1-nearest neighbor classifier applied on deep features progressively improves with depth , which indicates that the representation is progressively more regular . besides , we defined and analyzed local support vectors that separate classes locally . all our experiments are reproducible and code is available online , based on tensorflow .", "topics": ["nonlinear system", "neural networks"]}
{"title": "multiple retrieval models and regression models for prior art search", "abstract": "this paper presents the system called patatras ( patent and article tracking , retrieval and analysis ) realized for the ip track of clef 2009 . our approach presents three main characteristics : 1 . the usage of multiple retrieval models ( kl , okapi ) and term index definitions ( lemma , phrase , concept ) for the three languages considered in the present track ( english , french , german ) producing ten different sets of ranked results . 2 . the merging of the different results based on multiple regression models using an additional validation set created from the patent collection . 3 . the exploitation of patent metadata and of the citation structures for creating restricted initial working sets of patents and for producing a final re-ranking regression model . as we exploit specific metadata of the patent documents and the citation relations only at the creation of initial working sets and during the final post ranking step , our architecture remains generic and easy to extend .", "topics": ["test set"]}
{"title": "depth estimation through a generative model of light field synthesis", "abstract": "light field photography captures rich structural information that may facilitate a number of traditional image processing and computer vision tasks . a crucial ingredient in such endeavors is accurate depth recovery . we present a novel framework that allows the recovery of a high quality continuous depth map from light field data . to this end we propose a generative model of a light field that is fully parametrized by its corresponding depth map . the model allows for the integration of powerful regularization techniques such as a non-local means prior , facilitating accurate depth map estimation .", "topics": ["generative model"]}
{"title": "leveraging the power of gabor phase for face identification : a block matching approach", "abstract": "different from face verification , face identification is much more demanding . to reach comparable performance , an identifier needs to be roughly n times better than a verifier . to expect a breakthrough in face identification , we need a fresh look at the fundamental building blocks of face recognition . in this paper we focus on the selection of a suitable signal representation and better matching strategy for face identification . we demonstrate how gabor phase could be leveraged to improve the performance of face identification by using the block matching method . compared to the existing approaches , the proposed method features much lower algorithmic complexity : face images are only filtered by a single-scale gabor filter pair and the matching is performed between any pairs of face images at hand without involving any training process . benchmark evaluations show that the proposed approach is totally comparable to and even better than state-of-the-art algorithms , which are typically based on more features extracted from a large set of gabor faces and/or rely on heavy training processes .", "topics": ["computational complexity theory"]}
{"title": "deep convolutional features for image based retrieval and scene categorization", "abstract": "several recent approaches showed how the representations learned by convolutional neural networks can be repurposed for novel tasks . most commonly it has been shown that the activation features of the last fully connected layers ( fc7 or fc6 ) of the network , followed by a linear classifier outperform the state-of-the-art on several recognition challenge datasets . instead of recognition , this paper focuses on the image retrieval problem and proposes a examines alternative pooling strategies derived for cnn features . the presented scheme uses the features maps from an earlier layer 5 of the cnn architecture , which has been shown to preserve coarse spatial information and is semantically meaningful . we examine several pooling strategies and demonstrate superior performance on the image retrieval task ( inria holidays ) at the fraction of the computational cost , while using a relatively small memory requirements . in addition to retrieval , we see similar efficiency gains on the sun397 scene categorization dataset , demonstrating wide applicability of this simple strategy . we also introduce and evaluate a novel geoplaces5k dataset from different geographical locations in the world for image retrieval that stresses more dramatic changes in appearance and viewpoint .", "topics": ["neural networks", "map"]}
{"title": "beyond word embeddings : learning entity and concept representations from large scale knowledge bases", "abstract": "text representation using neural word embeddings has proven efficacy in many nlp applications . recently , a lot of research interest goes beyond word embeddings by adapting the traditional word embedding models to learn vectors of multiword expressions ( concepts/entities ) . however , current methods are limited to textual knowledge bases only ( e.g . , wikipedia ) . in this paper , we propose a novel approach for learning concept vectors from two large scale knowledge bases ( wikipedia , and probase ) . we adapt the skip-gram model to seamlessly learn from the knowledge in wikipedia text and probase concept graph . we evaluate our concept embedding models intrinsically on two tasks : 1 ) analogical reasoning where we achieve a state-of-the-art performance of 91 % on semantic analogies , 2 ) concept categorization where we achieve a state-of-the-art performance on two benchmark datasets achieving categorization accuracy of 100 % on one and 98 % on the other . additionally , we present a case study to extrinsically evaluate our model on unsupervised argument type identification for neural semantic parsing . we demonstrate the competitive accuracy of our unsupervised method and its ability to better generalize to out of vocabulary entity mentions compared to the tedious and error prone methods which depend on gazetteers and regular expressions .", "topics": ["natural language processing", "unsupervised learning"]}
{"title": "topic segmentation via community detection in complex networks", "abstract": "many real systems have been modelled in terms of network concepts , and written texts are a particular example of information networks . in recent years , the use of network methods to analyze language has allowed the discovery of several interesting findings , including the proposition of novel models to explain the emergence of fundamental universal patterns . while syntactical networks , one of the most prevalent networked models of written texts , display both scale-free and small-world properties , such representation fails in capturing other textual features , such as the organization in topics or subjects . in this context , we propose a novel network representation whose main purpose is to capture the semantical relationships of words in a simple way . to do so , we link all words co-occurring in the same semantic context , which is defined in a threefold way . we show that the proposed representations favours the emergence of communities of semantically related words , and this feature may be used to identify relevant topics . the proposed methodology to detect topics was applied to segment selected wikipedia articles . we have found that , in general , our methods outperform traditional bag-of-words representations , which suggests that a high-level textual representation may be useful to study semantical features of texts .", "topics": ["high- and low-level"]}
{"title": "nonnegative tensor factorization for directional blind audio source separation", "abstract": "we augment the nonnegative matrix factorization method for audio source separation with cues about directionality of sound propagation . this improves separation quality greatly and removes the need for training data , with only a twofold increase in run time . this is the first method which can exploit directional information from microphone arrays much smaller than the wavelength of sound , working both in simulation and in practice on millimeter-scale microphone arrays .", "topics": ["test set", "computation"]}
{"title": "exploiting the potential of unlabeled endoscopic video data with self-supervised learning", "abstract": "surgical data science is a new research field that aims to observe all aspects of the patient treatment process in order to provide the right assistance at the right time . due to the breakthrough successes of deep learning-based solutions for automatic image annotation , the availability of reference annotations for algorithm training is becoming a major bottleneck in the field . the purpose of this paper was to investigate the concept of self-supervised learning to address this issue . our approach is guided by the hypothesis that unlabeled video data can be used to learn a representation of the target domain that boosts the performance of state-of-the-art machine learning algorithms when used for pre-training . core of the method is an auxiliary task based on raw endoscopic video data of the target domain that is used to initialize the convolutional neural network ( cnn ) for the target task . in this paper , we propose the re-colorization of medical images with a generative adversarial network ( gan ) -based architecture as auxiliary task . a variant of the method involves a second pre-training step based on labeled data for the target task from a related domain . we validate both variants using medical instrument segmentation as target task . the proposed approach can be used to radically reduce the manual annotation effort involved in training cnns . compared to the baseline approach of generating annotated data from scratch , our method decreases exploratively the number of labeled images by up to 75 % without sacrificing performance . our method also outperforms alternative methods for cnn pre-training , such as pre-training on publicly available non-medical or medical data using the target task ( in this instance : segmentation ) . as it makes efficient use of available ( non- ) public and ( un- ) labeled data , the approach has the potential to become a valuable tool for cnn ( pre- ) training .", "topics": ["baseline ( configuration management )", "supervised learning"]}
{"title": "a consistent deterministic regression tree for non-parametric prediction of time series", "abstract": "we study online prediction of bounded stationary ergodic processes . to do so , we consider the setting of prediction of individual sequences and build a deterministic regression tree that performs asymptotically as well as the best l-lipschitz constant predictors . then , we show why the obtained regret bound entails the asymptotical optimality with respect to the class of bounded stationary ergodic processes .", "topics": ["regret ( decision theory )", "time series"]}
{"title": "an encoder-decoder model for icd-10 coding of death certificates", "abstract": "information extraction from textual documents such as hospital records and healthrelated user discussions has become a topic of intense interest . the task of medical concept coding is to map a variable length text to medical concepts and corresponding classification codes in some external system or ontology . in this work , we utilize recurrent neural networks to automatically assign icd-10 codes to fragments of death certificates written in english . we develop end-to-end neural architectures directly tailored to the task , including basic encoder-decoder architecture for statistical translation . in order to incorporate prior knowledge , we concatenate cosine similarities vector among the text and dictionary entry to the encoded state . being applied to a standard benchmark from clef ehealth 2017 challenge , our model achieved f-measure of 85.01 % on a full test set with significant improvement as compared to the average score of 62.2 % for all official participants approaches .", "topics": ["test set", "recurrent neural network"]}
{"title": "fine-grained analysis of sentence embeddings using auxiliary prediction tasks", "abstract": "there is a lot of research interest in encoding variable length sentences into fixed length vectors , in a way that preserves the sentence meanings . two common methods include representations based on averaging word vectors , and representations based on the hidden states of recurrent neural networks such as lstms . the sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning . however , not much is known about the properties that are encoded in these sentence representations and about the language information they capture . we propose a framework that facilitates better understanding of the encoded representations . we define prediction tasks around isolated aspects of sentence structure ( namely sentence length , word content , and word order ) , and score representations by the ability to train a classifier to solve each prediction task when using the representation as input . we demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms . the analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks , and on the effect of the encoded vector 's dimensionality on the resulting representations .", "topics": ["statistical classification", "recurrent neural network"]}
{"title": "a mixture of views network with applications to the classification of breast microcalcifications", "abstract": "in this paper we examine data fusion methods for multi-view data classification . we present a decision concept which explicitly takes into account the input multi-view structure , where for each case there is a different subset of relevant views . this data fusion concept , which we dub mixture of views , is implemented by a special purpose neural network architecture . it is demonstrated on the task of classifying breast microcalcifications as benign or malignant based on cc and mlo mammography views . the single view decisions are combined by a data-driven decision , according to the relevance of each view in a given case , into a global decision . the method is evaluated on a large multi-view dataset extracted from the standardized digital database for screening mammography ( ddsm ) . the experimental results show that our method outperforms previously suggested fusion methods .", "topics": ["relevance"]}
{"title": "generative models and model criticism via optimized maximum mean discrepancy", "abstract": "we propose a method to optimize the representation and distinguishability of samples from two probability distributions , by maximizing the estimated power of a statistical test based on the maximum mean discrepancy ( mmd ) . this optimized mmd is applied to the setting of unsupervised learning by generative adversarial networks ( gan ) , in which a model attempts to generate realistic samples , and a discriminator attempts to tell these apart from data samples . in this context , the mmd may be used in two roles : first , as a discriminator , either directly on the samples , or on features of the samples . second , the mmd can be used to evaluate the performance of a generative model , by testing the model 's samples against a reference data set . in the latter role , the optimized mmd is particularly helpful , as it gives an interpretable indication of how the model and data distributions differ , even in cases where individual model samples are not easily distinguished either by eye or by classifier .", "topics": ["generative model", "statistical classification"]}
{"title": "incremental clustering : the case for extra clusters", "abstract": "the explosion in the amount of data available for analysis often necessitates a transition from batch to incremental clustering methods , which process one element at a time and typically store only a small subset of the data . in this paper , we initiate the formal analysis of incremental clustering methods focusing on the types of cluster structure that they are able to detect . we find that the incremental setting is strictly weaker than the batch model , proving that a fundamental class of cluster structures that can readily be detected in the batch setting is impossible to identify using any incremental method . furthermore , we show how the limitations of incremental clustering can be overcome by allowing additional clusters .", "topics": ["cluster analysis"]}
{"title": "building chatbots from forum data : model selection using question answering metrics", "abstract": "we propose to use question answering ( qa ) data from web forums to train chatbots from scratch , i.e . , without dialog training data . first , we extract pairs of question and answer sentences from the typically much longer texts of questions and answers in a forum . we then use these shorter texts to train seq2seq models in a more efficient way . we further improve the parameter optimization using a new model selection strategy based on qa measures . finally , we propose to use extrinsic evaluation with respect to a qa task as an automatic evaluation method for chatbots . the evaluation shows that the model achieves a map of 63.5 % on the extrinsic task . moreover , it can answer correctly 49.5 % of the questions when they are similar to questions asked in the forum , and 47.3 % of the questions when they are more conversational in style .", "topics": ["test set", "map"]}
{"title": "attentionnet : aggregating weak directions for accurate object detection", "abstract": "we present a novel detection method using a deep convolutional neural network ( cnn ) , named attentionnet . we cast an object detection problem as an iterative classification problem , which is the most suitable form of a cnn . attentionnet provides quantized weak directions pointing a target object and the ensemble of iterative predictions from attentionnet converges to an accurate object boundary box . since attentionnet is a unified network for object detection , it detects objects without any separated models from the object proposal to the post bounding-box regression . we evaluate attentionnet by a human detection task and achieve the state-of-the-art performance of 65 % ( ap ) on pascal voc 2007/2012 with an 8-layered architecture only .", "topics": ["object detection", "statistical classification"]}
{"title": "a deep learning model for structured outputs with high-order interaction", "abstract": "many real-world applications are associated with structured data , where not only input but also output has interplay . however , typical classification and regression models often lack the ability of simultaneously exploring high-order interaction within input and that within output . in this paper , we present a deep learning model aiming to generate a powerful nonlinear functional mapping from structured input to structured output . more specifically , we propose to integrate high-order hidden units , guided discriminative pretraining , and high-order auto-encoders for this purpose . we evaluate the model with three datasets , and obtain state-of-the-art performances among competitive methods . our current work focuses on structured output regression , which is a less explored area , although the model can be extended to handle structured label classification .", "topics": ["nonlinear system", "autoencoder"]}
{"title": "search space analysis with wang-landau sampling and slow adaptive walks", "abstract": "two complementary techniques for analyzing search spaces are proposed : ( i ) an algorithm to detect search points with potential to be local optima ; and ( ii ) a slightly adjusted wang-landau sampling algorithm to explore larger search spaces . the detection algorithm assumes that local optima are points which are easier to reach and harder to leave by a slow adaptive walker . a slow adaptive walker moves to a nearest fitter point . thus , points with larger outgoing step sizes relative to incoming step sizes are marked using the local optima score formulae as potential local optima points ( plops ) . defining local optima in these more general terms allows their detection within the closure of a subset of a search space , and the sampling of a search space unshackled by a particular move set . tests are done with nk and hiff problems to confirm that plops detected in the manner proposed retain characteristics of local optima , and that the adjusted wang-landau samples are more representative of the search space than samples produced by choosing points uniformly at random . while our approach shows promise , more needs to be done to reduce its computation cost that it may pave a way toward analyzing larger search spaces of practical meaning .", "topics": ["sampling ( signal processing )"]}
{"title": "m3fusion : a deep learning architecture for multi- { scale/modal/temporal } satellite data fusion", "abstract": "modern earth observation systems provide sensing data at different temporal and spatial resolutions . among optical sensors , today the sentinel-2 program supplies high-resolution temporal ( every 5 days ) and high spatial resolution ( 10m ) images that can be useful to monitor land cover dynamics . on the other hand , very high spatial resolution images ( vhsr ) are still an essential tool to figure out land cover mapping characterized by fine spatial patterns . understand how to efficiently leverage these complementary sources of information together to deal with land cover mapping is still challenging . with the aim to tackle land cover mapping through the fusion of multi-temporal high spatial resolution and very high spatial resolution satellite images , we propose an end-to-end deep learning framework , named m3fusion , able to leverage simultaneously the temporal knowledge contained in time series data as well as the fine spatial information available in vhsr information . experiments carried out on the reunion island study area asses the quality of our proposal considering both quantitative and qualitative aspects .", "topics": ["time series", "sensor"]}
{"title": "material quality assessment of silk nanofibers based on swarm intelligence", "abstract": "in this paper , we propose a novel approach for texture analysis based on artificial crawler model . our method assumes that each agent can interact with the environment and each other . the evolution process converges to an equilibrium state according to the set of rules . for each textured image , the feature vector is composed by signatures of the live agents curve at each time . experimental results revealed that combining the minimum and maximum signatures into one increase the classification rate . in addition , we pioneer the use of autonomous agents for characterizing silk fibroin scaffolds . the results strongly suggest that our approach can be successfully employed for texture analysis .", "topics": ["feature vector", "numerical analysis"]}
{"title": "recovering block-structured activations using compressive measurements", "abstract": "we consider the problems of detection and localization of a contiguous block of weak activation in a large matrix , from a small number of noisy , possibly adaptive , compressive ( linear ) measurements . this is closely related to the problem of compressed sensing , where the task is to estimate a sparse vector using a small number of linear measurements . contrary to results in compressed sensing , where it has been shown that neither adaptivity nor contiguous structure help much , we show that for reliable localization the magnitude of the weakest signals is strongly influenced by both structure and the ability to choose measurements adaptively while for detection neither adaptivity nor structure reduce the requirement on the magnitude of the signal . we characterize the precise tradeoffs between the various problem parameters , the signal strength and the number of measurements required to reliably detect and localize the block of activation . the sufficient conditions are complemented with information theoretic lower bounds .", "topics": ["sparse matrix"]}
{"title": "a convolutional encoder model for neural machine translation", "abstract": "the prevalent approach to neural machine translation relies on bi-directional lstms to encode the source sentence . in this paper we present a faster and simpler architecture based on a succession of convolutional layers . this allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies . on wmt'16 english-romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the wmt'15 english-german task . our models obtain almost the same accuracy as a very deep lstm setup on wmt'14 english-french translation . our convolutional encoder speeds up cpu decoding by more than two times at the same or higher accuracy as a strong bi-directional lstm baseline .", "topics": ["machine translation", "computation"]}
{"title": "modular autoencoders for ensemble feature extraction", "abstract": "we introduce the concept of a modular autoencoder ( mae ) , capable of learning a set of diverse but complementary representations from unlabelled data , that can later be used for supervised tasks . the learning of the representations is controlled by a trade off parameter , and we show on six benchmark datasets the optimum lies between two extremes : a set of smaller , independent autoencoders each with low capacity , versus a single monolithic encoding , outperforming an appropriate baseline . in the present paper we explore the special case of linear mae , and derive an svd-based algorithm which converges several orders of magnitude faster than gradient descent .", "topics": ["baseline ( configuration management )", "mathematical optimization"]}
{"title": "feature extraction methods for color image similarity", "abstract": "many user interactive systems are proposed all methods are trying to implement as a user friendly and various approaches proposed but most of the systems not reached to the use specifications like user friendly systems with user interest , all proposed method implemented basic techniques some are improved methods also propose but not reaching to the user specifications . in this proposed paper we concentrated on image retrieval system with in early days many user interactive systems performed with basic concepts but such systems are not reaching to the user specifications and not attracted to the user so a lot of research interest in recent years with new specifications , recent approaches have user is interested in friendly interacted methods are expecting , many are concentrated for improvement in all methods . in this proposed system we focus on the retrieval of images within a large image collection based on color projections and different mathematical approaches are introduced and applied for retrieval of images . before appling proposed methods images are sub grouping using threshold values , in this paper r g b color combinations considered for retrieval of images , in proposed methods are implemented and results are included , through results it is observed that we obtaining efficient results comparatively previous and existing .", "topics": ["feature extraction"]}
{"title": "a genetic algorithm approach for solving a flexible job shop scheduling problem", "abstract": "flexible job shop scheduling has been noticed as an effective manufacturing system to cope with rapid development in today 's competitive environment . flexible job shop scheduling problem ( fjssp ) is known as a np-hard problem in the field of optimization . considering the dynamic state of the real world makes this problem more and more complicated . most studies in the field of fjssp have only focused on minimizing the total makespan . in this paper , a mathematical model for fjssp has been developed . the objective function is maximizing the total profit while meeting some constraints . time-varying raw material costs and selling prices and dissimilar demands for each period , have been considered to decrease gaps between reality and the model . a manufacturer that produces various parts of gas valves has been used as a case study . its scheduling problem for multi-part , multi-period , and multi-operation with parallel machines has been solved by using genetic algorithm ( ga ) . the best obtained answer determines the economic amount of production by different machines that belong to predefined operations for each part to satisfy customer demand in each period .", "topics": ["loss function"]}
{"title": "auxiliary image regularization for deep cnns with noisy labels", "abstract": "precisely-labeled data sets with sufficient amount of samples are very important for training deep convolutional neural networks ( cnns ) . however , many of the available real-world data sets contain erroneously labeled samples and those errors substantially hinder the learning of very accurate cnn models . in this work , we consider the problem of training a deep cnn model for image classification with mislabeled training samples - an issue that is common in real image data sets with tags supplied by amateur users . to solve this problem , we propose an auxiliary image regularization technique , optimized by the stochastic alternating direction method of multipliers ( admm ) algorithm , that automatically exploits the mutual context information among training images and encourages the model to select reliable images to robustify the learning process . comprehensive experiments on benchmark data sets clearly demonstrate our proposed regularized cnn model is resistant to label noise in training data .", "topics": ["test set", "matrix regularization"]}
{"title": "unite the people : closing the loop between 3d and 2d human representations", "abstract": "3d models provide a common ground for different representations of human bodies . in turn , robust 2d estimation has proven to be a powerful tool to obtain 3d fits `` in-the- wild '' . however , depending on the level of detail , it can be hard to impossible to acquire labeled data for training 2d estimators on large scale . we propose a hybrid approach to this problem : with an extended version of the recently introduced smplify method , we obtain high quality 3d body model fits for multiple human pose datasets . human annotators solely sort good and bad fits . this procedure leads to an initial dataset , up-3d , with rich annotations . with a comprehensive set of experiments , we show how this data can be used to train discriminative models that produce results with an unprecedented level of detail : our models predict 31 segments and 91 landmark locations on the body . using the 91 landmark pose estimator , we present state-of-the art results for 3d human pose and shape estimation using an order of magnitude less training data and without assumptions about gender or pose in the fitting procedure . we show that up-3d can be enhanced with these improved fits to grow in quantity and quality , which makes the system deployable on large scale . the data , code and models are available for research purposes .", "topics": ["test set"]}
{"title": "stochastic function norm regularization of deep networks", "abstract": "deep neural networks have had an enormous impact on image analysis . state-of-the-art training methods , based on weight decay and dropout , result in impressive performance when a very large training set is available . however , they tend to have large problems overfitting to small data sets . indeed , the available regularization methods deal with the complexity of the network function only indirectly . in this paper , we study the feasibility of directly using the $ l_2 $ function norm for regularization . two methods to integrate this new regularization in the stochastic backpropagation are proposed . moreover , the convergence of these new algorithms is studied . we finally show that they outperform the state-of-the-art methods in the low sample regime on benchmark datasets ( mnist and cifar10 ) . the obtained results demonstrate very clear improvement , especially in the context of small sample regimes with data laying in a low dimensional manifold . source code of the method can be found at \\url { https : //github.com/amalrt/dnn_reg } .", "topics": ["test set", "matrix regularization"]}
{"title": "on the effects of low-quality training data on information extraction from clinical reports", "abstract": "in the last five years there has been a flurry of work on information extraction from clinical documents , i.e . , on algorithms capable of extracting , from the informal and unstructured texts that are generated during everyday clinical practice , mentions of concepts relevant to such practice . most of this literature is about methods based on supervised learning , i.e . , methods for training an information extraction system from manually annotated examples . while a lot of work has been devoted to devising learning methods that generate more and more accurate information extractors , no work has been devoted to investigating the effect of the quality of training data on the learning process . low quality in training data often derives from the fact that the person who has annotated the data is different from the one against whose judgment the automatically annotated data must be evaluated . in this paper we test the impact of such data quality issues on the accuracy of information extraction systems as applied to the clinical domain . we do this by comparing the accuracy deriving from training data annotated by the authoritative coder ( i.e . , the one who has also annotated the test data , and by whose judgment we must abide ) , with the accuracy deriving from training data annotated by a different coder . the results indicate that , although the disagreement between the two coders ( as measured on the training set ) is substantial , the difference is ( surprisingly enough ) not always statistically significant .", "topics": ["test set", "supervised learning"]}
{"title": "fast detection of curved edges at low snr", "abstract": "detecting edges is a fundamental problem in computer vision with many applications , some involving very noisy images . while most edge detection methods are fast , they perform well only on relatively clean images . indeed , edges in such images can be reliably detected using only local filters . detecting faint edges under high levels of noise can not be done locally at the individual pixel level , and requires more sophisticated global processing . unfortunately , existing methods that achieve this goal are quite slow . in this paper we develop a novel multiscale method to detect curved edges in noisy images . while our algorithm searches for edges over a huge set of candidate curves , it does so in a practical runtime , nearly linear in the total number of image pixels . as we demonstrate experimentally , our algorithm is orders of magnitude faster than previous methods designed to deal with high noise levels . nevertheless , it obtains comparable , if not better , edge detection quality on a variety of challenging noisy images .", "topics": ["computer vision", "pixel"]}
{"title": "approximate stochastic subgradient estimation training for support vector machines", "abstract": "subgradient algorithms for training support vector machines have been quite successful for solving large-scale and online learning problems . however , they have been restricted to linear kernels and strongly convex formulations . this paper describes efficient subgradient approaches without such limitations . our approaches make use of randomized low-dimensional approximations to nonlinear kernels , and minimization of a reduced primal formulation using an algorithm based on robust stochastic approximation , which do not require strong convexity . experiments illustrate that our approaches produce solutions of comparable prediction accuracy with the solutions acquired from existing svm solvers , but often in much shorter time . we also suggest efficient prediction schemes that depend only on the dimension of kernel approximation , not on the number of support vectors .", "topics": ["support vector machine", "nonlinear system"]}
{"title": "emergence , competition and dynamical stabilization of dissipative rotating spiral waves in an excitable medium : a computational model based on cellular automata", "abstract": "we report some qualitatively new features of emergence , competition and dynamical stabilization of dissipative rotating spiral waves ( rsws ) in the cellular-automaton model of laser-like excitable media proposed in arxiv : cond-mat/0410460v2 ; arxiv : cond-mat/0602345 . part of the observed features are caused by unusual mechanism of excitation vorticity when the rsw 's core get into the surface layer of an active medium . instead of the well known scenario of rsw collapse , which takes place after collision of rsw 's core with absorbing boundary , we observed complicated transformations of the core leading to regeneration ( nonlinear `` reflection '' from the boundary ) of the rsw or even to birth of several new rsws in the surface layer . computer experiments on bottlenecked evolution of such the rsw-ensembles ( vortex matter ) are reported and a possible explanation of real experiments on spin-lattice relaxation in dilute paramagnets is proposed on the basis of an analysis of the rsws dynamics . chimera states in rsw-ensembles are revealed and compared with analogous states in ensembles of nonlocally coupled oscillators . generally , our computer experiments have shown that vortex matter states in laser-like excitable media have some important features of aggregate states of the usual matter .", "topics": ["nonlinear system", "simulation"]}
{"title": "deep learning with t-exponential bayesian kitchen sinks", "abstract": "bayesian learning has been recently considered as an effective means of accounting for uncertainty in trained deep network parameters . this is of crucial importance when dealing with small or sparse training datasets . on the other hand , shallow models that compute weighted sums of their inputs , after passing them through a bank of arbitrary randomized nonlinearities , have been recently shown to enjoy good test error bounds that depend on the number of nonlinearities . inspired from these advances , in this paper we examine novel deep network architectures , where each layer comprises a bank of arbitrary nonlinearities , linearly combined using multiple alternative sets of weights . we effect model training by means of approximate inference based on a t-divergence measure ; this generalizes the kullback-leibler divergence in the context of the t-exponential family of distributions . we adopt the t-exponential family since it can more flexibly accommodate real-world data , that entail outliers and distributions with fat tails , compared to conventional gaussian model assumptions . we extensively evaluate our approach using several challenging benchmarks , and provide comparative results to related state-of-the-art techniques .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "solving linear constraints in elementary abelian p-groups of symmetries", "abstract": "symmetries occur naturally in csp or sat problems and are not very difficult to discover , but using them to prune the search space tends to be very challenging . indeed , this usually requires finding specific elements in a group of symmetries that can be huge , and the problem of their very existence is np-hard . we formulate such an existence problem as a constraint problem on one variable ( the symmetry to be used ) ranging over a group , and try to find restrictions that may be solved in polynomial time . by considering a simple form of constraints ( restricted by a cardinality k ) and the class of groups that have the structure of fp-vector spaces , we propose a partial algorithm based on linear algebra . this polynomial algorithm always applies when k=p=2 , but may fail otherwise as we prove the problem to be np-hard for all other values of k and p. experiments show that this approach though restricted should allow for an efficient use of at least some groups of symmetries . we conclude with a few directions to be explored to efficiently solve this problem on the general case .", "topics": ["time complexity", "polynomial"]}
{"title": "multi-modality fusion based on consensus-voting and 3d convolution for isolated gesture recognition", "abstract": "recently , the popularity of depth-sensors such as kinect has made depth videos easily available while its advantages have not been fully exploited . this paper investigates , for gesture recognition , to explore the spatial and temporal information complementarily embedded in rgb and depth sequences . we propose a convolutional twostream consensus voting network ( 2scvn ) which explicitly models both the short-term and long-term structure of the rgb sequences . to alleviate distractions from background , a 3d depth-saliency convnet stream ( 3ddsn ) is aggregated in parallel to identify subtle motion characteristics . these two components in an unified framework significantly improve the recognition accuracy . on the challenging chalearn isogd benchmark , our proposed method outperforms the first place on the leader-board by a large margin ( 10.29 % ) while also achieving the best result on rgbd-hudaact dataset ( 96.74 % ) . both quantitative experiments and qualitative analysis shows the effectiveness of our proposed framework and codes will be released to facilitate future research .", "topics": ["convolution", "sensor"]}
{"title": "a quantitative assessment of the effect of different algorithmic schemes to the task of learning the structure of bayesian networks", "abstract": "one of the most challenging tasks when adopting bayesian networks ( bns ) is the one of learning their structure from data . this task is complicated by the huge search space of possible solutions and turned out to be a well-known np-hard problem and , hence , approximations are required . however , to the best of our knowledge , a quantitative analysis of the performance and characteristics of the different heuristics to solve this problem has never been done before . for this reason , in this work , we provide a detailed study of the different state-of-the-arts methods for structural learning on simulated data considering both bns with discrete and continuous variables , and with different rates of noise in the data . in particular , we investigate the characteristics of different widespread scores proposed for the inference and the statistical pitfalls within them .", "topics": ["simulation", "approximation"]}
{"title": "compression of deep neural networks on the fly", "abstract": "thanks to their state-of-the-art performance , deep neural networks are increasingly used for object recognition . to achieve these results , they use millions of parameters to be trained . however , when targeting embedded applications the size of these models becomes problematic . as a consequence , their usage on smartphones or other resource limited devices is prohibited . in this paper we introduce a novel compression method for deep neural networks that is performed during the learning phase . it consists in adding an extra regularization term to the cost function of fully-connected layers . we combine this method with product quantization ( pq ) of the trained weights for higher savings in storage consumption . we evaluate our method on two data sets ( mnist and cifar10 ) , on which we achieve significantly larger compression rates than state-of-the-art methods .", "topics": ["neural networks", "matrix regularization"]}
{"title": "invariants of multidimensional time series based on their iterated-integral signature", "abstract": "we introduce a novel class of features for multidimensional time series , that are invariant with respect to transformations of the ambient space . the general linear group , the group of rotations and the group of permutations of the axes are considered . the starting point for their construction is chen 's iterated-integral signature .", "topics": ["time series"]}
{"title": "gesture-based bootstrapping for egocentric hand segmentation", "abstract": "accurately identifying hands in images is a key sub-task for human activity understanding with wearable first-person point-of-view cameras . traditional hand segmentation approaches rely on a large corpus of manually labeled data to generate robust hand detectors . however , these approaches still face challenges as the appearance of the hand varies greatly across users , tasks , environments or illumination conditions . a key observation in the case of many wearable applications and interfaces is that , it is only necessary to accurately detect the user 's hands in a specific situational context . based on this observation , we introduce an interactive approach to learn a person-specific hand segmentation model that does not require any manually labeled training data . our approach proceeds in two steps , an interactive bootstrapping step for identifying moving hand regions , followed by learning a personalized user specific hand appearance model . concretely , our approach uses two convolutional neural networks : ( 1 ) a gesture network that uses pre-defined motion information to detect the hand region ; and ( 2 ) an appearance network that learns a person specific model of the hand region based on the output of the gesture network . during training , to make the appearance network robust to errors in the gesture network , the loss function of the former network incorporates the confidence of the gesture network while learning . experiments demonstrate the robustness of our approach with an f1 score over 0.8 on all challenging datasets across a wide range of illumination and hand appearance variations , improving over a baseline approach by over 10 % .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "de-identification of patient notes with recurrent neural networks", "abstract": "objective : patient notes in electronic health records ( ehrs ) may contain critical information for medical investigations . however , the vast majority of medical investigators can only access de-identified notes , in order to protect the confidentiality of patients . in the united states , the health insurance portability and accountability act ( hipaa ) defines 18 types of protected health information ( phi ) that needs to be removed to de-identify patient notes . manual de-identification is impractical given the size of ehr databases , the limited number of researchers with access to the non-de-identified notes , and the frequent mistakes of human annotators . a reliable automated de-identification system would consequently be of high value . materials and methods : we introduce the first de-identification system based on artificial neural networks ( anns ) , which requires no handcrafted features or rules , unlike existing systems . we compare the performance of the system with state-of-the-art systems on two datasets : the i2b2 2014 de-identification challenge dataset , which is the largest publicly available de-identification dataset , and the mimic de-identification dataset , which we assembled and is twice as large as the i2b2 2014 dataset . results : our ann model outperforms the state-of-the-art systems . it yields an f1-score of 97.85 on the i2b2 2014 dataset , with a recall 97.38 and a precision of 97.32 , and an f1-score of 99.23 on the mimic de-identification dataset , with a recall 99.25 and a precision of 99.06 . conclusion : our findings support the use of anns for de-identification of patient notes , as they show better performance than previously published systems while requiring no feature engineering .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "riffled independence for efficient inference with partial rankings", "abstract": "distributions over rankings are used to model data in a multitude of real world settings such as preference analysis and political elections . modeling such distributions presents several computational challenges , however , due to the factorial size of the set of rankings over an item set . some of these challenges are quite familiar to the artificial intelligence community , such as how to compactly represent a distribution over a combinatorially large space , and how to efficiently perform probabilistic inference with these representations . with respect to ranking , however , there is the additional challenge of what we refer to as human task complexity users are rarely willing to provide a full ranking over a long list of candidates , instead often preferring to provide partial ranking information . simultaneously addressing all of these challenges i.e . , designing a compactly representable model which is amenable to efficient inference and can be learned using partial ranking data is a difficult task , but is necessary if we would like to scale to problems with nontrivial size . in this paper , we show that the recently proposed riffled independence assumptions cleanly and efficiently address each of the above challenges . in particular , we establish a tight mathematical connection between the concepts of riffled independence and of partial rankings . this correspondence not only allows us to then develop efficient and exact algorithms for performing inference tasks using riffled independence based represen- tations with partial rankings , but somewhat surprisingly , also shows that efficient inference is not possible for riffle independent models ( in a certain sense ) with observations which do not take the form of partial rankings . finally , using our inference algorithm , we introduce the first method for learning riffled independence based models from partially ranked data .", "topics": ["computational complexity theory", "artificial intelligence"]}
{"title": "accelerating partial-order planners : some techniques for effective search control and pruning", "abstract": "we propose some domain-independent techniques for bringing well-founded partial-order planners closer to practicality . the first two techniques are aimed at improving search control while keeping overhead costs low . one is based on a simple adjustment to the default a* heuristic used by ucpop to select plans for refinement . the other is based on preferring `` zero commitment '' ( forced ) plan refinements whenever possible , and using lifo prioritization otherwise . a more radical technique is the use of operator parameter domains to prune search . these domains are initially computed from the definitions of the operators and the initial and goal conditions , using a polynomial-time algorithm that propagates sets of constants through the operator graph , starting in the initial conditions . during planning , parameter domains can be used to prune nonviable operator instances and to remove spurious clobbering threats . in experiments based on modifications of ucpop , our improved plan and goal selection strategies gave speedups by factors ranging from 5 to more than 1000 for a variety of problems that are nontrivial for the unmodified version . crucially , the hardest problems gave the greatest improvements . the pruning technique based on parameter domains often gave speedups by an order of magnitude or more for difficult problems , both with the default ucpop search strategy and with our improved strategy . the lisp code for our techniques and for the test problems is provided in on-line appendices .", "topics": ["time complexity", "heuristic"]}
{"title": "learning classifier systems with memory condition to solve non-markov problems", "abstract": "in the family of learning classifier systems , the classifier system xcs has been successfully used for many applications . however , the standard xcs has no memory mechanism and can only learn optimal policy in markov environments , where the optimal action is determined solely by the state of current sensory input . in practice , most environments are partially observable environments on agent 's sensation , which are also known as non-markov environments . within these environments , xcs either fails , or only develops a suboptimal policy , since it has no memory . in this work , we develop a new classifier system based on xcs to tackle this problem . it adds an internal message list to xcs as the memory list to record input sensation history , and extends a small number of classifiers with memory conditions . the classifier 's memory condition , as a foothold to disambiguate non-markov states , is used to sense a specified element in the memory list . besides , a detection method is employed to recognize non-markov states in environments , to avoid these states controlling over classifiers ' memory conditions . furthermore , four sets of different complex maze environments have been tested by the proposed method . experimental results show that our system is one of the best techniques to solve partially observable environments , compared with some well-known classifier systems proposed for these environments .", "topics": ["markov chain"]}
{"title": "a closer look at spatiotemporal convolutions for action recognition", "abstract": "in this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition . our motivation stems from the observation that 2d cnns applied to individual frames of the video have remained solid performers in action recognition . in this work we empirically demonstrate the accuracy advantages of 3d cnns over 2d cnns within the framework of residual learning . furthermore , we show that factorizing the 3d convolutional filters into separate spatial and temporal components yields significantly advantages in accuracy . our empirical study leads to the design of a new spatiotemporal convolutional block `` r ( 2+1 ) d '' which gives rise to cnns that achieve results comparable or superior to the state-of-the-art on sports-1m , kinetics , ucf101 and hmdb51 .", "topics": ["convolution"]}
{"title": "optimized imaging using non-rigid registration", "abstract": "the extraordinary improvements of modern imaging devices offer access to data with unprecedented information content . however , widely used image processing methodologies fall far short of exploiting the full breadth of information offered by numerous types of scanning probe , optical , and electron microscopies . in many applications , it is necessary to keep measurement intensities below a desired threshold . we propose a methodology for extracting an increased level of information by processing a series of data sets suffering , in particular , from high degree of spatial uncertainty caused by complex multiscale motion during the acquisition process . an important role is played by a nonrigid pixel-wise registration method that can cope with low signal-to-noise ratios . this is accompanied by formulating objective quality measures which replace human intervention and visual inspection in the processing chain . scanning transmission electron microscopy of siliceous zeolite material exhibits the above-mentioned obstructions and therefore serves as orientation and a test of our procedures .", "topics": ["image processing", "causality"]}
{"title": "vegac : visual saliency-based age , gender , and facial expression classification using convolutional neural networks", "abstract": "this paper explores the use of visual saliency to classify age , gender and facial expression for facial images . for multi-task classification , we propose our method vegac , which is based on visual saliency . using the deep multi-level network [ 1 ] and off-the-shelf face detector [ 2 ] , our proposed method first detects the face in the test image and extracts the cnn predictions on the cropped face . the cnn of vegac were fine-tuned on the collected dataset from different benchmarks . our convolutional neural network ( cnn ) uses the vgg-16 architecture [ 3 ] and is pre-trained on imagenet for image classification . we demonstrate the usefulness of our method for age estimation , gender classification , and facial expression classification . we show that we obtain the competitive result with our method on selected benchmarks . all our models and code will be publically available .", "topics": ["statistical classification", "computer vision"]}
{"title": "enhancement techniques for local content preservation and contrast improvement in images", "abstract": "there are several images that do not have uniform brightness which pose a challenging problem for image enhancement systems . as histogram equalization has been successfully used to correct for uniform brightness problems , a histogram equalization method that utilizes human visual system based thresholding ( human vision thresholding ) as well as logarithmic processing techniques were introduced later . but these methods are not good for preserving the local content of the image which is a major factor for various images like medical and aerial images . therefore new method is proposed here . this method is referred as `` human vision thresholding with enhancement technique for dark blurred images for local content preservation '' . it uses human vision thresholding together with an existing enhancement method for dark blurred images . furthermore a comparative study with another method for local content preservation is done which is further extended to make it suitable for contrast improvement . experimental results shows that the proposed methods outperforms the former existing methods in preserving the local content for standard images , medical and aerial images .", "topics": ["image processing"]}
{"title": "viewpoint invariant object detector", "abstract": "object detection is the task of identifying the existence of an object class instance and locating it within an image . difficulties in handling high intra-class variations constitute major obstacles to achieving high performance on standard benchmark datasets ( scale , viewpoint , lighting conditions and orientation variations provide good examples ) . suggested model aims at providing more robustness to detecting objects suffering severe distortion due to < 60 { \\deg } viewpoint changes . in addition , several model computational bottlenecks have been resolved leading to a significant increase in the model performance ( speed and space ) without compromising the resulting accuracy . finally , we produced two illustrative applications showing the potential of the object detection technology being deployed in real life applications ; namely content-based image search and content-based video search .", "topics": ["object detection"]}
{"title": "where to focus : query adaptive matching for instance retrieval using convolutional feature maps", "abstract": "instance retrieval requires one to search for images that contain a particular object within a large corpus . recent studies show that using image features generated by pooling convolutional layer feature maps ( cfms ) of a pretrained convolutional neural network ( cnn ) leads to promising performance for this task . however , due to the global pooling strategy adopted in those works , the generated image feature is less robust to image clutter and tends to be contaminated by the irrelevant image patterns . in this article , we alleviate this drawback by proposing a novel reranking algorithm using cfms to refine the retrieval result obtained by existing methods . our key idea , called query adaptive matching ( qam ) , is to first represent the cfms of each image by a set of base regions which can be freely combined into larger regions-of-interest . then the similarity between the query and a candidate image is measured by the best similarity score that can be attained by comparing the query feature and the feature pooled from a combined region . we show that the above procedure can be cast as an optimization problem and it can be solved efficiently with an off-the-shelf solver . besides this general framework , we also propose two practical ways to create the base regions . one is based on the property of the cfm and the other one is based on a multi-scale spatial pyramid scheme . through extensive experiments , we show that our reranking approaches bring substantial performance improvement and by applying them we can outperform the state of the art on several instance retrieval benchmarks .", "topics": ["optimization problem", "map"]}
{"title": "probabilistic inferences in bayesian networks", "abstract": "bayesian network is a complete model for the variables and their relationships , it can be used to answer probabilistic queries about them . a bayesian network can thus be considered a mechanism for automatically applying bayes ' theorem to complex problems . in the application of bayesian networks , most of the work is related to probabilistic inferences . any variable updating in any node of bayesian networks might result in the evidence propagation across the bayesian networks . this paper sums up various inference techniques in bayesian networks and provide guidance for the algorithm calculation in probabilistic inference in bayesian networks .", "topics": ["bayesian network"]}
{"title": "unifying decision trees split criteria using tsallis entropy", "abstract": "the construction of efficient and effective decision trees remains a key topic in machine learning because of their simplicity and flexibility . a lot of heuristic algorithms have been proposed to construct near-optimal decision trees . id3 , c4.5 and cart are classical decision tree algorithms and the split criteria they used are shannon entropy , gain ratio and gini index respectively . all the split criteria seem to be independent , actually , they can be unified in a tsallis entropy framework . tsallis entropy is a generalization of shannon entropy and provides a new approach to enhance decision trees ' performance with an adjustable parameter $ q $ . in this paper , a tsallis entropy criterion ( tec ) algorithm is proposed to unify shannon entropy , gain ratio and gini index , which generalizes the split criteria of decision trees . more importantly , we reveal the relations between tsallis entropy with different $ q $ and other split criteria . experimental results on uci data sets indicate that the tec algorithm achieves statistically significant improvement over the classical algorithms .", "topics": ["heuristic"]}
{"title": "discriminative k-means clustering", "abstract": "the k-means algorithm is a partitional clustering method . over 60 years old , it has been successfully used for a variety of problems . the popularity of k-means is in large part a consequence of its simplicity and efficiency . in this paper we are inspired by these appealing properties of k-means in the development of a clustering algorithm which accepts the notion of `` positively '' and `` negatively '' labelled data . the goal is to discover the cluster structure of both positive and negative data in a manner which allows for the discrimination between the two sets . the usefulness of this idea is demonstrated practically on the problem of face recognition , where the task of learning the scope of a person 's appearance should be done in a manner which allows this face to be differentiated from others .", "topics": ["cluster analysis"]}
{"title": "a hybrid deep learning architecture for privacy-preserving mobile analytics", "abstract": "the increasing quality of smartphone cameras and variety of photo editing applications , in addition to the rise in popularity of image-centric social media , have all led to a phenomenal growth in mobile-based photography . advances in computer vision and machine learning techniques provide a large number of cloud-based services with the ability to provide content analysis , face recognition , and object detection facilities to third parties . these inferences and analytics might come with undesired privacy risks to the individuals . in this paper , we address a fundamental challenge : can we utilize the local processing capabilities of modern smartphones efficiently to provide desired features to approved analytics services , while protecting against undesired inference attacks and preserving privacy on the cloud ? we propose a hybrid architecture for a distributed deep learning model between the smartphone and the cloud . we rely on the siamese network and machine learning approaches for providing privacy based on defined privacy constraints . we also use transfer learning techniques to evaluate the proposed method . using the latest deep learning models for face recognition , emotion detection , and gender classification techniques , we demonstrate the effectiveness of our technique in providing highly accurate classification results for the desired analytics , while proving strong privacy guarantees .", "topics": ["object detection", "computer vision"]}
{"title": "personalizing a dialogue system with transfer reinforcement learning", "abstract": "it is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient . personalized dialogue systems trained on a small dataset can overfit and make it difficult to adapt to different user needs . one way to solve this problem is to consider a collection of multiple users ' data as a source domain and an individual user 's data as a target domain , and to perform a transfer learning from the source to the target domain . by following this idea , we propose `` petal '' ( personalized task-oriented dialogue ) , a transfer-learning framework based on pomdp to learn a personalized dialogue system . the system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user . this framework can avoid the negative transfer problem by considering differences between source and target users . the policy in the personalized pomdp can learn to choose different actions appropriately for different users . experimental results on a real-world coffee-shopping data and simulation data show that our personalized dialogue system can choose different optimal actions for different users , and thus effectively improve the dialogue quality under the personalized setting .", "topics": ["reinforcement learning", "simulation"]}
{"title": "detecting context dependence in exercise item candidates selected from corpora", "abstract": "we explore the factors influencing the dependence of single sentences on their larger textual context in order to automatically identify candidate sentences for language learning exercises from corpora which are presentable in isolation . an in-depth investigation of this question has not been previously carried out . understanding this aspect can contribute to a more efficient selection of candidate sentences which , besides reducing the time required for item writing , can also ensure a higher degree of variability and authenticity . we present a set of relevant aspects collected based on the qualitative analysis of a smaller set of context-dependent corpus example sentences . furthermore , we implemented a rule-based algorithm using these criteria which achieved an average precision of 0.76 for the identification of different issues related to context dependence . the method has also been evaluated empirically where 80 % of the sentences in which our system did not detect context-dependent elements were also considered context-independent by human raters .", "topics": ["text corpus"]}
{"title": "statistical models for unsupervised prepositional phrase attachment", "abstract": "we present several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task . our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms , as opposed to attachment information . it is therefore less resource-intensive and more portable than previous corpus-based algorithms proposed for this task . we present results for prepositional phrase attachment in both english and spanish .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "impact of biases in big data", "abstract": "the underlying paradigm of big data-driven machine learning reflects the desire of deriving better conclusions from simply analyzing more data , without the necessity of looking at theory and models . is having simply more data always helpful ? in 1936 , the literary digest collected 2.3m filled in questionnaires to predict the outcome of that year 's us presidential election . the outcome of this big data prediction proved to be entirely wrong , whereas george gallup only needed 3k handpicked people to make an accurate prediction . generally , biases occur in machine learning whenever the distributions of training set and test set are different . in this work , we provide a review of different sorts of biases in ( big ) data sets in machine learning . we provide definitions and discussions of the most commonly appearing biases in machine learning : class imbalance and covariate shift . we also show how these biases can be quantified and corrected . this work is an introductory text for both researchers and practitioners to become more aware of this topic and thus to derive more reliable models for their learning problems .", "topics": ["test set"]}
{"title": "efficient learning of sparse invariant representations", "abstract": "we propose a simple and efficient algorithm for learning sparse invariant representations from unlabeled data with fast inference . when trained on short movies sequences , the learned features are selective to a range of orientations and spatial frequencies , but robust to a wide range of positions , similar to complex cells in the primary visual cortex . we give a hierarchical version of the algorithm , and give guarantees of fast convergence under certain conditions .", "topics": ["sparse matrix"]}
{"title": "global constraint catalog , volume ii , time-series constraints", "abstract": "first this report presents a restricted set of finite transducers used to synthesise structural time-series constraints described by means of a multi-layered function composition scheme . second it provides the corresponding synthesised catalogue of structural time-series constraints where each constraint is explicitly described in terms of automata with accumulators .", "topics": ["time series"]}
{"title": "topic discovery through data dependent and random projections", "abstract": "we present algorithms for topic modeling based on the geometry of cross-document word-frequency patterns . this perspective gains significance under the so called separability condition . this is a condition on existence of novel-words that are unique to each topic . we present a suite of highly efficient algorithms based on data-dependent and random projections of word-frequency patterns to identify novel words and associated topics . we will also discuss the statistical guarantees of the data-dependent projections method based on two mild assumptions on the prior density of topic document matrix . our key insight here is that the maximum and minimum values of cross-document frequency patterns projected along any direction are associated with novel words . while our sample complexity bounds for topic recovery are similar to the state-of-art , the computational complexity of our random projection scheme scales linearly with the number of documents and the number of words per document . we present several experiments on synthetic and real-world datasets to demonstrate qualitative and quantitative merits of our scheme .", "topics": ["computational complexity theory", "synthetic data"]}
{"title": "towards accurate binary convolutional neural network", "abstract": "we introduce a novel scheme to train binary convolutional neural networks ( cnns ) -- cnns with weights and activations constrained to { -1 , +1 } at run-time . it has been known that using binary weights and activations drastically reduce memory size and accesses , and can replace arithmetic operations with more efficient bitwise operations , leading to much faster test-time inference and lower power consumption . however , previous works on binarizing cnns usually result in severe prediction accuracy degradation . in this paper , we address this issue with two major innovations : ( 1 ) approximating full-precision weights with the linear combination of multiple binary weight bases ; ( 2 ) employing multiple binary activations to alleviate information loss . the implementation of the resulting binary cnn , denoted as abc-net , is shown to achieve much closer performance to its full-precision counterpart , and even reach the comparable prediction accuracy on imagenet and forest trail datasets , given adequate binary weight bases and activations .", "topics": ["approximation algorithm"]}
{"title": "mining a sub-matrix of maximal sum", "abstract": "biclustering techniques have been widely used to identify homogeneous subgroups within large data matrices , such as subsets of genes similarly expressed across subsets of patients . mining a max-sum sub-matrix is a related but distinct problem for which one looks for a ( non-necessarily contiguous ) rectangular sub-matrix with a maximal sum of its entries . le van et al . ( ranked tiling , 2014 ) already illustrated its applicability to gene expression analysis and addressed it with a constraint programming ( cp ) approach combined with large neighborhood search ( cp-lns ) . in this work , we exhibit some key properties of this np-hard problem and define a bounding function such that larger problems can be solved in reasonable time . two different algorithms are proposed in order to exploit the highlighted characteristics of the problem : a cp approach with a global constraint ( cpgc ) and mixed integer linear programming ( milp ) . practical experiments conducted both on synthetic and real gene expression data exhibit the characteristics of these approaches and their relative benefits over the original cp-lns method . overall , the cpgc approach tends to be the fastest to produce a good solution . yet , the milp formulation is arguably the easiest to formulate and can also be competitive .", "topics": ["synthetic data"]}
{"title": "knowledge transfer for surgical activity prediction", "abstract": "lack of training data hinders automatic recognition and prediction of surgical activities necessary for situation-aware operating rooms . we propose using knowledge transfer to compensate for data deficit and improve prediction . we used two approaches to extract and transfer surgical process knowledge . first , we encoded semantic information about surgical terms using word embedding which boosted learning process . secondly , we passed knowledge between different clinical datasets of neurosurgical procedures using transfer learning . transfer learning was shown to be more effective than a simple combination of data , especially for less similar procedures . the combination of two methods provided 22 % improvement of activity prediction . we also made several pertinent observations about surgical practices .", "topics": ["test set"]}
{"title": "assessing the performance of deep learning algorithms for newsvendor problem", "abstract": "in retailer management , the newsvendor problem has widely attracted attention as one of basic inventory models . in the traditional approach to solving this problem , it relies on the probability distribution of the demand . in theory , if the probability distribution is known , the problem can be considered as fully solved . however , in any real world scenario , it is almost impossible to even approximate or estimate a better probability distribution for the demand . in recent years , researchers start adopting machine learning approach to learn a demand prediction model by using other feature information . in this paper , we propose a supervised learning that optimizes the demand quantities for products based on feature information . we demonstrate that the original newsvendor loss function as the training objective outperforms the recently suggested quadratic loss function . the new algorithm has been assessed on both the synthetic data and real-world data , demonstrating better performance .", "topics": ["approximation algorithm", "supervised learning"]}
{"title": "neural networks for complex data", "abstract": "artificial neural networks are simple and efficient machine learning tools . defined originally in the traditional setting of simple vector data , neural network models have evolved to address more and more difficulties of complex real world problems , ranging from time evolving data to sophisticated data structures such as graphs and functions . this paper summarizes advances on those themes from the last decade , with a focus on results obtained by members of the samm team of universit\\'e paris 1", "topics": ["neural networks"]}
{"title": "towards an improved performance measure for language models", "abstract": "in this paper a first attempt at deriving an improved performance measure for language models , the probability ratio measure ( prm ) is described . in a proof of concept experiment , it is shown that prm correlates better with recognition accuracy and can lead to better recognition results when used as the optimisation criterion of a clustering algorithm . inspite of the approximations and limitations of this preliminary work , the results are very encouraging and should justify more work along the same lines .", "topics": ["cluster analysis", "mathematical optimization"]}
{"title": "adaptive regularization of some inverse problems in image analysis", "abstract": "we present an adaptive regularization scheme for optimizing composite energy functionals arising in image analysis problems . the scheme automatically trades off data fidelity and regularization depending on the current data fit during the iterative optimization , so that regularization is strongest initially , and wanes as data fidelity improves , with the weight of the regularizer being minimized at convergence . we also introduce the use of a huber loss function in both data fidelity and regularization terms , and present an efficient convex optimization algorithm based on the alternating direction method of multipliers ( admm ) using the equivalent relation between the huber function and the proximal operator of the one-norm . we illustrate and validate our adaptive huber-huber model on synthetic and real images in segmentation , motion estimation , and denoising problems .", "topics": ["noise reduction", "matrix regularization"]}
{"title": "a multi-objective deep reinforcement learning framework", "abstract": "this paper presents a new multi-objective deep reinforcement learning ( modrl ) framework based on deep q-networks . we propose linear and non-linear methods to develop the modrl framework that includes both single-policy and multi-policy strategies . the experimental results on a deep sea treasure environment indicate that the proposed approach is able to converge to the optimal pareto solutions . the proposed framework is generic , which allows implementation of different deep reinforcement learning algorithms in various complex environments . details of the framework implementation can be referred to http : //www.deakin.edu.au/~thanhthi/drl.htm .", "topics": ["reinforcement learning", "nonlinear system"]}
{"title": "unsupervised segmentation of multispectral images with cellular automata", "abstract": "multispectral images acquired by satellites are used to study phenomena on the earth 's surface . unsupervised classification techniques analyze multispectral image content without considering prior knowledge of the observed terrain ; this is done using techniques which group pixels that have similar statistics of digital level distribution in the various image channels . in this paper , we propose a methodology for unsupervised classification based on a deterministic cellular automaton . the automaton is initialized in an unsupervised manner by setting seed cells , selected according to two criteria : to be representative of the spatial distribution of the dominant elements in the image , and to take into account the diversity of spectral signatures in the image . the automaton 's evolution is based on an attack rule that is applied simultaneously to all its cells . among the noteworthy advantages of deterministic cellular automata for multispectral processing of satellite imagery is the consideration of topological information in the image via seed positioning , and the ability to modify the scale of the study .", "topics": ["unsupervised learning", "pixel"]}
{"title": "hardware based spatio-temporal neural processing backend for imaging sensors : towards a smart camera", "abstract": "in this work we show how we can build a technology platform for cognitive imaging sensors using recent advances in recurrent neural network architectures and training methods inspired from biology . we demonstrate learning and processing tasks specific to imaging sensors , including enhancement of sensitivity and signal-to-noise ratio ( snr ) purely through neural filtering beyond the fundamental limits sensor materials , and inferencing and spatio-temporal pattern recognition capabilities of these networks with applications in object detection , motion tracking and prediction . we then show designs of unit hardware cells built using complementary metal-oxide semiconductor ( cmos ) and emerging materials technologies for ultra-compact and energy-efficient embedded neural processors for smart cameras .", "topics": ["recurrent neural network", "object detection"]}
{"title": "extraction of semantic relations from a basque monolingual dictionary using constraint grammar", "abstract": "this paper deals with the exploitation of dictionaries for the semi-automatic construction of lexicons and lexical knowledge bases . the final goal of our research is to enrich the basque lexical database with semantic information such as senses , definitions , semantic relations , etc . , extracted from a basque monolingual dictionary . the work here presented focuses on the extraction of the semantic relations that best characterise the headword , that is , those of synonymy , antonymy , hypernymy , and other relations marked by specific relators and derivation . all nominal , verbal and adjectival entries were treated . basque uses morphological inflection to mark case , and therefore semantic relations have to be inferred from suffixes rather than from prepositions . our approach combines a morphological analyser and surface syntax parsing ( based on constraint grammar ) , and has proven very successful for highly inflected languages such as basque . both the effort to write the rules and the actual processing time of the dictionary have been very low . at present we have extracted 42,533 relations , leaving only 2,943 ( 9 % ) definitions without any extracted relation . the error rate is extremely low , as only 2.2 % of the extracted relations are wrong .", "topics": ["parsing", "dictionary"]}
{"title": "cross-language framework for word recognition and spotting of indic scripts", "abstract": "handwritten word recognition and spotting of low-resource scripts are difficult as sufficient training data is not available and it is often expensive for collecting data of such scripts . this paper presents a novel cross language platform for handwritten word recognition and spotting for such low-resource scripts where training is performed with a sufficiently large dataset of an available script ( considered as source script ) and testing is done on other scripts ( considered as target script ) . training with one source script and testing with another script to have a reasonable result is not easy in handwriting domain due to the complex nature of handwriting variability among scripts . also it is difficult in mapping between source and target characters when they appear in cursive word images . the proposed indic cross language framework exploits a large resource of dataset for training and uses it for recognizing and spotting text of other target scripts where sufficient amount of training data is not available . since , indic scripts are mostly written in 3 zones , namely , upper , middle and lower , we employ zone-wise character ( or component ) mapping for efficient learning purpose . the performance of our cross-language framework depends on the extent of similarity between the source and target scripts . hence , we devise an entropy based script similarity score using source to target character mapping that will provide a feasibility of cross language transcription . we have tested our approach in three indic scripts , namely , bangla , devanagari and gurumukhi , and the corresponding results are reported .", "topics": ["test set"]}
{"title": "leveraging sentence-level information with encoder lstm for semantic slot filling", "abstract": "recurrent neural network ( rnn ) and one of its specific architectures , long short-term memory ( lstm ) , have been widely used for sequence labeling . in this paper , we first enhance lstm-based sequence labeling to explicitly model label dependencies . then we propose another enhancement to incorporate the global information spanning over the whole input sequence . the latter proposed method , encoder-labeler lstm , first encodes the whole input sequence into a fixed length vector with the encoder lstm , and then uses this encoded vector as the initial state of another lstm for sequence labeling . combining these methods , we can predict the label sequence with considering label dependencies and information of whole input sequence . in the experiments of a slot filling task , which is an essential component of natural language understanding , with using the standard atis corpus , we achieved the state-of-the-art f1-score of 95.66 % .", "topics": ["recurrent neural network", "natural language"]}
{"title": "recurrent poisson factorization for temporal recommendation", "abstract": "poisson factorization is a probabilistic model of users and items for recommendation systems , where the so-called implicit consumer data is modeled by a factorized poisson distribution . there are many variants of poisson factorization methods who show state-of-the-art performance on real-world recommendation tasks . however , most of them do not explicitly take into account the temporal behavior and the recurrent activities of users which is essential to recommend the right item to the right user at the right time . in this paper , we introduce recurrent poisson factorization ( rpf ) framework that generalizes the classical pf methods by utilizing a poisson process for modeling the implicit feedback . rpf treats time as a natural constituent of the model and brings to the table a rich family of time-sensitive factorization models . to elaborate , we instantiate several variants of rpf who are capable of handling dynamic user preferences and item specification ( drpf ) , modeling the social-aspect of product adoption ( srpf ) , and capturing the consumption heterogeneity among users and items ( hrpf ) . we also develop a variational algorithm for approximate posterior inference that scales up to massive data sets . furthermore , we demonstrate rpf 's superior performance over many state-of-the-art methods on synthetic dataset , and large scale real-world datasets on music streaming logs , and user-item interactions in m-commerce platforms .", "topics": ["calculus of variations", "approximation algorithm"]}
{"title": "tiny descriptors for image retrieval with unsupervised triplet hashing", "abstract": "a typical image retrieval pipeline starts with the comparison of global descriptors from a large database to find a short list of candidate matches . a good image descriptor is key to the retrieval pipeline and should reconcile two contradictory requirements : providing recall rates as high as possible and being as compact as possible for fast matching . following the recent successes of deep convolutional neural networks ( dcnn ) for large scale image classification , descriptors extracted from dcnns are increasingly used in place of the traditional hand crafted descriptors such as fisher vectors ( fv ) with better retrieval performances . nevertheless , the dimensionality of a typical dcnn descriptor -- extracted either from the visual feature pyramid or the fully-connected layers -- remains quite high at several thousands of scalar values . in this paper , we propose unsupervised triplet hashing ( uth ) , a fully unsupervised method to compute extremely compact binary hashes -- in the 32-256 bits range -- from high-dimensional global descriptors . uth consists of two successive deep learning steps . first , stacked restricted boltzmann machines ( srbm ) , a type of unsupervised deep neural nets , are used to learn binary embedding functions able to bring the descriptor size down to the desired bitrate . srbms are typically able to ensure a very high compression rate at the expense of loosing some desirable metric properties of the original dcnn descriptor space . then , triplet networks , a rank learning scheme based on weight sharing nets is used to fine-tune the binary embedding functions to retain as much as possible of the useful metric properties of the original space . a thorough empirical evaluation conducted on multiple publicly available dataset using dcnn descriptors shows that our method is able to significantly outperform state-of-the-art unsupervised schemes in the target bit range .", "topics": ["unsupervised learning", "computer vision"]}
{"title": "oracle complexity of second-order methods for finite-sum problems", "abstract": "finite-sum optimization problems are ubiquitous in machine learning , and are commonly solved using first-order methods which rely on gradient computations . recently , there has been growing interest in \\emph { second-order } methods , which rely on both gradients and hessians . in principle , second-order methods can require much fewer iterations than first-order methods , and hold the promise for more efficient algorithms . although computing and manipulating hessians is prohibitive for high-dimensional problems in general , the hessians of individual functions in finite-sum problems can often be efficiently computed , e.g . because they possess a low-rank structure . can second-order information indeed be used to solve such problems more efficiently ? in this paper , we provide evidence that the answer -- perhaps surprisingly -- is negative , at least in terms of worst-case guarantees . however , we also discuss what additional assumptions and algorithmic approaches might potentially circumvent this negative result .", "topics": ["computation", "iteration"]}
{"title": "robust localized multi-view subspace clustering", "abstract": "in multi-view clustering , different views may have different confidence levels when learning a consensus representation . existing methods usually address this by assigning distinctive weights to different views . however , due to noisy nature of real-world applications , the confidence levels of samples in the same view may also vary . thus considering a unified weight for a view may lead to suboptimal solutions . in this paper , we propose a novel localized multi-view subspace clustering model that considers the confidence levels of both views and samples . by assigning weight to each sample under each view properly , we can obtain a robust consensus representation via fusing the noiseless structures among views and samples . we further develop a regularizer on weight parameters based on the convex conjugacy theory , and samples weights are determined in an adaptive manner . an efficient iterative algorithm is developed with a convergence guarantee . experimental results on four benchmarks demonstrate the correctness and effectiveness of the proposed model .", "topics": ["cluster analysis"]}
{"title": "bayesian nonparametric comorbidity analysis of psychiatric disorders", "abstract": "the analysis of comorbidity is an open and complex research field in the branch of psychiatry , where clinical experience and several studies suggest that the relation among the psychiatric disorders may have etiological and treatment implications . in this paper , we are interested in applying latent feature modeling to find the latent structure behind the psychiatric disorders that can help to examine and explain the relationships among them . to this end , we use the large amount of information collected in the national epidemiologic survey on alcohol and related conditions ( nesarc ) database and propose to model these data using a nonparametric latent model based on the indian buffet process ( ibp ) . due to the discrete nature of the data , we first need to adapt the observation model for discrete random variables . we propose a generative model in which the observations are drawn from a multinomial-logit distribution given the ibp matrix . the implementation of an efficient gibbs sampler is accomplished using the laplace approximation , which allows integrating out the weighting factors of the multinomial-logit likelihood model . we also provide a variational inference algorithm for this model , which provides a complementary ( and less expensive in terms of computational complexity ) alternative to the gibbs sampler allowing us to deal with a larger number of data . finally , we use the model to analyze comorbidity among the psychiatric disorders diagnosed by experts from the nesarc database .", "topics": ["sampling ( signal processing )", "generative model"]}
{"title": "deepsafe : a data-driven approach for checking adversarial robustness in neural networks", "abstract": "deep neural networks have become widely used , obtaining remarkable results in domains such as computer vision , speech recognition , natural language processing , audio recognition , social network filtering , machine translation , and bio-informatics , where they have produced results comparable to human experts . however , these networks can be easily fooled by adversarial perturbations : minimal changes to correctly-classified inputs , that cause the network to mis-classify them . this phenomenon represents a concern for both safety and security , but it is currently unclear how to measure a network 's robustness against such perturbations . existing techniques are limited to checking robustness around a few individual input points , providing only very limited guarantees . we propose a novel approach for automatically identifying safe regions of the input space , within which the network is robust against adversarial perturbations . the approach is data-guided , relying on clustering to identify well-defined geometric regions as candidate safe regions . we then utilize verification techniques to confirm that these regions are safe or to provide counter-examples showing that they are not safe . we also introduce the notion of targeted robustness which , for a given target label and region , ensures that a nn does not map any input in the region to the target label . we evaluated our technique on the mnist dataset and on a neural network implementation of a controller for the next-generation airborne collision avoidance system for unmanned aircraft ( acas xu ) . for these networks , our approach identified multiple regions which were completely safe as well as some which were only safe for specific labels . it also discovered several adversarial perturbations of interest .", "topics": ["cluster analysis", "natural language processing"]}
{"title": "local gaussian processes for efficient fine-grained traffic speed prediction", "abstract": "traffic speed is a key indicator for the efficiency of an urban transportation system . accurate modeling of the spatiotemporally varying traffic speed thus plays a crucial role in urban planning and development . this paper addresses the problem of efficient fine-grained traffic speed prediction using big traffic data obtained from static sensors . gaussian processes ( gps ) have been previously used to model various traffic phenomena , including flow and speed . however , gps do not scale with big traffic data due to their cubic time complexity . in this work , we address their efficiency issues by proposing local gps to learn from and make predictions for correlated subsets of data . the main idea is to quickly group speed variables in both spatial and temporal dimensions into a finite number of clusters , so that future and unobserved traffic speed queries can be heuristically mapped to one of such clusters . a local gp corresponding to that cluster can then be trained on the fly to make predictions in real-time . we call this method localization . we use non-negative matrix factorization for localization and propose simple heuristics for cluster mapping . we additionally leverage on the expressiveness of gp kernel functions to model road network topology and incorporate side information . extensive experiments using real-world traffic data collected in the two u.s. cities of pittsburgh and washington , d.c. , show that our proposed local gps significantly improve both runtime performances and prediction accuracies compared to the baseline global and local gps .", "topics": ["baseline ( configuration management )", "time complexity"]}
{"title": "a la carte - learning fast kernels", "abstract": "kernel methods have great promise for learning rich statistical representations of large modern datasets . however , compared to neural networks , kernel methods have been perceived as lacking in scalability and flexibility . we introduce a family of fast , flexible , lightly parametrized and general purpose kernel learning methods , derived from fastfood basis function expansions . we provide mechanisms to learn the properties of groups of spectral frequencies in these expansions , which require only o ( mlogd ) time and o ( m ) memory , for m basis functions and d input dimensions . we show that the proposed methods can learn a wide class of kernels , outperforming the alternatives in accuracy , speed , and memory consumption .", "topics": ["scalability"]}
{"title": "a hybrid model for enhancing lexical statistical machine translation ( smt )", "abstract": "the interest in statistical machine translation systems increases currently due to political and social events in the world . a proposed statistical machine translation ( smt ) based model that can be used to translate a sentence from the source language ( english ) to the target language ( arabic ) automatically through efficiently incorporating different statistical and natural language processing ( nlp ) models such as language model , alignment model , phrase based model , reordering model , and translation model . these models are combined to enhance the performance of statistical machine translation ( smt ) . many implementation tools have been used in this work such as moses , gizaa++ , irstlm , kenlm , and bleu . based on the implementation , evaluation of this model , and comparing the generated translation with other implemented machine translation systems like google translate , it was proved that this proposed model has enhanced the results of the statistical machine translation , and forms a reliable and efficient model in this field of research .", "topics": ["natural language processing", "machine translation"]}
{"title": "decision making for symbolic probability", "abstract": "this paper proposes a decision theory for a symbolic generalization of probability theory ( sp ) . darwiche and ginsberg [ 2,3 ] proposed sp to relax the requirement of using numbers for uncertainty while preserving desirable patterns of bayesian reasoning . sp represents uncertainty by symbolic supports that are ordered partially rather than completely as in the case of standard probability . we show that a preference relation on acts that satisfies a number of intuitive postulates is represented by a utility function whose domain is a set of pairs of supports . we argue that a subjective interpretation is as useful and appropriate for sp as it is for numerical probability . it is useful because the subjective interpretation provides a basis for uncertainty elicitation . it is appropriate because we can provide a decision theory that explains how preference on acts is based on support comparison .", "topics": ["numerical analysis"]}
{"title": "two novel evolutionary formulations of the graph coloring problem", "abstract": "we introduce two novel evolutionary formulations of the problem of coloring the nodes of a graph . the first formulation is based on the relationship that exists between a graph 's chromatic number and its acyclic orientations . it views such orientations as individuals and evolves them with the aid of evolutionary operators that are very heavily based on the structure of the graph and its acyclic orientations . the second formulation , unlike the first one , does not tackle one graph at a time , but rather aims at evolving a `program ' to color all graphs belonging to a class whose members all have the same number of nodes and other common attributes . the heuristics that result from these formulations have been tested on some of the second dimacs implementation challenge benchmark graphs , and have been found to be competitive when compared to the several other heuristics that have also been tested on those graphs .", "topics": ["heuristic"]}
{"title": "linear time computation of moments in sum-product networks", "abstract": "bayesian online algorithms for sum-product networks ( spns ) need to update their posterior distribution after seeing one single additional instance . to do so , they must compute moments of the model parameters under this distribution . the best existing method for computing such moments scales quadratically in the size of the spn , although it scales linearly for trees . this unfortunate scaling makes bayesian online algorithms prohibitively expensive , except for small or tree-structured spns . we propose an optimal linear-time algorithm that works even when the spn is a general directed acyclic graph ( dag ) , which significantly broadens the applicability of bayesian online algorithms for spns . there are three key ingredients in the design and analysis of our algorithm : 1 ) . for each edge in the graph , we construct a linear time reduction from the moment computation problem to a joint inference problem in spns . 2 ) . using the property that each spn computes a multilinear polynomial , we give an efficient procedure for polynomial evaluation by differentiation without expanding the network that may contain exponentially many monomials . 3 ) . we propose a dynamic programming method to further reduce the computation of the moments of all the edges in the graph from quadratic to linear . we demonstrate the usefulness of our linear time algorithm by applying it to develop a linear time assume density filter ( adf ) for spns .", "topics": ["computational complexity theory", "time complexity"]}
{"title": "stochastic optimization of smooth loss", "abstract": "in this paper , we first prove a high probability bound rather than an expectation bound for stochastic optimization with smooth loss . furthermore , the existing analysis requires the knowledge of optimal classifier for tuning the step size in order to achieve the desired bound . however , this information is usually not accessible in advanced . we also propose a strategy to address the limitation .", "topics": ["loss function", "iteration"]}
{"title": "improved strongly adaptive online learning using coin betting", "abstract": "this paper describes a new parameter-free online learning algorithm for changing environments . in comparing against algorithms with the same time complexity as ours , we obtain a strongly adaptive regret bound that is a factor of at least $ \\sqrt { \\log ( t ) } $ better , where $ t $ is the time horizon . empirical results show that our algorithm outperforms state-of-the-art methods in learning with expert advice and metric learning scenarios .", "topics": ["regret ( decision theory )", "time complexity"]}
{"title": "penalized estimation of directed acyclic graphs from discrete data", "abstract": "bayesian networks , with structure given by a directed acyclic graph ( dag ) , are a popular class of graphical models . however , learning bayesian networks from discrete or categorical data is particularly challenging , due to the large parameter space and the difficulty in searching for a sparse structure . in this article , we develop a maximum penalized likelihood method to tackle this problem . instead of the commonly used multinomial distribution , we model the conditional distribution of a node given its parents by multi-logit regression , in which an edge is parameterized by a set of coefficient vectors with dummy variables encoding the levels of a node . to obtain a sparse dag , a group norm penalty is employed , and a blockwise coordinate descent algorithm is developed to maximize the penalized likelihood subject to the acyclicity constraint of a dag . when interventional data are available , our method constructs a causal network , in which a directed edge represents a causal relation . we apply our method to various simulated and real data sets . the results show that our method is very competitive , compared to many existing methods , in dag estimation from both interventional and high-dimensional observational data .", "topics": ["graphical model", "sparse matrix"]}
{"title": "a comparative study on linguistic feature selection in sentiment polarity classification", "abstract": "sentiment polarity classification is perhaps the most widely studied topic . it classifies an opinionated document as expressing a positive or negative opinion . in this paper , using movie review dataset , we perform a comparative study with different single kind linguistic features and the combinations of these features . we find that the classic topic-based classifier ( naive bayes and support vector machine ) do not perform as well on sentiment polarity classification . and we find that with some combination of different linguistic features , the classification accuracy can be boosted a lot . we give some reasonable explanations about these boosting outcomes .", "topics": ["support vector machine"]}
{"title": "bethe projections for non-local inference", "abstract": "many inference problems in structured prediction are naturally solved by augmenting a tractable dependency structure with complex , non-local auxiliary objectives . this includes the mean field family of variational inference algorithms , soft- or hard-constrained inference using lagrangian relaxation or linear programming , collective graphical models , and forms of semi-supervised learning such as posterior regularization . we present a method to discriminatively learn broad families of inference objectives , capturing powerful non-local statistics of the latent variables , while maintaining tractable and provably fast inference using non-euclidean projected gradient descent with a distance-generating function given by the bethe entropy . we demonstrate the performance and flexibility of our method by ( 1 ) extracting structured citations from research papers by learning soft global constraints , ( 2 ) achieving state-of-the-art results on a widely-used handwriting recognition task using a novel learned non-convex inference procedure , and ( 3 ) providing a fast and highly scalable algorithm for the challenging problem of inference in a collective graphical model applied to bird migration .", "topics": ["graphical model", "calculus of variations"]}
{"title": "the role of macros in tractable planning", "abstract": "this paper presents several new tractability results for planning based on macros . we describe an algorithm that optimally solves planning problems in a class that we call inverted tree reducible , and is provably tractable for several subclasses of this class . by using macros to store partial plans that recur frequently in the solution , the algorithm is polynomial in time and space even for exponentially long plans . we generalize the inverted tree reducible class in several ways and describe modifications of the algorithm to deal with these new classes . theoretical results are validated in experiments .", "topics": ["mathematical optimization", "polynomial"]}
{"title": "efficient divide-and-conquer classification based on feature-space decomposition", "abstract": "this study presents a divide-and-conquer ( dc ) approach based on feature space decomposition for classification . when large-scale datasets are present , typical approaches usually employed truncated kernel methods on the feature space or dc approaches on the sample space . however , this did not guarantee separability between classes , owing to overfitting . to overcome such problems , this work proposes a novel dc approach on feature spaces consisting of three steps . firstly , we divide the feature space into several subspaces using the decomposition method proposed in this paper . subsequently , these feature subspaces are sent into individual local classifiers for training . finally , the outcomes of local classifiers are fused together to generate the final classification results . experiments on large-scale datasets are carried out for performance evaluation . the results show that the error rates of the proposed dc method decreased comparing with the state-of-the-art fast svm solvers , e.g . , reducing error rates by 10.53 % and 7.53 % on rcv1 and covtype datasets respectively .", "topics": ["feature vector", "support vector machine"]}
{"title": "causal discovery of linear cyclic models from multiple experimental data sets with overlapping variables", "abstract": "much of scientific data is collected as randomized experiments intervening on some and observing other variables of interest . quite often , a given phenomenon is investigated in several studies , and different sets of variables are involved in each study . in this article we consider the problem of integrating such knowledge , inferring as much as possible concerning the underlying causal structure with respect to the union of observed variables from such experimental or passive observational overlapping data sets . we do not assume acyclicity or joint causal sufficiency of the underlying data generating model , but we do restrict the causal relationships to be linear and use only second order statistics of the data . we derive conditions for full model identifiability in the most generic case , and provide novel techniques for incorporating an assumption of faithfulness to aid in inference . in each case we seek to establish what is and what is not determined by the data at hand .", "topics": ["causality"]}
{"title": "why and how to pay different attention to phrase alignments of different intensities", "abstract": "this work studies comparatively two typical sentence pair classification tasks : textual entailment ( te ) and answer selection ( as ) , observing that phrase alignments of different intensities contribute differently in these tasks . we address the problems of identifying phrase alignments of flexible granularity and pooling alignments of different intensities for these tasks . examples for flexible granularity are alignments between two single words , between a single word and a phrase and between a short phrase and a long phrase . by intensity we roughly mean the degree of match , it ranges from identity over surface-form co-occurrence , rephrasing and other semantic relatedness to unrelated words as in lots of parenthesis text . prior work ( i ) has limitations in phrase generation and representation , or ( ii ) conducts alignment at word and phrase levels by handcrafted features or ( iii ) utilizes a single attention mechanism over alignment intensities without considering the characteristics of specific tasks , which limits the system 's effectiveness across tasks . we propose an architecture based on gated recurrent unit that supports ( i ) representation learning of phrases of arbitrary granularity and ( ii ) task-specific focusing of phrase alignments between two sentences by attention pooling . experimental results on te and as match our observation and are state-of-the-art .", "topics": ["feature learning"]}
{"title": "using objective words in the reviews to improve the colloquial arabic sentiment analysis", "abstract": "one of the main difficulties in sentiment analysis of the arabic language is the presence of the colloquialism . in this paper , we examine the effect of using objective words in conjunction with sentimental words on sentiment classification for the colloquial arabic reviews , specifically jordanian colloquial reviews . the reviews often include both sentimental and objective words , however , the most existing sentiment analysis models ignore the objective words as they are considered useless . in this work , we created two lexicons : the first includes the colloquial sentimental words and compound phrases , while the other contains the objective words associated with values of sentiment tendency based on a particular estimation method . we used these lexicons to extract sentiment features that would be training input to the support vector machines ( svm ) to classify the sentiment polarity of the reviews . the reviews dataset have been collected manually from jeeran website . the results of the experiments show that the proposed approach improves the polarity classification in comparison to two baseline models , with accuracy 95.6 % .", "topics": ["baseline ( configuration management )", "support vector machine"]}
{"title": "learning dynamic boltzmann machines with spike-timing dependent plasticity", "abstract": "we propose a particularly structured boltzmann machine , which we refer to as a dynamic boltzmann machine ( dybm ) , as a stochastic model of a multi-dimensional time-series . the dybm can have infinitely many layers of units but allows exact and efficient inference and learning when its parameters have a proposed structure . this proposed structure is motivated by postulates and observations , from biological neural networks , that the synaptic weight is strengthened or weakened , depending on the timing of spikes ( i.e . , spike-timing dependent plasticity or stdp ) . we show that the learning rule of updating the parameters of the dybm in the direction of maximizing the likelihood of given time-series can be interpreted as stdp with long term potentiation and long term depression . the learning rule has a guarantee of convergence and can be performed in a distributed matter ( i.e . , local in space ) with limited memory ( i.e . , local in time ) .", "topics": ["time series"]}
{"title": "hierarchical gated recurrent neural tensor network for answer triggering", "abstract": "in this paper , we focus on the problem of answer triggering ad-dressed by yang et al . ( 2015 ) , which is a critical component for a real-world question answering system . we employ a hierarchical gated recurrent neural tensor ( hgrnt ) model to capture both the context information and the deep in-teractions between the candidate answers and the question . our result on f val-ue achieves 42.6 % , which surpasses the baseline by over 10 % .", "topics": ["baseline ( configuration management )", "interaction"]}
{"title": "injecting external solutions into cma-es", "abstract": "this report considers how to inject external candidate solutions into the cma-es algorithm . the injected solutions might stem from a gradient or a newton step , a surrogate model optimizer or any other oracle or search mechanism . they can also be the result of a repair mechanism , for example to render infeasible solutions feasible . only small modifications to the cma-es are necessary to turn injection into a reliable and effective method : too long steps need to be tightly renormalized . the main objective of this report is to reveal this simple mechanism . depending on the source of the injected solutions , interesting variants of cma-es arise . when the best-ever solution is always ( re- ) injected , an elitist variant of cma-es with weighted multi-recombination arises . when \\emph { all } solutions are injected from an \\emph { external } source , the resulting algorithm might be viewed as \\emph { adaptive encoding } with step-size control . in first experiments , injected solutions of very good quality lead to a convergence speed twice as fast as on the ( simple ) sphere function without injection . this means that we observe an impressive speed-up on otherwise difficult to solve functions . single bad injected solutions on the other hand do no significant harm .", "topics": ["gradient"]}
{"title": "gap analysis of natural language processing systems with respect to linguistic modality", "abstract": "modality is one of the important components of grammar in linguistics . it lets speaker to express attitude towards , or give assessment or potentiality of state of affairs . it implies different senses and thus has different perceptions as per the context . this paper presents an account showing the gap in the functionality of the current state of art natural language processing ( nlp ) systems . the contextual nature of linguistic modality is studied . in this paper , the works and logical approaches employed by natural language processing systems dealing with modality are reviewed . it sees human cognition and intelligence as multi-layered approach that can be implemented by intelligent systems for learning . lastly , current flow of research going on within this field is talked providing futurology .", "topics": ["natural language processing", "natural language"]}
{"title": "parallelizing linear recurrent neural nets over sequence length", "abstract": "recurrent neural networks ( rnns ) are widely used to model sequential data but their non-linear dependencies between sequence elements prevent parallelizing training over sequence length . we show the training of rnns with only linear sequential dependencies can be parallelized over the sequence length using the parallel scan algorithm , leading to rapid training on long sequences even with small minibatch size . we develop a parallel linear recurrence cuda kernel and show that it can be applied to immediately speed up training and inference of several state of the art rnn architectures by up to 9x . we abstract recent work on linear rnns into a new framework of linear surrogate rnns and develop a linear surrogate model for the long short-term memory unit , the gilr-lstm , that utilizes parallel linear recurrence . we extend sequence learning to new extremely long sequence regimes that were previously out of reach by successfully training a gilr-lstm on a synthetic sequence classification task with a one million timestep dependency .", "topics": ["recurrent neural network", "nonlinear system"]}
{"title": "unsupervised event abstraction using pattern abstraction and local process models", "abstract": "process mining analyzes business processes based on events stored in event logs . however , some recorded events may correspond to activities on a very low level of abstraction . when events are recorded on a too low level of granularity , process discovery methods tend to generate overgeneralizing process models . grouping low-level events to higher level activities , i.e . , event abstraction , can be used to discover better process models . existing event abstraction methods are mainly based on common sub-sequences and clustering techniques . in this paper , we propose to first discover local process models and then use those models to lift the event log to a higher level of abstraction . our conjecture is that process models discovered on the obtained high-level event log return process models of higher quality : their fitness and precision scores are more balanced . we show this with preliminary results on several real-life event logs .", "topics": ["cluster analysis", "high- and low-level"]}
{"title": "pain-free random differential privacy with sensitivity sampling", "abstract": "popular approaches to differential privacy , such as the laplace and exponential mechanisms , calibrate randomised smoothing through global sensitivity of the target non-private function . bounding such sensitivity is often a prohibitively complex analytic calculation . as an alternative , we propose a straightforward sampler for estimating sensitivity of non-private mechanisms . since our sensitivity estimates hold with high probability , any mechanism that would be $ ( \\epsilon , \\delta ) $ -differentially private under bounded global sensitivity automatically achieves $ ( \\epsilon , \\delta , \\gamma ) $ -random differential privacy ( hall et al . , 2012 ) , without any target-specific calculations required . we demonstrate on worked example learners how our usable approach adopts a naturally-relaxed privacy guarantee , while achieving more accurate releases even for non-private functions that are black-box computer programs .", "topics": ["sampling ( signal processing )", "time complexity"]}
{"title": "feature grouping from spatially constrained multiplicative interaction", "abstract": "we present a feature learning model that learns to encode relationships between images . the model is defined as a gated boltzmann machine , which is constrained such that hidden units that are nearby in space can gate each other 's connections . we show how frequency/orientation `` columns '' as well as topographic filter maps follow naturally from training the model on image pairs . the model also helps explain why square-pooling models yield feature groups with similar grouping properties . experimental results on synthetic image transformations show that spatially constrained gating is an effective way to reduce the number of parameters and thereby to regularize a transformation-learning model .", "topics": ["feature learning", "synthetic data"]}
{"title": "intrinsic grassmann averages for online linear and robust subspace learning", "abstract": "principal component analysis ( pca ) is a fundamental method for estimating a linear subspace approximation to high-dimensional data . many algorithms exist in literature to achieve a statistically robust version of pca called rpca . in this paper , we present a geometric framework for computing the principal linear subspaces in both situations that amounts to computing the intrinsic average on the space of all subspaces ( the grassmann manifold ) . points on this manifold are defined as the subspaces spanned by $ k $ -tuples of observations . we show that the intrinsic grassmann average of these subspaces coincide with the principal components of the observations when they are drawn from a gaussian distribution . similar results are also shown to hold for the rpca . further , we propose an efficient online algorithm to do subspace averaging which is of linear complexity in terms of number of samples and has a linear convergence rate . when the data has outliers , our proposed online robust subspace averaging algorithm shows significant performance ( accuracy and computation time ) gain over a recently published rpca methods with publicly accessible code . we have demonstrated competitive performance of our proposed online subspace algorithm method on one synthetic and two real data sets . experimental results depicting stability of our proposed method are also presented . furthermore , on two real outlier corrupted datasets , we present comparison experiments showing lower reconstruction error using our online rpca algorithm . in terms of reconstruction error and time required , both our algorithms outperform the competition .", "topics": ["time complexity", "synthetic data"]}
{"title": "human languages order information efficiently", "abstract": "most languages use the relative order between words to encode meaning relations . languages differ , however , in what orders they use and how these orders are mapped onto different meanings . we test the hypothesis that , despite these differences , human languages might constitute different `solutions ' to common pressures of language use . using monte carlo simulations over data from five languages , we find that their word orders are efficient for processing in terms of both dependency length and local lexical probability . this suggests that biases originating in how the brain understands language strongly constrain how human languages change over generations .", "topics": ["simulation"]}
{"title": "ensemble representation learning : an analysis of fitness and survival for wrapper-based genetic programming methods", "abstract": "recently we proposed a general , ensemble-based feature engineering wrapper ( few ) that was paired with a number of machine learning methods to solve regression problems . here , we adapt few for supervised classification and perform a thorough analysis of fitness and survival methods within this framework . our tests demonstrate that two fitness metrics , one introduced as an adaptation of the silhouette score , outperform the more commonly used fisher criterion . we analyze survival methods and demonstrate that $ \\epsilon $ -lexicase survival works best across our test problems , followed by random survival which outperforms both tournament and deterministic crowding . we conduct a benchmark comparison to several classification methods using a large set of problems and show that few can improve the best classifier performance in several cases . we show that few generates consistent , meaningful features for a biomedical problem with different ml pairings .", "topics": ["feature learning", "supervised learning"]}
{"title": "image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification", "abstract": "person re-identification ( re-id ) models trained on one domain often fail to generalize well to another . in our attempt , we present a `` learning via translation '' framework . in the baseline , we translate the labeled images from source to target domain in an unsupervised manner . we then train re-id models with the translated images by supervised methods . yet , being an essential part of this framework , unsupervised image-image translation suffers from the information loss of source-domain labels during translation . our motivation is two-fold . first , for each image , the discriminative cues contained in its id label should be maintained after translation . second , given the fact that two domains have entirely different persons , a translated image should be dissimilar to any of the target ids . to this end , we propose to preserve two types of unsupervised similarities , 1 ) self-similarity of an image before and after translation , and 2 ) domain-dissimilarity of a translated source image and a target image . both constraints are implemented in the similarity preserving generative adversarial network ( spgan ) which consists of a siamese network and a cyclegan . through domain adaptation experiment , we show that images generated by spgan are more suitable for domain adaptation and yield consistent and competitive re-id accuracy on two large-scale datasets .", "topics": ["baseline ( configuration management )", "supervised learning"]}
{"title": "a prototype malayalam to sign language automatic translator", "abstract": "sign language , which is a medium of communication for deaf people , uses manual communication and body language to convey meaning , as opposed to using sound . this paper presents a prototype malayalam text to sign language translation system . the proposed system takes malayalam text as input and generates corresponding sign language . output animation is rendered using a computer generated model . this system will help to disseminate information to the deaf people in public utility places like railways , banks , hospitals etc . this will also act as an educational tool in learning sign language .", "topics": ["machine translation"]}
{"title": "mental sampling in multimodal representations", "abstract": "both resources in the natural environment and concepts in a semantic space are distributed `` patchily '' , with large gaps in between the patches . to describe people 's internal and external foraging behavior , various random walk models have been proposed . in particular , internal foraging has been modeled as sampling : in order to gather relevant information for making a decision , people draw samples from a mental representation using random-walk algorithms such as markov chain monte carlo ( mcmc ) . however , two common empirical observations argue against simple sampling algorithms such as mcmc . first , the spatial structure is often best described by a l\\'evy flight distribution : the probability of the distance between two successive locations follows a power-law on the distances . second , the temporal structure of the sampling that humans and other animals produce have long-range , slowly decaying serial correlations characterized as $ 1/f $ -like fluctuations . we propose that mental sampling is not done by simple mcmc , but is instead adapted to multimodal representations and is implemented by metropolis-coupled markov chain monte carlo ( mc $ ^3 $ ) , one of the first algorithms developed for sampling from multimodal distributions . mc $ ^3 $ involves running multiple markov chains in parallel but with target distributions of different temperatures , and it swaps the states of the chains whenever a better location is found . heated chains more readily traverse valleys in the probability landscape to propose moves to far-away peaks , while the colder chains make the local steps that explore the current peak or patch . we show that mc $ ^3 $ generates distances between successive samples that follow a l\\'evy flight distribution and $ 1/f $ -like serial correlations , providing a single mechanistic account of these two puzzling empirical phenomena .", "topics": ["sampling ( signal processing )", "markov chain"]}
{"title": "random binary trees for approximate nearest neighbour search in binary space", "abstract": "approximate nearest neighbour ( ann ) search is one of the most important problems in computer science fields such as data mining or computer vision . in this paper , we focus on ann for high-dimensional binary vectors and we propose a simple yet powerful search method that uses random binary search trees ( rbst ) . we apply our method to a dataset of 1.25m binary local feature descriptors obtained from a real-life image-based localisation system provided by google as a part of project tango . an extensive evaluation of our method against the state-of-the-art variations of locality sensitive hashing ( lsh ) , namely uniform lsh and multi-probe lsh , shows the superiority of our method in terms of retrieval precision with performance boost of over 20 %", "topics": ["data mining", "computer vision"]}
{"title": "a note on the complexity of restricted attribute-value grammars", "abstract": "the recognition problem for attribute-value grammars ( avgs ) was shown to be undecidable by johnson in 1988 . therefore , the general form of avgs is of no practical use . in this paper we study a very restricted form of avg , for which the recognition problem is decidable ( though still np-complete ) , the r-avg . we show that the r-avg formalism captures all of the context free languages and more , and introduce a variation on the so-called `off-line parsability constraint ' , the `honest parsability constraint ' , which lets different types of r-avg coincide precisely with well-known time complexity classes .", "topics": ["time complexity"]}
{"title": "momentum and stochastic momentum for stochastic gradient , newton , proximal point and subspace descent methods", "abstract": "in this paper we study several classes of stochastic optimization algorithms enriched with heavy ball momentum . among the methods studied are : stochastic gradient descent , stochastic newton , stochastic proximal point and stochastic dual subspace ascent . this is the first time momentum variants of several of these methods are studied . we choose to perform our analysis in a setting in which all of the above methods are equivalent . we prove global nonassymptotic linear convergence rates for all methods and various measures of success , including primal function values , primal iterates ( in l2 sense ) , and dual function values . we also show that the primal iterates converge at an accelerated linear rate in the l1 sense . this is the first time a linear rate is shown for the stochastic heavy ball method ( i.e . , stochastic gradient descent method with momentum ) . under somewhat weaker conditions , we establish a sublinear convergence rate for cesaro averages of primal iterates . moreover , we propose a novel concept , which we call stochastic momentum , aimed at decreasing the cost of performing the momentum step . we prove linear convergence of several stochastic methods with stochastic momentum , and show that in some sparse data regimes and for sufficiently small momentum parameters , these methods enjoy better overall complexity than methods with deterministic momentum . finally , we perform extensive numerical testing on artificial and real datasets , including data coming from average consensus problems .", "topics": ["value ( ethics )", "numerical analysis"]}
{"title": "observational-interventional priors for dose-response learning", "abstract": "controlled interventions provide the most direct source of information for learning causal effects . in particular , a dose-response curve can be learned by varying the treatment level and observing the corresponding outcomes . however , interventions can be expensive and time-consuming . observational data , where the treatment is not controlled by a known mechanism , is sometimes available . under some strong assumptions , observational data allows for the estimation of dose-response curves . estimating such curves nonparametrically is hard : sample sizes for controlled interventions may be small , while in the observational case a large number of measured confounders may need to be marginalized . in this paper , we introduce a hierarchical gaussian process prior that constructs a distribution over the dose-response curve by learning from observational data , and reshapes the distribution with a nonparametric affine transform learned from controlled interventions . this function composition from different sources is shown to speed-up learning , which we demonstrate with a thorough sensitivity analysis and an application to modeling the effect of therapy on cognitive skills of premature infants .", "topics": ["causality"]}
{"title": "unsupervised aspect term extraction with b-lstm & crf using automatically labelled datasets", "abstract": "aspect term extraction ( ate ) identifies opinionated aspect terms in texts and is one of the tasks in the semeval aspect based sentiment analysis ( absa ) contest . the small amount of available datasets for supervised ate and the costly human annotation for aspect term labelling give rise to the need for unsupervised ate . in this paper , we introduce an architecture that achieves top-ranking performance for supervised ate . moreover , it can be used efficiently as feature extractor and classifier for unsupervised ate . our second contribution is a method to automatically construct datasets for ate . we train a classifier on our automatically labelled datasets and evaluate it on the human annotated semeval absa test sets . compared to a strong rule-based baseline , we obtain a dramatically higher f-score and attain precision values above 80 % . our unsupervised method beats the supervised absa baseline from semeval , while preserving high precision scores .", "topics": ["baseline ( configuration management )", "unsupervised learning"]}
{"title": "simple regret optimization in online planning for markov decision processes", "abstract": "we consider online planning in markov decision processes ( mdps ) . in online planning , the agent focuses on its current state only , deliberates about the set of possible policies from that state onwards and , when interrupted , uses the outcome of that exploratory deliberation to choose what action to perform next . the performance of algorithms for online planning is assessed in terms of simple regret , which is the agent 's expected performance loss when the chosen action , rather than an optimal one , is followed . to date , state-of-the-art algorithms for online planning in general mdps are either best effort , or guarantee only polynomial-rate reduction of simple regret over time . here we introduce a new monte-carlo tree search algorithm , brue , that guarantees exponential-rate reduction of simple regret and error probability . this algorithm is based on a simple yet non-standard state-space sampling scheme , mcts2e , in which different parts of each sample are dedicated to different exploratory objectives . our empirical evaluation shows that brue not only provides superior performance guarantees , but is also very effective in practice and favorably compares to state-of-the-art . we then extend brue with a variant of `` learning by forgetting . '' the resulting set of algorithms , brue ( alpha ) , generalizes brue , improves the exponential factor in the upper bound on its reduction rate , and exhibits even more attractive empirical performance .", "topics": ["sampling ( signal processing )", "regret ( decision theory )"]}
{"title": "convolutional networks on graphs for learning molecular fingerprints", "abstract": "we introduce a convolutional neural network that operates directly on graphs . these networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape . the architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints . we show that these data-driven features are more interpretable , and have better predictive performance on a variety of tasks .", "topics": ["end-to-end principle"]}
{"title": "a linearly convergent conditional gradient algorithm with applications to online and stochastic optimization", "abstract": "linear optimization is many times algorithmically simpler than non-linear convex optimization . linear optimization over matroid polytopes , matching polytopes and path polytopes are example of problems for which we have simple and efficient combinatorial algorithms , but whose non-linear convex counterpart is harder and admits significantly less efficient algorithms . this motivates the computational model of convex optimization , including the offline , online and stochastic settings , using a linear optimization oracle . in this computational model we give several new results that improve over the previous state-of-the-art . our main result is a novel conditional gradient algorithm for smooth and strongly convex optimization over polyhedral sets that performs only a single linear optimization step over the domain on each iteration and enjoys a linear convergence rate . this gives an exponential improvement in convergence rate over previous results . based on this new conditional gradient algorithm we give the first algorithms for online convex optimization over polyhedral sets that perform only a single linear optimization step over the domain while having optimal regret guarantees , answering an open question of kalai and vempala , and hazan and kale . our online algorithms also imply conditional gradient algorithms for non-smooth and stochastic convex optimization with the same convergence rates as projected ( sub ) gradient methods .", "topics": ["regret ( decision theory )", "time complexity"]}
{"title": "learning classifiers from synthetic data using a multichannel autoencoder", "abstract": "we propose a method for using synthetic data to help learning classifiers . synthetic data , even is generated based on real data , normally results in a shift from the distribution of real data in feature space . to bridge the gap between the real and synthetic data , and jointly learn from synthetic and real data , this paper proposes a multichannel autoencoder ( mcae ) . we show that by suing mcae , it is possible to learn a better feature representation for classification . to evaluate the proposed approach , we conduct experiments on two types of datasets . experimental results on two datasets validate the efficiency of our mcae model and our methodology of generating synthetic data .", "topics": ["synthetic data", "feature vector"]}
{"title": "sobolev gan", "abstract": "we propose a new integral probability metric ( ipm ) between distributions : the sobolev ipm . the sobolev ipm compares the mean discrepancy of two distributions for functions ( critic ) restricted to a sobolev ball defined with respect to a dominant measure $ \\mu $ . we show that the sobolev ipm compares two distributions in high dimensions based on weighted conditional cumulative distribution functions ( cdf ) of each coordinate on a leave one out basis . the dominant measure $ \\mu $ plays a crucial role as it defines the support on which conditional cdfs are compared . sobolev ipm can be seen as an extension of the one dimensional von-mises cram\\'er statistics to high dimensional distributions . we show how sobolev ipm can be used to train generative adversarial networks ( gans ) . we then exploit the intrinsic conditioning implied by sobolev ipm in text generation . finally we show that a variant of sobolev gan achieves competitive results in semi-supervised learning on cifar-10 , thanks to the smoothness enforced on the critic by sobolev gan which relates to laplacian regularization .", "topics": ["supervised learning", "matrix regularization"]}
{"title": "on using unsatisfiability for solving maximum satisfiability", "abstract": "maximum satisfiability ( maxsat ) is a well-known optimization pro- blem , with several practical applications . the most widely known maxs at algorithms are ineffective at solving hard problems instances from practical application domains . recent work proposed using efficient boolean satisfiability ( sat ) solvers for solving the maxsat problem , based on identifying and eliminating unsatisfiable subformulas . however , these algorithms do not scale in practice . this paper analyzes existing maxsat algorithms based on unsatisfiable subformula identification . moreover , the paper proposes a number of key optimizations to these maxsat algorithms and a new alternative algorithm . the proposed optimizations and the new algorithm provide significant performance improvements on maxsat instances from practical applications . moreover , the efficiency of the new generation of unsatisfiability-based maxsat solvers becomes effectively indexed to the ability of modern sat solvers to proving unsatisfiability and identifying unsatisfiable subformulas .", "topics": ["optimization problem"]}
{"title": "other topics you may also agree or disagree : modeling inter-topic preferences using tweets and matrix factorization", "abstract": "we present in this paper our approach for modeling inter-topic preferences of twitter users : for example , those who agree with the trans-pacific partnership ( tpp ) also agree with free trade . this kind of knowledge is useful not only for stance detection across multiple topics but also for various real-world applications including public opinion surveys , electoral predictions , electoral campaigns , and online debates . in order to extract users ' preferences on twitter , we design linguistic patterns in which people agree and disagree about specific topics ( e.g . , `` a is completely wrong '' ) . by applying these linguistic patterns to a collection of tweets , we extract statements agreeing and disagreeing with various topics . inspired by previous work on item recommendation , we formalize the task of modeling inter-topic preferences as matrix factorization : representing users ' preferences as a user-topic matrix and mapping both users and topics onto a latent feature space that abstracts the preferences . our experimental results demonstrate both that our proposed approach is useful in predicting missing preferences of users and that the latent vector representations of topics successfully encode inter-topic preferences .", "topics": ["feature vector"]}
{"title": "neural generative question answering", "abstract": "this paper presents an end-to-end neural network model , named neural generative question answering ( genqa ) , that can generate answers to simple factoid questions , based on the facts in a knowledge-base . more specifically , the model is built on the encoder-decoder framework for sequence-to-sequence learning , while equipped with the ability to enquire the knowledge-base , and is trained on a corpus of question-answer pairs , with their associated triples in the knowledge-base . empirical study shows the proposed model can effectively deal with the variations of questions and answers , and generate right and natural answers by referring to the facts in the knowledge-base . the experiment on question answering demonstrates that the proposed model can outperform an embedding-based qa model as well as a neural dialogue model trained on the same data .", "topics": ["natural language", "text corpus"]}
{"title": "multidimensional scaling in the poincare disk", "abstract": "multidimensional scaling ( mds ) is a class of projective algorithms traditionally used in euclidean space to produce two- or three-dimensional visualizations of datasets of multidimensional points or point distances . more recently however , several authors have pointed out that for certain datasets , hyperbolic target space may provide a better fit than euclidean space . in this paper we develop pd-mds , a metric mds algorithm designed specifically for the poincare disk ( pd ) model of the hyperbolic plane . emphasizing the importance of proceeding from first principles in spite of the availability of various black box optimizers , our construction is based on an elementary hyperbolic line search and reveals numerous particulars that need to be carefully addressed when implementing this as well as more sophisticated iterative optimization methods in a hyperbolic space model .", "topics": ["approximation algorithm", "iteration"]}
{"title": "unbiased estimates for linear regression via volume sampling", "abstract": "for a full rank $ n\\times d $ matrix $ x $ with $ n\\ge d $ , consider the task of solving the linear least squares problem , where we try to predict a response value for each of the $ n $ rows of $ x $ . assume that obtaining the responses is expensive and we can only afford to attain the responses for a small subset of rows . we show that a good approximate solution to this least squares problem can be obtained from just dimension $ d $ many responses . concretely , if the rows are in general position and if a subset of $ d $ rows is chosen proportional to the squared volume spanned by those rows , then the expected total square loss ( on all $ n $ rows ) of the least squares solution found for the subset is exactly $ d+1 $ times the minimum achievable total loss . we provide lower bounds showing that the factor of $ d+1 $ is optimal , and any iid row sampling procedure requires $ \\omega ( d\\log d ) $ responses to achieve a finite factor guarantee . moreover , the least squares solution obtained for the volume sampled subproblem is an unbiased estimator of optimal solution based on all $ n $ responses . our methods lead to general matrix expectation formulas for volume sampling which go beyond linear regression . in particular , we propose a matrix estimator for the pseudoinverse $ x^+ $ , computed from a small subset of rows of the matrix $ x $ . the estimator is unbiased and surprisingly its covariance also has a closed form : it equals a specific factor times $ x^ { + } x^ { +\\top } $ . we believe that these new formulas establish a fundamental connection between linear least squares and volume sampling . our analysis for computing matrix expectations is based on reverse iterative volume sampling , a technique which also leads to a new algorithm for volume sampling that is by a factor of $ n^2 $ faster than the state-of-the-art .", "topics": ["sampling ( signal processing )", "optimization problem"]}
{"title": "fast color transfer from multiple images", "abstract": "color transfer between images uses the statistics information of image effectively . we present a novel approach of local color transfer between images based on the simple statistics and locally linear embedding . a sketching interface is proposed for quickly and easily specifying the color correspondences between target and source image . the user can specify the correspondences of local region using scribes , which more accurately transfers the target color to the source image while smoothly preserving the boundaries , and exhibits more natural output results . our algorithm is not restricted to one-to-one image color transfer and can make use of more than one target images to transfer the color in different regions in the source image . moreover , our algorithm does not require to choose the same color style and image size between source and target images . we propose the sub-sampling to reduce the computational load . comparing with other approaches , our algorithm is much better in color blending in the input data . our approach preserves the other color details in the source image . various experimental results show that our approach specifies the correspondences of local color region in source and target images . and it expresses the intention of users and generates more actual and natural results of visual effect .", "topics": ["sampling ( signal processing )"]}
{"title": "a multifaceted evaluation of neural versus phrase-based machine translation for 9 language directions", "abstract": "we aim to shed light on the strengths and weaknesses of the newly introduced neural machine translation paradigm . to that end , we conduct a multifaceted evaluation in which we compare outputs produced by state-of-the-art neural machine translation and phrase-based machine translation systems for 9 language directions across a number of dimensions . specifically , we measure the similarity of the outputs , their fluency and amount of reordering , the effect of sentence length and performance across different error categories . we find out that translations produced by neural machine translation systems are considerably different , more fluent and more accurate in terms of word order compared to those produced by phrase-based systems . neural machine translation systems are also more accurate at producing inflected forms , but they perform poorly when translating very long sentences .", "topics": ["machine translation"]}
{"title": "pacgan : the power of two samples in generative adversarial networks", "abstract": "generative adversarial networks ( gans ) are innovative techniques for learning generative models of complex data distributions from samples . despite remarkable recent improvements in generating realistic images , one of their major shortcomings is the fact that in practice , they tend to produce samples with little diversity , even when trained on diverse datasets . this phenomenon , known as mode collapse , has been the main focus of several recent advances in gans . yet there is little understanding of why mode collapse happens and why existing approaches are able to mitigate mode collapse . we propose a principled approach to handling mode collapse , which we call packing . the main idea is to modify the discriminator to make decisions based on multiple samples from the same class , either real or artificially generated . we borrow analysis tools from binary hypothesis testing -- -in particular the seminal result of blackwell [ bla53 ] -- -to prove a fundamental connection between packing and mode collapse . we show that packing naturally penalizes generators with mode collapse , thereby favoring generator distributions with less mode collapse during the training process . numerical experiments on benchmark datasets suggests that packing provides significant improvements in practice as well .", "topics": ["numerical analysis"]}
{"title": "sequence to sequence learning with neural networks", "abstract": "deep neural networks ( dnns ) are powerful models that have achieved excellent performance on difficult learning tasks . although dnns work well whenever large labeled training sets are available , they can not be used to map sequences to sequences . in this paper , we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure . our method uses a multilayered long short-term memory ( lstm ) to map the input sequence to a vector of a fixed dimensionality , and then another deep lstm to decode the target sequence from the vector . our main result is that on an english to french translation task from the wmt'14 dataset , the translations produced by the lstm achieve a bleu score of 34.8 on the entire test set , where the lstm 's bleu score was penalized on out-of-vocabulary words . additionally , the lstm did not have difficulty on long sentences . for comparison , a phrase-based smt system achieves a bleu score of 33.3 on the same dataset . when we used the lstm to rerank the 1000 hypotheses produced by the aforementioned smt system , its bleu score increases to 36.5 , which is close to the previous best result on this task . the lstm also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice . finally , we found that reversing the order of the words in all source sentences ( but not target sentences ) improved the lstm 's performance markedly , because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier .", "topics": ["test set", "optimization problem"]}
{"title": "a directionally selective small target motion detecting visual neural network in cluttered backgrounds", "abstract": "discriminating targets moving against a cluttered background is a huge challenge , let alone detecting a target as small as one or a few pixels and tracking it in flight . in the fly 's visual system , a class of specific neurons , called small target motion detectors ( stmds ) , have been identified as showing exquisite selectivity for small target motion . some of the stmds have also demonstrated directional selectivity which means these stmds respond strongly only to their preferred motion direction . directional selectivity is an important property of these stmd neurons which could contribute to tracking small targets such as mates in flight . however , little has been done on systematically modeling these directional selective stmd neurons . in this paper , we propose a directional selective stmd-based neural network ( dstmd ) for small target detection in a cluttered background . in the proposed neural network , a new correlation mechanism is introduced for direction selectivity via correlating signals relayed from two pixels . then , a lateral inhibition mechanism is implemented on the spatial field for size selectivity of stmd neurons . extensive experiments showed that the proposed neural network not only is in accord with current biological findings , i.e . showing directional preferences , but also worked reliably in detecting small targets against cluttered backgrounds .", "topics": ["pixel"]}
{"title": "a new architecture for making highly scalable applications", "abstract": "an application is a logical image of the world on a computer . a scalable application is an application that allows one to update that logical image at run time . to put it in operational terms : an application is scalable if a client can change between time t1 and time t2 - the logic of the application as expressed by language l ; - the structure and volume of the stored knowledge ; - the user interface of the application ; while clients working with the application at time t1 will work with the changed application at time t2 without performing any special action between t1 and t2 . in order to realize such a scalable application a new architecture has been developed that fully orbits around language . in order to verify the soundness of that architecture a program has been build . both architecture and program are called communsens . the main purpose of this paper is : - to list the relevant elements of the architecture ; - to give a visual presentation of how the program and its image of the world look like ; - to give a visual presentation of how the image can be updated . some relevant philosophical and practical backgrounds are included in the appendixes .", "topics": ["scalability"]}
{"title": "generalized gradient learning on time series under elastic transformations", "abstract": "the majority of machine learning algorithms assumes that objects are represented as vectors . but often the objects we want to learn on are more naturally represented by other data structures such as sequences and time series . for these representations many standard learning algorithms are unavailable . we generalize gradient-based learning algorithms to time series under dynamic time warping . to this end , we introduce elastic functions , which extend functions on time series to matrix spaces . necessary conditions are presented under which generalized gradient learning on time series is consistent . we indicate how results carry over to arbitrary elastic distance functions and to sequences consisting of symbolic elements . specifically , four linear classifiers are extended to time series under dynamic time warping and applied to benchmark datasets . results indicate that generalized gradient learning via elastic functions have the potential to complement the state-of-the-art in statistical pattern recognition on time series .", "topics": ["time series", "gradient"]}
{"title": "effective sketching methods for value function approximation", "abstract": "high-dimensional representations , such as radial basis function networks or tile coding , are common choices for policy evaluation in reinforcement learning . learning with such high-dimensional representations , however , can be expensive , particularly for matrix methods , such as least-squares temporal difference learning or quasi-newton methods that approximate matrix step-sizes . in this work , we explore the utility of sketching for these two classes of algorithms . we highlight issues with sketching the high-dimensional features directly , which can incur significant bias . as a remedy , we demonstrate how to use sketching more sparingly , with only a left-sided sketch , that can still enable significant computational gains and the use of these matrix-based learning algorithms that are less sensitive to parameters . we empirically investigate these algorithms , in four domains with a variety of representations . our aim is to provide insights into effective use of sketching in practice .", "topics": ["approximation algorithm", "reinforcement learning"]}
{"title": "bi-directional attention with agreement for dependency parsing", "abstract": "we develop a novel bi-directional attention model for dependency parsing , which learns to agree on headword predictions from the forward and backward parsing directions . the parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings . the proposed parser makes use of { \\it soft } headword embeddings , allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity . we conduct experiments on english , chinese , and 12 other languages from the conll 2006 shared task , showing that the proposed model achieves state-of-the-art unlabeled attachment scores on 6 languages .", "topics": ["computational complexity theory", "parsing"]}
{"title": "diversity enhancement for micro-differential evolution", "abstract": "the differential evolution ( de ) algorithm suffers from high computational time due to slow nature of evaluation . in contrast , micro-de ( mde ) algorithms employ a very small population size , which can converge faster to a reasonable solution . however , these algorithms are vulnerable to a premature convergence as well as to high risk of stagnation . in this paper , mde algorithm with vectorized random mutation factor ( mdevm ) is proposed , which utilizes the small size population benefit while empowers the exploration ability of mutation factor through randomizing it in the decision variable level . the idea is supported by analyzing mutation factor using monte-carlo based simulations . to facilitate the usage of mde algorithms with very-small population sizes , new mutation schemes for population sizes less than four are also proposed . furthermore , comprehensive comparative simulations and analysis on performance of the mde algorithms over various mutation schemes , population sizes , problem types ( i.e . uni-modal , multi-modal , and composite ) , problem dimensionalities , and mutation factor ranges are conducted by considering population diversity analysis for stagnation and trapping in local optimum situations . the studies are conducted on 28 benchmark functions provided for the ieee cec-2013 competition . experimental results demonstrate high performance and convergence speed of the proposed mdevm algorithm .", "topics": ["time complexity", "simulation"]}
{"title": "learning in the model space for fault diagnosis", "abstract": "the emergence of large scaled sensor networks facilitates the collection of large amounts of real-time data to monitor and control complex engineering systems . however , in many cases the collected data may be incomplete or inconsistent , while the underlying environment may be time-varying or un-formulated . in this paper , we have developed an innovative cognitive fault diagnosis framework that tackles the above challenges . this framework investigates fault diagnosis in the model space instead of in the signal space . learning in the model space is implemented by fitting a series of models using a series of signal segments selected with a rolling window . by investigating the learning techniques in the fitted model space , faulty models can be discriminated from healthy models using one-class learning algorithm . the framework enables us to construct fault library when unknown faults occur , which can be regarded as cognitive fault isolation . this paper also theoretically investigates how to measure the pairwise distance between two models in the model space and incorporates the model distance into the learning algorithm in the model space . the results on three benchmark applications and one simulated model for the barcelona water distribution network have confirmed the effectiveness of the proposed framework .", "topics": ["simulation"]}
{"title": "revisiting recurrent networks for paraphrastic sentence embeddings", "abstract": "we consider the problem of learning general-purpose , paraphrastic sentence embeddings , revisiting the setting of wieting et al . ( 2016b ) . while they found lstm recurrent networks to underperform word averaging , we present several developments that together produce the opposite conclusion . these include training on sentence pairs rather than phrase pairs , averaging states to represent sequences , and regularizing aggressively . these improve lstms in both transfer learning and supervised settings . we also introduce a new recurrent architecture , the gated recurrent averaging network , that is inspired by averaging and lstms while outperforming them both . we analyze our learned models , finding evidence of preferences for particular parts of speech and dependency relations .", "topics": ["supervised learning"]}
{"title": "a neural conversational model", "abstract": "conversational modeling is an important task in natural language understanding and machine intelligence . although previous approaches exist , they are often restricted to specific domains ( e.g . , booking an airline ticket ) and require hand-crafted rules . in this paper , we present a simple approach for this task which uses the recently proposed sequence to sequence framework . our model converses by predicting the next sentence given the previous sentence or sentences in a conversation . the strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules . we find that this straightforward model can generate simple conversations given a large conversational training dataset . our preliminary results suggest that , despite optimizing the wrong objective function , the model is able to converse well . it is able extract knowledge from both a domain specific dataset , and from a large , noisy , and general domain dataset of movie subtitles . on a domain-specific it helpdesk dataset , the model can find a solution to a technical problem via conversations . on a noisy open-domain movie transcript dataset , the model can perform simple forms of common sense reasoning . as expected , we also find that the lack of consistency is a common failure mode of our model .", "topics": ["optimization problem", "natural language"]}
{"title": "zoom out-and-in network with map attention decision for region proposal and object detection", "abstract": "in this paper , we propose a zoom-out-and-in network for generating object proposals . a key observation is that it is difficult to classify anchors of different sizes with the same set of features . anchors of different sizes should be placed accordingly based on different depth within a network : smaller boxes on high-resolution layers with a smaller stride while larger boxes on low-resolution counterparts with a larger stride . inspired by the conv/deconv structure , we fully leverage the low-level local details and high-level regional semantics from two feature map streams , which are complimentary to each other , to identify the objectness in an image . a map attention decision ( mad ) unit is further proposed to aggressively search for neuron activations among two streams and attend the most contributive ones on the feature learning of the final loss . the unit serves as a decision maker to adaptively activate maps along certain channels with the solely purpose of optimizing the overall training loss . one advantage of mad is that the learned weights enforced on each feature channel is predicted on-the-fly based on the input context , which is more suitable than the fixed enforcement of a convolutional kernel . experimental results on three datasets demonstrate the effectiveness of our proposed algorithm over other state-of-the-arts , in terms of average recall for region proposal and average precision for object detection .", "topics": ["feature learning", "high- and low-level"]}
{"title": "bacterial foraging optimization based brain magnetic resonance image segmentation", "abstract": "segmentation partitions an image into its constituent parts . it is essentially the pre-processing stage of image analysis and computer vision . in this work , t1 and t2 weighted brain magnetic resonance images are segmented using multilevel thresholding and bacterial foraging optimization ( bfo ) algorithm . the thresholds are obtained by maximizing the between class variance ( multilevel otsu method ) of the image . the bfo algorithm is used to optimize the threshold searching process . the edges are then obtained from the thresholded image by comparing the intensity of each pixel with its eight connected neighbourhood . post processing is performed to remove spurious responses in the segmented image . the proposed segmentation technique is evaluated using edge detector evaluation parameters such as figure of merit , rand index and variation of information . the proposed brain mr image segmentation technique outperforms the traditional edge detectors such as canny and sobel .", "topics": ["image segmentation", "mathematical optimization"]}
{"title": "robust recovery of subspace structures by low-rank representation", "abstract": "in this work we address the subspace recovery problem . given a set of data samples ( vectors ) approximately drawn from a union of multiple subspaces , our goal is to segment the samples into their respective subspaces and correct the possible errors as well . to this end , we propose a novel method termed low-rank representation ( lrr ) , which seeks the lowest-rank representation among all the candidates that can represent the data samples as linear combinations of the bases in a given dictionary . it is shown that lrr well solves the subspace recovery problem : when the data is clean , we prove that lrr exactly captures the true subspace structures ; for the data contaminated by outliers , we prove that under certain conditions lrr can exactly recover the row space of the original data and detect the outlier as well ; for the data corrupted by arbitrary errors , lrr can also approximately recover the row space with theoretical guarantees . since the subspace membership is provably determined by the row space , these further imply that lrr can perform robust subspace segmentation and error correction , in an efficient way .", "topics": ["cluster analysis", "optimization problem"]}
{"title": "automated breast lesion segmentation in ultrasound images", "abstract": "the main objective of this project is to segment different breast ultrasound images to find out lesion area by discarding the low contrast regions as well as the inherent speckle noise . the proposed method consists of three stages ( removing noise , segmentation , classification ) in order to extract the correct lesion . we used normalized cuts approach to segment ultrasound images into regions of interest where we can possibly finds the lesion , and then k-means classifier is applied to decide finally the location of the lesion . for every original image , an annotated ground-truth image is given to perform comparison with the obtained experimental results , providing accurate evaluation measures .", "topics": ["statistical classification", "ground truth"]}
{"title": "interactive music generation with positional constraints using anticipation-rnns", "abstract": "recurrent neural networks ( rnns ) are now widely used on sequence generation tasks due to their ability to learn long-range dependencies and to generate sequences of arbitrary length . however , their left-to-right generation procedure only allows a limited control from a potential user which makes them unsuitable for interactive and creative usages such as interactive music generation . this paper introduces a novel architecture called anticipation-rnn which possesses the assets of the rnn-based generative models while allowing to enforce user-defined positional constraints . we demonstrate its efficiency on the task of generating melodies satisfying positional constraints in the style of the soprano parts of the j.s . bach chorale harmonizations . sampling using the anticipation-rnn is of the same order of complexity than sampling from the traditional rnn model . this fast and interactive generation of musical sequences opens ways to devise real-time systems that could be used for creative purposes .", "topics": ["generative model", "computational complexity theory"]}
{"title": "parallel mmf : a multiresolution approach to matrix computation", "abstract": "multiresolution matrix factorization ( mmf ) was recently introduced as a method for finding multiscale structure and defining wavelets on graphs/matrices . in this paper we derive pmmf , a parallel algorithm for computing the mmf factorization . empirically , the running time of pmmf scales linearly in the dimension for sparse matrices . we argue that this makes pmmf a valuable new computational primitive in its own right , and present experiments on using pmmf for two distinct purposes : compressing matrices and preconditioning large sparse linear systems .", "topics": ["time complexity", "sparse matrix"]}
{"title": "an ensemble-based online learning algorithm for streaming data", "abstract": "in this study , we introduce an ensemble-based approach for online machine learning . the ensemble of base classifiers in our approach is obtained by learning naive bayes classifiers on different training sets which are generated by projecting the original training set to lower dimensional space . we propose a mechanism to learn sequences of data using data chunks paradigm . the experiments conducted on a number of uci datasets and one synthetic dataset demonstrate that the proposed approach performs significantly better than some well-known online learning algorithms .", "topics": ["test set", "synthetic data"]}
{"title": "towards understanding generalization of deep learning : perspective of loss landscapes", "abstract": "it is widely observed that deep learning models with learned parameters generalize well , even with much more model parameters than the number of training samples . we systematically investigate the underlying reasons why deep neural networks often generalize well , and reveal the difference between the minima ( with the same training error ) that generalize well and those they do n't . we show that it is the characteristics the landscape of the loss function that explains the good generalization capability . for the landscape of loss function for deep networks , the volume of basin of attraction of good minima dominates over that of poor minima , which guarantees optimization methods with random initialization to converge to good minima . we theoretically justify our findings through analyzing 2-layer neural networks ; and show that the low-complexity solutions have a small norm of hessian matrix with respect to model parameters . for deeper networks , extensive numerical evidence helps to support our arguments .", "topics": ["numerical analysis", "loss function"]}
{"title": "3d seismic data denoising using two-dimensional sparse coding scheme", "abstract": "seismic data denoising is vital to geophysical applications and the transform-based function method is one of the most widely used techniques . however , it is challenging to design a suit- able sparse representation to express a transform-based func- tion group due to the complexity of seismic data . in this paper , we apply a seismic data denoising method based on learning- type overcomplete dictionaries which uses two-dimensional sparse coding ( 2dsc ) . first , we model the input seismic data and dictionaries as third-order tensors and introduce tensor- linear combinations for data approximation . second , we ap- ply learning-type overcomplete dictionary , i.e . , optimal sparse data representation is achieved through learning and training . third , we exploit the alternating minimization algorithm to solve the optimization problem of seismic denoising . finally we evaluate its denoising performance on synthetic seismic data and land data survey . experiment results show that the two-dimensional sparse coding scheme reduces computational costs and enhances the signal-to-noise ratio .", "topics": ["noise reduction", "sparse matrix"]}
{"title": "bridging the gap between legal practitioners and knowledge engineers using semi-formal kr", "abstract": "the use of structured english as a computation independent knowledge representation format for non-technical users in business rules representation has been proposed in omgs semantics and business vocabulary representation ( sbvr ) . in the legal domain we face a similar problem . formal representation languages , such as oasis legalruleml and legal ontologies ( lkif , legal owl2 ontologies etc . ) support the technical knowledge engineer and the automated reasoning . but , they can be hardly used directly by the legal domain experts who do not have a computer science background . in this paper we adapt the sbvr structured english approach for the legal domain and implement a proof-of-concept , called kr4iplaw , which enables legal domain experts to represent their knowledge in structured english in a computational independent and hence , for them , more usable way . the benefit of this approach is that the underlying pre-defined semantics of the structured english approach makes transformations into formal languages such as oasis legalruleml and owl2 ontologies possible . we exemplify our approach in the domain of patent law .", "topics": ["computation"]}
{"title": "structural controllability and observability in influence diagrams", "abstract": "influence diagram is a graphical representation of belief networks with uncertainty . this article studies the structural properties of a probabilistic model in an influence diagram . in particular , structural controllability theorems and structural observability theorems are developed and algorithms are formulated . controllability and observability are fundamental concepts in dynamic systems ( luenberger 1979 ) . controllability corresponds to the ability to control a system while observability analyzes the inferability of its variables . both properties can be determined by the ranks of the system matrices . structural controllability and observability , on the other hand , analyze the property of a system with its structure only , without the specific knowledge of the values of its elements ( tin 1974 , shields and pearson 1976 ) . the structural analysis explores the connection between the structure of a model and the functional dependence among its elements . it is useful in comprehending problem and formulating solution by challenging the underlying intuitions and detecting inconsistency in a model . this type of qualitative reasoning can sometimes provide insight even when there is insufficient numerical information in a model .", "topics": ["numerical analysis", "bayesian network"]}
{"title": "larger-context language modelling", "abstract": "in this work , we propose a novel method to incorporate corpus-level discourse information into language modelling . we call this larger-context language model . we introduce a late fusion approach to a recurrent language model based on long short-term memory units ( lstm ) , which helps the lstm unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other . through the evaluation on three corpora ( imdb , bbc , and penntree bank ) , we demon- strate that the proposed model improves perplexity significantly . in the experi- ments , we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the lstm . by analyzing the trained larger- context language model , we discover that content words , including nouns , adjec- tives and verbs , benefit most from an increasing number of context sentences . this analysis suggests that larger-context language model improves the unconditional language model by capturing the theme of a document better and more easily .", "topics": ["text corpus"]}
{"title": "automated planning in repeated adversarial games", "abstract": "game theory 's prescriptive power typically relies on full rationality and/or self-play interactions . in contrast , this work sets aside these fundamental premises and focuses instead on heterogeneous autonomous interactions between two or more agents . specifically , we introduce a new and concise representation for repeated adversarial ( constant-sum ) games that highlight the necessary features that enable an automated planing agent to reason about how to score above the game 's nash equilibrium , when facing heterogeneous adversaries . to this end , we present teamup , a model-based rl algorithm designed for learning and planning such an abstraction . in essence , it is somewhat similar to r-max with a cleverly engineered reward shaping that treats exploration as an adversarial optimization problem . in practice , it attempts to find an ally with which to tacitly collude ( in more than two-player games ) and then collaborates on a joint plan of actions that can consistently score a high utility in adversarial repeated games . we use the inaugural lemonade stand game tournament to demonstrate the effectiveness of our approach , and find that teamup is the best performing agent , demoting the tournament 's actual winning strategy into second place . in our experimental analysis , we show hat our strategy successfully and consistently builds collaborations with many different heterogeneous ( and sometimes very sophisticated ) adversaries .", "topics": ["optimization problem", "interaction"]}
{"title": "rebar : low-variance , unbiased gradient estimates for discrete latent variable models", "abstract": "learning in models with discrete latent variables is challenging due to high variance gradient estimators . generally , approaches have relied on control variates to reduce the variance of the reinforce estimator . recent work ( jang et al . 2016 , maddison et al . 2016 ) has taken a different approach , introducing a continuous relaxation of discrete variables to produce low-variance , but biased , gradient estimates . in this work , we combine the two approaches through a novel control variate that produces low-variance , \\emph { unbiased } gradient estimates . then , we introduce a modification to the continuous relaxation and show that the tightness of the relaxation can be adapted online , removing it as a hyperparameter . we show state-of-the-art variance reduction on several benchmark generative modeling tasks , generally leading to faster convergence to a better final log-likelihood .", "topics": ["gradient", "bayesian network"]}
{"title": "google 's neural machine translation system : bridging the gap between human and machine translation", "abstract": "neural machine translation ( nmt ) is an end-to-end learning approach for automated translation , with the potential to overcome many of the weaknesses of conventional phrase-based translation systems . unfortunately , nmt systems are known to be computationally expensive both in training and in translation inference . also , most nmt systems have difficulty with rare words . these issues have hindered nmt 's use in practical deployments and services , where both accuracy and speed are essential . in this work , we present gnmt , google 's neural machine translation system , which attempts to address many of these issues . our model consists of a deep lstm network with 8 encoder and 8 decoder layers using attention and residual connections . to improve parallelism and therefore decrease training time , our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder . to accelerate the final translation speed , we employ low-precision arithmetic during inference computations . to improve handling of rare words , we divide words into a limited set of common sub-word units ( `` wordpieces '' ) for both input and output . this method provides a good balance between the flexibility of `` character '' -delimited models and the efficiency of `` word '' -delimited models , naturally handles translation of rare words , and ultimately improves the overall accuracy of the system . our beam search technique employs a length-normalization procedure and uses a coverage penalty , which encourages generation of an output sentence that is most likely to cover all the words in the source sentence . on the wmt'14 english-to-french and english-to-german benchmarks , gnmt achieves competitive results to state-of-the-art . using a human side-by-side evaluation on a set of isolated simple sentences , it reduces translation errors by an average of 60 % compared to google 's phrase-based production system .", "topics": ["machine translation", "reinforcement learning"]}
{"title": "meta-learning for phonemic annotation of corpora", "abstract": "we apply rule induction , classifier combination and meta-learning ( stacked classifiers ) to the problem of bootstrapping high accuracy automatic annotation of corpora with pronunciation information . the task we address in this paper consists of generating phonemic representations reflecting the flemish and dutch pronunciations of a word on the basis of its orthographic representation ( which in turn is based on the actual speech recordings ) . we compare several possible approaches to achieve the text-to-pronunciation mapping task : memory-based learning , transformation-based learning , rule induction , maximum entropy modeling , combination of classifiers in stacked learning , and stacking of meta-learners . we are interested both in optimal accuracy and in obtaining insight into the linguistic regularities involved . as far as accuracy is concerned , an already high accuracy level ( 93 % for celex and 86 % for fonilex at word level ) for single classifiers is boosted significantly with additional error reductions of 31 % and 38 % respectively using combination of classifiers , and a further 5 % using combination of meta-learners , bringing overall word level accuracy to 96 % for the dutch variant and 92 % for the flemish variant . we also show that the application of machine learning methods indeed leads to increased insight into the linguistic regularities determining the variation between the two pronunciation variants studied .", "topics": ["text corpus"]}
{"title": "evolution and computational learning theory : a survey on valiant 's paper", "abstract": "darwin 's theory of evolution is considered to be one of the greatest scientific gems in modern science . it not only gives us a description of how living things evolve , but also shows how a population evolves through time and also , why only the fittest individuals continue the generation forward . the paper basically gives a high level analysis of the works of valiant [ 1 ] . though , we know the mechanisms of evolution , but it seems that there does not exist any strong quantitative and mathematical theory of the evolution of certain mechanisms . what is defined exactly as the fitness of an individual , why is that only certain individuals in a population tend to mutate , how computation is done in finite time when we have exponentially many examples : there seems to be a lot of questions which need to be answered . [ 1 ] basically treats darwinian theory as a form of computational learning theory , which calculates the net fitness of the hypotheses and thus distinguishes functions and their classes which could be evolvable using polynomial amount of resources . evolution is considered as a function of the environment and the previous evolutionary stages that chooses the best hypothesis using learning techniques that makes mutation possible and hence , gives a quantitative idea that why only the fittest individuals tend to survive and have the power to mutate .", "topics": ["computation", "polynomial"]}
{"title": "lfads - latent factor analysis via dynamical systems", "abstract": "neuroscience is experiencing a data revolution in which many hundreds or thousands of neurons are recorded simultaneously . currently , there is little consensus on how such data should be analyzed . here we introduce lfads ( latent factor analysis via dynamical systems ) , a method to infer latent dynamics from simultaneously recorded , single-trial , high-dimensional neural spiking data . lfads is a sequential model based on a variational auto-encoder . by making a dynamical systems hypothesis regarding the generation of the observed data , lfads reduces observed spiking to a set of low-dimensional temporal factors , per-trial initial conditions , and inferred inputs . we compare lfads to existing methods on synthetic data and show that it significantly out-performs them in inferring neural firing rates and latent dynamics .", "topics": ["calculus of variations", "synthetic data"]}
{"title": "accurate , large minibatch sgd : training imagenet in 1 hour", "abstract": "deep learning thrives with large neural networks and large datasets . however , larger networks and larger datasets result in longer training times that impede research and development progress . distributed synchronous sgd offers a potential solution to this problem by dividing sgd minibatches over a pool of parallel workers . yet to make this scheme efficient , the per-worker workload must be large , which implies nontrivial growth in the sgd minibatch size . in this paper , we empirically show that on the imagenet dataset large minibatches cause optimization difficulties , but when these are addressed the trained networks exhibit good generalization . specifically , we show no loss of accuracy when training with large minibatch sizes up to 8192 images . to achieve this result , we adopt a linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training . with these simple techniques , our caffe2-based system trains resnet-50 with a minibatch size of 8192 on 256 gpus in one hour , while matching small minibatch accuracy . using commodity hardware , our implementation achieves ~90 % scaling efficiency when moving from 8 to 256 gpus . this system enables us to train visual recognition models on internet-scale data with high efficiency .", "topics": ["computer vision"]}
{"title": "dis-s2v : discourse informed sen2vec", "abstract": "vector representation of sentences is important for many text processing tasks that involve clustering , classifying , or ranking sentences . recently , distributed representation of sentences learned by neural models from unlabeled data has been shown to outperform the traditional bag-of-words representation . however , most of these learning methods consider only the content of a sentence and disregard the relations among sentences in a discourse by and large . in this paper , we propose a series of novel models for learning latent representations of sentences ( sen2vec ) that consider the content of a sentence as well as inter-sentence relations . we first represent the inter-sentence relations with a language network and then use the network to induce contextual information into the content-based sen2vec models . two different approaches are introduced to exploit the information in the network . our first approach retrofits ( already trained ) sen2vec vectors with respect to the network in two different ways : ( 1 ) using the adjacency relations of a node , and ( 2 ) using a stochastic sampling method which is more flexible in sampling neighbors of a node . the second approach uses a regularizer to encode the information in the network into the existing sen2vec model . experimental results show that our proposed models outperform existing methods in three fundamental information system tasks demonstrating the effectiveness of our approach . the models leverage the computational power of multi-core cpus to achieve fine-grained computational efficiency . we make our code publicly available upon acceptance .", "topics": ["sampling ( signal processing )", "cluster analysis"]}
{"title": "learning unbiased features", "abstract": "a key element in transfer learning is representation learning ; if representations can be developed that expose the relevant factors underlying the data , then new tasks and domains can be learned readily based on mappings of these salient factors . we propose that an important aim for these representations are to be unbiased . different forms of representation learning can be derived from alternative definitions of unwanted bias , e.g . , bias to particular tasks , domains , or irrelevant underlying data dimensions . one very useful approach to estimating the amount of bias in a representation comes from maximum mean discrepancy ( mmd ) [ 5 ] , a measure of distance between probability distributions . we are not the first to suggest that mmd can be a useful criterion in developing representations that apply across multiple domains or tasks [ 1 ] . however , in this paper we describe a number of novel applications of this criterion that we have devised , all based on the idea of developing unbiased representations . these formulations include : a standard domain adaptation framework ; a method of learning invariant representations ; an approach based on noise-insensitive autoencoders ; and a novel form of generative model .", "topics": ["generative model", "feature learning"]}
{"title": "particle filter re-detection for visual tracking via correlation filters", "abstract": "most of the correlation filter based tracking algorithms can achieve good performance and maintain fast computational speed . however , in some complicated tracking scenes , there is a fatal defect that causes the object to be located inaccurately . in order to address this problem , we propose a particle filter redetection based tracking approach for accurate object localization . during the tracking process , the kernelized correlation filter ( kcf ) based tracker locates the object by relying on the maximum response value of the response map ; when the response map becomes ambiguous , the kcf tracking result becomes unreliable . our method can provide more candidates by particle resampling to detect the object accordingly . additionally , we give a new object scale evaluation mechanism , which merely considers the differences between the maximum response values in consecutive frames . extensive experiments on otb2013 and otb2015 datasets demonstrate that the proposed tracker performs favorably in relation to the state-of-the-art methods .", "topics": ["value ( ethics )"]}
{"title": "flex-convolution ( deep learning beyond grid-worlds )", "abstract": "the goal of this work is to enable deep neural networks to learn representations for irregular 3d structures -- just like in common approaches for 2d images . unfortunately , current network primitives such as convolution layers are specifically designed to exploit the natural data representation of images -- a fixed and regular grid structure . this represents a limitation when transferring these techniques to more unstructured data like 3d point clouds or higher dimensional data . in this work , we propose a surprisingly natural generalization flex-convolution of the conventional convolution layer and provide a highly efficient implementation . compared to very specific neural network architectures for point cloud processing , our more generic approach yields competitive results on the rather small standard benchmark sets using fewer parameters and lower memory consumption . our design even allows for raw neural networks prediction on several magnitudes larger point clouds , providing superior results compared to previous hand-tuned and well-engineered approaches on the 2d-3d-s dataset .", "topics": ["convolution"]}
{"title": "confidence-aware levenberg-marquardt optimization for joint motion estimation and super-resolution", "abstract": "motion estimation across low-resolution frames and the reconstruction of high-resolution images are two coupled subproblems of multi-frame super-resolution . this paper introduces a new joint optimization approach for motion estimation and image reconstruction to address this interdependence . our method is formulated via non-linear least squares optimization and combines two principles of robust super-resolution . first , to enhance the robustness of the joint estimation , we propose a confidence-aware energy minimization framework augmented with sparse regularization . second , we develop a tailor-made levenberg-marquardt iteration scheme to jointly estimate motion parameters and the high-resolution image along with the corresponding model confidence parameters . our experiments on simulated and real images confirm that the proposed approach outperforms decoupled motion estimation and image reconstruction as well as related state-of-the-art joint estimation algorithms .", "topics": ["mathematical optimization", "matrix regularization"]}
{"title": "a simple way to initialize recurrent networks of rectified linear units", "abstract": "learning long term dependencies in recurrent networks is difficult due to vanishing and exploding gradients . to overcome this difficulty , researchers have developed sophisticated optimization techniques and network architectures . in this paper , we propose a simpler solution that use recurrent neural networks composed of rectified linear units . key to our solution is the use of the identity matrix or its scaled version to initialize the recurrent weight matrix . we find that our solution is comparable to lstm on our four benchmarks : two toy problems involving long-range temporal structures , a large language modeling problem and a benchmark speech recognition problem .", "topics": ["recurrent neural network", "speech recognition"]}
{"title": "hardness results for agnostically learning low-degree polynomial threshold functions", "abstract": "hardness results for maximum agreement problems have close connections to hardness results for proper learning in computational learning theory . in this paper we prove two hardness results for the problem of finding a low degree polynomial threshold function ( ptf ) which has the maximum possible agreement with a given set of labeled examples in $ \\r^n \\times \\ { -1,1\\ } . $ we prove that for any constants $ d\\geq 1 , \\eps > 0 $ , { itemize } assuming the unique games conjecture , no polynomial-time algorithm can find a degree- $ d $ ptf that is consistent with a $ ( \\half + \\eps ) $ fraction of a given set of labeled examples in $ \\r^n \\times \\ { -1,1\\ } $ , even if there exists a degree- $ d $ ptf that is consistent with a $ 1-\\eps $ fraction of the examples . it is $ \\np $ -hard to find a degree-2 ptf that is consistent with a $ ( \\half + \\eps ) $ fraction of a given set of labeled examples in $ \\r^n \\times \\ { -1,1\\ } $ , even if there exists a halfspace ( degree-1 ptf ) that is consistent with a $ 1 - \\eps $ fraction of the examples . { itemize } these results immediately imply the following hardness of learning results : ( i ) assuming the unique games conjecture , there is no better-than-trivial proper learning algorithm that agnostically learns degree- $ d $ ptfs under arbitrary distributions ; ( ii ) there is no better-than-trivial learning algorithm that outputs degree-2 ptfs and agnostically learns halfspaces ( i.e . degree-1 ptfs ) under arbitrary distributions .", "topics": ["time complexity", "polynomial"]}
{"title": "competitive machine learning : best theoretical prediction vs optimization", "abstract": "machine learning is often used in competitive scenarios : participants learn and fit static models , and those models compete in a shared platform . the common assumption is that in order to win a competition one has to have the best predictive model , i.e . , the model with the smallest out-sample error . is that necessarily true ? does the best theoretical predictive model for a target always yield the best reward in a competition ? if not , can one take the best model and purposefully change it into a theoretically inferior model which in practice results in a higher competitive edge ? how does that modification look like ? and finally , if all participants modify their prediction models towards the best practical performance , who benefits the most ? players with inferior models , or those with theoretical superiority ? the main theme of this paper is to raise these important questions and propose a theoretical model to answer them . we consider a study case where two linear predictive models compete over a shared target . the model with the closest estimate gets the whole reward , which is equal to the absolute value of the target . we characterize the reward function of each model , and using a basic game theoretic approach , demonstrate that the inferior competitor can significantly improve his performance by choosing optimal model coefficients that are different from the best theoretical prediction . this is a preliminary study that emphasizes the fact that in many applications where predictive machine learning is at the service of competition , much can be gained from practical ( back-testing ) optimization of the model compared to static prediction improvement .", "topics": ["reinforcement learning", "coefficient"]}
{"title": "learning rank functionals : an empirical study", "abstract": "ranking is a key aspect of many applications , such as information retrieval , question answering , ad placement and recommender systems . learning to rank has the goal of estimating a ranking model automatically from training data . in practical settings , the task often reduces to estimating a rank functional of an object with respect to a query . in this paper , we investigate key issues in designing an effective learning to rank algorithm . these include data representation , the choice of rank functionals , the design of the loss function so that it is correlated with the rank metrics used in evaluation . for the loss function , we study three techniques : approximating the rank metric by a smooth function , decomposition of the loss into a weighted sum of element-wise losses and into a weighted sum of pairwise losses . we then present derivations of piecewise losses using the theory of high-order markov chains and markov random fields . in experiments , we evaluate these design aspects on two tasks : answer ranking in a social question answering site , and web information retrieval .", "topics": ["test set", "approximation algorithm"]}
{"title": "improving hypernymy detection with an integrated path-based and distributional method", "abstract": "detecting hypernymy relations is a key task in nlp , which is addressed in the literature using two complementary approaches . distributional methods , whose supervised variants are the current best performers , and path-based methods , which received less research attention . we suggest an improved path-based algorithm , in which the dependency paths are encoded using a recurrent neural network , that achieves results comparable to distributional methods . we then extend the approach to integrate both path-based and distributional signals , significantly improving upon the state-of-the-art on this task .", "topics": ["natural language processing", "recurrent neural network"]}
{"title": "qualitative and quantitative models of speech translation", "abstract": "this paper compares a qualitative reasoning model of translation with a quantitative statistical model . we consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design . the quantitative language and translation models are based on relations between lexical heads of phrases . statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance .", "topics": ["speech recognition"]}
{"title": "domain and language independent feature extraction for statistical text categorization", "abstract": "a generic system for text categorization is presented which uses a representative text corpus to adapt the processing steps : feature extraction , dimension reduction , and classification . feature extraction automatically learns features from the corpus by reducing actual word forms using statistical information of the corpus and general linguistic knowledge . the dimension of feature vector is then reduced by linear transformation keeping the essential information . the classification principle is a minimum least square approach based on polynomials . the described system can be readily adapted to new domains or new languages . in application , the system is reliable , fast , and processes completely automatically . it is shown that the text categorizer works successfully both on text generated by document image analysis - dia and on ground truth data .", "topics": ["feature extraction", "feature vector"]}
{"title": "alzheimer 's disease diagnostics by a deeply supervised adaptable 3d convolutional network", "abstract": "early diagnosis , playing an important role in preventing progress and treating the alzheimer 's disease ( ad ) , is based on classification of features extracted from brain images . the features have to accurately capture main ad-related variations of anatomical brain structures , such as , e.g . , ventricles size , hippocampus shape , cortical thickness , and brain volume . this paper proposes to predict the ad with a deep 3d convolutional neural network ( 3d-cnn ) , which can learn generic features capturing ad biomarkers and adapt to different domain datasets . the 3d-cnn is built upon a 3d convolutional autoencoder , which is pre-trained to capture anatomical shape variations in structural brain mri scans . fully connected upper layers of the 3d-cnn are then fine-tuned for each task-specific ad classification . experiments on the \\emph { adni } mri dataset with no skull-stripping preprocessing have shown our 3d-cnn outperforms several conventional classifiers by accuracy and robustness . abilities of the 3d-cnn to generalize the features learnt and adapt to other domains have been validated on the \\emph { caddementia } dataset .", "topics": ["autoencoder"]}
{"title": "on the effect of connectedness for biobjective multiple and long path problems", "abstract": "recently , the property of connectedness has been claimed to give a strong motivation on the design of local search techniques for multiobjective combinatorial optimization ( moco ) . indeed , when connectedness holds , a basic pareto local search , initialized with at least one non-dominated solution , allows to identify the efficient set exhaustively . however , this becomes quickly infeasible in practice as the number of efficient solutions typically grows exponentially with the instance size . as a consequence , we generally have to deal with a limited-size approximation , where a good sample set has to be found . in this paper , we propose the biobjective multiple and long path problems to show experimentally that , on the first problems , even if the efficient set is connected , a local search may be outperformed by a simple evolutionary algorithm in the sampling of the efficient set . at the opposite , on the second problems , a local search algorithm may successfully approximate a disconnected efficient set . then , we argue that connectedness is not the single property to study for the design of local search heuristics for moco . this work opens new discussions on a proper definition of the multiobjective fitness landscape .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "deep meta-learning : learning to learn in the concept space", "abstract": "few-shot learning remains challenging for meta-learning that learns a learning algorithm ( meta-learner ) from many related tasks . in this work , we argue that this is due to the lack of a good representation for meta-learning , and propose deep meta-learning to integrate the representation power of deep learning into meta-learning . the framework is composed of three modules , a concept generator , a meta-learner , and a concept discriminator , which are learned jointly . the concept generator , e.g . a deep residual net , extracts a representation for each instance that captures its high-level concept , on which the meta-learner performs few-shot learning , and the concept discriminator recognizes the concepts . by learning to learn in the concept space rather than in the complicated instance space , deep meta-learning can substantially improve vanilla meta-learning , which is demonstrated on various few-shot image recognition problems . for example , on 5-way-1-shot image recognition on cifar-100 and cub-200 , it improves matching nets from 50.53 % and 56.53 % to 58.18 % and 63.47 % , improves maml from 49.28 % and 50.45 % to 56.65 % and 64.63 % , and improves meta-sgd from 53.83 % and 53.34 % to 61.62 % and 66.95 % , respectively .", "topics": ["high- and low-level", "computer vision"]}
{"title": "auc-maximized deep convolutional neural fields for sequence labeling", "abstract": "deep convolutional neural networks ( dcnn ) has shown excellent performance in a variety of machine learning tasks . this manuscript presents deep convolutional neural fields ( deepcnf ) , a combination of dcnn with conditional random field ( crf ) , for sequence labeling with highly imbalanced label distribution . the widely-used training methods , such as maximum-likelihood and maximum labelwise accuracy , do not work well on highly imbalanced data . to handle this , we present a new training algorithm called maximum-auc for deepcnf . that is , we train deepcnf by directly maximizing the empirical area under the roc curve ( auc ) , which is an unbiased measurement for imbalanced data . to fulfill this , we formulate auc in a pairwise ranking framework , approximate it by a polynomial function and then apply a gradient-based procedure to optimize it . we then test our auc-maximized deepcnf on three very different protein sequence labeling tasks : solvent accessibility prediction , 8-state secondary structure prediction , and disorder prediction . our experimental results confirm that maximum-auc greatly outperforms the other two training methods on 8-state secondary structure prediction and disorder prediction since their label distributions are highly imbalanced and also have similar performance as the other two training methods on the solvent accessibility prediction problem which has three equally-distributed labels . furthermore , our experimental results also show that our auc-trained deepcnf models greatly outperform existing popular predictors of these three tasks .", "topics": ["approximation algorithm", "gradient"]}
{"title": "computing stable coalitions : approximation algorithms for reward sharing", "abstract": "consider a setting where selfish agents are to be assigned to coalitions or projects from a fixed set p. each project k is characterized by a valuation function ; v_k ( s ) is the value generated by a set s of agents working on project k. we study the following classic problem in this setting : `` how should the agents divide the value that they collectively create ? '' . one traditional approach in cooperative game theory is to study core stability with the implicit assumption that there are infinite copies of one project , and agents can partition themselves into any number of coalitions . in contrast , we consider a model with a finite number of non-identical projects ; this makes computing both high-welfare solutions and core payments highly non-trivial . the main contribution of this paper is a black-box mechanism that reduces the problem of computing a near-optimal core stable solution to the purely algorithmic problem of welfare maximization ; we apply this to compute an approximately core stable solution that extracts one-fourth of the optimal social welfare for the class of subadditive valuations . we also show much stronger results for several popular sub-classes : anonymous , fractionally subadditive , and submodular valuations , as well as provide new approximation algorithms for welfare maximization with anonymous functions . finally , we establish a connection between our setting and the well-studied simultaneous auctions with item bidding ; we adapt our results to compute approximate pure nash equilibria for these auctions .", "topics": ["approximation algorithm", "value ( ethics )"]}
{"title": "joint large-scale motion estimation and image reconstruction", "abstract": "this article describes the implementation of the joint motion estimation and image reconstruction framework presented by burger , dirks and sch\\ '' onlieb and extends this framework to large-scale motion between consecutive image frames . the variational framework uses displacements between consecutive frames based on the optical flow approach to improve the image reconstruction quality on the one hand and the motion estimation quality on the other . the energy functional consists of a data-fidelity term with a general operator that connects the input sequence to the solution , it has a total variation term for the image sequence and is connected to the underlying flow using an optical flow term . additional spatial regularity for the flow is modeled by a total variation regularizer for both components of the flow . the numerical minimization is performed in an alternating manner using primal-dual techniques . the resulting schemes are presented as pseudo-code together with a short numerical evaluation .", "topics": ["calculus of variations", "numerical analysis"]}
{"title": "multi-agent actor-critic for mixed cooperative-competitive environments", "abstract": "we explore deep reinforcement learning methods for multi-agent domains . we begin by analyzing the difficulty of traditional algorithms in the multi-agent case : q-learning is challenged by an inherent non-stationarity of the environment , while policy gradient suffers from a variance that increases as the number of agents grows . we then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination . additionally , we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies . we show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios , where agent populations are able to discover various physical and informational coordination strategies .", "topics": ["reinforcement learning", "gradient"]}
{"title": "improved regularization techniques for end-to-end speech recognition", "abstract": "regularization is important for end-to-end speech models , since the models are highly flexible and easy to overfit . data augmentation and dropout has been important for improving end-to-end models in other domains . however , they are relatively under explored for end-to-end speech models . therefore , we investigate the effectiveness of both methods for end-to-end trainable , deep speech recognition models . we augment audio data through random perturbations of tempo , pitch , volume , temporal alignment , and adding random noise.we further investigate the effect of dropout when applied to the inputs of all layers of the network . we show that the combination of data augmentation and dropout give a relative performance improvement on both wall street journal ( wsj ) and librispeech dataset of over 20 % . our model performance is also competitive with other end-to-end speech models on both datasets .", "topics": ["matrix regularization", "speech recognition"]}
{"title": "are elephants bigger than butterflies ? reasoning about sizes of objects", "abstract": "human vision greatly benefits from the information about sizes of objects . the role of size in several visual reasoning tasks has been thoroughly explored in human perception and cognition . however , the impact of the information about sizes of objects is yet to be determined in ai . we postulate that this is mainly attributed to the lack of a comprehensive repository of size information . in this paper , we introduce a method to automatically infer object sizes , leveraging visual and textual information from web . by maximizing the joint likelihood of textual and visual observations , our method learns reliable relative size estimates , with no explicit human supervision . we introduce the relative size dataset and show that our method outperforms competitive textual and visual baselines in reasoning about size comparisons .", "topics": ["baseline ( configuration management )", "artificial intelligence"]}
{"title": "submodularization for quadratic pseudo-boolean optimization", "abstract": "many computer vision problems require optimization of binary non-submodular energies . we propose a general optimization framework based on local submodular approximations ( lsa ) . unlike standard lp relaxation methods that linearize the whole energy globally , our approach iteratively approximates the energies locally . on the other hand , unlike standard local optimization methods ( e.g . gradient descent or projection techniques ) we use non-linear submodular approximations and optimize them without leaving the domain of integer solutions . we discuss two specific lsa algorithms based on `` trust region '' and `` auxiliary function '' principles , lsa-tr and lsa-aux . these methods obtain state-of-the-art results on a wide range of applications outperforming many standard techniques such as lbp , qpbo , and trws . while our paper is focused on pairwise energies , our ideas extend to higher-order problems . the code is available online ( http : //vision.csd.uwo.ca/code/ ) .", "topics": ["nonlinear system", "computer vision"]}
{"title": "on self-adaptive mutation restarts for evolutionary robotics with real rotorcraft", "abstract": "self-adaptive parameters are increasingly used in the field of evolutionary robotics , as they allow key evolutionary rates to vary autonomously in a context-sensitive manner throughout the optimisation process . a significant limitation to self-adaptive mutation is that rates can be set unfavourably , which hinders convergence . rate restarts are typically employed to remedy this , but thus far have only been applied in evolutionary robotics for mutation-only algorithms . this paper focuses on the level at which evolutionary rate restarts are applied in population-based algorithms with more than 1 evolutionary operator . after testing on a real hexacopter hovering task , we conclude that individual-level restarting results in higher fitness solutions without fitness stagnation , and population restarts provide a more stable rate evolution . without restarts , experiments can become stuck in suboptimal controller/rate combinations which can be difficult to escape from .", "topics": ["mathematical optimization"]}
{"title": "identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "abstract": "a central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous , high dimensional spaces . gradient descent or quasi-newton methods are almost ubiquitously used to perform such minimizations , and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum . here we argue , based on results from statistical physics , random matrix theory , neural network theory , and empirical evidence , that a deeper and more profound difficulty originates from the proliferation of saddle points , not local minima , especially in high dimensional problems of practical interest . such saddle points are surrounded by high error plateaus that can dramatically slow down learning , and give the illusory impression of the existence of a local minimum . motivated by these arguments , we propose a new approach to second-order optimization , the saddle-free newton method , that can rapidly escape high dimensional saddle points , unlike gradient descent and quasi-newton methods . we apply this algorithm to deep or recurrent neural network training , and provide numerical evidence for its superior optimization performance .", "topics": ["numerical analysis", "recurrent neural network"]}
{"title": "multi-task learning of deep neural networks for audio visual automatic speech recognition", "abstract": "multi-task learning ( mtl ) involves the simultaneous training of two or more related tasks over shared representations . in this work , we apply mtl to audio-visual automatic speech recognition ( av-asr ) . our primary task is to learn a mapping between audio-visual fused features and frame labels obtained from acoustic gmm/hmm model . this is combined with an auxiliary task which maps visual features to frame labels obtained from a separate visual gmm/hmm model . the mtl model is tested at various levels of babble noise and the results are compared with a base-line hybrid dnn-hmm av-asr model . our results indicate that mtl is especially useful at higher level of noise . compared to base-line , upto 7\\ % relative improvement in wer is reported at -3 snr db", "topics": ["speech recognition", "map"]}
{"title": "learning is planning : near bayes-optimal reinforcement learning via monte-carlo tree search", "abstract": "bayes-optimal behavior , while well-defined , is often difficult to achieve . recent advances in the use of monte-carlo tree search ( mcts ) have shown that it is possible to act near-optimally in markov decision processes ( mdps ) with very large or infinite state spaces . bayes-optimal behavior in an unknown mdp is equivalent to optimal behavior in the known belief-space mdp , although the size of this belief-space mdp grows exponentially with the amount of history retained , and is potentially infinite . we show how an agent can use one particular mcts algorithm , forward search sparse sampling ( fsss ) , in an efficient way to act nearly bayes-optimally for all but a polynomial number of steps , assuming that fsss can be used to act efficiently in any possible underlying mdp .", "topics": ["mathematical optimization", "reinforcement learning"]}
{"title": "ergo : a graphical environment for constructing bayesian", "abstract": "we describe an environment that considerably simplifies the process of generating bayesian belief networks . the system has been implemented on readily available , inexpensive hardware , and provides clarity and high performance . we present an introduction to bayesian belief networks , discuss algorithms for inference with these networks , and delineate the classes of problems that can be solved with this paradigm . we then describe the hardware and software that constitute the system , and illustrate ergo 's use with several example", "topics": ["bayesian network"]}
{"title": "unsupervised learning human 's activities by overexpressed recognized non-speech sounds", "abstract": "human activity and environment produces sounds such as , at home , the noise produced by water , cough , or television . these sounds can be used to determine the activity in the environment . the objective is to monitor a person 's activity or determine his environment using a single low cost microphone by sound analysis . the purpose is to adapt programs to the activity or environment or detect abnormal situations . some patterns of over expressed repeatedly in the sequences of recognized sounds inter and intra environment allow to characterize activities such as the entrance of a person in the house , or a tv program watched . we first manually annotated 1500 sounds of daily life activity of old persons living at home recognized sounds . then we inferred an ontology and enriched the database of annotation with a crowed sourced manual annotation of 7500 sounds to help with the annotation of the most frequent sounds . using learning sound algorithms , we defined 50 types of the most frequent sounds . we used this set of recognizable sounds as a base to tag sounds and put tags on them . by using over expressed number of motifs of sequences of the tags , we were able to categorize using only a single low-cost microphone , complex activities of daily life of a persona at home as watching tv , entrance in the apartment of a person , or phone conversation including detecting unknown activities as repeated tasks performed by users .", "topics": ["unsupervised learning"]}
{"title": "exact gradient updates in time independent of output size for the spherical loss family", "abstract": "an important class of problems involves training deep neural networks with sparse prediction targets of very high dimension d. these occur naturally in e.g . neural language models or the learning of word-embeddings , often posed as predicting the probability of next words among a vocabulary of size d ( e.g . 200,000 ) . computing the equally large , but typically non-sparse d-dimensional output vector from a last hidden layer of reasonable dimension d ( e.g . 500 ) incurs a prohibitive o ( dd ) computational cost for each example , as does updating the $ d \\times d $ output weight matrix and computing the gradient needed for backpropagation to previous layers . while efficient handling of large sparse network inputs is trivial , the case of large sparse targets is not , and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training . in this work we develop an original algorithmic approach which , for a family of loss functions that includes squared error and spherical softmax , can compute the exact loss , gradient update for the output weights , and gradient for backpropagation , all in $ o ( d^ { 2 } ) $ per example instead of $ o ( dd ) $ , remarkably without ever computing the d-dimensional output . the proposed algorithm yields a speedup of up to $ d/4d $ i.e . two orders of magnitude for typical sizes , for that critical part of the computations that often dominates the training time in this kind of network architecture .", "topics": ["approximation algorithm", "loss function"]}
{"title": "learning to reason : end-to-end module networks for visual question answering", "abstract": "natural language questions are inherently compositional , and many are most easily answered by reasoning about their decomposition into modular sub-problems . for example , to answer `` is there an equal number of balls and boxes ? '' we can look for balls , look for boxes , count them , and compare the results . the recently proposed neural module network ( nmn ) architecture implements this approach to question answering by parsing questions into linguistic substructures and assembling question-specific deep networks from smaller modules that each solve one subtask . however , existing nmn implementations rely on brittle off-the-shelf parsers , and are restricted to the module configurations proposed by these parsers rather than learning them from data . in this paper , we propose end-to-end module networks ( n2nmns ) , which learn to reason by directly predicting instance-specific network layouts without the aid of a parser . our model learns to generate network structures ( by imitating expert demonstrations ) while simultaneously learning network parameters ( using the downstream task loss ) . experimental results on the new clevr dataset targeted at compositional question answering show that n2nmns achieve an error reduction of nearly 50 % relative to state-of-the-art attentional approaches , while discovering interpretable network architectures specialized for each question .", "topics": ["natural language", "parsing"]}
{"title": "adopting level set theory based algorithms to segment human ear", "abstract": "human identification has always been a topic that interested researchers around the world . biometric methods are found to be more effective and much easier for the users than the traditional identification methods like keys , smart cards and passwords . unlike with the traditional methods , with biometric methods the data acquisition is most of the times passive , which means the users do not take active part in data acquisition . data acquisition can be performed using cameras , scanners or sensors . human physiological biometrics such as face , eye and ear are good candidates for uniquely identifying an individual . however , human ear scores over face and eye because of certain advantages it has over face . the most challenging phase in human identification based on ear biometric is the segmentation of the ear image from the captured image which may contain many unwanted details . in this work , pde based image processing techniques are used to segment out the ear image . level set theory based image processing is employed to obtain the contour of the ear image . a few level set algorithms are compared for their efficiency in segmenting test ear images .", "topics": ["image processing", "sensor"]}
{"title": "support vector machine and its bias correction in high-dimension , low-sample-size settings", "abstract": "in this paper , we consider asymptotic properties of the support vector machine ( svm ) in high-dimension , low-sample-size ( hdlss ) settings . we show that the hard-margin linear svm holds a consistency property in which misclassification rates tend to zero as the dimension goes to infinity under certain severe conditions . we show that the svm is very biased in hdlss settings and its performance is affected by the bias directly . in order to overcome such difficulties , we propose a bias-corrected svm ( bc-svm ) . we show that the bc-svm gives preferable performances in hdlss settings . we also discuss the svms in multiclass hdlss settings . finally , we check the performance of the classifiers in actual data analyses .", "topics": ["support vector machine"]}
{"title": "multi-pretrained deep neural network", "abstract": "pretraining is widely used in deep neutral network and one of the most famous pretraining models is deep belief network ( dbn ) . the optimization formulas are different during the pretraining process for different pretraining models . in this paper , we pretrained deep neutral network by different pretraining models and hence investigated the difference between dbn and stacked denoising autoencoder ( sda ) when used as pretraining model . the experimental results show that dbn get a better initial model . however the model converges to a relatively worse model after the finetuning process . yet after pretrained by sda for the second time the model converges to a better model if finetuned .", "topics": ["noise reduction", "autoencoder"]}
{"title": "structure in the value function of two-player zero-sum games of incomplete information", "abstract": "zero-sum stochastic games provide a rich model for competitive decision making . however , under general forms of state uncertainty as considered in the partially observable stochastic game ( posg ) , such decision making problems are still not very well understood . this paper makes a contribution to the theory of zero-sum posgs by characterizing structure in their value function . in particular , we introduce a new formulation of the value function for zs-posgs as a function of the `` plan-time sufficient statistics '' ( roughly speaking the information distribution in the posg ) , which has the potential to enable generalization over such information distributions . we further delineate this generalization capability by proving a structural result on the shape of value function : it exhibits concavity and convexity with respect to appropriately chosen marginals of the statistic space . this result is a key pre-cursor for developing solution methods that may be able to exploit such structure . finally , we show how these results allow us to reduce a zs-posg to a `` centralized '' model with shared observations , thereby transferring results for the latter , narrower class , to games with individual ( private ) observations .", "topics": ["scalability"]}
{"title": "equivalence between policy gradients and soft q-learning", "abstract": "two of the leading approaches for model-free reinforcement learning are policy gradient methods and $ q $ -learning methods . $ q $ -learning methods can be effective and sample-efficient when they work , however , it is not well-understood why they work , since empirically , the $ q $ -values they estimate are very inaccurate . a partial explanation may be that $ q $ -learning methods are secretly implementing policy gradient updates : we show that there is a precise equivalence between $ q $ -learning and policy gradient methods in the setting of entropy-regularized reinforcement learning , that `` soft '' ( entropy-regularized ) $ q $ -learning is exactly equivalent to a policy gradient method . we also point out a connection between $ q $ -learning methods and natural policy gradient methods . experimentally , we explore the entropy-regularized versions of $ q $ -learning and policy gradients , and we find them to perform as well as ( or slightly better than ) the standard variants on the atari benchmark . we also show that the equivalence holds in practical settings by constructing a $ q $ -learning method that closely matches the learning dynamics of a3c without using a target network or $ \\epsilon $ -greedy exploration schedule .", "topics": ["value ( ethics )", "reinforcement learning"]}
{"title": "automated fabric defect inspection : a survey of classifiers", "abstract": "quality control at each stage of production in textile industry has become a key factor to retaining the existence in the highly competitive global market . problems of manual fabric defect inspection are lack of accuracy and high time consumption , where early and accurate fabric defect detection is a significant phase of quality control . computer vision based , i.e . automated fabric defect inspection systems are thought by many researchers of different countries to be very useful to resolve these problems . there are two major challenges to be resolved to attain a successful automated fabric defect inspection system . they are defect detection and defect classification . in this work , we discuss different techniques used for automated fabric defect classification , then show a survey of classifiers used in automated fabric defect inspection systems , and finally , compare these classifiers by using performance metrics . this work is expected to be very useful for the researchers in the area of automated fabric defect inspection to understand and evaluate the many potential options in this field .", "topics": ["computer vision"]}
{"title": "feature selection using fisher 's ratio technique for automatic speech recognition", "abstract": "automatic speech recognition involves mainly two steps ; feature extraction and classification . mel frequency cepstral coefficient is used as one of the prominent feature extraction techniques in asr . usually , the set of all 12 mfcc coefficients is used as the feature vector in the classification step . but the question is whether the same or improved classification accuracy can be achieved by using a subset of 12 mfcc as feature vector . in this paper , fisher 's ratio technique is used for selecting a subset of 12 mfcc coefficients that contribute more in discriminating a pattern . the selected coefficients are used in classification with hidden markov model algorithm . the classification accuracies that we get by using 12 coefficients and by using the selected coefficients are compared .", "topics": ["feature vector", "feature extraction"]}
{"title": "centrality-constrained graph embedding", "abstract": "visual rendering of graphs is a key task in the mapping of complex network data . although most graph drawing algorithms emphasize aesthetic appeal , certain applications such as travel-time maps place more importance on visualization of structural network properties . the present paper advocates a graph embedding approach with centrality considerations to comply with node hierarchy . the problem is formulated as one of constrained multi-dimensional scaling ( mds ) , and it is solved via block coordinate descent iterations with successive approximations and guaranteed convergence to a kkt point . in addition , a regularization term enforcing graph smoothness is incorporated with the goal of reducing edge crossings . experimental results demonstrate that the algorithm converges , and can be used to efficiently embed large graphs on the order of thousands of nodes .", "topics": ["matrix regularization", "iteration"]}
{"title": "learning with average top-k loss", "abstract": "in this work , we introduce the { \\em average top- $ k $ } ( \\atk ) loss as a new aggregate loss for supervised learning , which is the average over the $ k $ largest individual losses over a training dataset . we show that the \\atk loss is a natural generalization of the two widely used aggregate losses , namely the average loss and the maximum loss , but can combine their advantages and mitigate their drawbacks to better adapt to different data distributions . furthermore , it remains a convex function over all individual losses , which can lead to convex optimization problems that can be solved effectively with conventional gradient-based methods . we provide an intuitive interpretation of the \\atk loss based on its equivalent effect on the continuous individual loss functions , suggesting that it can reduce the penalty on correctly classified data . we further give a learning theory analysis of \\matk learning on the classification calibration of the \\atk loss and the error bounds of \\atk-svm . we demonstrate the applicability of minimum average top- $ k $ learning for binary classification and regression using synthetic and real datasets .", "topics": ["supervised learning", "loss function"]}
{"title": "data-efficient reinforcement learning in continuous-state pomdps", "abstract": "we present a data-efficient reinforcement learning algorithm resistant to observation noise . our method extends the highly data-efficient pilco algorithm ( deisenroth & rasmussen , 2011 ) into partially observed markov decision processes ( pomdps ) by considering the filtering process during policy evaluation . pilco conducts policy search , evaluating each policy by first predicting an analytic distribution of possible system trajectories . we additionally predict trajectories w.r.t . a filtering process , achieving significantly higher performance than combining a filter with a policy optimised by the original ( unfiltered ) framework . our test setup is the cartpole swing-up task with sensor noise , which involves nonlinear dynamics and requires nonlinear control .", "topics": ["nonlinear system", "reinforcement learning"]}
{"title": "differential recurrent neural networks for action recognition", "abstract": "the long short-term memory ( lstm ) neural network is capable of processing complex sequential information since it utilizes special gating schemes for learning representations from long input sequences . it has the potential to model any sequential time-series data , where the current hidden state has to be considered in the context of the past hidden states . this property makes lstm an ideal choice to learn the complex dynamics of various actions . unfortunately , the conventional lstms do not consider the impact of spatio-temporal dynamics corresponding to the given salient motion patterns , when they gate the information that ought to be memorized through time . to address this problem , we propose a differential gating scheme for the lstm neural network , which emphasizes on the change in information gain caused by the salient motions between the successive frames . this change in information gain is quantified by derivative of states ( dos ) , and thus the proposed lstm model is termed as differential recurrent neural network ( drnn ) . we demonstrate the effectiveness of the proposed model by automatically recognizing actions from the real-world 2d and 3d human action datasets . our study is one of the first works towards demonstrating the potential of learning complex time-series representations via high-order derivatives of states .", "topics": ["recurrent neural network", "time series"]}
{"title": "bayesian optimal active search and surveying", "abstract": "we consider two active binary-classification problems with atypical objectives . in the first , active search , our goal is to actively uncover as many members of a given class as possible . in the second , active surveying , our goal is to actively query points to ultimately predict the proportion of a given class . numerous real-world problems can be framed in these terms , and in either case typical model-based concerns such as generalization error are only of secondary importance . we approach these problems via bayesian decision theory ; after choosing natural utility functions , we derive the optimal policies . we provide three contributions . in addition to introducing the active surveying problem , we extend previous work on active search in two ways . first , we prove a novel theoretical result , that less-myopic approximations to the optimal policy can outperform more-myopic approximations by any arbitrary degree . we then derive bounds that for certain models allow us to reduce ( in practice dramatically ) the exponential search space required by a naive implementation of the optimal policy , enabling further lookahead while still ensuring that optimal decisions are always made .", "topics": ["approximation", "parsing"]}
{"title": "graphical condition for identification in recursive sem", "abstract": "the paper concerns the problem of predicting the effect of actions or interventions on a system from a combination of ( i ) statistical data on a set of observed variables , and ( ii ) qualitative causal knowledge encoded in the form of a directed acyclic graph ( dag ) . the dag represents a set of linear equations called structural equations model ( sem ) , whose coefficients are parameters representing direct causal effects . reliable quantitative conclusions can only be obtained from the model if the causal effects are uniquely determined by the data . that is , if there exists a unique parametrization for the model that makes it compatible with the data . if this is the case , the model is called identified . the main result of the paper is a general sufficient condition for identification of recursive sem models .", "topics": ["causality", "coefficient"]}
{"title": "active dictionary learning in sparse representation based classification", "abstract": "sparse representation , which uses dictionary atoms to reconstruct input vectors , has been studied intensively in recent years . a proper dictionary is a key for the success of sparse representation . in this paper , an active dictionary learning ( adl ) method is introduced , in which classification error and reconstruction error are considered as the active learning criteria in selection of the atoms for dictionary construction . the learned dictionaries are caculated in sparse representation based classification ( src ) . the classification accuracy and reconstruction error are used to evaluate the proposed dictionary learning method . the performance of the proposed dictionary learning method is compared with other methods , including unsupervised dictionary learning and whole-training-data dictionary . the experimental results based on the uci data sets and face data set demonstrate the efficiency of the proposed method .", "topics": ["data mining", "sparse matrix"]}
{"title": "joint detection and tracking for multipath targets : a variational bayesian approach", "abstract": "different from traditional point target tracking systems assuming that a target generates at most one single measurement per scan , there exists a class of multipath target tracking systems where each measurement may originate from the interested target via one of multiple propagation paths or from clutter , while the correspondence among targets , measurements , and propagation paths is unknown . the performance of multipath target tracking systems can be improved if multiple measurements from the same target are effectively utilized , but suffers from two major challenges . the first is multipath detection that detects appearing and disappearing targets automatically , while one target may produce $ s $ tracks for $ s $ propagation paths . the second is multipath tracking that calculates the target-to-measurement-to-path assignment matrices to estimate target states , which is computationally intractable due to the combinatorial explosion . based on variational bayesian framework , this paper introduces a novel probabilistic joint detection and tracking algorithm ( jdt-vb ) that incorporates data association , path association , state estimation and automatic track management . the posterior probabilities of these latent variables are derived in a closed-form iterative manner , which is effective for dealing with the coupling issue of multipath data association identification risk and state estimation error . loopy belief propagation ( lbp ) is exploited to approximate the multipath data association , which significantly reduces the computational cost . the proposed jdt-vb algorithm can simultaneously deal with the track initiation , maintenance , and termination for multiple multipath target tracking with time-varying number of targets , and its performance is verified by a numerical simulation of over-the-horizon radar .", "topics": ["calculus of variations", "approximation algorithm"]}
{"title": "local minimax complexity of stochastic convex optimization", "abstract": "we extend the traditional worst-case , minimax analysis of stochastic convex optimization by introducing a localized form of minimax complexity for individual functions . our main result gives function-specific lower and upper bounds on the number of stochastic subgradient evaluations needed to optimize either the function or its `` hardest local alternative '' to a given numerical precision . the bounds are expressed in terms of a localized and computational analogue of the modulus of continuity that is central to statistical minimax analysis . we show how the computational modulus of continuity can be explicitly calculated in concrete cases , and relates to the curvature of the function at the optimum . we also prove a superefficiency result that demonstrates it is a meaningful benchmark , acting as a computational analogue of the fisher information in statistical estimation . the nature and practical implications of the results are demonstrated in simulations .", "topics": ["mathematical optimization", "numerical analysis"]}
{"title": "challenging language-dependent segmentation for arabic : an application to machine translation and part-of-speech tagging", "abstract": "word segmentation plays a pivotal role in improving any arabic nlp application . therefore , a lot of research has been spent in improving its accuracy . off-the-shelf tools , however , are : i ) complicated to use and ii ) domain/dialect dependent . we explore three language-independent alternatives to morphological segmentation using : i ) data-driven sub-word units , ii ) characters as a unit of learning , and iii ) word embeddings learned using a character cnn ( convolution neural network ) . on the tasks of machine translation and pos tagging , we found these methods to achieve close to , and occasionally surpass state-of-the-art performance . in our analysis , we show that a neural machine translation system is sensitive to the ratio of source and target tokens , and a ratio close to 1 or greater , gives optimal performance .", "topics": ["natural language processing", "machine translation"]}
{"title": "robust optimization for deep regression", "abstract": "convolutional neural networks ( convnets ) have successfully contributed to improve the accuracy of regression-based methods for computer vision tasks such as human pose estimation , landmark localization , and object detection . the network optimization has been usually performed with l2 loss and without considering the impact of outliers on the training process , where an outlier in this context is defined by a sample estimation that lies at an abnormal distance from the other training sample estimations in the objective space . in this work , we propose a regression model with convnets that achieves robustness to such outliers by minimizing tukey 's biweight function , an m-estimator robust to outliers , as the loss function for the convnet . in addition to the robust loss , we introduce a coarse-to-fine model , which processes input images of progressively higher resolutions for improving the accuracy of the regressed values . in our experiments , we demonstrate faster convergence and better generalization of our robust loss function for the tasks of human pose estimation and age estimation from face images . we also show that the combination of the robust loss function with the coarse-to-fine model produces comparable or better results than current state-of-the-art approaches in four publicly available human pose estimation datasets .", "topics": ["object detection", "loss function"]}
{"title": "neural semantic parsing over multiple knowledge-bases", "abstract": "a fundamental challenge in developing semantic parsers is the paucity of strong supervision in the form of language utterances annotated with logical form . in this paper , we propose to exploit structural regularities in language in different domains , and train semantic parsers over multiple knowledge-bases ( kbs ) , while sharing information across datasets . we find that we can substantially improve parsing accuracy by training a single sequence-to-sequence model over multiple kbs , when providing an encoding of the domain at decoding time . our model achieves state-of-the-art performance on the overnight dataset ( containing eight domains ) , improves performance over a single kb baseline from 75.6 % to 79.6 % , while obtaining a 7x reduction in the number of model parameters .", "topics": ["baseline ( configuration management )", "parsing"]}
{"title": "a primer on the signature method in machine learning", "abstract": "in these notes , we wish to provide an introduction to the signature method , focusing on its basic theoretical properties and recent numerical applications . the notes are split into two parts . the first part focuses on the definition and fundamental properties of the signature of a path , or the path signature . we have aimed for a minimalistic approach , assuming only familiarity with classical real analysis and integration theory , and supplementing theory with straightforward examples . we have chosen to focus in detail on the principle properties of the signature which we believe are fundamental to understanding its role in applications . we also present an informal discussion on some of its deeper properties and briefly mention the role of the signature in rough paths theory , which we hope could serve as a light introduction to rough paths for the interested reader . the second part of these notes discusses practical applications of the path signature to the area of machine learning . the signature approach represents a non-parametric way for extraction of characteristic features from data . the data are converted into a multi-dimensional path by means of various embedding algorithms and then processed for computation of individual terms of the signature which summarise certain information contained in the data . the signature thus transforms raw data into a set of features which are used in machine learning tasks . we will review current progress in applications of signatures to machine learning problems .", "topics": ["numerical analysis", "computation"]}
{"title": "hybrid optimized back propagation learning algorithm for multi-layer perceptron", "abstract": "standard neural network based on general back propagation learning using delta method or gradient descent method has some great faults like poor optimization of error-weight objective function , low learning rate , instability .this paper introduces a hybrid supervised back propagation learning algorithm which uses trust-region method of unconstrained optimization of the error objective function by using quasi-newton method .this optimization leads to more accurate weight update system for minimizing the learning error during learning phase of multi-layer perceptron . [ 13 ] [ 14 ] [ 15 ] in this paper augmented line search is used for finding points which satisfies wolfe condition . in this paper , this hybrid back propagation algorithm has strong global convergence properties & is robust & efficient in practice .", "topics": ["optimization problem", "gradient descent"]}
{"title": "generating images with perceptual similarity metrics based on deep networks", "abstract": "image-generating machine learning models are typically trained with loss functions based on distance in the image space . this often leads to over-smoothed results . we propose a class of loss functions , which we call deep perceptual similarity metrics ( deepsim ) , that mitigate this problem . instead of computing distances in the image space , we compute distances between image features extracted by deep neural networks . this metric better reflects perceptually similarity of images and thus leads to better results . we show three applications : autoencoder training , a modification of a variational autoencoder , and inversion of deep convolutional networks . in all cases , the generated images look sharp and resemble natural images .", "topics": ["calculus of variations", "loss function"]}
{"title": "deep learning analysis of breast mris for prediction of occult invasive disease in ductal carcinoma in situ", "abstract": "purpose : to determine whether deep learning-based algorithms applied to breast mr images can aid in the prediction of occult invasive disease following the di- agnosis of ductal carcinoma in situ ( dcis ) by core needle biopsy . material and methods : in this institutional review board-approved study , we analyzed dynamic contrast-enhanced fat-saturated t1-weighted mri sequences of 131 patients at our institution with a core needle biopsy-confirmed diagnosis of dcis . the patients had no preoperative therapy before breast mri and no prior history of breast cancer . we explored two different deep learning approaches to predict whether there was a hidden ( occult ) invasive component in the analyzed tumors that was ultimately detected at surgical excision . in the first approach , we adopted the transfer learning strategy , in which a network pre-trained on a large dataset of natural images is fine-tuned with our dcis images . specifically , we used the googlenet model pre-trained on the imagenet dataset . in the second approach , we used a pre-trained network to extract deep features , and a support vector machine ( svm ) that utilizes these features to predict the upstaging of the dcis . we used 10-fold cross validation and the area under the roc curve ( auc ) to estimate the performance of the predictive models . results : the best classification performance was obtained using the deep features approach with googlenet model pre-trained on imagenet as the feature extractor and a polynomial kernel svm used as the classifier ( auc = 0.70 , 95 % ci : 0.58- 0.79 ) . for the transfer learning based approach , the highest auc obtained was 0.53 ( 95 % ci : 0.41-0.62 ) . conclusion : convolutional neural networks could potentially be used to identify occult invasive disease in patients diagnosed with dcis at the initial core needle biopsy .", "topics": ["support vector machine"]}
{"title": "building subject-aligned comparable corpora and mining it for truly parallel sentence pairs", "abstract": "parallel sentences are a relatively scarce but extremely useful resource for many applications including cross-lingual retrieval and statistical machine translation . this research explores our methodology for mining such data from previously obtained comparable corpora . the task is highly practical since non-parallel multilingual data exist in far greater quantities than parallel corpora , but parallel sentences are a much more useful resource . here we propose a web crawling method for building subject-aligned comparable corpora from wikipedia articles . we also introduce a method for extracting truly parallel sentences that are filtered out from noisy or just comparable sentence pairs . we describe our implementation of a specialized tool for this task as well as training and adaption of a machine translation system that supplies our filter with additional information about the similarity of comparable sentence pairs .", "topics": ["machine translation", "text corpus"]}
{"title": "applying advanced spaceborne thermal emission and reflection radiometer ( aster ) spectral indices for geological mapping and mineral identification on the tibetan plateau", "abstract": "the tibetan plateau holds clues to understanding the dynamics and mechanisms associated with continental growth . part of the region is characterized by zones of ophiolitic melange believed to represent the remnants of ancient oceanic crust and underlying upper mantle emplaced during oceanic closures . however , due to the remoteness of the region and the inhospitable terrain many areas have not received detailed investigation . increased spatial and spectral resolution of satellite sensors have made it possible to map in greater detail the mineralogy and lithology than in the past . recent work by yoshiki ninomiya of the geological survey of japan has pioneered the use of several spectral indices for the mapping of quartzose , carbonate , and silicate rocks using advanced spaceborne thermal emission and reflection radiometer ( aster ) thermal infrared ( tir ) data . in this study , aster tir indices have been applied to a region in western-central tibet for the purposes of assessing their effectiveness for differentiating ophiolites and other lithologies . the results agree well with existing geological maps and other published data . the study area was chosen due to its diverse range of rock types , including an ophiolitic melange , associated with the bangong-nujiang suture ( bns ) that crops out on the northern shores of lagkor tso and dong tso ( `` tso '' is tibetan for lake ) . the techniques highlighted in this paper could be applied to other geographical regions where similar geological questions need to be resolved . the results of this study aim to show the utility of aster tir imagery for geological mapping in semi-arid and sparsely vegetated areas on the tibetan plateau .", "topics": ["map", "sensor"]}
{"title": "regularization strategies and empirical bayesian learning for mkl", "abstract": "multiple kernel learning ( mkl ) , structured sparsity , and multi-task learning have recently received considerable attention . in this paper , we show how different mkl algorithms can be understood as applications of either regularization on the kernel weights or block-norm-based regularization , which is more common in structured sparsity and multi-task learning . we show that these two regularization strategies can be systematically mapped to each other through a concave conjugate operation . when the kernel-weight-based regularizer is separable into components , we can naturally consider a generative probabilistic model behind mkl . based on this model , we propose learning algorithms for the kernel weights through the maximization of marginal likelihood . we show through numerical experiments that $ \\ell_2 $ -norm mkl and elastic-net mkl achieve comparable accuracy to uniform kernel combination . although uniform kernel combination might be preferable from its simplicity , $ \\ell_2 $ -norm mkl and elastic-net mkl can learn the usefulness of the information sources represented as kernels . in particular , elastic-net mkl achieves sparsity in the kernel weights .", "topics": ["kernel ( operating system )", "matrix regularization"]}
{"title": "a lexical semantic database for verbmobil", "abstract": "this paper describes the development and use of a lexical semantic database for the verbmobil speech-to-speech machine translation system . the motivation is to provide a common information source for the distributed development of the semantics , transfer and semantic evaluation modules and to store lexical semantic information application-independently . the database is organized around a set of abstract semantic classes and has been used to define the semantic contributions of the lemmata in the vocabulary of the system , to automatically create semantic lexica and to check the correctness of the semantic representations built up . the semantic classes are modelled using an inheritance hierarchy . the database is implemented using the lexicon formalism lex4 developed during the project .", "topics": ["machine translation"]}
{"title": "feature uncertainty bounding schemes for large robust nonlinear svm classifiers", "abstract": "we consider the binary classification problem when data are large and subject to unknown but bounded uncertainties . we address the problem by formulating the nonlinear support vector machine training problem with robust optimization . to do so , we analyze and propose two bounding schemes for uncertainties associated to random approximate features in low dimensional spaces . the proposed techniques are based on random fourier features and the nystr\\ '' om methods . the resulting formulations can be solved with efficient stochastic approximation techniques such as stochastic ( sub ) -gradient , stochastic proximal gradient techniques or their variants .", "topics": ["approximation algorithm", "support vector machine"]}
{"title": "second-order optimization for non-convex machine learning : an empirical study", "abstract": "while first-order optimization methods such as stochastic gradient descent ( sgd ) are popular in machine learning ( ml ) , they come with well-known deficiencies , including relatively-slow convergence , sensitivity to the settings of hyper-parameters such as learning rate , stagnation at high training errors , and difficulty in escaping flat regions and saddle points . these issues are particularly acute in highly non-convex settings such as those arising in neural networks . motivated by this , there has been recent interest in second-order methods that aim to alleviate these shortcomings by capturing curvature information . in this paper , we report detailed empirical evaluations of a class of newton-type methods , namely sub-sampled variants of trust region ( tr ) and adaptive regularization with cubics ( arc ) algorithms , for non-convex ml problems . in doing so , we demonstrate that these methods not only can be computationally competitive with hand-tuned sgd with momentum , obtaining comparable or better generalization performance , but also they are highly robust to hyper-parameter settings . further , in contrast to sgd with momentum , we show that the manner in which these newton-type methods employ curvature information allows them to seamlessly escape flat regions and saddle points .", "topics": ["mathematical optimization", "gradient descent"]}
{"title": "initial version of state transition algorithm", "abstract": "in terms of the concepts of state and state transition , a new algorithm-state transition algorithm ( sta ) is proposed in order to probe into classical and intelligent optimization algorithms . on the basis of state and state transition , it becomes much simpler and easier to understand . as for continuous function optimization problems , three special operators named rotation , translation and expansion are presented . while for discrete function optimization problems , an operator called general elementary transformation is introduced . finally , with 4 common benchmark continuous functions and a discrete problem used to test the performance of sta , the experiment shows that sta is a promising algorithm due to its good search capability .", "topics": ["mathematical optimization"]}
{"title": "scalable and accurate online feature selection for big data", "abstract": "feature selection is important in many big data applications . two critical challenges closely associate with big data . firstly , in many big data applications , the dimensionality is extremely high , in millions , and keeps growing . secondly , big data applications call for highly scalable feature selection algorithms in an online manner such that each feature can be processed in a sequential scan . we present saola , a scalable and accurate online approach for feature selection in this paper . with a theoretical analysis on bounds of the pairwise correlations between features , saola employs novel pairwise comparison techniques and maintain a parsimonious model over time in an online manner . furthermore , to deal with upcoming features that arrive by groups , we extend the saola algorithm , and then propose a new group-saola algorithm for online group feature selection . the group-saola algorithm can online maintain a set of feature groups that is sparse at the levels of both groups and individual features simultaneously . an empirical study using a series of benchmark real data sets shows that our two algorithms , saola and group-saola , are scalable on data sets of extremely high dimensionality , and have superior performance over the state-of-the-art feature selection methods .", "topics": ["sparse matrix", "scalability"]}
{"title": "compressive spectral clustering", "abstract": "spectral clustering has become a popular technique due to its high performance in many contexts . it comprises three main steps : create a similarity graph between n objects to cluster , compute the first k eigenvectors of its laplacian matrix to define a feature vector for each object , and run k-means on these features to separate objects into k classes . each of these three steps becomes computationally intensive for large n and/or k. we propose to speed up the last two steps based on recent results in the emerging field of graph signal processing : graph filtering of random signals , and random sampling of bandlimited graph signals . we prove that our method , with a gain in computation time that can reach several orders of magnitude , is in fact an approximation of spectral clustering , for which we are able to control the error . we test the performance of our method on artificial and real-world network data .", "topics": ["cluster analysis", "data mining"]}
{"title": "summarization of user-generated sports video by using deep action recognition features", "abstract": "automatically generating a summary of sports video poses the challenge of detecting interesting moments , or highlights , of a game . traditional sports video summarization methods leverage editing conventions of broadcast sports video that facilitate the extraction of high-level semantics . however , user-generated videos are not edited , and thus traditional methods are not suitable to generate a summary . in order to solve this problem , this work proposes a novel video summarization method that uses players ' actions as a cue to determine the highlights of the original video . a deep neural network-based approach is used to extract two types of action-related features and to classify video segments into interesting or uninteresting parts . the proposed method can be applied to any sports in which games consist of a succession of actions . especially , this work considers the case of kendo ( japanese fencing ) as an example of a sport to evaluate the proposed method . the method is trained using kendo videos with ground truth labels that indicate the video highlights . the labels are provided by annotators possessing different experience with respect to kendo to demonstrate how the proposed method adapts to different needs . the performance of the proposed method is compared with several combinations of different features , and the results show that it outperforms previous summarization methods .", "topics": ["high- and low-level", "ground truth"]}
{"title": "literal movement grammars", "abstract": "literal movement grammars ( lmgs ) provide a general account of extraposition phenomena through an attribute mechanism allowing top-down displacement of syntactical information . lmgs provide a simple and efficient treatment of complex linguistic phenomena such as cross-serial dependencies in german and dutch -- -separating the treatment of natural language into a parsing phase closely resembling traditional context-free treatment , and a disambiguation phase which can be carried out using matching , as opposed to full unification employed in most current grammar formalisms of linguistical relevance .", "topics": ["natural language", "parsing"]}
{"title": "ensemble of part detectors for simultaneous classification and localization", "abstract": "part-based representation has been proven to be effective for a variety of visual applications . however , automatic discovery of discriminative parts without object/part-level annotations is challenging . this paper proposes a discriminative mid-level representation paradigm based on the responses of a collection of part detectors , which only requires the image-level labels . towards this goal , we first develop a detector-based spectral clustering method to mine the representative and discriminative mid-level patterns for detector initialization . the advantage of the proposed pattern mining technology is that the distance metric based on detectors only focuses on discriminative details , and a set of such grouped detectors offer an effective way for consistent pattern mining . relying on the discovered patterns , we further formulate the detector learning process as a confidence-loss sparse multiple instance learning ( cls-mil ) task , which considers the diversity of the positive samples , while avoid drifting away the well localized ones by assigning a confidence value to each positive sample . the responses of the learned detectors can form an effective mid-level image representation for both image classification and object localization . experiments conducted on benchmark datasets demonstrate the superiority of our method over existing approaches .", "topics": ["data mining", "cluster analysis"]}
{"title": "network-regularized sparse logistic regression models for clinical risk prediction and biomarker discovery", "abstract": "molecular profiling data ( e.g . , gene expression ) has been used for clinical risk prediction and biomarker discovery . however , it is necessary to integrate other prior knowledge like biological pathways or gene interaction networks to improve the predictive ability and biological interpretability of biomarkers . here , we first introduce a general regularized logistic regression ( lr ) framework with regularized term $ \\lambda \\|\\bm { w } \\|_1 + \\eta\\bm { w } ^t\\bm { m } \\bm { w } $ , which can reduce to different penalties , including lasso , elastic net , and network-regularized terms with different $ \\bm { m } $ . this framework can be easily solved in a unified manner by a cyclic coordinate descent algorithm which can avoid inverse matrix operation and accelerate the computing speed . however , if those estimated $ \\bm { w } _i $ and $ \\bm { w } _j $ have opposite signs , then the traditional network-regularized penalty may not perform well . to address it , we introduce a novel network-regularized sparse lr model with a new penalty $ \\lambda \\|\\bm { w } \\|_1 + \\eta|\\bm { w } |^t\\bm { m } |\\bm { w } | $ to consider the difference between the absolute values of the coefficients . and we develop two efficient algorithms to solve it . finally , we test our methods and compare them with the related ones using simulated and real data to show their efficiency .", "topics": ["sparse matrix", "simulation"]}
{"title": "dense motion estimation for smoke", "abstract": "motion estimation for highly dynamic phenomena such as smoke is an open challenge for computer vision . traditional dense motion estimation algorithms have difficulties with non-rigid and large motions , both of which are frequently observed in smoke motion . we propose an algorithm for dense motion estimation of smoke . our algorithm is robust , fast , and has better performance over different types of smoke compared to other dense motion estimation algorithms , including state of the art and neural network approaches . the key to our contribution is to use skeletal flow , without explicit point matching , to provide a sparse flow . this sparse flow is upgraded to a dense flow . in this paper we describe our algorithm in greater detail , and provide experimental evidence to support our claims .", "topics": ["computer vision", "sparse matrix"]}
{"title": "geometry guided adversarial facial expression synthesis", "abstract": "facial expression synthesis has drawn much attention in the field of computer graphics and pattern recognition . it has been widely used in face animation and recognition . however , it is still challenging due to the high-level semantic presence of large and non-linear face geometry variations . this paper proposes a geometry-guided generative adversarial network ( g2-gan ) for photo-realistic and identity-preserving facial expression synthesis . we employ facial geometry ( fiducial points ) as a controllable condition to guide facial texture synthesis with specific expression . a pair of generative adversarial subnetworks are jointly trained towards opposite tasks : expression removal and expression synthesis . the paired networks form a mapping cycle between neutral expression and arbitrary expressions , which also facilitate other applications such as face transfer and expression invariant face recognition . experimental results show that our method can generate compelling perceptual results on various facial expression synthesis databases . an expression invariant face recognition experiment is also performed to further show the advantages of our proposed method .", "topics": ["high- and low-level", "nonlinear system"]}
{"title": "nonlinear laplacian spectral analysis : capturing intermittent and low-frequency spatiotemporal patterns in high-dimensional data", "abstract": "we present a technique for spatiotemporal data analysis called nonlinear laplacian spectral analysis ( nlsa ) , which generalizes singular spectrum analysis ( ssa ) to take into account the nonlinear manifold structure of complex data sets . the key principle underlying nlsa is that the functions used to represent temporal patterns should exhibit a degree of smoothness on the nonlinear data manifold m ; a constraint absent from classical ssa . nlsa enforces such a notion of smoothness by requiring that temporal patterns belong in low-dimensional hilbert spaces v_l spanned by the leading l laplace-beltrami eigenfunctions on m. these eigenfunctions can be evaluated efficiently in high ambient-space dimensions using sparse graph-theoretic algorithms . moreover , they provide orthonormal bases to expand a family of linear maps , whose singular value decomposition leads to sets of spatiotemporal patterns at progressively finer resolution on the data manifold . the riemannian measure of m and an adaptive graph kernel width enhances the capability of nlsa to detect important nonlinear processes , including intermittency and rare events . the minimum dimension of v_l required to capture these features while avoiding overfitting is estimated here using spectral entropy criteria .", "topics": ["nonlinear system", "sparse matrix"]}
{"title": "image segmentation to distinguish between overlapping human chromosomes", "abstract": "in medicine , visualizing chromosomes is important for medical diagnostics , drug development , and biomedical research . unfortunately , chromosomes often overlap and it is necessary to identify and distinguish between the overlapping chromosomes . a segmentation solution that is fast and automated will enable scaling of cost effective medicine and biomedical research . we apply neural network-based image segmentation to the problem of distinguishing between partially overlapping dna chromosomes . a convolutional neural network is customized for this problem . the results achieved intersection over union ( iou ) scores of 94.7 % for the overlapping region and 88-94 % on the non-overlapping chromosome regions .", "topics": ["image segmentation"]}
{"title": "aishell-1 : an open-source mandarin speech corpus and a speech recognition baseline", "abstract": "an open-source mandarin speech corpus called aishell-1 is released . it is by far the largest corpus which is suitable for conducting the speech recognition research and building speech recognition systems for mandarin . the recording procedure , including audio capturing devices and environments are presented in details . the preparation of the related resources , including transcriptions and lexicon are described . the corpus is released with a kaldi recipe . experimental results implies that the quality of audio recordings and transcriptions are promising .", "topics": ["speech recognition", "text corpus"]}
{"title": "clustering , coding , and the concept of similarity", "abstract": "this paper develops a theory of clustering and coding which combines a geometric model with a probabilistic model in a principled way . the geometric model is a riemannian manifold with a riemannian metric , $ { g } _ { ij } ( { \\bf x } ) $ , which we interpret as a measure of dissimilarity . the probabilistic model consists of a stochastic process with an invariant probability measure which matches the density of the sample input data . the link between the two models is a potential function , $ u ( { \\bf x } ) $ , and its gradient , $ \\nabla u ( { \\bf x } ) $ . we use the gradient to define the dissimilarity metric , which guarantees that our measure of dissimilarity will depend on the probability measure . finally , we use the dissimilarity metric to define a coordinate system on the embedded riemannian manifold , which gives us a low-dimensional encoding of our original data .", "topics": ["cluster analysis", "gradient"]}
{"title": "predicting natural hazards with neuronal networks", "abstract": "gravitational mass flows , such as avalanches , debris flows and rockfalls are common events in alpine regions with high impact on transport routes . within the last few decades , hazard zone maps have been developed to systematically approach this threat . these maps mark vulnerable zones in habitable areas to allow effective planning of hazard mitigation measures and development of settlements . hazard zone maps have shown to be an effective tool to reduce fatalities during extreme events . they are created in a complex process , based on experience , empirical models , physical simulations and historical data . the generation of such maps is therefore expensive and limited to crucially important regions , e.g . permanently inhabited areas . in this work we interpret the task of hazard zone mapping as a classification problem . every point in a specific area has to be classified according to its vulnerability . on a regional scale this leads to a segmentation problem , where the total area has to be divided in the respective hazard zones . the recent developments in artificial intelligence , namely convolutional neuronal networks , have led to major improvement in a very similar task , image classification and semantic segmentation , i.e . computer vision . we use a convolutional neuronal network to identify terrain formations with the potential for catastrophic snow avalanches and label points in their reach as vulnerable . repeating this procedure for all points allows us to generate an artificial hazard zone map . we demonstrate that the approach is feasible and promising based on the hazard zone map of the tirolean oberland . however , more training data and further improvement of the method is required before such techniques can be applied reliably .", "topics": ["test set", "computer vision"]}
{"title": "a fast and scalable joint estimator for learning multiple related sparse gaussian graphical models", "abstract": "estimating multiple sparse gaussian graphical models ( sggms ) jointly for many related tasks ( large $ k $ ) under a high-dimensional ( large $ p $ ) situation is an important task . most previous studies for the joint estimation of multiple sggms rely on penalized log-likelihood estimators that involve expensive and difficult non-smooth optimizations . we propose a novel approach , fasjem for \\underline { fa } st and \\underline { s } calable \\underline { j } oint structure-\\underline { e } stimation of \\underline { m } ultiple sggms at a large scale . as the first study of joint sggm using the elementary estimator framework , our work has three major contributions : ( 1 ) we solve fasjem through an entry-wise manner which is parallelizable . ( 2 ) we choose a proximal algorithm to optimize fasjem . this improves the computational efficiency from $ o ( kp^3 ) $ to $ o ( kp^2 ) $ and reduces the memory requirement from $ o ( kp^2 ) $ to $ o ( k ) $ . ( 3 ) we theoretically prove that fasjem achieves a consistent estimation with a convergence rate of $ o ( \\log ( kp ) /n_ { tot } ) $ . on several synthetic and four real-world datasets , fasjem shows significant improvements over baselines on accuracy , computational complexity , and memory costs .", "topics": ["graphical model", "computational complexity theory"]}
{"title": "ridi : robust imu double integration", "abstract": "this paper proposes a novel data-driven approach for inertial navigation , which learns to estimate trajectories of natural human motions just from an inertial measurement unit ( imu ) in every smartphone . the key observation is that human motions are repetitive and consist of a few major modes ( e.g . , standing , walking , or turning ) . our algorithm regresses a velocity vector from the history of linear accelerations and angular velocities , then corrects low-frequency bias in the linear accelerations , which are integrated twice to estimate positions . we have acquired training data with ground-truth motions across multiple human subjects and multiple phone placements ( e.g . , in a bag or a hand ) . the qualitatively and quantitatively evaluations have demonstrated that our algorithm has surprisingly shown comparable results to full visual inertial navigation . to our knowledge , this paper is the first to integrate sophisticated machine learning techniques with inertial navigation , potentially opening up a new line of research in the domain of data-driven inertial navigation . we will publicly share our code and data to facilitate further research .", "topics": ["test set", "ground truth"]}
{"title": "fiber orientation estimation guided by a deep network", "abstract": "diffusion magnetic resonance imaging ( dmri ) is currently the only tool for noninvasively imaging the brain 's white matter tracts . the fiber orientation ( fo ) is a key feature computed from dmri for fiber tract reconstruction . because the number of fos in a voxel is usually small , dictionary-based sparse reconstruction has been used to estimate fos with a relatively small number of diffusion gradients . however , accurate fo estimation in regions with complex fo configurations in the presence of noise can still be challenging . in this work we explore the use of a deep network for fo estimation in a dictionary-based framework and propose an algorithm named fiber orientation reconstruction guided by a deep network ( fordn ) . fordn consists of two steps . first , we use a smaller dictionary encoding coarse basis fos to represent the diffusion signals . to estimate the mixture fractions of the dictionary atoms ( and thus coarse fos ) , a deep network is designed specifically for solving the sparse reconstruction problem . here , the smaller dictionary is used to reduce the computational cost of training . second , the coarse fos inform the final fo estimation , where a larger dictionary encoding dense basis fos is used and a weighted l1-norm regularized least squares problem is solved to encourage fos that are consistent with the network output . fordn was evaluated and compared with state-of-the-art algorithms that estimate fos using sparse reconstruction on simulated and real dmri data , and the results demonstrate the benefit of using a deep network for fo estimation .", "topics": ["simulation", "sparse matrix"]}
{"title": "improved underwater image enhancement algorithms based on partial differential equations ( pdes )", "abstract": "the experimental results of improved underwater image enhancement algorithms based on partial differential equations ( pdes ) are presented in this report . this second work extends the study of previous work and incorporating several improvements into the revised algorithm . experiments show the evidence of the improvements when compared to previously proposed approaches and other conventional algorithms found in the literature .", "topics": ["image processing"]}
{"title": "entropy-based search algorithm for experimental design", "abstract": "the scientific method relies on the iterated processes of inference and inquiry . the inference phase consists of selecting the most probable models based on the available data ; whereas the inquiry phase consists of using what is known about the models to select the most relevant experiment . optimizing inquiry involves searching the parameterized space of experiments to select the experiment that promises , on average , to be maximally informative . in the case where it is important to learn about each of the model parameters , the relevance of an experiment is quantified by shannon entropy of the distribution of experimental outcomes predicted by a probable set of models . if the set of potential experiments is described by many parameters , we must search this high-dimensional entropy space . brute force search methods will be slow and computationally expensive . we present an entropy-based search algorithm , called nested entropy sampling , to select the most informative experiment for efficient experimental design . this algorithm is inspired by skilling 's nested sampling algorithm used in inference and borrows the concept of a rising threshold while a set of experiment samples are maintained . we demonstrate that this algorithm not only selects highly relevant experiments , but also is more efficient than brute force search . such entropic search techniques promise to greatly benefit autonomous experimental design .", "topics": ["relevance", "autonomous car"]}
{"title": "a decomposable attention model for natural language inference", "abstract": "we propose a simple neural architecture for natural language inference . our approach uses attention to decompose the problem into subproblems that can be solved separately , thus making it trivially parallelizable . on the stanford natural language inference ( snli ) dataset , we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information . adding intra-sentence attention that takes a minimum amount of order into account yields further improvements .", "topics": ["natural language"]}
{"title": "efficient algorithms for t-distributed stochastic neighborhood embedding", "abstract": "t-distributed stochastic neighborhood embedding ( t-sne ) is a method for dimensionality reduction and visualization that has become widely popular in recent years . efficient implementations of t-sne are available , but they scale poorly to datasets with hundreds of thousands to millions of high dimensional data-points . we present fast fourier transform-accelerated interpolation-based t-sne ( fit-sne ) , which dramatically accelerates the computation of t-sne . the most time-consuming step of t-sne is a convolution that we accelerate by interpolating onto an equispaced grid and subsequently using the fast fourier transform to perform the convolution . we also optimize the computation of input similarities in high dimensions using multi-threaded approximate nearest neighbors . we further present a modification to t-sne called `` late exaggeration , '' which allows for easier identification of clusters in t-sne embeddings . finally , for datasets that can not be loaded into the memory , we present out-of-core randomized principal component analysis ( oocpca ) , so that the top principal components of a dataset can be computed without ever fully loading the matrix , hence allowing for t-sne of large datasets to be computed on resource-limited machines .", "topics": ["approximation algorithm", "computation"]}
{"title": "e-qraq : a multi-turn reasoning dataset and simulator with explanations", "abstract": "in this paper we present a new dataset and user simulator e-qraq ( explainable query , reason , and answer question ) which tests an agent 's ability to read an ambiguous text ; ask questions until it can answer a challenge question ; and explain the reasoning behind its questions and answer . the user simulator provides the agent with a short , ambiguous story and a challenge question about the story . the story is ambiguous because some of the entities have been replaced by variables . at each turn the agent may ask for the value of a variable or try to answer the challenge question . in response the user simulator provides a natural language explanation of why the agent 's query or answer was useful in narrowing down the set of possible answers , or not . to demonstrate one potential application of the e-qraq dataset , we train a new neural architecture based on end-to-end memory networks to successfully generate both predictions and partial explanations of its current understanding of the problem . we observe a strong correlation between the quality of the prediction and explanation .", "topics": ["natural language", "simulation"]}
{"title": "vse-ens : visual-semantic embeddings with efficient negative sampling", "abstract": "jointing visual-semantic embeddings ( vse ) have become a research hotpot for the task of image annotation , which suffers from the issue of semantic gap , i.e . , the gap between images ' visual features ( low-level ) and labels ' semantic features ( high-level ) . this issue will be even more challenging if visual features can not be retrieved from images , that is , when images are only denoted by numerical ids as given in some real datasets . the typical way of existing vse methods is to perform a uniform sampling method for negative examples that violate the ranking order against positive examples , which requires a time-consuming search in the whole label space . in this paper , we propose a fast adaptive negative sampler that can work well in the settings of no figure pixels available . our sampling strategy is to choose the negative examples that are most likely to meet the requirements of violation according to the latent factors of images . in this way , our approach can linearly scale up to large datasets . the experiments demonstrate that our approach converges 5.02x faster than the state-of-the-art approaches on openimages , 2.5x on iapr-tci2 and 2.06x on nus-wide datasets , as well as better ranking accuracy across datasets .", "topics": ["sampling ( signal processing )", "high- and low-level"]}
{"title": "simple heuristics for the assembly line worker assignment and balancing problem", "abstract": "we propose simple heuristics for the assembly line worker assignment and balancing problem . this problem typically occurs in assembly lines in sheltered work centers for the disabled . different from the classical simple assembly line balancing problem , the task execution times vary according to the assigned worker . we develop a constructive heuristic framework based on task and worker priority rules defining the order in which the tasks and workers should be assigned to the workstations . we present a number of such rules and compare their performance across three possible uses : as a stand-alone method , as an initial solution generator for meta-heuristics , and as a decoder for a hybrid genetic algorithm . our results show that the heuristics are fast , they obtain good results as a stand-alone method and are efficient when used as a initial solution generator or as a solution decoder within more elaborate approaches .", "topics": ["heuristic"]}
{"title": "cyclical learning rates for training neural networks", "abstract": "it is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks . this paper describes a new method for setting the learning rate , named cyclical learning rates , which practically eliminates the need to experimentally find the best values and schedule for the global learning rates . instead of monotonically decreasing the learning rate , this method lets the learning rate cyclically vary between reasonable boundary values . training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations . this paper also describes a simple way to estimate `` reasonable bounds '' -- linearly increasing the learning rate of the network for a few epochs . in addition , cyclical learning rates are demonstrated on the cifar-10 and cifar-100 datasets with resnets , stochastic depth networks , and densenets , and the imagenet dataset with the alexnet and googlenet architectures . these are practical tools for everyone who trains neural networks .", "topics": ["supervised learning", "value ( ethics )"]}
{"title": "time-series scenario forecasting", "abstract": "many applications require the ability to judge uncertainty of time-series forecasts . uncertainty is often specified as point-wise error bars around a mean or median forecast . due to temporal dependencies , such a method obscures some information . we would ideally have a way to query the posterior probability of the entire time-series given the predictive variables , or at a minimum , be able to draw samples from this distribution . we use a bayesian dictionary learning algorithm to statistically generate an ensemble of forecasts . we show that the algorithm performs as well as a physics-based ensemble method for temperature forecasts for houston . we conclude that the method shows promise for scenario forecasting where physics-based methods are absent .", "topics": ["time series", "dictionary"]}
{"title": "coherent keyphrase extraction via web mining", "abstract": "keyphrases are useful for a variety of purposes , including summarizing , indexing , labeling , categorizing , clustering , highlighting , browsing , and searching . the task of automatic keyphrase extraction is to select keyphrases from within the text of a given document . automatic keyphrase extraction makes it feasible to generate keyphrases for the huge number of documents that do not have manually assigned keyphrases . a limitation of previous keyphrase extraction algorithms is that the selected keyphrases are occasionally incoherent . that is , the majority of the output keyphrases may fit together well , but there may be a minority that appear to be outliers , with no clear semantic relation to the majority or to each other . this paper presents enhancements to the kea keyphrase extraction algorithm that are designed to increase the coherence of the extracted keyphrases . the approach is to use the degree of statistical association among candidate keyphrases as evidence that they may be semantically related . the statistical association is measured using web mining . experiments demonstrate that the enhancements improve the quality of the extracted keyphrases . furthermore , the enhancements are not domain-specific : the algorithm generalizes well when it is trained on one domain ( computer science documents ) and tested on another ( physics documents ) .", "topics": ["cluster analysis"]}
{"title": "observing trends in automated multilingual media analysis", "abstract": "any large organisation , be it public or private , monitors the media for information to keep abreast of developments in their field of interest , and usually also to become aware of positive or negative opinions expressed towards them . at least for the written media , computer programs have become very efficient at helping the human analysts significantly in their monitoring task by gathering media reports , analysing them , detecting trends and - in some cases - even to issue early warnings or to make predictions of likely future developments . we present here trend recognition-related functionality of the europe media monitor ( emm ) system , which was developed by the european commission 's joint research centre ( jrc ) for public administrations in the european union ( eu ) and beyond . emm performs large-scale media analysis in up to seventy languages and recognises various types of trends , some of them combining information from news articles written in different languages and from social media posts . emm also lets users explore the huge amount of multilingual media data through interactive maps and graphs , allowing them to examine the data from various view points and according to multiple criteria . a lot of emm 's functionality is accessibly freely over the internet or via apps for hand-held devices .", "topics": ["map"]}
{"title": "a review of nonnegative matrix factorization methods for clustering", "abstract": "nonnegative matrix factorization ( nmf ) was first introduced as a low-rank matrix approximation technique , and has enjoyed a wide area of applications . although nmf does not seem related to the clustering problem at first , it was shown that they are closely linked . in this report , we provide a gentle introduction to clustering and nmf before reviewing the theoretical relationship between them . we then explore several nmf variants , namely sparse nmf , projective nmf , nonnegative spectral clustering and cluster-nmf , along with their clustering interpretations .", "topics": ["cluster analysis", "sparse matrix"]}
{"title": "multilinear wavelets : a statistical shape space for human faces", "abstract": "we present a statistical model for $ 3 $ d human faces in varying expression , which decomposes the surface of the face using a wavelet transform , and learns many localized , decorrelated multilinear models on the resulting coefficients . using this model we are able to reconstruct faces from noisy and occluded $ 3 $ d face scans , and facial motion sequences . accurate reconstruction of face shape is important for applications such as tele-presence and gaming . the localized and multi-scale nature of our model allows for recovery of fine-scale detail while retaining robustness to severe noise and occlusion , and is computationally efficient and scalable . we validate these properties experimentally on challenging data in the form of static scans and motion sequences . we show that in comparison to a global multilinear model , our model better preserves fine detail and is computationally faster , while in comparison to a localized pca model , our model better handles variation in expression , is faster , and allows us to fix identity parameters for a given subject .", "topics": ["computational complexity theory", "scalability"]}
{"title": "transforming and enriching documents for the semantic web", "abstract": "we suggest to employ techniques from natural language processing ( nlp ) and knowledge representation ( kr ) to transform existing documents into documents amenable for the semantic web . semantic web documents have at least part of their semantics and pragmatics marked up explicitly in both a machine processable as well as human readable manner . xml and its related standards ( xslt , rdf , topic maps etc . ) are the unifying platform for the tools and methodologies developed for different application scenarios .", "topics": ["natural language processing", "natural language"]}
{"title": "clustering without ( thinking about ) triangulation", "abstract": "the undirected technique for evaluating belief networks [ jensen , et.al . , 1990 , lauritzen and spiegelhalter , 1988 ] requires clustering the nodes in the network into a junction tree . in the traditional view , the junction tree is constructed from the cliques of the moralized and triangulated belief network : triangulation is taken to be the primitive concept , the goal towards which any clustering algorithm ( e.g . node elimination ) is directed . in this paper , we present an alternative conception of clustering , in which clusters and the junction tree property play the role of primitives : given a graph ( not a tree ) of clusters which obey ( a modified version of ) the junction tree property , we transform this graph until we have obtained a tree . there are several advantages to this approach : it is much clearer and easier to understand , which is important for humans who are constructing belief networks ; it admits a wider range of heuristics which may enable more efficient or superior clustering algorithms ; and it serves as the natural basis for an incremental clustering scheme , which we describe .", "topics": ["cluster analysis", "bayesian network"]}
{"title": "neural autoregressive distribution estimation", "abstract": "we present neural autoregressive distribution estimation ( nade ) models , which are neural network architectures applied to the problem of unsupervised distribution and density estimation . they leverage the probability product rule and a weight sharing scheme inspired from restricted boltzmann machines , to yield an estimator that is both tractable and has good generalization performance . we discuss how they achieve competitive performance in modeling both binary and real-valued observations . we also present how deep nade models can be trained to be agnostic to the ordering of input dimensions used by the autoregressive product rule decomposition . finally , we also show how to exploit the topological structure of pixels in images using a deep convolutional architecture for nade .", "topics": ["pixel"]}
{"title": "natural data structure extracted from neighborhood-similarity graphs", "abstract": "'big ' high-dimensional data are commonly analyzed in low-dimensions , after performing a dimensionality-reduction step that inherently distorts the data structure . for the same purpose , clustering methods are also often used . these methods also introduce a bias , either by starting from the assumption of a particular geometric form of the clusters , or by using iterative schemes to enhance cluster contours , with uncontrollable consequences . the goal of data analysis should , however , be to encode and detect structural data features at all scales and densities simultaneously , without assuming a parametric form of data point distances , or modifying them . we propose a novel approach that directly encodes data point neighborhood similarities as a sparse graph . our non-iterative framework permits a transparent interpretation of data , without altering the original data dimension and metric . several natural and synthetic data applications demonstrate the efficacy of our novel approach .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "dynamic evaluation of neural sequence models", "abstract": "we present methodology for using dynamic evaluation to improve neural sequence models . models are adapted to recent history via a gradient descent based mechanism , causing them to assign higher probabilities to re-occurring sequential patterns . dynamic evaluation outperforms existing adaptation approaches in our comparisons . dynamic evaluation improves the state-of-the-art word-level perplexities on the penn treebank and wikitext-2 datasets to 51.1 and 44.3 respectively , and the state-of-the-art character-level cross-entropies on the text8 and hutter prize datasets to 1.19 bits/char and 1.08 bits/char respectively .", "topics": ["gradient descent", "gradient"]}
{"title": "an associative memory for the on-line recognition and prediction of temporal sequences", "abstract": "this paper presents the design of an associative memory with feedback that is capable of on-line temporal sequence learning . a framework for on-line sequence learning has been proposed , and different sequence learning models have been analysed according to this framework . the network model is an associative memory with a separate store for the sequence context of a symbol . a sparse distributed memory is used to gain scalability . the context store combines the functionality of a neural layer with a shift register . the sensitivity of the machine to the sequence context is controllable , resulting in different characteristic behaviours . the model can store and predict on-line sequences of various types and length . numerical simulations on the model have been carried out to determine its properties .", "topics": ["numerical analysis", "simulation"]}
{"title": "learning similarity preserving representations with neural similarity encoders", "abstract": "many dimensionality reduction or manifold learning algorithms optimize for retaining the pairwise similarities , distances , or local neighborhoods of data points . spectral methods like kernel pca ( kpca ) or isomap achieve this by computing the singular value decomposition ( svd ) of some similarity matrix to obtain a low dimensional representation of the original data . however , this is computationally expensive if a lot of training examples are available and , additionally , representations for new ( out-of-sample ) data points can only be created when the similarities to the original training examples can be computed . we introduce similarity encoders ( simec ) , which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly . the model optimizes the same objective as kpca but in the process it learns a linear or non-linear embedding function ( in the form of the tuned neural network ) , with which the representations of novel data points can be computed - even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings . by creating embeddings for both image and text datasets , we demonstrate that simec can , on the one hand , reach the same solution as spectral methods , and , on the other hand , obtain meaningful embeddings from similarities based on human labels .", "topics": ["nonlinear system", "encoder"]}
{"title": "stochastic blockmodeling for online advertising", "abstract": "online advertising is an important and huge industry . having knowledge of the website attributes can contribute greatly to business strategies for ad-targeting , content display , inventory purchase or revenue prediction . classical inferences on users and sites impose challenge , because the data is voluminous , sparse , high-dimensional and noisy . in this paper , we introduce a stochastic blockmodeling for the website relations induced by the event of online user visitation . we propose two clustering algorithms to discover the instrinsic structures of websites , and compare the performance with a goodness-of-fit method and a deterministic graph partitioning method . we demonstrate the effectiveness of our algorithms on both simulation and aol website dataset .", "topics": ["cluster analysis", "simulation"]}
{"title": "computing by means of physics-based optical neural networks", "abstract": "we report recent research on computing with biology-based neural network models by means of physics-based opto-electronic hardware . new technology provides opportunities for very-high-speed computation and uncovers problems obstructing the wide-spread use of this new capability . the computation modeling community may be able to offer solutions to these cross-boundary research problems .", "topics": ["neural networks", "computation"]}
{"title": "real-time marker-less multi-person 3d pose estimation in rgb-depth camera networks", "abstract": "this paper proposes a novel system to estimate and track the 3d poses of multiple persons in calibrated rgb-depth camera networks . the multi-view 3d pose of each person is computed by a central node which receives the single-view outcomes from each camera of the network . each single-view outcome is computed by using a cnn for 2d pose estimation and extending the resulting skeletons to 3d by means of the sensor depth . the proposed system is marker-less , multi-person , independent of background and does not make any assumption on people appearance and initial pose . the system provides real-time outcomes , thus being perfectly suited for applications requiring user interaction . experimental results show the effectiveness of this work with respect to a baseline multi-view approach in different scenarios . to foster research and applications based on this work , we released the source code in openptrack , an open source project for rgb-d people tracking .", "topics": ["baseline ( configuration management )"]}
{"title": "sparsity invariant cnns", "abstract": "in this paper , we consider convolutional neural networks operating on sparse inputs with an application to depth upsampling from sparse laser scan data . first , we show that traditional convolutional networks perform poorly when applied to sparse data even when the location of missing data is provided to the network . to overcome this problem , we propose a simple yet effective sparse convolution layer which explicitly considers the location of missing data during the convolution operation . we demonstrate the benefits of the proposed network architecture in synthetic and real experiments with respect to various baseline approaches . compared to dense baselines , the proposed sparse convolution network generalizes well to novel datasets and is invariant to the level of sparsity in the data . for our evaluation , we derive a novel dataset from the kitti benchmark , comprising 93k depth annotated rgb images . our dataset allows for training and evaluating depth upsampling and depth prediction techniques in challenging real-world settings and will be made available upon publication .", "topics": ["baseline ( configuration management )", "synthetic data"]}
{"title": "neural and synaptic array transceiver : a brain-inspired computing framework for embedded learning", "abstract": "embedded , continual learning for autonomous and adaptive behavior is a key application of neuromorphic hardware designed to mimic the dynamics and architecture of biological neural networks . however , neuromorphic implementations of embedded learning at large scales that are both flexible and efficient have been hindered by a lack of a suitable algorithmic framework . as a result , most neuromorphic hardware are trained off-line on large clusters of dedicated processors or gpus and transferred post hoc to the device . we address this by introducing the neural and synaptic array transceiver ( nsat ) , a neuromorphic computational framework facilitating flexible and efficient embedded learning . nsat supports event-driven supervised , unsupervised and reinforcement learning algorithms including deep learning . we demonstrate the nsat in a wide range of tasks , including the simulation of mihalas-niebur neuron , dynamic neural fields , event-driven random back-propagation for event-based deep learning , event-based contrastive divergence for unsupervised learning , and voltage-based learning rules for sequence learning . we anticipate that this contribution will establish the foundation for a new generation of devices enabling adaptive mobile systems , wearable devices , and robots with data-driven autonomy .", "topics": ["unsupervised learning", "reinforcement learning"]}
{"title": "orthogonal least squares algorithm for the approximation of a map and its derivatives with a rbf network", "abstract": "radial basis function networks ( rbfns ) are used primarily to solve curve-fitting problems and for non-linear system modeling . several algorithms are known for the approximation of a non-linear curve from a sparse data set by means of rbfns . however , there are no procedures that permit to define constrains on the derivatives of the curve . in this paper , the orthogonal least squares algorithm for the identification of rbfns is modified to provide the approximation of a non-linear 1-in 1-out map along with its derivatives , given a set of training data . the interest on the derivatives of non-linear functions concerns many identification and control tasks where the study of system stability and robustness is addressed . the effectiveness of the proposed algorithm is demonstrated by a study on the stability of a single loop feedback system .", "topics": ["test set", "nonlinear system"]}
{"title": "adapted sampling for 3d x-ray computed tomography", "abstract": "in this paper , we introduce a method to build an adapted mesh representation of a 3d object for x-ray tomography reconstruction . using this representation , we provide means to reduce the computational cost of reconstruction by way of iterative algorithms . the adapted sampling of the reconstruction space is directly obtained from the projection dataset and prior to any reconstruction . it is built following two stages : firstly , 2d structural information is extracted from the projection images and is secondly merged in 3d to obtain a 3d pointcloud sampling the interfaces of the object . a relevant mesh is then built from this cloud by way of tetrahedralization . critical parameters selections have been automatized through a statistical framework , thus avoiding dependence on users expertise . applying this approach on geometrical shapes and on a 3d shepp-logan phantom , we show the relevance of such a sampling - obtained in a few seconds - and the drastic decrease in cells number to be estimated during reconstruction when compared to the usual regular voxel lattice . a first iterative reconstruction of the shepp-logan using this kind of sampling shows the relevant advantages in terms of low dose or sparse acquisition sampling contexts . the method can also prove useful for other applications such as finite element method computations .", "topics": ["sampling ( signal processing )", "computation"]}
{"title": "unsupervised neural-symbolic integration", "abstract": "symbolic has been long considered as a language of human intelligence while neural networks have advantages of robust computation and dealing with noisy data . the integration of neural-symbolic can offer better learning and reasoning while providing a means for interpretability through the representation of symbolic knowledge . although previous works focus intensively on supervised feedforward neural networks , little has been done for the unsupervised counterparts . in this paper we show how to integrate symbolic knowledge into unsupervised neural networks . we exemplify our approach with knowledge in different forms , including propositional logic for dna promoter prediction and first-order logic for understanding family relationship .", "topics": ["unsupervised learning", "computation"]}
{"title": "a simple and effective approach to the story cloze test", "abstract": "in the story cloze test , a system is presented with a 4-sentence prompt to a story , and must determine which one of two potential endings is the 'right ' ending to the story . previous work has shown that ignoring the training set and training a model on the validation set can achieve high accuracy on this task due to stylistic differences between the story endings in the training set and validation and test sets . following this approach , we present a simpler fully-neural approach to the story cloze test using skip-thought embeddings of the stories in a feed-forward network that achieves close to state-of-the-art performance on this task without any feature engineering . we also find that considering just the last sentence of the prompt instead of the whole prompt yields higher accuracy with our approach .", "topics": ["test set"]}
{"title": "extrapolation and learning equations", "abstract": "in classical machine learning , regression is treated as a black box process of identifying a suitable function from a hypothesis set without attempting to gain insight into the mechanism connecting inputs and outputs . in the natural sciences , however , finding an interpretable function for a phenomenon is the prime goal as it allows to understand and generalize results . this paper proposes a novel type of function learning network , called equation learner ( eql ) , that can learn analytical expressions and is able to extrapolate to unseen domains . it is implemented as an end-to-end differentiable feed-forward network and allows for efficient gradient based training . due to sparsity regularization concise interpretable expressions can be obtained . often the true underlying source expression is identified .", "topics": ["test set", "value ( ethics )"]}
{"title": "active regression by stratification", "abstract": "we propose a new active learning algorithm for parametric linear regression with random design . we provide finite sample convergence guarantees for general distributions in the misspecified model . this is the first active learner for this setting that provably can improve over passive learning . unlike other learning settings ( such as classification ) , in regression the passive learning rate of $ o ( 1/\\epsilon ) $ can not in general be improved upon . nonetheless , the so-called `constant ' in the rate of convergence , which is characterized by a distribution-dependent risk , can be improved in many cases . for a given distribution , achieving the optimal risk requires prior knowledge of the distribution . following the stratification technique advocated in monte-carlo function integration , our active learner approaches the optimal risk using piecewise constant approximations .", "topics": ["statistical classification", "approximation"]}
{"title": "generalized tree-based wavelet transform", "abstract": "in this paper we propose a new wavelet transform applicable to functions defined on graphs , high dimensional data and networks . the proposed method generalizes the haar-like transform proposed in [ 1 ] , and it is defined via a hierarchical tree , which is assumed to capture the geometry and structure of the input data . it is applied to the data using a modified version of the common one-dimensional ( 1d ) wavelet filtering and decimation scheme , which can employ different wavelet filters . in each level of this wavelet decomposition scheme , a permutation derived from the tree is applied to the approximation coefficients , before they are filtered . we propose a tree construction method that results in an efficient representation of the input function in the transform domain . we show that the proposed transform is more efficient than both the 1d and two-dimensional ( 2d ) separable wavelet transforms in representing images . we also explore the application of the proposed transform to image denoising , and show that combined with a subimage averaging scheme , it achieves denoising results which are similar to those obtained with the k-svd algorithm .", "topics": ["noise reduction", "coefficient"]}
{"title": "soft constraints of difference and equality", "abstract": "in many combinatorial problems one may need to model the diversity or similarity of assignments in a solution . for example , one may wish to maximise or minimise the number of distinct values in a solution . to formulate problems of this type , we can use soft variants of the well known alldifferent and allequal constraints . we present a taxonomy of six soft global constraints , generated by combining the two latter ones and the two standard cost functions , which are either maximised or minimised . we characterise the complexity of achieving arc and bounds consistency on these constraints , resolving those cases for which np-hardness was neither proven nor disproven . in particular , we explore in depth the constraint ensuring that at least k pairs of variables have a common value . we show that achieving arc consistency is np-hard , however achieving bounds consistency can be done in polynomial time through dynamic programming . moreover , we show that the maximum number of pairs of equal variables can be approximated by a factor 1/2 with a linear time greedy algorithm . finally , we provide a fixed parameter tractable algorithm with respect to the number of values appearing in more than two distinct domains . interestingly , this taxonomy shows that enforcing equality is harder than enforcing difference .", "topics": ["approximation algorithm", "value ( ethics )"]}
{"title": "discovering stock price prediction rules of bombay stock exchange using rough fuzzy multi layer perception networks", "abstract": "in india financial markets have existed for many years . a functionally accented , diverse , efficient and flexible financial system is vital to the national objective of creating a market driven , productive and competitive economy . today markets of varying maturity exist in equity , debt , commodities and foreign exchange . in this work we attempt to generate prediction rules scheme for stock price movement at bombay stock exchange using an important soft computing paradigm viz . , rough fuzzy multi layer perception . the use of computational intelligence systems such as neural networks , fuzzy sets , genetic algorithms , etc . for stock market predictions has been widely established . the process is to extract knowledge in the form of rules from daily stock movements . these rules can then be used to guide investors . to increase the efficiency of the prediction process , rough sets is used to discretize the data . the methodology uses a genetic algorithm to obtain a structured network suitable for both classification and rule extraction . the modular concept , based on divide and conquer strategy , provides accelerated training and a compact network suitable for generating a minimum number of rules with high certainty values . the concept of variable mutation operator is introduced for preserving the localized structure of the constituting knowledge based sub-networks , while they are integrated and evolved . rough set dependency rules are generated directly from the real valued attribute table containing fuzzy membership values . the paradigm is thus used to develop a rule extraction algorithm . the extracted rules are compared with some of the related rule extraction techniques on the basis of some quantitative performance indices . the proposed methodology extracts rules which are less in number , are accurate , have high certainty factor and have low confusion with less computation time .", "topics": ["time complexity", "neural networks"]}
{"title": "a dictionary-based approach to racism detection in dutch social media", "abstract": "we present a dictionary-based approach to racism detection in dutch social media comments , which were retrieved from two public belgian social media sites likely to attract racist reactions . these comments were labeled as racist or non-racist by multiple annotators . for our approach , three discourse dictionaries were created : first , we created a dictionary by retrieving possibly racist and more neutral terms from the training data , and then augmenting these with more general words to remove some bias . a second dictionary was created through automatic expansion using a \\texttt { word2vec } model trained on a large corpus of general dutch text . finally , a third dictionary was created by manually filtering out incorrect expansions . we trained multiple support vector machines , using the distribution of words over the different categories in the dictionaries as features . the best-performing model used the manually cleaned dictionary and obtained an f-score of 0.46 for the racist class on a test set consisting of unseen dutch comments , retrieved from the same sites used for the training set . the automated expansion of the dictionary only slightly boosted the model 's performance , and this increase in performance was not statistically significant . the fact that the coverage of the expanded dictionaries did increase indicates that the words that were automatically added did occur in the corpus , but were not able to meaningfully impact performance . the dictionaries , code , and the procedure for requesting the corpus are available at : https : //github.com/clips/hades", "topics": ["test set", "support vector machine"]}
{"title": "inference in hybrid networks : theoretical limits and practical algorithms", "abstract": "an important subclass of hybrid bayesian networks are those that represent conditional linear gaussian ( clg ) distributions -- - a distribution with a multivariate gaussian component for each instantiation of the discrete variables . in this paper we explore the problem of inference in clgs . we show that inference in clgs can be significantly harder than inference in bayes nets . in particular , we prove that even if the clg is restricted to an extremely simple structure of a polytree in which every continuous node has at most one discrete ancestor , the inference task is np-hard.to deal with the often prohibitive computational cost of the exact inference algorithm for clgs , we explore several approximate inference algorithms . these algorithms try to find a small subset of gaussians which are a good approximation to the full mixture distribution . we consider two monte carlo approaches and a novel approach that enumerates mixture components in order of prior probability . we compare these methods on a variety of problems and show that our novel algorithm is very promising for large , hybrid diagnosis problems .", "topics": ["approximation algorithm", "bayesian network"]}
{"title": "learning monocular visual odometry with dense 3d mapping from dense 3d flow", "abstract": "this paper introduces a fully deep learning approach to monocular slam , which can perform simultaneous localization using a neural network for learning visual odometry ( l-vo ) and dense 3d mapping . dense 2d flow and a depth image are generated from monocular images by sub-networks , which are then used by a 3d flow associated layer in the l-vo network to generate dense 3d flow . given this 3d flow , the dual-stream l-vo network can then predict the 6dof relative pose and furthermore reconstruct the vehicle trajectory . in order to learn the correlation between motion directions , the bivariate gaussian modelling is employed in the loss function . the l-vo network achieves an overall performance of 2.68 % for average translational error and 0.0143 deg/m for average rotational error on the kitti odometry benchmark . moreover , the learned depth is fully leveraged to generate a dense 3d map . as a result , an entire visual slam system , that is , learning monocular odometry combined with dense 3d mapping , is achieved .", "topics": ["loss function"]}
{"title": "collaborative prediction with expert advice", "abstract": "many practical learning systems aggregate data across many users , while learning theory traditionally considers a single learner who trusts all of their observations . a case in point is the foundational learning problem of prediction with expert advice . to date , there has been no theoretical study of the general collaborative version of prediction with expert advice , in which many users face a similar problem and would like to share their experiences in order to learn faster . a key issue in this collaborative framework is robustness : generally algorithms that aggregate data are vulnerable to manipulation by even a small number of dishonest users . we exhibit the first robust collaborative algorithm for prediction with expert advice . when all users are honest and have similar tastes our algorithm matches the performance of pooling data and using a traditional algorithm . but our algorithm also guarantees that adding users never significantly degrades performance , even if the additional users behave adversarially . we achieve strong guarantees even when the overwhelming majority of users behave adversarially . as a special case , our algorithm is extremely robust to variation amongst the users .", "topics": ["regret ( decision theory )", "computational complexity theory"]}
{"title": "a probabilistic quality representation approach to deep blind image quality prediction", "abstract": "blind image quality assessment ( biqa ) remains a very challenging problem due to the unavailability of a reference image . deep learning based biqa methods have been attracting increasing attention in recent years , yet it remains a difficult task to train a robust deep biqa model because of the very limited number of training samples with human subjective scores . most existing methods learn a regression network to minimize the prediction error of a scalar image quality score . however , such a scheme ignores the fact that an image will receive divergent subjective scores from different subjects , which can not be adequately represented by a single scalar number . this is particularly true on complex , real-world distorted images . moreover , images may broadly differ in their distributions of assigned subjective scores . recognizing this , we propose a new representation of perceptual image quality , called probabilistic quality representation ( pqr ) , to describe the image subjective score distribution , whereby a more robust loss function can be employed to train a deep biqa model . the proposed pqr method is shown to not only speed up the convergence of deep model training , but to also greatly improve the achievable level of quality prediction accuracy relative to scalar quality score regression methods . the source code is available at https : //github.com/huizeng/biqa_toolbox .", "topics": ["loss function"]}
{"title": "clustering with t-sne , provably", "abstract": "t-distributed stochastic neighborhood embedding ( t-sne ) , a clustering and visualization method proposed by van der maaten & hinton in 2008 , has rapidly become a standard tool in a number of natural sciences . despite its overwhelming success , there is a distinct lack of mathematical foundations and the inner workings of the algorithm are not well understood . the purpose of this paper is to prove that t-sne is able to recover well-separated clusters ; more precisely , we prove that t-sne in the `early exaggeration ' phase , an optimization technique proposed by van der maaten & hinton ( 2008 ) and van der maaten ( 2014 ) , can be rigorously analyzed . as a byproduct , the proof suggests novel ways for setting the exaggeration parameter $ \\alpha $ and step size $ h $ . numerical examples illustrate the effectiveness of these rules : in particular , the quality of embedding of topological structures ( e.g . the swiss roll ) improves . we also discuss a connection to spectral clustering methods .", "topics": ["cluster analysis", "numerical analysis"]}
{"title": "profiling of ocr'ed historical texts revisited", "abstract": "in the absence of ground truth it is not possible to automatically determine the exact spectrum and occurrences of ocr errors in an ocr'ed text . yet , for interactive postcorrection of ocr'ed historical printings it is extremely useful to have a statistical profile available that provides an estimate of error classes with associated frequencies , and that points to conjectured errors and suspicious tokens . the method introduced in reffle ( 2013 ) computes such a profile , combining lexica , pattern sets and advanced matching techniques in a specialized expectation maximization ( em ) procedure . here we improve this method in three respects : first , the method in reffle ( 2013 ) is not adaptive : user feedback obtained by actual postcorrection steps can not be used to compute refined profiles . we introduce a variant of the method that is open for adaptivity , taking correction steps of the user into account . this leads to higher precision with respect to recognition of erroneous ocr tokens . second , during postcorrection often new historical patterns are found . we show that adding new historical patterns to the linguistic background resources leads to a second kind of improvement , enabling even higher precision by telling historical spellings apart from ocr errors . third , the method in reffle ( 2013 ) does not make any active use of tokens that can not be interpreted in the underlying channel model . we show that adding these uninterpretable tokens to the set of conjectured errors leads to a significant improvement of the recall for error detection , at the same time improving precision .", "topics": ["ground truth"]}
{"title": "variational particle approximations", "abstract": "approximate inference in high-dimensional , discrete probabilistic models is a central problem in computational statistics and machine learning . this paper describes discrete particle variational inference ( dpvi ) , a new approach that combines key strengths of monte carlo , variational and search-based techniques . dpvi is based on a novel family of particle-based variational approximations that can be fit using simple , fast , deterministic search techniques . like monte carlo , dpvi can handle multiple modes , and yields exact results in a well-defined limit . like unstructured mean-field , dpvi is based on optimizing a lower bound on the partition function ; when this quantity is not of intrinsic interest , it facilitates convergence assessment and debugging . like both monte carlo and combinatorial search , dpvi can take advantage of factorization , sequential structure , and custom search operators . this paper defines dpvi particle-based approximation family and partition function lower bounds , along with the sequential dpvi and local dpvi algorithm templates for optimizing them . dpvi is illustrated and evaluated via experiments on lattice markov random fields , nonparametric bayesian mixtures and block-models , and parametric as well as non-parametric hidden markov models . results include applications to real-world spike-sorting and relational modeling problems , and show that dpvi can offer appealing time/accuracy trade-offs as compared to multiple alternatives .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "encoding reality : prediction-assisted cortical learning algorithm in hierarchical temporal memory", "abstract": "in the decade since jeff hawkins proposed hierarchical temporal memory ( htm ) as a model of neocortical computation , the theory and the algorithms have evolved dramatically . this paper presents a detailed description of htm 's cortical learning algorithm ( cla ) , including for the first time a rigorous mathematical formulation of all aspects of the computations . prediction assisted cla ( pacla ) , a refinement of the cla is presented , which is both closer to the neuroscience and adds significantly to the computational power . finally , we summarise the key functions of neocortex which are expressed in pacla implementations .", "topics": ["computation"]}
{"title": "learning protein dynamics with metastable switching systems", "abstract": "we introduce a machine learning approach for extracting fine-grained representations of protein evolution from molecular dynamics datasets . metastable switching linear dynamical systems extend standard switching models with a physically-inspired stability constraint . this constraint enables the learning of nuanced representations of protein dynamics that closely match physical reality . we derive an em algorithm for learning , where the e-step extends the forward-backward algorithm for hmms and the m-step requires the solution of large biconvex optimization problems . we construct an approximate semidefinite program solver based on the frank-wolfe algorithm and use it to solve the m-step . we apply our em algorithm to learn accurate dynamics from large simulation datasets for the opioid peptide met-enkephalin and the proto-oncogene src-kinase . our learned models demonstrate significant improvements in temporal coherence over hmms and standard switching models for met-enkephalin , and sample transition paths ( possibly useful in rational drug design ) for src-kinase .", "topics": ["approximation algorithm", "simulation"]}
{"title": "a stochastic finite-state word-segmentation algorithm for chinese", "abstract": "we present a stochastic finite-state model for segmenting chinese text into dictionary entries and productively derived words , and providing pronunciations for these words ; the method incorporates a class-based model in its treatment of personal names . we also evaluate the system 's performance , taking into account the fact that people often do not agree on a single segmentation .", "topics": ["natural language processing", "dictionary"]}
{"title": "trajectory-based radical analysis network for online handwritten chinese character recognition", "abstract": "recently , great progress has been made for online handwritten chinese character recognition due to the emergence of deep learning techniques . however , previous research mostly treated each chinese character as one class without explicitly considering its inherent structure , namely the radical components with complicated geometry . in this study , we propose a novel trajectory-based radical analysis network ( tran ) to firstly identify radicals and analyze two-dimensional structures among radicals simultaneously , then recognize chinese characters by generating captions of them based on the analysis of their internal radicals . the proposed tran employs recurrent neural networks ( rnns ) as both an encoder and a decoder . the rnn encoder makes full use of online information by directly transforming handwriting trajectory into high-level features . the rnn decoder aims at generating the caption by detecting radicals and spatial structures through an attention model . the manner of treating a chinese character as a two-dimensional composition of radicals can reduce the size of vocabulary and enable tran to possess the capability of recognizing unseen chinese character classes , only if the corresponding radicals have been seen . evaluated on casia-olhwdb database , the proposed approach significantly outperforms the state-of-the-art whole-character modeling approach with a relative character error rate ( cer ) reduction of 10 % . meanwhile , for the case of recognition of 500 unseen chinese characters , tran can achieve a character accuracy of about 60 % while the traditional whole-character method has no capability to handle them .", "topics": ["recurrent neural network", "high- and low-level"]}
{"title": "active learning for accurate estimation of linear models", "abstract": "we explore the sequential decision making problem where the goal is to estimate uniformly well a number of linear models , given a shared budget of random contexts independently sampled from a known distribution . the decision maker must query one of the linear models for each incoming context , and receives an observation corrupted by noise levels that are unknown , and depend on the model instance . we present trace-ucb , an adaptive allocation algorithm that learns the noise levels while balancing contexts accordingly across the different linear functions , and derive guarantees for simple regret in both expectation and high-probability . finally , we extend the algorithm and its guarantees to high dimensional settings , where the number of linear models times the dimension of the contextual space is higher than the total budget of samples . simulations with real data suggest that trace-ucb is remarkably robust , outperforming a number of baselines even when its assumptions are violated .", "topics": ["regret ( decision theory )", "simulation"]}
{"title": "compressed learning : a deep neural network approach", "abstract": "compressed learning ( cl ) is a joint signal processing and machine learning framework for inference from a signal , using a small number of measurements obtained by linear projections of the signal . in this paper we present an end-to-end deep learning approach for cl , in which a network composed of fully-connected layers followed by convolutional layers perform the linear sensing and non-linear inference stages . during the training phase , the sensing matrix and the non-linear inference operator are jointly optimized , and the proposed approach outperforms state-of-the-art for the task of image classification . for example , at a sensing rate of 1 % ( only 8 measurements of 28 x 28 pixels images ) , the classification error for the mnist handwritten digits dataset is 6.46 % compared to 41.06 % with state-of-the-art .", "topics": ["mnist database"]}
{"title": "ppr-fcn : weakly supervised visual relation detection via parallel pairwise r-fcn", "abstract": "we aim to tackle a novel vision task called weakly supervised visual relation detection ( wsvrd ) to detect `` subject-predicate-object '' relations in an image with object relation groundtruths available only at the image level . this is motivated by the fact that it is extremely expensive to label the combinatorial relations between objects at the instance level . compared to the extensively studied problem , weakly supervised object detection ( wsod ) , wsvrd is more challenging as it needs to examine a large set of regions pairs , which is computationally prohibitive and more likely stuck in a local optimal solution such as those involving wrong spatial context . to this end , we present a parallel , pairwise region-based , fully convolutional network ( ppr-fcn ) for wsvrd . it uses a parallel fcn architecture that simultaneously performs pair selection and classification of single regions and region pairs for object and relation detection , while sharing almost all computation shared over the entire image . in particular , we propose a novel position-role-sensitive score map with pairwise roi pooling to efficiently capture the crucial context associated with a pair of objects . we demonstrate the superiority of ppr-fcn over all baselines in solving the wsvrd challenge by using results of extensive experiments over two visual relation benchmarks .", "topics": ["object detection", "optimization problem"]}
{"title": "svdnet for pedestrian retrieval", "abstract": "this paper proposes the svdnet for retrieval problems , with focus on the application of person re-identification ( re-id ) . we view each weight vector within a fully connected ( fc ) layer in a convolutional neuron network ( cnn ) as a projection basis . it is observed that the weight vectors are usually highly correlated . this problem leads to correlations among entries of the fc descriptor , and compromises the retrieval performance based on the euclidean distance . to address the problem , this paper proposes to optimize the deep representation learning process with singular vector decomposition ( svd ) . specifically , with the restraint and relaxation iteration ( rri ) training scheme , we are able to iteratively integrate the orthogonality constraint in cnn training , yielding the so-called svdnet . we conduct experiments on the market-1501 , cuhk03 , and duke datasets , and show that rri effectively reduces the correlation among the projection vectors , produces more discriminative fc descriptors , and significantly improves the re-id accuracy . on the market-1501 dataset , for instance , rank-1 accuracy is improved from 55.3 % to 80.5 % for caffenet , and from 73.8 % to 82.3 % for resnet-50 .", "topics": ["feature learning", "iteration"]}
{"title": "on optimality conditions for auto-encoder signal recovery", "abstract": "auto-encoders are unsupervised models that aim to learn patterns from observed data by minimizing a reconstruction cost . the useful representations learned are often found to be sparse and distributed . on the other hand , compressed sensing and sparse coding assume a data generating process , where the observed data is generated from some true latent signal source , and try to recover the corresponding signal from measurements . looking at auto-encoders from this \\textit { signal recovery perspective } enables us to have a more coherent view of these techniques . in this paper , in particular , we show that the \\textit { true } hidden representation can be approximately recovered if the weight matrices are highly incoherent with unit $ \\ell^ { 2 } $ row length and the bias vectors takes the value ( approximately ) equal to the negative of the data mean . the recovery also becomes more and more accurate as the sparsity in hidden signals increases . additionally , we empirically demonstrate that auto-encoders are capable of recovering the data generating dictionary when only data samples are given .", "topics": ["unsupervised learning", "sparse matrix"]}
{"title": "enhancing qpns for trade-off resolution", "abstract": "qualitative probabilistic networks have been introduced as qualitative abstractions of bayesian belief networks . one of the major drawbacks of these qualitative networks is their coarse level of detail , which may lead to unresolved trade-offs during inference . we present an enhanced formalism for qualitative networks with a finer level of detail . an enhanced qualitative probabilistic network differs from a regular qualitative network in that it distinguishes between strong and weak influences . enhanced qualitative probabilistic networks are purely qualitative in nature , as regular qualitative networks are , yet allow for efficiently resolving trade-offs during inference .", "topics": ["bayesian network"]}
{"title": "pixelsnail : an improved autoregressive generative model", "abstract": "autoregressive generative models consistently achieve the best results in density estimation tasks involving high dimensional data , such as images or audio . they pose density estimation as a sequence modeling task , where a recurrent neural network ( rnn ) models the conditional distribution over the next element conditioned on all previous elements . in this paradigm , the bottleneck is the extent to which the rnn can model long-range dependencies , and the most successful approaches rely on causal convolutions , which offer better access to earlier parts of the sequence than conventional rnns . taking inspiration from recent work in meta reinforcement learning , where dealing with long-range dependencies is also essential , we introduce a new generative model architecture that combines causal convolutions with self attention . in this note , we describe the resulting model and present state-of-the-art log-likelihood results on cifar-10 ( 2.85 bits per dim ) and $ 32 \\times 32 $ imagenet ( 3.80 bits per dim ) . our implementation is available at https : //github.com/neocxi/pixelsnail-public", "topics": ["generative model", "recurrent neural network"]}
{"title": "the optimal assignment kernel is not positive definite", "abstract": "we prove that the optimal assignment kernel , proposed recently as an attempt to embed labeled graphs and more generally tuples of basic data to a hilbert space , is in fact not always positive definite .", "topics": ["kernel ( operating system )"]}
{"title": "inverse reinforcement learning via deep gaussian process", "abstract": "we propose a new approach to inverse reinforcement learning ( irl ) based on the deep gaussian process ( deep gp ) model , which is capable of learning complicated reward structures with few demonstrations . our model stacks multiple latent gp layers to learn abstract representations of the state feature space , which is linked to the demonstrations through the maximum entropy learning framework . incorporating the irl engine into the nonlinear latent structure renders existing deep gp inference approaches intractable . to tackle this , we develop a non-standard variational approximation framework which extends previous inference schemes . this allows for approximate bayesian treatment of the feature space and guards against overfitting . carrying out representation and inverse reinforcement learning simultaneously within our model outperforms state-of-the-art approaches , as we demonstrate with experiments on standard benchmarks ( `` object world '' , '' highway driving '' ) and a new benchmark ( `` binary world '' ) .", "topics": ["calculus of variations", "approximation algorithm"]}
{"title": "dealing with class imbalance using thresholding", "abstract": "we propose thresholding as an approach to deal with class imbalance . we define the concept of thresholding as a process of determining a decision boundary in the presence of a tunable parameter . the threshold is the maximum value of this tunable parameter where the conditions of a certain decision are satisfied . we show that thresholding is applicable not only for linear classifiers but also for non-linear classifiers . we show that this is the implicit assumption for many approaches to deal with class imbalance in linear classifiers . we then extend this paradigm beyond linear classification and show how non-linear classification can be dealt with under this umbrella framework of thresholding . the proposed method can be used for outlier detection in many real-life scenarios like in manufacturing . in advanced manufacturing units , where the manufacturing process has matured over time , the number of instances ( or parts ) of the product that need to be rejected ( based on a strict regime of quality tests ) becomes relatively rare and are defined as outliers . how to detect these rare parts or outliers beforehand ? how to detect combination of conditions leading to these outliers ? these are the questions motivating our research . this paper focuses on prediction of outliers and conditions leading to outliers using classification . we address the problem of outlier detection using classification . the classes are good parts ( those passing the quality tests ) and bad parts ( those failing the quality tests and can be considered as outliers ) . the rarity of outliers transforms this problem into a class-imbalanced classification problem .", "topics": ["statistical classification", "nonlinear system"]}
{"title": "unseen class discovery in open-world classification", "abstract": "this paper concerns open-world classification , where the classifier not only needs to classify test examples into seen classes that have appeared in training but also reject examples from unseen or novel classes that have not appeared in training . specifically , this paper focuses on discovering the hidden unseen classes of the rejected examples . clearly , without prior knowledge this is difficult . however , we do have the data from the seen training classes , which can tell us what kind of similarity/difference is expected for examples from the same class or from different classes . it is reasonable to assume that this knowledge can be transferred to the rejected examples and used to discover the hidden unseen classes in them . this paper aims to solve this problem . it first proposes a joint open classification model with a sub-model for classifying whether a pair of examples belongs to the same or different classes . this sub-model can serve as a distance function for clustering to discover the hidden classes of the rejected examples . experimental results show that the proposed model is highly promising .", "topics": ["cluster analysis", "statistical classification"]}
{"title": "long-term visual localization using semantically segmented images", "abstract": "robust cross-seasonal localization is one of the major challenges in long-term visual navigation of autonomous vehicles . in this paper , we exploit recent advances in semantic segmentation of images , i.e . , where each pixel is assigned a label related to the type of object it represents , to attack the problem of long-term visual localization . we show that semantically labeled 3-d point maps of the environment , together with semantically segmented images , can be efficiently used for vehicle localization without the need for detailed feature descriptors ( sift , surf , etc . ) . thus , instead of depending on hand-crafted feature descriptors , we rely on the training of an image segmenter . the resulting map takes up much less storage space compared to a traditional descriptor based map . a particle filter based semantic localization solution is compared to one based on sift-features , and even with large seasonal variations over the year we perform on par with the larger and more descriptive sift-features , and are able to localize with an error below 1 m most of the time .", "topics": ["map", "autonomous car"]}
{"title": "non-autoregressive neural machine translation", "abstract": "existing approaches to neural machine translation condition each output word on previously generated outputs . we introduce a model that avoids this autoregressive property and produces its outputs in parallel , allowing an order of magnitude lower latency during inference . through knowledge distillation , the use of input token fertilities as a latent variable , and policy gradient fine-tuning , we achieve this at a cost of as little as 2.0 bleu points relative to the autoregressive transformer network used as a teacher . we demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy , and validate our approach on iwslt 2016 english-german and two wmt language pairs . by sampling fertilities in parallel at inference time , our non-autoregressive model achieves near-state-of-the-art performance of 29.8 bleu on wmt 2016 english-romanian .", "topics": ["machine translation", "gradient"]}
{"title": "training linear ranking svms in linearithmic time using red-black trees", "abstract": "we introduce an efficient method for training the linear ranking support vector machine . the method combines cutting plane optimization with red-black tree based approach to subgradient calculations , and has o ( m*s+m*log ( m ) ) time complexity , where m is the number of training examples , and s the average number of non-zero features per example . best previously known training algorithms achieve the same efficiency only for restricted special cases , whereas the proposed approach allows any real valued utility scores in the training data . experiments demonstrate the superior scalability of the proposed approach , when compared to the fastest existing ranksvm implementations .", "topics": ["time complexity"]}
{"title": "two-dimensional cellular automata and the analysis of correlated time series", "abstract": "correlated time series are time series that , by virtue of the underlying process to which they refer , are expected to influence each other strongly . we introduce a novel approach to handle such time series , one that models their interaction as a two-dimensional cellular automaton and therefore allows them to be treated as a single entity . we apply our approach to the problems of filling gaps and predicting values in rainfall time series . computational results show that the new approach compares favorably to kalman smoothing and filtering .", "topics": ["time series"]}
{"title": "working hard to know your neighbor 's margins : local descriptor learning loss", "abstract": "we introduce a novel loss for learning local feature descriptors which is inspired by the lowe 's matching criterion for sift . we show that the proposed loss that maximizes the distance between the closest positive and closest negative patch in the batch is better than complex regularization methods ; it works well for both shallow and deep convolution network architectures . applying the novel loss to the l2net cnn architecture results in a compact descriptor -- it has the same dimensionality as sift ( 128 ) that shows state-of-art performance in wide baseline stereo , patch verification and instance retrieval benchmarks . it is fast , computing a descriptor takes about 1 millisecond on a low-end gpu .", "topics": ["baseline ( configuration management )", "matrix regularization"]}
{"title": "deep & cross network for ad click predictions", "abstract": "feature engineering has been the key to the success of many prediction models . however , the process is non-trivial and often requires manual feature engineering or exhaustive searching . dnns are able to automatically learn feature interactions ; however , they generate all the interactions implicitly , and are not necessarily efficient in learning all types of cross features . in this paper , we propose the deep & cross network ( dcn ) which keeps the benefits of a dnn model , and beyond that , it introduces a novel cross network that is more efficient in learning certain bounded-degree feature interactions . in particular , dcn explicitly applies feature crossing at each layer , requires no manual feature engineering , and adds negligible extra complexity to the dnn model . our experimental results have demonstrated its superiority over the state-of-art algorithms on the ctr prediction dataset and dense classification dataset , in terms of both model accuracy and memory usage .", "topics": ["interaction"]}
{"title": "conditional random field and deep feature learning for hyperspectral image segmentation", "abstract": "image segmentation is considered to be one of the critical tasks in hyperspectral remote sensing image processing . recently , convolutional neural network ( cnn ) has established itself as a powerful model in segmentation and classification by demonstrating excellent performances . the use of a graphical model such as a conditional random field ( crf ) contributes further in capturing contextual information and thus improving the segmentation performance . in this paper , we propose a method to segment hyperspectral images by considering both spectral and spatial information via a combined framework consisting of cnn and crf . we use multiple spectral cubes to learn deep features using cnn , and then formulate deep crf with cnn-based unary and pairwise potential functions to effectively extract the semantic correlations between patches consisting of three-dimensional data cubes . effective piecewise training is applied in order to avoid the computationally expensive iterative crf inference . furthermore , we introduce a deep deconvolution network that improves the segmentation masks . we also introduce a new dataset and experimented our proposed method on it along with several widely adopted benchmark datasets to evaluate the effectiveness of our method . by comparing our results with those from several state-of-the-art models , we show the promising potential of our method .", "topics": ["feature learning", "image processing"]}
{"title": "a new approach to content-based file type detection", "abstract": "file type identification and file type clustering may be difficult tasks that have an increasingly importance in the field of computer and network security . classical methods of file type detection including considering file extensions and magic bytes can be easily spoofed . content-based file type detection is a newer way that is taken into account recently . in this paper , a new content-based method for the purpose of file type detection and file type clustering is proposed that is based on the pca and neural networks . the proposed method has a good accuracy and is fast enough .", "topics": ["cluster analysis"]}
{"title": "cell segmentation with random ferns and graph-cuts", "abstract": "the progress in imaging techniques have allowed the study of various aspect of cellular mechanisms . to isolate individual cells in live imaging data , we introduce an elegant image segmentation framework that effectively extracts cell boundaries , even in the presence of poor edge details . our approach works in two stages . first , we estimate pixel interior/border/exterior class probabilities using random ferns . then , we use an energy minimization framework to compute boundaries whose localization is compliant with the pixel class probabilities . we validate our approach on a manually annotated dataset .", "topics": ["image segmentation", "pixel"]}
{"title": "active learning for graph embedding", "abstract": "graph embedding provides an efficient solution for graph analysis by converting the graph into a low-dimensional space which preserves the structure information . in contrast to the graph structure data , the i.i.d . node embedding can be processed efficiently in terms of both time and space . current semi-supervised graph embedding algorithms assume the labelled nodes are given , which may not be always true in the real world . while manually label all training data is inapplicable , how to select the subset of training data to label so as to maximize the graph analysis task performance is of great importance . this motivates our proposed active graph embedding ( age ) framework , in which we design a general active learning query strategy for any semi-supervised graph embedding algorithm . age selects the most informative nodes as the training labelled nodes based on the graphical information ( i.e . , node centrality ) as well as the learnt node embedding ( i.e . , node classification uncertainty and node embedding representativeness ) . different query criteria are combined with the time-sensitive parameters which shift the focus from graph based query criteria to embedding based criteria as the learning progresses . experiments have been conducted on three public data sets and the results verified the effectiveness of each component of our query strategy and the power of combining them using time-sensitive parameters . our code is available online at : https : //github.com/vwz/age .", "topics": ["test set"]}
{"title": "simple and efficient architecture search for convolutional neural networks", "abstract": "neural networks have recently had a lot of success for many tasks . however , neural network architectures that perform well are still typically designed manually by experts in a cumbersome trial-and-error process . we propose a new method to automatically search for well-performing cnn architectures based on a simple hill climbing procedure whose operators apply network morphisms , followed by short optimization runs by cosine annealing . surprisingly , this simple method yields competitive results , despite only requiring resources in the same order of magnitude as training a single network . e.g . , on cifar-10 , our method designs and trains networks with an error rate below 6 % in only 12 hours on a single gpu ; training for one day reduces this error further , to almost 5 % .", "topics": ["neural networks"]}
{"title": "rendering refraction and reflection of eyeglasses for synthetic eye tracker images", "abstract": "while for the evaluation of robustness of eye tracking algorithms the use of real-world data is essential , there are many applications where simulated , synthetic eye images are of advantage . they can generate labelled ground-truth data for appearance based gaze estimation algorithms or enable the development of model based gaze estimation techniques by showing the influence on gaze estimation error of different model factors that can then be simplified or extended . we extend the generation of synthetic eye images by a simulation of refraction and reflection for eyeglasses . on the one hand this allows for the testing of pupil and glint detection algorithms under different illumination and reflection conditions , on the other hand the error of gaze estimation routines can be estimated in conjunction with different eyeglasses . we show how a polynomial function fitting calibration performs equally well with and without eyeglasses , and how a geometrical eye model behaves when exposed to glasses .", "topics": ["synthetic data", "simulation"]}
{"title": "missing data reconstruction in remote sensing image with a unified spatial-temporal-spectral deep convolutional neural network", "abstract": "because of the internal malfunction of satellite sensors and poor atmospheric conditions such as thick cloud , the acquired remote sensing data often suffer from missing information , i.e . , the data usability is greatly reduced . in this paper , a novel method of missing information reconstruction in remote sensing images is proposed . the unified spatial-temporal-spectral framework based on a deep convolutional neural network ( sts-cnn ) employs a unified deep convolutional neural network combined with spatial-temporal-spectral supplementary information . in addition , to address the fact that most methods can only deal with a single missing information reconstruction task , the proposed approach can solve three typical missing information reconstruction tasks : 1 ) dead lines in aqua modis band 6 ; 2 ) the landsat etm+ scan line corrector ( slc ) -off problem ; and 3 ) thick cloud removal . it should be noted that the proposed model can use multi-source data ( spatial , spectral , and temporal ) as the input of the unified framework . the results of both simulated and real-data experiments demonstrate that the proposed model exhibits high effectiveness in the three missing information reconstruction tasks listed above .", "topics": ["simulation", "sensor"]}
{"title": "residual codean autoencoder for facial attribute analysis", "abstract": "facial attributes can provide rich ancillary information which can be utilized for different applications such as targeted marketing , human computer interaction , and law enforcement . this research focuses on facial attribute prediction using a novel deep learning formulation , termed as r-codean autoencoder . the paper first presents cosine similarity based loss function in an autoencoder which is then incorporated into the euclidean distance based autoencoder to formulate r-codean . the proposed loss function thus aims to incorporate both magnitude and direction of image vectors during feature learning . further , inspired by the utility of shortcut connections in deep models to facilitate learning of optimal parameters , without incurring the problem of vanishing gradient , the proposed formulation is extended to incorporate shortcut connections in the architecture . the proposed r-codean autoencoder is utilized in facial attribute prediction framework which incorporates patch-based weighting mechanism for assigning higher weights to relevant patches for each attribute . the experimental results on publicly available celeba and lfwa datasets demonstrate the efficacy of the proposed approach in addressing this challenging problem .", "topics": ["feature learning", "loss function"]}
{"title": "using the gene ontology hierarchy when predicting gene function", "abstract": "the problem of multilabel classification when the labels are related through a hierarchical categorization scheme occurs in many application domains such as computational biology . for example , this problem arises naturally when trying to automatically assign gene function using a controlled vocabularies like gene ontology . however , most existing approaches for predicting gene functions solve independent classification problems to predict genes that are involved in a given function category , independently of the rest . here , we propose two simple methods for incorporating information about the hierarchical nature of the categorization scheme . in the first method , we use information about a gene 's previous annotation to set an initial prior on its label . in a second approach , we extend a graph-based semi-supervised learning algorithm for predicting gene function in a hierarchy . we show that we can efficiently solve this problem by solving a linear system of equations . we compare these approaches with a previous label reconciliation-based approach . results show that using the hierarchy information directly , compared to using reconciliation methods , improves gene function prediction .", "topics": ["supervised learning"]}
{"title": "learning person trajectory representations for team activity analysis", "abstract": "activity analysis in which multiple people interact across a large space is challenging due to the interplay of individual actions and collective group dynamics . we propose an end-to-end approach for learning person trajectory representations for group activity analysis . the learned representations encode rich spatio-temporal dependencies and capture useful motion patterns for recognizing individual events , as well as characteristic group dynamics that can be used to identify groups from their trajectories alone . we develop our deep learning approach in the context of team sports , which provide well-defined sets of events ( e.g . pass , shot ) and groups of people ( teams ) . analysis of events and team formations using nhl hockey and nba basketball datasets demonstrate the generality of our approach .", "topics": ["end-to-end principle"]}
{"title": "confidence estimation in deep neural networks via density modelling", "abstract": "state-of-the-art deep neural networks can be easily fooled into providing incorrect high-confidence predictions for images with small amounts of adversarial noise . does this expose a flaw with deep neural networks , or do we simply need a better way to estimate confidence ? in this paper we consider the problem of accurately estimating predictive confidence . we formulate this problem as that of density modelling , and show how traditional methods such as softmax produce poor estimates . to address this issue , we propose a novel confidence measure based on density modelling approaches . we test these measures on images distorted by blur , jpeg compression , random noise and adversarial noise . experiments show that our confidence measure consistently shows reduced confidence scores in the presence of such distortions - a property which softmax often lacks .", "topics": ["neural networks"]}
{"title": "genetic algorithms for finding the weight enumerator of binary linear block codes", "abstract": "in this paper we present a new method for finding the weight enumerator of binary linear block codes by using genetic algorithms . this method consists in finding the binary weight enumerator of the code and its dual and to create from the famous macwilliams identity a linear system ( s ) of integer variables for which we add all known information obtained from the structure of the code . the knowledge of some subgroups of the automorphism group , under which the code remains invariant , permits to give powerful restrictions on the solutions of ( s ) and to approximate the weight enumerator . by applying this method and by using the stability of the extended quadratic residue codes ( erq ) by the projective special linear group psl2 , we find a list of all possible values of the weight enumerators for the two erq codes of lengths 192 and 200 . we also made a good approximation of the true value for these two enumerators .", "topics": ["approximation algorithm", "approximation"]}
{"title": "on image segmentation using fractional gradients-learning model parameters using approximate marginal inference", "abstract": "estimates of image gradients play a ubiquitous role in image segmentation and classification problems since gradients directly relate to the boundaries or the edges of a scene . this paper proposes an unified approach to gradient estimation based on fractional calculus that is computationally cheap and readily applicable to any existing algorithm that relies on image gradients . we show experiments on edge detection and image segmentation on the stanford backgrounds dataset where these improved local gradients outperforms state of the art , achieving a performance of 79.2 % average accuracy .", "topics": ["image segmentation", "gradient"]}
{"title": "scalable log determinants for gaussian process kernel learning", "abstract": "for applications as varied as bayesian neural networks , determinantal point processes , elliptical graphical models , and kernel learning for gaussian processes ( gps ) , one must compute a log determinant of an $ n \\times n $ positive definite matrix , and its derivatives - leading to prohibitive $ \\mathcal { o } ( n^3 ) $ computations . we propose novel $ \\mathcal { o } ( n ) $ approaches to estimating these quantities from only fast matrix vector multiplications ( mvms ) . these stochastic approximations are based on chebyshev , lanczos , and surrogate models , and converge quickly even for kernel matrices that have challenging spectra . we leverage these approximations to develop a scalable gaussian process approach to kernel learning . we find that lanczos is generally superior to chebyshev for kernel learning , and that a surrogate approach can be highly efficient and accurate with popular kernels .", "topics": ["kernel ( operating system )", "graphical model"]}
{"title": "visually-aware fashion recommendation and design with generative image models", "abstract": "building effective recommender systems for domains like fashion is challenging due to the high level of subjectivity and the semantic complexity of the features involved ( i.e . , fashion styles ) . recent work has shown that approaches to `visual ' recommendation ( e.g.~clothing , art , etc . ) can be made more accurate by incorporating visual signals directly into the recommendation objective , using `off-the-shelf ' feature representations derived from deep networks . here , we seek to extend this contribution by showing that recommendation performance can be significantly improved by learning `fashion aware ' image representations directly , i.e . , by training the image representation ( from the pixel level ) and the recommender system jointly ; this contribution is related to recent work using siamese cnns , though we are able to show improvements over state-of-the-art recommendation techniques such as bpr and variants that make use of pre-trained visual features . furthermore , we show that our model can be used \\emph { generatively } , i.e . , given a user and a product category , we can generate new images ( i.e . , clothing items ) that are most consistent with their personal taste . this represents a first step towards building systems that go beyond recommending existing items from a product corpus , but which can be used to suggest styles and aid the design of new products .", "topics": ["generative model", "pixel"]}
{"title": "ikbt : solving closed-form inverse kinematics with behavior tree", "abstract": "serial robot arms have complicated kinematic equations which must be solved to write effective arm planning and control software ( the inverse kinematics problem ) . existing software packages for inverse kinematics often rely on numerical methods which have significant shortcomings . here we report a new symbolic inverse kinematics solver which overcomes the limitations of numerical methods , and the shortcomings of previous symbolic software packages . we integrate behavior trees , an execution planning framework previously used for controlling intelligent robot behavior , to organize the equation solving process , and a modular architecture for each solution technique . the system successfully solved , generated a latex report , and generated a python code template for 18 out of 19 example robots of 4-6 dof . the system is readily extensible , maintainable , and multi-platform with few dependencies . the complete package is available with a modified bsd license on github .", "topics": ["numerical analysis", "robot"]}
{"title": "clusters , graphs , and networks for analysing internet-web-supported communication within a virtual community", "abstract": "the proposal is to use clusters , graphs and networks as models in order to analyse the web structure . clusters , graphs and networks provide knowledge representation and organization . clusters were generated by co-site analysis . the sample is a set of academic web sites from the countries belonging to the european union . these clusters are here revisited from the point of view of graph theory and social network analysis . this is a quantitative and structural analysis . in fact , the internet is a computer network that connects people and organizations . thus we may consider it to be a social network . the set of web academic sites represents an empirical social network , and is viewed as a virtual community . the network structural properties are here analysed applying together cluster analysis , graph theory and social network analysis .", "topics": ["cluster analysis"]}
{"title": "deep multi-instance transfer learning", "abstract": "we present a new approach for transferring knowledge from groups to individuals that comprise them . we evaluate our method in text , by inferring the ratings of individual sentences using full-review ratings . this approach , which combines ideas from transfer learning , deep learning and multi-instance learning , reduces the need for laborious human labelling of fine-grained data when abundant labels are available at the group level .", "topics": ["reinforcement learning"]}
{"title": "on optimal generalizability in parametric learning", "abstract": "we consider the parametric learning problem , where the objective of the learner is determined by a parametric loss function . employing empirical risk minimization with possibly regularization , the inferred parameter vector will be biased toward the training samples . such bias is measured by the cross validation procedure in practice where the data set is partitioned into a training set used for training and a validation set , which is not used in training and is left to measure the out-of-sample performance . a classical cross validation strategy is the leave-one-out cross validation ( loocv ) where one sample is left out for validation and training is done on the rest of the samples that are presented to the learner , and this process is repeated on all of the samples . loocv is rarely used in practice due to the high computational complexity . in this paper , we first develop a computationally efficient approximate loocv ( aloocv ) and provide theoretical guarantees for its performance . then we use aloocv to provide an optimization algorithm for finding the regularizer in the empirical risk minimization framework . in our numerical experiments , we illustrate the accuracy and efficiency of aloocv as well as our proposed framework for the optimization of the regularizer .", "topics": ["test set", "approximation algorithm"]}
{"title": "a route confidence evaluation method for reliable hierarchical text categorization", "abstract": "hierarchical text categorization ( htc ) is becoming increasingly important with the rapidly growing amount of text data available in the world wide web . among the different strategies proposed to cope with htc , the local classifier per node ( lcn ) approach attains good performance by mirroring the underlying class hierarchy while enforcing a top-down strategy in the testing step . however , the problem of embedding hierarchical information ( parent-child relationship ) to improve the performance of htc systems still remains open . a confidence evaluation method for a selected route in the hierarchy is proposed to evaluate the reliability of the final candidate labels in an htc system . in order to take into account the information embedded in the hierarchy , weight factors are used to take into account the importance of each level . an acceptance/rejection strategy in the top-down decision making process is proposed , which improves the overall categorization accuracy by rejecting a few percentage of samples , i.e . , those with low reliability score . experimental results on the reuters benchmark dataset ( rcv1- v2 ) confirm the effectiveness of the proposed method , compared to other state-of-the art htc methods .", "topics": ["text corpus"]}
{"title": "visual based navigation of mobile robots", "abstract": "we have developed an algorithm to generate a complete map of the traversable region for a personal assistant robot using monocular vision only . using multiple taken by a simple webcam , obstacle detection and avoidance algorithms have been developed . simple linear iterative clustering ( slic ) has been used for segmentation to reduce the memory and computation cost . a simple mapping technique using inverse perspective mapping and occupancy grids , which is robust , and supports very fast updates has been used to create the map for indoor navigation .", "topics": ["computation"]}
{"title": "signal reconstruction framework based on projections onto epigraph set of a convex cost function ( pesc )", "abstract": "a new signal processing framework based on making orthogonal projections onto the epigraph set of a convex cost function ( pesc ) is developed . in this way it is possible to solve convex optimization problems using the well-known projections onto convex set ( pocs ) approach . in this algorithm , the dimension of the minimization problem is lifted by one and a convex set corresponding to the epigraph of the cost function is defined . if the cost function is a convex function in $ r^n $ , the corresponding epigraph set is also a convex set in r^ { n+1 } . the pesc method provides globally optimal solutions for total-variation ( tv ) , filtered variation ( fv ) , l_1 , l_2 , and entropic cost function based convex optimization problems . in this article , the pesc based denoising and compressive sensing algorithms are developed . simulation examples are presented .", "topics": ["optimization problem", "loss function"]}
{"title": "a model for interpreting social interactions in local image regions", "abstract": "understanding social interactions ( such as 'hug ' or 'fight ' ) is a basic and important capacity of the human visual system , but a challenging and still open problem for modeling . in this work we study visual recognition of social interactions , based on small but recognizable local regions . the approach is based on two novel key components : ( i ) a given social interaction can be recognized reliably from reduced images ( called 'minimal images ' ) . ( ii ) the recognition of a social interaction depends on identifying components and relations within the minimal image ( termed 'interpretation ' ) . we show psychophysics data for minimal images and modeling results for their interpretation . we discuss the integration of minimal configurations in recognizing social interactions in a detailed , high-resolution image .", "topics": ["interaction", "computer vision"]}
{"title": "solving sdps for synchronization and maxcut problems via the grothendieck inequality", "abstract": "a number of statistical estimation problems can be addressed by semidefinite programs ( sdp ) . while sdps are solvable in polynomial time using interior point methods , in practice generic sdp solvers do not scale well to high-dimensional problems . in order to cope with this problem , burer and monteiro proposed a non-convex rank-constrained formulation , which has good performance in practice but is still poorly understood theoretically . in this paper we study the rank-constrained version of sdps arising in maxcut and in synchronization problems . we establish a grothendieck-type inequality that proves that all the local maxima and dangerous saddle points are within a small multiplicative gap from the global maximum . we use this structural information to prove that sdps can be solved within a known accuracy , by applying the riemannian trust-region method to this non-convex problem , while constraining the rank to be of order one . for the maxcut problem , our inequality implies that any local maximizer of the rank-constrained sdp provides a $ ( 1 - 1/ ( k-1 ) ) \\times 0.878 $ approximation of the maxcut , when the rank is fixed to $ k $ . we then apply our results to data matrices generated according to the gaussian $ { \\mathbb z } _2 $ synchronization problem , and the two-groups stochastic block model with large bounded degree . we prove that the error achieved by local maximizers undergoes a phase transition at the same threshold as for information-theoretically optimal methods .", "topics": ["time complexity"]}
{"title": "graph-community detection for cross-document topic segment relationship identification", "abstract": "in this paper we propose a graph-community detection approach to identify cross-document relationships at the topic segment level . given a set of related documents , we automatically find these relationships by clustering segments with similar content ( topics ) . in this context , we study how different weighting mechanisms influence the discovery of word communities that relate to the different topics found in the documents . finally , we test different mapping functions to assign topic segments to word communities , determining which topic segments are considered equivalent . by performing this task it is possible to enable efficient multi-document browsing , since when a user finds relevant content in one document we can provide access to similar topics in other documents . we deploy our approach in two different scenarios . one is an educational scenario where equivalence relationships between learning materials need to be found . the other consists of a series of dialogs in a social context where students discuss commonplace topics . results show that our proposed approach better discovered equivalence relationships in learning material documents and obtained close results in the social speech domain , where the best performing approach was a clustering technique .", "topics": ["cluster analysis"]}
{"title": "face recognition using principal component analysis and log-gabor filters", "abstract": "in this article we propose a novel face recognition method based on principal component analysis ( pca ) and log-gabor filters . the main advantages of the proposed method are its simple implementation , training , and very high recognition accuracy . for recognition experiments we used 5151 face images of 1311 persons from different sets of the feret and ar databases that allow to analyze how recognition accuracy is affected by the change of facial expressions , illumination , and aging . recognition experiments with the feret database ( containing photographs of 1196 persons ) showed that our method can achieve maximal 97-98 % first one recognition rate and 0.3-0.4 % equal error rate . the experiments also showed that the accuracy of our method is less affected by eye location errors and used image normalization method than of traditional pca -based recognition method .", "topics": ["database"]}
{"title": "calculating entropy at different scales among diverse communication systems", "abstract": "we evaluated the impact of changing the observation scale over the entropy measures for text descriptions . midi coded music , computer code and two human natural languages were studied at the scale of characters , words , and at the fundamental scale resulting from adjusting the symbols length used to interpret each text-description until it produced minimum entropy . the results show that the fundamental scale method is comparable with the use of words when measuring entropy levels in written texts . however , this method can also be used in communication systems lacking words such as music . measuring symbolic entropy at the fundamental scale allows to calculate quantitatively , relative levels of complexity for different communication systems . the results open novel vision on differences among the structure of the communication systems studied .", "topics": ["natural language"]}
{"title": "spatial phase-sweep : increasing temporal resolution of transient imaging using a light source array", "abstract": "transient imaging or light-in-flight techniques capture the propagation of an ultra-short pulse of light through a scene , which in effect captures the optical impulse response of the scene . recently , it has been shown that we can capture transient images using commercially available time-of-flight ( tof ) systems such as photonic mixer devices ( pmd ) . in this paper , we propose `spatial phase-sweep ' , a technique that exploits the speed of light to increase the temporal resolution beyond the 100 picosecond limit imposed by current electronics . spatial phase-sweep uses a linear array of light sources with spatial separation of about 3 mm between them , thereby resulting in a time shift of about 10 picoseconds , which translates into 100 gfps of transient imaging in theory . we demonstrate a prototype and transient imaging results using spatial phase-sweep .", "topics": ["sensor"]}
{"title": "value-directed sampling methods for pomdps", "abstract": "we consider the problem of approximate belief-state monitoring using particle filtering for the purposes of implementing a policy for a partially-observable markov decision process ( pomdp ) . while particle filtering has become a widely-used tool in ai for monitoring dynamical systems , rather scant attention has been paid to their use in the context of decision making . assuming the existence of a value function , we derive error bounds on decision quality associated with filtering using importance sampling . we also describe an adaptive procedure that can be used to dynamically determine the number of samples required to meet specific error bounds . empirical evidence is offered supporting this technique as a profitable means of directing sampling effort where it is needed to distinguish policies .", "topics": ["approximation algorithm"]}
{"title": "challenges in disentangling independent factors of variation", "abstract": "we study the problem of building models that disentangle independent factors of variation . such models could be used to encode features that can efficiently be used for classification and to transfer attributes between different images in image synthesis . as data we use a weakly labeled training set . our weak labels indicate what single factor has changed between two data samples , although the relative value of the change is unknown . this labeling is of particular interest as it may be readily available without annotation costs . to make use of weak labels we introduce an autoencoder model and train it through constraints on image pairs and triplets . we formally prove that without additional knowledge there is no guarantee that two images with the same factor of variation will be mapped to the same feature . we call this issue the reference ambiguity . moreover , we show the role of the feature dimensionality and adversarial training . we demonstrate experimentally that the proposed model can successfully transfer attributes on several datasets , but show also cases when the reference ambiguity occurs .", "topics": ["value ( ethics )", "autoencoder"]}
{"title": "resolution of difficult pronouns using the ross method", "abstract": "a new natural language understanding method for disambiguation of difficult pronouns is described . difficult pronouns are those pronouns for which a level of world or domain knowledge is needed in order to perform anaphoral or other types of resolution . resolution of difficult pronouns may in some cases require a prior step involving the application of inference to a situation that is represented by the natural language text . a general method is described : it performs entity resolution and pronoun resolution . an extension to the general pronoun resolution method performs inference as an embedded commonsense reasoning method . the general method and the embedded method utilize features of the ross representational scheme ; in particular the methods use ross ontology classes and the ross situation model . the overall method is a working solution that solves the following winograd schemas : a ) trophy and suitcase , b ) person lifts person , c ) person pays detective , and d ) councilmen and demonstrators .", "topics": ["natural language"]}
{"title": "online meta-learning by parallel algorithm competition", "abstract": "the efficiency of reinforcement learning algorithms depends critically on a few meta-parameters that modulates the learning updates and the trade-off between exploration and exploitation . the adaptation of the meta-parameters is an open question in reinforcement learning , which arguably has become more of an issue recently with the success of deep reinforcement learning in high-dimensional state spaces . the long learning times in domains such as atari 2600 video games makes it not feasible to perform comprehensive searches of appropriate meta-parameter values . we propose the online meta-learning by parallel algorithm competition ( ompac ) method . in the ompac method , several instances of a reinforcement learning algorithm are run in parallel with small differences in the initial values of the meta-parameters . after a fixed number of episodes , the instances are selected based on their performance in the task at hand . before continuing the learning , gaussian noise is added to the meta-parameters with a predefined probability . we validate the ompac method by improving the state-of-the-art results in stochastic sz-tetris and in standard tetris with a smaller , 10 $ \\times $ 10 , board , by 31 % and 84 % , respectively , and by improving the results for deep sarsa ( $ \\lambda $ ) agents in three atari 2600 games by 62 % or more . the experiments also show the ability of the ompac method to adapt the meta-parameters according to the learning progress in different tasks .", "topics": ["reinforcement learning"]}
{"title": "evaluation of session-based recommendation algorithms", "abstract": "recommender systems help users find relevant items of interest , for example on e-commerce or media streaming sites . most academic research is concerned with approaches that personalize the recommendations according to long-term user profiles . in many real-world applications , however , such long-term profiles often do not exist and recommendations therefore have to be made solely based on the observed behavior of a user during an ongoing session . given the high practical relevance of the problem , an increased interest in this problem can be observed in recent years , leading to a number of proposals for session-based recommendation algorithms that typically aim to predict the user 's immediate next actions . in this work , we present the results of an in-depth performance comparison of a number of such algorithms , using a variety of datasets and evaluation measures . our comparison includes the most recent approaches based on recurrent neural networks like gru4rec , factorized markov model approaches such as fism or fossil , as well as more simple methods based , e.g . , on nearest neighbor schemes . our experiments reveal that algorithms of this latter class , despite their sometimes almost trivial nature , often perform equally well or significantly better than today 's more complex approaches based on deep neural networks . our results therefore suggest that there is substantial room for improvement regarding the development of more sophisticated session-based recommendation algorithms .", "topics": ["recurrent neural network", "relevance"]}
{"title": "exploring context with deep structured models for semantic segmentation", "abstract": "state-of-the-art semantic image segmentation methods are mostly based on training deep convolutional neural networks ( cnns ) . in this work , we proffer to improve semantic segmentation with the use of contextual information . in particular , we explore `patch-patch ' context and `patch-background ' context in deep cnns . we formulate deep structured models by combining cnns and conditional random fields ( crfs ) for learning the patch-patch context between image regions . specifically , we formulate cnn-based pairwise potential functions to capture semantic correlations between neighboring patches . efficient piecewise training of the proposed deep structured model is then applied in order to avoid repeated expensive crf inference during the course of back propagation . for capturing the patch-background context , we show that a network design with traditional multi-scale image inputs and sliding pyramid pooling is very effective for improving performance . we perform comprehensive evaluation of the proposed method . we achieve new state-of-the-art performance on a number of challenging semantic segmentation datasets including $ nyudv2 $ , $ pascal $ - $ voc2012 $ , $ cityscapes $ , $ pascal $ - $ context $ , $ sun $ - $ rgbd $ , $ sift $ - $ flow $ , and $ kitti $ datasets . particularly , we report an intersection-over-union score of $ 77.8 $ on the $ pascal $ - $ voc2012 $ dataset .", "topics": ["image segmentation"]}
{"title": "face recognition using optimal representation ensemble", "abstract": "recently , the face recognizers based on linear representations have been shown to deliver state-of-the-art performance . in real-world applications , however , face images usually suffer from expressions , disguises and random occlusions . the problematic facial parts undermine the validity of the linear-subspace assumption and thus the recognition performance deteriorates significantly . in this work , we address the problem in a learning-inference-mixed fashion . by observing that the linear-subspace assumption is more reliable on certain face patches rather than on the holistic face , some bayesian patch representations ( bprs ) are randomly generated and interpreted according to the bayes ' theory . we then train an ensemble model over the patch-representations by minimizing the empirical risk w.r.t the `` leave-one-out margins '' . the obtained model is termed optimal representation ensemble ( ore ) , since it guarantees the optimality from the perspective of empirical risk minimization . to handle the unknown patterns in test faces , a robust version of bpr is proposed by taking the non-face category into consideration . equipped with the robust-bprs , the inference ability of ore is increased dramatically and several record-breaking accuracies ( 99.9 % on yale-b and 99.5 % on ar ) and desirable efficiencies ( below 20 ms per face in matlab ) are achieved . it also overwhelms other modular heuristics on the faces with random occlusions , extreme expressions and disguises . furthermore , to accommodate immense bprs sets , a boosting-like algorithm is also derived . the boosted model , a.k.a boosted-ore , obtains similar performance to its prototype . besides the empirical superiorities , two desirable features of the proposed methods , namely , the training-determined model-selection and the data-weight-free boosting procedure , are also theoretically verified .", "topics": ["heuristic"]}
{"title": "medical image compression using wavelet decomposition for prediction method", "abstract": "in this paper offers a simple and lossless compression method for compression of medical images . method is based on wavelet decomposition of the medical images followed by the correlation analysis of coefficients . the correlation analyses are the basis of prediction equation for each sub band . predictor variable selection is performed through coefficient graphic method to avoid multicollinearity problem and to achieve high prediction accuracy and compression rate . the method is applied on mri and ct images . results show that the proposed approach gives a high compression rate for mri and ct images comparing with state of the art methods .", "topics": ["coefficient"]}
{"title": "the web as a knowledge-base for answering complex questions", "abstract": "answering complex questions is a time-consuming activity for humans that requires reasoning and integration of information . recent work on reading comprehension made headway in answering simple questions , but tackling complex questions is still an ongoing research challenge . conversely , semantic parsers have been successful at handling compositionality , but only when the information resides in a target knowledge-base . in this paper , we present a novel framework for answering broad and complex questions , assuming answering simple questions is possible using a search engine and a reading comprehension model . we propose to decompose complex questions into a sequence of simple questions , and compute the final answer from the sequence of answers . to illustrate the viability of our approach , we create a new dataset of complex questions , complexwebquestions , and present a model that decomposes questions and interacts with the web to compute an answer . we empirically demonstrate that question decomposition improves performance from 20.8 precision @ 1 to 27.5 precision @ 1 on this new dataset .", "topics": ["parsing"]}
{"title": "recovering hard-to-find object instances by sampling context-based object proposals", "abstract": "in this paper we focus on improving object detection performance in terms of recall . we propose a post-detection stage during which we explore the image with the objective of recovering missed detections . this exploration is performed by sampling object proposals in the image . we analyze four different strategies to perform this sampling , giving special attention to strategies that exploit spatial relations between objects . in addition , we propose a novel method to discover higher-order relations between groups of objects . experiments on the challenging kitti dataset show that our proposed relations-based proposal generation strategies can help improving recall at the cost of a relatively low amount of object proposals .", "topics": ["sampling ( signal processing )", "object detection"]}
{"title": "online inference for relation extraction with a reduced feature set", "abstract": "access to web-scale corpora is gradually bringing robust automatic knowledge base creation and extension within reach . to exploit these large unannotated -- -and extremely difficult to annotate -- -corpora , unsupervised machine learning methods are required . probabilistic models of text have recently found some success as such a tool , but scalability remains an obstacle in their application , with standard approaches relying on sampling schemes that are known to be difficult to scale . in this report , we therefore present an empirical assessment of the sublinear time sparse stochastic variational inference ( ssvi ) scheme applied to rellda . we demonstrate that online inference leads to relatively strong qualitative results but also identify some of its pathologies -- -and those of the model -- -which will need to be overcome if ssvi is to be used for large-scale relation extraction .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "specific-to-general learning for temporal events with application to learning event definitions from video", "abstract": "we develop , analyze , and evaluate a novel , supervised , specific-to-general learner for a simple temporal logic and use the resulting algorithm to learn visual event definitions from video sequences . first , we introduce a simple , propositional , temporal , event-description language called ama that is sufficiently expressive to represent many events yet sufficiently restrictive to support learning . we then give algorithms , along with lower and upper complexity bounds , for the subsumption and generalization problems for ama formulas . we present a positive-examples -- only specific-to-general learning method based on these algorithms . we also present a polynomial-time -- computable `` syntactic '' subsumption test that implies semantic subsumption without being equivalent to it . a generalization algorithm based on syntactic subsumption can be used in place of semantic generalization to improve the asymptotic complexity of the resulting learning algorithm . finally , we apply this algorithm to the task of learning relational event definitions from video and show that it yields definitions that are competitive with hand-coded ones .", "topics": ["computational complexity theory"]}
{"title": "revisiting k-means : new algorithms via bayesian nonparametrics", "abstract": "bayesian models offer great flexibility for clustering applications -- -bayesian nonparametrics can be used for modeling infinite mixtures , and hierarchical bayesian models can be utilized for sharing clusters across multiple data sets . for the most part , such flexibility is lacking in classical clustering methods such as k-means . in this paper , we revisit the k-means clustering algorithm from a bayesian nonparametric viewpoint . inspired by the asymptotic connection between k-means and mixtures of gaussians , we show that a gibbs sampling algorithm for the dirichlet process mixture approaches a hard clustering algorithm in the limit , and further that the resulting algorithm monotonically minimizes an elegant underlying k-means-like clustering objective that includes a penalty for the number of clusters . we generalize this analysis to the case of clustering multiple data sets through a similar asymptotic argument with the hierarchical dirichlet process . we also discuss further extensions that highlight the benefits of our analysis : i ) a spectral relaxation involving thresholded eigenvectors , and ii ) a normalized cut graph clustering algorithm that does not fix the number of clusters in the graph .", "topics": ["sampling ( signal processing )", "cluster analysis"]}
{"title": "constructing situation specific belief networks", "abstract": "this paper describes a process for constructing situation-specific belief networks from a knowledge base of network fragments . a situation-specific network is a minimal query complete network constructed from a knowledge base in response to a query for the probability distribution on a set of target variables given evidence and context variables . we present definitions of query completeness and situation-specific networks . we describe conditions on the knowledge base that guarantee query completeness . the relationship of our work to earlier work on kbmc is also discussed .", "topics": ["bayesian network"]}
{"title": "learning with algebraic invariances , and the invariant kernel trick", "abstract": "when solving data analysis problems it is important to integrate prior knowledge and/or structural invariances . this paper contributes by a novel framework for incorporating algebraic invariance structure into kernels . in particular , we show that algebraic properties such as sign symmetries in data , phase independence , scaling etc . can be included easily by essentially performing the kernel trick twice . we demonstrate the usefulness of our theory in simulations on selected applications such as sign-invariant spectral clustering and underdetermined ica .", "topics": ["cluster analysis", "simulation"]}
{"title": "mbmf : model-based priors for model-free reinforcement learning", "abstract": "reinforcement learning is divided in two main paradigms : model-free and model-based . each of these two paradigms has strengths and limitations , and has been successfully applied to real world domains that are appropriate to its corresponding strengths . in this paper , we present a new approach aimed at bridging the gap between these two paradigms . we aim to take the best of the two paradigms and combine them in an approach that is at the same time data-efficient and cost-savvy . we do so by learning a probabilistic dynamics model and leveraging it as a prior for the intertwined model-free optimization . as a result , our approach can exploit the generality and structure of the dynamics model , but is also capable of ignoring its inevitable inaccuracies , by directly incorporating the evidence provided by the direct observation of the cost . preliminary results demonstrate that our approach outperforms purely model-based and model-free approaches , as well as the approach of simply switching from a model-based to a model-free setting .", "topics": ["reinforcement learning", "simulation"]}
{"title": "learning inference models for computer vision", "abstract": "computer vision can be understood as the ability to perform inference on image data . breakthroughs in computer vision technology are often marked by advances in inference techniques . this thesis proposes novel inference schemes and demonstrates applications in computer vision . we propose inference techniques for both generative and discriminative vision models . the use of generative models in vision is often hampered by the difficulty of posterior inference . we propose techniques for improving inference in mcmc sampling and message-passing inference . our inference strategy is to learn separate discriminative models that assist bayesian inference in a generative model . experiments on a range of generative models show that the proposed techniques accelerate the inference process and/or converge to better solutions . a main complication in the design of discriminative models is the inclusion of prior knowledge . we concentrate on cnn models and propose a generalization of standard spatial convolutions to bilateral convolutions . we generalize the existing use of bilateral filters and then propose new neural network architectures with learnable bilateral filters , which we call `bilateral neural networks ' . experiments demonstrate the use of the bilateral networks on a wide range of image and video tasks and datasets . in summary , we propose techniques for better inference in several vision models ranging from inverse graphics to freely parameterized neural networks . in generative models , our inference techniques alleviate some of the crucial hurdles in bayesian posterior inference , paving new ways for the use of model based machine learning in vision . in discriminative cnn models , the proposed filter generalizations aid in the design of new neural network architectures that can handle sparse high-dimensional data as well as provide a way to incorporate prior knowledge into cnns .", "topics": ["generative model", "image segmentation"]}
{"title": "optimal resampling for the noisy onemax problem", "abstract": "the onemax problem is a standard benchmark optimisation problem for a binary search space . recent work on applying a bandit-based random mutation hill-climbing algorithm to the noisy onemax problem showed that it is important to choose a good value for the resampling number to make a careful trade off between taking more samples in order to reduce noise , and taking fewer samples to reduce the total computational cost . this paper extends that observation , by deriving an analytical expression for the running time of the rmhc algorithm with resampling applied to the noisy onemax problem , and showing both theoretically and empirically that the optimal resampling number increases with the number of dimensions in the search space .", "topics": ["time complexity", "mathematical optimization"]}
{"title": "deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations", "abstract": "we propose a new algorithm for solving parabolic partial differential equations ( pdes ) and backward stochastic differential equations ( bsdes ) in high dimension , by making an analogy between the bsde and reinforcement learning with the gradient of the solution playing the role of the policy function , and the loss function given by the error between the prescribed terminal condition and the solution of the bsde . the policy function is then approximated by a neural network , as is done in deep reinforcement learning . numerical results using tensorflow illustrate the efficiency and accuracy of the proposed algorithms for several 100-dimensional nonlinear pdes from physics and finance such as the allen-cahn equation , the hamilton-jacobi-bellman equation , and a nonlinear pricing model for financial derivatives .", "topics": ["nonlinear system", "reinforcement learning"]}
{"title": "annealed map", "abstract": "maximum a posteriori assignment ( map ) is the problem of finding the most probable instantiation of a set of variables given the partial evidence on the other variables in a bayesian network . map has been shown to be a np-hard problem [ 22 ] , even for constrained networks , such as polytrees [ 18 ] . hence , previous approaches often fail to yield any results for map problems in large complex bayesian networks . to address this problem , we propose annealedmap algorithm , a simulated annealing-based map algorithm . the annealedmap algorithm simulates a non-homogeneous markov chain whose invariant function is a probability density that concentrates itself on the modes of the target density . we tested this algorithm on several real bayesian networks . the results show that , while maintaining good quality of the map solutions , the annealedmap algorithm is also able to solve many problems that are beyond the reach of previous approaches .", "topics": ["simulation", "bayesian network"]}
{"title": "phoneme recognition in timit with blstm-ctc", "abstract": "we compare the performance of a recurrent neural network with the best results published so far on phoneme recognition in the timit database . these published results have been obtained with a combination of classifiers . however , in this paper we apply a single recurrent neural network to the same task . our recurrent neural network attains an error rate of 24.6 % . this result is not significantly different from that obtained by the other best methods , but they rely on a combination of classifiers for achieving comparable performance .", "topics": ["recurrent neural network"]}
{"title": "identifying metastases in sentinel lymph nodes with deep convolutional neural networks", "abstract": "metastatic presence in lymph nodes is one of the most important prognostic variables of breast cancer . the current diagnostic procedure for manually reviewing sentinel lymph nodes , however , is very time-consuming and subjective . pathologists have to manually scan an entire digital whole-slide image ( wsi ) for regions of metastasis that are sometimes only detectable under high resolution or entirely hidden from the human visual cortex . from october 2015 to april 2016 , the international symposium on biomedical imaging ( isbi ) held the camelyon grand challenge 2016 to crowd-source ideas and algorithms for automatic detection of lymph node metastasis . using a generalizable stain normalization technique and the proscia pathology cloud computing platform , we trained a deep convolutional neural network on millions of tissue and tumor image tiles to perform slide-based evaluation on our testing set of whole-slide images images , with a sensitivity of 0.96 , specificity of 0.89 , and auc score of 0.90 . our results indicate that our platform can automatically scan any wsi for metastatic regions without institutional calibration to respective stain profiles .", "topics": ["neural networks"]}
{"title": "a new model for cerebellar computation", "abstract": "the standard state space model is widely believed to account for the cerebellar computation in motor adaptation tasks [ 1 ] . here we show that several recent experiments [ 2-4 ] where the visual feedback is irrelevant to the motor response challenge the standard model . furthermore , we propose a new model that accounts for the the results presented in [ 2-4 ] . according to this new model , learning and forgetting are coupled and are error size dependent . we also show that under reasonable assumptions , our proposed model is the only model that accounts for both the classical adaptation paradigm as well as the recent experiments [ 2-4 ] .", "topics": ["computation", "relevance"]}
{"title": "suppressing the unusual : towards robust cnns using symmetric activation functions", "abstract": "many deep convolutional neural networks ( cnn ) make incorrect predictions on adversarial samples obtained by imperceptible perturbations of clean samples . we hypothesize that this is caused by a failure to suppress unusual signals within network layers . as remedy we propose the use of symmetric activation functions ( saf ) in non-linear signal transducer units . these units suppress signals of exceptional magnitude . we prove that saf networks can perform classification tasks to arbitrary precision in a simplified situation . in practice , rather than use safs alone , we add them into cnns to improve their robustness . the modified cnns can be easily trained using popular strategies with the moderate training load . our experiments on mnist and cifar-10 show that the modified cnns perform similarly to plain ones on clean samples , and are remarkably more robust against adversarial and nonsense samples .", "topics": ["nonlinear system", "mnist database"]}
{"title": "an online structural plasticity rule for generating better reservoirs", "abstract": "in this article , a novel neuro-inspired low-resolution online unsupervised learning rule is proposed to train the reservoir or liquid of liquid state machine . the liquid is a sparsely interconnected huge recurrent network of spiking neurons . the proposed learning rule is inspired from structural plasticity and trains the liquid through formation and elimination of synaptic connections . hence , the learning involves rewiring of the reservoir connections similar to structural plasticity observed in biological neural networks . the network connections can be stored as a connection matrix and updated in memory by using address event representation ( aer ) protocols which are generally employed in neuromorphic systems . on investigating the 'pairwise separation property ' we find that trained liquids provide 1.36 $ \\pm $ 0.18 times more inter-class separation while retaining similar intra-class separation as compared to random liquids . moreover , analysis of the 'linear separation property ' reveals that trained liquids are 2.05 $ \\pm $ 0.27 times better than random liquids . furthermore , we show that our liquids are able to retain the 'generalization ' ability and 'generality ' of random liquids . a memory analysis shows that trained liquids have 83.67 $ \\pm $ 5.79 ms longer fading memory than random liquids which have shown 92.8 $ \\pm $ 5.03 ms fading memory for a particular type of spike train inputs . we also throw some light on the dynamics of the evolution of recurrent connections within the liquid . moreover , compared to 'separation driven synaptic modification ' - a recently proposed algorithm for iteratively refining reservoirs , our learning rule provides 9.30 % , 15.21 % and 12.52 % more liquid separations and 2.8 % , 9.1 % and 7.9 % better classification accuracies for four , eight and twelve class pattern recognition tasks respectively .", "topics": ["recurrent neural network", "unsupervised learning"]}
{"title": "options discovery with budgeted reinforcement learning", "abstract": "we consider the problem of learning hierarchical policies for reinforcement learning able to discover options , an option corresponding to a sub-policy over a set of primitive actions . different models have been proposed during the last decade that usually rely on a predefined set of options . we specifically address the problem of automatically discovering options in decision processes . we describe a new learning model called budgeted option neural network ( bonn ) able to discover options based on a budgeted learning objective . the bonn model is evaluated on different classical rl problems , demonstrating both quantitative and qualitative interesting results .", "topics": ["reinforcement learning"]}
{"title": "a new framework for distributed submodular maximization", "abstract": "a wide variety of problems in machine learning , including exemplar clustering , document summarization , and sensor placement , can be cast as constrained submodular maximization problems . a lot of recent effort has been devoted to developing distributed algorithms for these problems . however , these results suffer from high number of rounds , suboptimal approximation ratios , or both . we develop a framework for bringing existing algorithms in the sequential setting to the distributed setting , achieving near optimal approximation ratios for many settings in only a constant number of mapreduce rounds . our techniques also give a fast sequential algorithm for non-monotone maximization subject to a matroid constraint .", "topics": ["cluster analysis"]}
{"title": "online learning for structured loss spaces", "abstract": "we consider prediction with expert advice when the loss vectors are assumed to lie in a set described by the sum of atomic norm balls . we derive a regret bound for a general version of the online mirror descent ( omd ) algorithm that uses a combination of regularizers , each adapted to the constituent atomic norms . the general result recovers standard omd regret bounds , and yields regret bounds for new structured settings where the loss vectors are ( i ) noisy versions of points from a low-rank subspace , ( ii ) sparse vectors corrupted with noise , and ( iii ) sparse perturbations of low-rank vectors . for the problem of online learning with structured losses , we also show lower bounds on regret in terms of rank and sparsity of the source set of the loss vectors , which implies lower bounds for the above additive loss settings as well .", "topics": ["regret ( decision theory )", "sparse matrix"]}
{"title": "learning spontaneity to improve emotion recognition in speech", "abstract": "we investigate the effect and usefulness of spontaneity in speech ( i.e . whether a given speech data is spontaneous or not ) in the context of emotion recognition . we hypothesize that emotional content in speech is interrelated with its spontaneity , and thus propose to use spontaneity classification as an auxiliary task to the problem of emotion recognition . we propose two supervised learning settings that utilize spontaneity to improve speech emotion recognition : a hierarchical model that performs spontaneity detection before performing emotion recognition , and a multitask learning model that jointly learns to recognize both spontaneity and emotion . through various experiments on a benchmark database , we show that by using spontaneity as an additional information , significant improvement ( 3 % ) can be achieved over systems that are unaware of spontaneity . we also observe that spontaneity information is highly useful in recognizing positive emotions as the recognition accuracy improves by 12 % .", "topics": ["statistical classification", "supervised learning"]}
{"title": "a generation algorithm for f-structure representations", "abstract": "this paper shows that previously reported generation algorithms run into problems when dealing with f-structure representations . a generation algorithm that is suitable for this type of representations is presented : the semantic kernel generation ( skg ) algorithm . the skg method has the same processing strategy as the semantic head driven generation ( shdg ) algorithm and relies on the assumption that it is possible to compute the semantic kernel ( sk ) and non semantic kernel ( non-sk ) information for each input structure .", "topics": ["kernel ( operating system )"]}
{"title": "open-source code for manifold-based 3d rotation recovery of x-ray scattering patterns", "abstract": "single particle 3d imaging with ultrashort x-ray laser pulses is based on collecting and combining the information content of 2d scattering patterns of an object at different orientations . typical sample-delivery schemes leave little or no room for controlling the orientations . as such , the orientation associated with a given snapshot should be estimated after the experiment . here we present an open-source code for the most rigorous technique having been reported in this context . some practical issues along with proposed solutions are also discussed .", "topics": ["image processing", "pixel"]}
{"title": "convex and network flow optimization for structured sparsity", "abstract": "we consider a class of learning problems regularized by a structured sparsity-inducing norm defined as the sum of l_2- or l_infinity-norms over groups of variables . whereas much effort has been put in developing fast optimization techniques when the groups are disjoint or embedded in a hierarchy , we address here the case of general overlapping groups . to this end , we present two different strategies : on the one hand , we show that the proximal operator associated with a sum of l_infinity-norms can be computed exactly in polynomial time by solving a quadratic min-cost flow problem , allowing the use of accelerated proximal gradient methods . on the other hand , we use proximal splitting techniques , and address an equivalent formulation with non-overlapping groups , but in higher dimension and with additional constraints . we propose efficient and scalable algorithms exploiting these two strategies , which are significantly faster than alternative approaches . we illustrate these methods with several problems such as cur matrix factorization , multi-task learning of tree-structured dictionaries , background subtraction in video sequences , image denoising with wavelets , and topographic dictionary learning of natural image patches .", "topics": ["time complexity", "noise reduction"]}
{"title": "power-law graph cuts", "abstract": "algorithms based on spectral graph cut objectives such as normalized cuts , ratio cuts and ratio association have become popular in recent years because they are widely applicable and simple to implement via standard eigenvector computations . despite strong performance for a number of clustering tasks , spectral graph cut algorithms still suffer from several limitations : first , they require the number of clusters to be known in advance , but this information is often unknown a priori ; second , they tend to produce clusters with uniform sizes . in some cases , the true clusters exhibit a known size distribution ; in image segmentation , for instance , human-segmented images tend to yield segment sizes that follow a power-law distribution . in this paper , we propose a general framework of power-law graph cut algorithms that produce clusters whose sizes are power-law distributed , and also does not fix the number of clusters upfront . to achieve our goals , we treat the pitman-yor exchangeable partition probability function ( eppf ) as a regularizer to graph cut objectives . because the resulting objectives can not be solved by relaxing via eigenvectors , we derive a simple iterative algorithm to locally optimize the objectives . moreover , we show that our proposed algorithm can be viewed as performing map inference on a particular pitman-yor mixture model . our experiments on various data sets show the effectiveness of our algorithms .", "topics": ["cluster analysis", "image segmentation"]}
{"title": "tensor decomposition for compressing recurrent neural network", "abstract": "in the machine learning fields , recurrent neural network ( rnn ) has become a popular algorithm for sequential data modeling . however , behind the impressive performance , rnns require a large number of parameters for both training and inference . in this paper , we are trying to reduce the number of parameters and maintain the expressive power from rnn simultaneously . we utilize several tensor decompositions method including candecomp/parafac ( cp ) , tucker decomposition and tensor train ( tt ) to re-parameterize the gated recurrent unit ( gru ) rnn . we evaluate all tensor-based rnns performance on sequence modeling tasks with a various number of parameters . based on our experiment results , tt-gru achieved the best results in a various number of parameters compared to other decomposition methods .", "topics": ["recurrent neural network"]}
{"title": "learning-based quality control for cardiac mr images", "abstract": "the effectiveness of a cardiovascular magnetic resonance ( cmr ) scan depends on the ability of the operator to correctly tune the acquisition parameters to the subject being scanned and on the potential occurrence of imaging artefacts such as cardiac and respiratory motion . in clinical practice , a quality control step is performed by visual assessment of the acquired images : however , this procedure is strongly operator-dependent , cumbersome and sometimes incompatible with the time constraints in clinical settings and large-scale studies . we propose a fast , fully-automated , learning-based quality control pipeline for cmr images , specifically for short-axis image stacks . our pipeline performs three important quality checks : 1 ) heart coverage estimation , 2 ) inter-slice motion detection , 3 ) image contrast estimation in the cardiac region . the pipeline uses a hybrid decision forest method - integrating both regression and structured classification models - to extract landmarks as well as probabilistic segmentation maps from both long- and short-axis images as a basis to perform the quality checks . the technique was tested on up to 3000 cases from the uk biobank study and validated against manual annotations and visual inspections performed by expert interpreters . the results show the capability of the proposed pipeline to correctly detect incomplete or corrupted scans , allowing their exclusion from the analysed dataset or the triggering of a new acquisition .", "topics": ["map"]}
{"title": "accelerated stochastic quasi-newton optimization on riemann manifolds", "abstract": "we propose an l-bfgs optimization algorithm on riemannian manifolds using minibatched stochastic variance reduction techniques for fast convergence with constant step sizes , without resorting to linesearch methods designed to satisfy wolfe conditions . we provide a new convergence proof for strongly convex functions without using curvature conditions on the manifold , as well as a convergence discussion for nonconvex functions . we discuss a couple of ways to obtain the correction pairs used to calculate the product of the gradient with the inverse hessian , and empirically demonstrate their use in synthetic experiments on computation of karcher means for symmetric positive definite matrices and leading eigenvalues of large scale data matrices . we compare our method to vr-pca for the latter experiment , along with riemannian svrg for both cases , and show strong convergence results for a range of datasets .", "topics": ["synthetic data", "computation"]}
{"title": "segan : speech enhancement generative adversarial network", "abstract": "current speech enhancement techniques operate on the spectral domain and/or exploit some higher-level feature . the majority of them tackle a limited number of noise conditions and rely on first-order statistics . to circumvent these issues , deep networks are being increasingly used , thanks to their ability to learn complex functions from large example sets . in this work , we propose the use of generative adversarial networks for speech enhancement . in contrast to current techniques , we operate at the waveform level , training the model end-to-end , and incorporate 28 speakers and 40 different noise conditions into the same model , such that model parameters are shared across them . we evaluate the proposed model using an independent , unseen test set with two speakers and 20 alternative noise conditions . the enhanced samples confirm the viability of the proposed model , and both objective and subjective evaluations confirm the effectiveness of it . with that , we open the exploration of generative architectures for speech enhancement , which may progressively incorporate further speech-centric design choices to improve their performance .", "topics": ["test set", "end-to-end principle"]}
{"title": "hot swapping for online adaptation of optimization hyperparameters", "abstract": "we describe a general framework for online adaptation of optimization hyperparameters by `hot swapping ' their values during learning . we investigate this approach in the context of adaptive learning rate selection using an explore-exploit strategy from the multi-armed bandit literature . experiments on a benchmark neural network show that the hot swapping approach leads to consistently better solutions compared to well-known alternatives such as adadelta and stochastic gradient with exhaustive hyperparameter search .", "topics": ["mathematical optimization", "gradient"]}
{"title": "face detection with end-to-end integration of a convnet and a 3d model", "abstract": "this paper presents a method for face detection in the wild , which integrates a convnet and a 3d mean face model in an end-to-end multi-task discriminative learning framework . the 3d mean face model is predefined and fixed ( e.g . , we used the one provided in the aflw dataset ) . the convnet consists of two components : ( i ) the face pro- posal component computes face bounding box proposals via estimating facial key-points and the 3d transformation ( rotation and translation ) parameters for each predicted key-point w.r.t . the 3d mean face model . ( ii ) the face verification component computes detection results by prun- ing and refining proposals based on facial key-points based configuration pooling . the proposed method addresses two issues in adapting state- of-the-art generic object detection convnets ( e.g . , faster r-cnn ) for face detection : ( i ) one is to eliminate the heuristic design of prede- fined anchor boxes in the region proposals network ( rpn ) by exploit- ing a 3d mean face model . ( ii ) the other is to replace the generic roi ( region-of-interest ) pooling layer with a configuration pooling layer to respect underlying object structures . the multi-task loss consists of three terms : the classification softmax loss and the location smooth l1 -losses [ 14 ] of both the facial key-points and the face bounding boxes . in ex- periments , our convnet is trained on the aflw dataset only and tested on the fddb benchmark with fine-tuning and on the afw benchmark without fine-tuning . the proposed method obtains very competitive state-of-the-art performance in the two benchmarks .", "topics": ["end-to-end principle"]}
{"title": "signet : convolutional siamese network for writer independent offline signature verification", "abstract": "offline signature verification is one of the most challenging tasks in biometrics and document forensics . unlike other verification problems , it needs to model minute but critical details between genuine and forged signatures , because a skilled falsification might often resembles the real signature with small deformation . this verification task is even harder in writer independent scenarios which is undeniably fiscal for realistic cases . in this paper , we model an offline writer independent signature verification task with a convolutional siamese network . siamese networks are twin networks with shared weights , which can be trained to learn a feature space where similar observations are placed in proximity . this is achieved by exposing the network to a pair of similar and dissimilar observations and minimizing the euclidean distance between similar pairs while simultaneously maximizing it between dissimilar pairs . experiments conducted on cross-domain datasets emphasize the capability of our network to model forgery in different languages ( scripts ) and handwriting styles . moreover , our designed siamese network , named signet , exceeds the state-of-the-art results on most of the benchmark signature datasets , which paves the way for further research in this direction .", "topics": ["feature vector"]}
{"title": "optimizing filter size in convolutional neural networks for facial action unit recognition", "abstract": "recognizing facial action units ( aus ) during spontaneous facial displays is a challenging problem . most recently , convolutional neural networks ( cnns ) have shown promise for facial au recognition , where predefined and fixed convolution filter sizes are employed . in order to achieve the best performance , the optimal filter size is often empirically found by conducting extensive experimental validation . such a training process suffers from expensive training cost , especially as the network becomes deeper . this paper proposes a novel optimized filter size cnn ( ofs-cnn ) , where the filter sizes and weights of all convolutional layers are learned simultaneously from the training data along with learning convolution filters . specifically , the filter size is defined as a continuous variable , which is optimized by minimizing the training loss . experimental results on two au-coded spontaneous databases have shown that the proposed ofs-cnn is capable of estimating optimal filter size for varying image resolution and outperforms traditional cnns with the best filter size obtained by exhaustive search . the ofs-cnn also beats the cnn using multiple filter sizes and more importantly , is much more efficient during testing with the proposed forward-backward propagation algorithm .", "topics": ["test set", "convolution"]}
{"title": "exact and consistent interpretation for piecewise linear neural networks : a closed form solution", "abstract": "strong intelligent machines powered by deep neural networks are increasingly deployed as black boxes to make decisions in risk-sensitive domains , such as finance and medical . to reduce potential risk and build trust with users , it is critical to interpret how such machines make their decisions . existing works interpret a pre-trained neural network by analyzing hidden neurons , mimicking pre-trained models or approximating local predictions . however , these methods do not provide a guarantee on the exactness and consistency of their interpretation . in this paper , we propose an elegant closed form solution named $ openbox $ to compute exact and consistent interpretations for the family of piecewise linear neural networks ( plnn ) . the major idea is to first transform a plnn into a mathematically equivalent set of linear classifiers , then interpret each linear classifier by the features that dominate its prediction . we further apply $ openbox $ to demonstrate the effectiveness of non-negative and sparse constraints on improving the interpretability of plnns . the extensive experiments on both synthetic and real world data sets clearly demonstrate the exactness and consistency of our interpretation .", "topics": ["approximation algorithm", "neural networks"]}
{"title": "triviaqa : a large scale distantly supervised challenge dataset for reading comprehension", "abstract": "we present triviaqa , a challenging reading comprehension dataset containing over 650k question-answer-evidence triples . triviaqa includes 95k question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents , six per question on average , that provide high quality distant supervision for answering the questions . we show that , in comparison to other recently introduced large-scale datasets , triviaqa ( 1 ) has relatively complex , compositional questions , ( 2 ) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences , and ( 3 ) requires more cross sentence reasoning to find answers . we also present two baseline algorithms : a feature-based classifier and a state-of-the-art neural network , that performs well on squad reading comprehension . neither approach comes close to human performance ( 23 % and 40 % vs. 80 % ) , suggesting that triviaqa is a challenging testbed that is worth significant future study . data and code available at -- http : //nlp.cs.washington.edu/triviaqa/", "topics": ["baseline ( configuration management )"]}
{"title": "block coordinate descent for sparse nmf", "abstract": "nonnegative matrix factorization ( nmf ) has become a ubiquitous tool for data analysis . an important variant is the sparse nmf problem which arises when we explicitly require the learnt features to be sparse . a natural measure of sparsity is the l $ _0 $ norm , however its optimization is np-hard . mixed norms , such as l $ _1 $ /l $ _2 $ measure , have been shown to model sparsity robustly , based on intuitive attributes that such measures need to satisfy . this is in contrast to computationally cheaper alternatives such as the plain l $ _1 $ norm . however , present algorithms designed for optimizing the mixed norm l $ _1 $ /l $ _2 $ are slow and other formulations for sparse nmf have been proposed such as those based on l $ _1 $ and l $ _0 $ norms . our proposed algorithm allows us to solve the mixed norm sparsity constraints while not sacrificing computation time . we present experimental evidence on real-world datasets that shows our new algorithm performs an order of magnitude faster compared to the current state-of-the-art solvers optimizing the mixed norm and is suitable for large-scale datasets .", "topics": ["time complexity", "sparse matrix"]}
{"title": "learning an executable neural semantic parser", "abstract": "this paper describes a neural semantic parser that maps natural language utterances onto logical forms which can be executed against a task-specific environment , such as a knowledge base or a database , to produce a response . the parser generates tree-structured logical forms with a transition-based approach which combines a generic tree-generation algorithm with domain-general operations defined by the logical language . the generation process is modeled by structured recurrent neural networks , which provide a rich encoding of the sentential context and generation history for making predictions . to tackle mismatches between natural language and logical form tokens , various attention mechanisms are explored . finally , we consider different training settings for the neural semantic parser , including a fully supervised training where annotated logical forms are given , weakly-supervised training where denotations are provided , and distant supervision where only unlabeled sentences and a knowledge base are available . experiments across a wide range of datasets demonstrate the effectiveness of our parser .", "topics": ["recurrent neural network", "natural language"]}
{"title": "tracking noisy targets : a review of recent object tracking approaches", "abstract": "visual object tracking is an important computer vision problem with numerous real-world applications including human-computer interaction , autonomous vehicles , robotics , motion-based recognition , video indexing , surveillance and security . in this paper , we aim to extensively review the latest trends and advances in the tracking algorithms and evaluate the robustness of trackers in the presence of noise . the first part of this work comprises a comprehensive survey of recently proposed tracking algorithms . we broadly categorize trackers into correlation filter based trackers and the others as non-correlation filter trackers . each category is further classified into various types of trackers based on the architecture of the tracking mechanism . in the second part of this work , we experimentally evaluate tracking algorithms for robustness in the presence of additive white gaussian noise . multiple levels of additive noise are added to the object tracking benchmark ( otb ) 2015 , and the precision and success rates of the tracking algorithms are evaluated . some algorithms suffered more performance degradation than others , which brings to light a previously unexplored aspect of the tracking algorithms . the relative rank of the algorithms based on their performance on benchmark datasets may change in the presence of noise . our study concludes that no single tracker is able to achieve the same efficiency in the presence of noise as under noise-free conditions ; thus , there is a need to include a parameter for robustness to noise when evaluating newly proposed tracking algorithms .", "topics": ["computer vision", "autonomous car"]}
{"title": "chains of reasoning over entities , relations , and text using recurrent neural networks", "abstract": "our goal is to combine the rich multistep inference of symbolic logical reasoning with the generalization capabilities of neural networks . we are particularly interested in complex reasoning about entities and relations in text and large-scale knowledge bases ( kbs ) . neelakantan et al . ( 2015 ) use rnns to compose the distributed semantics of multi-hop paths in kbs ; however for multiple reasons , the approach lacks accuracy and practicality . this paper proposes three significant modeling advances : ( 1 ) we learn to jointly reason about relations , entities , and entity-types ; ( 2 ) we use neural attention modeling to incorporate multiple paths ; ( 3 ) we learn to share strength in a single rnn that represents logical composition across all relations . on a largescale freebase+clueweb prediction task , we achieve 25 % error reduction , and a 53 % error reduction on sparse relations due to shared strength . on chains of reasoning in wordnet we reduce error in mean quantile by 84 % versus previous state-of-the-art . the code and data are available at https : //rajarshd.github.io/chainsofreasoning", "topics": ["recurrent neural network", "neural networks"]}
{"title": "deepmovie : using optical flow and deep neural networks to stylize movies", "abstract": "a recent paper by gatys et al . describes a method for rendering an image in the style of another image . first , they use convolutional neural network features to build a statistical model for the style of an image . then they create a new image with the content of one image but the style statistics of another image . here , we extend this method to render a movie in a given artistic style . the naive solution that independently renders each frame produces poor results because the features of the style move substantially from one frame to the next . the other naive method that initializes the optimization for the next frame using the rendered version of the previous frame also produces poor results because the features of the texture stay fixed relative to the frame of the movie instead of moving with objects in the scene . the main contribution of this paper is to use optical flow to initialize the style transfer optimization so that the texture features move with the objects in the video . finally , we suggest a method to incorporate optical flow explicitly into the cost function .", "topics": ["loss function"]}
{"title": "similarity registration problems for 2d/3d ultrasound calibration", "abstract": "we propose a minimal solution for the similarity registration ( rigid pose and scale ) between two sets of 3d lines , and also between a set of co-planar points and a set of 3d lines . the first problem is solved up to 8 discrete solutions with a minimum of 2 line-line correspondences , while the second is solved up to 4 discrete solutions using 4 point-line correspondences . we use these algorithms to perform the extrinsic calibration between a pose tracking sensor and a 2d/3d ultrasound ( us ) curvilinear probe using a tracked needle as calibration target . the needle is tracked as a 3d line , and is scanned by the ultrasound as either a 3d line ( 3d us ) or as a 2d point ( 2d us ) . since the scale factor that converts us scan units to metric coordinates is unknown , the calibration is formulated as a similarity registration problem . we present results with both synthetic and real data and show that the minimum solutions outperform the correspondent non-minimal linear formulations .", "topics": ["synthetic data"]}
{"title": "projective reconstruction in algebraic vision", "abstract": "we discuss the geometry of rational maps from a projective space of an arbitrary dimension to the product of projective spaces of lower dimensions induced by linear projections . in particular , we give a purely algebro-geometric proof of the projective reconstruction theorem by hartley and schaffalitzky [ hs09 ] .", "topics": ["map"]}
{"title": "entailment relations on distributions", "abstract": "in this paper we give an overview of partial orders on the space of probability distributions that carry a notion of information content and serve as a generalisation of the bayesian order given in ( coecke and martin , 2011 ) . we investigate what constraints are necessary in order to get a unique notion of information content . these partial orders can be used to give an ordering on words in vector space models of natural language meaning relating to the contexts in which words are used , which is useful for a notion of entailment and word disambiguation . the construction used also points towards a way to create orderings on the space of density operators which allow a more fine-grained study of entailment . the partial orders in this paper are directed complete and form domains in the sense of domain theory .", "topics": ["natural language"]}
{"title": "evolutionary many-objective optimization based on adversarial decomposition", "abstract": "the decomposition-based method has been recognized as a major approach for multi-objective optimization . it decomposes a multi-objective optimization problem into several single-objective optimization subproblems , each of which is usually defined as a scalarizing function using a weight vector . due to the characteristics of the contour line of a particular scalarizing function , the performance of the decomposition-based method strongly depends on the pareto front 's shape by merely using a single scalarizing function , especially when facing a large number of objectives . to improve the flexibility of the decomposition-based method , this paper develops an adversarial decomposition method that leverages the complementary characteristics of two different scalarizing functions within a single paradigm . more specifically , we maintain two co-evolving populations simultaneously by using different scalarizing functions . in order to avoid allocating redundant computational resources to the same region of the pareto front , we stably match these two co-evolving populations into one-one solution pairs according to their working regions of the pareto front . then , each solution pair can at most contribute one mating parent during the mating selection process . comparing with nine state-of-the-art many-objective optimizers , we have witnessed the competitive performance of our proposed algorithm on 130 many-objective test instances with various characteristics and pareto front 's shapes .", "topics": ["optimization problem", "mathematical optimization"]}
{"title": "integration of lidar and hyperspectral data for land-cover classification : a case study", "abstract": "in this paper , an approach is proposed to fuse lidar and hyperspectral data , which considers both spectral and spatial information in a single framework . here , an extended self-dual attribute profile ( esdap ) is investigated to extract spatial information from a hyperspectral data set . to extract spectral information , a few well-known classifiers have been used such as support vector machines ( svms ) , random forests ( rfs ) , and artificial neural networks ( anns ) . the proposed method accurately classify the relatively volumetric data set in a few cpu processing time in a real ill-posed situation where there is no balance between the number of training samples and the number of features . the classification part of the proposed approach is fully-automatic .", "topics": ["support vector machine"]}
{"title": "statistics on the ( compact ) stiefel manifold : theory and applications", "abstract": "a stiefel manifold of the compact type is often encountered in many fields of engineering including , signal and image processing , machine learning , numerical optimization and others . the stiefel manifold is a riemannian homogeneous space but not a symmetric space . in previous work , researchers have defined probability distributions on symmetric spaces and performed statistical analysis of data residing in these spaces . in this paper , we present original work involving definition of gaussian distributions on a homogeneous space and show that the maximum-likelihood estimate of the location parameter of a gaussian distribution on the homogeneous space yields the fr\\'echet mean ( fm ) of the samples drawn from this distribution . further , we present an algorithm to sample from the gaussian distribution on the stiefel manifold and recursively compute the fm of these samples . we also prove the weak consistency of this recursive fm estimator . several synthetic and real data experiments are then presented , demonstrating the superior computational performance of this estimator over the gradient descent based non-recursive counter part as well as the stochastic gradient descent based method prevalent in literature .", "topics": ["image processing", "mathematical optimization"]}
{"title": "regularization with stochastic transformations and perturbations for deep semi-supervised learning", "abstract": "effective convolutional neural networks are trained on large sets of labeled data . however , creating large labeled datasets is a very costly and time-consuming task . semi-supervised learning uses unlabeled data to train a model with higher accuracy when there is a limited set of labeled data available . in this paper , we consider the problem of semi-supervised learning with convolutional neural networks . techniques such as randomized data augmentation , dropout and random max-pooling provide better generalization and stability for classifiers that are trained using gradient descent . multiple passes of an individual sample through the network might lead to different predictions due to the non-deterministic behavior of these techniques . we propose an unsupervised loss function that takes advantage of the stochastic nature of these methods and minimizes the difference between the predictions of multiple passes of a training sample through the network . we evaluate the proposed method on several benchmark datasets .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "learning frame models using cnn filters", "abstract": "the convolutional neural network ( convnet or cnn ) has proven to be very successful in many tasks such as those in computer vision . in this conceptual paper , we study the generative perspective of the discriminative cnn . in particular , we propose to learn the generative frame ( filters , random field , and maximum entropy ) model using the highly expressive filters pre-learned by the cnn at the convolutional layers . we show that the learning algorithm can generate realistic and rich object and texture patterns in natural scenes . we explain that each learned model corresponds to a new cnn unit at a layer above the layer of filters employed by the model . we further show that it is possible to learn a new layer of cnn units using a generative cnn model , which is a product of experts model , and the learning algorithm admits an em interpretation with binary latent variables .", "topics": ["generative model", "computer vision"]}
{"title": "recurrent neural network method in arabic words recognition system", "abstract": "the recognition of unconstrained handwriting continues to be a difficult task for computers despite active research for several decades . this is because handwritten text offers great challenges such as character and word segmentation , character recognition , variation between handwriting styles , different character size and no font constraints as well as the background clarity . in this paper primarily discussed online handwriting recognition methods for arabic words which being often used among then across the middle east and north africa people . because of the characteristic of the whole body of the arabic words , namely connectivity between the characters , thereby the segmentation of an arabic word is very difficult . we introduced a recurrent neural network to online handwriting arabic word recognition . the key innovation is a recently produce recurrent neural networks objective function known as connectionist temporal classification . the system consists of an advanced recurrent neural network with an output layer designed for sequence labeling , partially combined with a probabilistic language model . experimental results show that unconstrained arabic words achieve recognition rates about 79 % , which is significantly higher than the about 70 % using a previously developed hidden markov model based recognition system .", "topics": ["recurrent neural network", "loss function"]}
{"title": "online learning of a memory for learning rates", "abstract": "the promise of learning to learn for robotics rests on the hope that by extracting some information about the learning process itself we can speed up subsequent similar learning tasks . here , we introduce a computationally efficient online meta-learning algorithm that builds and optimizes a memory model of the optimal learning rate landscape from previously observed gradient behaviors . while performing task specific optimization , this memory of learning rates predicts how to scale currently observed gradients . after applying the gradient scaling our meta-learner updates its internal memory based on the observed effect its prediction had . our meta-learner can be combined with any gradient-based optimizer , learns on the fly and can be transferred to new optimization tasks . in our evaluations we show that our meta-learning algorithm speeds up learning of mnist classification and a variety of learning control tasks , either in batch or online learning settings .", "topics": ["computational complexity theory", "gradient"]}
{"title": "thai rhetorical structure analysis", "abstract": "rhetorical structure analysis ( rsa ) explores discourse relations among elementary discourse units ( edus ) in a text . it is very useful in many text processing tasks employing relationships among edus such as text understanding , summarization , and question-answering . thai language with its distinctive linguistic characteristics requires a unique technique . this article proposes an approach for thai rhetorical structure analysis . first , edus are segmented by two hidden markov models derived from syntactic rules . a rhetorical structure tree is constructed from a clustering technique with its similarity measure derived from thai semantic rules . then , a decision tree whose features derived from the semantic rules is used to determine discourse relations .", "topics": ["cluster analysis"]}
{"title": "inference , learning and attention mechanisms that exploit and preserve sparsity in convolutional networks", "abstract": "while cnns naturally lend themselves to densely sampled data , and sophisticated implementations are available , they lack the ability to efficiently process sparse data . in this work we introduce a suite of tools that exploit sparsity in both the feature maps and the filter weights , and thereby allow for significantly lower memory footprints and computation times than the conventional dense framework when processing data with a high degree of sparsity . our scheme provides ( i ) an efficient gpu implementation of a convolution layer based on direct , sparse convolution ; ( ii ) a filter step within the convolution layer , which we call attention , that prevents fill-in , i.e . , the tendency of convolution to rapidly decrease sparsity , and guarantees an upper bound on the computational resources ; and ( iii ) an adaptation of the back-propagation algorithm , which makes it possible to combine our approach with standard learning frameworks , while still exploiting sparsity in the data and the model .", "topics": ["sparse matrix", "computation"]}
{"title": "automatic symmetry based cluster approach for anomalous brain identification in pet scan image : an analysis", "abstract": "medical image segmentation is referred to the segmentation of known anatomic structures from different medical images . normally , the medical data researches are more complicated and an exclusive structures . this computer aided diagnosis is used for assisting doctors in evaluating medical imagery or in recognizing abnormal findings in a medical image . to integrate the specialized knowledge for medical data processing is helpful to form a real useful healthcare decision making system . this paper studies the different symmetry based distances applied in clustering algorithms and analyzes symmetry approach for positron emission tomography ( pet ) scan image segmentation . unlike ct and mri , the pet scan identifies the structure of blood flow to and from organs . pet scan also helps in early diagnosis of cancer and heart , brain and gastro intestinal ailments and to detect the progress of treatment . in this paper , the scope diagnostic task expands for pet image in various brain functions .", "topics": ["image segmentation", "cluster analysis"]}
{"title": "outlier regularization for vector data and l21 norm robustness", "abstract": "in many real-world applications , data usually contain outliers . one popular approach is to use l2,1 norm function as a robust error/loss function . however , the robustness of l2,1 norm function is not well understood so far . in this paper , we propose a new vector outlier regularization ( vor ) framework to understand and analyze the robustness of l2,1 norm function . our vor function defines a data point to be outlier if it is outside a threshold with respect to a theoretical prediction , and regularize it-pull it back to the threshold line . we then prove that l2,1 function is the limiting case of this vor with the usual least square/l2 error function as the threshold shrinks to zero . one interesting property of vor is that how far an outlier lies away from its theoretically predicted value does not affect the final regularization and analysis results . this vor property unmasks one of the most peculiar property of l2,1 norm function : the effects of outliers seem to be independent of how outlying they are-if an outlier is moved further away from the intrinsic manifold/subspace , the final analysis results do not change . vor provides a new way to understand and analyze the robustness of l2,1 norm function . applying vor to matrix factorization leads to a new vorpca model . we give a comprehensive comparison with trace-norm based l21-norm pca to demonstrate the advantages of vorpca .", "topics": ["matrix regularization", "loss function"]}
{"title": "fever : a large-scale dataset for fact extraction and verification", "abstract": "unlike other tasks and despite recent interest , research in textual claim verification has been hindered by the lack of large-scale manually annotated datasets . in this paper we introduce a new publicly available dataset for verification against textual sources , fever : fact extraction and verification . it consists of 185,441 claims generated by altering sentences extracted from wikipedia and subsequently verified without knowledge of the sentence they were derived from . the claims are classified as supported , refuted or notenoughinfo by annotators achieving 0.6841 in fleiss $ \\kappa $ . for the first two classes , the annotators also recorded the sentence ( s ) forming the necessary evidence for their judgment . to characterize the challenge of the dataset presented , we develop a pipeline approach using both baseline and state-of-the-art components and compare it to suitably designed oracles . the best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87 % , while if we ignore the evidence we achieve 50.91 % . thus we believe that fever is a challenging testbed that will help stimulate progress on claim verification against textual sources .", "topics": ["baseline ( configuration management )"]}
{"title": "a note on the evaluation of generative models", "abstract": "probabilistic generative models can be used for compression , denoising , inpainting , texture synthesis , semi-supervised learning , unsupervised feature learning , and other tasks . given this wide range of applications , it is not surprising that a lot of heterogeneity exists in the way these models are formulated , trained , and evaluated . as a consequence , direct comparison between models is often difficult . this article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models . in particular , we show that three of the currently most commonly used criteria -- -average log-likelihood , parzen window estimates , and visual fidelity of samples -- -are largely independent of each other when the data is high-dimensional . good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria . our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application ( s ) they were intended for . in addition , we provide examples demonstrating that parzen window estimates should generally be avoided .", "topics": ["generative model", "feature learning"]}
{"title": "on the development of intelligent agents for moba games", "abstract": "multiplayer online battle arena ( moba ) is one of the most played game genres nowadays . with the increasing growth of this genre , it becomes necessary to develop effective intelligent agents to play alongside or against human players . in this paper we address the problem of agent development for moba games . we implement a two-layered architecture agent that handles both navigation and game mechanics . this architecture relies on the use of influence maps , a widely used approach for tactical analysis . several experiments were performed using { \\em league of legends } as a testbed , and show promising results in this highly dynamic real-time context .", "topics": ["map"]}
{"title": "parallel gaussian process regression with low-rank covariance matrix approximations", "abstract": "gaussian processes ( gp ) are bayesian non-parametric models that are widely used for probabilistic regression . unfortunately , it can not scale well with large data nor perform real-time predictions due to its cubic time cost in the data size . this paper presents two parallel gp regression methods that exploit low-rank covariance matrix approximations for distributing the computational load among parallel machines to achieve time efficiency and scalability . we theoretically guarantee the predictive performances of our proposed parallel gps to be equivalent to that of some centralized approximate gp regression methods : the computation of their centralized counterparts can be distributed among parallel machines , hence achieving greater time efficiency and scalability . we analytically compare the properties of our parallel gps such as time , space , and communication complexity . empirical evaluation on two real-world datasets in a cluster of 20 computing nodes shows that our parallel gps are significantly more time-efficient and scalable than their centralized counterparts and exact/full gp while achieving predictive performances comparable to full gp .", "topics": ["time complexity", "scalability"]}
{"title": "virtual screening with support vector machines and structure kernels", "abstract": "support vector machines and kernel methods have recently gained considerable attention in chemoinformatics . they offer generally good performance for problems of supervised classification or regression , and provide a flexible and computationally efficient framework to include relevant information and prior knowledge about the data and problems to be handled . in particular , with kernel methods molecules do not need to be represented and stored explicitly as vectors or fingerprints , but only to be compared to each other through a comparison function technically called a kernel . while classical kernels can be used to compare vector or fingerprint representations of molecules , completely new kernels were developed in the recent years to directly compare the 2d or 3d structures of molecules , without the need for an explicit vectorization step through the extraction of molecular descriptors . while still in their infancy , these approaches have already demonstrated their relevance on several toxicity prediction and structure-activity relationship problems .", "topics": ["supervised learning", "support vector machine"]}
{"title": "universal probability-free prediction", "abstract": "we construct universal prediction systems in the spirit of popper 's falsifiability and kolmogorov complexity and randomness . these prediction systems do not depend on any statistical assumptions ( but under the iid assumption they dominate , to within the usual accuracy , conformal prediction ) . our constructions give rise to a theory of algorithmic complexity and randomness of time containing analogues of several notions and results of the classical theory of kolmogorov complexity and randomness .", "topics": ["computational complexity theory"]}
{"title": "tighter linear program relaxations for high order graphical models", "abstract": "graphical models with high order potentials ( hops ) have received considerable interest in recent years . while there are a variety of approaches to inference in these models , nearly all of them amount to solving a linear program ( lp ) relaxation with unary consistency constraints between the hop and the individual variables . in many cases , the resulting relaxations are loose , and in these cases the results of inference can be poor . it is thus desirable to look for more accurate ways of performing inference in these models . in this work , we study the lp relaxations that result from enforcing additional consistency constraints between the hop and the rest of the model . we address theoretical questions about the strength of the resulting relaxations compared to the relaxations that arise in standard approaches , and we develop practical and efficient message passing algorithms for optimizing the lps . empirically , we show that the lps with additional consistency constraints lead to more accurate inference on some challenging problems that include a combination of low order and high order terms .", "topics": ["graphical model"]}
{"title": "towards conceptual compression", "abstract": "we introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling . the system represents the state-of-the-art in latent variable models for both the imagenet and omniglot datasets . we show that it naturally separates global conceptual information from lower level details , thus addressing one of the fundamentally desired properties of unsupervised learning . furthermore , the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression ' .", "topics": ["calculus of variations", "unsupervised learning"]}
{"title": "latent gaussian process regression", "abstract": "we introduce latent gaussian process regression which is a latent variable extension allowing modelling of non-stationary multi-modal processes using gps . the approach is built on extending the input space of a regression problem with a latent variable that is used to modulate the covariance function over the training data . we show how our approach can be used to model multi-modal and non-stationary processes . we exemplify the approach on a set of synthetic data and provide results on real data from motion capture and geostatistics .", "topics": ["synthetic data"]}
{"title": "towards deep learning with spiking neurons in energy based models with contrastive hebbian plasticity", "abstract": "in machine learning , error back-propagation in multi-layer neural networks ( deep learning ) has been impressively successful in supervised and reinforcement learning tasks . as a model for learning in the brain , however , deep learning has long been regarded as implausible , since it relies in its basic form on a non-local plasticity rule . to overcome this problem , energy-based models with local contrastive hebbian learning were proposed and tested on a classification task with networks of rate neurons . we extended this work by implementing and testing such a model with networks of leaky integrate-and-fire neurons . preliminary results indicate that it is possible to learn a non-linear regression task with hidden layers , spiking neurons and a local synaptic plasticity rule .", "topics": ["reinforcement learning", "nonlinear system"]}
{"title": "fast model selection by limiting svm training times", "abstract": "kernelized support vector machines ( svms ) are among the best performing supervised learning methods . but for optimal predictive performance , time-consuming parameter tuning is crucial , which impedes application . to tackle this problem , the classic model selection procedure based on grid-search and cross-validation was refined , e.g . by data subsampling and direct search heuristics . here we focus on a different aspect , the stopping criterion for svm training . we show that by limiting the training time given to the svm solver during parameter tuning we can reduce model selection times by an order of magnitude .", "topics": ["supervised learning", "support vector machine"]}
{"title": "minimum probability flow learning", "abstract": "fitting probabilistic models to data is often difficult , due to the general intractability of the partition function and its derivatives . here we propose a new parameter estimation technique that does not require computing an intractable normalization factor or sampling from the equilibrium distribution of the model . this is achieved by establishing dynamics that would transform the observed data distribution into the model distribution , and then setting as the objective the minimization of the kl divergence between the data distribution and the distribution produced by running the dynamics for an infinitesimal time . score matching , minimum velocity learning , and certain forms of contrastive divergence are shown to be special cases of this learning technique . we demonstrate parameter estimation in ising models , deep belief networks and an independent component analysis model of natural scenes . in the ising model case , current state of the art techniques are outperformed by at least an order of magnitude in learning time , with lower error in recovered coupling parameters .", "topics": ["sampling ( signal processing )", "bayesian network"]}
{"title": "multimodal image captioning for marketing analysis", "abstract": "automatically captioning images with natural language sentences is an important research topic . state of the art models are able to produce human-like sentences . these models typically describe the depicted scene as a whole and do not target specific objects of interest or emotional relationships between these objects in the image . however , marketing companies require to describe these important attributes of a given scene . in our case , objects of interest are consumer goods , which are usually identifiable by a product logo and are associated with certain brands . from a marketing point of view , it is desirable to also evaluate the emotional context of a trademarked product , i.e . , whether it appears in a positive or a negative connotation . we address the problem of finding brands in images and deriving corresponding captions by introducing a modified image captioning network . we also add a third output modality , which simultaneously produces real-valued image ratings . our network is trained using a classification-aware loss function in order to stimulate the generation of sentences with an emphasis on words identifying the brand of a product . we evaluate our model on a dataset of images depicting interactions between humans and branded products . the introduced network improves mean class accuracy by 24.5 percent . thanks to adding the third output modality , it also considerably improves the quality of generated captions for images depicting branded products .", "topics": ["natural language", "interaction"]}
{"title": "stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking", "abstract": "the natural language generation ( nlg ) component of a spoken dialogue system ( sds ) usually needs a substantial amount of handcrafting or a well-labeled dataset to be trained on . these limitations add significantly to development costs and make cross-domain , multi-lingual dialogue systems intractable . moreover , human languages are context-aware . the most natural response should be directly learned from data rather than depending on predefined syntaxes or rules . this paper presents a statistical language generator based on a joint recurrent and convolutional neural network structure which can be trained on dialogue act-utterance pairs without any semantic alignments or predefined grammar trees . objective metrics suggest that this new model outperforms previous methods under the same experimental conditions . results of an evaluation by human judges indicate that it produces not only high quality but linguistically varied utterances which are preferred compared to n-gram and rule-based systems .", "topics": ["recurrent neural network", "natural language"]}
{"title": "visible light-based human visual system conceptual model", "abstract": "there is a widely held belief in the digital image and video processing community , which is as follows : the human visual system ( hvs ) is more sensitive to luminance ( often confused with brightness ) than photon energies ( often confused with chromaticity and chrominance ) . passages similar to the following occur with high frequency in the peer reviewed literature and academic text books : `` the hvs is much more sensitive to brightness than colour '' or `` the hvs is much more sensitive to luma than chroma '' . in this discussion paper , a visible light-based human visual system ( vl-hvs ) conceptual model is discussed . the objectives of vl-hvs are as follows : 1 . to facilitate a deeper theoretical reflection of the fundamental relationship between visible light , the manifestation of colour perception derived from visible light and the physiology of the perception of colour . that is , in terms of the physics of visible light , photobiology and the human subjective interpretation of visible light , it is appropriate to provide comprehensive background information in relation to the natural interactions between visible light , the retinal photoreceptors and the subsequent cortical processing of such . 2 . to provide a more wholesome account with respect to colour information in digital image and video processing applications . 3 . to recontextualise colour data in the rgb and ycbcr colour spaces , such that novel techniques in digital image and video processing , including quantisation and artifact reduction techniques , may be developed based on both luma and chroma information ( not luma data only ) .", "topics": ["synthetic data", "interaction"]}
{"title": "a guide to convolution arithmetic for deep learning", "abstract": "we introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures . the guide clarifies the relationship between various properties ( input shape , kernel shape , zero padding , strides and output shape ) of convolutional , pooling and transposed convolutional layers , as well as the relationship between convolutional and transposed convolutional layers . relationships are derived for various cases , and are illustrated in order to make them intuitive .", "topics": ["convolution"]}
{"title": "flow for meta control", "abstract": "the psychological state of flow has been linked to optimizing human performance . a key condition of flow emergence is a match between the human abilities and complexity of the task . we propose a simple computational model of flow for artificial intelligence ( ai ) agents . the model factors the standard agent-environment state into a self-reflective set of the agent 's abilities and a socially learned set of the environmental complexity . maximizing the flow serves as a meta control for the agent . we show how to apply the meta-control policy to a broad class of ai control policies and illustrate our approach with a specific implementation . results in a synthetic testbed are promising and open interesting directions for future work .", "topics": ["synthetic data", "artificial intelligence"]}
{"title": "evaluation of trackers for pan-tilt-zoom scenarios", "abstract": "tracking with a pan-tilt-zoom ( ptz ) camera has been a research topic in computer vision for many years . compared to tracking with a still camera , the images captured with a ptz camera are highly dynamic in nature because the camera can perform large motion resulting in quickly changing capture conditions . furthermore , tracking with a ptz camera involves camera control to position the camera on the target . for successful tracking and camera control , the tracker must be fast enough , or has to be able to predict accurately the next position of the target . therefore , standard benchmarks do not allow to assess properly the quality of a tracker for the ptz scenario . in this work , we use a virtual ptz framework to evaluate different tracking algorithms and compare their performances . we also extend the framework to add target position prediction for the next frame , accounting for camera motion and processing delays . by doing this , we can assess if predicting can make long-term tracking more robust as it may help slower algorithms for keeping the target in the field of view of the camera . results confirm that both speed and robustness are required for tracking under the ptz scenario .", "topics": ["computer vision"]}
{"title": "analyzing language learned by an active question answering agent", "abstract": "we analyze the language learned by an agent trained with reinforcement learning as a component of the activeqa system [ buck et al . , 2017 ] . in activeqa , question answering is framed as a reinforcement learning task in which an agent sits between the user and a black box question-answering system . the agent learns to reformulate the user 's questions to elicit the optimal answers . it probes the system with many versions of a question that are generated via a sequence-to-sequence question reformulation model , then aggregates the returned evidence to find the best answer . this process is an instance of \\emph { machine-machine } communication . the question reformulation model must adapt its language to increase the quality of the answers returned , matching the language of the question answering system . we find that the agent does not learn transformations that align with semantic intuitions but discovers through learning classical information retrieval techniques such as tf-idf re-weighting and stemming .", "topics": ["reinforcement learning"]}
{"title": "graph interpolation grammars : a rule-based approach to the incremental parsing of natural languages", "abstract": "graph interpolation grammars are a declarative formalism with an operational semantics . their goal is to emulate salient features of the human parser , and notably incrementality . the parsing process defined by gigs incrementally builds a syntactic representation of a sentence as each successive lexeme is read . a gig rule specifies a set of parse configurations that trigger its application and an operation to perform on a matching configuration . rules are partly context-sensitive ; furthermore , they are reversible , meaning that their operations can be undone , which allows the parsing process to be nondeterministic . these two factors confer enough expressive power to the formalism for parsing natural languages .", "topics": ["natural language", "parsing"]}
{"title": "face valuing : training user interfaces with facial expressions and reinforcement learning", "abstract": "an important application of interactive machine learning is extending or amplifying the cognitive and physical capabilities of a human . to accomplish this , machines need to learn about their human users ' intentions and adapt to their preferences . in most current research , a user has conveyed preferences to a machine using explicit corrective or instructive feedback ; explicit feedback imposes a cognitive load on the user and is expensive in terms of human effort . the primary objective of the current work is to demonstrate that a learning agent can reduce the amount of explicit feedback required for adapting to the user 's preferences pertaining to a task by learning to perceive a value of its behavior from the human user , particularly from the user 's facial expressions -- -we call this face valuing . we empirically evaluate face valuing on a grip selection task . our preliminary results suggest that an agent can quickly adapt to a user 's changing preferences with minimal explicit feedback by learning a value function that maps facial features extracted from a camera image to expected future reward . we believe that an agent learning to perceive a value from the body language of its human user is complementary to existing interactive machine learning approaches and will help in creating successful human-machine interactive applications .", "topics": ["reinforcement learning", "map"]}
{"title": "group lasso with overlaps : the latent group lasso approach", "abstract": "we study a norm for structured sparsity which leads to sparse linear predictors whose supports are unions of prede ned overlapping groups of variables . we call the obtained formulation latent group lasso , since it is based on applying the usual group lasso penalty on a set of latent variables . a detailed analysis of the norm and its properties is presented and we characterize conditions under which the set of groups associated with latent variables are correctly identi ed . we motivate and discuss the delicate choice of weights associated to each group , and illustrate this approach on simulated data and on the problem of breast cancer prognosis from gene expression data .", "topics": ["simulation", "sparse matrix"]}
{"title": "stochastic low-rank kernel learning for regression", "abstract": "we present a novel approach to learn a kernel-based regression function . it is based on the useof conical combinations of data-based parameterized kernels and on a new stochastic convex optimization procedure of which we establish convergence guarantees . the overall learning procedure has the nice properties that a ) the learned conical combination is automatically designed to perform the regression task at hand and b ) the updates implicated by the optimization procedure are quite inexpensive . in order to shed light on the appositeness of our learning strategy , we present empirical results from experiments conducted on various benchmark datasets .", "topics": ["kernel ( operating system )"]}
{"title": "proceedings of the eighteenth conference on uncertainty in artificial intelligence ( 2002 )", "abstract": "this is the proceedings of the eighteenth conference on uncertainty in artificial intelligence , which was held in alberta , canada , august 1-4 2002", "topics": ["artificial intelligence"]}
{"title": "the new modality : emoji challenges in prediction , anticipation , and retrieval", "abstract": "over the past decade , emoji have emerged as a new and widespread form of digital communication , spanning diverse social networks and spoken languages . we propose to treat these ideograms as a new modality in their own right , distinct in their semantic structure from both the text in which they are often embedded as well as the images which they resemble . as a new modality , emoji present rich novel possibilities for representation and interaction . in this paper , we explore the challenges that arise naturally from considering the emoji modality through the lens of multimedia research . specifically , the ways in which emoji can be related to other common modalities such as text and images . to do so , we first present a large scale dataset of real-world emoji usage collected from twitter . this dataset contains examples of both text-emoji and image-emoji relationships . we present baseline results on the challenge of predicting emoji from both text and images , using state-of-the-art neural networks . further , we offer a first consideration into the problem of how to account for new , unseen emoji - a relevant issue as the emoji vocabulary continues to expand on a yearly basis . finally , we present results for multimedia retrieval using emoji as queries .", "topics": ["baseline ( configuration management )"]}
{"title": "distributed deep learning for question answering", "abstract": "this paper is an empirical study of the distributed deep learning for question answering subtasks : answer selection and question classification . comparison studies of sgd , msgd , adadelta , adagrad , adam/adamax , rmsprop , downpour and easgd/eamsgd algorithms have been presented . experimental results show that the distributed framework based on the message passing interface can accelerate the convergence speed at a sublinear scale . this paper demonstrates the importance of distributed training . for example , with 48 workers , a 24x speedup is achievable for the answer selection task and running time is decreased from 138.2 hours to 5.81 hours , which will increase the productivity significantly .", "topics": ["time complexity"]}
{"title": "elimination of glass artifacts and object segmentation", "abstract": "many images nowadays are captured from behind the glasses and may have certain stains discrepancy because of glass and must be processed to make differentiation between the glass and objects behind it . this research paper proposes an algorithm to remove the damaged or corrupted part of the image and make it consistent with other part of the image and to segment objects behind the glass . the damaged part is removed using total variation inpainting method and segmentation is done using kmeans clustering , anisotropic diffusion and watershed transformation . the final output is obtained by interpolation . this algorithm can be useful to applications in which some part of the images are corrupted due to data transmission or needs to segment objects from an image for further processing .", "topics": ["cluster analysis"]}
{"title": "summary - terpret : a probabilistic programming language for program induction", "abstract": "we study machine learning formulations of inductive program synthesis ; that is , given input-output examples , synthesize source code that maps inputs to corresponding outputs . our key contribution is terpret , a domain-specific language for expressing program synthesis problems . a terpret model is composed of a specification of a program representation and an interpreter that describes how programs map inputs to outputs . the inference task is to observe a set of input-output examples and infer the underlying program . from a terpret model we automatically perform inference using four different back-ends : gradient descent ( thus each terpret model can be seen as defining a differentiable interpreter ) , linear program ( lp ) relaxations for graphical models , discrete satisfiability solving , and the sketch program synthesis system . terpret has two main benefits . first , it enables rapid exploration of a range of domains , program representations , and interpreter models . second , it separates the model specification from the inference algorithm , allowing proper comparisons between different approaches to inference . we illustrate the value of terpret by developing several interpreter models and performing an extensive empirical comparison between alternative inference algorithms on a variety of program models . to our knowledge , this is the first work to compare gradient-based search over program space to traditional search-based alternatives . our key empirical finding is that constraint solvers dominate the gradient descent and lp-based formulations . this is a workshop summary of a longer report at arxiv:1608.04428", "topics": ["graphical model", "gradient descent"]}
{"title": "stochastic primal-dual methods and sample complexity of reinforcement learning", "abstract": "we study the online estimation of the optimal policy of a markov decision process ( mdp ) . we propose a class of stochastic primal-dual ( spd ) methods which exploit the inherent minimax duality of bellman equations . the spd methods update a few coordinates of the value and policy estimates as a new state transition is observed . these methods use small storage and has low computational complexity per iteration . the spd methods find an absolute- $ \\epsilon $ -optimal policy , with high probability , using $ \\mathcal { o } \\left ( \\frac { |\\mathcal { s } |^4 |\\mathcal { a } |^2\\sigma^2 } { ( 1-\\gamma ) ^6\\epsilon^2 } \\right ) $ iterations/samples for the infinite-horizon discounted-reward mdp and $ \\mathcal { o } \\left ( \\frac { |\\mathcal { s } |^4 |\\mathcal { a } |^2h^6\\sigma^2 } { \\epsilon^2 } \\right ) $ for the finite-horizon mdp .", "topics": ["computational complexity theory", "reinforcement learning"]}
{"title": "limits to verification and validation of agentic behavior", "abstract": "verification and validation of agentic behavior have been suggested as important research priorities in efforts to reduce risks associated with the creation of general artificial intelligence ( russell et al 2015 ) . in this paper we question the appropriateness of using language of certainty with respect to efforts to manage that risk . we begin by establishing a very general formalism to characterize agentic behavior and to describe standards of acceptable behavior . we show that determination of whether an agent meets any particular standard is not computable . we discuss the extent of the burden associated with verification by manual proof and by automated behavioral governance . we show that to ensure decidability of the behavioral standard itself , one must further limit the capabilities of the agent . we then demonstrate that if our concerns relate to outcomes in the physical world , attempts at validation are futile . finally , we show that layered architectures aimed at making these challenges tractable mistakenly equate intentions with actions or outcomes , thereby failing to provide any guarantees . we conclude with a discussion of why language of certainty should be eradicated from the conversation about the safety of general artificial intelligence .", "topics": ["artificial intelligence"]}
{"title": "unsynthesizable cores - minimal explanations for unsynthesizable high-level robot behaviors", "abstract": "with the increasing ubiquity of multi-capable , general-purpose robots arises the need for enabling non-expert users to command these robots to perform complex high-level tasks . to this end , high-level robot control has seen the application of formal methods to automatically synthesize correct-by-construction controllers from user-defined specifications ; synthesis fails if and only if there exists no controller that achieves the specified behavior . recent work has also addressed the challenge of providing easy-to-understand feedback to users when a specification fails to yield a corresponding controller . existing techniques provide feedback on portions of the specification that cause the failure , but do so at a coarse granularity . this work presents techniques for refining this feedback , extracting minimal explanations of unsynthesizability .", "topics": ["high- and low-level", "robot"]}
{"title": "sparse similarity-preserving hashing", "abstract": "in recent years , a lot of attention has been devoted to efficient nearest neighbor search by means of similarity-preserving hashing . one of the plights of existing hashing techniques is the intrinsic trade-off between performance and computational complexity : while longer hash codes allow for lower false positive rates , it is very difficult to increase the embedding dimensionality without incurring in very high false negatives rates or prohibiting computational costs . in this paper , we propose a way to overcome this limitation by enforcing the hash codes to be sparse . sparse high-dimensional codes enjoy from the low false positive rates typical of long hashes , while keeping the false negative rates similar to those of a shorter dense hashing scheme with equal number of degrees of freedom . we use a tailored feed-forward neural network for the hashing function . extensive experimental evaluation involving visual and multi-modal data shows the benefits of the proposed method .", "topics": ["computational complexity theory", "sparse matrix"]}
{"title": "a survey on hardware implementations of visual object trackers", "abstract": "visual object tracking is an active topic in the computer vision domain with applications extending over numerous fields . the main sub-tasks required to build an object tracker ( e.g . object detection , feature extraction and object tracking ) are computation-intensive . in addition , real-time operation of the tracker is indispensable for almost all of its applications . therefore , complete hardware or hardware/software co-design approaches are pursued for better tracker implementations . this paper presents a literature survey of the hardware implementations of object trackers over the last two decades . although several tracking surveys exist in literature , a survey addressing the hardware implementations of the different trackers is missing . we believe this survey would fill the gap and complete the picture with the existing surveys of how to design an efficient tracker and point out the future directions researchers can follow in this field . we highlight the lack of hardware implementations for state-of-the-art tracking algorithms as well as for enhanced classical algorithms . we also stress the need for measuring the tracking performance of the hardware-based trackers . additionally , enough details of the hardware-based trackers need to be provided to allow reasonable comparison between the different implementations .", "topics": ["object detection", "feature extraction"]}
{"title": "improving efficiency in convolutional neural network with multilinear filters", "abstract": "the excellent performance of deep neural networks has enabled us to solve several automatization problems , opening an era of autonomous devices . however , current deep net architectures are heavy with millions of parameters and require billions of floating point operations . several works have been developed to compress a pre-trained deep network to reduce memory footprint and , possibly , computation . instead of compressing a pre-trained network , in this work , we propose a generic neural network layer structure employing multilinear projection as the primary feature extractor . the proposed architecture requires several times less memory as compared to the traditional convolutional neural networks ( cnn ) , while inherits the similar design principles of a cnn . in addition , the proposed architecture is equipped with two computation schemes that enable computation reduction or scalability . experimental results show the effectiveness of our compact projection that outperforms traditional cnn , while requiring far fewer parameters .", "topics": ["computation", "scalability"]}
{"title": "attention for fine-grained categorization", "abstract": "this paper presents experiments extending the work of ba et al . ( 2014 ) on recurrent neural models for attention into less constrained visual environments , specifically fine-grained categorization on the stanford dogs data set . in this work we use an rnn of the same structure but substitute a more powerful visual network and perform large-scale pre-training of the visual network outside of the attention rnn . most work in attention models to date focuses on tasks with toy or more constrained visual environments , whereas we present results for fine-grained categorization better than the state-of-the-art googlenet classification model . we show that our model learns to direct high resolution attention to the most discriminative regions without any spatial supervision such as bounding boxes , and it is able to discriminate fine-grained dog breeds moderately well even when given only an initial low-resolution context image and narrow , inexpensive glimpses at faces and fur patterns . this and similar attention models have the major advantage of being trained end-to-end , as opposed to other current detection and recognition pipelines with hand-engineered components where information is lost . while our model is state-of-the-art , further work is needed to fully leverage the sequential input .", "topics": ["end-to-end principle"]}
{"title": "on the high-dimensional power of linear-time kernel two-sample testing under mean-difference alternatives", "abstract": "nonparametric two sample testing deals with the question of consistently deciding if two distributions are different , given samples from both , without making any parametric assumptions about the form of the distributions . the current literature is split into two kinds of tests - those which are consistent without any assumptions about how the distributions may differ ( \\textit { general } alternatives ) , and those which are designed to specifically test easier alternatives , like a difference in means ( \\textit { mean-shift } alternatives ) . the main contribution of this paper is to explicitly characterize the power of a popular nonparametric two sample test , designed for general alternatives , under a mean-shift alternative in the high-dimensional setting . specifically , we explicitly derive the power of the linear-time maximum mean discrepancy statistic using the gaussian kernel , where the dimension and sample size can both tend to infinity at any rate , and the two distributions differ in their means . as a corollary , we find that if the signal-to-noise ratio is held constant , then the test 's power goes to one if the number of samples increases faster than the dimension increases . this is the first explicit power derivation for a general nonparametric test in the high-dimensional setting , and also the first analysis of how tests designed for general alternatives perform when faced with easier ones .", "topics": ["kernel ( operating system )", "time complexity"]}
{"title": "automatic knot adjustment using dolphin echolocation algorithm for b-spline curve approximation", "abstract": "in this paper , a new approach to solve the cubic b-spline curve fitting problem is presented based on a meta-heuristic algorithm called `` dolphin echolocation `` . the method minimizes the proximity error value of the selected nodes that measured using the least squares method and the euclidean distance method of the new curve generated by the reverse engineering . the results of the proposed method are compared with the genetic algorithm . as a result , this new method seems to be successful .", "topics": ["approximation", "heuristic"]}
{"title": "hdm-net : monocular non-rigid 3d reconstruction with learned deformation model", "abstract": "monocular dense 3d reconstruction of deformable objects is a hard ill-posed problem in computer vision . current techniques either require dense correspondences and rely on motion and deformation cues , or assume a highly accurate reconstruction ( referred to as a template ) of at least a single frame given in advance and operate in the manner of non-rigid tracking . accurate computation of dense point tracks often requires multiple frames and might be computationally expensive . availability of a template is a very strong prior which restricts system operation to a pre-defined environment and scenarios . in this work , we propose a new hybrid approach for monocular non-rigid reconstruction which we call hybrid deformation model network ( hdm-net ) . in our approach , deformation model is learned by a deep neural network , with a combination of domain-specific loss functions . we train the network with multiple states of a non-rigidly deforming structure with a known shape at rest . hdm-net learns different reconstruction cues including texture-dependent surface deformations , shading and contours . we show generalisability of hdm-net to states not presented in the training dataset , with unseen textures and under new illumination conditions . experiments with noisy data and a comparison with other methods demonstrate robustness and accuracy of the proposed approach and suggest possible application scenarios of the new technique in interventional diagnostics and augmented reality .", "topics": ["computer vision", "loss function"]}
{"title": "location-based reasoning about complex multi-agent behavior", "abstract": "recent research has shown that surprisingly rich models of human activity can be learned from gps ( positional ) data . however , most effort to date has concentrated on modeling single individuals or statistical properties of groups of people . moreover , prior work focused solely on modeling actual successful executions ( and not failed or attempted executions ) of the activities of interest . we , in contrast , take on the task of understanding human interactions , attempted interactions , and intentions from noisy sensor data in a fully relational multi-agent setting . we use a real-world game of capture the flag to illustrate our approach in a well-defined domain that involves many distinct cooperative and competitive joint activities . we model the domain using markov logic , a statistical-relational language , and learn a theory that jointly denoises the data and infers occurrences of high-level activities , such as a player capturing an enemy . our unified model combines constraints imposed by the geometry of the game area , the motion model of the players , and by the rules and dynamics of the game in a probabilistically and logically sound fashion . we show that while it may be impossible to directly detect a multi-agent activity due to sensor noise or malfunction , the occurrence of the activity can still be inferred by considering both its impact on the future behaviors of the people involved as well as the events that could have preceded it . further , we show that given a model of successfully performed multi-agent activities , along with a set of examples of failed attempts at the same activities , our system automatically learns an augmented model that is capable of recognizing success and failure , as well as goals of peoples actions with high accuracy . we compare our approach with other alternatives and show that our unified model , which takes into account not only relationships among individual players , but also relationships among activities over the entire length of a game , although more computationally costly , is significantly more accurate . finally , we demonstrate that explicitly modeling unsuccessful attempts boosts performance on other important recognition tasks .", "topics": ["high- and low-level", "interaction"]}
{"title": "large-scale feature selection of risk genetic factors for alzheimer 's disease via distributed group lasso regression", "abstract": "genome-wide association studies ( gwas ) have achieved great success in the genetic study of alzheimer 's disease ( ad ) . collaborative imaging genetics studies across different research institutions show the effectiveness of detecting genetic risk factors . however , the high dimensionality of gwas data poses significant challenges in detecting risk snps for ad . selecting relevant features is crucial in predicting the response variable . in this study , we propose a novel distributed feature selection framework ( dfsf ) to conduct the large-scale imaging genetics studies across multiple institutions . to speed up the learning process , we propose a family of distributed group lasso screening rules to identify irrelevant features and remove them from the optimization . then we select the relevant group features by performing the group lasso feature selection process in a sequence of parameters . finally , we employ the stability selection to rank the top risk snps that might help detect the early stage of ad . to the best of our knowledge , this is the first distributed feature selection model integrated with group lasso feature selection as well as detecting the risk genetic factors across multiple research institutions system . empirical studies are conducted on 809 subjects with 5.9 million snps which are distributed across several individual institutions , demonstrating the efficiency and effectiveness of the proposed method .", "topics": ["relevance"]}
{"title": "online adaptation of convolutional neural networks for video object segmentation", "abstract": "we tackle the task of semi-supervised video object segmentation , i.e . segmenting the pixels belonging to an object in the video using the ground truth pixel mask for the first frame . we build on the recently introduced one-shot video object segmentation ( osvos ) approach which uses a pretrained network and fine-tunes it on the first frame . while achieving impressive performance , at test time osvos uses the fine-tuned network in unchanged form and is not able to adapt to large changes in object appearance . to overcome this limitation , we propose online adaptive video object segmentation ( onavos ) which updates the network online using training examples selected based on the confidence of the network and the spatial configuration . additionally , we add a pretraining step based on objectness , which is learned on pascal . our experiments show that both extensions are highly effective and improve the state of the art on davis to an intersection-over-union score of 85.7 % .", "topics": ["image segmentation", "ground truth"]}
{"title": "diffusion convolutional recurrent neural network : data-driven traffic forecasting", "abstract": "spatiotemporal forecasting has various applications in neuroscience , climate and transportation domain . traffic forecasting is one canonical example of such learning task . the task is challenging due to ( 1 ) complex spatial dependency on road networks , ( 2 ) non-linear temporal dynamics with changing road conditions and ( 3 ) inherent difficulty of long-term forecasting . to address these challenges , we propose to model the traffic flow as a diffusion process on a directed graph and introduce diffusion convolutional recurrent neural network ( dcrnn ) , a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow . specifically , dcrnn captures the spatial dependency using bidirectional random walks on the graph , and the temporal dependency using the encoder-decoder architecture with scheduled sampling . we evaluate the framework on two real-world large scale road network traffic datasets and observe consistent improvement of 12 % - 15 % over state-of-the-art baselines .", "topics": ["baseline ( configuration management )", "sampling ( signal processing )"]}
{"title": "photo-to-caricature translation on faces in the wild", "abstract": "recently , image-to-image translation has been made much progress owing to the success of conditional generative adversarial networks ( cgans ) . however , it 's still very challenging for translation tasks with the requirement of high-level visual information conversion , such as photo-to-caricature translation that requires satire , exaggeration , lifelikeness and artistry . we present an approach for learning to translate faces in the wild from the source photo domain to the target caricature domain with different styles , which can also be used for other high-level image-to-image translation tasks . in order to capture global structure with local statistics while translation , we design a dual pathway model of cgan with one global discriminator and one patch discriminator . beyond standard convolution ( conv ) , we propose a new parallel convolution ( parconv ) to construct parallel convolutional neural networks ( parcnns ) for both global and patch discriminators , which can combine the information from previous layer with the current layer . for generator , we provide three more extra losses in association with adversarial loss to constrain consistency for generated output itself and with the target . also the style can be controlled by the input style info vector . experiments on photo-to-caricature translation of faces in the wild show considerable performance gain of our proposed method over state-of-the-art translation methods as well as its potential real applications .", "topics": ["high- and low-level", "convolution"]}
{"title": "on measuring the impact of human actions in the machine learning of a board game 's playing policies", "abstract": "we investigate systematically the impact of human intervention in the training of computer players in a strategy board game . in that game , computer players utilise reinforcement learning with neural networks for evolving their playing strategies and demonstrate a slow learning speed . human intervention can significantly enhance learning performance , but carry-ing it out systematically seems to be more of a problem of an integrated game development environment as opposed to automatic evolutionary learning .", "topics": ["reinforcement learning"]}
{"title": "bin packing under multiple objectives - a heuristic approximation approach", "abstract": "the article proposes a heuristic approximation approach to the bin packing problem under multiple objectives . in addition to the traditional objective of minimizing the number of bins , the heterogeneousness of the elements in each bin is minimized , leading to a biobjective formulation of the problem with a tradeoff between the number of bins and their heterogeneousness . an extension of the best-fit approximation algorithm is presented to solve the problem . experimental investigations have been carried out on benchmark instances of different size , ranging from 100 to 1000 items . encouraging results have been obtained , showing the applicability of the heuristic approach to the described problem .", "topics": ["approximation algorithm", "approximation"]}
{"title": "foot anthropometry device and single object image thresholding", "abstract": "this paper introduces a device , algorithm and graphical user interface to obtain anthropometric measurements of foot . presented device facilitates obtaining scale of image and image processing by taking one image from side foot and underfoot simultaneously . introduced image processing algorithm minimizes a noise criterion , which is suitable for object detection in single object images and outperforms famous image thresholding methods when lighting condition is poor . performance of image-based method is compared to manual method . image-based measurements of underfoot in average was 4mm less than actual measures . mean absolute error of underfoot length was 1.6mm , however length obtained from side foot had 4.4mm mean absolute error . furthermore , based on t-test and f-test results , no significant difference between manual and image-based anthropometry observed . in order to maintain anthropometry process performance in different situations user interface designed for handling changes in light conditions and altering speed of the algorithm .", "topics": ["image processing", "object detection"]}
{"title": "segmental recurrent neural networks", "abstract": "we introduce segmental recurrent neural networks ( srnns ) which define , given an input sequence , a joint probability distribution over segmentations of the input and labelings of the segments . representations of the input segments ( i.e . , contiguous subsequences of the input ) are computed by encoding their constituent tokens using bidirectional recurrent neural nets , and these `` segment embeddings '' are used to define compatibility scores with output labels . these local compatibility scores are integrated using a global semi-markov conditional random field . both fully supervised training -- in which segment boundaries and labels are observed -- as well as partially supervised training -- in which segment boundaries are latent -- are straightforward . experiments on handwriting recognition and joint chinese word segmentation/pos tagging show that , compared to models that do not explicitly represent segments such as bio tagging schemes and connectionist temporal classification ( ctc ) , srnns obtain substantially higher accuracies .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "hashmod : a hashing method for scalable 3d object detection", "abstract": "we present a scalable method for detecting objects and estimating their 3d poses in rgb-d data . to this end , we rely on an efficient representation of object views and employ hashing techniques to match these views against the input frame in a scalable way . while a similar approach already exists for 2d detection , we show how to extend it to estimate the 3d pose of the detected objects . in particular , we explore different hashing strategies and identify the one which is more suitable to our problem . we show empirically that the complexity of our method is sublinear with the number of objects and we enable detection and pose estimation of many 3d objects with high accuracy while outperforming the state-of-the-art in terms of runtime .", "topics": ["object detection", "scalability"]}
{"title": "deep recurrent neural networks for sequential phenotype prediction in genomics", "abstract": "in analyzing of modern biological data , we are often dealing with ill-posed problems and missing data , mostly due to high dimensionality and multicollinearity of the dataset . in this paper , we have proposed a system based on matrix factorization ( mf ) and deep recurrent neural networks ( drnns ) for genotype imputation and phenotype sequences prediction . in order to model the long-term dependencies of phenotype data , the new recurrent linear units ( relu ) learning strategy is utilized for the first time . the proposed model is implemented for parallel processing on central processing units ( cpus ) and graphic processing units ( gpus ) . performance of the proposed model is compared with other training algorithms for learning long-term dependencies as well as the sparse partial least square ( spls ) method on a set of genotype and phenotype data with 604 samples , 1980 single-nucleotide polymorphisms ( snps ) , and two traits . the results demonstrate performance of the relu training algorithm in learning long-term dependencies in rnns .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "wasserstein training of boltzmann machines", "abstract": "the boltzmann machine provides a useful framework to learn highly complex , multimodal and multiscale data distributions that occur in the real world . the default method to learn its parameters consists of minimizing the kullback-leibler ( kl ) divergence from training samples to the boltzmann model . we propose in this work a novel approach for boltzmann training which assumes that a meaningful metric between observations is given . this metric can be represented by the wasserstein distance between distributions , for which we derive a gradient with respect to the model parameters . minimization of this new wasserstein objective leads to generative models that are better when considering the metric and that have a cluster-like structure . we demonstrate the practical potential of these models for data completion and denoising , for which the metric between observations plays a crucial role .", "topics": ["generative model", "noise reduction"]}
{"title": "structural recurrent neural network ( srnn ) for group activity analysis", "abstract": "a group of persons can be analyzed at various semantic levels such as individual actions , their interactions , and the activity of the entire group . in this paper , we propose a structural recurrent neural network ( srnn ) that uses a series of interconnected rnns to jointly capture the actions of individuals , their interactions , as well as the group activity . while previous structural recurrent neural networks assumed that the number of nodes and edges is constant , we use a grid pooling layer to address the fact that the number of individuals in a group can vary . we evaluate two variants of the structural recurrent neural network on the volleyball dataset .", "topics": ["recurrent neural network", "interaction"]}
{"title": "the algorithm of noisy k-means", "abstract": "in this note , we introduce a new algorithm to deal with finite dimensional clustering with errors in variables . the design of this algorithm is based on recent theoretical advances ( see loustau ( 2013a , b ) ) in statistical learning with errors in variables . as the previous mentioned papers , the algorithm mixes different tools from the inverse problem literature and the machine learning community . coarsely , it is based on a two-step procedure : ( 1 ) a deconvolution step to deal with noisy inputs and ( 2 ) newton 's iterations as the popular k-means .", "topics": ["cluster analysis", "iteration"]}
{"title": "a generalized kernel approach to structured output learning", "abstract": "we study the problem of structured output learning from a regression perspective . we first provide a general formulation of the kernel dependency estimation ( kde ) problem using operator-valued kernels . we show that some of the existing formulations of this problem are special cases of our framework . we then propose a covariance-based operator-valued kernel that allows us to take into account the structure of the kernel feature space . this kernel operates on the output space and encodes the interactions between the outputs without any reference to the input space . to address this issue , we introduce a variant of our kde method based on the conditional covariance operator that in addition to the correlation between the outputs takes into account the effects of the input variables . finally , we evaluate the performance of our kde approach using both covariance and conditional covariance kernels on two structured output problems , and compare it to the state-of-the-art kernel-based structured output regression methods .", "topics": ["kernel ( operating system )", "feature vector"]}
{"title": "monte carlo matrix inversion policy evaluation", "abstract": "in 1950 , forsythe and leibler ( 1950 ) introduced a statistical technique for finding the inverse of a matrix by characterizing the elements of the matrix inverse as expected values of a sequence of random walks . barto and duff ( 1994 ) subsequently showed relations between this technique and standard dynamic programming and temporal differencing methods . the advantage of the monte carlo matrix inversion ( mcmi ) approach is that it scales better with respect to state-space size than alternative techniques . in this paper , we introduce an algorithm for performing reinforcement learning policy evaluation using mcmi . we demonstrate that mcmi improves on runtime over a maximum likelihood model-based policy evaluation approach and on both runtime and accuracy over the temporal differencing ( td ) policy evaluation approach . we further improve on mcmi policy evaluation by adding an importance sampling technique to our algorithm to reduce the variance of our estimator . lastly , we illustrate techniques for scaling up mcmi to large state spaces in order to perform policy improvement .", "topics": ["reinforcement learning"]}
{"title": "vision and learning for deliberative monocular cluttered flight", "abstract": "cameras provide a rich source of information while being passive , cheap and lightweight for small and medium unmanned aerial vehicles ( uavs ) . in this work we present the first implementation of receding horizon control , which is widely used in ground vehicles , with monocular vision as the only sensing mode for autonomous uav flight in dense clutter . we make it feasible on uavs via a number of contributions : novel coupling of perception and control via relevant and diverse , multiple interpretations of the scene around the robot , leveraging recent advances in machine learning to showcase anytime budgeted cost-sensitive feature selection , and fast non-linear regression for monocular depth prediction . we empirically demonstrate the efficacy of our novel pipeline via real world experiments of more than 2 kms through dense trees with a quadrotor built from off-the-shelf parts . moreover our pipeline is designed to combine information from other modalities like stereo and lidar as well if available .", "topics": ["nonlinear system", "autonomous car"]}
{"title": "practical kernel-based reinforcement learning", "abstract": "kernel-based reinforcement learning ( kbrl ) stands out among reinforcement learning algorithms for its strong theoretical guarantees . by casting the learning problem as a local kernel approximation , kbrl provides a way of computing a decision policy which is statistically consistent and converges to a unique solution . unfortunately , the model constructed by kbrl grows with the number of sample transitions , resulting in a computational cost that precludes its application to large-scale or on-line domains . in this paper we introduce an algorithm that turns kbrl into a practical reinforcement learning tool . kernel-based stochastic factorization ( kbsf ) builds on a simple idea : when a transition matrix is represented as the product of two stochastic matrices , one can swap the factors of the multiplication to obtain another transition matrix , potentially much smaller , which retains some fundamental properties of its precursor . kbsf exploits such an insight to compress the information contained in kbrl 's model into an approximator of fixed size . this makes it possible to build an approximation that takes into account both the difficulty of the problem and the associated computational cost . kbsf 's computational complexity is linear in the number of sample transitions , which is the best one can do without discarding data . moreover , the algorithm 's simple mechanics allow for a fully incremental implementation that makes the amount of memory used independent of the number of sample transitions . the result is a kernel-based reinforcement learning algorithm that can be applied to large-scale problems in both off-line and on-line regimes . we derive upper bounds for the distance between the value functions computed by kbrl and kbsf using the same data . we also illustrate the potential of our algorithm in an extensive empirical study in which kbsf is applied to difficult tasks based on real-world data .", "topics": ["approximation algorithm", "computational complexity theory"]}
{"title": "optimized method for iranian road signs detection and recognition system", "abstract": "road sign recognition is one of the core technologies in intelligent transport systems . in the current study , a robust and real-time method is presented to identify and detect the roads speed signs in road image in different situations . in our proposed method , first , the connected components are created in the main image using the edge detection and mathematical morphology and the location of the road signs extracted by the geometric and color data ; then the letters are segmented and recognized by multiclass support vector machine ( svms ) classifiers . regarding that the geometric and color features ate properly used in detection the location of the road signs , so it is not sensitive to the distance and noise and has higher speed and efficiency . in the result part , the proposed approach is applied on iranian road speed sign database and the detection and recognition accuracy rate achieved 98.66 % and 100 % respectively .", "topics": ["support vector machine"]}
{"title": "estimating continuous distributions in bayesian classifiers", "abstract": "when modeling a probability distribution with a bayesian network , we are faced with the problem of how to handle continuous variables . most previous work has either solved the problem by discretizing , or assumed that the data are generated by a single gaussian . in this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation . for a naive bayesian classifier , we present experimental results on a variety of natural and artificial domains , comparing two methods of density estimation : assuming normality and modeling each conditional distribution with a single gaussian ; and using nonparametric kernel density estimation . we observe large reductions in error on several natural and artificial data sets , which suggests that kernel estimation is a useful tool for learning bayesian models .", "topics": ["test set", "bayesian network"]}
{"title": "near-optimal adversarial policy switching for decentralized asynchronous multi-agent systems", "abstract": "a key challenge in multi-robot and multi-agent systems is generating solutions that are robust to other self-interested or even adversarial parties who actively try to prevent the agents from achieving their goals . the practicality of existing works addressing this challenge is limited to only small-scale synchronous decision-making scenarios or a single agent planning its best response against a single adversary with fixed , procedurally characterized strategies . in contrast this paper considers a more realistic class of problems where a team of asynchronous agents with limited observation and communication capabilities need to compete against multiple strategic adversaries with changing strategies . this problem necessitates agents that can coordinate to detect changes in adversary strategies and plan the best response accordingly . our approach first optimizes a set of stratagems that represent these best responses . these optimized stratagems are then integrated into a unified policy that can detect and respond when the adversaries change their strategies . the near-optimality of the proposed framework is established theoretically as well as demonstrated empirically in simulation and hardware .", "topics": ["simulation"]}
{"title": "a novel scheme for binarization of vehicle images using hierarchical histogram equalization technique", "abstract": "automatic license plate recognition system is a challenging area of research now-a-days and binarization is an integral and most important part of it . in case of a real life scenario , most of existing methods fail to properly binarize the image of a vehicle in a congested road , captured through a ccd camera . in the current work we have applied histogram equalization technique over the complete image and also over different hierarchy of image partitioning . a novel scheme is formulated for giving the membership value to each pixel for each hierarchy of histogram equalization . then the image is binarized depending on the net membership value of each pixel . the technique is exhaustively evaluated on the vehicle image dataset as well as the license plate dataset , giving satisfactory performances .", "topics": ["pixel"]}
{"title": "grapheme-to-phoneme conversion using multiple unbounded overlapping chunks", "abstract": "we present in this paper an original extension of two data-driven algorithms for the transcription of a sequence of graphemes into the corresponding sequence of phonemes . in particular , our approach generalizes the algorithm originally proposed by dedina and nusbaum ( d & n ) ( 1991 ) , which had originally been promoted as a model of the human ability to pronounce unknown words by analogy to familiar lexical items . we will show that dn 's algorithm performs comparatively poorly when evaluated on a realistic test set , and that our extension allows us to improve substantially the performance of the analogy-based model . we will also suggest that both algorithms can be reformulated in a much more general framework , which allows us to anticipate other useful extensions . however , considering the inability to define in these models important notions like lexical neighborhood , we conclude that both approaches fail to offer a proper model of the analogical processes involved in reading aloud .", "topics": ["test set"]}
{"title": "rendermap : exploiting the link between perception and rendering for dense mapping", "abstract": "we introduce an approach for the real-time ( 2hz ) creation of a dense map and alignment of a moving robotic agent within that map by rendering using a graphics processing unit ( gpu ) . this is done by recasting the scan alignment part of the dense mapping process as a rendering task . alignment errors are computed from rendering the scene , comparing with range data from the sensors , and minimized by an optimizer . the proposed approach takes advantage of the advances in rendering techniques for computer graphics and gpu hardware to accelerate the algorithm . moreover , it allows one to exploit information not used in classic dense mapping algorithms such as iterative closest point ( icp ) by rendering interfaces between the free space , occupied space and the unknown . the proposed approach leverages directly the rendering capabilities of the gpu , in contrast to other gpu-based approaches that deploy the gpu as a general purpose parallel computation platform . we argue that the proposed concept is a general consequence of treating perception problems as inverse problems of rendering . many perception problems can be recast into a form where much of the computation is replaced by render operations . this is not only efficient since rendering is fast , but also simpler to implement and will naturally benefit from future advancements in gpu speed and rendering techniques . furthermore , this general concept can go beyond addressing perception problems and can be used for other problem domains such as path planning .", "topics": ["computation", "sensor"]}
{"title": "one-shot visual imitation learning via meta-learning", "abstract": "in order for a robot to be a generalist that can perform a wide range of jobs , it must be able to acquire a wide variety of skills quickly and efficiently in complex unstructured environments . high-capacity models such as deep neural networks can enable a robot to represent complex skills , but learning each skill from scratch then becomes infeasible . in this work , we present a meta-imitation learning method that enables a robot to learn how to learn more efficiently , allowing it to acquire new skills from just a single demonstration . unlike prior methods for one-shot imitation , our method can scale to raw pixel inputs and requires data from significantly fewer prior tasks for effective learning of new skills . our experiments on both simulated and real robot platforms demonstrate the ability to learn new tasks , end-to-end , from a single visual demonstration .", "topics": ["simulation", "end-to-end principle"]}
{"title": "ranking-based black-box complexity", "abstract": "randomized search heuristics such as evolutionary algorithms , simulated annealing , and ant colony optimization are a broadly used class of general-purpose algorithms . analyzing them via classical methods of theoretical computer science is a growing field . while several strong runtime analysis results have appeared in the last 20 years , a powerful complexity theory for such algorithms is yet to be developed . we enrich the existing notions of black-box complexity by the additional restriction that not the actual objective values , but only the relative quality of the previously evaluated solutions may be taken into account by the black-box algorithm . many randomized search heuristics belong to this class of algorithms . we show that the new ranking-based model gives more realistic complexity estimates for some problems . for example , the class of all binary-value functions has a black-box complexity of $ o ( \\log n ) $ in the previous black-box models , but has a ranking-based complexity of $ \\theta ( n ) $ . for the class of all onemax functions , we present a ranking-based black-box algorithm that has a runtime of $ \\theta ( n / \\log n ) $ , which shows that the onemax problem does not become harder with the additional ranking-basedness restriction .", "topics": ["simulation", "heuristic"]}
{"title": "citlab argus for historical handwritten documents", "abstract": "we describe citlab 's recognition system for the htrts competition attached to the 13. international conference on document analysis and recognition , icdar 2015 . the task comprises the recognition of historical handwritten documents . the core algorithms of our system are based on multi-dimensional recurrent neural networks ( mdrnn ) and connectionist temporal classification ( ctc ) . the software modules behind that as well as the basic utility technologies are essentially powered by planet 's argus framework for intelligent text recognition and image processing .", "topics": ["image processing", "recurrent neural network"]}
{"title": "variable annealing length and parallelism in simulated annealing", "abstract": "in this paper , we propose : ( a ) a restart schedule for an adaptive simulated annealer , and ( b ) parallel simulated annealing , with an adaptive and parameter-free annealing schedule . the foundation of our approach is the modified lam annealing schedule , which adaptively controls the temperature parameter to track a theoretically ideal rate of acceptance of neighboring states . a sequential implementation of modified lam simulated annealing is almost parameter-free . however , it requires prior knowledge of the annealing length . we eliminate this parameter using restarts , with an exponentially increasing schedule of annealing lengths . we then extend this restart schedule to parallel implementation , executing several modified lam simulated annealers in parallel , with varying initial annealing lengths , and our proposed parallel annealing length schedule . to validate our approach , we conduct experiments on an np-hard scheduling problem with sequence-dependent setup constraints . we compare our approach to fixed length restarts , both sequentially and in parallel . our results show that our approach can achieve substantial performance gains , throughout the course of the run , demonstrating our approach to be an effective anytime algorithm .", "topics": ["simulation"]}
{"title": "thompson sampling for complex bandit problems", "abstract": "we consider stochastic multi-armed bandit problems with complex actions over a set of basic arms , where the decision maker plays a complex action rather than a basic arm in each round . the reward of the complex action is some function of the basic arms ' rewards , and the feedback observed may not necessarily be the reward per-arm . for instance , when the complex actions are subsets of the arms , we may only observe the maximum reward over the chosen subset . thus , feedback across complex actions may be coupled due to the nature of the reward function . we prove a frequentist regret bound for thompson sampling in a very general setting involving parameter , action and observation spaces and a likelihood function over them . the bound holds for discretely-supported priors over the parameter space and without additional structural properties such as closed-form posteriors , conjugate prior structure or independence across arms . the regret bound scales logarithmically with time but , more importantly , with an improved constant that non-trivially captures the coupling across complex actions due to the structure of the rewards . as applications , we derive improved regret bounds for classes of complex bandit problems involving selecting subsets of arms , including the first nontrivial regret bounds for nonlinear max reward feedback from subsets .", "topics": ["regret ( decision theory )", "nonlinear system"]}
{"title": "what can we learn about cnns from a large scale controlled object dataset ?", "abstract": "tolerance to image variations ( e.g . translation , scale , pose , illumination ) is an important desired property of any object recognition system , be it human or machine . moving towards increasingly bigger datasets has been trending in computer vision specially with the emergence of highly popular deep learning models . while being very useful for learning invariance to object inter- and intra-class shape variability , these large-scale wild datasets are not very useful for learning invariance to other parameters forcing researchers to resort to other tricks for training a model . in this work , we introduce a large-scale synthetic dataset , which is freely and publicly available , and use it to answer several fundamental questions regarding invariance and selectivity properties of convolutional neural networks . our dataset contains two parts : a ) objects shot on a turntable : 16 categories , 8 rotation angles , 11 cameras on a semicircular arch , 5 lighting conditions , 3 focus levels , variety of backgrounds ( 23.4 per instance ) generating 1320 images per instance ( over 20 million images in total ) , and b ) scenes : in which a robot arm takes pictures of objects on a 1:160 scale scene . we study : 1 ) invariance and selectivity of different cnn layers , 2 ) knowledge transfer from one object category to another , 3 ) systematic or random sampling of images to build a train set , 4 ) domain adaptation from synthetic to natural scenes , and 5 ) order of knowledge delivery to cnns . we also explore how our analyses can lead the field to develop more efficient cnns .", "topics": ["sampling ( signal processing )", "synthetic data"]}
{"title": "generalization tower network : a novel deep neural network architecture for multi-task learning", "abstract": "deep learning ( dl ) advances state-of-the-art reinforcement learning ( rl ) , by incorporating deep neural networks in learning representations from the input to rl . however , the conventional deep neural network architecture is limited in learning representations for multi-task rl ( mt-rl ) , as multiple tasks can refer to different kinds of representations . in this paper , we thus propose a novel deep neural network architecture , namely generalization tower network ( gtn ) , which can achieve mt-rl within a single learned model . specifically , the architecture of gtn is composed of both horizontal and vertical streams . in our gtn architecture , horizontal streams are used to learn representation shared in similar tasks . in contrast , the vertical streams are introduced to be more suitable for handling diverse tasks , which encodes hierarchical shared knowledge of these tasks . the effectiveness of the introduced vertical stream is validated by experimental results . experimental results further verify that our gtn architecture is able to advance the state-of-the-art mt-rl , via being tested on 51 atari games .", "topics": ["reinforcement learning"]}
{"title": "a theoretical guideline for designing an effective adaptive particle swarm", "abstract": "in this paper we theoretically investigate underlying assumptions that have been used for designing adaptive particle swarm optimization algorithms in the past years . we relate these assumptions to the movement patterns of particles controlled by coefficient values ( inertia weight and acceleration coefficient ) and introduce three factors , namely the autocorrelation of the particle positions , the average movement distance of the particle in each iteration , and the focus of the search , that describe these movement patterns . we show how these factors represent movement patterns of a particle within a swarm and how they are affected by particle coefficients ( i.e . , inertia weight and acceleration coefficients ) . we derive equations that provide exact coefficient values to guarantee achieving a desired movement pattern defined by these three factors within a swarm . we then relate these movements to the searching capability of particles and provide guideline for designing potentially successful adaptive methods to control coefficients in particle swarm . finally , we propose a new simple time adaptive particle swarm and compare its results with previous adaptive particle swarm approaches . our experiments show that the theoretical findings indeed provide a beneficial guideline for successful adaptation of the coefficients in the particle swarm optimization algorithm .", "topics": ["iteration", "coefficient"]}
{"title": "composing distributed representations of relational patterns", "abstract": "learning distributed representations for relation instances is a central technique in downstream nlp applications . in order to address semantic modeling of relational patterns , this paper constructs a new dataset that provides multiple similarity ratings for every pair of relational patterns on the existing dataset . in addition , we conduct a comparative study of different encoders including additive composition , rnn , lstm , and gru for composing distributed representations of relational patterns . we also present gated additive composition , which is an enhancement of additive composition with the gating mechanism . experiments show that the new dataset does not only enable detailed analyses of the different encoders , but also provides a gauge to predict successes of distributed representations of relational patterns in the relation classification task .", "topics": ["natural language processing", "encoder"]}
{"title": "learning the latent state space of time-varying graphs", "abstract": "from social networks to internet applications , a wide variety of electronic communication tools are producing streams of graph data ; where the nodes represent users and the edges represent the contacts between them over time . this has led to an increased interest in mechanisms to model the dynamic structure of time-varying graphs . in this work , we develop a framework for learning the latent state space of a time-varying email graph . we show how the framework can be used to find subsequences that correspond to global real-time events in the email graph ( e.g . vacations , breaks , ... etc . ) . these events impact the underlying graph process to make its characteristics non-stationary . within the framework , we compare two different representations of the temporal relationships ; discrete vs. probabilistic . we use the two representations as inputs to a mixture model to learn the latent state transitions that correspond to important changes in the email graph structure over time .", "topics": ["interaction"]}
{"title": "depth estimation using modified cost function for occlusion handling", "abstract": "the paper presents a novel approach to occlusion handling problem in depth estimation using three views . a solution based on modification of similarity cost function is proposed . during the depth estimation via optimization algorithms like graph cut similarity metric is constantly updated so that only non-occluded fragments in side views are considered . at each iteration of the algorithm non-occluded fragments are detected based on side view virtual depth maps synthesized from the best currently estimated depth map of the center view . then similarity metric is updated for correspondence search only in non-occluded regions of the side views . the experimental results , conducted on well-known 3d video test sequences , have proved that the depth maps estimated with the proposed approach provide about 1.25 db virtual view quality improvement in comparison to the virtual view synthesized based on depth maps generated by the state-of-the-art mpeg depth estimation reference software .", "topics": ["loss function", "map"]}
{"title": "network representation learning : a survey", "abstract": "with the widespread use of information technologies , information networks have increasingly become popular to capture complex relationships across various disciplines , such as social networks , citation networks , telecommunication networks , and biological networks . analyzing these networks sheds light on different aspects of social life such as the structure of society , information diffusion , and different patterns of communication . however , the large scale of information networks often makes network analytic tasks computationally expensive and intractable . recently , network representation learning has been proposed as a new learning paradigm that embeds network vertices into a low-dimensional vector space , by preserving network topology structure , vertex content , and other side information . this facilitates the original network to be easily handled in the new vector space for further analysis . in this survey , we perform a thorough review of the current literature on network representation learning in the field of data mining and machine learning . we propose a new categorization to analyze and summarize state-of-the-art network representation learning techniques according to the methodology they employ and the network information they preserve . finally , to facilitate research on this topic , we summarize benchmark datasets and evaluation methodologies , and discuss open issues and future research directions in this field .", "topics": ["feature learning", "data mining"]}
{"title": "intensity-only optical compressive imaging using a multiply scattering material and a double phase retrieval approach", "abstract": "in this paper , the problem of compressive imaging is addressed using natural randomization by means of a multiply scattering medium . to utilize the medium in this way , its corresponding transmission matrix must be estimated . to calibrate the imager , we use a digital micromirror device ( dmd ) as a simple , cheap , and high-resolution binary intensity modulator . we propose a phase retrieval algorithm which is well adapted to intensity-only measurements on the camera , and to the input binary intensity patterns , both to estimate the complex transmission matrix as well as image reconstruction . we demonstrate promising experimental results for the proposed algorithm using the mnist dataset of handwritten digits as example images .", "topics": ["mnist database"]}
{"title": "robojam : a musical mixture density network for collaborative touchscreen interaction", "abstract": "robojam is a machine-learning system for generating music that assists users of a touchscreen music app by performing responses to their short improvisations . this system uses a recurrent artificial neural network to generate sequences of touchscreen interactions and absolute timings , rather than high-level musical notes . to accomplish this , robojam 's network uses a mixture density layer to predict appropriate touch interaction locations in space and time . in this paper , we describe the design and implementation of robojam 's network and how it has been integrated into a touchscreen music app . a preliminary evaluation analyses the system in terms of training , musical generation and user interaction .", "topics": ["high- and low-level"]}
{"title": "sparse diffusion steepest-descent for one bit compressed sensing in wireless sensor networks", "abstract": "this letter proposes a sparse diffusion steepest-descent algorithm for one bit compressed sensing in wireless sensor networks . the approach exploits the diffusion strategy from distributed learning in the one bit compressed sensing framework . to estimate a common sparse vector cooperatively from only the sign of measurements , steepest-descent is used to minimize the suitable global and local convex cost functions . a diffusion strategy is suggested for distributive learning of the sparse vector . simulation results show the effectiveness of the proposed distributed algorithm compared to the state-of-the-art non distributive algorithms in the one bit compressed sensing framework .", "topics": ["simulation", "sparse matrix"]}
{"title": "automatic variational abc", "abstract": "approximate bayesian computation ( abc ) is a framework for performing likelihood-free posterior inference for simulation models . stochastic variational inference ( svi ) is an appealing alternative to the inefficient sampling approaches commonly used in abc . however , svi is highly sensitive to the variance of the gradient estimators , and this problem is exacerbated by approximating the likelihood . we draw upon recent advances in variance reduction for sv and likelihood-free inference using deterministic simulations to produce low variance gradient estimators of the variational lower-bound . by then exploiting automatic differentiation libraries we can avoid nearly all model-specific derivations . we demonstrate performance on three problems and compare to existing svi algorithms . our results demonstrate the correctness and efficiency of our algorithm .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "deep detection of people and their mobility aids for a hospital robot", "abstract": "robots operating in populated environments encounter many different types of people , some of whom might have an advanced need for cautious interaction , because of physical impairments or their advanced age . robots therefore need to recognize such advanced demands to provide appropriate assistance , guidance or other forms of support . in this paper , we propose a depth-based perception pipeline that estimates the position and velocity of people in the environment and categorizes them according to the mobility aids they use : pedestrian , person in wheelchair , person in a wheelchair with a person pushing them , person with crutches and person using a walker . we present a fast region proposal method that feeds a region-based convolutional network ( fast r-cnn ) . with this , we speed up the object detection process by a factor of seven compared to a dense sliding window approach . we furthermore propose a probabilistic position , velocity and class estimator to smooth the cnn 's detections and account for occlusions and misclassifications . in addition , we introduce a new hospital dataset with over 17,000 annotated rgb-d images . extensive experiments confirm that our pipeline successfully keeps track of people and their mobility aids , even in challenging situations with multiple people from different categories and frequent occlusions . videos of our experiments and the dataset are available at http : //www2.informatik.uni-freiburg.de/~kollmitz/mobilityaids", "topics": ["object detection", "sensor"]}
{"title": "an nlp assistant for clide", "abstract": "this report describes an nlp assistant for the collaborative development environment clide , that supports the development of nlp applications by providing easy access to some common nlp data structures . the assistant visualizes text fragments and their dependencies by displaying the semantic graph of a sentence , the coreference chain of a paragraph and mined triples that are extracted from a paragraph 's semantic graphs and linked using its coreference chain . using this information and a logic programming library , we create an nlp database which is used by a series of queries to mine the triples . the algorithm is tested by translating a natural language text describing a graph to an actual graph that is shown as an annotation in the text editor .", "topics": ["natural language processing"]}
{"title": "high performance latent variable models", "abstract": "latent variable models have accumulated a considerable amount of interest from the industry and academia for their versatility in a wide range of applications . a large amount of effort has been made to develop systems that is able to extend the systems to a large scale , in the hope to make use of them on industry scale data . in this paper , we describe a system that operates at a scale orders of magnitude higher than previous works , and an order of magnitude faster than state-of-the-art system at the same scale , at the same time showing more robustness and more accurate results . our system uses a number of advances in distributed inference : high performance in synchronization of sufficient statistics with relaxed consistency model ; fast sampling , using the metropolis-hastings-walker method to overcome dense generative models ; statistical modeling , moving beyond latent dirichlet allocation ( lda ) to pitman-yor distributions ( pdp ) and hierarchical dirichlet process ( hdp ) models ; sophisticated parameter projection schemes , to resolve the conflicts within the constraint between parameters arising from the relaxed consistency model . this work significantly extends the domain of applicability of what is commonly known as the parameter server . we obtain results with up to hundreds billion oftokens , thousands of topics , and a vocabulary of a few million token-types , using up to 60,000 processor cores operating on a production cluster of a large internet company . this demonstrates the feasibility to scale to problems orders of magnitude larger than any previously published work .", "topics": ["sampling ( signal processing )"]}
{"title": "sampled image tagging and retrieval methods on user generated content", "abstract": "traditional image tagging and retrieval algorithms have limited value as a result of being trained with heavily curated datasets . these limitations are most evident when arbitrary search words are used that do not intersect with training set labels . weak labels from user generated content ( ugc ) found in the wild ( e.g . , google photos , flickr , etc . ) have an almost unlimited number of unique words in the metadata tags . prior work on word embeddings successfully leveraged unstructured text with large vocabularies , and our proposed method seeks to apply similar cost functions to open source imagery . specifically , we train a deep learning image tagging and retrieval system on large scale , user generated content ( ugc ) using sampling methods and joint optimization of word embeddings . by using the yahoo ! flickr creative commons ( yfcc100m ) dataset , such an approach builds robustness to common unstructured data issues that include but are not limited to irrelevant tags , misspellings , multiple languages , polysemy , and tag imbalance . as a result , the final proposed algorithm will not only yield comparable results to state of the art in conventional image tagging , but will enable new capability to train algorithms on large , scale unstructured text in the yfcc100m dataset and outperform cited work in zero-shot capability .", "topics": ["relevance"]}
{"title": "understanding the disharmony between dropout and batch normalization by variance shift", "abstract": "this paper first answers the question `` why do the two most powerful techniques dropout and batch normalization ( bn ) often lead to a worse performance when they are combined together ? '' in both theoretical and statistical aspects . theoretically , we find that dropout would shift the variance of a specific neural unit when we transfer the state of that network from train to test . however , bn would maintain its statistical variance , which is accumulated from the entire learning procedure , in the test phase . the inconsistency of that variance ( we name this scheme as `` variance shift '' ) causes the unstable numerical behavior in inference that leads to more erroneous predictions finally , when applying dropout before bn . thorough experiments on densenet , resnet , resnext and wide resnet confirm our findings . according to the uncovered mechanism , we next explore several strategies that modifies dropout and try to overcome the limitations of their combination by avoiding the variance shift risks .", "topics": ["numerical analysis"]}
{"title": "fast approximate l_infty minimization : speeding up robust regression", "abstract": "minimization of the $ l_\\infty $ norm , which can be viewed as approximately solving the non-convex least median estimation problem , is a powerful method for outlier removal and hence robust regression . however , current techniques for solving the problem at the heart of $ l_\\infty $ norm minimization are slow , and therefore can not scale to large problems . a new method for the minimization of the $ l_\\infty $ norm is presented here , which provides a speedup of multiple orders of magnitude for data with high dimension . this method , termed fast $ l_\\infty $ minimization , allows robust regression to be applied to a class of problems which were previously inaccessible . it is shown how the $ l_\\infty $ norm minimization problem can be broken up into smaller sub-problems , which can then be solved extremely efficiently . experimental results demonstrate the radical reduction in computation time , along with robustness against large numbers of outliers in a few model-fitting problems .", "topics": ["time complexity", "computation"]}
{"title": "genetic representations for evolutionary minimization of network coding resources", "abstract": "we demonstrate how a genetic algorithm solves the problem of minimizing the resources used for network coding , subject to a throughput constraint , in a multicast scenario . a genetic algorithm avoids the computational complexity that makes the problem np-hard and , for our experiments , greatly improves on sub-optimal solutions of established methods . we compare two different genotype encodings , which tradeoff search space size with fitness landscape , as well as the associated genetic operators . our finding favors a smaller encoding despite its fewer intermediate solutions and demonstrates the impact of the modularity enforced by genetic operators on the performance of the algorithm .", "topics": ["computational complexity theory"]}
{"title": "end-to-end recovery of human shape and pose", "abstract": "we describe human mesh recovery ( hmr ) , an end-to-end framework for reconstructing a full 3d mesh of a human body from a single rgb image . in contrast to most current methods that compute 2d or 3d joint locations , we produce a richer and more useful mesh representation that is parameterized by shape and 3d joint angles . the main objective is to minimize the reprojection loss of keypoints , which allow our model to be trained using in-the-wild images that only have ground truth 2d annotations . however , reprojection loss alone is highly under constrained . in this work we address this problem by introducing an adversary trained to tell whether a human body parameter is real or not using a large database of 3d human meshes . we show that hmr can be trained with and without using any coupled 2d-to-3d supervision . we do not rely on intermediate 2d keypoint detection and infer 3d pose and shape parameters directly from image pixels . our model runs in real-time given a bounding box containing the person . we demonstrate our approach on various images in-the-wild and out-perform previous optimizationbased methods that output 3d meshes and show competitive results on tasks such as 3d joint location estimation and part segmentation .", "topics": ["ground truth", "sensor"]}
{"title": "the history began from alexnet : a comprehensive survey on deep learning approaches", "abstract": "deep learning has demonstrated tremendous success in variety of application domains in the past few years . this new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications , which helps to open new opportunity . there are different methods have been proposed on different category of learning approaches , which includes supervised , semi-supervised and un-supervised learning . the experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of image processing , computer vision , speech recognition , machine translation , art , medical imaging , medical information processing , robotics and control , bio-informatics , natural language processing ( nlp ) , cyber security , and many more . this report presents a brief survey on development of dl approaches , including deep neural network ( dnn ) , convolutional neural network ( cnn ) , recurrent neural network ( rnn ) including long short term memory ( lstm ) and gated recurrent units ( gru ) , auto-encoder ( ae ) , deep belief network ( dbn ) , generative adversarial network ( gan ) , and deep reinforcement learning ( drl ) . in addition , we have included recent development of proposed advanced variant dl techniques based on the mentioned dl approaches . furthermore , dl approaches have explored and evaluated in different application domains are also included in this survey . we have also comprised recently developed frameworks , sdks , and benchmark datasets that are used for implementing and evaluating deep learning approaches . there are some surveys have published on deep learning in neural networks [ 1 , 38 ] and a survey on rl [ 234 ] . however , those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [ 1 ] .", "topics": ["generative model", "supervised learning"]}
{"title": "unlabeled data for morphological generation with character-based sequence-to-sequence models", "abstract": "we present a semi-supervised way of training a character-based encoder-decoder recurrent neural network for morphological reinflection , the task of generating one inflected word form from another . this is achieved by using unlabeled tokens or random strings as training data for an autoencoding task , adapting a network for morphological reinflection , and performing multi-task training . we thus use limited labeled data more effectively , obtaining up to 9.9 % improvement over state-of-the-art baselines for 8 different languages .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "deep video generation , prediction and completion of human action sequences", "abstract": "current deep learning results on video generation are limited while there are only a few first results on video prediction and no relevant significant results on video completion . this is due to the severe ill-posedness inherent in these three problems . in this paper , we focus on human action videos , and propose a general , two-stage deep framework to generate human action videos with no constraints or arbitrary number of constraints , which uniformly address the three problems : video generation given no input frames , video prediction given the first few frames , and video completion given the first and last frames . to make the problem tractable , in the first stage we train a deep generative model that generates a human pose sequence from random noise . in the second stage , a skeleton-to-image network is trained , which is used to generate a human action video given the complete human pose sequence generated in the first stage . by introducing the two-stage strategy , we sidestep the original ill-posed problems while producing for the first time high-quality video generation/prediction/completion results of much longer duration . we present quantitative and qualitative evaluation to show that our two-stage approach outperforms state-of-the-art methods in video generation , prediction and video completion . our video result demonstration can be viewed at https : //iamacewhite.github.io/supp/index.html", "topics": ["generative model"]}
{"title": "thompson sampling for online learning with linear experts", "abstract": "in this note , we present a version of the thompson sampling algorithm for the problem of online linear generalization with full information ( i.e . , the experts setting ) , studied by kalai and vempala , 2005 . the algorithm uses a gaussian prior and time-varying gaussian likelihoods , and we show that it essentially reduces to kalai and vempala 's follow-the-perturbed-leader strategy , with exponentially distributed noise replaced by gaussian noise . this implies sqrt ( t ) regret bounds for thompson sampling ( with time-varying likelihood ) for online learning with full information .", "topics": ["sampling ( signal processing )", "regret ( decision theory )"]}
{"title": "a case study of algorithm selection for the traveling thief problem", "abstract": "many real-world problems are composed of several interacting components . in order to facilitate research on such interactions , the traveling thief problem ( ttp ) was created in 2013 as the combination of two well-understood combinatorial optimization problems . with this article , we contribute in four ways . first , we create a comprehensive dataset that comprises the performance data of 21 ttp algorithms on the full original set of 9720 ttp instances . second , we define 55 characteristics for all tpp instances that can be used to select the best algorithm on a per-instance basis . third , we use these algorithms and features to construct the first algorithm portfolios for ttp , clearly outperforming the single best algorithm . finally , we study which algorithms contribute most to this portfolio .", "topics": ["interaction"]}
{"title": "ranking with adaptive neighbors", "abstract": "retrieving the most similar objects in a large-scale database for a given query is a fundamental building block in many application domains , ranging from web searches , visual , cross media , and document retrievals . state-of-the-art approaches have mainly focused on capturing the underlying geometry of the data manifolds . graph-based approaches , in particular , define various diffusion processes on weighted data graphs . despite success , these approaches rely on fixed-weight graphs , making ranking sensitive to the input affinity matrix . in this study , we propose a new ranking algorithm that simultaneously learns the data affinity matrix and the ranking scores . the proposed optimization formulation assigns adaptive neighbors to each point in the data based on the local connectivity , and the smoothness constraint assigns similar ranking scores to similar data points . we develop a novel and efficient algorithm to solve the optimization problem . evaluations using synthetic and real datasets suggest that the proposed algorithm can outperform the existing methods .", "topics": ["optimization problem", "synthetic data"]}
{"title": "training recurrent answering units with joint loss minimization for vqa", "abstract": "we propose a novel algorithm for visual question answering based on a recurrent deep neural network , where every module in the network corresponds to a complete answering unit with attention mechanism by itself . the network is optimized by minimizing loss aggregated from all the units , which share model parameters while receiving different information to compute attention probability . for training , our model attends to a region within image feature map , updates its memory based on the question and attended image feature , and answers the question based on its memory state . this procedure is performed to compute loss in each step . the motivation of this approach is our observation that multi-step inferences are often required to answer questions while each problem may have a unique desirable number of steps , which is difficult to identify in practice . hence , we always make the first unit in the network solve problems , but allow it to learn the knowledge from the rest of units by backpropagation unless it degrades the model . to implement this idea , we early-stop training each unit as soon as it starts to overfit . note that , since more complex models tend to overfit on easier questions quickly , the last answering unit in the unfolded recurrent neural network is typically killed first while the first one remains last . we make a single-step prediction for a new question using the shared model . this strategy works better than the other options within our framework since the selected model is trained effectively from all units without overfitting . the proposed algorithm outperforms other multi-step attention based approaches using a single step prediction in vqa dataset .", "topics": ["recurrent neural network"]}
{"title": "automatic firewall rules generator for anomaly detection systems with apriori algorithm", "abstract": "network intrusion detection systems have become a crucial issue for computer systems security infrastructures . different methods and algorithms are developed and proposed in recent years to improve intrusion detection systems . the most important issue in current systems is that they are poor at detecting novel anomaly attacks . these kinds of attacks refer to any action that significantly deviates from the normal behaviour which is considered intrusion . this paper proposed a model to improve this problem based on data mining techniques . apriori algorithm is used to predict novel attacks and generate real-time rules for firewall . apriori algorithm extracts interesting correlation relationships among large set of data items . this paper illustrates how to use apriori algorithm in intrusion detection systems to cerate a automatic firewall rules generator to detect novel anomaly attack . apriori is the best-known algorithm to mine association rules . this is an innovative way to find association rules on large scale .", "topics": ["data mining"]}
{"title": "leaf vein segmentation using odd gabor filters and morphological operations", "abstract": "leaf vein forms the basis of leaf characterization and classification . different species have different leaf vein patterns . it is seen that leaf vein segmentation will help in maintaining a record of all the leaves according to their specific pattern of veins thus provide an effective way to retrieve and store information regarding various plant species in database as well as provide an effective means to characterize plants on the basis of leaf vein structure which is unique for every species . the algorithm proposes a new way of segmentation of leaf veins with the use of odd gabor filters and the use of morphological operations for producing a better output . the odd gabor filter gives an efficient output and is robust and scalable as compared with the existing techniques as it detects the fine fiber like veins present in leaves much more efficiently .", "topics": ["scalability"]}
{"title": "learning in the presence of corruption", "abstract": "in supervised learning one wishes to identify a pattern present in a joint distribution $ p $ , of instances , label pairs , by providing a function $ f $ from instances to labels that has low risk $ \\mathbb { e } _ { p } \\ell ( y , f ( x ) ) $ . to do so , the learner is given access to $ n $ iid samples drawn from $ p $ . in many real world problems clean samples are not available . rather , the learner is given access to samples from a corrupted distribution $ \\tilde { p } $ from which to learn , while the goal of predicting the clean pattern remains . there are many different types of corruption one can consider , and as of yet there is no general means to compare the relative ease of learning under these different corruption processes . in this paper we develop a general framework for tackling such problems as well as introducing upper and lower bounds on the risk for learning in the presence of corruption . our ultimate goal is to be able to make informed economic decisions in regards to the acquisition of data sets . for a certain subclass of corruption processes ( those that are \\emph { reconstructible } ) we achieve this goal in a particular sense . our lower bounds are in terms of the coefficient of ergodicity , a simple to calculate property of stochastic matrices . our upper bounds proceed via a generalization of the method of unbiased estimators appearing in recent work of natarajan et al and implicit in the earlier work of kearns .", "topics": ["supervised learning", "coefficient"]}
{"title": "materials to the russian-bulgarian comparative dictionary `` ead ''", "abstract": "this article presents a fragment of a new comparative dictionary `` a comparative dictionary of names of expansive action in russian and bulgarian languages '' . main features of the new web-based comparative dictionary are placed , the principles of its formation are shown , primary links between the word-matches are classified . the principal difference between translation dictionaries and the model of double comparison is also shown . the classification scheme of the pages is proposed . new concepts and keywords have been introduced . the real prototype of the dictionary with a few key pages is published . the broad debate about the possibility of this prototype to become a version of russian-bulgarian comparative dictionary of a new generation is available .", "topics": ["dictionary"]}
{"title": "spatial stimuli gradient sketch model", "abstract": "the inability of automated edge detection methods inspired from primal sketch models to accurately calculate object edges under the influence of pixel noise is an open problem . extending the principles of image perception i.e . weber-fechner law , and sheperd similarity law , we propose a new edge detection method and formulation that use perceived brightness and neighbourhood similarity calculations in the determination of robust object edges . the robustness of the detected edges is benchmark against sobel , sis , kirsch , and prewitt edge detection methods in an example face recognition problem showing statistically significant improvement in recognition accuracy and pixel noise tolerance .", "topics": ["gradient", "pixel"]}
{"title": "understanding zipf 's law of word frequencies through sample-space collapse in sentence formation", "abstract": "the formation of sentences is a highly structured and history-dependent process . the probability of using a specific word in a sentence strongly depends on the 'history ' of word-usage earlier in that sentence . we study a simple history-dependent model of text generation assuming that the sample-space of word usage reduces along sentence formation , on average . we first show that the model explains the approximate zipf law found in word frequencies as a direct consequence of sample-space reduction . we then empirically quantify the amount of sample-space reduction in the sentences of ten famous english books , by analysis of corresponding word-transition tables that capture which words can follow any given word in a text . we find a highly nested structure in these transition tables and show that this `nestedness ' is tightly related to the power law exponents of the observed word frequency distributions . with the proposed model it is possible to understand that the nestedness of a text can be the origin of the actual scaling exponent , and that deviations from the exact zipf law can be understood by variations of the degree of nestedness on a book-by-book basis . on a theoretical level we are able to show that in case of weak nesting , zipf 's law breaks down in a fast transition . unlike previous attempts to understand zipf 's law in language the sample-space reducing model is not based on assumptions of multiplicative , preferential , or self-organised critical mechanisms behind language formation , but simply used the empirically quantifiable parameter 'nestedness ' to understand the statistics of word frequencies .", "topics": ["approximation algorithm"]}
{"title": "a stochastic model of human visual attention with a dynamic bayesian network", "abstract": "recent studies in the field of human vision science suggest that the human responses to the stimuli on a visual display are non-deterministic . people may attend to different locations on the same visual input at the same time . based on this knowledge , we propose a new stochastic model of visual attention by introducing a dynamic bayesian network to predict the likelihood of where humans typically focus on a video scene . the proposed model is composed of a dynamic bayesian network with 4 layers . our model provides a framework that simulates and combines the visual saliency response and the cognitive state of a person to estimate the most probable attended regions . sample-based inference with markov chain monte-carlo based particle filter and stream processing with multi-core processors enable us to estimate human visual attention in near real time . experimental results have demonstrated that our model performs significantly better in predicting human visual attention compared to the previous deterministic models .", "topics": ["bayesian network", "markov chain"]}
{"title": "a computational perspective of the role of thalamus in cognition", "abstract": "thalamus has traditionally been considered as only a relay source of cortical inputs , with hierarchically organized cortical circuits serially transforming thalamic signals to cognitively-relevant representations . given the absence of local excitatory connections within the thalamus , the notion of thalamic `relay ' seemed like a reasonable description over the last several decades . recent advances in experimental approaches and theory provide a broader perspective on the role of the thalamus in cognitively-relevant cortical computations , and suggest that only a subset of thalamic circuit motifs fit the relay description . here , we discuss this perspective and highlight the potential role for the thalamus in dynamic selection of cortical representations through a combination of intrinsic thalamic computations and output signals that change cortical network functional parameters . we suggest that through the contextual modulation of cortical computation , thalamus and cortex jointly optimize the information/cost tradeoff in an emergent fashion . we emphasize that coordinated experimental and theoretical efforts will provide a path to understanding the role of the thalamus in cognition , along with an understanding to augment cognitive capacity in health and disease .", "topics": ["computation"]}
{"title": "learning graph matching", "abstract": "as a fundamental problem in pattern recognition , graph matching has applications in a variety of fields , from computer vision to computational biology . in graph matching , patterns are modeled as graphs and pattern recognition amounts to finding a correspondence between the nodes of different graphs . many formulations of this problem can be cast in general as a quadratic assignment problem , where a linear term in the objective function encodes node compatibility and a quadratic term encodes edge compatibility . the main research focus in this theme is about designing efficient algorithms for approximately solving the quadratic assignment problem , since it is np-hard . in this paper we turn our attention to a different question : how to estimate compatibility functions such that the solution of the resulting graph matching problem best matches the expected solution that a human would manually provide . we present a method for learning graph matching : the training examples are pairs of graphs and the `labels ' are matches between them . our experimental results reveal that learning can substantially improve the performance of standard graph matching algorithms . in particular , we find that simple linear assignment with such a learning scheme outperforms graduated assignment with bistochastic normalisation , a state-of-the-art quadratic assignment relaxation algorithm .", "topics": ["computer vision", "loss function"]}
{"title": "convex factorization machine for regression", "abstract": "we propose the convex factorization machine ( cfm ) , which is a convex variant of the widely used factorization machines ( fms ) . specifically , we employ a linear+quadratic model and regularize the linear term with the $ \\ell_2 $ -regularizer and the quadratic term with the trace norm regularizer . then , we formulate the cfm optimization as a semidefinite programming problem and propose an efficient optimization procedure with hazan 's algorithm . a key advantage of cfm over existing fms is that it can find a globally optimal solution , while fms may get a poor locally optimal solution since the objective function of fms is non-convex . in addition , the proposed algorithm is simple yet effective and can be implemented easily . finally , cfm is a general factorization method and can also be used for other factorization problems including including multi-view matrix factorization and tensor completion problems . through synthetic and movielens datasets , we first show that the proposed cfm achieves results competitive to fms . furthermore , in a toxicogenomics prediction task , we show that cfm outperforms a state-of-the-art tensor factorization method .", "topics": ["optimization problem", "loss function"]}
{"title": "bio-inspired data mining : treating malware signatures as biosequences", "abstract": "the application of machine learning to bioinformatics problems is well established . less well understood is the application of bioinformatics techniques to machine learning and , in particular , the representation of non-biological data as biosequences . the aim of this paper is to explore the effects of giving amino acid representation to problematic machine learning data and to evaluate the benefits of supplementing traditional machine learning with bioinformatics tools and techniques . the signatures of 60 computer viruses and 60 computer worms were converted into amino acid representations and first multiply aligned separately to identify conserved regions across different families within each class ( virus and worm ) . this was followed by a second alignment of all 120 aligned signatures together so that non-conserved regions were identified prior to input to a number of machine learning techniques . differences in length between virus and worm signatures after the first alignment were resolved by the second alignment . our first set of experiments indicates that representing computer malware signatures as amino acid sequences followed by alignment leads to greater classification and prediction accuracy . our second set of experiments indicates that checking the results of data mining from artificial virus and worm data against known proteins can lead to generalizations being made from the domain of naturally occurring proteins to malware signatures . however , further work is needed to determine the advantages and disadvantages of different representations and sequence alignment methods for handling problematic machine learning data .", "topics": ["data mining"]}
{"title": "fast parallel svm using data augmentation", "abstract": "as one of the most popular classifiers , linear svms still have challenges in dealing with very large-scale problems , even though linear or sub-linear algorithms have been developed recently on single machines . parallel computing methods have been developed for learning large-scale svms . however , existing methods rely on solving local sub-optimization problems . in this paper , we develop a novel parallel algorithm for learning large-scale linear svm . our approach is based on a data augmentation equivalent formulation , which casts the problem of learning svm as a bayesian inference problem , for which we can develop very efficient parallel sampling methods . we provide empirical results for this parallel sampling svm , and provide extensions for svr , non-linear kernels , and provide a parallel implementation of the crammer and singer model . this approach is very promising in its own right , and further is a very useful technique to parallelize a broader family of general maximum-margin models .", "topics": ["sampling ( signal processing )"]}
{"title": "d numbers theory : a generalization of dempster-shafer evidence theory", "abstract": "efficient modeling of uncertain information in real world is still an open issue . dempster-shafer evidence theory is one of the most commonly used methods . however , the dempster-shafer evidence theory has the assumption that the hypothesis in the framework of discernment is exclusive of each other . this condition can be violated in real applications , especially in linguistic decision making since the linguistic variables are not exclusive of each others essentially . in this paper , a new theory , called as d numbers theory ( dnt ) , is systematically developed to address this issue . the combination rule of two d numbers is presented . an coefficient is defined to measure the exclusive degree among the hypotheses in the framework of discernment . the combination rule of two d numbers is presented . if the exclusive coefficient is one which means that the hypothesis in the framework of discernment is exclusive of each other totally , the d combination is degenerated as the classical dempster combination rule . finally , a linguistic variables transformation of d numbers is presented to make a decision . a numerical example on linguistic evidential decision making is used to illustrate the efficiency of the proposed d numbers theory .", "topics": ["numerical analysis", "coefficient"]}
{"title": "universal dependencies for learner english", "abstract": "we introduce the treebank of learner english ( tle ) , the first publicly available syntactic treebank for english as a second language ( esl ) . the tle provides manually annotated pos tags and universal dependency ( ud ) trees for 5,124 sentences from the cambridge first certificate in english ( fce ) corpus . the ud annotations are tied to a pre-existing error annotation of the fce , whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence . further on , we delineate esl annotation guidelines that allow for consistent syntactic treatment of ungrammatical english . finally , we benchmark pos tagging and dependency parsing performance on the tle dataset and measure the effect of grammatical errors on parsing accuracy . we envision the treebank to support a wide range of linguistic and computational research on second language acquisition as well as automatic processing of ungrammatical language . the treebank is available at universaldependencies.org . the annotation manual used in this project and a graphical query engine are available at esltreebank.org .", "topics": ["parsing"]}
{"title": "binaryconnect : training deep neural networks with binary weights during propagations", "abstract": "deep neural networks ( dnn ) have achieved state-of-the-art results in a wide range of tasks , with the best results obtained with large training sets and large models . in the past , gpus enabled these breakthroughs because of their greater computational speed . in the future , faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices . as a result , there is much interest in research and development of dedicated hardware for deep learning ( dl ) . binary weights , i.e . , weights which are constrained to only two possible values ( e.g . -1 or 1 ) , would bring great benefits to specialized dl hardware by replacing many multiply-accumulate operations by simple accumulations , as multipliers are the most space and power-hungry components of the digital implementation of neural networks . we introduce binaryconnect , a method which consists in training a dnn with binary weights during the forward and backward propagations , while retaining precision of the stored weights in which gradients are accumulated . like other dropout schemes , we show that binaryconnect acts as regularizer and we obtain near state-of-the-art results with binaryconnect on the permutation-invariant mnist , cifar-10 and svhn .", "topics": ["value ( ethics )", "neural networks"]}
{"title": "hiding in the crowd : a massively distributed algorithm for private averaging with malicious adversaries", "abstract": "the amount of personal data collected in our everyday interactions with connected devices offers great opportunities for innovative services fueled by machine learning , as well as raises serious concerns for the privacy of individuals . in this paper , we propose a massively distributed protocol for a large set of users to privately compute averages over their joint data , which can then be used to learn predictive models . our protocol can find a solution of arbitrary accuracy , does not rely on a third party and preserves the privacy of users throughout the execution in both the honest-but-curious and malicious adversary models . specifically , we prove that the information observed by the adversary ( the set of maliciours users ) does not significantly reduce the uncertainty in its prediction of private values compared to its prior belief . the level of privacy protection depends on a quantity related to the laplacian matrix of the network graph and generally improves with the size of the graph . furthermore , we design a verification procedure which offers protection against malicious users joining the service with the goal of manipulating the outcome of the algorithm .", "topics": ["interaction"]}
{"title": "variational deep embedding : an unsupervised and generative approach to clustering", "abstract": "clustering is among the most fundamental tasks in computer vision and machine learning . in this paper , we propose variational deep embedding ( vade ) , a novel unsupervised generative clustering approach within the framework of variational auto-encoder ( vae ) . specifically , vade models the data generative procedure with a gaussian mixture model ( gmm ) and a deep neural network ( dnn ) : 1 ) the gmm picks a cluster ; 2 ) from which a latent embedding is generated ; 3 ) then the dnn decodes the latent embedding into observables . inference in vade is done in a variational way : a different dnn is used to encode observables to latent embeddings , so that the evidence lower bound ( elbo ) can be optimized using stochastic gradient variational bayes ( sgvb ) estimator and the reparameterization trick . quantitative comparisons with strong baselines are included in this paper , and experimental results show that vade significantly outperforms the state-of-the-art clustering methods on 4 benchmarks from various modalities . moreover , by vade 's generative nature , we show its capability of generating highly realistic samples for any specified cluster , without using supervised information during training . lastly , vade is a flexible and extensible framework for unsupervised generative clustering , more general mixture models than gmm can be easily plugged in .", "topics": ["cluster analysis", "calculus of variations"]}
{"title": "self-localization using visual experience across domains", "abstract": "in this study , we aim to solve the single-view robot self-localization problem by using visual experience across domains . although the bag-of-words method constitutes a popular approach to single-view localization , it fails badly when it 's visual vocabulary is learned and tested in different domains . further , we are interested in using a cross-domain setting , in which the visual vocabulary is learned in different seasons and routes from the input query/database scenes . our strategy is to mine a cross-domain visual experience , a library of raw visual images collected in different domains , to discover the relevant visual patterns that effectively explain the input scene , and use them for scene retrieval . in particular , we show that the appearance and the pose of the mined visual patterns of a query scene can be efficiently and discriminatively matched against those of the database scenes by employing image-to-class distance and spatial pyramid matching . experimental results obtained using a novel cross-domain dataset show that our system achieves promising results despite our visual vocabulary being learned and tested in different domains .", "topics": ["robot"]}
{"title": "an unsupervised algorithm for learning lie group transformations", "abstract": "we present several theoretical contributions which allow lie groups to be fit to high dimensional datasets . transformation operators are represented in their eigen-basis , reducing the computational complexity of parameter estimation to that of training a linear transformation model . a transformation specific `` blurring '' operator is introduced that allows inference to escape local minima via a smoothing of the transformation space . a penalty on traversed manifold distance is added which encourages the discovery of sparse , minimal distance , transformations between states . both learning and inference are demonstrated using these methods for the full set of affine transformations on natural image patches . transformation operators are then trained on natural video sequences . it is shown that the learned video transformations provide a better description of inter-frame differences than the standard motion model based on rigid translation .", "topics": ["computational complexity theory", "sparse matrix"]}
{"title": "graduality in argumentation", "abstract": "argumentation is based on the exchange and valuation of interacting arguments , followed by the selection of the most acceptable of them ( for example , in order to take a decision , to make a choice ) . starting from the framework proposed by dung in 1995 , our purpose is to introduce 'graduality ' in the selection of the best arguments , i.e . , to be able to partition the set of the arguments in more than the two usual subsets of 'selected ' and 'non-selected ' arguments in order to represent different levels of selection . our basic idea is that an argument is all the more acceptable if it can be preferred to its attackers . first , we discuss general principles underlying a 'gradual ' valuation of arguments based on their interactions . following these principles , we define several valuation models for an abstract argumentation system . then , we introduce 'graduality ' in the concept of acceptability of arguments . we propose new acceptability classes and a refinement of existing classes taking advantage of an available 'gradual ' valuation .", "topics": ["value ( ethics )", "interaction"]}
{"title": "a harmonic extension approach for collaborative ranking", "abstract": "we present a new perspective on graph-based methods for collaborative ranking for recommender systems . unlike user-based or item-based methods that compute a weighted average of ratings given by the nearest neighbors , or low-rank approximation methods using convex optimization and the nuclear norm , we formulate matrix completion as a series of semi-supervised learning problems , and propagate the known ratings to the missing ones on the user-user or item-item graph globally . the semi-supervised learning problems are expressed as laplace-beltrami equations on a manifold , or namely , harmonic extension , and can be discretized by a point integral method . we show that our approach does not impose a low-rank euclidean subspace on the data points , but instead minimizes the dimension of the underlying manifold . our method , named ldm ( low dimensional manifold ) , turns out to be particularly effective in generating rankings of items , showing decent computational efficiency and robust ranking quality compared to state-of-the-art methods .", "topics": ["supervised learning"]}
{"title": "design , implementation and simulation of a cloud computing system for enhancing real-time video services by using vanet and onboard navigation systems", "abstract": "in this paper , we propose a design for novel and experimental cloud computing systems . the proposed system aims at enhancing computational , communicational and annalistic capabilities of road navigation services by merging several independent technologies , namely vision-based embedded navigation systems , prominent cloud computing systems ( ccss ) and vehicular ad-hoc network ( vanet ) . this work presents our initial investigations by describing the design of a global generic system . the designed system has been experimented with various scenarios of video-based road services . moreover , the associated architecture has been implemented on a small-scale simulator of an in-vehicle embedded system . the implemented architecture has been experimented in the case of a simulated road service to aid the police agency . the goal of this service is to recognize and track searched individuals and vehicles in a real-time monitoring system remotely connected to moving cars . the presented work demonstrates the potential of our system for efficiently enhancing and diversifying real-time video services in road environments .", "topics": ["simulation"]}
{"title": "learning transformation rules to find grammatical relations", "abstract": "grammatical relationships are an important level of natural language processing . we present a trainable approach to find these relationships through transformation sequences and error-driven learning . our approach finds grammatical relationships between core syntax groups and bypasses much of the parsing phase . on our training and test set , our procedure achieves 63.6 % recall and 77.3 % precision ( f-score = 69.8 ) .", "topics": ["test set", "natural language processing"]}
{"title": "the graphical lasso : new insights and alternatives", "abstract": "the graphical lasso \\citep { fht2007a } is an algorithm for learning the structure in an undirected gaussian graphical model , using $ \\ell_1 $ regularization to control the number of zeros in the precision matrix $ { \\b\\theta } = { \\b\\sigma } ^ { -1 } $ \\citep { bga2008 , yuan_lin_07 } . the { \\texttt r } package \\gl\\ \\citep { fht2007a } is popular , fast , and allows one to efficiently build a path of models for different values of the tuning parameter . convergence of \\gl\\ can be tricky ; the converged precision matrix might not be the inverse of the estimated covariance , and occasionally it fails to converge with warm starts . in this paper we explain this behavior , and propose new algorithms that appear to outperform \\gl . by studying the `` normal equations '' we see that , \\gl\\ is solving the { \\em dual } of the graphical lasso penalized likelihood , by block coordinate ascent ; a result which can also be found in \\cite { bga2008 } . in this dual , the target of estimation is $ \\b\\sigma $ , the covariance matrix , rather than the precision matrix $ \\b\\theta $ . we propose similar primal algorithms \\pgl\\ and \\dpgl , that also operate by block-coordinate descent , where $ \\b\\theta $ is the optimization target . we study all of these algorithms , and in particular different approaches to solving their coordinate sub-problems . we conclude that \\dpgl\\ is superior from several points of view .", "topics": ["graphical model", "matrix regularization"]}
{"title": "rnn fisher vectors for action recognition and image annotation", "abstract": "recurrent neural networks ( rnns ) have had considerable success in classifying and predicting sequences . we demonstrate that rnns can be effectively used in order to encode sequences and provide effective representations . the methodology we use is based on fisher vectors , where the rnns are the generative probabilistic models and the partial derivatives are computed using backpropagation . state of the art results are obtained in two central but distant tasks , which both rely on sequences : video action recognition and image annotation . we also show a surprising transfer learning result from the task of image annotation to the task of video action recognition .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "sparse subspace clustering : algorithm , theory , and applications", "abstract": "in many real-world problems , we are dealing with collections of high-dimensional data , such as images , videos , text and web documents , dna microarray data , and more . often , high-dimensional data lie close to low-dimensional structures corresponding to several classes or categories the data belongs to . in this paper , we propose and study an algorithm , called sparse subspace clustering ( ssc ) , to cluster data points that lie in a union of low-dimensional subspaces . the key idea is that , among infinitely many possible representations of a data point in terms of other points , a sparse representation corresponds to selecting a few points from the same subspace . this motivates solving a sparse optimization program whose solution is used in a spectral clustering framework to infer the clustering of data into subspaces . since solving the sparse optimization program is in general np-hard , we consider a convex relaxation and show that , under appropriate conditions on the arrangement of subspaces and the distribution of data , the proposed minimization program succeeds in recovering the desired sparse representations . the proposed algorithm can be solved efficiently and can handle data points near the intersections of subspaces . another key advantage of the proposed algorithm with respect to the state of the art is that it can deal with data nuisances , such as noise , sparse outlying entries , and missing entries , directly by incorporating the model of the data into the sparse optimization program . we demonstrate the effectiveness of the proposed algorithm through experiments on synthetic data as well as the two real-world problems of motion segmentation and face clustering .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "collaborative representation classification ensemble for face recognition", "abstract": "collaborative representation classification ( crc ) for face recognition attracts a lot attention recently due to its good recognition performance and fast speed . compared to sparse representation classification ( src ) , crc achieves a comparable recognition performance with 10-1000 times faster speed . in this paper , we propose to ensemble several crc models to promote the recognition rate , where each crc model uses different and divergent randomly generated biologically-inspired features as the face representation . the proposed ensemble algorithm calculates an ensemble weight for each crc model that guided by the underlying classification rule of crc . the obtained weights reflect the confidences of those crc models where the more confident crc models have larger weights . the proposed weighted ensemble method proves to be very effective and improves the performance of each crc model significantly . extensive experiments are conducted to show the superior performance of the proposed method .", "topics": ["sparse matrix"]}
{"title": "low-dose ct denoising with convolutional neural network", "abstract": "to reduce the potential radiation risk , low-dose ct has attracted much attention . however , simply lowering the radiation dose will lead to significant deterioration of the image quality . in this paper , we propose a noise reduction method for low-dose ct via deep neural network without accessing original projection data . a deep convolutional neural network is trained to transform low-dose ct images towards normal-dose ct images , patch by patch . visual and quantitative evaluation demonstrates a competing performance of the proposed method .", "topics": ["noise reduction"]}
{"title": "frame stacking and retaining for recurrent neural network acoustic model", "abstract": "frame stacking is broadly applied in end-to-end neural network training like connectionist temporal classification ( ctc ) , and it leads to more accurate models and faster decoding . however , it is not well-suited to conventional neural network based on context-dependent state acoustic model , if the decoder is unchanged . in this paper , we propose a novel frame retaining method which is applied in decoding . the system which combined frame retaining with frame stacking could reduces the time consumption of both training and decoding . long short-term memory ( lstm ) recurrent neural networks ( rnns ) using it achieve almost linear training speedup and reduces relative 41\\ % real time factor ( rtf ) . at the same time , recognition performance is no degradation or improves sightly on shenma voice search dataset in mandarin .", "topics": ["recurrent neural network", "end-to-end principle"]}
{"title": "continuous optimization for fields of experts denoising works", "abstract": "several recent papers use image denoising with a fields of experts prior to benchmark discrete optimization methods . we show that a non-linear least squares solver significantly outperforms all known discrete methods on this problem .", "topics": ["noise reduction"]}
{"title": "memory-efficient kernel pca via partial matrix sampling and nonconvex optimization : a model-free analysis of local minima", "abstract": "kernel pca is a widely used nonlinear dimension reduction technique in machine learning , but storing the kernel matrix is notoriously challenging when the sample size is large . inspired by yi et al . [ 2016 ] , where the idea of partial matrix sampling followed by nonconvex optimization is proposed for matrix completion and robust pca , we apply a similar approach to memory-efficient kernel pca . in theory , with no assumptions on the kernel matrix in terms of eigenvalues or eigenvectors , we established a model-free theory for the low-rank approximation based on any local minimum of the proposed objective function . as interesting byproducts , when the underlying positive semidefinite matrix is assumed to be low-rank and highly structured , corollaries of our main theorem improve the state-of-the-art results of ge et al . [ 2016 , 2017 ] for nonconvex matrix completion with no spurious local minima . numerical experiments also show that our approach is competitive in terms of approximation accuracy compared to the well-known nystr\\ '' { o } m algorithm for kernel pca .", "topics": ["sampling ( signal processing )", "numerical analysis"]}
{"title": "on the practically interesting instances of maxcut", "abstract": "the complexity of a computational problem is traditionally quantified based on the hardness of its worst case . this approach has many advantages and has led to a deep and beautiful theory . however , from the practical perspective , this leaves much to be desired . in application areas , practically interesting instances very often occupy just a tiny part of an algorithm 's space of instances , and the vast majority of instances are simply irrelevant . addressing these issues is a major challenge for theoretical computer science which may make theory more relevant to the practice of computer science . following bilu and linial , we apply this perspective to maxcut , viewed as a clustering problem . using a variety of techniques , we investigate practically interesting instances of this problem . specifically , we show how to solve in polynomial time distinguished , metric , expanding and dense instances of maxcut under mild stability assumptions . in particular , $ ( 1+\\epsilon ) $ -stability ( which is optimal ) suffices for metric and dense maxcut . we also show how to solve in polynomial time $ \\omega ( \\sqrt { n } ) $ -stable instances of maxcut , substantially improving the best previously known result .", "topics": ["cluster analysis", "time complexity"]}
{"title": "robust subspace recovery via bi-sparsity pursuit", "abstract": "successful applications of sparse models in computer vision and machine learning imply that in many real-world applications , high dimensional data is distributed in a union of low dimensional subspaces . nevertheless , the underlying structure may be affected by sparse errors and/or outliers . in this paper , we propose a bi-sparse model as a framework to analyze this problem and provide a novel algorithm to recover the union of subspaces in presence of sparse corruptions . we further show the effectiveness of our method by experiments on both synthetic data and real-world vision data .", "topics": ["synthetic data", "computer vision"]}
{"title": "natural tts synthesis by conditioning wavenet on mel spectrogram predictions", "abstract": "this paper describes tacotron 2 , a neural network architecture for speech synthesis directly from text . the system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms , followed by a modified wavenet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms . our model achieves a mean opinion score ( mos ) of $ 4.53 $ comparable to a mos of $ 4.58 $ for professionally recorded speech . to validate our design choices , we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to wavenet instead of linguistic , duration , and $ f_0 $ features . we further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the wavenet architecture .", "topics": ["map"]}
{"title": "climbing the kaggle leaderboard by exploiting the log-loss oracle", "abstract": "in the context of data-mining competitions ( e.g . , kaggle , kddcup , ilsvrc challenge ) , we show how access to an oracle that reports a contestant 's log-loss score on the test set can be exploited to deduce the ground-truth of some of the test examples . by applying this technique iteratively to batches of $ m $ examples ( for small $ m $ ) , all of the test labels can eventually be inferred . in this paper , ( 1 ) we demonstrate this attack on the first stage of a recent kaggle competition ( intel & mobileodt cancer screening ) and use it to achieve a log-loss of $ 0.00000 $ ( and thus attain a rank of # 4 out of 848 contestants ) , without ever training a classifier to solve the actual task . ( 2 ) we prove an upper bound on the batch size $ m $ as a function of the floating-point resolution of the probability estimates that the contestant submits for the labels . ( 3 ) we derive , and demonstrate in simulation , a more flexible attack that can be used even when the oracle reports the accuracy on an unknown ( but fixed ) subset of the test set 's labels . these results underline the importance of evaluating contestants based only on test data that the oracle does not examine .", "topics": ["test set", "data mining"]}
{"title": "distributed evolutionary computation using rest", "abstract": "this paper analises distributed evolutionary computation based on the representational state transfer ( rest ) protocol , which overlays a farming model on evolutionary computation . an approach to evolutionary distributed optimisation of multilayer perceptrons ( mlp ) using rest and language perl has been done . in these experiments , a master-slave based evolutionary algorithm ( ea ) has been implemented , where slave processes evaluate the costly fitness function ( training a mlp to solve a classification problem ) . obtained results show that the parallel version of the developed programs obtains similar or better results using much less time than the sequential version , obtaining a good speedup .", "topics": ["mathematical optimization", "computation"]}
{"title": "stochastic phonological grammars and acceptability", "abstract": "in foundational works of generative phonology it is claimed that subjects can reliably discriminate between possible but non-occurring words and words that could not be english . in this paper we examine the use of a probabilistic phonological parser for words to model experimentally-obtained judgements of the acceptability of a set of nonsense words . we compared various methods of scoring the goodness of the parse as a predictor of acceptability . we found that the probability of the worst part is not the best score of acceptability , indicating that classical generative phonology and optimality theory miss an important fact , as these approaches do not recognise a mechanism by which the frequency of well-formed parts may ameliorate the unacceptability of low-frequency parts . we argue that probabilistic generative grammars are demonstrably a more psychologically realistic model of phonological competence than standard generative phonology or optimality theory .", "topics": ["parsing"]}
{"title": "optimization of tree ensembles", "abstract": "tree ensemble models such as random forests and boosted trees are among the most widely used and practically successful predictive models in applied machine learning and business analytics . although such models have been used to make predictions based on exogenous , uncontrollable independent variables , they are increasingly being used to make predictions where the independent variables are controllable and are also decision variables . in this paper , we study the problem of tree ensemble optimization : given a tree ensemble that predicts some dependent variable using controllable independent variables , how should we set these variables so as to maximize the predicted value ? we formulate the problem as a mixed-integer optimization problem . we theoretically examine the strength of our formulation , provide a hierarchy of approximate formulations with bounds on approximation quality and exploit the structure of the problem to develop two large-scale solution methods , one based on benders decomposition and one based on iteratively generating tree split constraints . we test our methodology on real data sets , including two case studies in drug design and customized pricing , and show that our methodology can efficiently solve large-scale instances to near or full optimality , and outperforms solutions obtained by heuristic approaches . in our drug design case , we show how our approach can identify compounds that efficiently trade-off predicted performance and novelty with respect to existing , known compounds . in our customized pricing case , we show how our approach can efficiently determine optimal store-level prices under a random forest model that delivers excellent predictive accuracy .", "topics": ["approximation algorithm", "optimization problem"]}
{"title": "a context-aware natural language generator for dialogue systems", "abstract": "we present a novel natural language generation system for spoken dialogue systems capable of entraining ( adapting ) to users ' way of speaking , providing contextually appropriate responses . the generator is based on recurrent neural networks and the sequence-to-sequence approach . it is fully trainable from data which include preceding context along with responses to be generated . we show that the context-aware generator yields significant improvements over the baseline in both automatic metrics and a human pairwise preference test .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "selection of future events from a time series in relation to estimations of forecasting uncertainty", "abstract": "a new general procedure for a priori selection of more predictable events from a time series of observed variable is proposed . the procedure is applicable to time series which contains different types of events that feature significantly different predictability , or , in other words , to heteroskedastic time series . a priori selection of future events in accordance to expected uncertainty of their forecasts may be helpful for making practical decisions . the procedure first implies creation of two neural network based forecasting models , one of which is aimed at prediction of conditional mean and other - conditional dispersion , and then elaboration of the rule for future event selection into groups of more and less predictable events . the method is demonstrated and tested by the example of the computer generated time series , and then applied to the real world time series , dow jones industrial average index .", "topics": ["time series"]}
{"title": "decision under uncertainty in diagnosis", "abstract": "this paper describes the incorporation of uncertainty in diagnostic reasoning based on the set covering model of reggia et . al . extended to what in the artificial intelligence dichotomy between deep and compiled ( shallow , surface ) knowledge based diagnosis may be viewed as the generic form at the compiled end of the spectrum . a major undercurrent in this is advocating the need for a strong underlying model and an integrated set of support tools for carrying such a model in order to deal with uncertainty .", "topics": ["artificial intelligence"]}
{"title": "a very low resource language speech corpus for computational language documentation experiments", "abstract": "most speech and language technologies are trained with massive amounts of speech and text information . however , most of the world languages do not have such resources or stable orthography . systems constructed under these almost zero resource conditions are not only promising for speech technology but also for computational language documentation . the goal of computational language documentation is to help field linguists to ( semi- ) automatically analyze and annotate audio recordings of endangered and unwritten languages . example tasks are automatic phoneme discovery or lexicon discovery from the speech signal . this paper presents a speech corpus collected during a realistic language documentation process . it is made up of 5k speech utterances in mboshi ( bantu c25 ) aligned to french text translations . speech transcriptions are also made available : they correspond to a non-standard graphemic form close to the language phonology . we present how the data was collected , cleaned and processed and we illustrate its use through a zero-resource task : spoken term discovery . the dataset is made available to the community for reproducible computational language documentation experiments and their evaluation .", "topics": ["speech recognition"]}
{"title": "efficient loss-based decoding on graphs for extreme classification", "abstract": "in extreme classification problems , learning algorithms are required to map instances to labels from an extremely large label set . we build on a recent extreme classification framework with logarithmic time and space , and on a general approach for error correcting output coding ( ecoc ) , and introduce a flexible and efficient approach accompanied by bounds . our framework employs output codes induced by graphs , and offers a tradeoff between accuracy and model size . we show how to find the sweet spot of this tradeoff using only the training data . our experimental study demonstrates the validity of our assumptions and claims , and shows the superiority of our method compared with state-of-the-art algorithms .", "topics": ["test set", "time complexity"]}
{"title": "sshmt : semi-supervised hierarchical merge tree for electron microscopy image segmentation", "abstract": "region-based methods have proven necessary for improving segmentation accuracy of neuronal structures in electron microscopy ( em ) images . most region-based segmentation methods use a scoring function to determine region merging . such functions are usually learned with supervised algorithms that demand considerable ground truth data , which are costly to collect . we propose a semi-supervised approach that reduces this demand . based on a merge tree structure , we develop a differentiable unsupervised loss term that enforces consistent predictions from the learned function . we then propose a bayesian model that combines the supervised and the unsupervised information for probabilistic learning . the experimental results on three em data sets demonstrate that by using a subset of only 3 % to 7 % of the entire ground truth data , our approach consistently performs close to the state-of-the-art supervised method with the full labeled data set , and significantly outperforms the supervised method with the same labeled subset .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "hyperparameter importance across datasets", "abstract": "with the advent of automated machine learning , automated hyperparameter optimization methods are by now routinely used . however , this progress is not yet matched by equal progress on automatic analyses that yield information beyond performance-optimizing hyperparameter settings . in this work , we aim to answer the following two questions : given an algorithm , what are generally its most important hyperparameters , and what are good priors over their hyperparameters ' ranges to draw values from ? we present methodology and a framework to answer these questions based on meta-learning across many datasets . we apply this methodology using the experimental meta-data available on openml to determine the most important hyperparameters of support vector machines , random forests and adaboost , and to infer priors for all their hyperparameters . our results , obtained fully automatically , provide a quantitative basis to focus efforts in both manual algorithm design and in automated hyperparameter optimization . our experiments confirm that the selected hyperparameters are indeed the most important ones and that our obtained priors also lead to improvements in hyperparameter optimization .", "topics": ["support vector machine", "value ( ethics )"]}
{"title": "towards end-to-end optimisation of functional image analysis pipelines", "abstract": "the study of neurocognitive tasks requiring accurate localisation of activity often rely on functional magnetic resonance imaging , a widely adopted technique that makes use of a pipeline of data processing modules , each involving a variety of parameters . these parameters are frequently set according to the local goal of each specific module , not accounting for the rest of the pipeline . given recent success of neural network research in many different domains , we propose to convert the whole data pipeline into a deep neural network , where the parameters involved are jointly optimised by the network to best serve a common global goal . as a proof of concept , we develop a module able to adaptively apply the most suitable spatial smoothing to every brain volume for each specific neuroimaging task , and we validate its results in a standard brain decoding experiment .", "topics": ["mathematical optimization", "end-to-end principle"]}
{"title": "how not to evaluate your dialogue system : an empirical study of unsupervised evaluation metrics for dialogue response generation", "abstract": "we investigate evaluation metrics for dialogue response generation systems where supervised labels , such as task completion , are not available . recent works in response generation have adopted metrics from machine translation to compare a model 's generated response to a single target response . we show that these metrics correlate very weakly with human judgements in the non-technical twitter domain , and not at all in the technical ubuntu domain . we provide quantitative and qualitative results highlighting specific weaknesses in existing metrics , and provide recommendations for future development of better automatic evaluation metrics for dialogue systems .", "topics": ["machine translation", "end-to-end principle"]}
{"title": "a neural autoregressive approach to collaborative filtering", "abstract": "this paper proposes cf-nade , a neural autoregressive architecture for collaborative filtering ( cf ) tasks , which is inspired by the restricted boltzmann machine ( rbm ) based cf model and the neural autoregressive distribution estimator ( nade ) . we first describe the basic cf-nade model for cf tasks . then we propose to improve the model by sharing parameters between different ratings . a factored version of cf-nade is also proposed for better scalability . furthermore , we take the ordinal nature of the preferences into consideration and propose an ordinal cost to optimize cf-nade , which shows superior performance . finally , cf-nade can be extended to a deep model , with only moderately increased computational complexity . experimental results show that cf-nade with a single hidden layer beats all previous state-of-the-art methods on movielens 1m , movielens 10m , and netflix datasets , and adding more hidden layers can further improve the performance .", "topics": ["computational complexity theory", "scalability"]}
{"title": "zm-net : real-time zero-shot image manipulation network", "abstract": "many problems in image processing and computer vision ( e.g . colorization , style transfer ) can be posed as 'manipulating ' an input image into a corresponding output image given a user-specified guiding signal . a holy-grail solution towards generic image manipulation should be able to efficiently alter an input image with any personalized signals ( even signals unseen during training ) , such as diverse paintings and arbitrary descriptive attributes . however , existing methods are either inefficient to simultaneously process multiple signals ( let alone generalize to unseen signals ) , or unable to handle signals from other modalities . in this paper , we make the first attempt to address the zero-shot image manipulation task . we cast this problem as manipulating an input image according to a parametric model whose key parameters can be conditionally generated from any guiding signal ( even unseen ones ) . to this end , we propose the zero-shot manipulation net ( zm-net ) , a fully-differentiable architecture that jointly optimizes an image-transformation network ( tnet ) and a parameter network ( pnet ) . the pnet learns to generate key transformation parameters for the tnet given any guiding signal while the tnet performs fast zero-shot image manipulation according to both signal-dependent parameters from the pnet and signal-invariant parameters from the tnet itself . extensive experiments show that our zm-net can perform high-quality image manipulation conditioned on different forms of guiding signals ( e.g . style images and attributes ) in real-time ( tens of milliseconds per image ) even for unseen signals . moreover , a large-scale style dataset with over 20,000 style images is also constructed to promote further research .", "topics": ["image processing", "computer vision"]}
{"title": "an adaptive genetic algorithm for solving n-queens problem", "abstract": "in this paper a metaheuristic approach for solving the n-queens problem is introduced to find the best possible solution in a reasonable amount of time . genetic algorithm is used with a novel fitness function as the metaheuristic . the aim of n-queens problem is to place n queens on an n x n chessboard , in a way so that no queen is in conflict with the others . chromosome representation and genetic operations like mutation and crossover are described in detail . results show that this approach yields promising and satisfactory results in less time compared to that obtained from the previous approaches for several large values of n .", "topics": ["heuristic"]}
{"title": "youtube-8m : a large-scale video classification benchmark", "abstract": "many recent advancements in computer vision are attributed to large datasets . open-source software packages for machine learning and inexpensive commodity hardware have reduced the barrier of entry for exploring novel approaches at scale . it is possible to train models over millions of examples within a few days . although large-scale datasets exist for image understanding , such as imagenet , there are no comparable size video classification datasets . in this paper , we introduce youtube-8m , the largest multi-label video classification dataset , composed of ~8 million videos ( 500k hours of video ) , annotated with a vocabulary of 4800 visual entities . to get the videos and their labels , we used a youtube video annotation system , which labels videos with their main topics . while the labels are machine-generated , they have high-precision and are derived from a variety of human-based signals including metadata and query click signals . we filtered the video labels ( knowledge graph entities ) using both automated and manual curation strategies , including asking human raters if the labels are visually recognizable . then , we decoded each video at one-frame-per-second , and used a deep cnn pre-trained on imagenet to extract the hidden representation immediately prior to the classification layer . finally , we compressed the frame features and make both the features and video-level labels available for download . we trained various ( modest ) classification models on the dataset , evaluated them using popular evaluation metrics , and report them as baselines . despite the size of the dataset , some of our models train to convergence in less than a day on a single machine using tensorflow . we plan to release code for training a tensorflow model and for computing metrics .", "topics": ["baseline ( configuration management )", "feature learning"]}
{"title": "unrestricted facial geometry reconstruction using image-to-image translation", "abstract": "it has been recently shown that neural networks can recover the geometric structure of a face from a single given image . a common denominator of most existing face geometry reconstruction methods is the restriction of the solution space to some low-dimensional subspace . while such a model significantly simplifies the reconstruction problem , it is inherently limited in its expressiveness . as an alternative , we propose an image-to-image translation network that jointly maps the input image to a depth image and a facial correspondence map . this explicit pixel-based mapping can then be utilized to provide high quality reconstructions of diverse faces under extreme expressions , using a purely geometric refinement process . in the spirit of recent approaches , the network is trained only with synthetic data , and is then evaluated on in-the-wild facial images . both qualitative and quantitative analyses demonstrate the accuracy and the robustness of our approach .", "topics": ["synthetic data", "map"]}
{"title": "towards trainable media : using waves for neural network-style training", "abstract": "in this paper we study the concept of using the interaction between waves and a trainable medium in order to construct a matrix-vector multiplier . in particular we study such a device in the context of the backpropagation algorithm , which is commonly used for training neural networks . here , the weights of the connections between neurons are trained by multiplying a `forward ' signal with a backwards propagating `error ' signal . we show that this concept can be extended to trainable media , where the gradient for the local wave number is given by multiplying signal waves and error waves . we provide a numerical example of such a system with waves traveling freely in a trainable medium , and we discuss a potential way to build such a device in an integrated photonics chip .", "topics": ["numerical analysis", "machine translation"]}
{"title": "designing statistical language learners : experiments on noun compounds", "abstract": "the goal of this thesis is to advance the exploration of the statistical language learning design space . in pursuit of that goal , the thesis makes two main theoretical contributions : ( i ) it identifies a new class of designs by specifying an architecture for natural language analysis in which probabilities are given to semantic forms rather than to more superficial linguistic elements ; and ( ii ) it explores the development of a mathematical theory to predict the expected accuracy of statistical language learning systems in terms of the volume of data used to train them . the theoretical work is illustrated by applying statistical language learning designs to the analysis of noun compounds . both syntactic and semantic analysis of noun compounds are attempted using the proposed architecture . empirical comparisons demonstrate that the proposed syntactic model is significantly better than those previously suggested , approaching the performance of human judges on the same task , and that the proposed semantic model , the first statistical approach to this problem , exhibits significantly better accuracy than the baseline strategy . these results suggest that the new class of designs identified is a promising one . the experiments also serve to highlight the need for a widely applicable theory of data requirements .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "analogs of linguistic structure in deep representations", "abstract": "we investigate the compositional structure of message vectors computed by a deep network trained on a communication game . by comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions , we are able to identify aligned ( vector , utterance ) pairs with the same meaning . we then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation , conjunction , and disjunction . our results suggest that neural representations are capable of spontaneously developing a `` syntax '' with functional analogues to qualitative properties of natural language .", "topics": ["natural language", "encoder"]}
{"title": "generating weather forecast texts with case based reasoning", "abstract": "several techniques have been used to generate weather forecast texts . in this paper , case based reasoning ( cbr ) is proposed for weather forecast text generation because similar weather conditions occur over time and should have similar forecast texts . cbr-meteo , a system for generating weather forecast texts was developed using a generic framework ( jcolibri ) which provides modules for the standard components of the cbr architecture . the advantage in a cbr approach is that systems can be built in minimal time with far less human effort after initial consultation with experts . the approach depends heavily on the goodness of the retrieval and revision components of the cbr process . we evaluated cbrmeteo with nist , an automated metric which has been shown to correlate well with human judgements for this domain . the system shows comparable performance with other nlg systems that perform the same task .", "topics": ["autonomous car"]}
{"title": "on timeml-compliant temporal expression extraction in turkish", "abstract": "it is commonly acknowledged that temporal expression extractors are important components of larger natural language processing systems like information retrieval and question answering systems . extraction and normalization of temporal expressions in turkish has not been given attention so far except the extraction of some date and time expressions within the course of named entity recognition . as timeml is the current standard of temporal expression and event annotation in natural language texts , in this paper , we present an analysis of temporal expressions in turkish based on the related timeml classification ( i.e . , date , time , duration , and set expressions ) . we have created a lexicon for turkish temporal expressions and devised considerably wide-coverage patterns using the lexical classes as the building blocks . we believe that the proposed patterns , together with convenient normalization rules , can be readily used by prospective temporal expression extraction tools for turkish .", "topics": ["natural language processing", "natural language"]}
{"title": "an algorithmic solution to the five-point pose problem based on the cayley representation of rotations", "abstract": "we give a new algorithmic solution to the well-known five-point relative pose problem . our approach does not deal with the famous cubic constraint on an essential matrix . instead , we use the cayley representation of rotations in order to obtain a polynomial system from epipolar constraints . solving that system , we directly get relative rotation and translation parameters of the cameras in terms of roots of a 10th degree polynomial .", "topics": ["polynomial"]}
{"title": "towards automatic wild animal monitoring : identification of animal species in camera-trap images using very deep convolutional neural networks", "abstract": "non intrusive monitoring of animals in the wild is possible using camera trapping framework , which uses cameras triggered by sensors to take a burst of images of animals in their habitat . however camera trapping framework produces a high volume of data ( in the order on thousands or millions of images ) , which must be analyzed by a human expert . in this work , a method for animal species identification in the wild using very deep convolutional neural networks is presented . multiple versions of the snapshot serengeti dataset were used in order to probe the ability of the method to cope with different challenges that camera-trap images demand . the method reached 88.9 % of accuracy in top-1 and 98.1 % in top-5 in the evaluation set using a residual network topology . also , the results show that the proposed method outperforms previous approximations and proves that recognition in camera-trap images can be automated .", "topics": ["approximation", "sensor"]}
{"title": "piecewise linear multilayer perceptrons and dropout", "abstract": "we propose a new type of hidden layer for a multilayer perceptron , and demonstrate that it obtains the best reported performance for an mlp on the mnist dataset .", "topics": ["gradient", "mnist database"]}
{"title": "learning using 1-local membership queries", "abstract": "classic machine learning algorithms learn from labelled examples . for example , to design a machine translation system , a typical training set will consist of english sentences and their translation . there is a stronger model , in which the algorithm can also query for labels of new examples it creates . e.g , in the translation task , the algorithm can create a new english sentence , and request its translation from the user during training . this combination of examples and queries has been widely studied . yet , despite many theoretical results , query algorithms are almost never used . one of the main causes for this is a report ( baum and lang , 1992 ) on very disappointing empirical performance of a query algorithm . these poor results were mainly attributed to the fact that the algorithm queried for labels of examples that are artificial , and impossible to interpret by humans . in this work we study a new model of local membership queries ( awasthi et al . , 2012 ) , which tries to resolve the problem of artificial queries . in this model , the algorithm is only allowed to query the labels of examples which are close to examples from the training set . e.g . , in translation , the algorithm can change individual words in a sentence it has already seen , and then ask for the translation . in this model , the examples queried by the algorithm will be close to natural examples and hence , hopefully , will not appear as artificial or random . we focus on 1-local queries ( i.e . , queries of distance 1 from an example in the training sample ) . we show that 1-local membership queries are already stronger than the standard learning model . we also present an experiment on a well known nlp task of sentiment analysis . in this experiment , the users were asked to provide more information than merely indicating the label . we present results that illustrate that this extra information is beneficial in practice .", "topics": ["test set", "natural language processing"]}
{"title": "parallel predictive entropy search for batch global optimization of expensive objective functions", "abstract": "we develop parallel predictive entropy search ( ppes ) , a novel algorithm for bayesian optimization of expensive black-box objective functions . at each iteration , ppes aims to select a batch of points which will maximize the information gain about the global maximizer of the objective . well known strategies exist for suggesting a single evaluation point based on previous observations , while far fewer are known for selecting batches of points to evaluate in parallel . the few batch selection schemes that have been studied all resort to greedy methods to compute an optimal batch . to the best of our knowledge , ppes is the first non-greedy batch bayesian optimization strategy . we demonstrate the benefit of this approach in optimization performance on both synthetic and real world applications , including problems in machine learning , rocket science and robotics .", "topics": ["synthetic data", "iteration"]}
{"title": "direction-aware semi-dense slam", "abstract": "to aide simultaneous localization and mapping ( slam ) , future perception systems will incorporate forms of scene understanding . in a step towards fully integrated probabilistic geometric scene understanding , localization and mapping we propose the first direction-aware semi-dense slam system . it jointly infers the directional stata center world ( scw ) segmentation and a surfel-based semi-dense map while performing real-time camera tracking . the joint scw map model connects a scene-wide bayesian nonparametric dirichlet process von-mises-fisher mixture model ( dp-vmf ) prior on surfel orientations with the local surfel locations via a conditional random field ( crf ) . camera tracking leverages the scw segmentation to improve efficiency via guided observation selection . results demonstrate improved slam accuracy and tracking efficiency at state of the art performance .", "topics": ["cluster analysis", "autonomous car"]}
{"title": "applying gsat to non-clausal formulas", "abstract": "in this paper we describe how to modify gsat so that it can be applied to non-clausal formulas . the idea is to use a particular `` score '' function which gives the number of clauses of the cnf conversion of a formula which are false under a given truth assignment . its value is computed in linear time , without constructing the cnf conversion itself . the proposed methodology applies to most of the variants of gsat proposed so far .", "topics": ["time complexity"]}
{"title": "self-reflective risk-aware artificial cognitive modeling for robot response to human behaviors", "abstract": "in order for cooperative robots ( `` co-robots '' ) to respond to human behaviors accurately and efficiently in human-robot collaboration , interpretation of human actions , awareness of new situations , and appropriate decision making are all crucial abilities for co-robots . for this purpose , the human behaviors should be interpreted by co-robots in the same manner as human peers . to address this issue , a novel interpretability indicator is introduced so that robot actions are appropriate to the current human behaviors . in addition , the complete consideration of all potential situations of a robot 's environment is nearly impossible in real-world applications , making it difficult for the co-robot to act appropriately and safely in new scenarios . this is true even when the pretrained model is highly accurate in a known situation . for effective and safe teaming with humans , we introduce a new generalizability indicator that allows a co-robot to self-reflect and reason about when an observation falls outside the co-robot 's learned model . based on topic modeling and two novel indicators , we propose a new self-reflective risk-aware artificial cognitive ( srac ) model . the co-robots are able to consider action risks and identify new situations so that better decisions can be made . experiments both using real-world datasets and on physical robots suggest that our srac model significantly outperforms the traditional methodology and enables better decision making in response to human activities .", "topics": ["robot"]}
{"title": "learning to gather without communication", "abstract": "a standard belief on emerging collective behavior is that it emerges from simple individual rules . most of the mathematical research on such collective behavior starts from imperative individual rules , like always go to the center . but how could an ( optimal ) individual rule emerge during a short period within the group lifetime , especially if communication is not available . we argue that such rules can actually emerge in a group in a short span of time via collective ( multi-agent ) reinforcement learning , i.e learning via rewards and punishments . we consider the gathering problem : several agents ( social animals , swarming robots ... ) must gather around a same position , which is not determined in advance . they must do so without communication on their planned decision , just by looking at the position of other agents . we present the first experimental evidence that a gathering behavior can be learned without communication in a partially observable environment . the learned behavior has the same properties as a self-stabilizing distributed algorithm , as processes can gather from any initial state ( and thus tolerate any transient failure ) . besides , we show that it is possible to tolerate the brutal loss of up to 90\\ % of agents without significant impact on the behavior .", "topics": ["reinforcement learning", "robot"]}
{"title": "initialization of multilayer forecasting artifical neural networks", "abstract": "in this paper , a new method was developed for initialising artificial neural networks predicting dynamics of time series . initial weighting coefficients were determined for neurons analogously to the case of a linear prediction filter . moreover , to improve the accuracy of the initialization method for a multilayer neural network , some variants of decomposition of the transformation matrix corresponding to the linear prediction filter were suggested . the efficiency of the proposed neural network prediction method by forecasting solutions of the lorentz chaotic system is shown in this paper .", "topics": ["time series", "coefficient"]}
{"title": "recurrent memory networks for language modeling", "abstract": "recurrent neural networks ( rnn ) have obtained excellent result in many natural language processing ( nlp ) tasks . however , understanding and interpreting the source of this success remains a challenge . in this paper , we propose recurrent memory network ( rmn ) , a novel rnn architecture , that not only amplifies the power of rnn but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data . we demonstrate the power of rmn on language modeling and sentence completion tasks . on language modeling , rmn outperforms long short-term memory ( lstm ) network on three large german , italian , and english dataset . additionally we perform in-depth analysis of various linguistic dimensions that rmn captures . on sentence completion challenge , for which it is essential to capture sentence coherence , our rmn obtains 69.2 % accuracy , surpassing the previous state-of-the-art by a large margin .", "topics": ["natural language processing", "recurrent neural network"]}
{"title": "learning the reward function for a misspecified model", "abstract": "in model-based reinforcement learning it is typical to treat the problems of learning the dynamics model and learning the reward function separately . however , when the dynamics model is flawed , it may generate erroneous states that would never occur in the true environment . a reward function trained only to map environment states to rewards ( as is typical ) would have little guidance in such states . this paper presents a novel error bound that accounts for the reward model 's behavior in states sampled from the model . this bound is used to extend the existing hallucinated dagger-mc algorithm , which offers theoretical performance guarantees in deterministic mdps that do not assume a perfect model can be learned . empirically , this approach to reward learning can yield dramatic improvements in control performance when the dynamics model is flawed .", "topics": ["reinforcement learning"]}
{"title": "computerized tomography with total variation and with shearlets", "abstract": "to reduce the x-ray dose in computerized tomography ( ct ) , many constrained optimization approaches have been proposed aiming at minimizing a regularizing function that measures lack of consistency with some prior knowledge about the object that is being imaged , subject to a ( predetermined ) level of consistency with the detected attenuation of x-rays . proponents of the shearlet transform in the regularizing function claim that the reconstructions so obtained are better than those produced using tv for texture preservation ( but may be worse for noise reduction ) . in this paper we report results related to this claim . in our reported experiments using simulated ct data collection of the head , reconstructions whose shearlet transform has a small $ \\ell_1 $ -norm are not more efficacious than reconstructions that have a small tv value . our experiments for making such comparisons use the recently-developed superiorization methodology for both regularizing functions . superiorization is an automated procedure for turning an iterative algorithm for producing images that satisfy a primary criterion ( such as consistency with the observed measurements ) into its superiorized version that will produce results that , according to the primary criterion are as good as those produced by the original algorithm , but in addition are superior to them according to a secondary ( regularizing ) criterion . the method presented for superiorization involving the $ \\ell_1 $ -norm of the shearlet transform is novel and is quite general : it can be used for any regularizing function that is defined as the $ \\ell_1 $ -norm of a transform specified by the application of a matrix . because in the previous literature the split bregman algorithm is used for similar purposes , a section is included comparing the results of the superiorization algorithm with the split bregman algorithm .", "topics": ["noise reduction", "simulation"]}
{"title": "modification of conceptual clustering algorithm cobweb for numerical data using fuzzy membership function", "abstract": "modification of a conceptual clustering algorithm cobweb for the purpose of its application for numerical data is offered . keywords : clustering , algorithm cobweb , numerical data , fuzzy membership function .", "topics": ["cluster analysis", "numerical analysis"]}
{"title": "domain generalization via invariant feature representation", "abstract": "this paper investigates domain generalization : how to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains ? we propose domain-invariant component analysis ( dica ) , a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains , whilst preserving the functional relationship between input and output variables . a learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains , motivating the proposed algorithm . experimental results on synthetic and real-world datasets demonstrate that dica successfully learns invariant features and improves classifier performance in practice .", "topics": ["synthetic data"]}
{"title": "towards universal semantic tagging", "abstract": "the paper proposes the task of universal semantic tagging -- -tagging word tokens with language-neutral , semantically informative tags . we argue that the task , with its independent nature , contributes to better semantic analysis for wide-coverage multilingual text . we present the initial version of the semantic tagset and show that ( a ) the tags provide semantically fine-grained information , and ( b ) they are suitable for cross-lingual semantic parsing . an application of the semantic tagging in the parallel meaning bank supports both of these points as the tags contribute to formal lexical semantics and their cross-lingual projection . as a part of the application , we annotate a small corpus with the semantic tags and present new baseline result for universal semantic tagging .", "topics": ["baseline ( configuration management )", "parsing"]}
{"title": "understanding the social cascading of geekspeak and the upshots for social cognitive systems", "abstract": "barring swarm robotics , a substantial share of current machine-human and machine-machine learning and interaction mechanisms are being developed and fed by results of agent-based computer simulations , game-theoretic models , or robotic experiments based on a dyadic communication pattern . yet , in real life , humans no less frequently communicate in groups , and gain knowledge and take decisions basing on information cumulatively gleaned from more than one single source . these properties should be taken into consideration in the design of autonomous artificial cognitive systems construed to interact with learn from more than one contact or 'neighbour ' . to this end , significant practical import can be gleaned from research applying strict science methodology to human and social phenomena , e.g . to discovery of realistic creativity potential spans , or the 'exposure thresholds ' after which new information could be accepted by a cognitive agent . the results will be presented of a project analysing the social propagation of neologisms in a microblogging service . from local , low-level interactions and information flows between agents inventing and imitating discrete lexemes we aim to describe the processes of the emergence of more global systemic order and dynamics , using the latest methods of complexity science . whether in order to mimic them , or to 'enhance ' them , parameters gleaned from complexity science approaches to humans ' social and humanistic behaviour should subsequently be incorporated as points of reference in the field of robotics and human-machine interaction .", "topics": ["value ( ethics )", "high- and low-level"]}
{"title": "learning approximate inference networks for structured prediction", "abstract": "structured prediction energy networks ( spens ; belanger & mccallum 2016 ) use neural network architectures to define energy functions that can capture arbitrary dependencies among parts of structured outputs . prior work used gradient descent for inference , relaxing the structured output to a set of continuous variables and then optimizing the energy with respect to them . we replace this use of gradient descent with a neural network trained to approximate structured argmax inference . this `` inference network '' outputs continuous values that we treat as the output structure . we develop large-margin training criteria for joint training of the structured energy function and inference network . on multi-label classification we report speed-ups of 10-60x compared to ( belanger et al , 2017 ) while also improving accuracy . for sequence labeling with simple structured energies , our approach performs comparably to exact inference while being much faster at test time . we then demonstrate improved accuracy by augmenting the energy with a `` label language model '' that scores entire output label sequences , showing it can improve handling of long-distance dependencies in part-of-speech tagging . finally , we show how inference networks can replace dynamic programming for test-time inference in conditional random fields , suggestive for their general use for fast inference in structured settings .", "topics": ["mathematical optimization", "gradient descent"]}
{"title": "variational auto-encoded deep gaussian processes", "abstract": "we develop a scalable deep non-parametric generative model by augmenting deep gaussian processes with a recognition model . inference is performed in a novel scalable variational framework where the variational posterior distributions are reparametrized through a multilayer perceptron . the key aspect of this reformulation is that it prevents the proliferation of variational parameters which otherwise grow linearly in proportion to the sample size . we derive a new formulation of the variational lower bound that allows us to distribute most of the computation in a way that enables to handle datasets of the size of mainstream deep learning tasks . we show the efficacy of the method on a variety of challenges including deep unsupervised learning and deep bayesian optimization .", "topics": ["generative model", "calculus of variations"]}
{"title": "evolving a-type artificial neural networks", "abstract": "we investigate turing 's notion of an a-type artificial neural network . we study a refinement of turing 's original idea , motivated by work of teuscher , bull , preen and copeland . our a-types can process binary data by accepting and outputting sequences of binary vectors ; hence we can associate a function to an a-type , and we say the a-type { \\em represents } the function . there are two modes of data processing : clamped and sequential . we describe an evolutionary algorithm , involving graph-theoretic manipulations of a-types , which searches for a-types representing a given function . the algorithm uses both mutation and crossover operators . we implemented the algorithm and applied it to three benchmark tasks . we found that the algorithm performed much better than a random search . for two out of the three tasks , the algorithm with crossover performed better than a mutation-only version .", "topics": ["neural networks"]}
{"title": "one-shot concept learning by simulating evolutionary instinct development", "abstract": "object recognition has become a crucial part of machine learning and computer vision recently . the current approach to object recognition involves deep learning and uses convolutional neural networks to learn the pixel patterns of the objects implicitly through backpropagation . however , cnns require thousands of examples in order to generalize successfully and often require heavy computing resources for training . this is considered rather sluggish when compared to the human ability to generalize and learn new categories given just a single example . additionally , cnns make it difficult to explicitly programmatically modify or intuitively interpret their learned representations . we propose a computational model that can successfully learn an object category from as few as one example and allows its learning style to be tailored explicitly to a scenario . our model decomposes each image into two attributes : shape and color distribution . we then use a bayesian criterion to probabilistically determine the likelihood of each category . the model takes each factor into account based on importance and calculates the conditional probability of the object belonging to each learned category . our model is not only applicable to visual scenarios , but can also be implemented in a broader and more practical scope of situations such as natural language processing as well as other places where it is possible to retrieve and construct individual attributes . because the only condition our model presents is the ability to retrieve and construct individual attributes such as shape and color , it can be applied to essentially any class of visual objects .", "topics": ["natural language processing", "computer vision"]}
{"title": "large margin object tracking with circulant feature maps", "abstract": "structured output support vector machine ( svm ) based tracking algorithms have shown favorable performance recently . nonetheless , the time-consuming candidate sampling and complex optimization limit their real-time applications . in this paper , we propose a novel large margin object tracking method which absorbs the strong discriminative ability from structured output svm and speeds up by the correlation filter algorithm significantly . secondly , a multimodal target detection technique is proposed to improve the target localization precision and prevent model drift introduced by similar objects or background noise . thirdly , we exploit the feedback from high-confidence tracking results to avoid the model corruption problem . we implement two versions of the proposed tracker with the representations from both conventional hand-crafted and deep convolution neural networks ( cnns ) based features to validate the strong compatibility of the algorithm . the experimental results demonstrate that the proposed tracker performs superiorly against several state-of-the-art algorithms on the challenging benchmark sequences while runs at speed in excess of 80 frames per second . the source code and experimental results will be made publicly available .", "topics": ["sampling ( signal processing )", "support vector machine"]}
{"title": "regret bounds for lifelong learning", "abstract": "we consider the problem of transfer learning in an online setting . different tasks are presented sequentially and processed by a within-task algorithm . we propose a lifelong learning strategy which refines the underlying data representation used by the within-task algorithm , thereby transferring information from one task to the next . we show that when the within-task algorithm comes with some regret bound , our strategy inherits this good property . our bounds are in expectation for a general loss function , and uniform for a convex loss . we discuss applications to dictionary learning and finite set of predictors . in the latter case , we improve previous $ o ( 1/\\sqrt { m } ) $ bounds to $ o ( 1/m ) $ where $ m $ is the per task sample size .", "topics": ["regret ( decision theory )", "loss function"]}
{"title": "encoding source language with convolutional neural network for machine translation", "abstract": "the recently proposed neural network joint model ( nnjm ) ( devlin et al . , 2014 ) augments the n-gram target language model with a heuristically chosen source context window , achieving state-of-the-art performance in smt . in this paper , we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information . with different guiding signals during decoding , our specifically designed convolution+gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word , and fuse them with the context of entire source sentence to form a unified representation . this representation , together with target language words , are fed to a deep neural network ( dnn ) to form a stronger nnjm . experiments on two nist chinese-english translation tasks show that the proposed model can achieve significant improvements over the previous nnjm by up to +1.08 bleu points on average", "topics": ["machine translation", "heuristic"]}
{"title": "meteorological time series forecasting based on mlp modelling using heterogeneous transfer functions", "abstract": "in this paper , we propose to study four meteorological and seasonal time series coupled with a multi-layer perceptron ( mlp ) modeling . we chose to combine two transfer functions for the nodes of the hidden layer , and to use a temporal indicator ( time index as input ) in order to take into account the seasonal aspect of the studied time series . the results of the prediction concern two years of measurements and the learning step , eight independent years . we show that this methodology can improve the accuracy of meteorological data estimation compared to a classical mlp modelling with a homogenous transfer function .", "topics": ["time series"]}
{"title": "sample complexity of stochastic variance-reduced cubic regularization for nonconvex optimization", "abstract": "the popular cubic regularization ( cr ) method converges with first- and second-order optimality guarantee for nonconvex optimization , but encounters a high sample complexity issue for solving large-scale problems . various sub-sampling variants of cr have been proposed to improve the sample complexity.in this paper , we propose a stochastic variance-reduced cubic-regularized ( svrc ) newton 's method under both sampling with and without replacement schemes . we characterize the per-iteration sample complexity bounds which guarantee the same rate of convergence as that of cr for nonconvex optimization . furthermore , our method achieves a total hessian sample complexity of $ \\mathcal { o } ( n^ { 8/11 } \\epsilon^ { -3/2 } ) $ and $ \\mathcal { o } ( n^ { 3/4 } \\epsilon^ { -3/2 } ) $ respectively under sampling without and with replacement , which improve that of cr as well as other sub-sampling variant methods via the variance reduction scheme . our result also suggests that sampling without replacement yields lower sample complexity than that of sampling with replacement . we further compare the practical performance of svrc with other cubic regularization methods via experiments .", "topics": ["sampling ( signal processing )", "matrix regularization"]}
{"title": "plan , attend , generate : planning for sequence-to-sequence models", "abstract": "we investigate the integration of a planning mechanism into sequence-to-sequence models using attention . we develop a model which can plan ahead in the future when it computes its alignments between input and output sequences , constructing a matrix of proposed future alignments and a commitment vector that governs whether to follow or recompute the plan . this mechanism is inspired by the recently proposed strategic attentive reader and writer ( straw ) model for reinforcement learning . our proposed model is end-to-end trainable using primarily differentiable operations . we show that it outperforms a strong baseline on character-level translation tasks from wmt'15 , the algorithmic task of finding eulerian circuits of graphs , and question generation from the text . our analysis demonstrates that the model computes qualitatively intuitive alignments , converges faster than the baselines , and achieves superior performance with fewer parameters .", "topics": ["baseline ( configuration management )", "reinforcement learning"]}
{"title": "photographic dataset : random peppercorns", "abstract": "this is a photographic dataset collected for testing image processing algorithms . the idea is to have sets of different but statistically similar images . in this work the images show randomly distributed peppercorns . the dataset is made available at www.fips.fi/photographic_dataset.php .", "topics": ["image processing"]}
{"title": "neural architecture search with bayesian optimisation and optimal transport", "abstract": "bayesian optimisation ( bo ) refers to a class of methods for global optimisation of a function $ f $ which is only accessible via point evaluations . it is typically used in settings where $ f $ is expensive to evaluate . a common use case for bo in machine learning is model selection , where it is not possible to analytically model the generalisation performance of a statistical model , and we resort to noisy and expensive training and validation procedures to choose the best model . conventional bo methods have focused on euclidean and categorical domains , which , in the context of model selection , only permits tuning scalar hyper-parameters of machine learning algorithms . however , with the surge of interest in deep learning , there is an increasing demand to tune neural network \\emph { architectures } . in this work , we develop nasbot , a gaussian process based bo framework for neural architecture search . to accomplish this , we develop a distance metric in the space of neural network architectures which can be computed efficiently via an optimal transport program . this distance might be of independent interest to the deep learning community as it may find applications outside of bo . we demonstrate that nasbot outperforms other alternatives for architecture search in several cross validation based model selection tasks on multi-layer perceptrons and convolutional neural networks .", "topics": ["mathematical optimization", "bayesian network"]}
{"title": "learning the structure of dynamic probabilistic networks", "abstract": "dynamic probabilistic networks are a compact representation of complex stochastic processes . in this paper we examine how to learn the structure of a dpn from data . we extend structure scoring rules for standard probabilistic networks to the dynamic case , and show how to search for structure when some of the variables are hidden . finally , we examine two applications where such a technology might be useful : predicting and classifying dynamic behaviors , and learning causal orderings in biological processes . we provide empirical results that demonstrate the applicability of our methods in both domains .", "topics": ["causality"]}
{"title": "deep learning with eigenvalue decay regularizer", "abstract": "this paper extends our previous work on regularization of neural networks using eigenvalue decay by employing a soft approximation of the dominant eigenvalue in order to enable the calculation of its derivatives in relation to the synaptic weights , and therefore the application of back-propagation , which is a primary demand for deep learning . moreover , we extend our previous theoretical analysis to deep neural networks and multiclass classification problems . our method is implemented as an additional regularizer in keras , a modular neural networks library written in python , and evaluated in the benchmark data sets reuters newswire topics classification , imdb database for binary sentiment classification , mnist database of handwritten digits and cifar-10 data set for image classification .", "topics": ["computer vision", "matrix regularization"]}
{"title": "faster graphical model identification of tandem mass spectra using peptide word lattices", "abstract": "liquid chromatography coupled with tandem mass spectrometry , also known as shotgun proteomics , is a widely-used high-throughput technology for identifying proteins in complex biological samples . analysis of the tens of thousands of fragmentation spectra produced by a typical shotgun proteomics experiment begins by assigning to each observed spectrum the peptide hypothesized to be responsible for generating the spectrum , typically done by searching each spectrum against a database of peptides . we have recently described a machine learning method -- -dynamic bayesian network for rapid identification of peptides ( drip ) -- -that not only achieves state-of-the-art spectrum identification performance on a variety of datasets but also provides a trainable model capable of returning valuable auxiliary information regarding specific peptide-spectrum matches . in this work , we present two significant improvements to drip . first , we describe how to use word lattices , which are widely used in natural language processing , to significantly speed up drip 's computations . to our knowledge , all existing shotgun proteomics search engines compute independent scores between a given observed spectrum and each possible candidate peptide from the database . the key idea of the word lattice is to represent the set of candidate peptides in a single data structure , thereby allowing sharing of redundant computations among the different candidates . we demonstrate that using lattices in conjunction with drip leads to speedups on the order of tens across yeast and worm data sets . second , we introduce a variant of drip that uses a discriminative training framework , performing maximum mutual entropy estimation rather than maximum likelihood estimation . this modification improves drip 's statistical power , enabling us to increase the number of identified spectrum at a 1 % false discovery rate on yeast and worm data sets .", "topics": ["graphical model", "natural language processing"]}
{"title": "computing with contextual numbers", "abstract": "self organizing map ( som ) has been applied into several classical modeling tasks including clustering , classification , function approximation and visualization of high dimensional spaces . the final products of a trained som are a set of ordered ( low dimensional ) indices and their associated high dimensional weight vectors . while in the above-mentioned applications , the final high dimensional weight vectors play the primary role in the computational steps , from a certain perspective , one can interpret som as a nonparametric encoder , in which the final low dimensional indices of the trained som are pointer to the high dimensional space . we showed how using a one-dimensional som , which is not common in usual applications of som , one can develop a nonparametric mapping from a high dimensional space to a continuous one-dimensional numerical field . these numerical values , called contextual numbers , are ordered in a way that in a given context , similar numbers refer to similar high dimensional states . further , as these numbers can be treated similarly to usual continuous numbers , they can be replaced with their corresponding high dimensional states within any data driven modeling problem . as a potential application , we showed how using contextual numbers could be used for the problem of high dimensional spatiotemporal dynamics .", "topics": ["cluster analysis", "statistical classification"]}
{"title": "incremental map generation by low cost robots based on possibility/necessity grids", "abstract": "in this paper we present some results obtained with a troupe of low-cost robots designed to cooperatively explore and adquire the map of unknown structured orthogonal environments . in order to improve the covering of the explored zone , the robots show different behaviours and cooperate by transferring each other the perceived environment when they meet . the returning robots deliver to a host computer their partial maps and the host incrementally generates the map of the environment by means of apossibility/ necessity grid .", "topics": ["map", "robot"]}
{"title": "beliefs in markov trees - from local computations to local valuation", "abstract": "this paper is devoted to expressiveness of hypergraphs for which uncertainty propagation by local computations via shenoy/shafer method applies . it is demonstrated that for this propagation method for a given joint belief distribution no valuation of hyperedges of a hypergraph may provide with simpler hypergraph structure than valuation of hyperedges by conditional distributions . this has vital implication that methods recovering belief networks from data have no better alternative for finding the simplest hypergraph structure for belief propagation . a method for recovery tree-structured belief networks has been developed and specialized for dempster-shafer belief functions", "topics": ["value ( ethics )", "bayesian network"]}
{"title": "learning probabilistic models of word sense disambiguation", "abstract": "this dissertation presents several new methods of supervised and unsupervised learning of word sense disambiguation models . the supervised methods focus on performing model searches through a space of probabilistic models , and the unsupervised methods rely on the use of gibbs sampling and the expectation maximization ( em ) algorithm . in both the supervised and unsupervised case , the naive bayesian model is found to perform well . an explanation for this success is presented in terms of learning rates and bias-variance decompositions .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "modeling variations of first-order horn abduction in answer set programming", "abstract": "we study abduction in first order horn logic theories where all atoms can be abduced and we are looking for preferred solutions with respect to three objective functions : cardinality minimality , coherence , and weighted abduction . we represent this reasoning problem in answer set programming ( asp ) , in order to obtain a flexible framework for experimenting with global constraints and objective functions , and to test the boundaries of what is possible with asp . realizing this problem in asp is challenging as it requires value invention and equivalence between certain constants , because the unique names assumption does not hold in general . to permit reasoning in cyclic theories , we formally describe fine-grained variations of limiting skolemization . we identify term equivalence as a main instantiation bottleneck , and improve the efficiency of our approach with on-demand constraints that were used to eliminate the same bottleneck in state-of-the-art solvers . we evaluate our approach experimentally on the accel benchmark for plan recognition in natural language understanding . our encodings are publicly available , modular , and our approach is more efficient than state-of-the-art solvers on the accel benchmark .", "topics": ["natural language"]}
{"title": "the case for temporal transparency : detecting policy change events in black-box decision making systems", "abstract": "bringing transparency to black-box decision making systems ( dms ) has been a topic of increasing research interest in recent years . traditional active and passive approaches to make these systems transparent are often limited by scalability and/or feasibility issues . in this paper , we propose a new notion of black-box dms transparency , named , temporal transparency , whose goal is to detect if/when the dms policy changes over time , and is mostly invariant to the drawbacks of traditional approaches . we map our notion of temporal transparency to time series changepoint detection methods , and develop a framework to detect policy changes in real-world dms 's . experiments on new york stop-question-and-frisk dataset reveal a number of publicly announced and unannounced policy changes , highlighting the utility of our framework .", "topics": ["time series", "scalability"]}
{"title": "asymmetric lsh ( alsh ) for sublinear time maximum inner product search ( mips )", "abstract": "we present the first provably sublinear time algorithm for approximate \\emph { maximum inner product search } ( mips ) . our proposal is also the first hashing algorithm for searching with ( un-normalized ) inner product as the underlying similarity measure . finding hashing schemes for mips was considered hard . we formally show that the existing locality sensitive hashing ( lsh ) framework is insufficient for solving mips , and then we extend the existing lsh framework to allow asymmetric hashing schemes . our proposal is based on an interesting mathematical phenomenon in which inner products , after independent asymmetric transformations , can be converted into the problem of approximate near neighbor search . this key observation makes efficient sublinear hashing scheme for mips possible . in the extended asymmetric lsh ( alsh ) framework , we provide an explicit construction of provably fast hashing scheme for mips . the proposed construction and the extended lsh framework could be of independent theoretical interest . our proposed algorithm is simple and easy to implement . we evaluate the method , for retrieving inner products , in the collaborative filtering task of item recommendations on netflix and movielens datasets .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "weakly supervised semantic segmentation based on web image co-segmentation", "abstract": "training a fully convolutional network ( fcn ) for semantic segmentation requires a large number of masks with pixel level labelling , which involves a large amount of human labour and time for annotation . in contrast , web images and their image-level labels are much easier and cheaper to obtain . in this work , we propose a novel method for weakly supervised semantic segmentation with only image-level labels . the method utilizes the internet to retrieve a large number of images and uses a large scale co-segmentation framework to generate masks for the retrieved images . we first retrieve images from search engines , e.g . flickr and google , using semantic class names as queries , e.g . class names in the dataset pascal voc 2012 . we then use high quality masks produced by co-segmentation on the retrieved images as well as the target dataset images with image level labels to train segmentation networks . we obtain an iou score of 56.9 on test set of pascal voc 2012 , which reaches the state-of-the-art performance .", "topics": ["test set", "pixel"]}
{"title": "the automatic creation of concept maps from documents written using morphologically rich languages", "abstract": "concept map is a graphical tool for representing knowledge . they have been used in many different areas , including education , knowledge management , business and intelligence . constructing of concept maps manually can be a complex task ; an unskilled person may encounter difficulties in determining and positioning concepts relevant to the problem area . an application that recommends concept candidates and their position in a concept map can significantly help the user in that situation . this paper gives an overview of different approaches to automatic and semi-automatic creation of concept maps from textual and non-textual sources . the concept map mining process is defined , and one method suitable for the creation of concept maps from unstructured textual sources in highly inflected languages such as the croatian language is described in detail . proposed method uses statistical and data mining techniques enriched with linguistic tools . with minor adjustments , that method can also be used for concept map mining from textual sources in other morphologically rich languages .", "topics": ["data mining", "map"]}
{"title": "domain separation networks", "abstract": "the cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive . one approach circumventing this cost is training models on synthetic data where annotations are provided automatically . despite their appeal , such models often fail to generalize from synthetic to real images , necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied . existing approaches focus either on mapping representations from one domain to the other , or on learning to extract features that are invariant to the domain from which they were extracted . however , by focusing only on creating a mapping or shared representation between the two domains , they ignore the individual characteristics of each domain . we suggest that explicitly modeling what is unique to each domain can improve a model 's ability to extract domain-invariant features . inspired by work on private-shared component analysis , we explicitly learn to extract image representations that are partitioned into two subspaces : one component which is private to each domain and one which is shared across domains . our model is trained not only to perform the task we care about in the source domain , but also to use the partitioned representation to reconstruct the images from both domains . our novel architecture results in a model that outperforms the state-of-the-art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process .", "topics": ["unsupervised learning", "synthetic data"]}
{"title": "big batch sgd : automated inference using adaptive batch sizes", "abstract": "classical stochastic gradient methods for optimization rely on noisy gradient approximations that become progressively less accurate as iterates approach a solution . the large noise and small signal in the resulting gradients makes it difficult to use them for adaptive stepsize selection and automatic stopping . we propose alternative `` big batch '' sgd schemes that adaptively grow the batch size over time to maintain a nearly constant signal-to-noise ratio in the gradient approximation . the resulting methods have similar convergence rates to classical sgd , and do not require convexity of the objective . the high fidelity gradients enable automated learning rate selection and do not require stepsize decay . big batch methods are thus easily automated and can run with little or no oversight .", "topics": ["loss function", "gradient"]}
{"title": "search personalization with embeddings", "abstract": "recent research has shown that the performance of search personalization depends on the richness of user profiles which normally represent the user 's topical interests . in this paper , we propose a new embedding approach to learning user profiles , where users are embedded on a topical interest space . we then directly utilize the user profiles for search personalization . experiments on query logs from a major commercial web search engine demonstrate that our embedding approach improves the performance of the search engine and also achieves better search performance than other strong baselines .", "topics": ["baseline ( configuration management )"]}
{"title": "cooperative hierarchical dirichlet processes : superposition vs. maximization", "abstract": "the cooperative hierarchical structure is a common and significant data structure observed in , or adopted by , many research areas , such as : text mining ( author-paper-word ) and multi-label classification ( label-instance-feature ) . renowned bayesian approaches for cooperative hierarchical structure modeling are mostly based on topic models . however , these approaches suffer from a serious issue in that the number of hidden topics/factors needs to be fixed in advance and an inappropriate number may lead to overfitting or underfitting . one elegant way to resolve this issue is bayesian nonparametric learning , but existing work in this area still can not be applied to cooperative hierarchical structure modeling . in this paper , we propose a cooperative hierarchical dirichlet process ( chdp ) to fill this gap . each node in a cooperative hierarchical structure is assigned a dirichlet process to model its weights on the infinite hidden factors/topics . together with measure inheritance from hierarchical dirichlet process , two kinds of measure cooperation , i.e . , superposition and maximization , are defined to capture the many-to-many relationships in the cooperative hierarchical structure . furthermore , two constructive representations for chdp , i.e . , stick-breaking and international restaurant process , are designed to facilitate the model inference . experiments on synthetic and real-world data with cooperative hierarchical structures demonstrate the properties and the ability of chdp for cooperative hierarchical structure modeling and its potential for practical application scenarios .", "topics": ["synthetic data"]}
{"title": "tagging and morphological disambiguation of turkish text", "abstract": "automatic text tagging is an important component in higher level analysis of text corpora , and its output can be used in many natural language processing applications . in languages like turkish or finnish , with agglutinative morphology , morphological disambiguation is a very crucial process in tagging , as the structures of many lexical forms are morphologically ambiguous . this paper describes a pos tagger for turkish text based on a full-scale two-level specification of turkish morphology that is based on a lexicon of about 24,000 root words . this is augmented with a multi-word and idiomatic construct recognizer , and most importantly morphological disambiguator based on local neighborhood constraints , heuristics and limited amount of statistical information . the tagger also has functionality for statistics compilation and fine tuning of the morphological analyzer , such as logging erroneous morphological parses , commonly used roots , etc . preliminary results indicate that the tagger can tag about 98-99\\ % of the texts accurately with very minimal user intervention . furthermore for sentences morphologically disambiguated with the tagger , an lfg parser developed for turkish , generates , on the average , 50\\ % less ambiguous parses and parses almost 2.5 times faster . the tagging functionality is not specific to turkish , and can be applied to any language with a proper morphological analysis interface .", "topics": ["natural language processing", "text corpus"]}
{"title": "towards biologically plausible deep learning", "abstract": "neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology . we explore more biologically plausible versions of deep representation learning , focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised , unsupervised and reinforcement learning . the starting point is that the basic learning rule believed to govern synaptic weight updates ( spike-timing-dependent plasticity ) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function ( be it supervised , unsupervised , or reward-driven ) . the second main idea is that this corresponds to a form of the variational em algorithm , i.e . , with approximate rather than exact posteriors , implemented by neural dynamics . another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward , with pairs of layers learning to form a denoising auto-encoder . finally , we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders , and we validate all these ideas on generative learning tasks .", "topics": ["sampling ( signal processing )", "feature learning"]}
{"title": "a new data representation based on training data characteristics to extract drug named-entity in medical text", "abstract": "one essential task in information extraction from the medical corpus is drug name recognition . compared with text sources come from other domains , the medical text is special and has unique characteristics . in addition , the medical text mining poses more challenges , e.g . , more unstructured text , the fast growing of new terms addition , a wide range of name variation for the same drug . the mining is even more challenging due to the lack of labeled dataset sources and external knowledge , as well as multiple token representations for a single drug name that is more common in the real application setting . although many approaches have been proposed to overwhelm the task , some problems remained with poor f-score performance ( less than 0.75 ) . this paper presents a new treatment in data representation techniques to overcome some of those challenges . we propose three data representation techniques based on the characteristics of word distribution and word similarities as a result of word embedding training . the first technique is evaluated with the standard nn model , i.e . , mlp ( multi-layer perceptrons ) . the second technique involves two deep network classifiers , i.e . , dbn ( deep belief networks ) , and sae ( stacked denoising encoders ) . the third technique represents the sentence as a sequence that is evaluated with a recurrent nn model , i.e . , lstm ( long short term memory ) . in extracting the drug name entities , the third technique gives the best f-score performance compared to the state of the art , with its average f-score being 0.8645 .", "topics": ["noise reduction", "bayesian network"]}
{"title": "learning 3d object categories by looking around them", "abstract": "traditional approaches for learning 3d object categories use either synthetic data or manual supervision . in this paper , we propose a method which does not require manual annotations and is instead cued by observing objects from a moving vantage point . our system builds on two innovations : a siamese viewpoint factorization network that robustly aligns different videos together without explicitly comparing 3d shapes ; and a 3d shape completion network that can extract the full shape of an object from partial observations . we also demonstrate the benefits of configuring networks to perform probabilistic predictions as well as of geometry-aware data augmentation schemes . we obtain state-of-the-art results on publicly-available benchmarks .", "topics": ["synthetic data"]}
{"title": "dpc-net : deep pose correction for visual localization", "abstract": "we present a novel method to fuse the power of deep networks with the computational efficiency of geometric and probabilistic localization algorithms . in contrast to other methods that completely replace a classical visual estimator with a deep network , we propose an approach that uses a convolutional neural network to learn difficult-to-model corrections to the estimator from ground-truth training data . to this end , we derive a novel loss function for learning se ( 3 ) corrections based on a matrix lie groups approach , with a natural formulation for balancing translation and rotation errors . we use this loss to train a deep pose correction network ( dpc-net ) that predicts corrections for a particular estimator , sensor and environment . using the kitti odometry dataset , we demonstrate significant improvements to the accuracy of a computationally-efficient sparse stereo visual odometry pipeline , that render it as accurate as a modern computationally-intensive dense estimator . further , we show how dpc-net can be used to mitigate the effect of poorly calibrated lens distortion parameters .", "topics": ["test set", "loss function"]}
{"title": "retrofitting distributional embeddings to knowledge graphs with functional relations", "abstract": "knowledge graphs are a versatile framework to encode richly structured data relationships , but it not always apparent how to combine these with existing entity representations . methods for retrofitting pre-trained entity representations to the structure of a knowledge graph typically assume that entities are embedded in a connected space and that relations imply similarity . however , useful knowledge graphs often contain diverse entities and relations ( with potentially disjoint underlying corpora ) which do not accord with these assumptions . to overcome these limitations , we present functional retrofitting , a framework that generalizes current retrofitting methods by explicitly modeling pairwise relations . our framework can directly incorporate a variety of pairwise penalty functions previously developed for knowledge graph completion . we present both linear and neural instantiations of the framework . functional retrofitting significantly outperforms existing retrofitting methods on complex knowledge graphs and loses no accuracy on simpler graphs ( in which relations do imply similarity ) . finally , we demonstrate the utility of the framework by predicting new drug -- disease treatment pairs in a large , complex health knowledge graph .", "topics": ["feature learning", "data mining"]}
{"title": "one big net for everything", "abstract": "i apply recent work on `` learning to think '' ( 2015 ) and on powerplay ( 2011 ) to the incremental training of an increasingly general problem solver , continually learning to solve new tasks without forgetting previous skills . the problem solver is a single recurrent neural network ( or similar general purpose computer ) called one . one is unusual in the sense that it is trained in various ways , e.g . , by black box optimization / reinforcement learning / artificial evolution as well as supervised / unsupervised learning . for example , one may learn through neuroevolution to control a robot through environment-changing actions , and learn through unsupervised gradient descent to predict future inputs and vector-valued reward signals as suggested in 1990 . user-given tasks can be defined through extra goal-defining input patterns , also proposed in 1990 . suppose one has already learned many skills . now a copy of one can be re-trained to learn a new skill , e.g . , through neuroevolution without a teacher . here it may profit from re-using previously learned subroutines , but it may also forget previous skills . then one is retrained in powerplay style ( 2011 ) on stored input/output traces of ( a ) one 's copy executing the new skill and ( b ) previous instances of one whose skills are still considered worth memorizing . simultaneously , one is retrained on old traces ( even those of unsuccessful trials ) to become a better predictor , without additional expensive interaction with the enviroment . more and more control and prediction skills are thus collapsed into one , like in the chunker-automatizer system of the neural history compressor ( 1991 ) . this forces one to relate partially analogous skills ( with shared algorithmic information ) to each other , creating common subroutines in form of shared subnetworks of one , to greatly speed up subsequent learning of additional , novel but algorithmically related skills .", "topics": ["unsupervised learning", "recurrent neural network"]}
{"title": "testing for differences in gaussian graphical models : applications to brain connectivity", "abstract": "functional brain networks are well described and estimated from data with gaussian graphical models ( ggms ) , e.g . using sparse inverse covariance estimators . comparing functional connectivity of subjects in two populations calls for comparing these estimated ggms . our goal is to identify differences in ggms known to have similar structure . we characterize the uncertainty of differences with confidence intervals obtained using a parametric distribution on parameters of a sparse estimator . sparse penalties enable statistical guarantees and interpretable models even in high-dimensional and low-sample settings . characterizing the distributions of sparse models is inherently challenging as the penalties produce a biased estimator . recent work invokes the sparsity assumptions to effectively remove the bias from a sparse estimator such as the lasso . these distributions can be used to give confidence intervals on edges in ggms , and by extension their differences . however , in the case of comparing ggms , these estimators do not make use of any assumed joint structure among the ggms . inspired by priors from brain functional connectivity we derive the distribution of parameter differences under a joint penalty when parameters are known to be sparse in the difference . this leads us to introduce the debiased multi-task fused lasso , whose distribution can be characterized in an efficient manner . we then show how the debiased lasso and multi-task fused lasso can be used to obtain confidence intervals on edge differences in ggms . we validate the techniques proposed on a set of synthetic examples as well as neuro-imaging dataset created for the study of autism .", "topics": ["graphical model", "synthetic data"]}
{"title": "covariance matrices for mean field variational bayes", "abstract": "mean field variational bayes ( mfvb ) is a popular posterior approximation method due to its fast runtime on large-scale data sets . however , it is well known that a major failing of mfvb is its ( sometimes severe ) underestimates of the uncertainty of model variables and lack of information about model variable covariance . we develop a fast , general methodology for exponential families that augments mfvb to deliver accurate uncertainty estimates for model variables -- both for individual variables and coherently across variables . mfvb for exponential families defines a fixed-point equation in the means of the approximating posterior , and our approach yields a covariance estimate by perturbing this fixed point . inspired by linear response theory , we call our method linear response variational bayes ( lrvb ) . we demonstrate the accuracy of our method on simulated data sets .", "topics": ["bayesian network"]}
{"title": "predicting individual physiologically acceptable states for discharge from a pediatric intensive care unit", "abstract": "objective : predict patient-specific vitals deemed medically acceptable for discharge from a pediatric intensive care unit ( icu ) . design : the means of each patient 's hr , sbp and dbp measurements between their medical and physical discharge from the icu were computed as a proxy for their physiologically acceptable state space ( pass ) for successful icu discharge . these individual pass values were compared via root mean squared error ( rmse ) to population age-normal vitals , a polynomial regression through the pass values of a pediatric icu ( picu ) population and predictions from two recurrent neural network models designed to predict personalized pass within the first twelve hours following icu admission . setting : picu at children 's hospital los angeles ( chla ) . patients : 6,899 picu episodes ( 5,464 patients ) collected between 2009 and 2016 . interventions : none . measurements : each episode data contained 375 variables representing vitals , labs , interventions , and drugs . they also included a time indicator for picu medical discharge and physical discharge . main results : the rmses between individual pass values and population age-normals ( hr : 25.9 bpm , sbp : 13.4 mmhg , dbp : 13.0 mmhg ) were larger than the rmses corresponding to the polynomial regression ( hr : 19.1 bpm , sbp : 12.3 mmhg , dbp : 10.8 mmhg ) . the rmses from the best performing rnn model were the lowest ( hr : 16.4 bpm ; sbp : 9.9 mmhg , dbp : 9.0 mmhg ) . conclusion : picu patients are a unique subset of the general population , and general age-normal vitals may not be suitable as target values indicating physiologic stability at discharge . age-normal vitals that were specifically derived from the medical-to-physical discharge window of icu patients may be more appropriate targets for 'acceptable ' physiologic state for critical care patients . going beyond simple age bins , an rnn model can provide more personalized target values .", "topics": ["value ( ethics )", "recurrent neural network"]}
{"title": "definition and properties to assess multi-agent environments as social intelligence tests", "abstract": "social intelligence in natural and artificial systems is usually measured by the evaluation of associated traits or tasks that are deemed to represent some facets of social behaviour . the amalgamation of these traits is then used to configure the intuitive notion of social intelligence . instead , in this paper we start from a parametrised definition of social intelligence as the expected performance in a set of environments with several agents , and we assess and derive tests from it . this definition makes several dependencies explicit : ( 1 ) the definition depends on the choice ( and weight ) of environments and agents , ( 2 ) the definition may include both competitive and cooperative behaviours depending on how agents and rewards are arranged into teams , ( 3 ) the definition mostly depends on the abilities of other agents , and ( 4 ) the actual difference between social intelligence and general intelligence ( or other abilities ) depends on these choices . as a result , we address the problem of converting this definition into a more precise one where some fundamental properties ensuring social behaviour ( such as action and reward dependency and anticipation on competitive/cooperative behaviours ) are met as well as some other more instrumental properties ( such as secernment , boundedness , symmetry , validity , reliability , efficiency ) , which are convenient to convert the definition into a practical test . from the definition and the formalised properties , we take a look at several representative multi-agent environments , tests and games to see whether they meet these properties .", "topics": ["artificial intelligence"]}
{"title": "hierarchical label inference for video classification", "abstract": "videos are a rich source of high-dimensional structured data , with a wide range of interacting components at varying levels of granularity . in order to improve understanding of unconstrained internet videos , it is important to consider the role of labels at separate levels of abstraction . in this paper , we consider the use of the bidirectional inference neural network ( binn ) for performing graph-based inference in label space for the task of video classification . we take advantage of the inherent hierarchy between labels at increasing granularity . the binn is evaluated on the first and second release of the youtube-8m large scale multilabel video dataset . our results demonstrate the effectiveness of binn , achieving significant improvements against baseline models .", "topics": ["baseline ( configuration management )"]}
{"title": "the online coupon-collector problem and its application to lifelong reinforcement learning", "abstract": "transferring knowledge across a sequence of related tasks is an important challenge in reinforcement learning ( rl ) . despite much encouraging empirical evidence , there has been little theoretical analysis . in this paper , we study a class of lifelong rl problems : the agent solves a sequence of tasks modeled as finite markov decision processes ( mdps ) , each of which is from a finite set of mdps with the same state/action sets and different transition/reward functions . motivated by the need for cross-task exploration in lifelong learning , we formulate a novel online coupon-collector problem and give an optimal algorithm . this allows us to develop a new lifelong rl algorithm , whose overall sample complexity in a sequence of tasks is much smaller than single-task learning , even if the sequence of tasks is generated by an adversary . benefits of the algorithm are demonstrated in simulated problems , including a recently introduced human-robot interaction problem .", "topics": ["reinforcement learning", "simulation"]}
{"title": "on the linearity of semantic change : investigating meaning variation via dynamic graph models", "abstract": "we consider two graph models of semantic change . the first is a time-series model that relates embedding vectors from one time period to embedding vectors of previous time periods . in the second , we construct one graph for each word : nodes in this graph correspond to time points and edge weights to the similarity of the word 's meaning across two time points . we apply our two models to corpora across three different languages . we find that semantic change is linear in two senses . firstly , today 's embedding vectors ( = meaning ) of words can be derived as linear combinations of embedding vectors of their neighbors in previous time periods . secondly , self-similarity of words decays linearly in time . we consider both findings as new laws/hypotheses of semantic change .", "topics": ["time series", "text corpus"]}
{"title": "structured learning of metric ensembles with application to person re-identification", "abstract": "matching individuals across non-overlapping camera networks , known as person re-identification , is a fundamentally challenging problem due to the large visual appearance changes caused by variations of viewpoints , lighting , and occlusion . approaches in literature can be categoried into two streams : the first stream is to develop reliable features against realistic conditions by combining several visual features in a pre-defined way ; the second stream is to learn a metric from training data to ensure strong inter-class differences and intra-class similarities . however , seeking an optimal combination of visual features which is generic yet adaptive to different benchmarks is a unsoved problem , and metric learning models easily get over-fitted due to the scarcity of training data in person re-identification . in this paper , we propose two effective structured learning based approaches which explore the adaptive effects of visual features in recognizing persons in different benchmark data sets . our framework is built on the basis of multiple low-level visual features with an optimal ensemble of their metrics . we formulate two optimization algorithms , cmctriplet and cmcstruct , which directly optimize evaluation measures commonly used in person re-identification , also known as the cumulative matching characteristic ( cmc ) curve .", "topics": ["test set", "high- and low-level"]}
{"title": "adapting resilient propagation for deep learning", "abstract": "the resilient propagation ( rprop ) algorithm has been very popular for backpropagation training of multilayer feed-forward neural networks in various applications . the standard rprop however encounters difficulties in the context of deep neural networks as typically happens with gradient-based learning algorithms . in this paper , we propose a modification of the rprop that combines standard rprop steps with a special drop out technique . we apply the method for training deep neural networks as standalone components and in ensemble formulations . results on the mnist dataset show that the proposed modification alleviates standard rprop 's problems demonstrating improved learning speed and accuracy .", "topics": ["gradient", "mnist database"]}
{"title": "learning model-based sparsity via projected gradient descent", "abstract": "several convex formulation methods have been proposed previously for statistical estimation with structured sparsity as the prior . these methods often require a carefully tuned regularization parameter , often a cumbersome or heuristic exercise . furthermore , the estimate that these methods produce might not belong to the desired sparsity model , albeit accurately approximating the true parameter . therefore , greedy-type algorithms could often be more desirable in estimating structured-sparse parameters . so far , these greedy methods have mostly focused on linear statistical models . in this paper we study the projected gradient descent with non-convex structured-sparse parameter model as the constraint set . should the cost function have a stable model-restricted hessian the algorithm produces an approximation for the desired minimizer . as an example we elaborate on application of the main results to estimation in generalized linear model .", "topics": ["approximation algorithm", "matrix regularization"]}
{"title": "coverage embedding models for neural machine translation", "abstract": "in this paper , we enhance the attention-based neural machine translation ( nmt ) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in nmt . for each source word , our model starts with a full coverage embedding vector to track the coverage status , and then keeps updating it with neural networks as the translation goes . experiments on the large-scale chinese-to-english task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary nmt system .", "topics": ["machine translation"]}
{"title": "a novel experimental platform for in-vessel multi-chemical molecular communications", "abstract": "this work presents a new multi-chemical experimental platform for molecular communication where the transmitter can release different chemicals . this platform is designed to be inexpensive and accessible , and it can be expanded to simulate different environments including the cardiovascular system and complex network of pipes in industrial complexes and city infrastructures . to demonstrate the capabilities of the platform , we implement a time-slotted binary communication system where a bit-0 is represented by an acid pulse , a bit-1 by a base pulse , and information is carried via ph signals . the channel model for this system , which is nonlinear and has long memories , is unknown . therefore , we devise novel detection algorithms that use techniques from machine learning and deep learning to train a maximum-likelihood detector . using these algorithms the bit error rate improves by an order of magnitude relative to the approach used in previous works . moreover , our system achieves a data rate that is an order of magnitude higher than any of the previous molecular communication platforms .", "topics": ["nonlinear system", "simulation"]}
{"title": "an approach to speed-up the word sense disambiguation procedure through sense filtering", "abstract": "in this paper , we are going to focus on speed up of the word sense disambiguation procedure by filtering the relevant senses of an ambiguous word through part-of-speech tagging . first , this proposed approach performs the part-of-speech tagging operation before the disambiguation procedure using bigram approximation . as a result , the exact part-of-speech of the ambiguous word at a particular text instance is derived . in the next stage , only those dictionary definitions ( glosses ) are retrieved from an online dictionary , which are associated with that particular part-of-speech to disambiguate the exact sense of the ambiguous word . in the training phase , we have used brown corpus for part-of-speech tagging and wordnet as an online dictionary . the proposed approach reduces the execution time upto half ( approximately ) of the normal execution time for a text , containing around 200 sentences . not only that , we have found several instances , where the correct sense of an ambiguous word is found for using the part-of-speech tagging before the disambiguation procedure .", "topics": ["dictionary"]}
{"title": "learning causal bayes networks using interventional path queries in polynomial time and sample complexity", "abstract": "causal discovery from empirical data is a fundamental problem in many scientific domains . observational data allows for identifiability only up to markov equivalence class . in this paper we first propose a polynomial time algorithm for learning the exact correctly-oriented structure of the transitive reduction of any causal bayesian networks with high probability , by using interventional path queries . each path query takes as input an origin node and a target node , and answers whether there is a directed path from the origin to the target . this is done by intervening the origin node and observing samples from the target node . we theoretically show the logarithmic sample complexity for the size of interventional data per path query , for continuous and discrete networks . we further extend our work to learn the transitive edges using logarithmic sample complexity ( albeit in time exponential in the maximum number of parents for discrete networks ) . this allows us to learn the full network . we also provide an analysis of imperfect interventions .", "topics": ["time complexity", "synthetic data"]}
{"title": "automation of pedestrian tracking in a crowded situation", "abstract": "studies on microscopic pedestrian requires large amounts of trajectory data from real-world pedestrian crowds . such data collection , if done manually , needs tremendous effort and is very time consuming . though many studies have asserted the possibility of automating this task using video cameras , we found that only a few have demonstrated good performance in very crowded situations or from a top-angled view scene . this paper deals with tracking pedestrian crowd under heavy occlusions from an angular scene . our automated tracking system consists of two modules that perform sequentially . the first module detects moving objects as blobs . the second module is a tracking system . we employ probability distribution from the detection of each pedestrian and use bayesian update to track the next position . the result of such tracking is a database of pedestrian trajectories over time and space . with certain prior information , we showed that the system can track a large number of people under occlusion and clutter scene .", "topics": ["database"]}
{"title": "a pac-bayesian analysis of randomized learning with application to stochastic gradient descent", "abstract": "we study the generalization error of randomized learning algorithms -- focusing on stochastic gradient descent ( sgd ) -- using a novel combination of pac-bayes and algorithmic stability . importantly , our generalization bounds hold for all posterior distributions on an algorithm 's random hyperparameters , including distributions that depend on the training data . this inspires an adaptive sampling algorithm for sgd that optimizes the posterior at runtime . we analyze this algorithm in the context of our generalization bounds and evaluate it on a benchmark dataset . our experiments demonstrate that adaptive sampling can reduce empirical risk faster than uniform sampling while also improving out-of-sample accuracy .", "topics": ["test set", "gradient descent"]}
{"title": "conjunctive query answering for the description logic shiq", "abstract": "conjunctive queries play an important role as an expressive query language for description logics ( dls ) . although modern dls usually provide for transitive roles , conjunctive query answering over dl knowledge bases is only poorly understood if transitive roles are admitted in the query . in this paper , we consider unions of conjunctive queries over knowledge bases formulated in the prominent dl shiq and allow transitive roles in both the query and the knowledge base . we show decidability of query answering in this setting and establish two tight complexity bounds : regarding combined complexity , we prove that there is a deterministic algorithm for query answering that needs time single exponential in the size of the kb and double exponential in the size of the query , which is optimal . regarding data complexity , we prove containment in co-np .", "topics": ["time complexity"]}
{"title": "column generation for interaction coverage in combinatorial software testing", "abstract": "this paper proposes a novel column generation framework for combinatorial software testing . in particular , it combines mathematical programming and constraint programming in a hybrid decomposition to generate covering arrays . the approach allows generating parameterized test cases with coverage guarantees between parameter interactions of a given application . compared to exhaustive testing , combinatorial test case generation reduces the number of tests to run significantly . our column generation algorithm is generic and can accommodate mixed coverage arrays over heterogeneous alphabets . the algorithm is realized in practice as a cloud service and recognized as one of the five winners of the company-wide cloud application challenge at oracle . the service is currently helping software developers from a range of different product teams in their testing efforts while exposing declarative constraint models and hybrid optimization techniques to a broader audience .", "topics": ["mathematical optimization", "interaction"]}
{"title": "graph learning under sparsity priors", "abstract": "graph signals offer a very generic and natural representation for data that lives on networks or irregular structures . the actual data structure is however often unknown a priori but can sometimes be estimated from the knowledge of the application domain . if this is not possible , the data structure has to be inferred from the mere signal observations . this is exactly the problem that we address in this paper , under the assumption that the graph signals can be represented as a sparse linear combination of a few atoms of a structured graph dictionary . the dictionary is constructed on polynomials of the graph laplacian , which can sparsely represent a general class of graph signals composed of localized patterns on the graph . we formulate a graph learning problem , whose solution provides an ideal fit between the signal observations and the sparse graph signal model . as the problem is non-convex , we propose to solve it by alternating between a signal sparse coding and a graph update step . we provide experimental results that outline the good graph recovery performance of our method , which generally compares favourably to other recent network inference algorithms .", "topics": ["sparse matrix", "dictionary"]}
{"title": "change detection under global viewpoint uncertainty", "abstract": "this paper addresses the problem of change detection from a novel perspective of long-term map learning . we are particularly interested in designing an approach that can scale to large maps and that can function under global uncertainty in the viewpoint ( i.e . , gps-denied situations ) . our approach , which utilizes a compact bag-of-words ( bow ) scene model , makes several contributions to the problem : 1 ) two kinds of prior information are extracted from the view sequence map and used for change detection . further , we propose a novel type of prior , called motion prior , to predict the relative motions of stationary objects and anomaly ego-motion detection . the proposed prior is also useful for distinguishing stationary from non-stationary objects . 2 ) a small set of good reference images ( e.g . , 10 ) are efficiently retrieved from the view sequence map by employing the recently developed bag-of-local-convolutional-features ( bolcf ) scene model . 3 ) change detection is reformulated as a scene retrieval over these reference images to find changed objects using a novel spatial bag-of-words ( sbow ) scene model . evaluations conducted of individual techniques and also their combinations on a challenging dataset of highly dynamic scenes in the publicly available malaga dataset verify their efficacy .", "topics": ["map"]}
{"title": "neural networks for text correction and completion in keyboard decoding", "abstract": "despite the ubiquity of mobile and wearable text messaging applications , the problem of keyboard text decoding is not tackled sufficiently in the light of the enormous success of the deep learning recurrent neural network ( rnn ) and convolutional neural networks ( cnn ) for natural language understanding . in particular , considering that the keyboard decoders should operate on devices with memory and processor resource constraints , makes it challenging to deploy industrial scale deep neural network ( dnn ) models . this paper proposes a sequence-to-sequence neural attention network system for automatic text correction and completion . given an erroneous sequence , our model encodes character level hidden representations and then decodes the revised sequence thus enabling auto-correction and completion . we achieve this by a combination of character level cnn and gated recurrent unit ( gru ) encoder along with and a word level gated recurrent unit ( gru ) attention decoder . unlike traditional language models that learn from billions of words , our corpus size is only 12 million words ; an order of magnitude smaller . the memory footprint of our learnt model for inference and prediction is also an order of magnitude smaller than the conventional language model based text decoders . we report baseline performance for neural keyboard decoders in such limited domain . our models achieve a word level accuracy of $ 90\\ % $ and a character error rate cer of $ 2.4\\ % $ over the twitter typo dataset . we present a novel dataset of noisy to corrected mappings by inducing the noise distribution from the twitter data over the opensubtitles 2009 dataset ; on which our model predicts with a word level accuracy of $ 98\\ % $ and sequence accuracy of $ 68.9\\ % $ . in our user study , our model achieved an average cer of $ 2.6\\ % $ with the state-of-the-art non-neural touch-screen keyboard decoder at cer of $ 1.6\\ % $ .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "online adaptive machine learning based algorithm for implied volatility surface modeling", "abstract": "in this work , we design a machine learning based method , online adaptive primal support vector regression ( svr ) , to model the implied volatility surface . the algorithm proposed is the first derivation and implementation of an online primal kernel svr . it features enhancements that allow online adaptive learning by embedding the idea of local fitness and budget maintenance . to accelerate our algorithm , we implement its most computationally intensive parts in a field programmable gate arrays hardware . using intraday tick data from the e-mini s & p 500 options market , we show that our algorithm outperforms two competing methods and the gaussian kernel is a better choice than the linear kernel . sensitivity analysis is also presented to demonstrate how hyper parameters affect the error rates and the number of support vectors in our models .", "topics": ["support vector machine"]}
{"title": "learning from lions : inferring the utility of agents from their trajectories", "abstract": "we build a model using gaussian processes to infer a spatio-temporal vector field from observed agent trajectories . significant landmarks or influence points in agent surroundings are jointly derived through vector calculus operations that indicate presence of sources and sinks . we evaluate these influence points by using the kullback-leibler divergence between the posterior and prior laplacian of the inferred spatio-temporal vector field . through locating significant features that influence trajectories , our model aims to give greater insight into underlying causal utility functions that determine agent decision-making . a key feature of our model is that it infers a joint gaussian process over the observed trajectories , the time-varying vector field of utility and canonical vector calculus operators . we apply our model to both synthetic data and lion gps data collected at the bubye valley conservancy in southern zimbabwe .", "topics": ["synthetic data", "causality"]}
{"title": "a light touch for heavily constrained sgd", "abstract": "minimizing empirical risk subject to a set of constraints can be a useful strategy for learning restricted classes of functions , such as monotonic functions , submodular functions , classifiers that guarantee a certain class label for some subset of examples , etc . however , these restrictions may result in a very large number of constraints . projected stochastic gradient descent ( sgd ) is often the default choice for large-scale optimization in machine learning , but requires a projection after each update . for heavily-constrained objectives , we propose an efficient extension of sgd that stays close to the feasible region while only applying constraints probabilistically at each iteration . theoretical analysis shows a compelling trade-off between per-iteration work and the number of iterations needed on problems with a large number of constraints .", "topics": ["gradient descent", "iteration"]}
{"title": "semi-supervised instance population of an ontology using word vector embeddings", "abstract": "in many modern day systems such as information extraction and knowledge management agents , ontologies play a vital role in maintaining the concept hierarchies of the selected domain . however , ontology population has become a problematic process due to its nature of heavy coupling with manual human intervention . with the use of word embeddings in the field of natural language processing , it became a popular topic due to its ability to cope up with semantic sensitivity . hence , in this study , we propose a novel way of semi-supervised ontology population through word embeddings as the basis . we built several models including traditional benchmark models and new types of models which are based on word embeddings . finally , we ensemble them together to come up with a synergistic model with better accuracy . we demonstrate that our ensemble model can outperform the individual models .", "topics": ["natural language processing"]}
{"title": "sketching for kronecker product regression and p-splines", "abstract": "tensorsketch is an oblivious linear sketch introduced in pagh'13 and later used in pham , pagh'13 in the context of svms for polynomial kernels . it was shown in avron , nguyen , woodruff'14 that tensorsketch provides a subspace embedding , and therefore can be used for canonical correlation analysis , low rank approximation , and principal component regression for the polynomial kernel . we take tensorsketch outside of the context of polynomials kernels , and show its utility in applications in which the underlying design matrix is a kronecker product of smaller matrices . this allows us to solve kronecker product regression and non-negative kronecker product regression , as well as regularized spline regression . our main technical result is then in extending tensorsketch to other norms . that is , tensorsketch only provides input sparsity time for kronecker product regression with respect to the $ 2 $ -norm . we show how to solve kronecker product regression with respect to the $ 1 $ -norm in time sublinear in the time required for computing the kronecker product , as well as for more general $ p $ -norms .", "topics": ["sparse matrix", "polynomial"]}
{"title": "lose the views : limited angle ct reconstruction via implicit sinogram completion", "abstract": "computed tomography ( ct ) reconstruction is a fundamental component to a wide variety of applications ranging from security , to healthcare . the classical techniques require measuring projections , called sinograms , from a full 180 $ ^\\circ $ view of the object . this is impractical in a limited angle scenario , when the viewing angle is less than 180 $ ^\\circ $ , which can occur due to different factors including restrictions on scanning time , limited flexibility of scanner rotation , etc . the sinograms obtained as a result , cause existing techniques to produce highly artifact-laden reconstructions . in this paper , we propose to address this problem through implicit sinogram completion , on a challenging real world dataset containing scans of common checked-in luggage . we propose a system , consisting of 1d and 2d convolutional neural networks , that operates on a limited angle sinogram to directly produce the best estimate of a reconstruction . next , we use the x-ray transform on this reconstruction to obtain a `` completed '' sinogram , as if it came from a full 180 $ ^\\circ $ measurement . we feed this to standard analytical and iterative reconstruction techniques to obtain the final reconstruction . we show with extensive experimentation that this combined strategy outperforms many competitive baselines . we also propose a measure of confidence for the reconstruction that enables a practitioner to gauge the reliability of a prediction made by our network . we show that this measure is a strong indicator of quality as measured by the psnr , while not requiring ground truth at test time . finally , using a segmentation experiment , we show that our reconstruction preserves the 3d structure of objects effectively .", "topics": ["baseline ( configuration management )", "ground truth"]}
{"title": "towards a knowledge graph based speech interface", "abstract": "applications which use human speech as an input require a speech interface with high recognition accuracy . the words or phrases in the recognised text are annotated with a machine-understandable meaning and linked to knowledge graphs for further processing by the target application . these semantic annotations of recognised words can be represented as a subject-predicate-object triples which collectively form a graph often referred to as a knowledge graph . this type of knowledge representation facilitates to use speech interfaces with any spoken input application , since the information is represented in logical , semantic form , retrieving and storing can be followed using any web standard query languages . in this work , we develop a methodology for linking speech input to knowledge graphs and study the impact of recognition errors in the overall process . we show that for a corpus with lower wer , the annotation and linking of entities to the dbpedia knowledge graph is considerable . dbpedia spotlight , a tool to interlink text documents with the linked open data is used to link the speech recognition output to the dbpedia knowledge graph . such a knowledge-based speech recognition interface is useful for applications such as question answering or spoken dialog systems .", "topics": ["speech recognition"]}
{"title": "bayesian structured prediction using gaussian processes", "abstract": "we introduce a conceptually novel structured prediction model , gpstruct , which is kernelized , non-parametric and bayesian , by design . we motivate the model with respect to existing approaches , among others , conditional random fields ( crfs ) , maximum margin markov networks ( m3n ) , and structured support vector machines ( svmstruct ) , which embody only a subset of its properties . we present an inference procedure based on markov chain monte carlo . the framework can be instantiated for a wide range of structured objects such as linear chains , trees , grids , and other general graphs . as a proof of concept , the model is benchmarked on several natural language processing tasks and a video gesture segmentation task involving a linear chain structure . we show prediction accuracies for gpstruct which are comparable to or exceeding those of crfs and svmstruct .", "topics": ["support vector machine", "natural language processing"]}
{"title": "undeepvo : monocular visual odometry through unsupervised deep learning", "abstract": "we propose a novel monocular visual odometry ( vo ) system called undeepvo in this paper . undeepvo is able to estimate the 6-dof pose of a monocular camera and the depth of its view by using deep neural networks . there are two salient features of the proposed undeepvo : one is the unsupervised deep learning scheme , and the other is the absolute scale recovery . specifically , we train undeepvo by using stereo image pairs to recover the scale but test it by using consecutive monocular images . thus , undeepvo is a monocular system . the loss function defined for training the networks is based on spatial and temporal dense information . a system overview is shown in fig . 1 . the experiments on kitti dataset show our undeepvo achieves good performance in terms of pose accuracy .", "topics": ["loss function"]}
{"title": "accnet : actor-coordinator-critic net for `` learning-to-communicate '' with deep multi-agent reinforcement learning", "abstract": "communication is a critical factor for the big multi-agent world to stay organized and productive . typically , most previous multi-agent `` learning-to-communicate '' studies try to predefine the communication protocols or use technologies such as tabular reinforcement learning and evolutionary algorithm , which can not generalize to changing environment or large collection of agents . in this paper , we propose an actor-coordinator-critic net ( accnet ) framework for solving `` learning-to-communicate '' problem . the accnet naturally combines the powerful actor-critic reinforcement learning technology with deep learning technology . it can efficiently learn the communication protocols even from scratch under partially observable environment . we demonstrate that the accnet can achieve better results than several baselines under both continuous and discrete action space environments . we also analyse the learned protocols and discuss some design considerations .", "topics": ["reinforcement learning"]}
{"title": "another facet of lig parsing", "abstract": "in this paper we present a new parsing algorithm for linear indexed grammars ( ligs ) in the same spirit as the one described in ( vijay-shanker and weir , 1993 ) for tree adjoining grammars . for a lig $ l $ and an input string $ x $ of length $ n $ , we build a non ambiguous context-free grammar whose sentences are all ( and exclusively ) valid derivation sequences in $ l $ which lead to $ x $ . we show that this grammar can be built in $ { \\cal o } ( n^6 ) $ time and that individual parses can be extracted in linear time with the size of the extracted parse tree . though this $ { \\cal o } ( n^6 ) $ upper bound does not improve over previous results , the average case behaves much better . moreover , practical parsing times can be decreased by some statically performed computations .", "topics": ["time complexity", "parsing"]}
{"title": "faster coordinate descent via adaptive importance sampling", "abstract": "coordinate descent methods employ random partial updates of decision variables in order to solve huge-scale convex optimization problems . in this work , we introduce new adaptive rules for the random selection of their updates . by adaptive , we mean that our selection rules are based on the dual residual or the primal-dual gap estimates and can change at each iteration . we theoretically characterize the performance of our selection rules and demonstrate improvements over the state-of-the-art , and extend our theory and algorithms to general convex objectives . numerical evidence with hinge-loss support vector machines and lasso confirm that the practice follows the theory .", "topics": ["support vector machine", "gradient descent"]}
{"title": "cross-moments computation for stochastic context-free grammars", "abstract": "in this paper we consider the problem of efficient computation of cross-moments of a vector random variable represented by a stochastic context-free grammar . two types of cross-moments are discussed . the sample space for the first one is the set of all derivations of the context-free grammar , and the sample space for the second one is the set of all derivations which generate a string belonging to the language of the grammar . in the past , this problem was widely studied , but mainly for the cross-moments of scalar variables and up to the second order . this paper presents new algorithms for computing the cross-moments of an arbitrary order , and the previously developed ones are derived as special cases .", "topics": ["computation"]}
{"title": "causal mechanism-based model construction", "abstract": "we propose a framework for building graphical causal model that is based on the concept of causal mechanisms . causal models are intuitive for human users and , more importantly , support the prediction of the effect of manipulation . we describe an implementation of the proposed framework as an interactive model construction module , imagenie , in smile ( structural modeling , inference , and learning engine ) and in genie ( smile 's windows user interface ) .", "topics": ["causality"]}
{"title": "on the semantics and complexity of probabilistic logic programs", "abstract": "we examine the meaning and the complexity of probabilistic logic programs that consist of a set of rules and a set of independent probabilistic facts ( that is , programs based on sato 's distribution semantics ) . we focus on two semantics , respectively based on stable and on well-founded models . we show that the semantics based on stable models ( referred to as the `` credal semantics '' ) produces sets of probability models that dominate infinitely monotone choquet capacities , we describe several useful consequences of this result . we then examine the complexity of inference with probabilistic logic programs . we distinguish between the complexity of inference when a probabilistic program and a query are given ( the inferential complexity ) , and the complexity of inference when the probabilistic program is fixed and the query is given ( the query complexity , akin to data complexity as used in database theory ) . we obtain results on the inferential and query complexity for acyclic , stratified , and cyclic propositional and relational programs , complexity reaches various levels of the counting hierarchy and even exponential levels .", "topics": ["time complexity"]}
{"title": "datr theories and datr models", "abstract": "evans and gazdar introduced datr as a simple , non-monotonic language for representing natural language lexicons . although a number of implementations of datr exist , the full language has until now lacked an explicit , declarative semantics . this paper rectifies the situation by providing a mathematical semantics for datr . we present a view of datr as a language for defining certain kinds of partial functions by cases . the formal model provides a transparent treatment of datr 's notion of global context . it is shown that datr 's default mechanism can be accounted for by interpreting value descriptors as families of values indexed by paths .", "topics": ["natural language processing", "natural language"]}
{"title": "regret analysis of the finite-horizon gittins index strategy for multi-armed bandits", "abstract": "i analyse the frequentist regret of the famous gittins index strategy for multi-armed bandits with gaussian noise and a finite horizon . remarkably it turns out that this approach leads to finite-time regret guarantees comparable to those available for the popular ucb algorithm . along the way i derive finite-time bounds on the gittins index that are asymptotically exact and may be of independent interest . i also discuss some computational issues and present experimental results suggesting that a particular version of the gittins index strategy is a modest improvement on existing algorithms with finite-time regret guarantees such as ucb and thompson sampling .", "topics": ["regret ( decision theory )"]}
{"title": "bounded planning in passive pomdps", "abstract": "in passive pomdps actions do not affect the world state , but still incur costs . when the agent is bounded by information-processing constraints , it can only keep an approximation of the belief . we present a variational principle for the problem of maintaining the information which is most useful for minimizing the cost , and introduce an efficient and simple algorithm for finding an optimum .", "topics": ["mathematical optimization"]}
{"title": "robopinion : opinion mining framework inspired by autonomous robot navigation", "abstract": "data association methods are used by autonomous robots to find matches between the current landmarks and the new set of observed features . we seek a framework for opinion mining to benefit from advancements in autonomous robot navigation in both research and development", "topics": ["autonomous car", "robot"]}
{"title": "evaluating informal-domain word representations with urbandictionary", "abstract": "existing corpora for intrinsic evaluation are not targeted towards tasks in informal domains such as twitter or news comment forums . we want to test whether a representation of informal words fulfills the promise of eliding explicit text normalization as a preprocessing step . one possible evaluation metric for such domains is the proximity of spelling variants . we propose how such a metric might be computed and how a spelling variant dataset can be collected using urbandictionary .", "topics": ["natural language processing", "text corpus"]}
{"title": "provably accurate double-sparse coding", "abstract": "sparse coding is a crucial subroutine in algorithms for various signal processing , deep learning , and other machine learning applications . the central goal is to learn an overcomplete dictionary that can sparsely represent a given input dataset . however , a key challenge is that storage , transmission , and processing of the learned dictionary can be untenably high if the data dimension is high . in this paper , we consider the double-sparsity model introduced by rubinstein et al . ( 2010b ) where the dictionary itself is the product of a fixed , known basis and a data-adaptive sparse component . first , we introduce a simple algorithm for double-sparse coding that can be amenable to efficient implementation via neural architectures . second , we theoretically analyze its performance and demonstrate asymptotic sample complexity and running time benefits over existing ( provable ) approaches for sparse coding . to our knowledge , our work introduces the first computationally efficient algorithm for double-sparse coding that enjoys rigorous statistical guarantees . finally , we support our analysis via several numerical experiments on simulated data , confirming that our method can indeed be useful in problem sizes encountered in practical applications .", "topics": ["computational complexity theory", "time complexity"]}
{"title": "control of gene regulatory networks with noisy measurements and uncertain inputs", "abstract": "this paper is concerned with the problem of stochastic control of gene regulatory networks ( grns ) observed indirectly through noisy measurements and with uncertainty in the intervention inputs . the partial observability of the gene states and uncertainty in the intervention process are accounted for by modeling grns using the partially-observed boolean dynamical system ( pobds ) signal model with noisy gene expression measurements . obtaining the optimal infinite-horizon control strategy for this problem is not attainable in general , and we apply reinforcement learning and gaussian process techniques to find a near-optimal solution . the pobds is first transformed to a directly-observed markov decision process in a continuous belief space , and the gaussian process is used for modeling the cost function over the belief and intervention spaces . reinforcement learning then is used to learn the cost function from the available gene expression data . in addition , we employ sparsification , which enables the control of large partially-observed grns . the performance of the resulting algorithm is studied through a comprehensive set of numerical experiments using synthetic gene expression data generated from a melanoma gene regulatory network .", "topics": ["optimization problem", "reinforcement learning"]}
{"title": "bayesian extensions of kernel least mean squares", "abstract": "the kernel least mean squares ( klms ) algorithm is a computationally efficient nonlinear adaptive filtering method that `` kernelizes '' the celebrated ( linear ) least mean squares algorithm . we demonstrate that the least mean squares algorithm is closely related to the kalman filtering , and thus , the klms can be interpreted as an approximate bayesian filtering method . this allows us to systematically develop extensions of the klms by modifying the underlying state-space and observation models . the resulting extensions introduce many desirable properties such as `` forgetting '' , and the ability to learn from discrete data , while retaining the computational simplicity and time complexity of the original algorithm .", "topics": ["kernel ( operating system )", "computational complexity theory"]}
{"title": "clustering for different scales of measurement - the gap-ratio weighted k-means algorithm", "abstract": "this paper describes a method for clustering data that are spread out over large regions and which dimensions are on different scales of measurement . such an algorithm was developed to implement a robotics application consisting in sorting and storing objects in an unsupervised way . the toy dataset used to validate such application consists of lego bricks of different shapes and colors . the uncontrolled lighting conditions together with the use of rgb color features , respectively involve data with a large spread and different levels of measurement between data dimensions . to overcome the combination of these two characteristics in the data , we have developed a new weighted k-means algorithm , called gap-ratio k-means , which consists in weighting each dimension of the feature space before running the k-means algorithm . the weight associated with a feature is proportional to the ratio of the biggest gap between two consecutive data points , and the average of all the other gaps . this method is compared with two other variants of k-means on the lego bricks clustering problem as well as two other common classification datasets .", "topics": ["cluster analysis", "feature vector"]}
{"title": "bidirectional heuristic search reconsidered", "abstract": "the assessment of bidirectional heuristic search has been incorrect since it was first published more than a quarter of a century ago . for quite a long time , this search strategy did not achieve the expected results , and there was a major misunderstanding about the reasons behind it . although there is still wide-spread belief that bidirectional heuristic search is afflicted by the problem of search frontiers passing each other , we demonstrate that this conjecture is wrong . based on this finding , we present both a new generic approach to bidirectional heuristic search and a new approach to dynamically improving heuristic values that is feasible in bidirectional search only . these approaches are put into perspective with both the traditional and more recently proposed approaches in order to facilitate a better overall understanding . empirical results of experiments with our new approaches show that bidirectional heuristic search can be performed very efficiently and also with limited memory . these results suggest that bidirectional heuristic search appears to be better for solving certain difficult problems than corresponding unidirectional search . this provides some evidence for the usefulness of a search strategy that was long neglected . in summary , we show that bidirectional heuristic search is viable and consequently propose that it be reconsidered .", "topics": ["heuristic"]}
{"title": "learning multilingual word representations using a bag-of-words autoencoder", "abstract": "recent work on learning multilingual word representations usually relies on the use of word-level alignements ( e.g . infered with the help of giza++ ) between translated sentences , in order to align the word embeddings in different languages . in this workshop paper , we investigate an autoencoder model for learning multilingual word representations that does without such word-level alignements . the autoencoder is trained to reconstruct the bag-of-word representation of given sentence from an encoded representation extracted from its translation . we evaluate our approach on a multilingual document classification task , where labeled data is available only for one language ( e.g . english ) while classification must be performed in a different language ( e.g . french ) . in our experiments , we observe that our method compares favorably with a previously proposed method that exploits word-level alignments to learn word representations .", "topics": ["autoencoder"]}
{"title": "stochastic gradient descent , weighted sampling , and the randomized kaczmarz algorithm", "abstract": "we obtain an improved finite-sample guarantee on the linear convergence of stochastic gradient descent for smooth and strongly convex objectives , improving from a quadratic dependence on the conditioning $ ( l/\\mu ) ^2 $ ( where $ l $ is a bound on the smoothness and $ \\mu $ on the strong convexity ) to a linear dependence on $ l/\\mu $ . furthermore , we show how reweighting the sampling distribution ( i.e . importance sampling ) is necessary in order to further improve convergence , and obtain a linear dependence in the average smoothness , dominating previous results . we also discuss importance sampling for sgd more broadly and show how it can improve convergence also in other scenarios . our results are based on a connection we make between sgd and the randomized kaczmarz algorithm , which allows us to transfer ideas between the separate bodies of literature studying each of the two methods . in particular , we recast the randomized kaczmarz algorithm as an instance of sgd , and apply our results to prove its exponential convergence , but to the solution of a weighted least squares problem rather than the original least squares problem . we then present a modified kaczmarz algorithm with partially biased sampling which does converge to the original least squares solution with the same exponential convergence rate .", "topics": ["sampling ( signal processing )", "time complexity"]}
{"title": "untangling local and global deformations in deep convolutional networks for image classification and sliding window detection", "abstract": "deep convolutional neural networks ( dcnns ) commonly use generic `max-pooling ' ( mp ) layers to extract deformation-invariant features , but we argue in favor of a more refined treatment . first , we introduce epitomic convolution as a building block alternative to the common convolution-mp cascade of dcnns ; while having identical complexity to mp , epitomic convolution allows for parameter sharing across different filters , resulting in faster convergence and better generalization . second , we introduce a multiple instance learning approach to explicitly accommodate global translation and scaling when training a dcnn exclusively with class labels . for this we rely on a `patchwork ' data structure that efficiently lays out all image scales and positions as candidates to a dcnn . factoring global and local deformations allows a dcnn to `focus its resources ' on the treatment of non-rigid deformations and yields a substantial classification accuracy improvement . third , further pursuing this idea , we develop an efficient dcnn sliding window object detector that employs explicit search over position , scale , and aspect ratio . we provide competitive image classification and localization results on the imagenet dataset and object detection results on the pascal voc 2007 benchmark .", "topics": ["object detection", "computer vision"]}
{"title": "utilizing the world wide web as an encyclopedia : extracting term descriptions from semi-structured texts", "abstract": "in this paper , we propose a method to extract descriptions of technical terms from web pages in order to utilize the world wide web as an encyclopedia . we use linguistic patterns and html text structures to extract text fragments containing term descriptions . we also use a language model to discard extraneous descriptions , and a clustering method to summarize resultant descriptions . we show the effectiveness of our method by way of experiments .", "topics": ["cluster analysis"]}
{"title": "pac-learning recursive logic programs : efficient algorithms", "abstract": "we present algorithms that learn certain classes of function-free recursive logic programs in polynomial time from equivalence queries . in particular , we show that a single k-ary recursive constant-depth determinate clause is learnable . two-clause programs consisting of one learnable recursive clause and one constant-depth determinate non-recursive clause are also learnable , if an additional `` basecase '' oracle is assumed . these results immediately imply the pac-learnability of these classes . although these classes of learnable recursive programs are very constrained , it is shown in a companion paper that they are maximally general , in that generalizing either class in any natural way leads to a computationally difficult learning problem . thus , taken together with its companion paper , this paper establishes a boundary of efficient learnability for recursive logic programs .", "topics": ["time complexity", "polynomial"]}
{"title": "a map approach for $ \\ell_q $ -norm regularized sparse parameter estimation using the em algorithm", "abstract": "in this paper , bayesian parameter estimation through the consideration of the maximum a posteriori ( map ) criterion is revisited under the prism of the expectation-maximization ( em ) algorithm . by incorporating a sparsity-promoting penalty term in the cost function of the estimation problem through the use of an appropriate prior distribution , we show how the em algorithm can be used to efficiently solve the corresponding optimization problem . to this end , we rely on variance-mean gaussian mixtures ( vmgm ) to describe the prior distribution , while we incorporate many nice features of these mixtures to our estimation problem . the corresponding map estimation problem is completely expressed in terms of the em algorithm , which allows for handling nonlinearities and hidden variables that can not be easily handled with traditional methods . for comparison purposes , we also develop a coordinate descent algorithm for the $ \\ell_q $ -norm penalized problem and present the performance results via simulations .", "topics": ["optimization problem", "loss function"]}
{"title": "fairness-aware machine learning : a perspective", "abstract": "algorithms learned from data are increasingly used for deciding many aspects in our life : from movies we see , to prices we pay , or medicine we get . yet there is growing evidence that decision making by inappropriately trained algorithms may unintentionally discriminate people . for example , in automated matching of candidate cvs with job descriptions , algorithms may capture and propagate ethnicity related biases . several repairs for selected algorithms have already been proposed , but the underlying mechanisms how such discrimination happens from the computational perspective are not yet scientifically understood . we need to develop theoretical understanding how algorithms may become discriminatory , and establish fundamental machine learning principles for prevention . we need to analyze machine learning process as a whole to systematically explain the roots of discrimination occurrence , which will allow to devise global machine learning optimization criteria for guaranteed prevention , as opposed to pushing empirical constraints into existing algorithms case-by-case . as a result , the state-of-the-art will advance from heuristic repairing , to proactive and theoretically supported prevention . this is needed not only because law requires to protect vulnerable people . penetration of big data initiatives will only increase , and computer science needs to provide solid explanations and accountability to the public , before public concerns lead to unnecessarily restrictive regulations against machine learning .", "topics": ["heuristic"]}
{"title": "preference elicitation and inverse reinforcement learning", "abstract": "we state the problem of inverse reinforcement learning in terms of preference elicitation , resulting in a principled ( bayesian ) statistical formulation . this generalises previous work on bayesian inverse reinforcement learning and allows us to obtain a posterior distribution on the agent 's preferences , policy and optionally , the obtained reward sequence , from observations . we examine the relation of the resulting approach to other statistical methods for inverse reinforcement learning via analysis and experimental results . we show that preferences can be determined accurately , even if the observed agent 's policy is sub-optimal with respect to its own preferences . in that case , significantly improved policies with respect to the agent 's preferences are obtained , compared to both other methods and to the performance of the demonstrated policy .", "topics": ["reinforcement learning"]}
{"title": "planning by prioritized sweeping with small backups", "abstract": "efficient planning plays a crucial role in model-based reinforcement learning . traditionally , the main planning operation is a full backup based on the current estimates of the successor states . consequently , its computation time is proportional to the number of successor states . in this paper , we introduce a new planning backup that uses only the current value of a single successor state and has a computation time independent of the number of successor states . this new backup , which we call a small backup , opens the door to a new class of model-based reinforcement learning methods that exhibit much finer control over their planning process than traditional methods . we empirically demonstrate that this increased flexibility allows for more efficient planning by showing that an implementation of prioritized sweeping based on small backups achieves a substantial performance improvement over classical implementations .", "topics": ["time complexity", "reinforcement learning"]}
{"title": "generalized k-fan multimodal deep model with shared representations", "abstract": "multimodal learning with deep boltzmann machines ( dbms ) is an generative approach to fuse multimodal inputs , and can learn the shared representation via contrastive divergence ( cd ) for classification and information retrieval tasks . however , it is a 2-fan dbm model , and can not effectively handle multiple prediction tasks . moreover , this model can not recover the hidden representations well by sampling from the conditional distribution when more than one modalities are missing . in this paper , we propose a k-fan deep structure model , which can handle the multi-input and muti-output learning problems effectively . in particular , the deep structure has k-branch for different inputs where each branch can be composed of a multi-layer deep model , and a shared representation is learned in an discriminative manner to tackle multimodal tasks . given the deep structure , we propose two objective functions to handle two multi-input and multi-output tasks : joint visual restoration and labeling , and the multi-view multi-calss object recognition tasks . to estimate the model parameters , we initialize the deep model parameters with cd to maximize the joint distribution , and then we use backpropagation to update the model according to specific objective function . the experimental results demonstrate that the model can effectively leverages multi-source information and predict multiple tasks well over competitive baselines .", "topics": ["sampling ( signal processing )", "baseline ( configuration management )"]}
{"title": "filter sharing : efficient learning of parameters for volumetric convolutions", "abstract": "typical convolutional neural networks ( cnns ) have several millions of parameters and require a large amount of annotated data to train them . in medical applications where training data is hard to come by , these sophisticated machine learning models are difficult to train . in this paper , we propose a method to reduce the inherent complexity of cnns during training by exploiting the significant redundancy that is noticed in the learnt cnn filters . our method relies on finding a small set of filters and mixing coefficients to derive every filter in each convolutional layer at the time of training itself , thereby reducing the number of parameters to be trained . we consider the problem of 3d lung nodule segmentation in ct images and demonstrate the effectiveness of our method in achieving good results with only few training examples .", "topics": ["test set", "convolution"]}
{"title": "multi-task learning with gradient guided policy specialization", "abstract": "we present a method for efficient learning of control policies for multiple related robotic motor skills . our approach consists of two stages , joint training and specialization training . during the joint training stage , a neural network policy is trained with minimal information to disambiguate the motor skills . this forces the policy to learn a common representation of the different tasks . then , during the specialization training stage we selectively split the weights of the policy based on a per-weight metric that measures the disagreement among the multiple tasks . by splitting part of the control policy , it can be further trained to specialize to each task . to update the control policy during learning , we use trust region policy optimization with generalized advantage function ( trpogae ) . we propose a modification to the gradient update stage of trpo to better accommodate multi-task learning scenarios . we evaluate our approach on three continuous motor skill learning problems in simulation : 1 ) a locomotion task where three single legged robots with considerable difference in shape and size are trained to hop forward , 2 ) a manipulation task where three robot manipulators with different sizes and joint types are trained to reach different locations in 3d space , and 3 ) locomotion of a two-legged robot , whose range of motion of one leg is constrained in different ways . we compare our training method to three baselines . the first baseline uses only joint training for the policy , the second trains independent policies for each task , and the last randomly selects weights to split . we show that our approach learns more efficiently than each of the baseline methods .", "topics": ["simulation", "iteration"]}
{"title": "node2vec : scalable feature learning for networks", "abstract": "prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms . recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves . however , present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks . here we propose node2vec , an algorithmic framework for learning continuous feature representations for nodes in networks . in node2vec , we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes . we define a flexible notion of a node 's network neighborhood and design a biased random walk procedure , which efficiently explores diverse neighborhoods . our algorithm generalizes prior work which is based on rigid notions of network neighborhoods , and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations . we demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains . taken together , our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks .", "topics": ["feature learning"]}
{"title": "hybrid systems knowledge representation using modelling environment system techniques artificial intelligence", "abstract": "knowledge-based or artificial intelligence techniques are used increasingly as alternatives to more classical techniques to model environmental systems . use of artificial intelligence ( ai ) in environmental modelling has increased with recognition of its potential . in this paper we examine the different techniques of artificial intelligence with profound examples of human perception , learning and reasoning to solve complex problems . however with the increase of complexity better methods are required . keeping in view of the above some researchers introduced the idea of hybrid mechanism in which two or more methods can be combined which seems to be a positive effort for creating a more complex ; advanced and intelligent system which has the capability to in- cooperate human decisions thus driving the landscape changes .", "topics": ["artificial intelligence"]}
{"title": "latent human traits in the language of social media : an open-vocabulary approach", "abstract": "over the past century , personality theory and research has successfully identified core sets of characteristics that consistently describe and explain fundamental differences in the way people think , feel and behave . such characteristics were derived through theory , dictionary analyses , and survey research using explicit self-reports . the availability of social media data spanning millions of users now makes it possible to automatically derive characteristics from language use -- at large scale . taking advantage of linguistic information available through facebook , we study the process of inferring a new set of potential human traits based on unprompted language use . we subject these new traits to a comprehensive set of evaluations and compare them with a popular five factor model of personality . we find that our language-based trait construct is often more generalizable in that it often predicts non-questionnaire-based outcomes better than questionnaire-based traits ( e.g . entities someone likes , income and intelligence quotient ) , while the factors remain nearly as stable as traditional factors . our approach suggests a value in new constructs of personality derived from everyday human language use .", "topics": ["entity", "dictionary"]}
{"title": "modeling state-conditional observation distribution using weighted stereo samples for factorial speech processing models", "abstract": "this paper investigates the effectiveness of factorial speech processing models in noise-robust automatic speech recognition tasks . for this purpose , the paper proposes an idealistic approach for modeling state-conditional observation distribution of factorial models based on weighted stereo samples . this approach is an extension to previous single pass retraining for ideal model compensation which is extended here to support multiple audio sources . non-stationary noises can be considered as one of these audio sources with multiple states . experiments of this paper over the set a of the aurora 2 dataset show that recognition performance can be improved by this consideration . the improvement is significant in low signal to noise energy conditions , up to 4 % absolute word recognition accuracy . in addition to the power of the proposed method in accurate representation of state-conditional observation distribution , it has an important advantage over previous methods by providing the opportunity to independently select feature spaces for both source and corrupted features . this opens a new window for seeking better feature spaces appropriate for noisy speech , independent from clean speech features .", "topics": ["feature vector", "speech recognition"]}
{"title": "sockpuppet detection in wikipedia : a corpus of real-world deceptive writing for linking identities", "abstract": "this paper describes the corpus of sockpuppet cases we gathered from wikipedia . a sockpuppet is an online user account created with a fake identity for the purpose of covering abusive behavior and/or subverting the editing regulation process . we used a semi-automated method for crawling and curating a dataset of real sockpuppet investigation cases . to the best of our knowledge , this is the first corpus available on real-world deceptive writing . we describe the process for crawling the data and some preliminary results that can be used as baseline for benchmarking research . the dataset will be released under a creative commons license from our project website : http : //docsig.cis.uab.edu .", "topics": ["baseline ( configuration management )", "text corpus"]}
{"title": "video retrieval based on deep convolutional neural network", "abstract": "recently , with the enormous growth of online videos , fast video retrieval research has received increasing attention . as an extension of image hashing techniques , traditional video hashing methods mainly depend on hand-crafted features and transform the real-valued features into binary hash codes . as videos provide far more diverse and complex visual information than images , extracting features from videos is much more challenging than that from images . therefore , high-level semantic features to represent videos are needed rather than low-level hand-crafted methods . in this paper , a deep convolutional neural network is proposed to extract high-level semantic features and a binary hash function is then integrated into this framework to achieve an end-to-end optimization . particularly , our approach also combines triplet loss function which preserves the relative similarity and difference of videos and classification loss function as the optimization objective . experiments have been performed on two public datasets and the results demonstrate the superiority of our proposed method compared with other state-of-the-art video retrieval methods .", "topics": ["high- and low-level", "mathematical optimization"]}
{"title": "pre-translation for neural machine translation", "abstract": "recently , the development of neural machine translation ( nmt ) has significantly improved the translation quality of automatic machine translation . while most sentences are more accurate and fluent than translations by statistical machine translation ( smt ) -based systems , in some cases , the nmt system produces translations that have a completely different meaning . this is especially the case when rare words occur . when using statistical machine translation , it has already been shown that significant gains can be achieved by simplifying the input in a preprocessing step . a commonly used example is the pre-reordering approach . in this work , we used phrase-based machine translation to pre-translate the input into the target language . then a neural machine translation system generates the final hypothesis using the pre-translation . thereby , we use either only the output of the phrase-based machine translation ( pbmt ) system or a combination of the pbmt output and the source sentence . we evaluate the technique on the english to german translation task . using this approach we are able to outperform the pbmt system as well as the baseline neural mt system by up to 2 bleu points . we analyzed the influence of the quality of the initial system on the final result .", "topics": ["baseline ( configuration management )", "machine translation"]}
{"title": "predictive neural networks", "abstract": "recurrent neural networks are a powerful means to cope with time series . we show that already linearly activated recurrent neural networks can approximate any time-dependent function f ( t ) given by a number of function values . the approximation can effectively be learned by simply solving a linear equation system ; no backpropagation or similar methods are needed . furthermore the network size can be reduced by taking only the most relevant components of the network . thus , in contrast to others , our approach not only learns network weights but also the network architecture . the networks have interesting properties : in the stationary case they end up in ellipse trajectories in the long run , and they allow the prediction of further values and compact representations of functions . we demonstrate this by several experiments , among them multiple superimposed oscillators ( mso ) and robotic soccer . predictive neural networks outperform the previous state-of-the-art for the mso task with a minimal number of units .", "topics": ["time series", "approximation algorithm"]}
{"title": "preconditioned temporal difference learning", "abstract": "this paper has been withdrawn by the author . this draft is withdrawn for its poor quality in english , unfortunately produced by the author when he was just starting his science route . look at the icml version instead : http : //icml2008.cs.helsinki.fi/papers/111.pdf", "topics": ["numerical analysis", "markov chain"]}
{"title": "long-term memory networks for question answering", "abstract": "question answering is an important and difficult task in the natural language processing domain , because many basic natural language processing tasks can be cast into a question answering task . several deep neural network architectures have been developed recently , which employ memory and inference components to memorize and reason over text information , and generate answers to questions . however , a major drawback of many such models is that they are capable of only generating single-word answers . in addition , they require large amount of training data to generate accurate answers . in this paper , we introduce the long-term memory network ( ltmn ) , which incorporates both an external memory module and a long short-term memory ( lstm ) module to comprehend the input data and generate multi-word answers . the ltmn model can be trained end-to-end using back-propagation and requires minimal supervision . we test our model on two synthetic data sets ( based on facebook 's babi data set ) and the real-world stanford question answering data set , and show that it can achieve state-of-the-art performance .", "topics": ["test set", "natural language processing"]}
{"title": "review-level sentiment classification with sentence-level polarity correction", "abstract": "we propose an effective technique to solving review-level sentiment classification problem by using sentence-level polarity correction . our polarity correction technique takes into account the consistency of the polarities ( positive and negative ) of sentences within each product review before performing the actual machine learning task . while sentences with inconsistent polarities are removed , sentences with consistent polarities are used to learn state-of-the-art classifiers . the technique achieved better results on different types of products reviews and outperforms baseline models without the correction technique . experimental results show an average of 82 % f-measure on four different product review domains .", "topics": ["baseline ( configuration management )"]}
{"title": "on the preliminary design of multiple gravity-assist trajectories", "abstract": "in this paper the preliminary design of multiple gravity-assist trajectories is formulated as a global optimization problem . an analysis of the structure of the solution space reveals a strong multimodality , which is strictly dependent on the complexity of the model . on the other hand it is shown how an oversimplification could prevent finding potentially interesting solutions . a trajectory model , which represents a compromise between model completeness and optimization problem complexity is then presented . the exploration of the resulting solution space is performed through a novel global search approach , which hybridizes an evolutionary based algorithm with a systematic branching strategy . this approach allows an efficient exploration of complex solution domains by automatically balancing local convergence and global search . a number of difficult multiple gravity-assist trajectory design cases demonstrates the effectiveness of the proposed methodology .", "topics": ["optimization problem"]}
{"title": "shape animation with combined captured and simulated dynamics", "abstract": "we present a novel volumetric animation generation framework to create new types of animations from raw 3d surface or point cloud sequence of captured real performances . the framework considers as input time incoherent 3d observations of a moving shape , and is thus particularly suitable for the output of performance capture platforms . in our system , a suitable virtual representation of the actor is built from real captures that allows seamless combination and simulation with virtual external forces and objects , in which the original captured actor can be reshaped , disassembled or reassembled from user-specified virtual physics . instead of using the dominant surface-based geometric representation of the capture , which is less suitable for volumetric effects , our pipeline exploits centroidal voronoi tessellation decompositions as unified volumetric representation of the real captured actor , which we show can be used seamlessly as a building block for all processing stages , from capture and tracking to virtual physic simulation . the representation makes no human specific assumption and can be used to capture and re-simulate the actor with props or other moving scenery elements . we demonstrate the potential of this pipeline for virtual reanimation of a real captured event with various unprecedented volumetric visual effects , such as volumetric distortion , erosion , morphing , gravity pull , or collisions .", "topics": ["simulation"]}
{"title": "playing for data : ground truth from computer games", "abstract": "recent progress in computer vision has been driven by high-capacity models trained on large datasets . unfortunately , creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required . in this paper , we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games . although the source code and the internal operation of commercial games are inaccessible , we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware . this enables rapid propagation of semantic labels within and across images synthesized by the game , with no access to the source code or the content . we validate the presented approach by producing dense pixel-level semantic annotations for 25 thousand images synthesized by a photorealistic open-world computer game . experiments on semantic segmentation datasets show that using the acquired data to supplement real-world images significantly increases accuracy and that the acquired data enables reducing the amount of hand-labeled real-world data : models trained with game data and just 1/3 of the camvid training set outperform models trained on the complete camvid training set .", "topics": ["computer vision", "map"]}
{"title": "dssd : deconvolutional single shot detector", "abstract": "the main contribution of this paper is an approach for introducing additional context into state-of-the-art general object detection . to achieve this we first combine a state-of-the-art classifier ( residual-101 [ 14 ] ) with a fast detection framework ( ssd [ 18 ] ) . we then augment ssd+residual-101 with deconvolution layers to introduce additional large-scale context in object detection and improve accuracy , especially for small objects , calling our resulting system dssd for deconvolutional single shot detector . while these two contributions are easily described at a high-level , a naive implementation does not succeed . instead we show that carefully adding additional stages of learned transformations , specifically a module for feed-forward connections in deconvolution and a new output module , enables this new approach and forms a potential way forward for further detection research . results are shown on both pascal voc and coco detection . our dssd with $ 513 \\times 513 $ input achieves 81.5 % map on voc2007 test , 80.0 % map on voc2012 test , and 33.2 % map on coco , outperforming a state-of-the-art method r-fcn [ 3 ] on each dataset .", "topics": ["object detection", "high- and low-level"]}
{"title": "webal-1 : workshop on artificial life and the web 2014 proceedings", "abstract": "proceedings of webal-1 : workshop on artificial life and the web 2014 , held at the 14th international conference on the synthesis and simulation of living systems ( alife 14 ) , new york , ny , 31 july 2014 .", "topics": ["simulation"]}
{"title": "manifold optimization for gaussian mixture models", "abstract": "we take a new look at parameter estimation for gaussian mixture models ( gmms ) . in particular , we propose using \\emph { riemannian manifold optimization } as a powerful counterpart to expectation maximization ( em ) . an out-of-the-box invocation of manifold optimization , however , fails spectacularly : it converges to the same solution but vastly slower . driven by intuition from manifold convexity , we then propose a reparamerization that has remarkable empirical consequences . it makes manifold optimization not only match em -- -a highly encouraging result in itself given the poor record nonlinear programming methods have had against em so far -- -but also outperform em in many practical settings , while displaying much less variability in running times . we further highlight the strengths of manifold optimization by developing a somewhat tuned manifold lbfgs method that proves even more competitive and reliable than existing manifold optimization tools . we hope that our results encourage a wider consideration of manifold optimization for parameter estimation problems .", "topics": ["mathematical optimization", "nonlinear system"]}
{"title": "evaluating causal models by comparing interventional distributions", "abstract": "the predominant method for evaluating the quality of causal models is to measure the graphical accuracy of the learned model structure . we present an alternative method for evaluating causal models that directly measures the accuracy of estimated interventional distributions . we contrast such distributional measures with structural measures , such as structural hamming distance and structural intervention distance , showing that structural measures often correspond poorly to the accuracy of estimated interventional distributions . we use a number of real and synthetic datasets to illustrate various scenarios in which structural measures provide misleading results with respect to algorithm selection and parameter tuning , and we recommend that distributional measures become the new standard for evaluating causal models .", "topics": ["synthetic data", "causality"]}
{"title": "streaming an image through the eye : the retina seen as a dithered scalable image coder", "abstract": "we propose the design of an original scalable image coder/decoder that is inspired from the mammalians retina . our coder accounts for the time-dependent and also nondeterministic behavior of the actual retina . the present work brings two main contributions : as a first step , ( i ) we design a deterministic image coder mimicking most of the retinal processing stages and then ( ii ) we introduce a retinal noise in the coding process , that we model here as a dither signal , to gain interesting perceptual features . regarding our first contribution , our main source of inspiration will be the biologically plausible model of the retina called virtual retina . the main novelty of this coder is to show that the time-dependent behavior of the retina cells could ensure , in an implicit way , scalability and bit allocation . regarding our second contribution , we reconsider the inner layers of the retina . we emit a possible interpretation for the non-determinism observed by neurophysiologists in their output . for this sake , we model the retinal noise that occurs in these layers by a dither signal . the dithering process that we propose adds several interesting features to our image coder . the dither noise whitens the reconstruction error and decorrelates it from the input stimuli . furthermore , integrating the dither noise in our coder allows a faster recognition of the fine details of the image during the decoding process . our present paper goal is twofold . first , we aim at mimicking as closely as possible the retina for the design of a novel image coder while keeping encouraging performances . second , we bring a new insight concerning the non-deterministic behavior of the retina .", "topics": ["scalability"]}
{"title": "aggregated residual transformations for deep neural networks", "abstract": "we present a simple , highly modularized network architecture for image classification . our network is constructed by repeating a building block that aggregates a set of transformations with the same topology . our simple design results in a homogeneous , multi-branch architecture that has only a few hyper-parameters to set . this strategy exposes a new dimension , which we call `` cardinality '' ( the size of the set of transformations ) , as an essential factor in addition to the dimensions of depth and width . on the imagenet-1k dataset , we empirically show that even under the restricted condition of maintaining complexity , increasing cardinality is able to improve classification accuracy . moreover , increasing cardinality is more effective than going deeper or wider when we increase the capacity . our models , named resnext , are the foundations of our entry to the ilsvrc 2016 classification task in which we secured 2nd place . we further investigate resnext on an imagenet-5k set and the coco detection set , also showing better results than its resnet counterpart . the code and models are publicly available online .", "topics": ["neural networks", "computer vision"]}
{"title": "assessment of customer credit through combined clustering of artificial neural networks , genetics algorithm and bayesian probabilities", "abstract": "today , with respect to the increasing growth of demand to get credit from the customers of banks and finance and credit institutions , using an effective and efficient method to decrease the risk of non-repayment of credit given is very necessary . assessment of customers ' credit is one of the most important and the most essential duties of banks and institutions , and if an error occurs in this field , it would leads to the great losses for banks and institutions . thus , using the predicting computer systems has been significantly progressed in recent decades . the data that are provided to the credit institutions ' managers help them to make a straight decision for giving the credit or not-giving it . in this paper , we will assess the customer credit through a combined classification using artificial neural networks , genetics algorithm and bayesian probabilities simultaneously , and the results obtained from three methods mentioned above would be used to achieve an appropriate and final result . we use the k_folds cross validation test in order to assess the method and finally , we compare the proposed method with the methods such as clustering-launched classification ( clc ) , support vector machine ( svm ) as well as ga+svm where the genetics algorithm has been used to improve them .", "topics": ["support vector machine", "neural networks"]}
{"title": "ista-net : iterative shrinkage-thresholding algorithm inspired deep network for image compressive sensing", "abstract": "traditional methods for image compressive sensing ( cs ) reconstruction solve a well-defined inverse problem that is based on a predefined cs model , which defines the underlying structure of the problem and is generally solved by employing convergent iterative solvers . these optimization-based cs methods face the challenge of choosing optimal transforms and tuning parameters in their solvers , while also suffering from high computational complexity in most cases . recently , some deep network based cs algorithms have been proposed to improve cs reconstruction performance , while dramatically reducing time complexity as compared to optimization-based methods . despite their impressive results , the proposed networks ( either with fully-connected or repetitive convolutional layers ) lack any structural diversity and they are trained as a black box , void of any insights from the cs domain . in this paper , we combine the merits of both types of cs methods : the structure insights of optimization-based method and the performance/speed of network-based ones . we propose a novel structured deep network , dubbed ista-net , which is inspired by the iterative shrinkage-thresholding algorithm ( ista ) for optimizing a general $ l_1 $ norm cs reconstruction model . ista-net essentially implements a truncated form of ista , where all ista-net parameters are learned end-to-end to minimize a reconstruction error in training . borrowing more insights from the optimization realm , we propose an accelerated version of ista-net , dubbed fista-net , which is inspired by the fast iterative shrinkage-thresholding algorithm ( fista ) . interestingly , this acceleration naturally leads to skip connections in the underlying network design . extensive cs experiments demonstrate that the proposed ista-net and fista-net outperform existing optimization-based and network-based cs methods by large margins , while maintaining a fast runtime .", "topics": ["computational complexity theory", "time complexity"]}
{"title": "a spectral learning approach to range-only slam", "abstract": "we present a novel spectral learning algorithm for simultaneous localization and mapping ( slam ) from range data with known correspondences . this algorithm is an instance of a general spectral system identification framework , from which it inherits several desirable properties , including statistical consistency and no local optima . compared with popular batch optimization or multiple-hypothesis tracking ( mht ) methods for range-only slam , our spectral approach offers guaranteed low computational requirements and good tracking performance . compared with popular extended kalman filter ( ekf ) or extended information filter ( eif ) approaches , and many mht ones , our approach does not need to linearize a transition or measurement model ; such linearizations can cause severe errors in ekfs and eifs , and to a lesser extent mht , particularly for the highly non-gaussian posteriors encountered in range-only slam . we provide a theoretical analysis of our method , including finite-sample error bounds . finally , we demonstrate on a real-world robotic slam problem that our algorithm is not only theoretically justified , but works well in practice : in a comparison of multiple methods , the lowest errors come from a combination of our algorithm with batch optimization , but our method alone produces nearly as good a result at far lower computational cost .", "topics": ["robot"]}
{"title": "feedback detection for live predictors", "abstract": "a predictor that is deployed in a live production system may perturb the features it uses to make predictions . such a feedback loop can occur , for example , when a model that predicts a certain type of behavior ends up causing the behavior it predicts , thus creating a self-fulfilling prophecy . in this paper we analyze predictor feedback detection as a causal inference problem , and introduce a local randomization scheme that can be used to detect non-linear feedback in real-world problems . we conduct a pilot study for our proposed methodology using a predictive system currently deployed as a part of a search engine .", "topics": ["nonlinear system", "causality"]}
{"title": "improving data driven wordclass tagging by system combination", "abstract": "in this paper we examine how the differences in modelling between different data driven systems performing the same nlp task can be exploited to yield a higher accuracy than the best individual system . we do this by means of an experiment involving the task of morpho-syntactic wordclass tagging . four well-known tagger generators ( hidden markov model , memory-based , transformation rules and maximum entropy ) are trained on the same corpus data . after comparison , their outputs are combined using several voting strategies and second stage classifiers . all combination taggers outperform their best component , with the best combination showing a 19.1 % lower error rate than the best individual tagger .", "topics": ["natural language processing", "natural language"]}
{"title": "lossy image compression with compressive autoencoders", "abstract": "we propose a new approach to the problem of optimizing autoencoders for lossy image compression . new media formats , changing hardware technology , as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs . autoencoders have the potential to address this need , but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss . we here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with jpeg 2000 and outperforming recently proposed approaches based on rnns . our network is furthermore computationally efficient thanks to a sub-pixel architecture , which makes it suitable for high-resolution images . this is in contrast to previous work on autoencoders for compression using coarser approximations , shallower architectures , computationally expensive methods , or focusing on small images .", "topics": ["computational complexity theory", "approximation"]}
{"title": "monotone conditional complexity bounds on future prediction errors", "abstract": "we bound the future loss when predicting any ( computably ) stochastic sequence online . solomonoff finitely bounded the total deviation of his universal predictor m from the true distribution m by the algorithmic complexity of m . here we assume we are at a time t > 1 and already observed x=x_1 ... x_t . we bound the future prediction performance on x_ { t+1 } x_ { t+2 } ... by a new variant of algorithmic complexity of m given x , plus the complexity of the randomness deficiency of x . the new complexity is monotone in its condition in the sense that this complexity can only decrease if the condition is prolonged . we also briefly discuss potential generalizations to bayesian model classes and to classification problems .", "topics": ["computational complexity theory", "bayesian network"]}
{"title": "image annotation using multi-layer sparse coding", "abstract": "automatic annotation of images with descriptive words is a challenging problem with vast applications in the areas of image search and retrieval . this problem can be viewed as a label-assignment problem by a classifier dealing with a very large set of labels , i.e . , the vocabulary set . we propose a novel annotation method that employs two layers of sparse coding and performs coarse-to-fine labeling . themes extracted from the training data are treated as coarse labels . each theme is a set of training images that share a common subject in their visual and textual contents . our system extracts coarse labels for training and test images without requiring any prior knowledge . vocabulary words are the fine labels to be associated with images . most of the annotation methods achieve low recall due to the large number of available fine labels , i.e . , vocabulary words . these systems also tend to achieve high precision for highly frequent words only while relatively rare words are more important for search and retrieval purposes . our system not only outperforms various previously proposed annotation systems , but also achieves symmetric response in terms of precision and recall . our system scores and maintains high precision for words with a wide range of frequencies . such behavior is achieved by intelligently reducing the number of available fine labels or words for each image based on coarse labels assigned to it .", "topics": ["test set", "sparse matrix"]}
{"title": "a non-binary associative memory with exponential pattern retrieval capacity and iterative learning : extended results", "abstract": "we consider the problem of neural association for a network of non-binary neurons . here , the task is to first memorize a set of patterns using a network of neurons whose states assume values from a finite number of integer levels . later , the same network should be able to recall previously memorized patterns from their noisy versions . prior work in this area consider storing a finite number of purely random patterns , and have shown that the pattern retrieval capacities ( maximum number of patterns that can be memorized ) scale only linearly with the number of neurons in the network . in our formulation of the problem , we concentrate on exploiting redundancy and internal structure of the patterns in order to improve the pattern retrieval capacity . our first result shows that if the given patterns have a suitable linear-algebraic structure , i.e . comprise a sub-space of the set of all possible patterns , then the pattern retrieval capacity is in fact exponential in terms of the number of neurons . the second result extends the previous finding to cases where the patterns have weak minor components , i.e . the smallest eigenvalues of the correlation matrix tend toward zero . we will use these minor components ( or the basis vectors of the pattern null space ) to both increase the pattern retrieval capacity and error correction capabilities . an iterative algorithm is proposed for the learning phase , and two simple neural update algorithms are presented for the recall phase . using analytical results and simulations , we show that the proposed methods can tolerate a fair amount of errors in the input while being able to memorize an exponentially large number of patterns .", "topics": ["time complexity", "simulation"]}
{"title": "which neural net architectures give rise to exploding and vanishing gradients ?", "abstract": "we give a rigorous analysis of the statistical behavior of gradients in randomly initialized feed-forward networks with relu activations . our results show that a fully connected depth $ d $ relu net with hidden layer widths $ n_j $ will have exploding and vanishing gradients if and only if $ \\sum_ { j=1 } ^ { d-1 } 1/n_j $ is large . the point of view of this article is that whether a given neural net will have exploding/vanishing gradients is a function mainly of the architecture of the net , and hence can be tested at initialization . our results imply that a fully connected network that produces manageable gradients at initialization must have many hidden layers that are about as wide as the network is deep . this work is related to the mean field theory approach to random neural nets . from this point of view , we give a rigorous computation of the $ 1/n_j $ corrections to the propagation of gradients at the so-called edge of chaos .", "topics": ["computation"]}
{"title": "robust bayesian optimization with student-t likelihood", "abstract": "bayesian optimization has recently attracted the attention of the automatic machine learning community for its excellent results in hyperparameter tuning . bo is characterized by the sample efficiency with which it can optimize expensive black-box functions . the efficiency is achieved in a similar fashion to the learning to learn methods : surrogate models ( typically in the form of gaussian processes ) learn the target function and perform intelligent sampling . this surrogate model can be applied even in the presence of noise ; however , as with most regression methods , it is very sensitive to outlier data . this can result in erroneous predictions and , in the case of bo , biased and inefficient exploration . in this work , we present a gp model that is robust to outliers which uses a student-t likelihood to segregate outliers and robustly conduct bayesian optimization . we present numerical results evaluating the proposed method in both artificial functions and real problems .", "topics": ["sampling ( signal processing )", "numerical analysis"]}
{"title": "on understanding and machine understanding", "abstract": "in the present paper , we try to propose a self-similar network theory for the basic understanding . by extending the natural languages to a kind of so called idealy sufficient language , we can proceed a few steps to the investigation of the language searching and the language understanding of ai . image understanding , and the familiarity of the brain to the surrounding environment are also discussed . group effects are discussed by addressing the essense of the power of influences , and constructing the influence network of a society . we also give a discussion of inspirations .", "topics": ["natural language", "artificial intelligence"]}
{"title": "learning shape and texture characteristics of ct tree-in-bud opacities for cad systems", "abstract": "although radiologists can employ cad systems to characterize malignancies , pulmonary fibrosis and other chronic diseases ; the design of imaging techniques to quantify infectious diseases continue to lag behind . there exists a need to create more cad systems capable of detecting and quantifying characteristic patterns often seen in respiratory tract infections such as influenza , bacterial pneumonia , or tuborculosis . one of such patterns is tree-in-bud ( tib ) which presents \\textit { thickened } bronchial structures surrounding by clusters of \\textit { micro-nodules } . automatic detection of tib patterns is a challenging task because of their weak boundary , noisy appearance , and small lesion size . in this paper , we present two novel methods for automatically detecting tib patterns : ( 1 ) a fast localization of candidate patterns using information from local scale of the images , and ( 2 ) a m\\ '' { o } bius invariant feature extraction method based on learned local shape and texture properties . a comparative evaluation of the proposed methods is presented with a dataset of 39 laboratory confirmed viral bronchiolitis human parainfluenza ( hpiv ) cts and 21 normal lung cts . experimental results demonstrate that the proposed cad system can achieve high detection rate with an overall accuracy of 90.96 % .", "topics": ["feature extraction"]}
{"title": "deep learning for detecting robotic grasps", "abstract": "we consider the problem of detecting robotic grasps in an rgb-d view of a scene containing objects . in this work , we apply a deep learning approach to solve this problem , which avoids time-consuming hand-design of features . this presents two main challenges . first , we need to evaluate a huge number of candidate grasps . in order to make detection fast , as well as robust , we present a two-step cascaded structure with two deep networks , where the top detections from the first are re-evaluated by the second . the first network has fewer features , is faster to run , and can effectively prune out unlikely candidate grasps . the second , with more features , is slower but has to run only on the top few detections . second , we need to handle multimodal inputs well , for which we present a method to apply structured regularization on the weights based on multimodal group regularization . we demonstrate that our method outperforms the previous state-of-the-art methods in robotic grasp detection , and can be used to successfully execute grasps on two different robotic platforms .", "topics": ["matrix regularization", "sensor"]}
{"title": "neural random forests", "abstract": "given an ensemble of randomized regression trees , it is possible to restructure them as a collection of multilayered neural networks with particular connection weights . following this principle , we reformulate the random forest method of breiman ( 2001 ) into a neural network setting , and in turn propose two new hybrid procedures that we call neural random forests . both predictors exploit prior knowledge of regression trees for their architecture , have less parameters to tune than standard networks , and less restrictions on the geometry of the decision boundaries . consistency results are proved , and substantial numerical evidence is provided on both synthetic and real data sets to assess the excellent performance of our methods in a large variety of prediction problems .", "topics": ["numerical analysis", "synthetic data"]}
{"title": "resset : a recurrent model for sequence of sets with applications to electronic medical records", "abstract": "modern healthcare is ripe for disruption by ai . a game changer would be automatic understanding the latent processes from electronic medical records , which are being collected for billions of people worldwide . however , these healthcare processes are complicated by the interaction between at least three dynamic components : the illness which involves multiple diseases , the care which involves multiple treatments , and the recording practice which is biased and erroneous . existing methods are inadequate in capturing the dynamic structure of care . we propose resset , an end-to-end recurrent model that reads medical record and predicts future risk . the model adopts the algebraic view in that discrete medical objects are embedded into continuous vectors lying in the same space . we formulate the problem as modeling sequences of sets , a novel setting that have rarely , if not , been addressed . within resset , the bag of diseases recorded at each clinic visit is modeled as function of sets . the same hold for the bag of treatments . the interaction between the disease bag and the treatment bag at a visit is modeled in several , one of which as residual of diseases minus the treatments . finally , the health trajectory , which is a sequence of visits , is modeled using a recurrent neural network . we report results on over a hundred thousand hospital visits by patients suffered from two costly chronic diseases -- diabetes and mental health . resset shows promises in multiple predictive tasks such as readmission prediction , treatments recommendation and diseases progression .", "topics": ["recurrent neural network", "end-to-end principle"]}
{"title": "analysis of multibeam sonar data using dissimilarity representations", "abstract": "this paper considers the problem of low-dimensional visualisation of very high dimensional information sources for the purpose of situation awareness in the maritime environment . in response to the requirement for human decision support aids to reduce information overload ( and specifically , data amenable to inter-point relative similarity measures ) appropriate to the below-water maritime domain , we are investigating a preliminary prototype topographic visualisation model . the focus of the current paper is on the mathematical problem of exploiting a relative dissimilarity representation of signals in a visual informatics mapping model , driven by real-world sonar systems . an independent source model is used to analyse the sonar beams from which a simple probabilistic input model to represent uncertainty is mapped to a latent visualisation space where data uncertainty can be accommodated . the use of euclidean and non-euclidean measures are used and the motivation for future use of non-euclidean measures is made . concepts are illustrated using a simulated 64 beam weak snr dataset with realistic sonar targets .", "topics": ["simulation"]}
{"title": "zoom-in-net : deep mining lesions for diabetic retinopathy detection", "abstract": "we propose a convolution neural network based algorithm for simultaneously diagnosing diabetic retinopathy and highlighting suspicious regions . our contributions are two folds : 1 ) a network termed zoom-in-net which mimics the zoom-in process of a clinician to examine the retinal images . trained with only image-level supervisions , zoomin-net can generate attention maps which highlight suspicious regions , and predicts the disease level accurately based on both the whole image and its high resolution suspicious patches . 2 ) only four bounding boxes generated from the automatically learned attention maps are enough to cover 80 % of the lesions labeled by an experienced ophthalmologist , which shows good localization ability of the attention maps . by clustering features at high response locations on the attention maps , we discover meaningful clusters which contain potential lesions in diabetic retinopathy . experiments show that our algorithm outperform the state-of-the-art methods on two datasets , eyepacs and messidor .", "topics": ["cluster analysis", "map"]}
{"title": "a computational study of rotating spiral waves and spatio-temporal transient chaos in a deterministic three-level active system", "abstract": "spatio-temporal dynamics of a deterministic three-level cellular automaton ( tlca ) of zykov-mikhailov type ( sov . phys . - dokl . , 1986 , vol.31 , no.1 , p.51 ) is studied numerically . evolution of spatial structures is investigated both for the original zykov-mikhailov model ( which is applicable to , for example , belousov-zhabotinskii chemical reactions ) and for proposed by us tlca , which is a generalization of zykov-mikhailov model for the case of two-channel diffusion . such the tlca is a minimal model for an excitable medium of microwave phonon laser , called phaser ( d. n. makovetskii , tech . phys . , 2004 , vol.49 , no.2 , p.224 ; cond-mat/0402640 ) . the most interesting observed forms of tlca dynamics are as follows : ( a ) spatio-temporal transient chaos in form of highly bottlenecked collective evolution of excitations by rotating spiral waves ( rsw ) with variable topological charges ; ( b ) competition of left-handed and right-handed rsw with unexpected features , including self-induced alteration of integral effective topological charge ; ( c ) transient chimera states , i.e . coexistence of regular and chaotic domains in tlca patterns ; ( d ) branching of tlca states with different symmetry which may lead to full restoring of symmetry of imperfect starting pattern . phenomena ( a ) and ( c ) are directly related to phaser dynamics features observed earlier in real experiments at liquid helium temperatures on corundum crystals doped by iron-group ions . acm : f.1.1 , i.6 , j.2 ; pacs:05.65.+b , 07.05.tp , 82.20.wt", "topics": ["numerical analysis"]}
{"title": "extension of three-variable counterfactual casual graphic model : from two-value to three-value random variable", "abstract": "the extension of counterfactual causal graphic model with three variables of vertex set in directed acyclic graph ( dag ) is discussed in this paper by extending two- value distribution to three-value distribution of the variables involved in dag . using the conditional independence as ancillary information , 6 kinds of extension counterfactual causal graphic models with some variables are extended from two-value distribution to three-value distribution and the sufficient conditions of identifiability are derived .", "topics": ["causality"]}
{"title": "image posterization using fuzzy logic and bilateral filter", "abstract": "image posterization is converting images with a large number of tones into synthetic images with distinct flat areas and a fewer number of tones . in this technical report , we present the implementation and results of using fuzzy logic in order to generate a posterized image in a simple and fast way . the image filter is based on fuzzy logic and bilateral filtering ; where , the given image is blurred to remove small details . then , the fuzzy logic is used to classify each pixel into one of three specific categories in order to reduce the number of colors . this filter was developed during building the specs on face dataset in order to add a new level of difficulty to the original face images in the dataset . this filter does not hurt the human detection performance ; however , it is considered a hindrance evading the face detection process . this filter can be used generally for posterizing images , especially those have a high contrast to get images with vivid colors .", "topics": ["synthetic data", "pixel"]}
{"title": "trial without error : towards safe reinforcement learning via human intervention", "abstract": "ai systems are increasingly applied to complex tasks that involve interaction with humans . during training , such systems are potentially dangerous , as they have n't yet learned to avoid actions that could cause serious harm . how can an ai system explore and learn without making a single mistake that harms humans or otherwise causes serious damage ? for model-free reinforcement learning , having a human `` in the loop '' and ready to intervene is currently the only way to prevent all catastrophes . we formalize human intervention for rl and show how to reduce the human labor required by training a supervised learner to imitate the human 's intervention decisions . we evaluate this scheme on atari games , with a deep rl agent being overseen by a human for four hours . when the class of catastrophes is simple , we are able to prevent all catastrophes without affecting the agent 's learning ( whereas an rl baseline fails due to catastrophic forgetting ) . however , this scheme is less successful when catastrophes are more complex : it reduces but does not eliminate catastrophes and the supervised learner fails on adversarial examples found by the agent . extrapolating to more challenging environments , we show that our implementation would not scale ( due to the infeasible amount of human labor required ) . we outline extensions of the scheme that are necessary if we are to train model-free agents without a single catastrophe .", "topics": ["baseline ( configuration management )", "supervised learning"]}
{"title": "logarithmic-time updates and queries in probabilistic networks", "abstract": "traditional databases commonly support efficient query and update procedures that operate in time which is sublinear in the size of the database . our goal in this paper is to take a first step toward dynamic reasoning in probabilistic databases with comparable efficiency . we propose a dynamic data structure that supports efficient algorithms for updating and querying singly connected bayesian networks . in the conventional algorithm , new evidence is absorbed in o ( 1 ) time and queries are processed in time o ( n ) , where n is the size of the network . we propose an algorithm which , after a preprocessing phase , allows us to answer queries in time o ( log n ) at the expense of o ( log n ) time per evidence absorption . the usefulness of sub-linear processing time manifests itself in applications requiring ( near ) real-time response over large probabilistic databases . we briefly discuss a potential application of dynamic probabilistic reasoning in computational biology .", "topics": ["time complexity", "bayesian network"]}
{"title": "language acquisition in computers", "abstract": "this project explores the nature of language acquisition in computers , guided by techniques similar to those used in children . while existing natural language processing methods are limited in scope and understanding , our system aims to gain an understanding of language from first principles and hence minimal initial input . the first portion of our system was implemented in java and is focused on understanding the morphology of language using bigrams . we use frequency distributions and differences between them to define and distinguish languages . english and french texts were analyzed to determine a difference threshold of 55 before the texts are considered to be in different languages , and this threshold was verified using spanish texts . the second portion of our system focuses on gaining an understanding of the syntax of a language using a recursive method . the program uses one of two possible methods to analyze given sentences based on either sentence patterns or surrounding words . both methods have been implemented in c++ . the program is able to understand the structure of simple sentences and learn new words . in addition , we have provided some suggestions regarding future work and potential extensions of the existing program .", "topics": ["natural language processing"]}
{"title": "graph based manifold regularized deep neural networks for automatic speech recognition", "abstract": "deep neural networks ( dnns ) have been successfully applied to a wide variety of acoustic modeling tasks in recent years . these include the applications of dnns either in a discriminative feature extraction or in a hybrid acoustic modeling scenario . despite the rapid progress in this area , a number of challenges remain in training dnns . this paper presents an effective way of training dnns using a manifold learning based regularization framework . in this framework , the parameters of the network are optimized to preserve underlying manifold based relationships between speech feature vectors while minimizing a measure of loss between network outputs and targets . this is achieved by incorporating manifold based locality constraints in the objective criterion of dnns . empirical evidence is provided to demonstrate that training a network with manifold constraints preserves structural compactness in the hidden layers of the network . manifold regularization is applied to train bottleneck dnns for feature extraction in hidden markov model ( hmm ) based speech recognition . the experiments in this work are conducted on the aurora-2 spoken digits and the aurora-4 read news large vocabulary continuous speech recognition tasks . the performance is measured in terms of word error rate ( wer ) on these tasks . it is shown that the manifold regularized dnns result in up to 37 % reduction in wer relative to standard dnns .", "topics": ["feature vector", "feature extraction"]}
{"title": "active inference for binary symmetric hidden markov models", "abstract": "we consider active maximum a posteriori ( map ) inference problem for hidden markov models ( hmm ) , where , given an initial map estimate of the hidden sequence , we select to label certain states in the sequence to improve the estimation accuracy of the remaining states . we develop an analytical approach to this problem for the case of binary symmetric hmms , and obtain a closed form solution that relates the expected error reduction to model parameters under the specified active inference scheme . we then use this solution to determine most optimal active inference scheme in terms of error reduction , and examine the relation of those schemes to heuristic principles of uncertainty reduction and solution unicity .", "topics": ["heuristic"]}
{"title": "towards deep symbolic reinforcement learning", "abstract": "deep reinforcement learning ( drl ) brings the power of deep neural networks to bear on the generic task of trial-and-error learning , and its effectiveness has been convincingly demonstrated on tasks such as atari video games and the game of go . however , contemporary drl systems inherit a number of shortcomings from the current generation of deep learning techniques . for example , they require very large datasets to work effectively , entailing that they are slow to learn even when such datasets are available . moreover , they lack the ability to reason on an abstract level , which makes it difficult to implement high-level cognitive functions such as transfer learning , analogical reasoning , and hypothesis-based reasoning . finally , their operation is largely opaque to humans , rendering them unsuitable for domains in which verifiability is important . in this paper , we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings . as proof-of-concept , we present a preliminary implementation of the architecture and apply it to several variants of a simple video game . we show that the resulting system -- though just a prototype -- learns effectively , and , by acquiring a set of symbolic rules that are easily comprehensible to humans , dramatically outperforms a conventional , fully neural drl system on a stochastic variant of the game .", "topics": ["high- and low-level", "reinforcement learning"]}
{"title": "bilbowa : fast bilingual distributed representations without word alignments", "abstract": "we introduce bilbowa ( bilingual bag-of-words without alignments ) , a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training data . instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data . this is achieved using a novel sampled bag-of-words cross-lingual objective , which is used to regularize two noise-contrastive language models for efficient cross-lingual feature learning . we show that bilingual embeddings learned using the proposed model outperform state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on wmt11 data .", "topics": ["feature learning", "test set"]}
{"title": "high-quality correspondence and segmentation estimation for dual-lens smart-phone portraits", "abstract": "estimating correspondence between two images and extracting the foreground object are two challenges in computer vision . with dual-lens smart phones , such as iphone 7plus and huawei p9 , coming into the market , two images of slightly different views provide us new information to unify the two topics . we propose a joint method to tackle them simultaneously via a joint fully connected conditional random field ( crf ) framework . the regional correspondence is used to handle textureless regions in matching and make our crf system computationally efficient . our method is evaluated over 2,000 new image pairs , and produces promising results on challenging portrait images .", "topics": ["computational complexity theory", "computer vision"]}
{"title": "bayesian approach for near-duplicate image detection", "abstract": "in this paper we propose a bayesian approach for near-duplicate image detection , and investigate how different probabilistic models affect the performance obtained . the task of identifying an image whose metadata are missing is often demanded for a myriad of applications : metadata retrieval in cultural institutions , detection of copyright violations , investigation of latent cross-links in archives and libraries , duplicate elimination in storage management , etc . the majority of current solutions are based either on voting algorithms , which are very precise , but expensive ; either on the use of visual dictionaries , which are efficient , but less precise . our approach , uses local descriptors in a novel way , which by a careful application of decision theory , allows a very fine control of the compromise between precision and efficiency . in addition , the method attains a great compromise between those two axes , with more than 99 % accuracy with less than 10 database operations .", "topics": ["feature vector", "dictionary"]}
{"title": "where is my device ? - detecting the smart device 's wearing location in the context of active safety for vulnerable road users", "abstract": "this article describes an approach to detect the wearing location of smart devices worn by pedestrians and cyclists . the detection , which is based solely on the sensors of the smart devices , is important context-information which can be used to parametrize subsequent algorithms , e.g . for dead reckoning or intention detection to improve the safety of vulnerable road users . the wearing location recognition can in terms of organic computing ( oc ) be seen as a step towards self-awareness and self-adaptation . for the wearing location detection a two-stage process is presented . it is subdivided into moving detection followed by the wearing location classification . finally , the approach is evaluated on a real world dataset consisting of pedestrians and cyclists .", "topics": ["sensor"]}
{"title": "tensor train neighborhood preserving embedding", "abstract": "in this paper , we propose a tensor train neighborhood preserving embedding ( ttnpe ) to embed multi-dimensional tensor data into low dimensional tensor subspace . novel approaches to solve the optimization problem in ttnpe are proposed . for this embedding , we evaluate novel trade-off gain among classification , computation , and dimensionality reduction ( storage ) for supervised learning . it is shown that compared to the state-of-the-arts tensor embedding methods , ttnpe achieves superior trade-off in classification , computation , and dimensionality reduction in mnist handwritten digits and weizmann face datasets .", "topics": ["supervised learning", "statistical classification"]}
{"title": "horizontally scalable submodular maximization", "abstract": "a variety of large-scale machine learning problems can be cast as instances of constrained submodular maximization . existing approaches for distributed submodular maximization have a critical drawback : the capacity - number of instances that can fit in memory - must grow with the data set size . in practice , while one can provision many machines , the capacity of each machine is limited by physical constraints . we propose a truly scalable approach for distributed submodular maximization under fixed capacity . the proposed framework applies to a broad class of algorithms and constraints and provides theoretical guarantees on the approximation factor for any available capacity . we empirically evaluate the proposed algorithm on a variety of data sets and demonstrate that it achieves performance competitive with the centralized greedy solution .", "topics": ["scalability"]}
{"title": "algorithms for differentially private multi-armed bandits", "abstract": "we present differentially private algorithms for the stochastic multi-armed bandit ( mab ) problem . this is a problem for applications such as adaptive clinical trials , experiment design , and user-targeted advertising where private information is connected to individual rewards . our major contribution is to show that there exist $ ( \\epsilon , \\delta ) $ differentially private variants of upper confidence bound algorithms which have optimal regret , $ o ( \\epsilon^ { -1 } + \\log t ) $ . this is a significant improvement over previous results , which only achieve poly-log regret $ o ( \\epsilon^ { -2 } \\log^ { 2 } t ) $ , because of our use of a novel interval-based mechanism . we also substantially improve the bounds of previous family of algorithms which use a continual release mechanism . experiments clearly validate our theoretical bounds .", "topics": ["regret ( decision theory )"]}
{"title": "using neural networks to improve classical operating system fingerprinting techniques", "abstract": "we present remote operating system detection as an inference problem : given a set of observations ( the target host responses to a set of tests ) , we want to infer the os type which most probably generated these observations . classical techniques used to perform this analysis present several limitations . to improve the analysis , we have developed tools using neural networks and statistics tools . we present two working modules : one which uses dce-rpc endpoints to distinguish windows versions , and another which uses nmap signatures to distinguish different version of windows , linux , solaris , openbsd , freebsd and netbsd systems . we explain the details of the topology and inner workings of the neural networks used , and the fine tuning of their parameters . finally we show positive experimental results .", "topics": ["neural networks"]}
{"title": "on the universality of cognitive tests", "abstract": "the analysis of the adaptive behaviour of many different kinds of systems such as humans , animals and machines , requires more general ways of assessing their cognitive abilities . this need is strengthened by increasingly more tasks being analysed for and completed by a wider diversity of systems , including swarms and hybrids . the notion of universal test has recently emerged in the context of machine intelligence evaluation as a way to define and use the same cognitive test for a variety of systems , using some principled tasks and adapting the interface to each particular subject . however , how far can universal tests be taken ? this paper analyses this question in terms of subjects , environments , space-time resolution , rewards and interfaces . this leads to a number of findings , insights and caveats , according to several levels where universal tests may be progressively more difficult to conceive , implement and administer . one of the most significant contributions is given by the realisation that more universal tests are defined as maximisations of less universal tests for a variety of configurations . this means that universal tests must be necessarily adaptive .", "topics": ["artificial intelligence"]}
{"title": "adc : automated deep compression and acceleration with reinforcement learning", "abstract": "model compression is an effective technique facilitating the deployment of neural network models on mobile devices that have limited computation resources and a tight power budget . however , conventional model compression techniques use hand-crafted features and require domain experts to explore the large design space trading off model size , speed , and accuracy , which is usually sub-optimal and time-consuming . in this paper , we propose automated deep compression ( adc ) that leverages reinforcement learning in order to efficiently sample the design space and greatly improve the model compression quality . we achieved state-of-the-art model compression results in a fully automated way without any human efforts . under 4x flops reduction , we achieved 2.7 % better accuracy than hand-crafted model compression method for vgg-16 on imagenet . we applied this automated , push-the-button compression pipeline to mobilenet and achieved a 2x reduction in flops , and a speedup of 1.49x on titan xp and 1.65x on an android phone ( samsung galaxy s7 ) , with negligible loss of accuracy .", "topics": ["reinforcement learning", "computation"]}
{"title": "acquisition of translation lexicons for historically unwritten languages via bridging loanwords", "abstract": "with the advent of informal electronic communications such as social media , colloquial languages that were historically unwritten are being written for the first time in heavily code-switched environments . we present a method for inducing portions of translation lexicons through the use of expert knowledge in these settings where there are approximately zero resources available other than a language informant , potentially not even large amounts of monolingual data . we investigate inducing a moroccan darija-english translation lexicon via french loanwords bridging into english and find that a useful lexicon is induced for human-assisted translation and statistical machine translation .", "topics": ["machine translation"]}
{"title": "fast admm algorithm for distributed optimization with adaptive penalty", "abstract": "we propose new methods to speed up convergence of the alternating direction method of multipliers ( admm ) , a common optimization tool in the context of large scale and distributed learning . the proposed method accelerates the speed of convergence by automatically deciding the constraint penalty needed for parameter consensus in each iteration . in addition , we also propose an extension of the method that adaptively determines the maximum number of iterations to update the penalty . we show that this approach effectively leads to an adaptive , dynamic network topology underlying the distributed optimization . the utility of the new penalty update schemes is demonstrated on both synthetic and real data , including a computer vision application of distributed structure from motion .", "topics": ["mathematical optimization", "iteration"]}
{"title": "introducing a calculus of effects and handlers for natural language semantics", "abstract": "in compositional model-theoretic semantics , researchers assemble truth-conditions or other kinds of denotations using the lambda calculus . it was previously observed that the lambda terms and/or the denotations studied tend to follow the same pattern : they are instances of a monad . in this paper , we present an extension of the simply-typed lambda calculus that exploits this uniformity using the recently discovered technique of effect handlers . we prove that our calculus exhibits some of the key formal properties of the lambda calculus and we use it to construct a modular semantics for a small fragment that involves multiple distinct semantic phenomena .", "topics": ["natural language"]}
{"title": "database learning : toward a database that becomes smarter every time", "abstract": "in today 's databases , previous query answers rarely benefit answering future queries . for the first time , to the best of our knowledge , we change this paradigm in an approximate query processing ( aqp ) context . we make the following observation : the answer to each query reveals some degree of knowledge about the answer to another query because their answers stem from the same underlying distribution that has produced the entire dataset . exploiting and refining this knowledge should allow us to answer queries more analytically , rather than by reading enormous amounts of raw data . also , processing more queries should continuously enhance our knowledge of the underlying distribution , and hence lead to increasingly faster response times for future queries . we call this novel idea -- -learning from past query answers -- -database learning . we exploit the principle of maximum entropy to produce answers , which are in expectation guaranteed to be more accurate than existing sample-based approximations . empowered by this idea , we build a query engine on top of spark sql , called verdict . we conduct extensive experiments on real-world query traces from a large customer of a major database vendor . our results demonstrate that verdict supports 73.7 % of these queries , speeding them up by up to 23.0x for the same accuracy level compared to existing aqp systems .", "topics": ["approximation algorithm", "approximation"]}
{"title": "restricted boltzmann machine for classification with hierarchical correlated prior", "abstract": "restricted boltzmann machines ( rbm ) and its variants have become hot research topics recently , and widely applied to many classification problems , such as character recognition and document categorization . often , classification rbm ignores the interclass relationship or prior knowledge of sharing information among classes . in this paper , we are interested in rbm with the hierarchical prior over classes . we assume parameters for nearby nodes are correlated in the hierarchical tree , and further the parameters at each node of the tree be orthogonal to those at its ancestors . we propose a hierarchical correlated rbm for classification problem , which generalizes the classification rbm with sharing information among different classes . in order to reduce the redundancy between node parameters in the hierarchy , we also introduce orthogonal restrictions to our objective function . we test our method on challenge datasets , and show promising results compared to competitive baselines .", "topics": ["baseline ( configuration management )", "optimization problem"]}
{"title": "approximation algorithms for bayesian multi-armed bandit problems", "abstract": "in this paper , we consider several finite-horizon bayesian multi-armed bandit problems with side constraints which are computationally intractable ( np-hard ) and for which no optimal ( or near optimal ) algorithms are known to exist with sub-exponential running time . all of these problems violate the standard exchange property , which assumes that the reward from the play of an arm is not contingent upon when the arm is played . not only are index policies suboptimal in these contexts , there has been little analysis of such policies in these problem settings . we show that if we consider near-optimal policies , in the sense of approximation algorithms , then there exists ( near ) index policies . conceptually , if we can find policies that satisfy an approximate version of the exchange property , namely , that the reward from the play of an arm depends on when the arm is played to within a constant factor , then we have an avenue towards solving these problems . however such an approximate version of the idling bandit property does not hold on a per-play basis and are shown to hold in a global sense . clearly , such a property is not necessarily true of arbitrary single arm policies and finding such single arm policies is nontrivial . we show that by restricting the state spaces of arms we can find single arm policies and that these single arm policies can be combined into global ( near ) index policies where the approximate version of the exchange property is true in expectation . the number of different bandit problems that can be addressed by this technique already demonstrate its wide applicability .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "modeling uncertain and vague knowledge in possibility and evidence theories", "abstract": "this paper advocates the usefulness of new theories of uncertainty for the purpose of modeling some facets of uncertain knowledge , especially vagueness , in ai . it can be viewed as a partial reply to cheeseman 's ( among others ) defense of probability .", "topics": ["artificial intelligence"]}
{"title": "an automatic contextual analysis and clustering classifiers ensemble approach to sentiment analysis", "abstract": "products reviews are one of the major resources to determine the public sentiment . the existing literature on reviews sentiment analysis mainly utilizes supervised paradigm , which needs labeled data to be trained on and suffers from domain-dependency . this article addresses these issues by describes a completely automatic approach for sentiment analysis based on unsupervised ensemble learning . the method consists of two phases . the first phase is contextual analysis , which has five processes , namely ( 1 ) data preparation ; ( 2 ) spelling correction ; ( 3 ) intensifier handling ; ( 4 ) negation handling and ( 5 ) contrast handling . the second phase comprises the unsupervised learning approach , which is an ensemble of clustering classifiers using a majority voting mechanism with different weight schemes . the base classifier of the ensemble method is a modified k-means algorithm . the base classifier is modified by extracting initial centroids from the feature set via using sentwordnet ( swn ) . we also introduce new sentiment analysis problems of australian airlines and home builders which offer potential benchmark problems in the sentiment analysis field . our experiments on datasets from different domains show that contextual analysis and the ensemble phases improve the clustering performance in term of accuracy , stability and generalization ability .", "topics": ["cluster analysis", "unsupervised learning"]}
{"title": "combination of evidence using the principle of minimum information gain", "abstract": "one of the most important aspects in any treatment of uncertain information is the rule of combination for updating the degrees of uncertainty . the theory of belief functions uses the dempster rule to combine two belief functions defined by independent bodies of evidence . however , with limited dependency information about the accumulated belief the dempster rule may lead to unsatisfactory results . the present study suggests a method to determine the accumulated belief based on the premise that the information gain from the combination process should be minimum . this method provides a mechanism that is equivalent to the bayes rule when all the conditional probabilities are available and to the dempster rule when the normalization constant is equal to one . the proposed principle of minimum information gain is shown to be equivalent to the maximum entropy formalism , a special case of the principle of minimum cross-entropy . the application of this principle results in a monotonic increase in belief with accumulation of consistent evidence . the suggested approach may provide a more reasonable criterion for identifying conflicts among various bodies of evidence .", "topics": ["eisenstein 's criterion"]}
{"title": "visualization and clustering by 3d cellular automata : application to unstructured data", "abstract": "given the limited performance of 2d cellular automata in terms of space when the number of documents increases and in terms of visualization clusters , our motivation was to experiment these cellular automata by increasing the size to view the impact of size on quality of results . the representation of textual data was carried out by a vector model whose components are derived from the overall balancing of the used corpus , term frequency inverse document frequency ( tf-idf ) . the worldnet thesaurus has been used to address the problem of the lemmatization of the words because the representation used in this study is that of the bags of words . another independent method of the language was used to represent textual records is that of the n-grams . several measures of similarity have been tested . to validate the classification we have used two measures of assessment based on the recall and precision ( f-measure and entropy ) . the results are promising and confirm the idea to increase the dimension to the problem of the spatiality of the classes . the results obtained in terms of purity class ( i.e . the minimum value of entropy ) shows that the number of documents over longer believes the results are better for 3d cellular automata , which was not obvious to the 2d dimension . in terms of spatial navigation , cellular automata provide very good 3d performance visualization than 2d cellular automata .", "topics": ["cluster analysis", "text corpus"]}
{"title": "proceedings of enterface 2015 workshop on intelligent interfaces", "abstract": "the 11th summer workshop on multimodal interfaces enterface 2015 was hosted by the numediart institute of creative technologies of the university of mons from august 10th to september 2015 . during the four weeks , students and researchers from all over the world came together in the numediart institute of the university of mons to work on eight selected projects structured around intelligent interfaces . eight projects were selected and their reports are shown here .", "topics": ["high- and low-level", "feature extraction"]}
{"title": "optimizing recurrent neural networks architectures under time constraints", "abstract": "recurrent neural network ( rnn ) 's architecture is a key factor influencing its performance . we propose algorithms to optimize hidden sizes under running time constraint . we convert the discrete optimization into a subset selection problem . by novel transformations , the objective function becomes submodular and constraint becomes supermodular . a greedy algorithm with bounds is suggested to solve the transformed problem . and we show how transformations influence the bounds . to speed up optimization , surrogate functions are proposed which balance exploration and exploitation . experiments show that our algorithms can find more accurate models or faster models than manually tuned state-of-the-art and random search . we also compare popular rnn architectures using our algorithms .", "topics": ["optimization problem", "time complexity"]}
{"title": "min max generalization for two-stage deterministic batch mode reinforcement learning : relaxation schemes", "abstract": "we study the minmax optimization problem introduced in [ 22 ] for computing policies for batch mode reinforcement learning in a deterministic setting . first , we show that this problem is np-hard . in the two-stage case , we provide two relaxation schemes . the first relaxation scheme works by dropping some constraints in order to obtain a problem that is solvable in polynomial time . the second relaxation scheme , based on a lagrangian relaxation where all constraints are dualized , leads to a conic quadratic programming problem . we also theoretically prove and empirically illustrate that both relaxation schemes provide better results than those given in [ 22 ] .", "topics": ["optimization problem", "time complexity"]}
{"title": "bayesian image restoration for poisson corrupted image using a latent variational method with gaussian mrf", "abstract": "we treat an image restoration problem with a poisson noise chan- nel using a bayesian framework . the poisson randomness might be appeared in observation of low contrast object in the field of imaging . the noise observation is often hard to treat in a theo- retical analysis . in our formulation , we interpret the observation through the poisson noise channel as a likelihood , and evaluate the bound of it with a gaussian function using a latent variable method . we then introduce a gaussian markov random field ( gmrf ) as the prior for the bayesian approach , and derive the posterior as a gaussian distribution . the latent parameters in the likelihood and the hyperparameter in the gmrf prior could be treated as hid- den parameters , so that , we propose an algorithm to infer them in the expectation maximization ( em ) framework using loopy belief propagation ( lbp ) . we confirm the ability of our algorithm in the computer simulation , and compare it with the results of other im- age restoration frameworks .", "topics": ["calculus of variations", "simulation"]}
{"title": "stereo image de-fencing using smartphones", "abstract": "conventional approaches to image de-fencing have limited themselves to using only image data in adjacent frames of the captured video of an approximately static scene . in this work , we present a method to harness disparity using a stereo pair of fenced images in order to detect fence pixels . tourists and amateur photographers commonly carry smartphones/phablets which can be used to capture a short video sequence of the fenced scene . we model the formation of the occluded frames in the captured video . furthermore , we propose an optimization framework to estimate the de-fenced image using the total variation prior to regularize the ill-posed problem .", "topics": ["pixel"]}
{"title": "memory-efficient backpropagation through time", "abstract": "we propose a novel approach to reduce memory consumption of the backpropagation through time ( bptt ) algorithm when training recurrent neural networks ( rnns ) . our approach uses dynamic programming to balance a trade-off between caching of intermediate results and recomputation . the algorithm is capable of tightly fitting within almost any user-set memory budget while finding an optimal execution policy minimizing the computational cost . computational devices have limited memory capacity and maximizing a computational performance given a fixed memory budget is a practical use-case . we provide asymptotic computational upper bounds for various regimes . the algorithm is particularly effective for long sequences . for sequences of length 1000 , our algorithm saves 95\\ % of memory usage while using only one third more time per iteration than the standard bptt .", "topics": ["recurrent neural network", "iteration"]}
{"title": "visual graph mining", "abstract": "in this study , we formulate the concept of `` mining maximal-size frequent subgraphs '' in the challenging domain of visual data ( images and videos ) . in general , visual knowledge can usually be modeled as attributed relational graphs ( args ) with local attributes representing local parts and pairwise attributes describing the spatial relationship between parts . thus , from a practical perspective , such mining of maximal-size subgraphs can be regarded as a general platform for discovering and modeling the common objects within cluttered and unlabeled visual data . then , from a theoretical perspective , visual graph mining should encode and overcome the great fuzziness of messy data collected from complex real-world situations , which conflicts with the conventional theoretical basis of graph mining designed for tabular data . common subgraphs hidden in these args usually have soft attributes , with considerable inter-graph variation . more importantly , we should also discover the latent pattern space , including similarity metrics for the pattern and hidden node relations , during the mining process . in this study , we redefine the visual subgraph pattern that encodes all of these challenges in a general way , and propose an approximate but efficient solution to graph mining . we conduct five experiments to evaluate our method with different kinds of visual data , including videos and rgb/rgb-d images . these experiments demonstrate the generality of the proposed method .", "topics": ["approximation algorithm"]}
{"title": "may we have your attention : analysis of a selective attention task", "abstract": "in this paper we present a deeper analysis than has previously been carried out of a selective attention problem , and the evolution of continuous-time recurrent neural networks to solve it . we show that the task has a rich structure , and agents must solve a variety of subproblems to perform well . we consider the relationship between the complexity of an agent and the ease with which it can evolve behavior that generalizes well across subproblems , and demonstrate a shaping protocol that improves generalization .", "topics": ["recurrent neural network"]}
{"title": "aggressive sampling for multi-class to binary reduction with applications to text classification", "abstract": "we address the problem of multi-class classification in the case where the number of classes is very large . we propose a double sampling strategy on top of a multi-class to binary reduction strategy , which transforms the original multi-class problem into a binary classification problem over pairs of examples . the aim of the sampling strategy is to overcome the curse of long-tailed class distributions exhibited in majority of large-scale multi-class classification problems and to reduce the number of pairs of examples in the expanded data . we show that this strategy does not alter the consistency of the empirical risk minimization principle defined over the double sample reduction . experiments are carried out on dmoz and wikipedia collections with 10,000 to 100,000 classes where we show the efficiency of the proposed approach in terms of training and prediction time , memory consumption , and predictive performance with respect to state-of-the-art approaches .", "topics": ["sampling ( signal processing )"]}
{"title": "continuous video to simple signals for swimming stroke detection with convolutional neural networks", "abstract": "in many sports , it is useful to analyse video of an athlete in competition for training purposes . in swimming , stroke rate is a common metric used by coaches ; requiring a laborious labelling of each individual stroke . we show that using a convolutional neural network ( cnn ) we can automatically detect discrete events in continuous video ( in this case , swimming strokes ) . we create a cnn that learns a mapping from a window of frames to a point on a smooth 1d target signal , with peaks denoting the location of a stroke , evaluated as a sliding window . to our knowledge this process of training and utilizing a cnn has not been investigated before ; either in sports or fundamental computer vision research . most research has been focused on action recognition and using it to classify many clips in continuous video for action localisation . in this paper we demonstrate our process works well on the task of detecting swimming strokes in the wild . however , without modifying the model architecture or training method , the process is also shown to work equally well on detecting tennis strokes , implying that this is a general process . the outputs of our system are surprisingly smooth signals that predict an arbitrary event at least as accurately as humans ( manually evaluated from a sample of negative results ) . a number of different architectures are evaluated , pertaining to slightly different problem formulations and signal targets .", "topics": ["computer vision", "neural networks"]}
{"title": "an intelligent pixel replication technique by binary decomposition for digital image zooming", "abstract": "image zooming is the process of enlarging the spatial resolution of a given digital image . we present a novel technique that intelligently modifies the classical pixel replication method for zooming . our method decomposes a given image into layer of binary images , interpolates them by magnifying the binary patterns preserving their geometric shape and finally aggregates them all to obtain the zoomed image . although the quality of our zoomed images is much higher than that of nearest neighbor and bilinear interpolation and comparable with bicubic interpolation , the running time of our technique is extremely fast like nearest neighbor interpolation and much faster than bilinear and bicubic interpolation .", "topics": ["time complexity", "pixel"]}
{"title": "modeling with copulas and vines in estimation of distribution algorithms", "abstract": "the aim of this work is studying the use of copulas and vines in the optimization with estimation of distribution algorithms ( edas ) . two edas are built around the multivariate product and normal copulas , and other two are based on pair-copula decomposition of vine models . empirically we study the effect of both marginal distributions and dependence structure separately , and show that both aspects play a crucial role in the success of the optimization . the results show that the use of copulas and vines opens new opportunities to a more appropriate modeling of search distributions in edas .", "topics": ["mathematical optimization", "numerical analysis"]}
{"title": "on the difficulty of a distributional semantics of spoken language", "abstract": "the bulk of research in the area of speech processing concerns itself with supervised approaches to transcribing spoken language into text . in the domain of unsupervised learning most work on speech has focused on discovering relatively low level constructs such as phoneme inventories or word-like units . this is in contrast to research on written language , where there is a large body of work on unsupervised induction of semantic representations of words and whole sentences and longer texts . in this study we examine the challenges of adapting these approaches from written to spoken language . we conjecture that unsupervised learning of spoken language semantics becomes possible if we abstract from the surface variability . we simulate this setting by using a dataset of utterances spoken by a realistic but uniform synthetic voice . we evaluate two simple unsupervised models which , to varying degrees of success , learn semantic representations of speech fragments . finally we suggest possible routes toward transferring our methods to the domain of unrestricted natural speech .", "topics": ["unsupervised learning", "high- and low-level"]}
{"title": "position and vector detection of blind spot motion with the horn-schunck optical flow", "abstract": "the proposed method uses live image footage which , based on calculations of pixel motion , decides whether or not an object is in the blind-spot . if found , the driver is notified by a sensory light or noise built into the vehicle 's cpu . the new technology incorporates optical vectors and flow fields rather than expensive radar-waves , creating cheaper detection systems that retain the needed accuracy while adapting to the current processor speeds .", "topics": ["object detection", "pixel"]}
{"title": "using self-organising mappings to learn the structure of data manifolds", "abstract": "in this paper it is shown how to map a data manifold into a simpler form by progressively discarding small degrees of freedom . this is the key to self-organising data fusion , where the raw data is embedded in a very high-dimensional space ( e.g . the pixel values of one or more images ) , and the requirement is to isolate the important degrees of freedom which lie on a low-dimensional manifold . a useful advantage of the approach used in this paper is that the computations are arranged as a feed-forward processing chain , where all the details of the processing in each stage of the chain are learnt by self-organisation . this approach is demonstrated using hierarchically correlated data , which causes the processing chain to split the data into separate processing channels , and then to progressively merge these channels wherever they are correlated with each other . this is the key to self-organising data fusion .", "topics": ["computation", "pixel"]}
{"title": "computational cost reduction in learned transform classifications", "abstract": "we present a theoretical analysis and empirical evaluations of a novel set of techniques for computational cost reduction of classifiers that are based on learned transform and soft-threshold . by modifying optimization procedures for dictionary and classifier training , as well as the resulting dictionary entries , our techniques allow to reduce the bit precision and to replace each floating-point multiplication by a single integer bit shift . we also show how the optimization algorithms in some dictionary training methods can be modified to penalize higher-energy dictionaries . we applied our techniques with the classifier learning algorithm for soft-thresholding , testing on the datasets used in its original paper . our results indicate it is feasible to use solely sums and bit shifts of integers to classify at test time with a limited reduction of the classification accuracy . these low power operations are a valuable trade off in fpga implementations as they increase the classification throughput while decrease both energy consumption and manufacturing cost .", "topics": ["dictionary"]}
{"title": "fast label embeddings for extremely large output spaces", "abstract": "many modern multiclass and multilabel problems are characterized by increasingly large output spaces . for these problems , label embeddings have been shown to be a useful primitive that can improve computational and statistical efficiency . in this work we utilize a correspondence between rank constrained estimation and low dimensional label embeddings that uncovers a fast label embedding algorithm which works in both the multiclass and multilabel settings . the result is a randomized algorithm for partial least squares , whose running time is exponentially faster than naive algorithms . we demonstrate our techniques on two large-scale public datasets , from the large scale hierarchical text challenge and the open directory project , where we obtain state of the art results .", "topics": ["time complexity"]}
{"title": "updating probabilities", "abstract": "as examples such as the monty hall puzzle show , applying conditioning to update a probability distribution on a `` naive space ' , which does not take into account the protocol used , can often lead to counterintuitive results . here we examine why . a criterion known as car ( coarsening at random ) in the statistical literature characterizes when `` naive ' conditioning in a naive space works . we show that the car condition holds rather infrequently . we then consider more generalized notions of update such as jeffrey conditioning and minimizing relative entropy ( mre ) . we give a generalization of the car condition that characterizes when jeffrey conditioning leads to appropriate answers , but show that there are no such conditions for mre . this generalizes and interconnects previous results obtained in the literature on car and mre .", "topics": ["eisenstein 's criterion"]}
{"title": "time series segmentation through automatic feature learning", "abstract": "internet of things ( iot ) applications have become increasingly popular in recent years , with applications ranging from building energy monitoring to personal health tracking and activity recognition . in order to leverage these data , automatic knowledge extraction - whereby we map from observations to interpretable states and transitions - must be done at scale . as such , we have seen many recent iot data sets include annotations with a human expert specifying states , recorded as a set of boundaries and associated labels in a data sequence . these data can be used to build automatic labeling algorithms that produce labels as an expert would . here , we refer to human-specified boundaries as breakpoints . traditional changepoint detection methods only look for statistically-detectable boundaries that are defined as abrupt variations in the generative parameters of a data sequence . however , we observe that breakpoints occur on more subtle boundaries that are non-trivial to detect with these statistical methods . in this work , we propose a new unsupervised approach , based on deep learning , that outperforms existing techniques and learns the more subtle , breakpoint boundaries with a high accuracy . through extensive experiments on various real-world data sets - including human-activity sensing data , speech signals , and electroencephalogram ( eeg ) activity traces - we demonstrate the effectiveness of our algorithm for practical applications . furthermore , we show that our approach achieves significantly better performance than previous methods .", "topics": ["feature learning", "time series"]}
{"title": "robust propensity score computation method based on machine learning with label-corrupted data", "abstract": "in biostatistics , propensity score is a common approach to analyze the imbalance of covariate and process confounding covariates to eliminate differences between groups . while there are an abundant amount of methods to compute propensity score , a common issue of them is the corrupted labels in the dataset . for example , the data collected from the patients could contain samples that are treated mistakenly , and the computing methods could incorporate them as a misleading information . in this paper , we propose a machine learning-based method to handle the problem . specifically , we utilize the fact that the majority of sample should be labeled with the correct instance and design an approach to first cluster the data with spectral clustering and then sample a new dataset with a distribution processed from the clustering results . the propensity score is computed by xgboost , and a mathematical justification of our method is provided in this paper . the experimental results illustrate that xgboost propensity scores computing with the data processed by our method could outperform the same method with original data , and the advantages of our method increases as we add some artificial corruptions to the dataset . meanwhile , the implementation of xgboost to compute propensity score for multiple treatments is also a pioneering work in the area .", "topics": ["cluster analysis", "computation"]}
{"title": "detecting and classifying lesions in mammograms with deep learning", "abstract": "in the last two decades computer aided diagnostics ( cad ) systems were developed to help radiologists analyze screening mammograms . the benefits of current cad technologies appear to be contradictory and they should be improved to be ultimately considered useful . since 2012 deep convolutional neural networks ( cnn ) have been a tremendous success in image recognition , reaching human performance . these methods have greatly surpassed the traditional approaches , which are similar to currently used cad solutions . deep cnn-s have the potential to revolutionize medical image analysis . we propose a cad system based on one of the most successful object detection frameworks , faster r-cnn . the system detects and classifies malignant or benign lesions on a mammogram without any human intervention . the proposed method sets the state of the art classification performance on the public inbreast database , auc = 0.95 . the approach described here has achieved the 2nd place in the digital mammography dream challenge with auc = 0.85 . when used as a detector , the system reaches high sensitivity with very few false positive marks per image on the inbreast dataset . source code , the trained model and an osirix plugin are availaible online at https : //github.com/riblidezso/frcnn_cad .", "topics": ["object detection", "computer vision"]}
{"title": "all-in-1 : short text classification with one model for all languages", "abstract": "we present all-in-1 , a simple model for multilingual text classification that does not require any parallel data . it is based on a traditional support vector machine classifier exploiting multilingual word embeddings and character n-grams . our model is simple , easily extendable yet very effective , overall ranking 1st ( out of 12 teams ) in the ijcnlp 2017 shared task on customer feedback analysis in four languages : english , french , japanese and spanish .", "topics": ["support vector machine"]}
{"title": "generalized two-dimensional linear discriminant analysis with regularization", "abstract": "recent advances show that two-dimensional linear discriminant analysis ( 2dlda ) is a successful matrix based dimensionality reduction method . however , 2dlda may encounter the singularity issue theoretically and the sensitivity to outliers . in this paper , a generalized lp-norm 2dlda framework with regularization for an arbitrary $ p > 0 $ is proposed , named g2dlda . there are mainly two contributions of g2dlda : one is g2dlda model uses an arbitrary lp-norm to measure the between-class and within-class scatter , and hence a proper $ p $ can be selected to achieve the robustness . the other one is that by introducing an extra regularization term , g2dlda achieves better generalization performance , and solves the singularity problem . in addition , g2dlda can be solved through a series of convex problems with equality constraint , and it has closed solution for each single problem . its convergence can be guaranteed theoretically when $ 1\\leq p\\leq2 $ . preliminary experimental results on three contaminated human face databases show the effectiveness of the proposed g2dlda .", "topics": ["matrix regularization", "database"]}
{"title": "a simple language model based on pmi matrix approximations", "abstract": "in this study , we introduce a new approach for learning language models by training them to estimate word-context pointwise mutual information ( pmi ) , and then deriving the desired conditional probabilities from pmi at test time . specifically , we show that with minor modifications to word2vec 's algorithm , we get principled language models that are closely related to the well-established noise contrastive estimation ( nce ) based language models . a compelling aspect of our approach is that our models are trained with the same simple negative sampling objective function that is commonly used in word2vec to learn word embeddings .", "topics": ["sampling ( signal processing )", "loss function"]}
{"title": "deep learning in multi-layer architectures of dense nuclei", "abstract": "we assume that , within the dense clusters of neurons that can be found in nuclei , cells may interconnect via soma-to-soma interactions , in addition to conventional synaptic connections . we illustrate this idea with a multi-layer architecture ( mla ) composed of multiple clusters of recurrent sub-networks of spiking random neural networks ( rnn ) with dense soma-to-soma interactions , and use this rnn-mla architecture for deep learning . the inputs to the clusters are first normalised by adjusting the external arrival rates of spikes to each cluster . then we apply this architecture to learning from multi-channel datasets . numerical results based on both images and sensor based data , show the value of this novel architecture for deep learning .", "topics": ["neural networks", "numerical analysis"]}
{"title": "label-embedding for image classification", "abstract": "attributes act as intermediate representations that enable parameter sharing between classes , a must when training data is scarce . we propose to view attribute-based image classification as a label-embedding problem : each class is embedded in the space of attribute vectors . we introduce a function that measures the compatibility between an image and a label embedding . the parameters of this function are learned on a training set of labeled samples to ensure that , given an image , the correct classes rank higher than the incorrect ones . results on the animals with attributes and caltech-ucsd-birds datasets show that the proposed framework outperforms the standard direct attribute prediction baseline in a zero-shot learning scenario . label embedding enjoys a built-in ability to leverage alternative sources of information instead of or in addition to attributes , such as e.g . class hierarchies or textual descriptions . moreover , label embedding encompasses the whole range of learning settings from zero-shot learning to regular learning with a large number of labeled examples .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "robust metric learning by smooth optimization", "abstract": "most existing distance metric learning methods assume perfect side information that is usually given in pairwise or triplet constraints . instead , in many real-world applications , the constraints are derived from side information , such as users ' implicit feedbacks and citations among articles . as a result , these constraints are usually noisy and contain many mistakes . in this work , we aim to learn a distance metric from noisy constraints by robust optimization in a worst-case scenario , to which we refer as robust metric learning . we formulate the learning task initially as a combinatorial optimization problem , and show that it can be elegantly transformed to a convex programming problem . we present an efficient learning algorithm based on smooth optimization [ 7 ] . it has a worst-case convergence rate of o ( 1/ { \\surd } { \\varepsilon } ) for smooth optimization problems , where { \\varepsilon } is the desired error of the approximate solution . finally , our empirical study with uci data sets demonstrate the effectiveness of the proposed method in comparison to state-of-the-art methods .", "topics": ["approximation algorithm", "optimization problem"]}
{"title": "casenet : deep category-aware semantic edge detection", "abstract": "boundary and edge cues are highly beneficial in improving a wide variety of vision tasks such as semantic segmentation , object recognition , stereo , and object proposal generation . recently , the problem of edge detection has been revisited and significant progress has been made with deep learning . while classical edge detection is a challenging binary problem in itself , the category-aware semantic edge detection by nature is an even more challenging multi-label problem . we model the problem such that each edge pixel can be associated with more than one class as they appear in contours or junctions belonging to two or more semantic classes . to this end , we propose a novel end-to-end deep semantic edge learning architecture based on resnet and a new skip-layer architecture where category-wise edge activations at the top convolution layer share and are fused with the same set of bottom layer features . we then propose a multi-label loss function to supervise the fused activations . we show that our proposed architecture benefits this problem with better performance , and we outperform the current state-of-the-art semantic edge detection methods by a large margin on standard data sets such as sbd and cityscapes .", "topics": ["loss function", "end-to-end principle"]}
{"title": "kafnets : kernel-based non-parametric activation functions for neural networks", "abstract": "neural networks are generally built by interleaving ( adaptable ) linear layers with ( fixed ) nonlinear activation functions . to increase their flexibility , several authors have proposed methods for adapting the activation functions themselves , endowing them with varying degrees of flexibility . none of these approaches , however , have gained wide acceptance in practice , and research in this topic remains open . in this paper , we introduce a novel family of flexible activation functions that are based on an inexpensive kernel expansion at every neuron . leveraging over several properties of kernel-based models , we propose multiple variations for designing and initializing these kernel activation functions ( kafs ) , including a multidimensional scheme allowing to nonlinearly combine information from different paths in the network . the resulting kafs can approximate any mapping defined over a subset of the real line , either convex or nonconvex . furthermore , they are smooth over their entire domain , linear in their parameters , and they can be regularized using any known scheme , including the use of $ \\ell_1 $ penalties to enforce sparseness . to the best of our knowledge , no other known model satisfies all these properties simultaneously . in addition , we provide a relatively complete overview on alternative techniques for adapting the activation functions , which is currently lacking in the literature . a large set of experiments validates our proposal .", "topics": ["kernel ( operating system )", "approximation algorithm"]}
{"title": "distributed algorithms for feature extraction off-loading in multi-camera visual sensor networks", "abstract": "real-time visual analysis tasks , like tracking and recognition , require swift execution of computationally intensive algorithms . visual sensor networks can be enabled to perform such tasks by augmenting the sensor network with processing nodes and distributing the computational burden in a way that the cameras contend for the processing nodes while trying to minimize their task completion times . in this paper , we formulate the problem of minimizing the completion time of all camera sensors as an optimization problem . we propose algorithms for fully distributed optimization , analyze the existence of equilibrium allocations , evaluate the effect of the network topology and of the video characteristics , and the benefits of central coordination . our results demonstrate that with sufficient information available , distributed optimization can provide low completion times , moreover predictable and stable performance can be achieved with additional , sparse central coordination .", "topics": ["optimization problem", "feature extraction"]}
{"title": "conditional adversarial domain adaptation", "abstract": "adversarial learning has been embedded into deep networks to learn transferable representations for domain adaptation . existing adversarial domain adaptation methods may struggle to align different domains of multimode distributions that are native in classification problems . in this paper , we present conditional adversarial domain adaptation , a novel framework that conditions the adversarial adaptation models on discriminative information conveyed in the classifier predictions . conditional domain adversarial networks are proposed to enable discriminative adversarial adaptation of multimode domains . the experiments testify that the proposed approaches exceed the state-of-the-art performance on three domain adaptation datasets .", "topics": ["time complexity", "interaction"]}
{"title": "pseudo-marginal bayesian inference for gaussian processes", "abstract": "the main challenges that arise when adopting gaussian process priors in probabilistic modeling are how to carry out exact bayesian inference and how to account for uncertainty on model parameters when making model-based predictions on out-of-sample data . using probit regression as an illustrative working example , this paper presents a general and effective methodology based on the pseudo-marginal approach to markov chain monte carlo that efficiently addresses both of these issues . the results presented in this paper show improvements over existing sampling methods to simulate from the posterior distribution over the parameters defining the covariance function of the gaussian process prior . this is particularly important as it offers a powerful tool to carry out full bayesian inference of gaussian process based hierarchic statistical models in general . the results also demonstrate that monte carlo based integration of all model parameters is actually feasible in this class of models providing a superior quantification of uncertainty in predictions . extensive comparisons with respect to state-of-the-art probabilistic classifiers confirm this assertion .", "topics": ["sampling ( signal processing )", "simulation"]}
{"title": "on connecting stochastic gradient mcmc and differential privacy", "abstract": "significant success has been realized recently on applying machine learning to real-world applications . there have also been corresponding concerns on the privacy of training data , which relates to data security and confidentiality issues . differential privacy provides a principled and rigorous privacy guarantee on machine learning models . while it is common to design a model satisfying a required differential-privacy property by injecting noise , it is generally hard to balance the trade-off between privacy and utility . we show that stochastic gradient markov chain monte carlo ( sg-mcmc ) -- a class of scalable bayesian posterior sampling algorithms proposed recently -- satisfies strong differential privacy with carefully chosen step sizes . we develop theory on the performance of the proposed differentially-private sg-mcmc method . we conduct experiments to support our analysis and show that a standard sg-mcmc sampler without any modification ( under a default setting ) can reach state-of-the-art performance in terms of both privacy and utility on bayesian learning .", "topics": ["sampling ( signal processing )", "test set"]}
{"title": "an expert system for automatic reading of a text written in standard arabic", "abstract": "in this work we present our expert system of automatic reading or speech synthesis based on a text written in standard arabic , our work is carried out in two great stages : the creation of the sound data base , and the transformation of the written text into speech ( text to speech tts ) . this transformation is done firstly by a phonetic orthographical transcription ( pot ) of any written standard arabic text with the aim of transforming it into his corresponding phonetics sequence , and secondly by the generation of the voice signal which corresponds to the chain transcribed . we spread out the different of conception of the system , as well as the results obtained compared to others works studied to realize tts based on standard arabic .", "topics": ["database"]}
{"title": "a new convex relaxation for tensor completion", "abstract": "we study the problem of learning a tensor from a set of linear measurements . a prominent methodology for this problem is based on a generalization of trace norm regularization , which has been used extensively for learning low rank matrices , to the tensor setting . in this paper , we highlight some limitations of this approach and propose an alternative convex relaxation on the euclidean ball . we then describe a technique to solve the associated regularization problem , which builds upon the alternating direction method of multipliers . experiments on one synthetic dataset and two real datasets indicate that the proposed method improves significantly over tensor trace norm regularization in terms of estimation error , while remaining computationally tractable .", "topics": ["matrix regularization", "synthetic data"]}
{"title": "relative depth order estimation using multi-scale densely connected convolutional networks", "abstract": "we study the problem of estimating the relative depth order of point pairs in a monocular image . recent advances mainly focus on using deep convolutional neural networks ( dcnns ) to learn and infer the ordinal information from multiple contextual information of the points pair such as global scene context , local contextual information , and the locations . however , it remains unclear how much each context contributes to the task . to address this , we first examine the contribution of each context cue [ 1 ] , [ 2 ] to the performance in the context of depth order estimation . we find out the local context surrounding the points pair contributes the most and the global scene context helps little . based on the findings , we propose a simple method , using a multi-scale densely-connected network to tackle the task . instead of learning the global structure , we dedicate to explore the local structure by learning to regress from regions of multiple sizes around the point pairs . moreover , we use the recent densely connected network [ 3 ] to encourage substantial feature reuse as well as deepen our network to boost the performance . we show in experiments that the results of our approach is on par with or better than the state-of-the-art methods with the benefit of using only a small number of training data .", "topics": ["test set"]}
{"title": "automatic dialect detection in arabic broadcast speech", "abstract": "we investigate different approaches for dialect identification in arabic broadcast speech , using phonetic , lexical features obtained from a speech recognition system , and acoustic features using the i-vector framework . we studied both generative and discriminate classifiers , and we combined these features using a multi-class support vector machine ( svm ) . we validated our results on an arabic/english language identification task , with an accuracy of 100 % . we used these features in a binary classifier to discriminate between modern standard arabic ( msa ) and dialectal arabic , with an accuracy of 100 % . we further report results using the proposed method to discriminate between the five most widely used dialects of arabic : namely egyptian , gulf , levantine , north african , and msa , with an accuracy of 52 % . we discuss dialect identification errors in the context of dialect code-switching between dialectal arabic and msa , and compare the error pattern between manually labeled data , and the output from our classifier . we also release the train and test data as standard corpus for dialect identification .", "topics": ["support vector machine", "speech recognition"]}
{"title": "maximum entropy deep inverse reinforcement learning", "abstract": "this paper presents a general framework for exploiting the representational capacity of neural networks to approximate complex , nonlinear reward functions in the context of solving the inverse reinforcement learning ( irl ) problem . we show in this context that the maximum entropy paradigm for irl lends itself naturally to the efficient training of deep architectures . at test time , the approach leads to a computational complexity independent of the number of demonstrations , which makes it especially well-suited for applications in life-long learning scenarios . our approach achieves performance commensurate to the state-of-the-art on existing benchmarks while exceeding on an alternative benchmark based on highly varying reward structures . finally , we extend the basic architecture - which is equivalent to a simplified subclass of fully convolutional neural networks ( fcnns ) with width one - to include larger convolutions in order to eliminate dependency on precomputed spatial features and work on raw input representations .", "topics": ["approximation algorithm", "computational complexity theory"]}
{"title": "latest datasets and technologies presented in the workshop on grasping and manipulation datasets", "abstract": "this paper reports the activities and outcomes in the workshop on grasping and manipulation datasets that was organized under the international conference on robotics and automation ( icra ) 2016 . the half day workshop was packed with nine invited talks , 12 interactive presentations , and one panel discussion with ten panelists . this paper summarizes all the talks and presentations and recaps what has been discussed in the panels session . this summary servers as a review of recent developments in data collection in grasping and manipulation . many of the presentations describe ongoing efforts or explorations that could be achieved and fully available in a year or two . the panel discussion not only commented on the current approaches , but also indicates new directions and focuses . the workshop clearly displayed the importance of quality datasets in robotics and robotic grasping and manipulation field . hopefully the workshop could motivate larger efforts to create big datasets that are comparable with big datasets in other communities such as computer vision .", "topics": ["robot"]}
{"title": "neural networks for handwritten english alphabet recognition", "abstract": "this paper demonstrates the use of neural networks for developing a system that can recognize hand-written english alphabets . in this system , each english alphabet is represented by binary values that are used as input to a simple feature extraction system , whose output is fed to our neural network system .", "topics": ["feature extraction"]}
{"title": "on the sampling problem for kernel quadrature", "abstract": "the standard kernel quadrature method for numerical integration with random point sets ( also called bayesian monte carlo ) is known to converge in root mean square error at a rate determined by the ratio $ s/d $ , where $ s $ and $ d $ encode the smoothness and dimension of the integrand . however , an empirical investigation reveals that the rate constant $ c $ is highly sensitive to the distribution of the random points . in contrast to standard monte carlo integration , for which optimal importance sampling is well-understood , the sampling distribution that minimises $ c $ for kernel quadrature does not admit a closed form . this paper argues that the practical choice of sampling distribution is an important open problem . one solution is considered ; a novel automatic approach based on adaptive tempering and sequential monte carlo . empirical results demonstrate a dramatic reduction in integration error of up to 4 orders of magnitude can be achieved with the proposed method .", "topics": ["sampling ( signal processing )", "kernel ( operating system )"]}
{"title": "beyond the hazard rate : more perturbation algorithms for adversarial multi-armed bandits", "abstract": "recent work on follow the perturbed leader ( ftpl ) algorithms for the adversarial multi-armed bandit problem has highlighted the role of the hazard rate of the distribution generating the perturbations . assuming that the hazard rate is bounded , it is possible to provide regret analyses for a variety of ftpl algorithms for the multi-armed bandit problem . this paper pushes the inquiry into regret bounds for ftpl algorithms beyond the bounded hazard rate condition . there are good reasons to do so : natural distributions such as the uniform and gaussian violate the condition . we give regret bounds for both bounded support and unbounded support distributions without assuming the hazard rate condition . we also disprove a conjecture that the gaussian distribution can not lead to a low-regret algorithm . in fact , it turns out that it leads to near optimal regret , up to logarithmic factors . a key ingredient in our approach is the introduction of a new notion called the generalized hazard rate .", "topics": ["regret ( decision theory )"]}
{"title": "off the beaten lane : ai challenges in mobas beyond player control", "abstract": "mobas represent a huge segment of online gaming and are growing as both an esport and a casual genre . the natural starting point for ai researchers interested in mobas is to develop an ai to play the game better than a human - but mobas have many more challenges besides adversarial ai . in this paper we introduce the reader to the wider context of moba culture , propose a range of challenges faced by the community today , and posit concrete ai projects that can be undertaken to begin solving them .", "topics": ["artificial intelligence"]}
{"title": "planar cycle covering graphs", "abstract": "we describe a new variational lower-bound on the minimum energy configuration of a planar binary markov random field ( mrf ) . our method is based on adding auxiliary nodes to every face of a planar embedding of the graph in order to capture the effect of unary potentials . a ground state of the resulting approximation can be computed efficiently by reduction to minimum-weight perfect matching . we show that optimization of variational parameters achieves the same lower-bound as dual-decomposition into the set of all cycles of the original graph . we demonstrate that our variational optimization converges quickly and provides high-quality solutions to hard combinatorial problems 10-100x faster than competing algorithms that optimize the same bound .", "topics": ["calculus of variations"]}
{"title": "self-attentional acoustic models", "abstract": "self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities . these models have recently shown promising results for modeling discrete sequences , but they are non-trivial to apply to acoustic modeling due to computational and modeling issues . in this paper , we apply self-attention to acoustic modeling , proposing several improvements to mitigate these issues : first , self-attention memory grows quadratically in the sequence length , which we address through a downsampling technique . second , we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end . third , to stress the importance of local context in the acoustic signal , we propose a gaussian biasing approach that allows explicit control over the context range . experiments find that our model approaches a strong baseline based on lstms with network-in-network connections while being much faster to compute . besides speed , we find that interpretability is a strength of self-attentional acoustic models , and demonstrate that self-attention heads learn a linguistically plausible division of labor .", "topics": ["baseline ( configuration management )"]}
{"title": "identifying influential entries in a matrix", "abstract": "for any matrix a in r^ ( m x n ) of rank \\rho , we present a probability distribution over the entries of a ( the element-wise leverage scores of equation ( 2 ) ) that reveals the most influential entries in the matrix . from a theoretical perspective , we prove that sampling at most s = o ( ( m + n ) \\rho^2 ln ( m + n ) ) entries of the matrix ( see eqn . ( 3 ) for the precise value of s ) with respect to these scores and solving the nuclear norm minimization problem on the sampled entries , reconstructs a exactly . to the best of our knowledge , these are the strongest theoretical guarantees on matrix completion without any incoherence assumptions on the matrix a . from an experimental perspective , we show that entries corresponding to high element-wise leverage scores reveal structural properties of the data matrix that are of interest to domain scientists .", "topics": ["sampling ( signal processing )"]}
{"title": "decaf : a deep convolutional activation feature for generic visual recognition", "abstract": "we evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large , fixed set of object recognition tasks can be re-purposed to novel generic tasks . our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks . we investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks , including scene recognition , domain adaptation , and fine-grained recognition challenges . we compare the efficacy of relying on various network levels to define a fixed feature , and report novel results that significantly outperform the state-of-the-art on several important vision challenges . we are releasing decaf , an open-source implementation of these deep convolutional activation features , along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms .", "topics": ["cluster analysis"]}
{"title": "top-down unsupervised image segmentation ( it sounds like oxymoron , but actually it is not )", "abstract": "pattern recognition is generally assumed as an interaction of two inversely directed image-processing streams : the bottom-up information details gathering and localization ( segmentation ) stream , and the top-down information features aggregation , association and interpretation ( recognition ) stream . inspired by recent evidence from biological vision research and by the insights of kolmogorov complexity theory , we propose a new , just top-down evolving , procedure of initial image segmentation . we claim that traditional top-down cognitive reasoning , which is supposed to guide the segmentation process to its final result , is not at all a part of the image information content evaluation . and that initial image segmentation is certainly an unsupervised process . we present some illustrative examples , which support our claims .", "topics": ["image processing", "image segmentation"]}
{"title": "about compression of vocabulary in computer oriented languages", "abstract": "the author uses the entropy of the ideal bose-einstein gas to minimize losses in computer-oriented languages .", "topics": ["natural language", "synthetic data"]}
{"title": "multitask training with unlabeled data for end-to-end sign language fingerspelling recognition", "abstract": "we address the problem of automatic american sign language fingerspelling recognition from video . prior work has largely relied on frame-level labels , hand-crafted features , or other constraints , and has been hampered by the scarcity of data for this task . we introduce a model for fingerspelling recognition that addresses these issues . the model consists of an auto-encoder-based feature extractor and an attention-based neural encoder-decoder , which are trained jointly . the model receives a sequence of image frames and outputs the fingerspelled word , without relying on any frame-level training labels or hand-crafted features . in addition , the auto-encoder subcomponent makes it possible to leverage unlabeled data to improve the feature learning . the model achieves 11.6 % and 4.4 % absolute letter accuracy improvement respectively in signer-independent and signer- adapted fingerspelling recognition over previous approaches that required frame-level training labels .", "topics": ["feature learning", "end-to-end principle"]}
{"title": "gradient boosting with piece-wise linear regression trees", "abstract": "gradient boosting using decision trees as base learners , so called gradient boosted decision trees ( gbdt ) , is a very successful ensemble learning algorithm widely used across a variety of applications . recently , various gdbt construction algorithms and implementation have been designed and heavily optimized in some very popular open sourced toolkits such as xgboost and lightgbm . in this paper , we show that both the accuracy and efficiency of gbdt can be further enhanced by using more complex base learners . specifically , we extend gradient boosting to use piecewise linear regression trees ( pl trees ) , instead of piecewise constant regression trees . we show pl trees can accelerate convergence of gbdt . moreover , our new algorithm fits better to modern computer architectures with powerful single instruction multiple data ( simd ) parallelism . we propose optimization techniques to speedup our algorithm . the experimental results show that gbdt with pl trees can provide very competitive testing accuracy with comparable or less training time . our algorithm also produces much concise tree ensembles , thus can often reduce testing time costs .", "topics": ["gradient"]}
{"title": "learning shared representations in multi-task reinforcement learning", "abstract": "we investigate a paradigm in multi-task reinforcement learning ( mt-rl ) in which an agent is placed in an environment and needs to learn to perform a series of tasks , within this space . since the environment does not change , there is potentially a lot of common ground amongst tasks and learning to solve them individually seems extremely wasteful . in this paper , we explicitly model and learn this shared structure as it arises in the state-action value space . we will show how one can jointly learn optimal value-functions by modifying the popular value-iteration and policy-iteration procedures to accommodate this shared representation assumption and leverage the power of multi-task supervised learning . finally , we demonstrate that the proposed model and training procedures , are able to infer good value functions , even under low samples regimes . in addition to data efficiency , we will show in our analysis , that learning abstractions of the state space jointly across tasks leads to more robust , transferable representations with the potential for better generalization . this shared representation assumption and leverage the power of multi-task supervised learning . finally , we demonstrate that the proposed model and training procedures , are able to infer good value functions , even under low samples regimes . in addition to data efficiency , we will show in our analysis , that learning abstractions of the state space jointly across tasks leads to more robust , transferable representations with the potential for better generalization .", "topics": ["reinforcement learning"]}
{"title": "spatio-activity based object detection", "abstract": "we present the sammi lightweight object detection method which has a high level of accuracy and robustness , and which is able to operate in an environment with a large number of cameras . background modeling is based on dct coefficients provided by cameras . foreground detection uses similarity in temporal characteristics of adjacent blocks of pixels , which is a computationally inexpensive way to make use of object coherence . scene model updating uses the approximated median method for improved performance . evaluation at pixel level and application level shows that sammi object detection performs better and faster than the conventional mixture of gaussians method .", "topics": ["object detection", "pixel"]}
{"title": "frame-recurrent video super-resolution", "abstract": "recent advances in video super-resolution have shown that convolutional neural networks combined with motion compensation are able to merge information from multiple low-resolution ( lr ) frames to generate high-quality images . current state-of-the-art methods process a batch of lr frames to generate a single high-resolution ( hr ) frame and run this scheme in a sliding window fashion over the entire video , effectively treating the problem as a large number of separate multi-frame super-resolution tasks . this approach has two main weaknesses : 1 ) each input frame is processed and warped multiple times , increasing the computational cost , and 2 ) each output frame is estimated independently conditioned on the input frames , limiting the system 's ability to produce temporally consistent results . in this work , we propose an end-to-end trainable frame-recurrent video super-resolution framework that uses the previously inferred hr estimate to super-resolve the subsequent frame . this naturally encourages temporally consistent results and reduces the computational cost by warping only one image in each step . furthermore , due to its recurrent nature , the proposed method has the ability to assimilate a large number of previous frames without increased computational demands . extensive evaluations and comparisons with previous methods validate the strengths of our approach and demonstrate that the proposed framework is able to significantly outperform the current state of the art .", "topics": ["recurrent neural network", "end-to-end principle"]}
{"title": "one-vs-each approximation to softmax for scalable estimation of probabilities", "abstract": "the softmax representation of probabilities for categorical variables plays a prominent role in modern machine learning with numerous applications in areas such as large scale classification , neural language modeling and recommendation systems . however , softmax estimation is very expensive for large scale inference because of the high cost associated with computing the normalizing constant . here , we introduce an efficient approximation to softmax probabilities which takes the form of a rigorous lower bound on the exact probability . this bound is expressed as a product over pairwise probabilities and it leads to scalable estimation based on stochastic optimization . it allows us to perform doubly stochastic estimation by subsampling both training instances and class labels . we show that the new bound has interesting theoretical properties and we demonstrate its use in classification problems .", "topics": ["approximation", "scalability"]}
{"title": "part-of-speech-tagging using morphological information", "abstract": "this paper presents the results of an experiment to decide the question of authenticity of the supposedly spurious rhesus - a attic tragedy sometimes credited to euripides . the experiment involves use of statistics in order to test whether significant deviations in the distribution of word categories between rhesus and the other works of euripides can or can not be found . to count frequencies of word categories in the corpus , a part-of-speech-tagger for greek has been implemented . some special techniques for reducing the problem of sparse data are used resulting in an accuracy of ca . 96.6 % .", "topics": ["sparse matrix"]}
{"title": "tienet : text-image embedding network for common thorax disease classification and reporting in chest x-rays", "abstract": "chest x-rays are one of the most common radiological examinations in daily clinical routines . reporting thorax diseases using chest x-rays is often an entry-level task for radiologist trainees . yet , reading a chest x-ray image remains a challenging job for learning-oriented machine intelligence , due to ( 1 ) shortage of large-scale machine-learnable medical image datasets , and ( 2 ) lack of techniques that can mimic the high-level reasoning of human radiologists that requires years of knowledge accumulation and professional training . in this paper , we show the clinical free-text radiological reports can be utilized as a priori knowledge for tackling these two key problems . we propose a novel text-image embedding network ( tienet ) for extracting the distinctive image and text representations . multi-level attention models are integrated into an end-to-end trainable cnn-rnn architecture for highlighting the meaningful text words and image regions . we first apply tienet to classify the chest x-rays by using both image features and text embeddings extracted from associated reports . the proposed auto-annotation framework achieves high accuracy ( over 0.9 on average in aucs ) in assigning disease labels for our hand-label evaluation dataset . furthermore , we transform the tienet into a chest x-ray reporting system . it simulates the reporting process and can output disease classification and a preliminary report together . the classification results are significantly improved ( 6 % increase on average in aucs ) compared to the state-of-the-art baseline on an unseen and hand-labeled dataset ( openi ) .", "topics": ["baseline ( configuration management )", "high- and low-level"]}
{"title": "comlex syntax : building a computational lexicon", "abstract": "we describe the design of comlex syntax , a computational lexicon providing detailed syntactic information for approximately 38,000 english headwords . we consider the types of errors which arise in creating such a lexicon , and how such errors can be measured and controlled .", "topics": ["dictionary"]}
{"title": "breast cancer detection using multilevel thresholding", "abstract": "this paper presents an algorithm which aims to assist the radiologist in identifying breast cancer at its earlier stages . it combines several image processing techniques like image negative , thresholding and segmentation techniques for detection of tumor in mammograms . the algorithm is verified by using mammograms from mammographic image analysis society . the results obtained by applying these techniques are described .", "topics": ["image processing"]}
{"title": "a compromise principle in deep monocular depth estimation", "abstract": "monocular depth estimation , which plays a key role in understanding 3d scene geometry , is fundamentally an ill-posed problem . existing methods based on deep convolutional neural networks ( dcnns ) have examined this problem by learning convolutional networks to estimate continuous depth maps from monocular images . however , we find that training a network to predict a high spatial resolution continuous depth map often suffers from poor local solutions . in this paper , we hypothesize that achieving a compromise between spatial and depth resolutions can improve network training . based on this `` compromise principle '' , we propose a regression-classification cascaded network ( rccn ) , which consists of a regression branch predicting a low spatial resolution continuous depth map and a classification branch predicting a high spatial resolution discrete depth map . the two branches form a cascaded structure allowing the classification and regression branches to benefit from each other . by leveraging large-scale raw training datasets and some data augmentation strategies , our network achieves top or state-of-the-art results on the nyu depth v2 , kitti , and make3d benchmarks .", "topics": ["map"]}
{"title": "noise benefits in expectation-maximization algorithms", "abstract": "this dissertation shows that careful injection of noise into sample data can substantially speed up expectation-maximization algorithms . expectation-maximization algorithms are a class of iterative algorithms for extracting maximum likelihood estimates from corrupted or incomplete data . the convergence speed-up is an example of a noise benefit or `` stochastic resonance '' in statistical signal processing . the dissertation presents derivations of sufficient conditions for such noise-benefits and demonstrates the speed-up in some ubiquitous signal-processing algorithms . these algorithms include parameter estimation for mixture models , the $ k $ -means clustering algorithm , the baum-welch algorithm for training hidden markov models , and backpropagation for training feedforward artificial neural networks . this dissertation also analyses the effects of data and model corruption on the more general bayesian inference estimation framework . the main finding is a theorem guaranteeing that uniform approximators for bayesian model functions produce uniform approximators for the posterior pdf via bayes theorem . this result also applies to hierarchical and multidimensional bayesian models .", "topics": ["cluster analysis", "bayesian network"]}
{"title": "federated learning : strategies for improving communication efficiency", "abstract": "federated learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections . we consider learning algorithms for this setting where on each round , each client independently computes an update to the current model based on its local data , and communicates this update to a central server , where the client-side updates are aggregated to compute a new global model . the typical clients in this setting are mobile phones , and communication efficiency is of the utmost importance . in this paper , we propose two ways to reduce the uplink communication costs : structured updates , where we directly learn an update from a restricted space parametrized using a smaller number of variables , e.g . either low-rank or a random mask ; and sketched updates , where we learn a full model update and then compress it using a combination of quantization , random rotations , and subsampling before sending it to the server . experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude .", "topics": ["test set", "computer vision"]}
{"title": "evaluation of machine learning methods to predict coronary artery disease using metabolomic data", "abstract": "metabolomic data can potentially enable accurate , non-invasive and low-cost prediction of coronary artery disease . regression-based analytical approaches however might fail to fully account for interactions between metabolites , rely on a priori selected input features and thus might suffer from poorer accuracy . supervised machine learning methods can potentially be used in order to fully exploit the dimensionality and richness of the data . in this paper , we systematically implement and evaluate a set of supervised learning methods ( l1 regression , random forest classifier ) and compare them to traditional regression-based approaches for disease prediction using metabolomic data .", "topics": ["supervised learning", "interaction"]}
{"title": "bounded rational decision-making in feedforward neural networks", "abstract": "bounded rational decision-makers transform sensory input into motor output under limited computational resources . mathematically , such decision-makers can be modeled as information-theoretic channels with limited transmission rate . here , we apply this formalism for the first time to multilayer feedforward neural networks . we derive synaptic weight update rules for two scenarios , where either each neuron is considered as a bounded rational decision-maker or the network as a whole . in the update rules , bounded rationality translates into information-theoretically motivated types of regularization in weight space . in experiments on the mnist benchmark classification task for handwritten digits , we show that such information-theoretic regularization successfully prevents overfitting across different architectures and attains results that are competitive with other recent techniques like dropout , dropconnect and bayes by backprop , for both ordinary and convolutional neural networks .", "topics": ["neural networks", "matrix regularization"]}
{"title": "overlapping trace norms in multi-view learning", "abstract": "multi-view learning leverages correlations between different sources of data to make predictions in one view based on observations in another view . a popular approach is to assume that , both , the correlations between the views and the view-specific covariances have a low-rank structure , leading to inter-battery factor analysis , a model closely related to canonical correlation analysis . we propose a convex relaxation of this model using structured norm regularization . further , we extend the convex formulation to a robust version by adding an l1-penalized matrix to our estimator , similarly to convex robust pca . we develop and compare scalable algorithms for several convex multi-view models . we show experimentally that the view-specific correlations are improving data imputation performances , as well as labeling accuracy in real-world multi-label prediction tasks .", "topics": ["matrix regularization", "scalability"]}
{"title": "convex sparse matrix factorizations", "abstract": "we present a convex formulation of dictionary learning for sparse signal decomposition . convexity is obtained by replacing the usual explicit upper bound on the dictionary size by a convex rank-reducing term similar to the trace norm . in particular , our formulation introduces an explicit trade-off between size and sparsity of the decomposition of rectangular matrices . using a large set of synthetic examples , we compare the estimation abilities of the convex and non-convex approaches , showing that while the convex formulation has a single local minimum , this may lead in some cases to performance which is inferior to the local minima of the non-convex formulation .", "topics": ["synthetic data", "sparse matrix"]}
{"title": "parallelized tensor train learning of polynomial classifiers", "abstract": "in pattern classification , polynomial classifiers are well-studied methods as they are capable of generating complex decision surfaces . unfortunately , the use of multivariate polynomials is limited to kernels as in support vector machines , because polynomials quickly become impractical for high-dimensional problems . in this paper , we effectively overcome the curse of dimensionality by employing the tensor train format to represent a polynomial classifier . based on the structure of tensor trains , two learning algorithms are proposed which involve solving different optimization problems of low computational complexity . furthermore , we show how both regularization to prevent overfitting and parallelization , which enables the use of large training sets , are incorporated into these methods . both the efficiency and efficacy of our tensor-based polynomial classifier are then demonstrated on the two popular datasets usps and mnist .", "topics": ["statistical classification", "computational complexity theory"]}
{"title": "real-time human motion capture with multiple depth cameras", "abstract": "commonly used human motion capture systems require intrusive attachment of markers that are visually tracked with multiple cameras . in this work we present an efficient and inexpensive solution to markerless motion capture using only a few kinect sensors . unlike the previous work on 3d pose estimation using a single depth camera , we relax constraints on the camera location and do not assume a co-operative user . we apply recent image segmentation techniques to depth images and use curriculum learning to train our system on purely synthetic data . our method accurately localizes body parts without requiring an explicit shape model . the body joint locations are then recovered by combining evidence from multiple views in real-time . we also introduce a dataset of ~6 million synthetic depth frames for pose estimation from multiple cameras and exceed state-of-the-art results on the berkeley mhad dataset .", "topics": ["image segmentation", "synthetic data"]}
{"title": "qualitative order of magnitude energy-flow-based failure modes and effects analysis", "abstract": "this paper presents a structured power and energy-flow-based qualitative modelling approach that is applicable to a variety of system types including electrical and fluid flow . the modelling is split into two parts . power flow is a global phenomenon and is therefore naturally represented and analysed by a network comprised of the relevant structural elements from the components of a system . the power flow analysis is a platform for higher-level behaviour prediction of energy related aspects using local component behaviour models to capture a state-based representation with a global time . the primary application is failure modes and effects analysis ( fmea ) and a form of exaggeration reasoning is used , combined with an order of magnitude representation to derive the worst case failure modes . the novel aspects of the work are an order of magnitude ( om ) qualitative network analyser to represent any power domain and topology , including multiple power sources , a feature that was not required for earlier specialised electrical versions of the approach . secondly , the representation of generalised energy related behaviour as state-based local models is presented as a modelling strategy that can be more vivid and intuitive for a range of topologically complex applications than qualitative equation-based representations.the two-level modelling strategy allows the broad system behaviour coverage of qualitative simulation to be exploited for the fmea task , while limiting the difficulties of qualitative ambiguity explanation that can arise from abstracted numerical models . we have used the method to support an automated fmea system with examples of an aircraft fuel system and domestic a heating system discussed in this paper .", "topics": ["simulation"]}
{"title": "depthsynth : real-time realistic synthetic data generation from cad models for 2.5d recognition", "abstract": "recent progress in computer vision has been dominated by deep neural networks trained over large amounts of labeled data . collecting such datasets is however a tedious , often impossible task ; hence a surge in approaches relying solely on synthetic data for their training . for depth images however , discrepancies with real scans still noticeably affect the end performance . we thus propose an end-to-end framework which simulates the whole mechanism of these devices , generating realistic depth data from 3d models by comprehensively modeling vital factors e.g . sensor noise , material reflectance , surface geometry . not only does our solution cover a wider range of sensors and achieve more realistic results than previous methods , assessed through extended evaluation , but we go further by measuring the impact on the training of neural networks for various recognition tasks ; demonstrating how our pipeline seamlessly integrates such architectures and consistently enhances their performance .", "topics": ["synthetic data", "feature vector"]}
{"title": "game-theoretic design of secure and resilient distributed support vector machines with adversaries", "abstract": "with a large number of sensors and control units in networked systems , distributed support vector machines ( dsvms ) play a fundamental role in scalable and efficient multi-sensor classification and prediction tasks . however , dsvms are vulnerable to adversaries who can modify and generate data to deceive the system to misclassification and misprediction . this work aims to design defense strategies for dsvm learner against a potential adversary . we establish a game-theoretic framework to capture the conflicting interests between the dsvm learner and the attacker . the nash equilibrium of the game allows predicting the outcome of learning algorithms in adversarial environments , and enhancing the resilience of the machine learning through dynamic distributed learning algorithms . we show that the dsvm learner is less vulnerable when he uses a balanced network with fewer nodes and higher degree . we also show that adding more training samples is an efficient defense strategy against an attacker . we present secure and resilient dsvm algorithms with verification method and rejection method , and show their resiliency against adversary with numerical experiments .", "topics": ["support vector machine", "numerical analysis"]}
{"title": "deep supervision with shape concepts for occlusion-aware 3d object parsing", "abstract": "monocular 3d object parsing is highly desirable in various scenarios including occlusion reasoning and holistic scene interpretation . we present a deep convolutional neural network ( cnn ) architecture to localize semantic parts in 2d image and 3d space while inferring their visibility states , given a single rgb image . our key insight is to exploit domain knowledge to regularize the network by deeply supervising its hidden layers , in order to sequentially infer intermediate concepts associated with the final task . to acquire training data in desired quantities with ground truth 3d shape and relevant concepts , we render 3d object cad models to generate large-scale synthetic data and simulate challenging occlusion configurations between objects . we train the network only on synthetic data and demonstrate state-of-the-art performances on real image benchmarks including an extended version of kitti , pascal voc , pascal3d+ and ikea for 2d and 3d keypoint localization and instance segmentation . the empirical results substantiate the utility of our deep supervision scheme by demonstrating effective transfer of knowledge from synthetic data to real images , resulting in less overfitting compared to standard end-to-end training .", "topics": ["test set", "synthetic data"]}
{"title": "a formal approach to modeling the memory of a living organism", "abstract": "we consider a living organism as an observer of the evolution of its environment recording sensory information about the state space x of the environment in real time . sensory information is sampled and then processed on two levels . on the biological level , the organism serves as an evaluation mechanism of the subjective relevance of the incoming data to the observer : the observer assigns excitation values to events in x it could recognize using its sensory equipment . on the algorithmic level , sensory input is used for updating a database , the memory of the observer whose purpose is to serve as a geometric/combinatorial model of x , whose nodes are weighted by the excitation values produced by the evaluation mechanism . these values serve as a guidance system for deciding how the database should transform as observation data mounts . we define a searching problem for the proposed model and discuss the model 's flexibility and its computational efficiency , as well as the possibility of implementing it as a dynamic network of neuron-like units . we show how various easily observable properties of the human memory and thought process can be explained within the framework of this model . these include : reasoning ( with efficiency bounds ) , errors , temporary and permanent loss of information . we are also able to define general learning problems in terms of the new model , such as the language acquisition problem .", "topics": ["relevance"]}
{"title": "cost and actual causation", "abstract": "i propose the purpose our concept of actual causation serves is minimizing various cost in intervention practice . actual causation has three features : nonredundant sufficiency , continuity and abnormality ; these features correspond to the minimization of exploitative cost , exploratory cost and risk cost in intervention practice . incorporating these three features , a definition of actual causation is given . i test the definition in 66 causal cases from actual causation literature and show that this definition 's application fit intuition better than some other causal modelling based definitions .", "topics": ["causality"]}
{"title": "fully convolutional networks to detect clinical dermoscopic features", "abstract": "we use a pretrained fully convolutional neural network to detect clinical dermoscopic features from dermoscopy skin lesion images . we reformulate the superpixel classification task as an image segmentation problem , and extend a neural network architecture originally designed for image classification to detect dermoscopic features . specifically , we interpolate the feature maps from several layers in the network to match the size of the input , concatenate the resized feature maps , and train the network to minimize a smoothed negative f1 score . over the public validation leaderboard of the 2017 isic/isbi lesion dermoscopic feature extraction challenge , our approach achieves 89.3 % auroc , the highest averaged score when compared to the other two entries . results over the private test leaderboard are still to be announced .", "topics": ["image segmentation", "feature extraction"]}
{"title": "the statistics of streaming sparse regression", "abstract": "we present a sparse analogue to stochastic gradient descent that is guaranteed to perform well under similar conditions to the lasso . in the linear regression setup with irrepresentable noise features , our algorithm recovers the support set of the optimal parameter vector with high probability , and achieves a statistically quasi-optimal rate of convergence of op ( k log ( d ) /t ) , where k is the sparsity of the solution , d is the number of features , and t is the number of training examples . meanwhile , our algorithm does not require any more computational resources than stochastic gradient descent . in our experiments , we find that our method substantially out-performs existing streaming algorithms on both real and simulated data .", "topics": ["gradient descent", "simulation"]}
{"title": "qmdp-net : deep learning for planning under partial observability", "abstract": "this paper introduces the qmdp-net , a neural network architecture for planning under partial observability . the qmdp-net combines the strengths of model-free learning and model-based planning . it is a recurrent policy network , but it represents a policy for a parameterized set of tasks by connecting a model with a planning algorithm that solves the model , thus embedding the solution structure of planning in a network learning architecture . the qmdp-net is fully differentiable and allows for end-to-end training . we train a qmdp-net on different tasks so that it can generalize to new ones in the parameterized task set and `` transfer '' to other similar tasks beyond the set . in preliminary experiments , qmdp-net showed strong performance on several robotic tasks in simulation . interestingly , while qmdp-net encodes the qmdp algorithm , it sometimes outperforms the qmdp algorithm in the experiments , as a result of end-to-end learning .", "topics": ["simulation", "end-to-end principle"]}
{"title": "fast and accurate oov decoder on high-level features", "abstract": "this work proposes a novel approach to out-of-vocabulary ( oov ) keyword search ( kws ) task . the proposed approach is based on using high-level features from an automatic speech recognition ( asr ) system , so called phoneme posterior based ( ppb ) features , for decoding . these features are obtained by calculating time-dependent phoneme posterior probabilities from word lattices , followed by their smoothing . for the ppb features we developed a special novel very fast , simple and efficient oov decoder . experimental results are presented on the georgian language from the iarpa babel program , which was the test language in the openkws 2016 evaluation campaign . the results show that in terms of maximum term weighted value ( mtwv ) metric and computational speed , for single asr systems , the proposed approach significantly outperforms the state-of-the-art approach based on using in-vocabulary proxies for oov keywords in the indexed database . the comparison of the two oov kws approaches on the fusion results of the nine different asr systems demonstrates that the proposed oov decoder outperforms the proxy-based approach in terms of mtwv metric given the comparable processing speed . other important advantages of the oov decoder include extremely low memory consumption and simplicity of its implementation and parameter optimization .", "topics": ["high- and low-level", "speech recognition"]}
{"title": "gaussian process landmarking on manifolds", "abstract": "as a means of improving analysis of biological shapes , we propose a greedy algorithm for sampling a riemannian manifold based on the uncertainty of a gaussian process . this is known to produce a near optimal experimental design with the manifold as the domain , and appears to outperform the use of user-placed landmarks in representing geometry of biological objects . we provide an asymptotic analysis for the decay of the maximum conditional variance , which is frequently employed as a greedy criterion for similar variance- or uncertainty-based sequential experimental design strategies , to our knowledge this is the first result of this type for experimental design . the key observation is to link the greedy algorithm with reduced basis methods in the context of model reduction for partial differential equations . we apply the proposed landmarking algorithm to geometric morphometrics , a branch of evolutionary biology focusing on the analysis and comparisons of anatomical shapes , and compare the automatically sampled landmarks with the `` ground truth '' landmarks manually placed by evolutionary anthropologists , the results suggest that gaussian process landmarks perform equally well or better , in terms of both spatial coverage and downstream statistical analysis . we expect this approach will find additional applications in other fields of research .", "topics": ["sampling ( signal processing )", "ground truth"]}
{"title": "an interpretable reasoning network for multi-relation question answering", "abstract": "multi-relation question answering is a challenging task , due to the requirement of elaborated analysis on questions and reasoning over multiple fact triples in knowledge base . in this paper , we present a novel model called interpretable reasoning network that employs an interpretable , hop-by-hop reasoning process for question answering . the model dynamically decides which part of an input question should be analyzed at each hop ; predicts a relation that corresponds to the current parsed results ; utilizes the predicted relation to update the question representation and the state of the reasoning process ; and then drives the next-hop reasoning . experiments show that our model yields state-of-the-art results on two datasets . more interestingly , the model can offer traceable and observable intermediate predictions for reasoning analysis and failure diagnosis .", "topics": ["parsing"]}
{"title": "a flexible framework for hypothesis testing in high-dimensions", "abstract": "hypothesis testing in the linear regression model is a fundamental statistical problem . we consider linear regression in the high-dimensional regime where the number of parameters exceeds the number of samples ( $ p > n $ ) and assume that the high-dimensional parameters vector is $ s_0 $ sparse . we develop a general and flexible $ \\ell_\\infty $ projection statistic for hypothesis testing in this model . our framework encompasses testing whether the parameter lies in a convex cone , testing the signal strength , testing arbitrary functionals of the parameter , and testing adaptive hypothesis . we show that the proposed procedure controls the type i error under the standard assumption of $ s_0 ( \\log p ) /\\sqrt { n } \\to 0 $ , and also analyze the power of the procedure . our numerical experiments confirms our theoretical findings and demonstrate that we control false positive rate ( type i error ) near the nominal level , and have high power .", "topics": ["numerical analysis", "sparse matrix"]}
{"title": "successor features for transfer in reinforcement learning", "abstract": "transfer in reinforcement learning refers to the notion that generalization should occur not only within a task but also across tasks . our focus is on transfer where the reward functions vary across tasks while the environment 's dynamics remain the same . the method we propose rests on two key ideas : `` successor features , '' a value function representation that decouples the dynamics of the environment from the rewards , and `` generalized policy improvement , '' a generalization of dynamic programming 's policy improvement step that considers a set of policies rather than a single one . put together , the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows transfer to take place between tasks without any restriction . the proposed method also provides performance guarantees for the transferred policy even before any learning has taken place . we derive two theorems that set our approach in firm theoretical ground and present experiments that show that it successfully promotes transfer in practice .", "topics": ["reinforcement learning", "simulation"]}
{"title": "naturalizing a programming language via interactive learning", "abstract": "our goal is to create a convenient natural language interface for performing well-specified but complex actions such as analyzing data , manipulating text , and querying databases . however , existing natural language interfaces for such tasks are quite primitive compared to the power one wields with a programming language . to bridge this gap , we start with a core programming language and allow users to `` naturalize '' the core language incrementally by defining alternative , more natural syntax and increasingly complex concepts in terms of compositions of simpler ones . in a voxel world , we show that a community of users can simultaneously teach a common system a diverse language and use it to build hundreds of complex voxel structures . over the course of three days , these users went from using only the core language to using the naturalized language in 85.9\\ % of the last 10k utterances .", "topics": ["natural language", "database"]}
{"title": "cutset sampling for bayesian networks", "abstract": "the paper presents a new sampling methodology for bayesian networks that samples only a subset of variables and applies exact inference to the rest . cutset sampling is a network structure-exploiting application of the rao-blackwellisation principle to sampling in bayesian networks . it improves convergence by exploiting memory-based inference algorithms . it can also be viewed as an anytime approximation of the exact cutset-conditioning algorithm developed by pearl . cutset sampling can be implemented efficiently when the sampled variables constitute a loop-cutset of the bayesian network and , more generally , when the induced width of the networks graph conditioned on the observed sampled variables is bounded by a constant w. we demonstrate empirically the benefit of this scheme on a range of benchmarks .", "topics": ["sampling ( signal processing )", "bayesian network"]}
{"title": "linear classification of data with support vector machines and generalized support vector machines", "abstract": "in this paper , we study the support vector machine and introduced the notion of generalized support vector machine for classification of data . we show that the problem of generalized support vector machine is equivalent to the problem of generalized variational inequality and establish various results for the existence of solutions . moreover , we provide various examples to support our results .", "topics": ["support vector machine", "calculus of variations"]}
{"title": "exposing computer generated images by using deep convolutional neural networks", "abstract": "the recent computer graphics developments have upraised the quality of the generated digital content , astonishing the most skeptical viewer . games and movies have taken advantage of this fact but , at the same time , these advances have brought serious negative impacts like the ones yielded by fakeimages produced with malicious intents . digital artists can compose artificial images capable of deceiving the great majority of people , turning this into a very dangerous weapon in a timespan currently know as fake news/post-truth '' era . in this work , we propose a new approach for dealing with the problem of detecting computer generated images , through the application of deep convolutional networks and transfer learning techniques . we start from residual networks and develop different models adapted to the binary problem of identifying if an image was or not computer generated . differently from the current state-of-the-art approaches , we do n't rely on hand-crafted features , but provide to the model the raw pixel information , achieving the same 0.97 of state-of-the-art methods with two main advantages : our methods show more stable results ( depicted by lower variance ) and eliminate the laborious and manual step of specialized features extraction and selection .", "topics": ["pixel"]}
{"title": "real-time 3d reconstruction on construction site using visual slam and uav", "abstract": "3d reconstruction can be used as a platform to monitor the performance of activities on construction site , such as construction progress monitoring , structure inspection and post-disaster rescue . comparing to other sensors , rgb image has the advantages of low-cost , texture rich and easy to implement that has been used as the primary method for 3d reconstruction in construction industry . however , the image-based 3d reconstruction always requires extended time to acquire and/or to process the image data , which limits its application on time critical projects . recent progress in visual simultaneous localization and mapping ( slam ) make it possible to reconstruct a 3d map of construction site in real-time . integrated with unmanned aerial vehicle ( uav ) , the obstacles areas that are inaccessible for the ground equipment can also be sensed . despite these advantages of visual slam and uav , until now , such technique has not been fully investigated on construction site . therefore , the objective of this research is to present a pilot study of using visual slam and uav for real-time construction site reconstruction . the system architecture and the experimental setup are introduced , and the preliminary results and the potential applications using visual slam and uav on construction site are discussed .", "topics": ["image processing", "feature extraction"]}
{"title": "three approaches to probability model selection", "abstract": "this paper compares three approaches to the problem of selecting among probability models to fit data ( 1 ) use of statistical criteria such as akaike 's information criterion and schwarz 's `` bayesian information criterion , '' ( 2 ) maximization of the posterior probability of the model , and ( 3 ) maximization of an effectiveness ratio ? trading off accuracy and computational cost . the unifying characteristic of the approaches is that all can be viewed as maximizing a penalized likelihood function . the second approach with suitable prior distributions has been shown to reduce to the first . this paper shows that the third approach reduces to the second for a particular form of the effectiveness ratio , and illustrates all three approaches with the problem of selecting the number of components in a mixture of gaussian distributions . unlike the first two approaches , the third can be used even when the candidate models are chosen for computational efficiency , without regard to physical interpretation , so that the likelihood and the prior distribution over models can not be interpreted literally . as the most general and computationally oriented of the approaches , it is especially useful for artificial intelligence applications .", "topics": ["artificial intelligence", "eisenstein 's criterion"]}
{"title": "handwritten digit recognition with a committee of deep neural nets on gpus", "abstract": "the competitive mnist handwritten digit recognition benchmark has a long history of broken records since 1998 . the most recent substantial improvement by others dates back 7 years ( error rate 0.4 % ) . recently we were able to significantly improve this result , using graphics cards to greatly speed up training of simple but deep mlps , which achieved 0.35 % , outperforming all the previous more complex methods . here we report another substantial improvement : 0.31 % obtained using a committee of mlps .", "topics": ["mnist database"]}
{"title": "learning hash functions using column generation", "abstract": "fast nearest neighbor searching is becoming an increasingly important tool in solving many large-scale problems . recently a number of approaches to learning data-dependent hash functions have been developed . in this work , we propose a column generation based method for learning data-dependent hash functions on the basis of proximity comparison information . given a set of triplets that encode the pairwise proximity comparison information , our method learns hash functions that preserve the relative comparison relationships in the data as well as possible within the large-margin learning framework . the learning procedure is implemented using column generation and hence is named cghash . at each iteration of the column generation procedure , the best hash function is selected . unlike most other hashing methods , our method generalizes to new data points naturally ; and has a training objective which is convex , thus ensuring that the global optimum can be identified . experiments demonstrate that the proposed method learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on a few benchmark datasets .", "topics": ["iteration"]}
{"title": "a monte carlo algorithm for universally optimal bayesian sequence prediction and planning", "abstract": "the aim of this work is to address the question of whether we can in principle design rational decision-making agents or artificial intelligences embedded in computable physics such that their decisions are optimal in reasonable mathematical senses . recent developments in rare event probability estimation , recursive bayesian inference , neural networks , and probabilistic planning are sufficient to explicitly approximate reinforcement learners of the aixi style with non-trivial model classes ( here , the class of resource-bounded turing machines ) . consideration of the effects of resource limitations in a concrete implementation leads to insights about possible architectures for learning systems using optimal decision makers as components .", "topics": ["approximation algorithm", "artificial intelligence"]}
{"title": "an efficient sufficient dimension reduction method for identifying genetic variants of clinical significance", "abstract": "fast and cheaper next generation sequencing technologies will generate unprecedentedly massive and highly-dimensional genomic and epigenomic variation data . in the near future , a routine part of medical record will include the sequenced genomes . a fundamental question is how to efficiently extract genomic and epigenomic variants of clinical utility which will provide information for optimal wellness and interference strategies . traditional paradigm for identifying variants of clinical validity is to test association of the variants . however , significantly associated genetic variants may or may not be usefulness for diagnosis and prognosis of diseases . alternative to association studies for finding genetic variants of predictive utility is to systematically search variants that contain sufficient information for phenotype prediction . to achieve this , we introduce concepts of sufficient dimension reduction and coordinate hypothesis which project the original high dimensional data to very low dimensional space while preserving all information on response phenotypes . we then formulate clinically significant genetic variant discovery problem into sparse sdr problem and develop algorithms that can select significant genetic variants from up to or even ten millions of predictors with the aid of dividing sdr for whole genome into a number of subsdr problems defined for genomic regions . the sparse sdr is in turn formulated as sparse optimal scoring problem , but with penalty which can remove row vectors from the basis matrix . to speed up computation , we develop the modified alternating direction method for multipliers to solve the sparse optimal scoring problem which can easily be implemented in parallel . to illustrate its application , the proposed method is applied to simulation data and the nhlbi 's exome sequencing project dataset", "topics": ["simulation", "sparse matrix"]}
{"title": "a tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing", "abstract": "dual decomposition , and more generally lagrangian relaxation , is a classical method for combinatorial optimization ; it has recently been applied to several inference problems in natural language processing ( nlp ) . this tutorial gives an overview of the technique . we describe example algorithms , describe formal guarantees for the method , and describe practical issues in implementing the algorithms . while our examples are predominantly drawn from the nlp literature , the material should be of general relevance to inference problems in machine learning . a central theme of this tutorial is that lagrangian relaxation is naturally applied in conjunction with a broad class of combinatorial algorithms , allowing inference in models that go significantly beyond previous work on lagrangian relaxation for inference in graphical models .", "topics": ["graphical model", "natural language processing"]}
{"title": "adaptive load balancing : a study in multi-agent learning", "abstract": "we study the process of multi-agent reinforcement learning in the context of load balancing in a distributed system , without use of either central coordination or explicit communication . we first define a precise framework in which to study adaptive load balancing , important features of which are its stochastic nature and the purely local information available to individual agents . given this framework , we show illuminating results on the interplay between basic adaptive behavior parameters and their effect on system efficiency . we then investigate the properties of adaptive load balancing in heterogeneous populations , and address the issue of exploration vs . exploitation in that context . finally , we show that naive use of communication may not improve , and might even harm system efficiency .", "topics": ["reinforcement learning"]}
{"title": "test set selection using active information acquisition for predictive models", "abstract": "in this paper , we consider active information acquisition when the prediction model is meant to be applied on a targeted subset of the population . the goal is to label a pre-specified fraction of customers in the target or test set by iteratively querying for information from the non-target or training set . the number of queries is limited by an overall budget . arising in the context of two rather disparate applications- banking and medical diagnosis , we pose the active information acquisition problem as a constrained optimization problem . we propose two greedy iterative algorithms for solving the above problem . we conduct experiments with synthetic data and compare results of our proposed algorithms with few other baseline approaches . the experimental results show that our proposed approaches perform better than the baseline schemes .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "variational autoencoders for feature detection of magnetic resonance imaging data", "abstract": "independent component analysis ( ica ) , as an approach to the blind source-separation ( bss ) problem , has become the de-facto standard in many medical imaging settings . despite successes and a large ongoing research effort , the limitation of ica to square linear transformations have not been overcome , so that general infomax is still far from being realized . as an alternative , we present feature analysis in medical imaging as a problem solved by helmholtz machines , which include dimensionality reduction and reconstruction of the raw data under the same objective , and which recently have overcome major difficulties in inference and learning with deep and nonlinear configurations . we demonstrate one approach to training helmholtz machines , variational auto-encoders ( vae ) , as a viable approach toward feature extraction with magnetic resonance imaging ( mri ) data .", "topics": ["calculus of variations", "feature extraction"]}
{"title": "interactive tools and tasks for the hebrew bible", "abstract": "this contribution to a special issue on `` computer-aided processing of intertextuality '' in ancient texts will illustrate how using digital tools to interact with the hebrew bible offers new promising perspectives for visualizing the texts and for performing tasks in education and research . this contribution explores how the corpus of the hebrew bible created and maintained by the eep talstra centre for bible and computer can support new methods for modern knowledge workers within the field of digital humanities and theology be applied to ancient texts , and how this can be envisioned as a new field of digital intertextuality . the article first describes how the corpus was used to develop the bible online learner as a persuasive technology to enhance language learning with , in , and around a database that acts as the engine driving interactive tasks for learners . intertextuality in this case is a matter of active exploration and ongoing practice . furthermore , interactive corpus-technology has an important bearing on the task of textual criticism as a specialized area of research that depends increasingly on the availability of digital resources . commercial solutions developed by software companies like logos and accordance offer a market-based intertextuality defined by the production of advanced digital resources for scholars and students as useful alternatives to often inaccessible and expensive printed versions . it is reasonable to expect that in the future interactive corpus technology will allow scholars to do innovative academic tasks in textual criticism and interpretation . we have already seen the emergence of promising tools for text categorization , analysis of translation shifts , and interpretation . broadly speaking , interactive tools and tasks within the three areas of language learning , textual criticism , and biblical studies illustrate a new kind of intertextuality emerging within digital humanities .", "topics": ["text corpus", "database"]}
{"title": "logics for the relational syllogistic", "abstract": "the aristotelian syllogistic can not account for the validity of many inferences involving relational facts . in this paper , we investigate the prospects for providing a relational syllogistic . we identify several fragments based on ( a ) whether negation is permitted on all nouns , including those in the subject of a sentence ; and ( b ) whether the subject noun phrase may contain a relative clause . the logics we present are extensions of the classical syllogistic , and we pay special attention to the question of whether reductio ad absurdum is needed . thus our main goal is to derive results on the existence ( or non-existence ) of syllogistic proof systems for relational fragments . we also determine the computational complexity of all our fragments .", "topics": ["computational complexity theory"]}
{"title": "compact model representation for 3d reconstruction", "abstract": "3d reconstruction from 2d images is a central problem in computer vision . recent works have been focusing on reconstruction directly from a single image . it is well known however that only one image can not provide enough information for such a reconstruction . a prior knowledge that has been entertained are 3d cad models due to its online ubiquity . a fundamental question is how to compactly represent millions of cad models while allowing generalization to new unseen objects with fine-scaled geometry . we introduce an approach to compactly represent a 3d mesh . our method first selects a 3d model from a graph structure by using a novel free-form deformation ffd 3d-2d registration , and then the selected 3d model is refined to best fit the image silhouette . we perform a comprehensive quantitative and qualitative analysis that demonstrates impressive dense and realistic 3d reconstruction from single images .", "topics": ["computer vision"]}
{"title": "scene-centric joint parsing of cross-view videos", "abstract": "cross-view video understanding is an important yet under-explored area in computer vision . in this paper , we introduce a joint parsing framework that integrates view-centric proposals into scene-centric parse graphs that represent a coherent scene-centric understanding of cross-view scenes . our key observations are that overlapping fields of views embed rich appearance and geometry correlations and that knowledge fragments corresponding to individual vision tasks are governed by consistency constraints available in commonsense knowledge . the proposed joint parsing framework represents such correlations and constraints explicitly and generates semantic scene-centric parse graphs . quantitative experiments show that scene-centric predictions in the parse graph outperform view-centric predictions .", "topics": ["computer vision", "parsing"]}
{"title": "parallel computation is ess", "abstract": "there are enormous amount of examples of computation in nature , exemplified across multiple species in biology . one crucial aim for these computations across all life forms their ability to learn and thereby increase the chance of their survival . in the current paper a formal definition of autonomous learning is proposed . from that definition we establish a turing machine model for learning , where rule tables can be added or deleted , but can not be modified . sequential and parallel implementations of this model are discussed . it is found that for general purpose learning based on this model , the implementations capable of parallel execution would be evolutionarily stable . this is proposed to be of the reasons why in nature parallelism in computation is found in abundance .", "topics": ["computation", "autonomous car"]}
{"title": "monitoring teams by overhearing : a multi-agent plan-recognition approach", "abstract": "recent years are seeing an increasing need for on-line monitoring of teams of cooperating agents , e.g . , for visualization , or performance tracking . however , in monitoring deployed teams , we often can not rely on the agents to always communicate their state to the monitoring system . this paper presents a non-intrusive approach to monitoring by 'overhearing ' , where the monitored team 's state is inferred ( via plan-recognition ) from team-members ' routine communications , exchanged as part of their coordinated task execution , and observed ( overheard ) by the monitoring system . key challenges in this approach include the demanding run-time requirements of monitoring , the scarceness of observations ( increasing monitoring uncertainty ) , and the need to scale-up monitoring to address potentially large teams . to address these , we present a set of complementary novel techniques , exploiting knowledge of the social structures and procedures in the monitored team : ( i ) an efficient probabilistic plan-recognition algorithm , well-suited for processing communications as observations ; ( ii ) an approach to exploiting knowledge of the team 's social behavior to predict future observations during execution ( reducing monitoring uncertainty ) ; and ( iii ) monitoring algorithms that trade expressivity for scalability , representing only certain useful monitoring hypotheses , but allowing for any number of agents and their different activities to be represented in a single coherent entity . we present an empirical evaluation of these techniques , in combination and apart , in monitoring a deployed team of agents , running on machines physically distributed across the country , and engaged in complex , dynamic task execution . we also compare the performance of these techniques to human expert and novice monitors , and show that the techniques presented are capable of monitoring at human-expert levels , despite the difficulty of the task .", "topics": ["scalability"]}
{"title": "efficient deep aesthetic image classification using connected local and global features", "abstract": "in this paper we investigate the aesthetic image classification problem , also known as automatically classifying an image into low or high aesthetic quality , which is quite a challenging problem . considering both the local and global information of images is quite important for image aesthetic quality assessment . currently , a powerful inception module is proposed which shows very high performance in object classification . we have the observation that the inception module has the ability of considering both the local and global features in nature . thus , in this paper , we propose a novel dcnn structure codenamed ilgnet for image aesthetics classification , which introduces the inception module and connects intermediate local layers to the global layer for the output . in addition , the ilgnet is derived from part of the googlenet . thus , we can easily use a pre-trained image classification googlelenet model on the imagenet dataset and fine tune our connected local and global layer on the large scale aesthetics assessment ava dataset . the experimental results show that the proposed ilgnet outperforms the state of the art results in image aesthetics assessment in the ava benchmark . the time cost of both training and test of the ilgnet are significantly less than those of full googlenet with only a little reduction of the classification accuracy . our ilgnet can achieve similar classification accuracy as that of 2/3 googlenet , whose computational cost is nearly twice of ours . this makes the aesthetic assessment model more easily to be integrated into mobile and embedded systems .", "topics": ["computer vision"]}
{"title": "scalable multi-class gaussian process classification using expectation propagation", "abstract": "this paper describes an expectation propagation ( ep ) method for multi-class classification with gaussian processes that scales well to very large datasets . in such a method the estimate of the log-marginal-likelihood involves a sum across the data instances . this enables efficient training using stochastic gradients and mini-batches . when this type of training is used , the computational cost does not depend on the number of data instances $ n $ . furthermore , extra assumptions in the approximate inference process make the memory cost independent of $ n $ . the consequence is that the proposed ep method can be used on datasets with millions of instances . we compare empirically this method with alternative approaches that approximate the required computations using variational inference . the results show that it performs similar or even better than these techniques , which sometimes give significantly worse predictive distributions in terms of the test log-likelihood . besides this , the training process of the proposed approach also seems to converge in a smaller number of iterations .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "cp-nets and nash equilibria", "abstract": "we relate here two formalisms that are used for different purposes in reasoning about multi-agent systems . one of them are strategic games that are used to capture the idea that agents interact with each other while pursuing their own interest . the other are cp-nets that were introduced to express qualitative and conditional preferences of the users and which aim at facilitating the process of preference elicitation . to relate these two formalisms we introduce a natural , qualitative , extension of the notion of a strategic game . we show then that the optimal outcomes of a cp-net are exactly the nash equilibria of an appropriately defined strategic game in the above sense . this allows us to use the techniques of game theory to search for optimal outcomes of cp-nets and vice-versa , to use techniques developed for cp-nets to search for nash equilibria of the considered games .", "topics": ["artificial intelligence"]}
{"title": "kernel selection using multiple kernel learning and domain adaptation in reproducing kernel hilbert space , for face recognition under surveillance scenario", "abstract": "face recognition ( fr ) has been the interest to several researchers over the past few decades due to its passive nature of biometric authentication . despite high accuracy achieved by face recognition algorithms under controlled conditions , achieving the same performance for face images obtained in surveillance scenarios , is a major hurdle . some attempts have been made to super-resolve the low-resolution face images and improve the contrast , without considerable degree of success . the proposed technique in this paper tries to cope with the very low resolution and low contrast face images obtained from surveillance cameras , for fr under surveillance conditions . for support vector machine classification , the selection of appropriate kernel has been a widely discussed issue in the research community . in this paper , we propose a novel kernel selection technique termed as mfkl ( multi-feature kernel learning ) to obtain the best feature-kernel pairing . our proposed technique employs a effective kernel selection by multiple kernel learning ( mkl ) method , to choose the optimal kernel to be used along with unsupervised domain adaptation method in the reproducing kernel hilbert space ( rkhs ) , for a solution to the problem . rigorous experimentation has been performed on three real-world surveillance face datasets : fr\\_surv , scface and chokepoint . results have been shown using rank-1 recognition accuracy , roc and cmc measures . our proposed method outperforms all other recent state-of-the-art techniques by a considerable margin .", "topics": ["kernel ( operating system )", "support vector machine"]}
{"title": "sooner than expected : hitting the wall of complexity in evolution", "abstract": "in evolutionary robotics an encoding of the control software , which maps sensor data ( input ) to motor control values ( output ) , is shaped by stochastic optimization methods to complete a predefined task . this approach is assumed to be beneficial compared to standard methods of controller design in those cases where no a-priori model is available that could help to optimize performance . also for robots that have to operate in unpredictable environments , an evolutionary robotics approach is favorable . we demonstrate here that such a model-free approach is not a free lunch , as already simple tasks can represent unsolvable barriers for fully open-ended uninformed evolutionary computation techniques . we propose here the 'wankelmut ' task as an objective for an evolutionary approach that starts from scratch without pre-shaped controller software or any other informed approach that would force the behavior to be evolved in a desired way . our focal claim is that 'wankelmut ' represents the simplest set of problems that makes plain-vanilla evolutionary computation fail . we demonstrate this by a series of simple standard evolutionary approaches using different fitness functions and standard artificial neural networks as well as continuous-time recurrent neural networks . all our tested approaches failed . we claim that any other evolutionary approach will also fail that does per-se not favor or enforce modularity and does not freeze or protect already evolved functionalities . thus we propose a hard-to-pass benchmark and make a strong statement for self-complexifying and generative approaches in evolutionary computation . we anticipate that defining such a 'simplest task to fail ' is a valuable benchmark for promoting future development in the field of artificial intelligence , evolutionary robotics and artificial life .", "topics": ["recurrent neural network", "map"]}
{"title": "convexified modularity maximization for degree-corrected stochastic block models", "abstract": "the stochastic block model ( sbm ) is a popular framework for studying community detection in networks . this model is limited by the assumption that all nodes in the same community are statistically equivalent and have equal expected degrees . the degree-corrected stochastic block model ( dcsbm ) is a natural extension of sbm that allows for degree heterogeneity within communities . this paper proposes a convexified modularity maximization approach for estimating the hidden communities under dcsbm . our approach is based on a convex programming relaxation of the classical ( generalized ) modularity maximization formulation , followed by a novel doubly-weighted $ \\ell_1 $ -norm $ k $ -median procedure . we establish non-asymptotic theoretical guarantees for both approximate clustering and perfect clustering . our approximate clustering results are insensitive to the minimum degree , and hold even in sparse regime with bounded average degrees . in the special case of sbm , these theoretical results match the best-known performance guarantees of computationally feasible algorithms . numerically , we provide an efficient implementation of our algorithm , which is applied to both synthetic and real-world networks . experiment results show that our method enjoys competitive performance compared to the state of the art in the literature .", "topics": ["approximation algorithm", "cluster analysis"]}
{"title": "do artificial reinforcement-learning agents matter morally ?", "abstract": "artificial reinforcement learning ( rl ) is a widely used technique in artificial intelligence that provides a general method for training agents to perform a wide variety of behaviours . rl as used in computer science has striking parallels to reward and punishment learning in animal and human brains . i argue that present-day artificial rl agents have a very small but nonzero degree of ethical importance . this is particularly plausible for views according to which sentience comes in degrees based on the abilities and complexities of minds , but even binary views on consciousness should assign nonzero probability to rl programs having morally relevant experiences . while rl programs are not a top ethical priority today , they may become more significant in the coming decades as rl is increasingly applied to industry , robotics , video games , and other areas . i encourage scientists , philosophers , and citizens to begin a conversation about our ethical duties to reduce the harm that we inflict on powerless , voiceless rl agents .", "topics": ["reinforcement learning", "artificial intelligence"]}
{"title": "discovering the hidden structure of complex dynamic systems", "abstract": "dynamic bayesian networks provide a compact and natural representation for complex dynamic systems . however , in many cases , there is no expert available from whom a model can be elicited . learning provides an alternative approach for constructing models of dynamic systems . in this paper , we address some of the crucial computational aspects of learning the structure of dynamic systems , particularly those where some relevant variables are partially observed or even entirely unknown . our approach is based on the structural expectation maximization ( sem ) algorithm . the main computational cost of the sem algorithm is the gathering of expected sufficient statistics . we propose a novel approximation scheme that allows these sufficient statistics to be computed efficiently . we also investigate the fundamental problem of discovering the existence of hidden variables without exhaustive and expensive search . our approach is based on the observation that , in dynamic systems , ignoring a hidden variable typically results in a violation of the markov property . thus , our algorithm searches for such violations in the data , and introduces hidden variables to explain them . we provide empirical results showing that the algorithm is able to learn the dynamics of complex systems in a computationally tractable way .", "topics": ["bayesian network"]}
{"title": "all-but-the-top : simple and effective postprocessing for word representations", "abstract": "real-valued word representations have transformed nlp applications ; popular examples are word2vec and glove , recognized for their ability to capture linguistic regularities . in this paper , we demonstrate a { \\em very simple } , and yet counter-intuitive , postprocessing technique -- eliminate the common mean vector and a few top dominating directions from the word vectors -- that renders off-the-shelf representations { \\em even stronger } . the postprocessing is empirically validated on a variety of lexical-level intrinsic tasks ( word similarity , concept categorization , word analogy ) and sentence-level tasks ( semantic textural similarity and { text classification } ) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages ; in each case , the processed representations are consistently better than the original ones .", "topics": ["natural language processing"]}
{"title": "a comparison of directional distances for hand pose estimation", "abstract": "benchmarking methods for 3d hand tracking is still an open problem due to the difficulty of acquiring ground truth data . we introduce a new dataset and benchmarking protocol that is insensitive to the accumulative error of other protocols . to this end , we create testing frame pairs of increasing difficulty and measure the pose estimation error separately for each of them . this approach gives new insights and allows to accurately study the performance of each feature or method without employing a full tracking pipeline . following this protocol , we evaluate various directional distances in the context of silhouette-based 3d hand tracking , expressed as special cases of a generalized chamfer distance form . an appropriate parameter setup is proposed for each of them , and a comparative study reveals the best performing method in this context .", "topics": ["ground truth"]}
{"title": "determinantal point processes for coresets", "abstract": "when one is faced with a dataset too large to be used all at once , an obvious solution is to retain only part of it . in practice this takes a wide variety of different forms , but among them `` coresets '' are especially appealing . a coreset is a ( small ) weighted sample of the original data that comes with a guarantee : that a cost function can be evaluated on the smaller set instead of the larger one , with low relative error . for some classes of problems , and via a careful choice of sampling distribution , iid random sampling has turned to be one of the most successful methods to build coresets efficiently . however , independent samples are sometimes overly redundant , and one could hope that enforcing diversity would lead to better performance . the difficulty lies in proving coreset properties in non-iid samples . we show that the coreset property holds for samples formed with determinantal point processes ( dpp ) . dpps are interesting because they are a rare example of repulsive point processes with tractable theoretical properties , enabling us to construct general coreset theorems . we apply our results to the $ k $ -means problem , and give empirical evidence of the superior performance of dpp samples over state of the art methods .", "topics": ["sampling ( signal processing )", "loss function"]}
{"title": "human body orientation estimation using convolutional neural network", "abstract": "personal robots are expected to interact with the user by recognizing the user 's face . however , in most of the service robot applications , the user needs to move himself/herself to allow the robot to see him/her face to face . to overcome such limitations , a method for estimating human body orientation is required . previous studies used various components such as feature extractors and classification models to classify the orientation which resulted in low performance . for a more robust and accurate approach , we propose the light weight convolutional neural networks , an end to end system , for estimating human body orientation . our body orientation estimation model achieved 81.58 % and 94 % accuracy with the benchmark dataset and our own dataset respectively . the proposed method can be used in a wide range of service robot applications which depend on the ability to estimate human body orientation . to show its usefulness in service robot applications , we designed a simple robot application which allows the robot to move towards the user 's frontal plane . with this , we demonstrated an improved face detection rate .", "topics": ["feature extraction", "robot"]}
{"title": "fast convolutional nets with fbfft : a gpu performance evaluation", "abstract": "we examine the performance profile of convolutional neural network training on the current generation of nvidia graphics processing units . we introduce two new fast fourier transform convolution implementations : one based on nvidia 's cufft library , and another based on a facebook authored fft implementation , fbfft , that provides significant speedups over cufft ( over 1.5x ) for whole cnns . both of these convolution implementations are available in open source , and are faster than nvidia 's cudnn implementation for many common convolutional layers ( up to 23.5x for some synthetic kernel configurations ) . we discuss different performance regimes of convolutions , comparing areas where straightforward time domain convolutions outperform fourier frequency domain convolutions . details on algorithmic applications of nvidia gpu hardware specifics in the implementation of fbfft are also provided .", "topics": ["synthetic data", "convolution"]}
{"title": "learning undirected graphical models with structure penalty", "abstract": "in undirected graphical models , learning the graph structure and learning the functions that relate the predictive variables ( features ) to the responses given the structure are two topics that have been widely investigated in machine learning and statistics . learning graphical models in two stages will have problems because graph structure may change after considering the features . the main contribution of this paper is the proposed method that learns the graph structure and functions on the graph at the same time . general graphical models with binary outcomes conditioned on predictive variables are proved to be equivalent to multivariate bernoulli model . the reparameterization of the potential functions in graphical model by conditional log odds ratios in multivariate bernoulli model offers advantage in the representation of the conditional independence structure in the model . additionally , we impose a structure penalty on groups of conditional log odds ratios to learn the graph structure . these groups of functions are designed with overlaps to enforce hierarchical function selection . in this way , we are able to shrink higher order interactions to obtain a sparse graph structure . simulation studies show that the method is able to recover the graph structure . the analysis of county data from census bureau gives interesting relations between unemployment rate , crime and others discovered by the model .", "topics": ["graphical model", "interaction"]}
{"title": "a survey of paraphrasing and textual entailment methods", "abstract": "paraphrasing methods recognize , generate , or extract phrases , sentences , or longer natural language expressions that convey almost the same information . textual entailment methods , on the other hand , recognize , generate , or extract pairs of natural language expressions , such that a human who reads ( and trusts ) the first element of a pair would most likely infer that the other element is also true . paraphrasing can be seen as bidirectional textual entailment and methods from the two areas are often similar . both kinds of methods are useful , at least in principle , in a wide range of natural language processing applications , including question answering , summarization , text generation , and machine translation . we summarize key ideas from the two areas by considering in turn recognition , generation , and extraction methods , also pointing to prominent articles and resources .", "topics": ["natural language processing", "machine translation"]}
{"title": "multi-step stochastic admm in high dimensions : applications to sparse optimization and noisy matrix decomposition", "abstract": "we propose an efficient admm method with guarantees for high-dimensional problems . we provide explicit bounds for the sparse optimization problem and the noisy matrix decomposition problem . for sparse optimization , we establish that the modified admm method has an optimal convergence rate of $ \\mathcal { o } ( s\\log d/t ) $ , where $ s $ is the sparsity level , $ d $ is the data dimension and $ t $ is the number of steps . this matches with the minimax lower bounds for sparse estimation . for matrix decomposition into sparse and low rank components , we provide the first guarantees for any online method , and prove a convergence rate of $ \\tilde { \\mathcal { o } } ( ( s+r ) \\beta^2 ( p ) /t ) + \\mathcal { o } ( 1/p ) $ for a $ p\\times p $ matrix , where $ s $ is the sparsity level , $ r $ is the rank and $ \\theta ( \\sqrt { p } ) \\leq \\beta ( p ) \\leq \\theta ( p ) $ . our guarantees match the minimax lower bound with respect to $ s , r $ and $ t $ . in addition , we match the minimax lower bound with respect to the matrix dimension $ p $ , i.e . $ \\beta ( p ) =\\theta ( \\sqrt { p } ) $ , for many important statistical models including the independent noise model , the linear bayesian network and the latent gaussian graphical model under some conditions . our admm method is based on epoch-based annealing and consists of inexpensive steps which involve projections on to simple norm balls . experiments show that for both sparse optimization and matrix decomposition problems , our algorithm outperforms the state-of-the-art methods . in particular , we reach higher accuracy with same time complexity .", "topics": ["regret ( decision theory )", "graphical model"]}
{"title": "training deep nets with sublinear memory cost", "abstract": "we propose a systematic approach to reduce the memory consumption of deep neural network training . specifically , we design an algorithm that costs o ( sqrt ( n ) ) memory to train a n layer network , with only the computational cost of an extra forward pass per mini-batch . as many of the state-of-the-art models hit the upper bound of the gpu memory , our algorithm allows deeper and more complex models to be explored , and helps advance the innovations in deep learning research . we focus on reducing the memory cost to store the intermediate feature maps and gradients during training . computation graph analysis is used for automatic in-place operation and memory sharing optimizations . we show that it is possible to trade computation for memory - giving a more memory efficient training algorithm with a little extra computation cost . in the extreme case , our analysis also shows that the memory consumption can be reduced to o ( log n ) with as little as o ( n log n ) extra cost for forward computation . our experiments show that we can reduce the memory cost of a 1,000-layer deep residual network from 48g to 7g with only 30 percent additional running time cost on imagenet problems . similarly , significant memory cost reduction is observed in training complex recurrent neural networks on very long sequences .", "topics": ["time complexity", "recurrent neural network"]}
{"title": "soft rule ensembles for statistical learning", "abstract": "in this article supervised learning problems are solved using soft rule ensembles . we first review the importance sampling learning ensembles ( isle ) approach that is useful for generating hard rules . the soft rules are then obtained with logistic regression from the corresponding hard rules . in order to deal with the perfect separation problem related to the logistic regression , firth 's bias corrected likelihood is used . various examples and simulation results show that soft rule ensembles can improve predictive performance over hard rule ensembles .", "topics": ["sampling ( signal processing )", "supervised learning"]}
{"title": "cnn image retrieval learns from bow : unsupervised fine-tuning with hard examples", "abstract": "convolutional neural networks ( cnns ) achieve state-of-the-art performance in many computer vision tasks . however , this achievement is preceded by extreme manual annotation in order to perform either training from scratch or fine-tuning for the target task . in this work , we propose to fine-tune cnn for image retrieval from a large collection of unordered images in a fully automated manner . we employ state-of-the-art retrieval and structure-from-motion ( sfm ) methods to obtain 3d models , which are used to guide the selection of the training data for cnn fine-tuning . we show that both hard positive and hard negative examples enhance the final performance in particular object retrieval with compact codes .", "topics": ["test set", "computer vision"]}
{"title": "on redundant topological constraints", "abstract": "the region connection calculus ( rcc ) is a well-known calculus for representing part-whole and topological relations . it plays an important role in qualitative spatial reasoning , geographical information science , and ontology . the computational complexity of reasoning with rcc5 and rcc8 ( two fragments of rcc ) as well as other qualitative spatial/temporal calculi has been investigated in depth in the literature . most of these works focus on the consistency of qualitative constraint networks . in this paper , we consider the important problem of redundant qualitative constraints . for a set $ \\gamma $ of qualitative constraints , we say a constraint $ ( x r y ) $ in $ \\gamma $ is redundant if it is entailed by the rest of $ \\gamma $ . a prime subnetwork of $ \\gamma $ is a subset of $ \\gamma $ which contains no redundant constraints and has the same solution set as $ \\gamma $ . it is natural to ask how to compute such a prime subnetwork , and when it is unique . in this paper , we show that this problem is in general intractable , but becomes tractable if $ \\gamma $ is over a tractable subalgebra $ \\mathcal { s } $ of a qualitative calculus . furthermore , if $ \\mathcal { s } $ is a subalgebra of rcc5 or rcc8 in which weak composition distributes over nonempty intersections , then $ \\gamma $ has a unique prime subnetwork , which can be obtained in cubic time by removing all redundant constraints simultaneously from $ \\gamma $ . as a byproduct , we show that any path-consistent network over such a distributive subalgebra is weakly globally consistent and minimal . a thorough empirical analysis of the prime subnetwork upon real geographical data sets demonstrates the approach is able to identify significantly more redundant constraints than previously proposed algorithms , especially in constraint networks with larger proportions of partial overlap relations .", "topics": ["computational complexity theory"]}
{"title": "a general method for deciding about logically constrained issues", "abstract": "a general method is given for revising degrees of belief and arriving at consistent decisions about a system of logically constrained issues . in contrast to other works about belief revision , here the constraints are assumed to be fixed . the method has two variants , dual of each other , whose revised degrees of belief are respectively above and below the original ones . the upper [ resp . lower ] revised degrees of belief are uniquely characterized as the lowest [ resp . highest ] ones that are invariant by a certain max-min [ resp . min-max ] operation determined by the logical constraints . in both variants , making balance between the revised degree of belief of a proposition and that of its negation leads to decisions that are ensured to be consistent with the logical constraints . these decisions are ensured to agree with the majority criterion as applied to the original degrees of belief whenever this gives a consistent result . they are also also ensured to satisfy a property of respect for unanimity about any particular issue , as well as a property of monotonicity with respect to the original degrees of belief . the application of the method to certain special domains comes down to well established or increasingly accepted methods , such as the single-link method of cluster analysis and the method of paths in preferential voting .", "topics": ["cluster analysis", "eisenstein 's criterion"]}
{"title": "nested expectation propagation for gaussian process classification with a multinomial probit likelihood", "abstract": "we consider probabilistic multinomial probit classification using gaussian process ( gp ) priors . the challenges with the multiclass gp classification are the integration over the non-gaussian posterior distribution , and the increase of the number of unknown latent variables as the number of target classes grows . expectation propagation ( ep ) has proven to be a very accurate method for approximate inference but the existing ep approaches for the multinomial probit gp classification rely on numerical quadratures or independence assumptions between the latent values from different classes to facilitate the computations . in this paper , we propose a novel nested ep approach which does not require numerical quadratures , and approximates accurately all between-class posterior dependencies of the latent values , but still scales linearly in the number of classes . the predictive accuracy of the nested ep approach is compared to laplace , variational bayes , and markov chain monte carlo ( mcmc ) approximations with various benchmark data sets . in the experiments nested ep was the most consistent method with respect to mcmc sampling , but the differences between the compared methods were small if only the classification accuracy is concerned .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "deep transform : cocktail party source separation via probabilistic re-synthesis", "abstract": "in cocktail party listening scenarios , the human brain is able to separate competing speech signals . however , the signal processing implemented by the brain to perform cocktail party listening is not well understood . here , we trained two separate convolutive autoencoder deep neural networks ( dnn ) to separate monaural and binaural mixtures of two concurrent speech streams . we then used these dnns as convolutive deep transform ( cdt ) devices to perform probabilistic re-synthesis . the cdts operated directly in the time-domain . our simulations demonstrate that very simple neural networks are capable of exploiting monaural and binaural information available in a cocktail party listening scenario .", "topics": ["simulation", "autoencoder"]}
{"title": "belief revision and rational inference", "abstract": "the ( extended ) agm postulates for belief revision seem to deal with the revision of a given theory k by an arbitrary formula , but not to constrain the revisions of two different theories by the same formula . a new postulate is proposed and compared with other similar postulates that have been proposed in the literature . the agm revisions that satisfy this new postulate stand in one-to-one correspondence with the rational , consistency-preserving relations . this correspondence is described explicitly . two viewpoints on iterative revisions are distinguished and discussed .", "topics": ["iteration"]}
{"title": "3d scanning : a comprehensive survey", "abstract": "this paper provides an overview of 3d scanning methodologies and technologies proposed in the existing scientific and industrial literature . throughout the paper , various types of the related techniques are reviewed , which consist , mainly , of close-range , aerial , structure-from-motion and terrestrial photogrammetry , and mobile , terrestrial and airborne laser scanning , as well as time-of-flight , structured-light and phase-comparison methods , along with comparative and combinational studies , the latter being intended to help make a clearer distinction on the relevance and reliability of the possible choices . moreover , outlier detection and surface fitting procedures are discussed concisely , which are necessary post-processing stages .", "topics": ["relevance"]}
{"title": "learning in auctions : regret is hard , envy is easy", "abstract": "a line of recent work provides welfare guarantees of simple combinatorial auction formats , such as selling m items via simultaneous second price auctions ( sispas ) ( christodoulou et al . 2008 , bhawalkar and roughgarden 2011 , feldman et al . 2013 ) . these guarantees hold even when the auctions are repeatedly executed and players use no-regret learning algorithms . unfortunately , off-the-shelf no-regret algorithms for these auctions are computationally inefficient as the number of actions is exponential . we show that this obstacle is insurmountable : there are no polynomial-time no-regret algorithms for sispas , unless rp $ \\supseteq $ np , even when the bidders are unit-demand . our lower bound raises the question of how good outcomes polynomially-bounded bidders may discover in such auctions . to answer this question , we propose a novel concept of learning in auctions , termed `` no-envy learning . '' this notion is founded upon walrasian equilibrium , and we show that it is both efficiently implementable and results in approximately optimal welfare , even when the bidders have fractionally subadditive ( xos ) valuations ( assuming demand oracles ) or coverage valuations ( without demand oracles ) . no-envy learning outcomes are a relaxation of no-regret outcomes , which maintain their approximate welfare optimality while endowing them with computational tractability . our results extend to other auction formats that have been studied in the literature via the smoothness paradigm . our results for xos valuations are enabled by a novel follow-the-perturbed-leader algorithm for settings where the number of experts is infinite , and the payoff function of the learner is non-linear . this algorithm has applications outside of auction settings , such as in security games . our result for coverage valuations is based on a novel use of convex rounding schemes and a reduction to online convex optimization .", "topics": ["regret ( decision theory )", "approximation algorithm"]}
{"title": "learning discriminative affine regions via discriminability", "abstract": "we present an accurate method for estimation of the affine shape of local features . the method is trained in a novel way , exploiting the recently proposed hardnet triplet loss . the loss function is driven by patch descriptor differences , avoiding problems with symmetries . moreover , such training process does not require precisely geometrically aligned patches . the affine shape is represented in a way amenable to learning by stochastic gradient descent . when plugged into a state-of-the-art wide baseline matching algorithm , the performance on standard datasets improves in both the number of challenging pairs matched and the number of inliers . finally , affnet with combination of hessian detector and hardnet descriptor improves bag-of-visual-words based state of the art on oxford5k and paris6k by large margin , 4.5 and 4.2 map points respectively . the source code and trained networks are available at https : //github.com/ducha-aiki/affnet", "topics": ["baseline ( configuration management )", "loss function"]}
{"title": "kernel feature selection via conditional covariance minimization", "abstract": "we propose a framework for feature selection that employs kernel-based measures of independence to find a subset of covariates that is maximally predictive of the response . building on past work in kernel dimension reduction , we formulate our approach as a constrained optimization problem involving the trace of the conditional covariance operator , and additionally provide some consistency results . we then demonstrate on a variety of synthetic and real data sets that our method compares favorably with other state-of-the-art algorithms .", "topics": ["kernel ( operating system )", "optimization problem"]}
{"title": "exploiting subgraph structure in multi-robot path planning", "abstract": "multi-robot path planning is difficult due to the combinatorial explosion of the search space with every new robot added . complete search of the combined state-space soon becomes intractable . in this paper we present a novel form of abstraction that allows us to plan much more efficiently . the key to this abstraction is the partitioning of the map into subgraphs of known structure with entry and exit restrictions which we can represent compactly . planning then becomes a search in the much smaller space of subgraph configurations . once an abstract plan is found , it can be quickly resolved into a correct ( but possibly sub-optimal ) concrete plan without the need for further search . we prove that this technique is sound and complete and demonstrate its practical effectiveness on a real map . a contending solution , prioritised planning , is also evaluated and shown to have similar performance albeit at the cost of completeness . the two approaches are not necessarily conflicting ; we demonstrate how they can be combined into a single algorithm which outperforms either approach alone .", "topics": ["robot"]}
{"title": "scaling pomdps for selecting sellers in e-markets-extended version", "abstract": "in multiagent e-marketplaces , buying agents need to select good sellers by querying other buyers ( called advisors ) . partially observable markov decision processes ( pomdps ) have shown to be an effective framework for optimally selecting sellers by selectively querying advisors . however , current solution methods do not scale to hundreds or even tens of agents operating in the e-market . in this paper , we propose the mixture of pomdp experts ( mope ) technique , which exploits the inherent structure of trust-based domains , such as the seller selection problem in e-markets , by aggregating the solutions of smaller sub-pomdps . we propose a number of variants of the mope approach that we analyze theoretically and empirically . experiments show that mope can scale up to a hundred agents thereby leveraging the presence of more advisors to significantly improve buyer satisfaction .", "topics": ["mathematical optimization"]}
{"title": "dac-h3 : a proactive robot cognitive architecture to acquire and express knowledge about the world and the self", "abstract": "this paper introduces a cognitive architecture for a humanoid robot to engage in a proactive , mixed-initiative exploration and manipulation of its environment , where the initiative can originate from both the human and the robot . the framework , based on a biologically-grounded theory of the brain and mind , integrates a reactive interaction engine , a number of state-of-the-art perceptual and motor learning algorithms , as well as planning abilities and an autobiographical memory . the architecture as a whole drives the robot behavior to solve the symbol grounding problem , acquire language capabilities , execute goal-oriented behavior , and express a verbal narrative of its own experience in the world . we validate our approach in human-robot interaction experiments with the icub humanoid robot , showing that the proposed cognitive architecture can be applied in real time within a realistic scenario and that it can be used with naive users .", "topics": ["robot"]}
{"title": "dynamic bayesian multinets", "abstract": "in this work , dynamic bayesian multinets are introduced where a markov chain state at time t determines conditional independence patterns between random variables lying within a local time window surrounding t. it is shown how information-theoretic criterion functions can be used to induce sparse , discriminative , and class-conditional network structures that yield an optimal approximation to the class posterior probability , and therefore are useful for the classification task . using a new structure learning heuristic , the resulting models are tested on a medium-vocabulary isolated-word speech recognition task . it is demonstrated that these discriminatively structured dynamic bayesian multinets , when trained in a maximum likelihood setting using em , can outperform both hmms and other dynamic bayesian networks with a similar number of parameters .", "topics": ["bayesian network", "sparse matrix"]}
{"title": "rethinking feature distribution for loss functions in image classification", "abstract": "we propose a large-margin gaussian mixture ( l-gm ) loss for deep neural networks in classification tasks . different from the softmax cross-entropy loss , our proposal is established on the assumption that the deep features of the training set follow a gaussian mixture distribution . by involving a classification margin and a likelihood regularization , the l-gm loss facilitates both a high classification performance and an accurate modeling of the training feature distribution . as such , the l-gm loss is superior to the softmax loss and its major variants in the sense that besides classification , it can be readily used to distinguish abnormal inputs , such as the adversarial examples , based on their features ' likelihood to the training feature distribution . extensive experiments on various recognition benchmarks like mnist , cifar , imagenet and lfw , as well as on adversarial examples demonstrate the effectiveness of our proposal .", "topics": ["loss function", "matrix regularization"]}
{"title": "pattern localization in time series through signal-to-model alignment in latent space", "abstract": "in this paper , we study the problem of locating a predefined sequence of patterns in a time series . in particular , the studied scenario assumes a theoretical model is available that contains the expected locations of the patterns . this problem is found in several contexts , and it is commonly solved by first synthesizing a time series from the model , and then aligning it to the true time series through dynamic time warping . we propose a technique that increases the similarity of both time series before aligning them , by mapping them into a latent correlation space . the mapping is learned from the data through a machine-learning setup . experiments on data from non-destructive testing demonstrate that the proposed approach shows significant improvements over the state of the art .", "topics": ["time series"]}
{"title": "the game imitation : deep supervised convolutional networks for quick video game ai", "abstract": "we present a vision-only model for gaming ai which uses a late integration deep convolutional network architecture trained in a purely supervised imitation learning context . although state-of-the-art deep learning models for video game tasks generally rely on more complex methods such as deep-q learning , we show that a supervised model which requires substantially fewer resources and training time can already perform well at human reaction speeds on the n64 classic game super smash bros. we frame our learning task as a 30-class classification problem , and our cnn model achieves 80 % top-1 and 95 % top-3 validation accuracy . with slight test-time fine-tuning , our model is also competitive during live simulation with the highest-level ai built into the game . we will further show evidence through network visualizations that the network is successfully leveraging temporal information during inference to aid in decision making . our work demonstrates that supervised cnn models can provide good performance in challenging policy prediction tasks while being significantly simpler and more lightweight than alternatives .", "topics": ["simulation"]}
{"title": "three generative , lexicalised models for statistical parsing", "abstract": "in this paper we first propose a new statistical parsing model , which is a generative model of lexicalised context-free grammar . we then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement . results on wall street journal text show that the parser performs at 88.1/87.5 % constituent precision/recall , an average improvement of 2.3 % over ( collins 96 ) .", "topics": ["generative model", "parsing"]}
{"title": "towards open world recognition", "abstract": "with the of advent rich classification models and high computational power visual recognition systems have found many operational applications . recognition in the real world poses multiple challenges that are not apparent in controlled lab environments . the datasets are dynamic and novel categories must be continuously detected and then added . at prediction time , a trained system has to deal with myriad unseen categories . operational systems require minimum down time , even to learn . to handle these operational issues , we present the problem of open world recognition and formally define it . we prove that thresholding sums of monotonically decreasing functions of distances in linearly transformed feature space can balance `` open space risk '' and empirical risk . our theory extends existing algorithms for open world recognition . we present a protocol for evaluation of open world recognition systems . we present the nearest non-outlier ( nno ) algorithm which evolves model efficiently , adding object categories incrementally while detecting outliers and managing open space risk . we perform experiments on the imagenet dataset with 1.2m+ images to validate the effectiveness of our method on large scale visual recognition tasks . nno consistently yields superior results on open world recognition .", "topics": ["feature vector", "computer vision"]}
{"title": "per-pixel feedback for improving semantic segmentation", "abstract": "semantic segmentation is the task of assigning a label to each pixel in the image.in recent years , deep convolutional neural networks have been driving advances in multiple tasks related to cognition . although , dcnns have resulted in unprecedented visual recognition performances , they offer little transparency . to understand how dcnn based models work at the task of semantic segmentation , we try to analyze the dcnn models in semantic segmentation . we try to find the importance of global image information for labeling pixels . based on the experiments on discriminative regions , and modeling of fixations , we propose a set of new training loss functions for fine-tuning dcnn based models . the proposed training regime has shown improvement in performance of deeplab large fov ( vgg-16 ) segmentation model for pascal voc 2012 dataset . however , further test remains to conclusively evaluate the benefits due to the proposed loss functions across models , and data-sets . submitted in part fulfillment of the requirements for the degree of integrated masters of science in applied mathematics . update : further experiment showed minimal benefits . code available [ here ] ( https : //github.com/bardofcodes/seg-unravel ) .", "topics": ["image segmentation", "loss function"]}
{"title": "should i stay or should i go : coordinating biological needs with continuously-updated assessments of the environment", "abstract": "this paper presents wanderer , a model of how autonomous adaptive systems coordinate internal biological needs with moment-by-moment assessments of the probabilities of events in the external world . the extent to which wanderer moves about or explores its environment reflects the relative activations of two competing motivational sub-systems : one represents the need to acquire energy and it excites exploration , and the other represents the need to avoid predators and it inhibits exploration . the environment contains food , predators , and neutral stimuli . wanderer responds to these events in a way that is adaptive in the short turn , and reassesses the probabilities of these events so that it can modify its long term behaviour appropriately . when food appears , wanderer be-comes satiated and exploration temporarily decreases . when a predator appears , wanderer both decreases exploration in the short term , and becomes more `` cautious '' about exploring in the future . wanderer also forms associations between neutral features and salient ones ( food and predators ) when they are present at the same time , and uses these associations to guide its behaviour .", "topics": ["autonomous car"]}
{"title": "efficient character-level document classification by combining convolution and recurrent layers", "abstract": "document classification tasks were primarily tackled at word level . recent research that works with character-level inputs shows several benefits over word-level approaches such as natural incorporation of morphemes and better handling of rare words . we propose a neural network architecture that utilizes both convolution and recurrent layers to efficiently encode character inputs . we validate the proposed model on eight large scale document classification tasks and compare with character-level convolution-only models . it achieves comparable performances with much less parameters .", "topics": ["convolution"]}
{"title": "on the exploration of convolutional fusion networks for visual recognition", "abstract": "despite recent advances in multi-scale deep representations , their limitations are attributed to expensive parameters and weak fusion modules . hence , we propose an efficient approach to fuse multi-scale deep representations , called convolutional fusion networks ( cfn ) . owing to using 1 $ \\times $ 1 convolution and global average pooling , cfn can efficiently generate the side branches while adding few parameters . in addition , we present a locally-connected fusion module , which can learn adaptive weights for the side branches and form a discriminatively fused feature . cfn models trained on the cifar and imagenet datasets demonstrate remarkable improvements over the plain cnns . furthermore , we generalize cfn to three new tasks , including scene recognition , fine-grained recognition and image retrieval . our experiments show that it can obtain consistent improvements towards the transferring tasks .", "topics": ["convolution"]}
{"title": "multi-dimensional inheritance", "abstract": "in this paper , we present an alternative approach to multiple inheritance for typed feature structures . in our approach , a feature structure can be associated with several types coming from different hierarchies ( dimensions ) . in case of multiple inheritance , a type has supertypes from different hierarchies . we contrast this approach with approaches based on a single type hierarchy where a feature structure has only one unique most general type , and multiple inheritance involves computation of greatest lower bounds in the hierarchy . the proposed approach supports current linguistic analyses in constraint-based formalisms like hpsg , inheritance in the lexicon , and knowledge representation for nlp systems . finally , we show that multi-dimensional inheritance hierarchies can be compiled into a prolog term representation , which allows to compute the conjunction of two types efficiently by prolog term unification .", "topics": ["natural language processing", "computation"]}
{"title": "lexical-semantic resources : yet powerful resources for automatic personality classification", "abstract": "in this paper , we aim to reveal the impact of lexical-semantic resources , used in particular for word sense disambiguation and sense-level semantic categorization , on automatic personality classification task . while stylistic features ( e.g . , part-of-speech counts ) have been shown their power in this task , the impact of semantics beyond targeted word lists is relatively unexplored . we propose and extract three types of lexical-semantic features , which capture high-level concepts and emotions , overcoming the lexical gap of word n-grams . our experimental results are comparable to state-of-the-art methods , while no personality-specific resources are required .", "topics": ["high- and low-level"]}
{"title": "feature extraction and soft computing methods for aerospace structure defect classification", "abstract": "this study concerns the effectiveness of several techniques and methods of signals processing and data interpretation for the diagnosis of aerospace structure defects . this is done by applying different known feature extraction methods , in addition to a new cbir-based one ; and some soft computing techniques including a recent hpc parallel implementation of the u-brain learning algorithm on non destructive testing data . the performance of the resulting detection systems are measured in terms of accuracy , sensitivity , specificity , and precision . their effectiveness is evaluated by the matthews correlation , the area under curve ( auc ) , and the f-measure . several experiments are performed on a standard dataset of eddy current signal samples for aircraft structures . our experimental results evidence that the key to a successful defect classifier is the feature extraction method - namely the novel cbir-based one outperforms all the competitors - and they illustrate the greater effectiveness of the u-brain algorithm and the mlp neural network among the soft computing methods in this kind of application . keywords- non-destructive testing ( ndt ) ; soft computing ; feature extraction ; classification algorithms ; content-based image retrieval ( cbir ) ; eddy currents ( ec ) .", "topics": ["feature extraction"]}
{"title": "seeing behind the camera : identifying the authorship of a photograph", "abstract": "we introduce the novel problem of identifying the photographer behind a photograph . to explore the feasibility of current computer vision techniques to address this problem , we created a new dataset of over 180,000 images taken by 41 well-known photographers . using this dataset , we examined the effectiveness of a variety of features ( low and high-level , including cnn features ) at identifying the photographer . we also trained a new deep convolutional neural network for this task . our results show that high-level features greatly outperform low-level features . we provide qualitative results using these learned models that give insight into our method 's ability to distinguish between photographers , and allow us to draw interesting conclusions about what specific photographers shoot . we also demonstrate two applications of our method .", "topics": ["high- and low-level", "computer vision"]}
{"title": "refined particle swarm intelligence method for abrupt motion tracking", "abstract": "conventional tracking solutions are not feasible in handling abrupt motion as they are based on smooth motion assumption or an accurate motion model . abrupt motion is not subject to motion continuity and smoothness . to assuage this , we deem tracking as an optimisation problem and propose a novel abrupt motion tracker that based on swarm intelligence - the swatrack . unlike existing swarm-based filtering methods , we first of all introduce an optimised swarm-based sampling strategy to tradeoff between the exploration and exploitation of the search space in search for the optimal proposal distribution . secondly , we propose dynamic acceleration parameters ( dap ) allow on the fly tuning of the best mean and variance of the distribution for sampling . such innovating idea of combining these strategies in an ingenious way in the pso framework to handle the abrupt motion , which so far no existing works are found . experimental results in both quantitative and qualitative had shown the effectiveness of the proposed method in tracking abrupt motions .", "topics": ["sampling ( signal processing )", "mathematical optimization"]}
{"title": "mixing time estimation in reversible markov chains from a single sample path", "abstract": "this article provides the first procedure for computing a fully data-dependent interval that traps the mixing time $ t_ { \\text { mix } } $ of a finite reversible ergodic markov chain at a prescribed confidence level . the interval is computed from a single finite-length sample path from the markov chain , and does not require the knowledge of any parameters of the chain . this stands in contrast to previous approaches , which either only provide point estimates , or require a reset mechanism , or additional prior knowledge . the interval is constructed around the relaxation time $ t_ { \\text { relax } } $ , which is strongly related to the mixing time , and the width of the interval converges to zero roughly at a $ \\sqrt { n } $ rate , where $ n $ is the length of the sample path . upper and lower bounds are given on the number of samples required to achieve constant-factor multiplicative accuracy . the lower bounds indicate that , unless further restrictions are placed on the chain , no procedure can achieve this accuracy level before seeing each state at least $ \\omega ( t_ { \\text { relax } } ) $ times on the average . finally , future directions of research are identified .", "topics": ["markov chain"]}
{"title": "efficient variational bayesian neural network ensembles for outlier detection", "abstract": "in this work we perform outlier detection using ensembles of neural networks obtained by variational approximation of the posterior in a bayesian neural network setting . the variational parameters are obtained by sampling from the true posterior by gradient descent . we show our outlier detection results are comparable to those obtained using other efficient ensembling methods .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "deep structured energy-based image inpainting", "abstract": "in this paper , we propose a structured image inpainting method employing an energy based model . in order to learn structural relationship between patterns observed in images and missing regions of the images , we employ an energy-based structured prediction method . the structural relationship is learned by minimizing an energy function which is defined by a simple convolutional neural network . the experimental results on various benchmark datasets show that our proposed method significantly outperforms the state-of-the-art methods which use generative adversarial networks ( gans ) . we obtained 497.35 mean squared error ( mse ) on the olivetti face dataset compared to 833.0 mse provided by the state-of-the-art method . moreover , we obtained 28.4 db peak signal to noise ratio ( psnr ) on the svhn dataset and 23.53 db on the celeba dataset , compared to 22.3 db and 21.3 db , provided by the state-of-the-art methods , respectively . the code is publicly available .", "topics": ["mathematical optimization"]}
{"title": "el lenguaje natural como lenguaje formal", "abstract": "formal languages theory is useful for the study of natural language . in particular , it is of interest to study the adequacy of the grammatical formalisms to express syntactic phenomena present in natural language . first , it helps to draw hypothesis about the nature and complexity of the speaker-hearer linguistic competence , a fundamental question in linguistics and other cognitive sciences . moreover , from an engineering point of view , it allows the knowledge of practical limitations of applications based on those formalisms . in this article i introduce the adequacy problem of grammatical formalisms for natural language , also introducing some formal language theory concepts required for this discussion . then , i review the formalisms that have been proposed in history , and the arguments that have been given to support or reject their adequacy . -- -- - la teor\\'ia de lenguajes formales es \\'util para el estudio de los lenguajes naturales . en particular , resulta de inter\\'es estudiar la adecuaci\\'on de los formalismos gramaticales para expresar los fen\\'omenos sint\\'acticos presentes en el lenguaje natural . primero , ayuda a trazar hip\\'otesis acerca de la naturaleza y complejidad de las competencias ling\\ '' u\\'isticas de los hablantes-oyentes del lenguaje , un interrogante fundamental de la ling\\ '' u\\'istica y otras ciencias cognitivas . adem\\'as , desde el punto de vista de la ingenier\\'ia , permite conocer limitaciones pr\\'acticas de las aplicaciones basadas en dichos formalismos . en este art\\'iculo hago una introducci\\'on al problema de la adecuaci\\'on de los formalismos gramaticales para el lenguaje natural , introduciendo tambi\\'en algunos conceptos de la teor\\'ia de lenguajes formales necesarios para esta discusi\\'on . luego , hago un repaso de los formalismos que han sido propuestos a lo largo de la historia , y de los argumentos que se han dado para sostener o refutar su adecuaci\\'on .", "topics": ["natural language"]}
{"title": "bidirectional attention for sql generation", "abstract": "generating structural query language ( sql ) queries from natural language is a long-standing open problem . answering a natural language question about a database table requires modeling complex interactions between the columns of the table and the question . it has been attracting considerable interest recently and driven by the explosive development of deep learning techniques . in this paper , we apply the sketch-based approach or synthesizing way to solve this problem . based on the structure of sql queries , we break down the model to three sub-modules and design specific deep neural networks for each of them . we employ the bidirectional attention mechanisms and character-level embedding with convolutional neural networks ( cnns ) to improve the result . experimental evaluations show that our model achieves the state-of-the-art results in wikisql dataset .", "topics": ["natural language"]}
{"title": "cover : learning covariate-specific vector representations with tensor decompositions", "abstract": "word embedding is a useful approach to capture co-occurrence structures in a large corpus of text . in addition to the text data itself , we often have additional covariates associated with individual documents in the corpus -- -e.g . the demographic of the author , time and venue of publication , etc . -- -and we would like the embedding to naturally capture the information of the covariates . in this paper , we propose cover , a new tensor decomposition model for vector embeddings with covariates . cover jointly learns a base embedding for all the words as well as a weighted diagonal transformation to model how each covariate modifies the base embedding . to obtain the specific embedding for a particular author or venue , for example , we can then simply multiply the base embedding by the transformation matrix associated with that time or venue . the main advantages of our approach is data efficiency and interpretability of the covariate transformation matrix . our experiments demonstrate that our joint model learns substantially better embeddings conditioned on each covariate compared to the standard approach of learning a separate embedding for each covariate using only the relevant subset of data , as well as other related methods . furthermore , cover encourages the embeddings to be `` topic-aligned '' in the sense that the dimensions have specific independent meanings . this allows our covariate-specific embeddings to be compared by topic , enabling downstream differential analysis . we empirically evaluate the benefits of our algorithm on several datasets , and demonstrate how it can be used to address many natural questions about the effects of covariates .", "topics": ["text corpus"]}
{"title": "efficient optimization for rank-based loss functions", "abstract": "the accuracy of information retrieval systems is often measured using complex loss functions such as the average precision ( ap ) or the normalized discounted cumulative gain ( ndcg ) . given a set of positive and negative samples , the parameters of a retrieval system can be estimated by minimizing these loss functions . however , the non-differentiability and non-decomposability of these loss functions does not allow for simple gradient based optimization algorithms . this issue is generally circumvented by either optimizing a structured hinge-loss upper bound to the loss function or by using asymptotic methods like the direct-loss minimization framework . yet , the high computational complexity of loss-augmented inference , which is necessary for both the frameworks , prohibits its use in large training data sets . to alleviate this deficiency , we present a novel quicksort flavored algorithm for a large class of non-decomposable loss functions . we provide a complete characterization of the loss functions that are amenable to our algorithm , and show that it includes both ap and ndcg based loss functions . furthermore , we prove that no comparison based algorithm can improve upon the computational complexity of our approach asymptotically . we demonstrate the effectiveness of our approach in the context of optimizing the structured hinge loss upper bound of ap and ndcg loss for learning models for a variety of vision tasks . we show that our approach provides significantly better results than simpler decomposable loss functions , while requiring a comparable training time .", "topics": ["baseline ( configuration management )", "computational complexity theory"]}
{"title": "deep choice model using pointer networks for airline itinerary prediction", "abstract": "travel providers such as airlines and on-line travel agents are becoming more and more interested in understanding how passengers choose among alternative itineraries when searching for flights . this knowledge helps them better display and adapt their offer , taking into account market conditions and customer needs . some common applications are not only filtering and sorting alternatives , but also changing certain attributes in real-time ( e.g . , changing the price ) . in this paper , we concentrate with the problem of modeling air passenger choices of flight itineraries . this problem has historically been tackled using classical discrete choice modelling techniques . traditional statistical approaches , in particular the multinomial logit model ( mnl ) , is widely used in industrial applications due to its simplicity and general good performance . however , mnl models present several shortcomings and assumptions that might not hold in real applications . to overcome these difficulties , we present a new choice model based on pointer networks . given an input sequence , this type of deep neural architecture combines recurrent neural networks with the attention mechanism to learn the conditional probability of an output whose values correspond to positions in an input sequence . therefore , given a sequence of different alternatives presented to a customer , the model can learn to point to the one most likely to be chosen by the customer . the proposed method was evaluated on a real dataset that combines on-line user search logs and airline flight bookings . experimental results show that the proposed model outperforms the traditional mnl model on several metrics .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "memory-augmented neural machine translation", "abstract": "neural machine translation ( nmt ) has achieved notable success in recent times , however it is also widely recognized that this approach has limitations with handling infrequent words and word pairs . this paper presents a novel memory-augmented nmt ( m-nmt ) architecture , which stores knowledge about how words ( usually infrequently encountered ones ) should be translated in a memory and then utilizes them to assist the neural model . we use this memory mechanism to combine the knowledge learned from a conventional statistical machine translation system and the rules learned by an nmt system , and also propose a solution for out-of-vocabulary ( oov ) words based on this framework . our experiments on two chinese-english translation tasks demonstrated that the m-nmt architecture outperformed the nmt baseline by $ 9.0 $ and $ 2.7 $ bleu points on the two tasks , respectively . additionally , we found this architecture resulted in a much more effective oov treatment compared to competitive methods .", "topics": ["baseline ( configuration management )", "machine translation"]}
{"title": "towards a deep reinforcement learning approach for tower line wars", "abstract": "there have been numerous breakthroughs with reinforcement learning in the recent years , perhaps most notably on deep reinforcement learning successfully playing and winning relatively advanced computer games . there is undoubtedly an anticipation that deep reinforcement learning will play a major role when the first ai masters the complicated game plays needed to beat a professional real-time strategy game player . for this to be possible , there needs to be a game environment that targets and fosters ai research , and specifically deep reinforcement learning . some game environments already exist , however , these are either overly simplistic such as atari 2600 or complex such as starcraft ii from blizzard entertainment . we propose a game environment in between atari 2600 and starcraft ii , particularly targeting deep reinforcement learning algorithm research . the environment is a variant of tower line wars from warcraft iii , blizzard entertainment . further , as a proof of concept that the environment can harbor deep reinforcement algorithms , we propose and apply a deep q-reinforcement architecture . the architecture simplifies the state space so that it is applicable to q-learning , and in turn improves performance compared to current state-of-the-art methods . our experiments show that the proposed architecture can learn to play the environment well , and score 33 % better than standard deep q-learning which in turn proves the usefulness of the game environment .", "topics": ["reinforcement learning"]}
{"title": "predicting drug interactions and mutagenicity with ensemble classifiers on subgraphs of molecules", "abstract": "in this study , we intend to solve a mutual information problem in interacting molecules of any type , such as proteins , nucleic acids , and small molecules . using machine learning techniques , we accurately predict pairwise interactions , which can be of medical and biological importance . graphs are are useful in this problem for their generality to all types of molecules , due to the inherent association of atoms through atomic bonds . subgraphs can represent different molecular domains . these domains can be biologically significant as most molecules only have portions that are of functional significance and can interact with other domains . thus , we use subgraphs as features in different machine learning algorithms to predict if two drugs interact and predict potential single molecule effects .", "topics": ["interaction"]}
{"title": "learning curves for multi-task gaussian process regression", "abstract": "we study the average case performance of multi-task gaussian process ( gp ) regression as captured in the learning curve , i.e . the average bayes error for a chosen task versus the total number of examples $ n $ for all tasks . for gp covariances that are the product of an input-dependent covariance function and a free-form inter-task covariance matrix , we show that accurate approximations for the learning curve can be obtained for an arbitrary number of tasks $ t $ . we use these to study the asymptotic learning behaviour for large $ n $ . surprisingly , multi-task learning can be asymptotically essentially useless , in the sense that examples from other tasks help only when the degree of inter-task correlation , $ \\rho $ , is near its maximal value $ \\rho=1 $ . this effect is most extreme for learning of smooth target functions as described by e.g . squared exponential kernels . we also demonstrate that when learning many tasks , the learning curves separate into an initial phase , where the bayes error on each task is reduced down to a plateau value by `` collective learning '' even though most tasks have not seen examples , and a final decay that occurs once the number of examples is proportional to the number of tasks .", "topics": ["time complexity", "approximation"]}
{"title": "deep stacked networks with residual polishing for image inpainting", "abstract": "deep neural networks have shown promising results in image inpainting even if the missing area is relatively large . however , most of the existing inpainting networks introduce undesired artifacts and noise to the repaired regions . to solve this problem , we present a novel framework which consists of two stacked convolutional neural networks that inpaint the image and remove the artifacts , respectively . the first network considers the global structure of the damaged image and coarsely fills the blank area . then the second network modifies the repaired image to cancel the noise introduced by the first network . the proposed framework splits the problem into two distinct partitions that can be optimized separately , therefore it can be applied to any inpainting algorithm by changing the first network . second stage in our framework which aims at polishing the inpainted images can be treated as a denoising problem where a wide range of algorithms can be employed . our results demonstrate that the proposed framework achieves significant improvement on both visual and quantitative evaluations .", "topics": ["noise reduction"]}
{"title": "3d surface reconstruction of underwater objects", "abstract": "in this paper , we propose a novel technique to reconstruct 3d surface of an underwater object using stereo images . reconstructing the 3d surface of an underwater object is really a challenging task due to degraded quality of underwater images . there are various reason of quality degradation of underwater images i.e . , non-uniform illumination of light on the surface of objects , scattering and absorption effects . floating particles present in underwater produces gaussian noise on the captured underwater images which degrades the quality of images . the degraded underwater images are preprocessed by applying homomorphic , wavelet denoising and anisotropic filtering sequentially . the uncalibrated rectification technique is applied to preprocessed images to rectify the left and right images . the rectified left and right image lies on a common plane . to find the correspondence points in a left and right images , we have applied dense stereo matching technique i.e . , graph cut method . finally , we estimate the depth of images using triangulation technique . the experimental result shows that the proposed method reconstruct 3d surface of underwater objects accurately using captured underwater stereo images .", "topics": ["noise reduction"]}
{"title": "a tiered move-making algorithm for general non-submodular pairwise energies", "abstract": "a large number of problems in computer vision can be modelled as energy minimization problems in a markov random field ( mrf ) or conditional random field ( crf ) framework . graph-cuts based $ \\alpha $ -expansion is a standard move-making method to minimize the energy functions with sub-modular pairwise terms . however , certain problems require more complex pairwise terms where the $ \\alpha $ -expansion method is generally not applicable . in this paper , we propose an iterative { \\em tiered move making algorithm } which is able to handle general pairwise terms . each move to the next configuration is based on the current labeling and an optimal tiered move , where each tiered move requires one application of the dynamic programming based tiered labeling method introduced in felzenszwalb et . al . \\cite { tiered_cvpr_felzenszwalbv10 } . the algorithm converges to a local minimum for any general pairwise potential , and we give a theoretical analysis of the properties of the algorithm , characterizing the situations in which we can expect good performance . we first evaluate our method on an object-class segmentation problem using the pascal voc-11 segmentation dataset where we learn general pairwise terms . further we evaluate the algorithm on many other benchmark labeling problems such as stereo , image segmentation , image stitching and image denoising . our method consistently gets better accuracy and energy values than alpha-expansion , loopy belief propagation ( lbp ) , quadratic pseudo-boolean optimization ( qpbo ) , and is competitive with trws .", "topics": ["image segmentation", "noise reduction"]}
{"title": "anatomical labeling of brain ct scan anomalies using multi-context nearest neighbor relation networks", "abstract": "this work is an endeavor to develop a deep learning methodology for automated anatomical labeling of a given region of interest ( roi ) in brain computed tomography ( ct ) scans . we combine both local and global context to obtain a representation of the roi . we then use relation networks ( rns ) to predict the corresponding anatomy of the roi based on its relationship score for each class . further , we propose a novel strategy employing nearest neighbors approach for training rns . we train rns to learn the relationship of the target roi with the joint representation of its nearest neighbors in each class instead of all data-points in each class . the proposed strategy leads to better training of rns along with increased performance as compared to training baseline rn network .", "topics": ["baseline ( configuration management )"]}
{"title": "pointer networks", "abstract": "we introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence . such problems can not be trivially addressed by existent approaches such as sequence-to-sequence and neural turing machines , because the number of target classes in each step of the output depends on the length of the input , which is variable . problems such as sorting variable sized sequences , and various combinatorial optimization problems belong to this class . our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention . it differs from the previous attention attempts in that , instead of using attention to blend hidden units of an encoder to a context vector at each decoder step , it uses attention as a pointer to select a member of the input sequence as the output . we call this architecture a pointer net ( ptr-net ) . we show ptr-nets can be used to learn approximate solutions to three challenging geometric problems -- finding planar convex hulls , computing delaunay triangulations , and the planar travelling salesman problem -- using training examples alone . ptr-nets not only improve over sequence-to-sequence with input attention , but also allow us to generalize to variable size output dictionaries . we show that the learnt models generalize beyond the maximum lengths they were trained on . we hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems .", "topics": ["approximation algorithm", "encoder"]}
{"title": "supervised descent method for solving nonlinear least squares problems in computer vision", "abstract": "many computer vision problems ( e.g . , camera calibration , image alignment , structure from motion ) are solved with nonlinear optimization methods . it is generally accepted that second order descent methods are the most robust , fast , and reliable approaches for nonlinear optimization of a general smooth function . however , in the context of computer vision , second order descent methods have two main drawbacks : ( 1 ) the function might not be analytically differentiable and numerical approximations are impractical , and ( 2 ) the hessian may be large and not positive definite . to address these issues , this paper proposes generic descent maps , which are average `` descent directions '' and rescaling factors learned in a supervised fashion . using generic descent maps , we derive a practical algorithm - supervised descent method ( sdm ) - for minimizing nonlinear least squares ( nls ) problems . during training , sdm learns a sequence of decent maps that minimize the nls . in testing , sdm minimizes the nls objective using the learned descent maps without computing the jacobian or the hessian . we prove the conditions under which the sdm is guaranteed to converge . we illustrate the effectiveness and accuracy of sdm in three computer vision problems : rigid image alignment , non-rigid image alignment , and 3d pose estimation . in particular , we show how sdm achieves state-of-the-art performance in the problem of facial feature detection . the code has been made available at www.humansensing.cs.cmu.edu/intraface .", "topics": ["nonlinear system", "numerical analysis"]}
{"title": "leveraging twitter for low-resource conversational speech language modeling", "abstract": "in applications involving conversational speech , data sparsity is a limiting factor in building a better language model . we propose a simple , language-independent method to quickly harvest large amounts of data from twitter to supplement a smaller training set that is more closely matched to the domain . the techniques lead to a significant reduction in perplexity on four low-resource languages even though the presence on twitter of these languages is relatively small . we also find that the twitter text is more useful for learning word classes than the in-domain text and that use of these word classes leads to further reductions in perplexity . additionally , we introduce a method of using social and textual information to prioritize the download queue during the twitter crawling . this maximizes the amount of useful data that can be collected , impacting both perplexity and vocabulary coverage .", "topics": ["sparse matrix"]}
{"title": "interpretable explanations of black boxes by meaningful perturbation", "abstract": "as machine learning algorithms are increasingly applied to high impact yet high risk tasks , such as medical diagnosis or autonomous driving , it is critical that researchers can explain how such algorithms arrived at their predictions . in recent years , a number of image saliency methods have been developed to summarize where highly complex neural networks `` look '' in an image for evidence for their predictions . however , these techniques are limited by their heuristic nature and architectural constraints . in this paper , we make two main contributions : first , we propose a general framework for learning different kinds of explanations for any black box algorithm . second , we specialise the framework to find the part of an image most responsible for a classifier decision . unlike previous works , our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations .", "topics": ["statistical classification", "heuristic"]}
{"title": "lifted probabilistic inference for asymmetric graphical models", "abstract": "lifted probabilistic inference algorithms have been successfully applied to a large number of symmetric graphical models . unfortunately , the majority of real-world graphical models is asymmetric . this is even the case for relational representations when evidence is given . therefore , more recent work in the community moved to making the models symmetric and then applying existing lifted inference algorithms . however , this approach has two shortcomings . first , all existing over-symmetric approximations require a relational representation such as markov logic networks . second , the induced symmetries often change the distribution significantly , making the computed probabilities highly biased . we present a framework for probabilistic sampling-based inference that only uses the induced approximate symmetries to propose steps in a metropolis-hastings style markov chain . the framework , therefore , leads to improved probability estimates while remaining unbiased . experiments demonstrate that the approach outperforms existing mcmc algorithms .", "topics": ["graphical model", "approximation"]}
{"title": "variance regularizing adversarial learning", "abstract": "we introduce a novel approach for training adversarial models by replacing the discriminator score with a bi-modal gaussian distribution over the real/fake indicator variables . in order to do this , we train the gaussian classifier to match the target bi-modal distribution implicitly through meta-adversarial training . we hypothesize that this approach ensures a non-zero gradient to the generator , even in the limit of a perfect classifier . we test our method against standard benchmark image datasets as well as show the classifier output distribution is smooth and has overlap between the real and fake modes .", "topics": ["gradient"]}
{"title": "accurate streaming support vector machines", "abstract": "a widely-used tool for binary classification is the support vector machine ( svm ) , a supervised learning technique that finds the `` maximum margin '' linear separator between the two classes . while svms have been well studied in the batch ( offline ) setting , there is considerably less work on the streaming ( online ) setting , which requires only a single pass over the data using sub-linear space . existing streaming algorithms are not yet competitive with the batch implementation . in this paper , we use the formulation of the svm as a minimum enclosing ball ( meb ) problem to provide a streaming svm algorithm based off of the blurred ball cover originally proposed by agarwal and sharathkumar . our implementation consistently outperforms existing streaming svm approaches and provides higher accuracies than libsvm on several datasets , thus making it competitive with the standard svm batch implementation .", "topics": ["supervised learning", "support vector machine"]}
{"title": "learning robust rewards with adversarial inverse reinforcement learning", "abstract": "reinforcement learning provides a powerful and general framework for decision making and control , but its application in practice is often hindered by the need for extensive feature and reward engineering . deep reinforcement learning methods can remove the need for explicit engineering of policy or value features , but still require a manually specified reward function . inverse reinforcement learning holds the promise of automatic reward acquisition , but has proven exceptionally difficult to apply to large , high-dimensional problems with unknown dynamics . in this work , we propose adverserial inverse reinforcement learning ( airl ) , a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation . we demonstrate that airl is able to recover reward functions that are robust to changes in dynamics , enabling us to learn policies even under significant variation in the environment seen during training . our experiments show that airl greatly outperforms prior methods in these transfer settings .", "topics": ["reinforcement learning", "scalability"]}
{"title": "bird species categorization using pose normalized deep convolutional nets", "abstract": "we propose an architecture for fine-grained visual categorization that approaches expert human performance in the classification of bird species . our architecture first computes an estimate of the object 's pose ; this is used to compute local image features which are , in turn , used for classification . the features are computed by applying deep convolutional nets to image patches that are located and normalized by the pose . we perform an empirical study of a number of pose normalization schemes , including an investigation of higher order geometric warping functions . we propose a novel graph-based clustering algorithm for learning a compact pose normalization space . we perform a detailed investigation of state-of-the-art deep convolutional feature implementations and fine-tuning feature learning for fine-grained classification . we observe that a model that integrates lower-level feature layers with pose-normalized extraction routines and higher-level feature layers with unaligned image features works best . our experiments advance state-of-the-art performance on bird species recognition , with a large improvement of correct classification rates over previous methods ( 75 % vs . 55-65 % ) .", "topics": ["feature learning", "cluster analysis"]}
{"title": "structure based extended resolution for constraint programming", "abstract": "nogood learning is a powerful approach to reducing search in constraint programming ( cp ) solvers . the current state of the art , called lazy clause generation ( lcg ) , uses resolution to derive nogoods expressing the reasons for each search failure . such nogoods can prune other parts of the search tree , producing exponential speedups on a wide variety of problems . nogood learning solvers can be seen as resolution proof systems . the stronger the proof system , the faster it can solve a cp problem . it has recently been shown that the proof system used in lcg is at least as strong as general resolution . however , stronger proof systems such as \\emph { extended resolution } exist . extended resolution allows for literals expressing arbitrary logical concepts over existing variables to be introduced and can allow exponentially smaller proofs than general resolution . the primary problem in using extended resolution is to figure out exactly which literals are useful to introduce . in this paper , we show that we can use the structural information contained in a cp model in order to introduce useful literals , and that this can translate into significant speedups on a range of problems .", "topics": ["time complexity"]}
{"title": "non-stationary stochastic optimization with local spatial and temporal changes", "abstract": "we consider a non-stationary sequential stochastic optimization problem , in which the underlying cost functions change over time under a variation budget constraint . we propose an $ l_ { p , q } $ -variation functional to quantify the change , which captures local spatial and temporal variations of the sequence of functions . under the $ l_ { p , q } $ -variation functional constraint , we derive both upper and matching lower regret bounds for smooth and strongly convex function sequences , which generalize previous results in ( besbes et al . , 2015 ) . our results reveal some surprising phenomena under this general variation functional , such as the curse of dimensionality of the function domain . the key technical novelties in our analysis include an affinity lemma that characterizes the distance of the minimizers of two convex functions with bounded $ l_p $ difference , and a cubic spline based construction that attains matching lower bounds .", "topics": ["regret ( decision theory )", "optimization problem"]}
{"title": "fast adaptive weight noise", "abstract": "marginalising out uncertain quantities within the internal representations or parameters of neural networks is of central importance for a wide range of learning techniques , such as empirical , variational or full bayesian methods . we set out to generalise fast dropout ( wang & manning , 2013 ) to cover a wider variety of noise processes in neural networks . this leads to an efficient calculation of the marginal likelihood and predictive distribution which evades sampling and the consequential increase in training time due to highly variant gradient estimates . this allows us to approximate variational bayes for the parameters of feed-forward neural networks . inspired by the minimum description length principle , we also propose and experimentally verify the direct optimisation of the regularised predictive distribution . the methods yield results competitive with previous neural network based approaches and gaussian processes on a wide range of regression tasks .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "on convergence property of implicit self-paced objective", "abstract": "self-paced learning ( spl ) is a new methodology that simulates the learning principle of humans/animals to start learning easier aspects of a learning task , and then gradually take more complex examples into training . this new-coming learning regime has been empirically substantiated to be effective in various computer vision and pattern recognition tasks . recently , it has been proved that the spl regime has a close relationship to a implicit self-paced objective function . while this implicit objective could provide helpful interpretations to the effectiveness , especially the robustness , insights under the spl paradigms , there are still no theoretical results strictly proved to verify such relationship . to this issue , in this paper , we provide some convergence results on this implicit objective of spl . specifically , we prove that the learning process of spl always converges to critical points of this implicit objective under some mild conditions . this result verifies the intrinsic relationship between spl and this implicit objective , and makes the previous robustness analysis on spl complete and theoretically rational .", "topics": ["optimization problem", "computer vision"]}
{"title": "automatic segmentation method of pelvic floor levator hiatus in ultrasound using a self-normalising neural network", "abstract": "segmentation of the levator hiatus in ultrasound allows to extract biometrics which are of importance for pelvic floor disorder assessment . in this work , we present a fully automatic method using a convolutional neural network ( cnn ) to outline the levator hiatus in a 2d image extracted from a 3d ultrasound volume . in particular , our method uses a recently developed scaled exponential linear unit ( selu ) as a nonlinear self-normalising activation function , which for the first time has been applied in medical imaging with cnn . selu has important advantages such as being parameter-free and mini-batch independent , which may help to overcome memory constraints during training . a dataset with 91 images from 35 patients during valsalva , contraction and rest , all labelled by three operators , is used for training and evaluation in a leave-one-patient-out cross-validation . results show a median dice similarity coefficient of 0.90 with an interquartile range of 0.08 , with equivalent performance to the three operators ( with a williams ' index of 1.03 ) , and outperforming a u-net architecture without the need for batch normalisation . we conclude that the proposed fully automatic method achieved equivalent accuracy in segmenting the pelvic floor levator hiatus compared to a previous semi-automatic approach .", "topics": ["time complexity", "nonlinear system"]}
{"title": "a comparison of different machine transliteration models", "abstract": "machine transliteration is a method for automatically converting words in one language into phonetically equivalent ones in another language . machine transliteration plays an important role in natural language applications such as information retrieval and machine translation , especially for handling proper nouns and technical terms . four machine transliteration models -- grapheme-based transliteration model , phoneme-based transliteration model , hybrid transliteration model , and correspondence-based transliteration model -- have been proposed by several researchers . to date , however , there has been little research on a framework in which multiple transliteration models can operate simultaneously . furthermore , there has been no comparison of the four models within the same framework and using the same data . we addressed these problems by 1 ) modeling the four models within the same framework , 2 ) comparing them under the same conditions , and 3 ) developing a way to improve machine transliteration through this comparison . our comparison showed that the hybrid and correspondence-based models were the most effective and that the four models can be used in a complementary manner to improve machine transliteration performance .", "topics": ["machine translation", "natural language"]}
{"title": "consistency of cheeger and ratio graph cuts", "abstract": "this paper establishes the consistency of a family of graph-cut-based algorithms for clustering of data clouds . we consider point clouds obtained as samples of a ground-truth measure . we investigate approaches to clustering based on minimizing objective functionals defined on proximity graphs of the given sample . our focus is on functionals based on graph cuts like the cheeger and ratio cuts . we show that minimizers of the these cuts converge as the sample size increases to a minimizer of a corresponding continuum cut ( which partitions the ground truth measure ) . moreover , we obtain sharp conditions on how the connectivity radius can be scaled with respect to the number of sample points for the consistency to hold . we provide results for two-way and for multiway cuts . furthermore we provide numerical experiments that illustrate the results and explore the optimality of scaling in dimension two .", "topics": ["cluster analysis", "numerical analysis"]}
{"title": "distilling an ensemble of greedy dependency parsers into one mst parser", "abstract": "we introduce two first-order graph-based dependency parsers achieving a new state of the art . the first is a consensus parser built from an ensemble of independently trained greedy lstm transition-based parsers with different random initializations . we cast this approach as minimum bayes risk decoding ( under the hamming cost ) and argue that weaker consensus within the ensemble is a useful signal of difficulty or ambiguity . the second parser is a `` distillation '' of the ensemble into a single model . we train the distillation parser using a structured hinge loss objective with a novel cost that incorporates ensemble uncertainty estimates for each possible attachment , thereby avoiding the intractable cross-entropy computations required by applying standard distillation objectives to problems with structured outputs . the first-order distillation parser matches or surpasses the state of the art on english , chinese , and german .", "topics": ["parsing", "computation"]}
{"title": "ec3 : combining clustering and classification for ensemble learning", "abstract": "classification and clustering algorithms have been proved to be successful individually in different contexts . both of them have their own advantages and limitations . for instance , although classification algorithms are more powerful than clustering methods in predicting class labels of objects , they do not perform well when there is a lack of sufficient manually labeled reliable data . on the other hand , although clustering algorithms do not produce label information for objects , they provide supplementary constraints ( e.g . , if two objects are clustered together , it is more likely that the same label is assigned to both of them ) that one can leverage for label prediction of a set of unknown objects . therefore , systematic utilization of both these types of algorithms together can lead to better prediction performance . in this paper , we propose a novel algorithm , called ec3 that merges classification and clustering together in order to support both binary and multi-class classification . ec3 is based on a principled combination of multiple classification and multiple clustering methods using an optimization function . we theoretically show the convexity and optimality of the problem and solve it by block coordinate descent method . we additionally propose iec3 , a variant of ec3 that handles imbalanced training data . we perform an extensive experimental analysis by comparing ec3 and iec3 with 14 baseline methods ( 7 well-known standalone classifiers , 5 ensemble classifiers , and 2 existing methods that merge classification and clustering ) on 13 standard benchmark datasets . we show that our methods outperform other baselines for every single dataset , achieving at most 10 % higher auc . moreover our methods are faster ( 1.21 times faster than the best baseline ) , more resilient to noise and class imbalance than the best baseline method .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "atomnet : a deep convolutional neural network for bioactivity prediction in structure-based drug discovery", "abstract": "deep convolutional neural networks comprise a subclass of deep neural networks ( dnn ) with a constrained architecture that leverages the spatial and temporal structure of the domain they model . convolutional networks achieve the best predictive performance in areas such as speech and image recognition by hierarchically composing simple local features into complex models . although dnns have been used in drug discovery for qsar and ligand-based bioactivity predictions , none of these models have benefited from this powerful convolutional architecture . this paper introduces atomnet , the first structure-based , deep convolutional neural network designed to predict the bioactivity of small molecules for drug discovery applications . we demonstrate how to apply the convolutional concepts of feature locality and hierarchical composition to the modeling of bioactivity and chemical interactions . in further contrast to existing dnn techniques , we show that atomnet 's application of local convolutional filters to structural target information successfully predicts new active molecules for targets with no previously known modulators . finally , we show that atomnet outperforms previous docking approaches on a diverse set of benchmarks by a large margin , achieving an auc greater than 0.9 on 57.8 % of the targets in the dude benchmark .", "topics": ["computer vision", "interaction"]}
{"title": "robust principal component analysis on graphs", "abstract": "principal component analysis ( pca ) is the most widely used tool for linear dimensionality reduction and clustering . still it is highly sensitive to outliers and does not scale well with respect to the number of data samples . robust pca solves the first issue with a sparse penalty term . the second issue can be handled with the matrix factorization model , which is however non-convex . besides , pca based clustering can also be enhanced by using a graph of data similarity . in this article , we introduce a new model called `` robust pca on graphs '' which incorporates spectral graph regularization into the robust pca framework . our proposed model benefits from 1 ) the robustness of principal components to occlusions and missing values , 2 ) enhanced low-rank recovery , 3 ) improved clustering property due to the graph smoothness assumption on the low-rank matrix , and 4 ) convexity of the resulting optimization problem . extensive experiments on 8 benchmark , 3 video and 2 artificial datasets with corruptions clearly reveal that our model outperforms 10 other state-of-the-art models in its clustering and low-rank recovery tasks .", "topics": ["cluster analysis", "optimization problem"]}
{"title": "multi-scale video frame-synthesis network with transitive consistency loss", "abstract": "traditional approaches to interpolate/extrapolate frames in a video sequence require accurate pixel correspondences between images , e.g . , using optical flow . their results stem on the accuracy of optical flow estimation , and could generate heavy artifacts when flow estimation failed . recently methods using auto-encoder has shown impressive progress , however they are usually trained for specific interpolation/extrapolation settings and lack of flexibility and in order to reduce these limitations , we propose a unified network to parameterize the interest frame position and therefore infer interpolate/extrapolate frames within the same framework . to achieve this , we introduce a transitive consistency loss to better regularize the network . we adopt a multi-scale structure for the network so that the parameters can be shared across multi-layers . our approach avoids expensive global optimization of optical flow methods , and is efficient and flexible for video interpolation/extrapolation applications . experimental results have shown that our method performs favorably against state-of-the-art methods .", "topics": ["encoder", "autoencoder"]}
{"title": "a report on multilinear pca plus multilinear lda to deal with tensorial data : visual classification as an example", "abstract": "in practical applications , we often have to deal with high order data , such as a grayscale image and a video sequence are intrinsically 2nd-order tensor and 3rd-order tensor , respectively . for doing clustering or classification of these high order data , it is a conventional way to vectorize these data before hand , as pca or fda does , which often induce the curse of dimensionality problem . for this reason , experts have developed many methods to deal with the tensorial data , such as multilinear pca , multilinear lda , and so on . in this paper , we still address the problem of high order data representation and recognition , and propose to study the result of merging multilinear pca and multilinear lda into one scenario , we name it \\textbf { gda } for the abbreviation of generalized discriminant analysis . to evaluate gda , we perform a series of experiments , and the experimental results demonstrate our gda outperforms a selection of competing methods such ( 2d ) $ ^2 $ pca , ( 2d ) $ ^2 $ lda , and mda .", "topics": ["cluster analysis"]}
{"title": "first order decision diagrams for relational mdps", "abstract": "markov decision processes capture sequential decision making under uncertainty , where an agent must choose actions so as to optimize long term reward . the paper studies efficient reasoning mechanisms for relational markov decision processes ( rmdp ) where world states have an internal relational structure that can be naturally described in terms of objects and relations among them . two contributions are presented . first , the paper develops first order decision diagrams ( fodd ) , a new compact representation for functions over relational structures , together with a set of operators to combine fodds , and novel reduction techniques to keep the representation small . second , the paper shows how fodds can be used to develop solutions for rmdps , where reasoning is performed at the abstract level and the resulting optimal policy is independent of domain size ( number of objects ) or instantiation . in particular , a variant of the value iteration algorithm is developed by using special operations over fodds , and the algorithm is shown to converge to the optimal policy .", "topics": ["iteration"]}
{"title": "being robust ( in high dimensions ) can be practical", "abstract": "robust estimation is much more challenging in high dimensions than it is in one dimension : most techniques either lead to intractable optimization problems or estimators that can tolerate only a tiny fraction of errors . recent work in theoretical computer science has shown that , in appropriate distributional models , it is possible to robustly estimate the mean and covariance with polynomial time algorithms that can tolerate a constant fraction of corruptions , independent of the dimension . however , the sample and time complexity of these algorithms is prohibitively large for high-dimensional applications . in this work , we address both of these issues by establishing sample complexity bounds that are optimal , up to logarithmic factors , as well as giving various refinements that allow the algorithms to tolerate a much larger fraction of corruptions . finally , we show on both synthetic and real data that our algorithms have state-of-the-art performance and suddenly make high-dimensional robust estimation a realistic possibility .", "topics": ["time complexity", "synthetic data"]}
