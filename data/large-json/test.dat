{"title": "improving term extraction with terminological resources", "abstract": "studies of different term extractors on a corpus of the biomedical domain revealed decreasing performances when applied to highly technical texts . the difficulty or impossibility of customising them to new domains is an additional limitation . in this paper , we propose to use external terminologies to influence generic linguistic data in order to augment the quality of the extraction . the tool we implemented exploits testified terms at different steps of the process : chunking , parsing and extraction of term candidates . experiments reported here show that , using this method , more term candidates can be acquired with a higher level of reliability . we further describe the extraction process involving endogenous disambiguation implemented in the term extractor yatea .", "topics": ["text corpus"]}
{"title": "guided co-training for large-scale multi-view spectral clustering", "abstract": "in many real-world applications , we have access to multiple views of the data , each of which characterizes the data from a distinct aspect . several previous algorithms have demonstrated that one can achieve better clustering accuracy by integrating information from all views appropriately than using only an individual view . owing to the effectiveness of spectral clustering , many multi-view clustering methods are based on it . unfortunately , they have limited applicability to large-scale data due to the high computational complexity of spectral clustering . in this work , we propose a novel multi-view spectral clustering method for large-scale data . our approach is structured under the guided co-training scheme to fuse distinct views , and uses the sampling technique to accelerate spectral clustering . more specifically , we first select $ p $ ( $ \\ll n $ ) landmark points and then approximate the eigen-decomposition accordingly . the augmented view , which is essential to guided co-training process , can then be quickly determined by our method . the proposed algorithm scales linearly with the number of given data . extensive experiments have been performed and the results support the advantage of our method for handling the large-scale multi-view situation .", "topics": ["cluster analysis", "approximation algorithm"]}
{"title": "deep learning markov random field for semantic segmentation", "abstract": "semantic segmentation tasks can be well modeled by markov random field ( mrf ) . this paper addresses semantic segmentation by incorporating high-order relations and mixture of label contexts into mrf . unlike previous works that optimized mrfs using iterative algorithm , we solve mrf by proposing a convolutional neural network ( cnn ) , namely deep parsing network ( dpn ) , which enables deterministic end-to-end computation in a single forward pass . specifically , dpn extends a contemporary cnn to model unary terms and additional layers are devised to approximate the mean field ( mf ) algorithm for pairwise terms . it has several appealing properties . first , different from the recent works that required many iterations of mf during back-propagation , dpn is able to achieve high performance by approximating one iteration of mf . second , dpn represents various types of pairwise terms , making many existing models as its special cases . furthermore , pairwise terms in dpn provide a unified framework to encode rich contextual information in high-dimensional data , such as images and videos . third , dpn makes mf easier to be parallelized and speeded up , thus enabling efficient inference . dpn is thoroughly evaluated on standard semantic image/video segmentation benchmarks , where a single dpn model yields state-of-the-art segmentation accuracies on pascal voc 2012 , cityscapes dataset and camvid dataset .", "topics": ["approximation algorithm", "parsing"]}
{"title": "regal : a regularization based algorithm for reinforcement learning in weakly communicating mdps", "abstract": "we provide an algorithm that achieves the optimal regret rate in an unknown weakly communicating markov decision process ( mdp ) . the algorithm proceeds in episodes where , in each episode , it picks a policy using regularization based on the span of the optimal bias vector . for an mdp with s states and a actions whose optimal bias vector has span bounded by h , we show a regret bound of ~o ( hspat ) . we also relate the span to various diameter-like quantities associated with the mdp , demonstrating how our results improve on previous regret bounds .", "topics": ["regret ( decision theory )", "reinforcement learning"]}
{"title": "the role of the gricean maxims in the generation of referring expressions", "abstract": "grice 's maxims of conversation [ grice 1975 ] are framed as directives to be followed by a speaker of the language . this paper argues that , when considered from the point of view of natural language generation , such a characterisation is rather misleading , and that the desired behaviour falls out quite naturally if we view language generation as a goal-oriented process . we argue this position with particular regard to the generation of referring expressions .", "topics": ["natural language"]}
{"title": "joint calibration of ensemble of exemplar svms", "abstract": "we present a method for calibrating the ensemble of exemplar svms model . unlike the standard approach , which calibrates each svm independently , our method optimizes their joint performance as an ensemble . we formulate joint calibration as a constrained optimization problem and devise an efficient optimization algorithm to find its global optimum . the algorithm dynamically discards parts of the solution space that can not contain the optimum early on , making the optimization computationally feasible . we experiment with ee-svm trained on state-of-the-art cnn descriptors . results on the ilsvrc 2014 and pascal voc 2007 datasets show that ( i ) our joint calibration procedure outperforms independent calibration on the task of classifying windows as belonging to an object class or not ; and ( ii ) this improved window classifier leads to better performance on the object detection task .", "topics": ["optimization problem", "mathematical optimization"]}
{"title": "is the deconvolution layer the same as a convolutional layer ?", "abstract": "in this note , we want to focus on aspects related to two questions most people asked us at cvpr about the network we presented . firstly , what is the relationship between our proposed layer and the deconvolution layer ? and secondly , why are convolutions in low-resolution ( lr ) space a better choice ? these are key questions we tried to answer in the paper , but we were not able to go into as much depth and clarity as we would have liked in the space allowance . to better answer these questions in this note , we first discuss the relationships between the deconvolution layer in the forms of the transposed convolution layer , the sub-pixel convolutional layer and our efficient sub-pixel convolutional layer . we will refer to our efficient sub-pixel convolutional layer as a convolutional layer in lr space to distinguish it from the common sub-pixel convolutional layer . we will then show that for a fixed computational budget and complexity , a network with convolutions exclusively in lr space has more representation power at the same speed than a network that first upsamples the input in high resolution space .", "topics": ["convolution", "pixel"]}
{"title": "action classification and highlighting in videos", "abstract": "inspired by recent advances in neural machine translation , that jointly align and translate using encoder-decoder networks equipped with attention , we propose an attentionbased lstm model for human activity recognition . our model jointly learns to classify actions and highlight frames associated with the action , by attending to salient visual information through a jointly learned soft-attention networks . we explore attention informed by various forms of visual semantic features , including those encoding actions , objects and scenes . we qualitatively show that soft-attention can learn to effectively attend to important objects and scene information correlated with specific human actions . further , we show that , quantitatively , our attention-based lstm outperforms the vanilla lstm and cnn models used by stateof-the-art methods . on a large-scale youtube video dataset , activitynet , our model outperforms competing methods in action classification .", "topics": ["machine translation", "encoder"]}
{"title": "fine-tuning cnn image retrieval with no human annotation", "abstract": "image descriptors based on activations of convolutional neural networks ( cnns ) have become dominant in image retrieval due to their discriminative power , compactness of the representation , and the efficiency of search . training of cnns , either from scratch or fine-tuning , requires a large amount of annotated data , where high quality of the annotation is often crucial . in this work , we propose to fine-tune cnns for image retrieval on a large collection of unordered images in a fully automatic manner . reconstructed 3d models , obtained by the state-of-the-art retrieval and structure-from-motion methods , guide the selection of the training data . we show that both hard positive and hard negative examples , selected by exploiting the geometry and the camera positions available from the 3d models , enhance the performance in particular object retrieval . cnn descriptor whitening discriminatively learned from the same training data outperforms the commonly used pca whitening . we propose a novel trainable generalized-mean ( gem ) pooling layer that generalizes max and average pooling and show that it boosts retrieval performance . applying the proposed method on vgg network achieves state-of-the-art performance on standard benchmarks : oxford buildings , paris , and holidays datasets .", "topics": ["test set"]}
{"title": "learning to transduce with unbounded memory", "abstract": "recently , strong results have been demonstrated by deep recurrent neural networks on natural language transduction problems . in this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation . these experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as stacks , queues , and deques . we show that these architectures exhibit superior generalisation performance to deep rnns and are often able to learn the underlying generating algorithms in our transduction experiments .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "birnet : brain image registration using dual-supervised fully convolutional networks", "abstract": "in this paper , we propose a deep learning approach for image registration by predicting deformation from image appearance . since obtaining ground-truth deformation fields for training can be challenging , we design a fully convolutional network that is subject to dual-guidance : ( 1 ) coarse guidance using deformation fields obtained by an existing registration method ; and ( 2 ) fine guidance using image similarity . the latter guidance helps avoid overly relying on the supervision from the training deformation fields , which could be inaccurate . for effective training , we further improve the deep convolutional network with gap filling , hierarchical loss , and multi-source strategies . experiments on a variety of datasets show promising registration accuracy and efficiency compared with state-of-the-art methods .", "topics": ["ground truth"]}
{"title": "coactive learning for locally optimal problem solving", "abstract": "coactive learning is an online problem solving setting where the solutions provided by a solver are interactively improved by a domain expert , which in turn drives learning . in this paper we extend the study of coactive learning to problems where obtaining a globally optimal or near-optimal solution may be intractable or where an expert can only be expected to make small , local improvements to a candidate solution . the goal of learning in this new setting is to minimize the cost as measured by the expert effort over time . we first establish theoretical bounds on the average cost of the existing coactive perceptron algorithm . in addition , we consider new online algorithms that use cost-sensitive and passive-aggressive ( pa ) updates , showing similar or improved theoretical bounds . we provide an empirical evaluation of the learners in various domains , which show that the perceptron based algorithms are quite effective and that unlike the case for online classification , the pa algorithms do not yield significant performance gains .", "topics": ["optimization problem"]}
{"title": "document image classification with intra-domain transfer learning and stacked generalization of deep convolutional neural networks", "abstract": "in this work , a region-based deep convolutional neural network framework is proposed for document structure learning . the contribution of this work involves efficient training of region based classifiers and effective ensembling for document image classification . a primary level of `inter-domain ' transfer learning is used by exporting weights from a pre-trained vgg16 architecture on the imagenet dataset to train a document classifier on whole document images . exploiting the nature of region based influence modelling , a secondary level of `intra-domain ' transfer learning is used for rapid training of deep learning models for image segments . finally , stacked generalization based ensembling is utilized for combining the predictions of the base deep neural network models . the proposed method achieves state-of-the-art accuracy of 92.2 % on the popular rvl-cdip document image dataset , exceeding benchmarks set by existing algorithms .", "topics": ["computer vision"]}
{"title": "deep kalman filters", "abstract": "kalman filters are one of the most influential models of time-varying phenomena . they admit an intuitive probabilistic interpretation , have a simple functional form , and enjoy widespread adoption in a variety of disciplines . motivated by recent variational methods for learning deep generative models , we introduce a unified algorithm to efficiently learn a broad spectrum of kalman filters . of particular interest is the use of temporal generative models for counterfactual inference . we investigate the efficacy of such models for counterfactual inference , and to that end we introduce the `` healing mnist '' dataset where long-term structure , noise and actions are applied to sequences of digits . we show the efficacy of our method for modeling this dataset . we further show how our model can be used for counterfactual inference for patients , based on electronic health record data of 8,000 patients over 4.5 years .", "topics": ["calculus of variations", "mnist database"]}
{"title": "a bayesian approach to policy recognition and state representation learning", "abstract": "learning from demonstration ( lfd ) is the process of building behavioral models of a task from demonstrations provided by an expert . these models can be used e.g . for system control by generalizing the expert demonstrations to previously unencountered situations . most lfd methods , however , make strong assumptions about the expert behavior , e.g . they assume the existence of a deterministic optimal ground truth policy or require direct monitoring of the expert 's controls , which limits their practical use as part of a general system identification framework . in this work , we consider the lfd problem in a more general setting where we allow for arbitrary stochastic expert policies , without reasoning about the optimality of the demonstrations . following a bayesian methodology , we model the full posterior distribution of possible expert controllers that explain the provided demonstration data . moreover , we show that our methodology can be applied in a nonparametric context to infer the complexity of the state representation used by the expert , and to learn task-appropriate partitionings of the system state space .", "topics": ["ground truth"]}
{"title": "holistic planimetric prediction to local volumetric prediction for 3d human pose estimation", "abstract": "we propose a novel approach to 3d human pose estimation from a single depth map . recently , convolutional neural network ( cnn ) has become a powerful paradigm in computer vision . many of computer vision tasks have benefited from cnns , however , the conventional approach to directly regress 3d body joint locations from an image does not yield a noticeably improved performance . in contrast , we formulate the problem as estimating per-voxel likelihood of key body joints from a 3d occupancy grid . we argue that learning a mapping from volumetric input to volumetric output with 3d convolution consistently improves the accuracy when compared to learning a regression from depth map to 3d joint coordinates . we propose a two-stage approach to reduce the computational overhead caused by volumetric representation and 3d convolution : holistic 2d prediction and local 3d prediction . in the first stage , planimetric network ( p-net ) estimates per-pixel likelihood for each body joint in the holistic 2d space . in the second stage , volumetric network ( v-net ) estimates the per-voxel likelihood of each body joints in the local 3d space around the 2d estimations of the first stage , effectively reducing the computational cost . our model outperforms existing methods by a large margin in publicly available datasets .", "topics": ["computer vision", "convolution"]}
{"title": "motion primitives for robotic flight control", "abstract": "we introduce a simple framework for learning aggressive maneuvers in flight control of uavs . having inspired from biological environment , dynamic movement primitives are analyzed and extended using nonlinear contraction theory . accordingly , primitives of an observed movement are stably combined and concatenated . we demonstrate our results experimentally on the quanser helicopter , in which we first imitate aggressive maneuvers and then use them as primitives to achieve new maneuvers that can fly over an obstacle .", "topics": ["nonlinear system"]}
{"title": "multiscale adaptive representation of signals : i . the basic framework", "abstract": "we introduce a framework for designing multi-scale , adaptive , shift-invariant frames and bi-frames for representing signals . the new framework , called adaframe , improves over dictionary learning-based techniques in terms of computational efficiency at inference time . it improves classical multi-scale basis such as wavelet frames in terms of coding efficiency . it provides an attractive alternative to dictionary learning-based techniques for low level signal processing tasks , such as compression and denoising , as well as high level tasks , such as feature extraction for object recognition . connections with deep convolutional networks are also discussed . in particular , the proposed framework reveals a drawback in the commonly used approach for visualizing the activations of the intermediate layers in convolutional networks , and suggests a natural alternative .", "topics": ["feature extraction", "high- and low-level"]}
{"title": "learning deep disentangled embeddings with the f-statistic loss", "abstract": "deep-embedding methods aim to discover representations of a domain that make explicit the domain 's class structure . disentangling methods aim to make explicit compositional or factorial structure . we combine these two active but independent lines of research and propose a new paradigm for discovering disentangled representations of class structure ; these representations reveal the underlying factors that jointly determine class . we propose and evaluate a novel loss function based on the $ f $ statistic , which describes the separation of two or more distributions . by ensuring that distinct classes are well separated on a subset of embedding dimensions , we obtain embeddings that are useful for few-shot learning . by not requiring separation on all dimensions , we encourage the discovery of disentangled representations . our embedding procedure matches or beats state-of-the-art procedures on deep embeddings , as evaluated by performance on recall @ $ k $ and few-shot learning tasks . to evaluate alternative approaches on disentangling , we formalize two key properties of a disentangled representation : modularity and explicitness . by these criteria , our procedure yields disentangled representations , whereas traditional procedures fail . the goal of our work is to obtain more interpretable , manipulable , and generalizable deep representations of concepts and categories .", "topics": ["statistical classification", "unsupervised learning"]}
{"title": "text network exploration via heterogeneous web of topics", "abstract": "a text network refers to a data type that each vertex is associated with a text document and the relationship between documents is represented by edges . the proliferation of text networks such as hyperlinked webpages and academic citation networks has led to an increasing demand for quickly developing a general sense of a new text network , namely text network exploration . in this paper , we address the problem of text network exploration through constructing a heterogeneous web of topics , which allows people to investigate a text network associating word level with document level . to achieve this , a probabilistic generative model for text and links is proposed , where three different relationships in the heterogeneous topic web are quantified . we also develop a prototype demo system named topicatlas to exhibit such heterogeneous topic web , and demonstrate how this system can facilitate the task of text network exploration . extensive qualitative analyses are included to verify the effectiveness of this heterogeneous topic web . besides , we validate our model on real-life text networks , showing that it preserves good performance on objective evaluation metrics .", "topics": ["generative model"]}
{"title": "probabilistic diffeomorphic registration : representing uncertainty", "abstract": "this paper presents a novel mathematical framework for representing uncertainty in large deformation diffeomorphic image registration . the bayesian posterior distribution over the deformations aligning a moving and a fixed image is approximated via a variational formulation . a stochastic differential equation ( sde ) modeling the deformations as the evolution of a time-varying velocity field leads to a prior density over deformations in the form of a gaussian process . this permits estimating the full posterior distribution in order to represent uncertainty , in contrast to methods in which the posterior is approximated via monte carlo sampling or maximized in maximum a-posteriori ( map ) estimation . the frame-work is demonstrated in the case of landmark-based image registration , including simulated data and annotated pre and intra-operative 3d images .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "a novel approach of harris corner detection of noisy images using adaptive wavelet thresholding technique", "abstract": "in this paper we propose a method of corner detection for obtaining features which is required to track and recognize objects within a noisy image . corner detection of noisy images is a challenging task in image processing . natural images often get corrupted by noise during acquisition and transmission . though corner detection of these noisy images does not provide desired results , hence de-noising is required . adaptive wavelet thresholding approach is applied for the same .", "topics": ["image processing", "noise reduction"]}
{"title": "adapting phrase-based machine translation to normalise medical terms in social media messages", "abstract": "previous studies have shown that health reports in social media , such as dailystrength and twitter , have potential for monitoring health conditions ( e.g . adverse drug reactions , infectious diseases ) in particular communities . however , in order for a machine to understand and make inferences on these health conditions , the ability to recognise when laymen 's terms refer to a particular medical concept ( i.e.\\ text normalisation ) is required . to achieve this , we propose to adapt an existing phrase-based machine translation ( mt ) technique and a vector representation of words to map between a social media phrase and a medical concept . we evaluate our proposed approach using a collection of phrases from tweets related to adverse drug reactions . our experimental results show that the combination of a phrase-based mt technique and the similarity between word vector representations outperforms the baselines that apply only either of them by up to 55 % .", "topics": ["baseline ( configuration management )", "machine translation"]}
{"title": "structural compression of convolutional neural networks based on greedy filter pruning", "abstract": "convolutional neural networks ( cnns ) have state-of-the-art performance on many problems in machine vision . however , networks with superior performance often have millions of weights so that it is difficult or impossible to use cnns on computationally limited devices or to humanly interpret them . a myriad of cnn compression approaches have been proposed and they involve pruning and compressing the weights and filters . in this article , we introduce a greedy structural compression scheme that prunes filters in a trained cnn . we define a filter importance index equal to the classification accuracy reduction ( car ) of the network after pruning that filter ( similarly defined as rar for regression ) . we then iteratively prune filters based on the car index . this algorithm achieves substantially higher classification accuracy in alexnet compared to other structural compression schemes that prune filters . pruning half of the filters in the first or second layer of alexnet , our car algorithm achieves 26 % and 20 % higher classification accuracies respectively , compared to the best benchmark filter pruning scheme . our car algorithm , combined with further weight pruning and compressing , reduces the size of first or second convolutional layer in alexnet by a factor of 42 , while achieving close to original classification accuracy through retraining ( or fine-tuning ) network . finally , we demonstrate the interpretability of car-compressed cnns by showing that our algorithm prunes filters with visually redundant functionalities . in fact , out of top 20 car-pruned filters in alexnet , 17 of them in the first layer and 14 of them in the second layer are color-selective filters as opposed to shape-selective filters . to our knowledge , this is the first reported result on the connection between compression and interpretability of cnns .", "topics": ["neural networks"]}
{"title": "active learning for visual question answering : an empirical study", "abstract": "we present an empirical study of active learning for visual question answering , where a deep vqa model selects informative question-image pairs from a pool and queries an oracle for answers to maximally improve its performance under a limited query budget . drawing analogies from human learning , we explore cramming ( entropy ) , curiosity-driven ( expected model change ) , and goal-driven ( expected error reduction ) active learning approaches , and propose a fast and effective goal-driven active learning scoring function to pick question-image pairs for deep vqa models under the bayesian neural network framework . we find that deep vqa models need large amounts of training data before they can start asking informative questions . but once they do , all three approaches outperform the random selection baseline and achieve significant query savings . for the scenario where the model is allowed to ask generic questions about images but is evaluated only on specific questions ( e.g . , questions whose answer is either yes or no ) , our proposed goal-driven scoring function performs the best .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "pixelnet : towards a general pixel-level architecture", "abstract": "we explore architectures for general pixel-level prediction problems , from low-level edge detection to mid-level surface normal estimation to high-level semantic segmentation . convolutional predictors , such as the fully-convolutional network ( fcn ) , have achieved remarkable success by exploiting the spatial redundancy of neighboring pixels through convolutional processing . though computationally efficient , we point out that such approaches are not statistically efficient during learning precisely because spatial redundancy limits the information learned from neighboring pixels . we demonstrate that ( 1 ) stratified sampling allows us to add diversity during batch updates and ( 2 ) sampled multi-scale features allow us to explore more nonlinear predictors ( multiple fully-connected layers followed by relu ) that improve overall accuracy . finally , our objective is to show how a architecture can get performance better than ( or comparable to ) the architectures designed for a particular task . interestingly , our single architecture produces state-of-the-art results for semantic segmentation on pascal-context , surface normal estimation on nyudv2 dataset , and edge detection on bsds without contextual post-processing .", "topics": ["high- and low-level", "computational complexity theory"]}
{"title": "scene recognition by combining local and global image descriptors", "abstract": "object recognition is an important problem in computer vision , having diverse applications . in this work , we construct an end-to-end scene recognition pipeline consisting of feature extraction , encoding , pooling and classification . our approach simultaneously utilize global feature descriptors as well as local feature descriptors from images , to form a hybrid feature descriptor corresponding to each image . we utilize daisy features associated with key points within images as our local feature descriptor and histogram of oriented gradients ( hog ) corresponding to an entire image as a global descriptor . we make use of a bag-of-visual-words encoding and apply mini- batch k-means algorithm to reduce the complexity of our feature encoding scheme . a 2-level pooling procedure is used to combine daisy and hog features corresponding to each image . finally , we experiment with a multi-class svm classifier with several kernels , in a cross-validation setting , and tabulate our results on the fifteen scene categories dataset . the average accuracy of our model was 76.4 % in the case of a 40 % -60 % random split of images into training and testing datasets respectively . the primary objective of this work is to clearly outline the practical implementation of a basic screne-recognition pipeline having a reasonable accuracy , in python , using open-source libraries . a full implementation of the proposed model is available in our github repository .", "topics": ["feature extraction", "computer vision"]}
{"title": "detecting visual relationships with deep relational networks", "abstract": "relationships among objects play a crucial role in image understanding . despite the great success of deep learning techniques in recognizing individual objects , reasoning about the relationships among objects remains a challenging task . previous methods often treat this as a classification problem , considering each type of relationship ( e.g . `` ride '' ) or each distinct visual phrase ( e.g . `` person-ride-horse '' ) as a category . such approaches are faced with significant difficulties caused by the high diversity of visual appearance for each kind of relationships or the large number of distinct visual phrases . we propose an integrated framework to tackle this problem . at the heart of this framework is the deep relational network , a novel formulation designed specifically for exploiting the statistical dependencies between objects and their relationships . on two large datasets , the proposed method achieves substantial improvement over state-of-the-art .", "topics": ["computer vision", "causality"]}
{"title": "pac-bayesian theorems for domain adaptation with specialization to linear classifiers", "abstract": "in this paper , we provide two main contributions in pac-bayesian theory for domain adaptation where the objective is to learn , from a source distribution , a well-performing majority vote on a different target distribution . on the one hand , we propose an improvement of the previous approach proposed by germain et al . ( 2013 ) , that relies on a novel distribution pseudodistance based on a disagreement averaging , allowing us to derive a new tighter pac-bayesian domain adaptation bound for the stochastic gibbs classifier . we specialize it to linear classifiers , and design a learning algorithm which shows interesting results on a synthetic problem and on a popular sentiment annotation task . on the other hand , we generalize these results to multisource domain adaptation allowing us to take into account different source domains . this study opens the door to tackle domain adaptation tasks by making use of all the pac-bayesian tools .", "topics": ["synthetic data"]}
{"title": "ontology verbalization using semantic-refinement", "abstract": "we propose a rule-based technique to generate redundancy-free nl descriptions of owl entities.the existing approaches which address the problem of verbalizing owl ontologies generate nl text segments which are close to their counterpart owl statements.some of these approaches also perform grouping and aggregating of these nl text segments to generate a more fluent and comprehensive form of the content.restricting our attention to description of individuals and concepts , we find that the approach currently followed in the available tools is that of determining the set of all logical conditions that are satisfied by the given individual/concept name and translate these conditions verbatim into corresponding nl descriptions.human-understandability of such descriptions is affected by the presence of repetitions and redundancies , as they have high fidelity to their owl representation.in the literature , no efforts had been taken to remove redundancies and repetitions at the logical-level before generating the nl descriptions of entities and we find this to be the main reason for lack of readability of the generated text.herein , we propose a technique called semantic-refinement ( sr ) to generate meaningful and easily-understandable descriptions of individuals and concepts of a given owlontology.we identify the combinations of owl/dl constructs that lead to repetitive/redundant descriptions and propose a series of refinement rules to rewrite the conditions that are satisfied by an individual/concept in a meaning-preserving manner.the reduced set of conditions are then employed for generating nl descriptions.our experiments show that , sr leads to significantly improved descriptions of ontology entities.we also test the effectiveness and usefulness of the the generated descriptions for the purpose of validating the ontologies and find that the proposed technique is indeed helpful in the context .", "topics": ["natural language", "entity"]}
{"title": "model-driven simulations for deep convolutional neural networks", "abstract": "the use of simulated virtual environments to train deep convolutional neural networks ( cnn ) is a currently active practice to reduce the ( real ) data-hungriness of the deep cnn models , especially in application domains in which large scale real data and/or groundtruth acquisition is difficult or laborious . recent approaches have attempted to harness the capabilities of existing video games , animated movies to provide training data with high precision groundtruth . however , a stumbling block is in how one can certify generalization of the learned models and their usefulness in real world data sets . this opens up fundamental questions such as : what is the role of photorealism of graphics simulations in training cnn models ? are the trained models valid in reality ? what are possible ways to reduce the performance bias ? in this work , we begin to address theses issues systematically in the context of urban semantic understanding with cnns . towards this end , we ( a ) propose a simple probabilistic urban scene model , ( b ) develop a parametric rendering tool to synthesize the data with groundtruth , followed by ( c ) a systematic exploration of the impact of level-of-realism on the generality of the trained cnn model to real world ; and domain adaptation concepts to minimize the performance bias .", "topics": ["test set", "simulation"]}
{"title": "sensitivity analysis in decision circuits", "abstract": "decision circuits have been developed to perform efficient evaluation of influence diagrams [ bhattacharjya and shachter , 2007 ] , building on the advances in arithmetic circuits for belief network inference [ darwiche,2003 ] . in the process of model building and analysis , we perform sensitivity analysis to understand how the optimal solution changes in response to changes in the model . when sequential decision problems under uncertainty are represented as decision circuits , we can exploit the efficient solution process embodied in the decision circuit and the wealth of derivative information available to compute the value of information for the uncertainties in the problem and the effects of changes to model parameters on the value and the optimal strategy .", "topics": ["optimization problem", "bayesian network"]}
{"title": "a deep learning approach to unsupervised ensemble learning", "abstract": "we show how deep learning methods can be applied in the context of crowdsourcing and unsupervised ensemble learning . first , we prove that the popular model of dawid and skene , which assumes that all classifiers are conditionally independent , is { \\em equivalent } to a restricted boltzmann machine ( rbm ) with a single hidden node . hence , under this model , the posterior probabilities of the true labels can be instead estimated via a trained rbm . next , to address the more general case , where classifiers may strongly violate the conditional independence assumption , we propose to apply rbm-based deep neural net ( dnn ) . experimental results on various simulated and real-world datasets demonstrate that our proposed dnn approach outperforms other state-of-the-art methods , in particular when the data violates the conditional independence assumption .", "topics": ["simulation"]}
{"title": "functional frank-wolfe boosting for general loss functions", "abstract": "boosting is a generic learning method for classification and regression . yet , as the number of base hypotheses becomes larger , boosting can lead to a deterioration of test performance . overfitting is an important and ubiquitous phenomenon , especially in regression settings . to avoid overfitting , we consider using $ l_1 $ regularization . we propose a novel frank-wolfe type boosting algorithm ( fwboost ) applied to general loss functions . by using exponential loss , the fwboost algorithm can be rewritten as a variant of adaboost for binary classification . fwboost algorithms have exactly the same form as existing boosting methods , in terms of making calls to a base learning algorithm with different weights update . this direct connection between boosting and frank-wolfe yields a new algorithm that is as practical as existing boosting methods but with new guarantees and rates of convergence . experimental results show that the test performance of fwboost is not degraded with larger rounds in boosting , which is consistent with the theoretical analysis .", "topics": ["time complexity", "loss function"]}
{"title": "lexis : an optimization framework for discovering the hierarchical structure of sequential data", "abstract": "data represented as strings abounds in biology , linguistics , document mining , web search and many other fields . such data often have a hierarchical structure , either because they were artificially designed and composed in a hierarchical manner or because there is an underlying evolutionary process that creates repeatedly more complex strings from simpler substrings . we propose a framework , referred to as `` lexis '' , that produces an optimized hierarchical representation of a given set of `` target '' strings . the resulting hierarchy , `` lexis-dag '' , shows how to construct each target through the concatenation of intermediate substrings , minimizing the total number of such concatenations or dag edges . the lexis optimization problem is related to the smallest grammar problem . after we prove its np-hardness for two cost formulations , we propose an efficient greedy algorithm for the construction of lexis-dags . we also consider the problem of identifying the set of intermediate nodes ( substrings ) that collectively form the `` core '' of a lexis-dag , which is important in the analysis of lexis-dags . we show that the lexis framework can be applied in diverse applications such as optimized synthesis of dna fragments in genomic libraries , hierarchical structure discovery in protein sequences , dictionary-based text compression , and feature extraction from a set of documents .", "topics": ["optimization problem", "feature extraction"]}
{"title": "graph convolutional matrix completion", "abstract": "we consider matrix completion for recommender systems from the point of view of link prediction on graphs . interaction data such as movie ratings can be represented by a bipartite user-item graph with labeled edges denoting observed ratings . building on recent progress in deep learning on graph-structured data , we propose a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph . our model shows competitive performance on standard collaborative filtering benchmarks . in settings where complimentary feature information or structured data such as a social network is available , our framework outperforms recent state-of-the-art methods .", "topics": ["autoencoder"]}
{"title": "algorithms for learning decomposable models and chordal graphs", "abstract": "decomposable dependency models and their graphical counterparts , i.e . , chordal graphs , possess a number of interesting and useful properties . on the basis of two characterizations of decomposable models in terms of independence relationships , we develop an exact algorithm for recovering the chordal graphical representation of any given decomposable model . we also propose an algorithm for learning chordal approximations of dependency models isomorphic to general undirected graphs .", "topics": ["approximation"]}
{"title": "near-optimal nonmyopic value of information in graphical models", "abstract": "a fundamental issue in real-world systems , such as sensor networks , is the selection of observations which most effectively reduce uncertainty . more specifically , we address the long standing problem of nonmyopically selecting the most informative subset of variables in a graphical model . we present the first efficient randomized algorithm providing a constant factor ( 1-1/e-epsilon ) approximation guarantee for any epsilon > 0 with high confidence . the algorithm leverages the theory of submodular functions , in combination with a polynomial bound on sample complexity . we furthermore prove that no polynomial time algorithm can provide a constant factor approximation better than ( 1 - 1/e ) unless p = np . finally , we provide extensive evidence of the effectiveness of our method on two complex real-world datasets .", "topics": ["graphical model", "time complexity"]}
{"title": "training auto-encoders effectively via eliminating task-irrelevant input variables", "abstract": "auto-encoders are often used as building blocks of deep network classifier to learn feature extractors , but task-irrelevant information in the input data may lead to bad extractors and result in poor generalization performance of the network . in this paper , via dropping the task-irrelevant input variables the performance of auto-encoders can be obviously improved .specifically , an importance-based variable selection method is proposed to aim at finding the task-irrelevant input variables and dropping them.it firstly estimates importance of each variable , and then drops the variables with importance value lower than a threshold . in order to obtain better performance , the method can be employed for each layer of stacked auto-encoders . experimental results show that when combined with our method the stacked denoising auto-encoders achieves significantly improved performance on three challenging datasets .", "topics": ["noise reduction", "relevance"]}
{"title": "exploiting context when learning to classify", "abstract": "this paper addresses the problem of classifying observations when features are context-sensitive , specifically when the testing set involves a context that is different from the training set . the paper begins with a precise definition of the problem , then general strategies are presented for enhancing the performance of classification algorithms on this type of problem . these strategies are tested on two domains . the first domain is the diagnosis of gas turbine engines . the problem is to diagnose a faulty engine in one context , such as warm weather , when the fault has previously been seen only in another context , such as cold weather . the second domain is speech recognition . the problem is to recognize words spoken by a new speaker , not represented in the training set . for both domains , exploiting context results in substantially more accurate classification .", "topics": ["test set", "speech recognition"]}
{"title": "an optimized hybrid approach for path finding", "abstract": "path finding algorithm addresses problem of finding shortest path from source to destination avoiding obstacles . there exist various search algorithms namely a* , dijkstra 's and ant colony optimization . unlike most path finding algorithms which require destination co-ordinates to compute path , the proposed algorithm comprises of a new method which finds path using backtracking without requiring destination co-ordinates . moreover , in existing path finding algorithm , the number of iterations required to find path is large . hence , to overcome this , an algorithm is proposed which reduces number of iterations required to traverse the path . the proposed algorithm is hybrid of backtracking and a new technique ( modified 8- neighbor approach ) . the proposed algorithm can become essential part in location based , network , gaming applications . grid traversal , navigation , gaming applications , mobile robot and artificial intelligence .", "topics": ["iteration", "artificial intelligence"]}
{"title": "self-organising stochastic encoders", "abstract": "the processing of mega-dimensional data , such as images , scales linearly with image size only if fixed size processing windows are used . it would be very useful to be able to automate the process of sizing and interconnecting the processing windows . a stochastic encoder that is an extension of the standard linde-buzo-gray vector quantiser , called a stochastic vector quantiser ( svq ) , includes this required behaviour amongst its emergent properties , because it automatically splits the input space into statistically independent subspaces , which it then separately encodes . various optimal svqs have been obtained , both analytically and numerically . analytic solutions which demonstrate how the input space is split into independent subspaces may be obtained when an svq is used to encode data that lives on a 2-torus ( e.g . the superposition of a pair of uncorrelated sinusoids ) . many numerical solutions have also been obtained , using both svqs and chains of linked svqs : ( 1 ) images of multiple independent targets ( encoders for single targets emerge ) , ( 2 ) images of multiple correlated targets ( various types of encoder for single and multiple targets emerge ) , ( 3 ) superpositions of various waveforms ( encoders for the separate waveforms emerge - this is a type of independent component analysis ( ica ) ) , ( 4 ) maternal and foetal ecgs ( another example of ica ) , ( 5 ) images of textures ( orientation maps and dominance stripes emerge ) . overall , svqs exhibit a rich variety of self-organising behaviour , which effectively discovers the internal structure of the training data . this should have an immediate impact on `` intelligent '' computation , because it reduces the need for expert human intervention in the design of data processing algorithms .", "topics": ["test set", "numerical analysis"]}
{"title": "on the generalization ability of online learning algorithms for pairwise loss functions", "abstract": "in this paper , we study the generalization properties of online learning based stochastic methods for supervised learning problems where the loss function is dependent on more than one training sample ( e.g . , metric learning , ranking ) . we present a generic decoupling technique that enables us to provide rademacher complexity-based generalization error bounds . our bounds are in general tighter than those obtained by wang et al ( colt 2012 ) for the same problem . using our decoupling technique , we are further able to obtain fast convergence rates for strongly convex pairwise loss functions . we are also able to analyze a class of memory efficient online learning algorithms for pairwise learning problems that use only a bounded subset of past training samples to update the hypothesis at each step . finally , in order to complement our generalization bounds , we propose a novel memory efficient online learning algorithm for higher order learning problems with bounded regret guarantees .", "topics": ["regret ( decision theory )", "supervised learning"]}
{"title": "demosaicing and superresolution for color filter array via residual image reconstruction and sparse representation", "abstract": "a framework of demosaicing and superresolution for color filter array ( cfa ) via residual image reconstruction and sparse representation is presented.given the intermediate image produced by certain demosaicing and interpolation technique , a residual image between the final reconstruction image and the intermediate image is reconstructed using sparse representation.the final reconstruction image has richer edges and details than that of the intermediate image . specifically , a generic dictionary is learned from a large set of composite training data composed of intermediate data and residual data . the learned dictionary implies a mapping between the two data . a specific dictionary adaptive to the input cfa is learned thereafter . using the adaptive dictionary , the sparse coefficients of intermediate data are computed and transformed to predict residual image . the residual image is added back into the intermediate image to obtain the final reconstruction image . experimental results demonstrate the state-of-the-art performance in terms of psnr and subjective visual perception .", "topics": ["test set", "sparse matrix"]}
{"title": "dismo : a morphosyntactic , disfluency and multi-word unit annotator . an evaluation on a corpus of french spontaneous and read speech", "abstract": "we present dismo , a multi-level annotator for spoken language corpora that integrates part-of-speech tagging with basic disfluency detection and annotation , and multi-word unit recognition . dismo is a hybrid system that uses a combination of lexical resources , rules , and statistical models based on conditional random fields ( crf ) . in this paper , we present the first public version of dismo for french . the system is trained and its performance evaluated on a 57k-token corpus , including different varieties of french spoken in three countries ( belgium , france and switzerland ) . dismo supports a multi-level annotation scheme , in which the tokenisation to minimal word units is complemented with multi-word unit groupings ( each having associated pos tags ) , as well as separate levels for annotating disfluencies and discourse phenomena . we present the system 's architecture , linguistic resources and its hierarchical tag-set . results show that dismo achieves a precision of 95 % ( finest tag-set ) to 96.8 % ( coarse tag-set ) in pos-tagging non-punctuated , sound-aligned transcriptions of spoken french , while also offering substantial possibilities for automated multi-level annotation .", "topics": ["text corpus"]}
{"title": "alternating directions dual decomposition", "abstract": "we propose ad3 , a new algorithm for approximate maximum a posteriori ( map ) inference on factor graphs based on the alternating directions method of multipliers . like dual decomposition algorithms , ad3 uses worker nodes to iteratively solve local subproblems and a controller node to combine these local solutions into a global update . the key characteristic of ad3 is that each local subproblem has a quadratic regularizer , leading to a faster consensus than subgradient-based dual decomposition , both theoretically and in practice . we provide closed-form solutions for these ad3 subproblems for binary pairwise factors and factors imposing first-order logic constraints . for arbitrary factors ( large or combinatorial ) , we introduce an active set method which requires only an oracle for computing a local map configuration , making ad3 applicable to a wide range of problems . experiments on synthetic and realworld problems show that ad3 compares favorably with the state-of-the-art .", "topics": ["approximation algorithm", "synthetic data"]}
{"title": "the mirage of action-dependent baselines in reinforcement learning", "abstract": "policy gradient methods are a widely used class of model-free reinforcement learning algorithms where a state-dependent baseline is used to reduce gradient estimator variance . several recent papers extend the baseline to depend on both the state and action and suggest that this significantly reduces variance and improves sample efficiency without introducing bias into the gradient estimates . to better understand this development , we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in commonly tested benchmark domains . we confirm this unexpected result by reviewing the open-source code accompanying these prior papers , and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the source of the previously observed empirical gains . furthermore , the variance decomposition highlights areas for improvement , which we demonstrate by illustrating a simple change to the typical value function parameterization that can significantly improve performance .", "topics": ["baseline ( configuration management )", "numerical analysis"]}
{"title": "object detection for comics using manga109 annotations", "abstract": "with the growth of digitized comics , image understanding techniques are becoming important . in this paper , we focus on object detection , which is a fundamental task of image understanding . although convolutional neural networks ( cnn ) -based methods archived good performance in object detection for naturalistic images , there are two problems in applying these methods to the comic object detection task . first , there is no large-scale annotated comics dataset . the cnn-based methods require large-scale annotations for training . secondly , the objects in comics are highly overlapped compared to naturalistic images . this overlap causes the assignment problem in the existing cnn-based methods . to solve these problems , we proposed a new annotation dataset and a new cnn model . we annotated an existing image dataset of comics and created the largest annotation dataset , named manga109-annotations . for the assignment problem , we proposed a new cnn-based detector , ssd300-fork . we compared ssd300-fork with other detection methods using manga109-annotations and confirmed that our model outperformed them based on the map score .", "topics": ["object detection", "computer vision"]}
{"title": "adversarial learning for chinese ner from crowd annotations", "abstract": "to quickly obtain new labeled data , we can choose crowdsourcing as an alternative way at lower cost in a short time . but as an exchange , crowd annotations from non-experts may be of lower quality than those from experts . in this paper , we propose an approach to performing crowd annotation learning for chinese named entity recognition ( ner ) to make full use of the noisy sequence labels from multiple annotators . inspired by adversarial learning , our approach uses a common bi-lstm and a private bi-lstm for representing annotator-generic and -specific information . the annotator-generic information is the common knowledge for entities easily mastered by the crowd . finally , we build our chinese ne tagger based on the lstm-crf model . in our experiments , we create two data sets for chinese ner tasks from two domains . the experimental results show that our system achieves better scores than strong baseline systems .", "topics": ["baseline ( configuration management )", "entity"]}
{"title": "generation and pruning of pronunciation variants to improve asr accuracy", "abstract": "speech recognition , especially name recognition , is widely used in phone services such as company directory dialers , stock quote providers or location finders . it is usually challenging due to pronunciation variations . this paper proposes an efficient and robust data-driven technique which automatically learns acceptable word pronunciations and updates the pronunciation dictionary to build a better lexicon without affecting recognition of other words similar to the target word . it generalizes well on datasets with various sizes , and reduces the error rate on a database with 13000+ human names by 42 % , compared to a baseline with regular dictionaries already covering canonical pronunciations of 97 % + words in names , plus a well-trained spelling-to-pronunciation ( stp ) engine .", "topics": ["baseline ( configuration management )", "speech recognition"]}
{"title": "optimal approximation of piecewise smooth functions using deep relu neural networks", "abstract": "we study the necessary and sufficient complexity of relu neural networks-in terms of depth and number of weights-required for approximating classifier functions in an $ l^2 $ -sense . as a model , we consider the set $ \\mathcal { e } ^\\beta ( \\mathbb { r } ^d ) $ of possibly discontinuous piecewise $ c^\\beta $ functions $ f : [ -1/2 , 1/2 ] ^d \\to \\mathbb { r } $ , where the different 'smooth regions ' of $ f $ are separated by $ c^\\beta $ hypersurfaces . for given dimension $ d \\geq 2 $ , regularity $ \\beta > 0 $ , and accuracy $ \\varepsilon > 0 $ , we construct relu neural networks that approximate functions from $ \\mathcal { e } ^\\beta ( \\mathbb { r } ^d ) $ up to an $ l^2 $ error of $ \\varepsilon $ . the constructed networks have a fixed number of layers , depending only on $ d $ and $ \\beta $ and they have $ o ( \\varepsilon^ { -2 ( d-1 ) /\\beta } ) $ many nonzero weights , which we prove to be optimal . in addition to the optimality in terms of the number of weights , we show that in order to achieve this optimal approximation rate , one needs relu networks of a certain minimal depth . precisely , for piecewise $ c^\\beta ( \\mathbb { r } ^d ) $ functions , this minimal depth is given-up to a multiplicative constant-by $ \\beta/d $ . up to a log factor , our constructed networks match this bound . this partly explains the benefits of depth for relu networks by showing that deep networks are necessary to achieve efficient approximation of ( piecewise ) smooth functions . finally , we analyze approximation in high-dimensional spaces where the function $ f $ to be approximated can be factorized into a smooth dimension reducing feature map $ \\tau $ and classifier function $ g $ -defined on a low-dimensional feature space-as $ f = g \\circ \\tau $ . we show that in this case the approximation rate depends only on the dimension of the feature space and not the input dimension .", "topics": ["feature vector", "approximation algorithm"]}
{"title": "morpho-syntactic lexicon generation using graph-based semi-supervised learning", "abstract": "morpho-syntactic lexicons provide information about the morphological and syntactic roles of words in a language . such lexicons are not available for all languages and even when available , their coverage can be limited . we present a graph-based semi-supervised learning method that uses the morphological , syntactic and semantic relations between words to automatically construct wide coverage lexicons from small seed sets . our method is language-independent , and we show that we can expand a 1000 word seed lexicon to more than 100 times its size with high quality for 11 languages . in addition , the automatically created lexicons provide features that improve performance in two downstream tasks : morphological tagging and dependency parsing .", "topics": ["supervised learning", "parsing"]}
{"title": "isolating sources of disentanglement in variational autoencoders", "abstract": "we decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables . we use this to motivate our $ \\beta $ -tcvae ( total correlation variational autoencoder ) , a refinement of the state-of-the-art $ \\beta $ -vae objective for learning disentangled representations , requiring no additional hyperparameters during training . we further propose a principled classifier-free measure of disentanglement called the mutual information gap ( mig ) . we perform extensive quantitative and qualitative experiments , in both restricted and non-restricted settings , and show a strong relation between total correlation and disentanglement , when the latent variables model is trained using our framework .", "topics": ["autoencoder"]}
{"title": "topology and geometry of half-rectified network optimization", "abstract": "the loss surface of deep neural networks has recently attracted interest in the optimization and machine learning communities as a prime example of high-dimensional non-convex problem . some insights were recently gained using spin glass models and mean-field approximations , but at the expense of strongly simplifying the nonlinear nature of the model . in this work , we do not make any such assumption and study conditions on the data distribution and model architecture that prevent the existence of bad local minima . our theoretical work quantifies and formalizes two important \\emph { folklore } facts : ( i ) the landscape of deep linear networks has a radically different topology from that of deep half-rectified ones , and ( ii ) that the energy landscape in the non-linear case is fundamentally controlled by the interplay between the smoothness of the data distribution and model over-parametrization . our main theoretical contribution is to prove that half-rectified single layer networks are asymptotically connected , and we provide explicit bounds that reveal the aforementioned interplay . the conditioning of gradient descent is the next challenge we address . we study this question through the geometry of the level sets , and we introduce an algorithm to efficiently estimate the regularity of such sets on large-scale networks . our empirical results show that these level sets remain connected throughout all the learning phase , suggesting a near convex behavior , but they become exponentially more curvy as the energy level decays , in accordance to what is observed in practice with very low curvature attractors .", "topics": ["nonlinear system", "gradient descent"]}
{"title": "fractional moments on bandit problems", "abstract": "reinforcement learning addresses the dilemma between exploration to find profitable actions and exploitation to act according to the best observations already made . bandit problems are one such class of problems in stateless environments that represent this explore/exploit situation . we propose a learning algorithm for bandit problems based on fractional expectation of rewards acquired . the algorithm is theoretically shown to converge on an eta-optimal arm and achieve o ( n ) sample complexity . experimental results show the algorithm incurs substantially lower regrets than parameter-optimized eta-greedy and softmax approaches and other low sample complexity state-of-the-art techniques .", "topics": ["reinforcement learning"]}
{"title": "optimal best arm identification with fixed confidence", "abstract": "we give a complete characterization of the complexity of best-arm identification in one-parameter bandit problems . we prove a new , tight lower bound on the sample complexity . we propose the `track-and-stop ' strategy , which we prove to be asymptotically optimal . it consists in a new sampling rule ( which tracks the optimal proportions of arm draws highlighted by the lower bound ) and in a stopping rule named after chernoff , for which we give a new analysis .", "topics": ["sampling ( signal processing )"]}
{"title": "automatic lymphocyte detection in h & e images with deep neural networks", "abstract": "automatic detection of lymphocyte in h & e images is a necessary first step in lots of tissue image analysis algorithms . an accurate and robust automated lymphocyte detection approach is of great importance in both computer science and clinical studies . most of the existing approaches for lymphocyte detection are based on traditional image processing algorithms and/or classic machine learning methods . in the recent years , deep learning techniques have fundamentally transformed the way that a computer interprets images and have become a matchless solution in various pattern recognition problems . in this work , we design a new deep neural network model which extends the fully convolutional network by combining the ideas in several recent techniques , such as shortcut links . also , we design a new training scheme taking the prior knowledge about lymphocytes into consideration . the training scheme not only efficiently exploits the limited amount of free-form annotations from pathologists , but also naturally supports efficient fine-tuning . as a consequence , our model has the potential of self-improvement by leveraging the errors collected during real applications . our experiments show that our deep neural network model achieves good performance in the images of different staining conditions or different types of tissues .", "topics": ["image processing"]}
{"title": "local network community detection with continuous optimization of conductance and weighted kernel k-means", "abstract": "local network community detection is the task of finding a single community of nodes concentrated around few given seed nodes in a localized way . conductance is a popular objective function used in many algorithms for local community detection . this paper studies a continuous relaxation of conductance . we show that continuous optimization of this objective still leads to discrete communities . we investigate the relation of conductance with weighted kernel k-means for a single community , which leads to the introduction of a new objective function , $ \\sigma $ -conductance . conductance is obtained by setting $ \\sigma $ to $ 0 $ . two algorithms , emc and pgdc , are proposed to locally optimize $ \\sigma $ -conductance and automatically tune the parameter $ \\sigma $ . they are based on expectation maximization and projected gradient descent , respectively . we prove locality and give performance guarantees for emc and pgdc for a class of dense and well separated communities centered around the seeds . experiments are conducted on networks with ground-truth communities , comparing to state-of-the-art graph diffusion algorithms for conductance optimization . on large graphs , results indicate that emc and pgdc stay localized and produce communities most similar to the ground , while graph diffusion algorithms generate large communities of lower quality .", "topics": ["kernel ( operating system )", "loss function"]}
{"title": "bridging the gap between neural networks and neuromorphic hardware with a neural network compiler", "abstract": "different from developing neural networks ( nns ) for general-purpose processors , the development for nn chips usually faces with some hardware-specific restrictions , such as limited precision of network signals and parameters , constrained computation scale , and limited types of non-linear functions . this paper proposes a general methodology to address the challenges . we decouple the nn applications from the target hardware by introducing a compiler that can transform an existing trained , unrestricted nn into an equivalent network that meets the given hardware 's constraints . we propose multiple techniques to make the transformation adaptable to different kinds of nn chips , and reliable for restrict hardware constraints . we have built such a software tool that supports both spiking neural networks ( snns ) and traditional artificial neural networks ( anns ) . we have demonstrated its effectiveness with a fabricated neuromorphic chip and a processing-in-memory ( pim ) design . tests show that the inference error caused by this solution is insignificant and the transformation time is much shorter than the retraining time . also , we have studied the parameter-sensitivity evaluations to explore the tradeoffs between network error and resource utilization for different transformation strategies , which could provide insights for co-design optimization of neuromorphic hardware and software .", "topics": ["nonlinear system", "neural networks"]}
{"title": "learning detailed face reconstruction from a single image", "abstract": "reconstructing the detailed geometric structure of a face from a given image is a key to many computer vision and graphics applications , such as motion capture and reenactment . the reconstruction task is challenging as human faces vary extensively when considering expressions , poses , textures , and intrinsic geometries . while many approaches tackle this complexity by using additional data to reconstruct the face of a single subject , extracting facial surface from a single image remains a difficult problem . as a result , single-image based methods can usually provide only a rough estimate of the facial geometry . in contrast , we propose to leverage the power of convolutional neural networks to produce a highly detailed face reconstruction from a single image . for this purpose , we introduce an end-to-end cnn framework which derives the shape in a coarse-to-fine fashion . the proposed architecture is composed of two main blocks , a network that recovers the coarse facial geometry ( coarsenet ) , followed by a cnn that refines the facial features of that geometry ( finenet ) . the proposed networks are connected by a novel layer which renders a depth image given a mesh in 3d . unlike object recognition and detection problems , there are no suitable datasets for training cnns to perform face geometry reconstruction . therefore , our training regime begins with a supervised phase , based on synthetic images , followed by an unsupervised phase that uses only unconstrained facial images . the accuracy and robustness of the proposed model is demonstrated by both qualitative and quantitative evaluation tests .", "topics": ["unsupervised learning", "synthetic data"]}
{"title": "deconvolution of vibroacoustic images using a simulation model based on a three dimensional point spread function", "abstract": "vibro-acoustography ( va ) is a medical imaging method based on the difference-frequency generation produced by the mixture of two focused ultrasound beams . va has been applied to different problems in medical imaging such as imaging bones , microcalcifications in the breast , mass lesions , and calcified arteries . the obtained images may have a resolution of 0.7 -- 0.8 mm . current va systems based on confocal or linear array transducers generate c-scan images at the beam focal plane . images on the axial plane are also possible , however the system resolution along depth worsens when compared to the lateral one . typical axial resolution is about 1.0 cm . furthermore , the elevation resolution of linear array systems is larger than that in lateral direction . this asymmetry degrades c-scan images obtained using linear arrays . the purpose of this article is to study va image restoration based on a 3d point spread function ( psf ) using classical deconvolution algorithms : wiener , constrained least-squares ( clss ) , and geometric mean filters . to assess the filters ' performance , we use an image quality index that accounts for correlation loss , luminance and contrast distortion . results for simulated va images show that the quality index achieved with the wiener filter is 0.9 ( 1 indicates perfect restoration ) . this filter yielded the best result in comparison with the other ones . moreover , the deconvolution algorithms were applied to an experimental va image of a phantom composed of three stretched 0.5 mm wires . experiments were performed using transducer driven at two frequencies , 3075 khz and 3125 khz , which resulted in the difference-frequency of 50 khz . restorations with the theoretical line spread function ( lsf ) did not recover sufficient information to identify the wires in the images . however , using an estimated lsf the obtained results displayed enough information to spot the wires in the images .", "topics": ["simulation"]}
{"title": "model selection in bayesian neural networks via horseshoe priors", "abstract": "bayesian neural networks ( bnns ) have recently received increasing attention for their ability to provide well-calibrated posterior uncertainties . however , model selection -- -even choosing the number of nodes -- -remains an open question . in this work , we apply a horseshoe prior over node pre-activations of a bayesian neural network , which effectively turns off nodes that do not help explain the data . we demonstrate that our prior prevents the bnn from under-fitting even when the number of nodes required is grossly over-estimated . moreover , this model selection over the number of nodes does n't come at the expense of predictive or computational performance ; in fact , we learn smaller networks with comparable predictive performance to current approaches .", "topics": ["neural networks", "bayesian network"]}
{"title": "learning with rethinking : recurrently improving convolutional neural networks through feedback", "abstract": "recent years have witnessed the great success of convolutional neural network ( cnn ) based models in the field of computer vision . cnn is able to learn hierarchically abstracted features from images in an end-to-end training manner . however , most of the existing cnn models only learn features through a feedforward structure and no feedback information from top to bottom layers is exploited to enable the networks to refine themselves . in this paper , we propose a `` learning with rethinking '' algorithm . by adding a feedback layer and producing the emphasis vector , the model is able to recurrently boost the performance based on previous prediction . particularly , it can be employed to boost any pre-trained models . this algorithm is tested on four object classification benchmark datasets : cifar-100 , cifar-10 , mnist-background-image and ilsvrc-2012 dataset . these results have demonstrated the advantage of training cnn models with the proposed feedback mechanism .", "topics": ["neural networks", "computer vision"]}
{"title": "a c-lstm neural network for text classification", "abstract": "neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling . convolutional neural network ( cnn ) and recurrent neural network ( rnn ) are two mainstream architectures for such modeling tasks , which adopt totally different ways of understanding natural languages . in this work , we combine the strengths of both architectures and propose a novel and unified model called c-lstm for sentence representation and text classification . c-lstm utilizes cnn to extract a sequence of higher-level phrase representations , and are fed into a long short-term memory recurrent neural network ( lstm ) to obtain the sentence representation . c-lstm is able to capture both local features of phrases as well as global and temporal sentence semantics . we evaluate the proposed architecture on sentiment classification and question classification tasks . the experimental results show that the c-lstm outperforms both cnn and lstm and can achieve excellent performance on these tasks .", "topics": ["recurrent neural network", "natural language"]}
{"title": "flood-filling networks", "abstract": "state-of-the-art image segmentation algorithms generally consist of at least two successive and distinct computations : a boundary detection process that uses local image information to classify image locations as boundaries between objects , followed by a pixel grouping step such as watershed or connected components that clusters pixels into segments . prior work has varied the complexity and approach employed in these two steps , including the incorporation of multi-layer neural networks to perform boundary prediction , and the use of global optimizations during pixel clustering . we propose a unified and end-to-end trainable machine learning approach , flood-filling networks , in which a recurrent 3d convolutional network directly produces individual segments from a raw image . the proposed approach robustly segments images with an unknown and variable number of objects as well as highly variable object sizes . we demonstrate the approach on a challenging 3d image segmentation task , connectomic reconstruction from volume electron microscopy data , on which flood-filling neural networks substantially improve accuracy over other state-of-the-art methods . the proposed approach can replace complex multi-step segmentation pipelines with a single neural network that is learned end-to-end .", "topics": ["image segmentation", "cluster analysis"]}
{"title": "self-supervised multi-level face model learning for monocular reconstruction at over 250 hz", "abstract": "the reconstruction of dense 3d models of face geometry and appearance from a single image is highly challenging and ill-posed . to constrain the problem , many approaches rely on strong priors , such as parametric face models learned from limited 3d scan data . however , prior models restrict generalization of the true diversity in facial geometry , skin reflectance and illumination . to alleviate this problem , we present the first approach that jointly learns 1 ) a regressor for face shape , expression , reflectance and illumination on the basis of 2 ) a concurrently learned parametric face model . our multi-level face model combines the advantage of 3d morphable models for regularization with the out-of-space generalization of a learned corrective space . we train end-to-end on in-the-wild images without dense annotations by fusing a convolutional encoder with a differentiable expert-designed renderer and a self-supervised training loss , both defined at multiple detail levels . our approach compares favorably to the state-of-the-art in terms of reconstruction quality , better generalizes to real world faces , and runs at over 250 hz .", "topics": ["matrix regularization", "end-to-end principle"]}
{"title": "semi-supervised emotion lexicon expansion with label propagation and specialized word embeddings", "abstract": "there exist two main approaches to automatically extract affective orientation : lexicon-based and corpus-based . in this work , we argue that these two methods are compatible and show that combining them can improve the accuracy of emotion classifiers . in particular , we introduce a novel variant of the label propagation algorithm that is tailored to distributed word representations , we apply batch gradient descent to accelerate the optimization of label propagation and to make the optimization feasible for large graphs , and we propose a reproducible method for emotion lexicon expansion . we conclude that label propagation can expand an emotion lexicon in a meaningful way and that the expanded emotion lexicon can be leveraged to improve the accuracy of an emotion classifier .", "topics": ["gradient descent", "text corpus"]}
{"title": "post-processing of discovered association rules using ontologies", "abstract": "in data mining , the usefulness of association rules is strongly limited by the huge amount of delivered rules . in this paper we propose a new approach to prune and filter discovered rules . using domain ontologies , we strengthen the integration of user knowledge in the post-processing task . furthermore , an interactive and iterative framework is designed to assist the user along the analyzing task . on the one hand , we represent user domain knowledge using a domain ontology over database . on the other hand , a novel technique is suggested to prune and to filter discovered rules . the proposed framework was applied successfully over the client database provided by nantes habitat .", "topics": ["data mining", "iteration"]}
{"title": "registration of brain images using fast walsh hadamard transform", "abstract": "a lot of image registration techniques have been developed with great significance for data analysis in medicine , astrophotography , satellite imaging and few other areas . this work proposes a method for medical image registration using fast walsh hadamard transform . this algorithm registers images of the same or different modalities . each image bit is lengthened in terms of fast walsh hadamard basis functions . each basis function is a notion of determining various aspects of local structure , e.g . , horizontal edge , corner , etc . these coefficients are normalized and used as numerals in a chosen number system which allows one to form a unique number for each type of local structure . the experimental results show that fast walsh hadamard transform accomplished better results than the conventional walsh transform in the time domain . also fast walsh hadamard transform is more reliable in medical image registration consuming less time .", "topics": ["coefficient"]}
{"title": "fitted learning : models with awareness of their limits", "abstract": "though deep learning has pushed the boundaries of classification forward , in recent years hints of the limits of standard classification have begun to emerge . problems such as fooling , adding new classes over time , and the need to retrain learning models only for small changes to the original problem all point to a potential shortcoming in the classic classification regime , where a comprehensive a priori knowledge of the possible classes or concepts is critical . without such knowledge , classifiers misjudge the limits of their knowledge and overgeneralization therefore becomes a serious obstacle to consistent performance . in response to these challenges , this paper extends the classic regime by reframing classification instead with the assumption that concepts present in the training set are only a sample of the hypothetical final set of concepts . to bring learning models into this new paradigm , a novel elaboration of standard architectures called the competitive overcomplete output layer ( cool ) neural network is introduced . experiments demonstrate the effectiveness of cool by applying it to fooling , separable concept learning , one-class neural networks , and standard classification benchmarks . the results suggest that , unlike conventional classifiers , the amount of generalization in cool networks can be tuned to match the problem .", "topics": ["test set"]}
{"title": "dmath : a scalable linear algebra and math library for heterogeneous gp-gpu architectures", "abstract": "a new scalable parallel math library , dmath , is presented in this paper that demonstrates leading scaling when using intranode , or internode , hybrid-parallelism for deep-learning . dmath provides easy-to-use distributed base primitives and a variety of domain-specific algorithms . these include matrix multiplication , convolutions , and others allowing for rapid development of highly scalable applications , including deep neural networks ( dnn ) , whereas previously one was restricted to libraries that provided effective primitives for only a single gpu , like nvidia cublas and cudnn or dnn primitives from nervana neon framework . development of hpc software is difficult , labor-intensive work , requiring a unique skill set . dmath allows a wide range of developers to utilize parallel and distributed hardware easily . one contribution of this approach is that data is stored persistently on the gpu hardware , avoiding costly transfers between host and device . advanced memory management techniques are utilized , including caching of transferred data and memory reuse through pooling . a key contribution of dmath is that it delivers performance , portability , and productivity to its specific domain of support . it enables algorithm and application programmers to quickly solve problems without managing the significant complexity associated with multi-level parallelism .", "topics": ["neural networks", "scalability"]}
{"title": "learning scalable deep kernels with recurrent structure", "abstract": "many applications in speech , robotics , finance , and biology deal with sequential data , where ordering matters and recurrent structures are common . however , this structure can not be easily captured by standard kernel functions . to model such structure , we propose expressive closed-form kernel functions for gaussian processes . the resulting model , gp-lstm , fully encapsulates the inductive biases of long short-term memory ( lstm ) recurrent networks , while retaining the non-parametric probabilistic advantages of gaussian processes . we learn the properties of the proposed kernels by optimizing the gaussian process marginal likelihood using a new provably convergent semi-stochastic gradient procedure and exploit the structure of these kernels for scalable training and prediction . this approach provides a practical representation for bayesian lstms . we demonstrate state-of-the-art performance on several benchmarks , and thoroughly investigate a consequential autonomous driving application , where the predictive uncertainties provided by gp-lstm are uniquely valuable .", "topics": ["scalability", "gradient"]}
{"title": "a versatile approach to evaluating and testing automated vehicles based on kernel methods", "abstract": "evaluation and validation of complicated control systems are crucial to guarantee usability and safety . usually , failure happens in some very rarely encountered situations , but once triggered , the consequence is disastrous . accelerated evaluation is a methodology that efficiently tests those rarely-occurring yet critical failures via smartly-sampled test cases . the distribution used in sampling is pivotal to the performance of the method , but building a suitable distribution requires case-by-case analysis . this paper proposes a versatile approach for constructing sampling distribution using kernel method . the approach uses statistical learning tools to approximate the critical event sets and constructs distributions based on the unique properties of gaussian distributions . we applied the method to evaluate the automated vehicles . numerical experiments show proposed approach can robustly identify the rare failures and significantly reduce the evaluation time .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "development of an ideal observer that incorporates nuisance parameters and processes list-mode data", "abstract": "observer models were developed to process data in list-mode format in order to perform binary discrimination tasks for use in an arms-control-treaty context . data used in this study was generated using geant4 monte carlo simulations for photons using custom models of plutonium inspection objects and a radiation imaging system . observer model performance was evaluated and presented using the area under the receiver operating characteristic curve . the ideal observer was studied under both signal-known-exactly conditions and in the presence of unknowns such as object orientation and absolute count-rate variability ; when these additional sources of randomness were present , their incorporation into the observer yielded superior performance .", "topics": ["simulation"]}
{"title": "a java implementation of the sga , umda , ecga , and hboa", "abstract": "the simple genetic algorithm , the univariate marginal distribution algorithm , the extended compact genetic algorithm , and the hierarchical bayesian optimization algorithm are all well known evolutionary algorithms . in this report we present a java implementation of these four algorithms with detailed instructions on how to use each of them to solve a given set of optimization problems . additionally , it is explained how to implement and integrate new problems within the provided set . the source and binary files of the java implementations are available for free download at https : //github.com/josecpereira/2015evolutionaryalgorithmsjava .", "topics": ["optimization problem"]}
{"title": "american sign language fingerspelling recognition from video : methods for unrestricted recognition and signer-independence", "abstract": "in this thesis , we study the problem of recognizing video sequences of fingerspelled letters in american sign language ( asl ) . fingerspelling comprises a significant but relatively understudied part of asl , and recognizing it is challenging for a number of reasons : it involves quick , small motions that are often highly coarticulated ; it exhibits significant variation between signers ; and there has been a dearth of continuous fingerspelling data collected . in this work , we propose several types of recognition approaches , and explore the signer variation problem . our best-performing models are segmental ( semi-markov ) conditional random fields using deep neural network-based features . in the signer-dependent setting , our recognizers achieve up to about 8 % letter error rates . the signer-independent setting is much more challenging , but with neural network adaptation we achieve up to 17 % letter error rates .", "topics": ["markov chain"]}
{"title": "detecting the moment of completion : temporal models for localising action completion", "abstract": "action completion detection is the problem of modelling the action 's progression towards localising the moment of completion - when the action 's goal is confidently considered achieved . in this work , we assess the ability of two temporal models , namely hidden markov models ( hmm ) and long-short term memory ( lstm ) , to localise completion for six object interactions : switch , plug , open , pull , pick and drink . we use a supervised approach , where annotations of pre-completion and post-completion frames are available per action , and fine-tuned cnn features are used to train temporal models . tested on the action-completion-2016 dataset , we detect completion within 10 frames of annotations for ~75 % of completed action sequences using both temporal models . results show that fine-tuned cnn features outperform hand-crafted features for localisation , and that observing incomplete instances is necessary when incomplete sequences are also present in the test set .", "topics": ["test set", "interaction"]}
{"title": "fluorescence microscopy image segmentation using convolutional neural network with generative adversarial networks", "abstract": "recent advance in fluorescence microscopy enables acquisition of 3d image volumes with better quality and deeper penetration into tissue . segmentation is a required step to characterize and analyze biological structures in the images . 3d segmentation using deep learning has achieved promising results in microscopy images . one issue is that deep learning techniques require a large set of groundtruth data which is impractical to annotate manually for microscopy volumes . this paper describes a 3d nuclei segmentation method using 3d convolutional neural networks . a set of synthetic volumes and the corresponding groundtruth volumes are generated automatically using a generative adversarial network . segmentation results demonstrate that our proposed method is capable of segmenting nuclei successfully in 3d for various data sets .", "topics": ["image segmentation", "synthetic data"]}
{"title": "compressive adaptive computational ghost imaging", "abstract": "compressive sensing is considered a huge breakthrough in signal acquisition . it allows recording an image consisting of $ n^2 $ pixels using much fewer than $ n^2 $ measurements if it can be transformed to a basis where most pixels take on negligibly small values . standard compressive sensing techniques suffer from the computational overhead needed to reconstruct an image with typical computation times between hours and days and are thus not optimal for applications in physics and spectroscopy . we demonstrate an adaptive compressive sampling technique that performs measurements directly in a sparse basis . it needs much fewer than $ n^2 $ measurements without any computational overhead , so the result is available instantly .", "topics": ["sampling ( signal processing )", "sparse matrix"]}
{"title": "bi-directional shape correspondences ( bsc ) : a novel technique for 2-d shape warping in quadratic time ?", "abstract": "we propose bidirectional shape correspondence ( bsc ) as a possible improvement on the famous shape contexts ( sc ) framework . our proposals derive from the observation that the sc framework enforces a one-to-one correspondence between sample points , and that this leads to two possible drawbacks . first , this denies the framework the opportunity to effect advantageous many-to-many matching between points on the two shapes being compared . second , this calls for the hungarian algorithm which unfortunately usurps cubic time . while the dynamic-space-warping dynamic programming algorithm has provided a standard solution to the first problem above , it demands quintic time for general multi-contour shapes , and w times quadratic time for the special case of single-contour shapes , even after an heuristic search window of width w has been chosen . therefore , in this work , we propose a simple method for computing `` many-to-many '' correspondences for the class of all 2-d shapes in quadratic time . our approach is to explicitly let each point on the first shape choose a best match on the second shape , and vice versa . along the way , we also propose the use of data-clustering techniques for dealing with the outliers problem , and , from another viewpoint , it turns out that this clustering can be seen as an autonomous , rather than pre-computed , sampling of shape boundary .", "topics": ["sampling ( signal processing )", "cluster analysis"]}
{"title": "a combined method of fractal and glcm features for mri and ct scan images classification", "abstract": "fractal analysis has been shown to be useful in image processing for characterizing shape and gray-scale complexity . the fractal feature is a compact descriptor used to give a numerical measure of the degree of irregularity of the medical images . this descriptor property does not give ownership of the local image structure . in this paper , we present a combination of this parameter based on box counting with glcm features . this powerful combination has proved good results especially in classification of medical texture from mri and ct scan images of trabecular bone . this method has the potential to improve clinical diagnostics tests for osteoporosis pathologies .", "topics": ["image processing", "numerical analysis"]}
{"title": "end-to-end learning for the deep multivariate probit model", "abstract": "the multivariate probit model ( mvp ) is a popular classic model for studying binary responses of multiple entities . nevertheless , the computational challenge of learning the mvp model , given that its likelihood involves integrating over a multidimensional constrained space of latent variables , significantly limits its application in practice . we propose a flexible deep generalization of the classic mvp , the deep multivariate probit model ( dmvp ) , which is an end-to-end learning scheme that uses an efficient parallel sampling process of the multivariate probit model to exploit gpu-boosted deep neural networks . we present both theoretical and empirical analysis of the convergence behavior of dmvp 's sampling process with respect to the resolution of the correlation structure . we provide convergence guarantees for dmvp and our empirical analysis demonstrates the advantages of dmvp 's sampling compared with standard mcmc-based methods . we also show that when applied to multi-entity modelling problems , which are natural dmvp applications , dmvp trains faster than classical mvp , by at least an order of magnitude , captures rich correlations among entities , and further improves the joint likelihood of entities compared with several competitive models .", "topics": ["sampling ( signal processing )", "entity"]}
{"title": "deep character-level click-through rate prediction for sponsored search", "abstract": "predicting the click-through rate of an advertisement is a critical component of online advertising platforms . in sponsored search , the click-through rate estimates the probability that a displayed advertisement is clicked by a user after she submits a query to the search engine . commercial search engines typically rely on machine learning models trained with a large number of features to make such predictions . this is inevitably requires a lot of engineering efforts to define , compute , and select the appropriate features . in this paper , we propose two novel approaches ( one working at character level and the other working at word level ) that use deep convolutional neural networks to predict the click-through rate of a query-advertisement pair . specially , the proposed architectures only consider the textual content appearing in a query-advertisement pair as input , and produce as output a click-through rate prediction . by comparing the character-level model with the word-level model , we show that language representation can be learnt from scratch at character level when trained on enough data . through extensive experiments using billions of query-advertisement pairs of a popular commercial search engine , we demonstrate that both approaches significantly outperform a baseline model built on well-selected text features and a state-of-the-art word2vec-based approach . finally , by combining the predictions of the deep models introduced in this study with the prediction of the model in production of the same commercial search engine , we significantly improve the accuracy and the calibration of the click-through rate prediction of the production system .", "topics": ["baseline ( configuration management )"]}
{"title": "precisely verifying the null space conditions in compressed sensing : a sandwiching algorithm", "abstract": "in this paper , we propose new efficient algorithms to verify the null space condition in compressed sensing ( cs ) . given an $ ( n-m ) \\times n $ ( $ m > 0 $ ) cs matrix $ a $ and a positive $ k $ , we are interested in computing $ \\displaystyle \\alpha_k = \\max_ { \\ { z : az=0 , z\\neq 0\\ } } \\max_ { \\ { k : |k|\\leq k\\ } } $ $ { \\|z_k \\|_ { 1 } } { \\|z\\|_ { 1 } } $ , where $ k $ represents subsets of $ \\ { 1,2 , ... , n\\ } $ , and $ |k| $ is the cardinality of $ k $ . in particular , we are interested in finding the maximum $ k $ such that $ \\alpha_k < { 1 } { 2 } $ . however , computing $ \\alpha_k $ is known to be extremely challenging . in this paper , we first propose a series of new polynomial-time algorithms to compute upper bounds on $ \\alpha_k $ . based on these new polynomial-time algorithms , we further design a new sandwiching algorithm , to compute the \\emph { exact } $ \\alpha_k $ with greatly reduced complexity . when needed , this new sandwiching algorithm also achieves a smooth tradeoff between computational complexity and result accuracy . empirical results show the performance improvements of our algorithm over existing known methods ; and our algorithm outputs precise values of $ \\alpha_k $ , with much lower complexity than exhaustive search .", "topics": ["time complexity", "computational complexity theory"]}
{"title": "generating synthetic data for text recognition", "abstract": "generating synthetic images is an art which emulates the natural process of image generation in a closest possible manner . in this work , we exploit such a framework for data generation in handwritten domain . we render synthetic data using open source fonts and incorporate data augmentation schemes . as part of this work , we release 9m synthetic handwritten word image corpus which could be useful for training deep network architectures and advancing the performance in handwritten word spotting and recognition tasks .", "topics": ["synthetic data"]}
{"title": "node splitting : a scheme for generating upper bounds in bayesian networks", "abstract": "we formulate in this paper the mini-bucket algorithm for approximate inference in terms of exact inference on an approximate model produced by splitting nodes in a bayesian network . the new formulation leads to a number of theoretical and practical implications . first , we show that branchand- bound search algorithms that use minibucket bounds may operate in a drastically reduced search space . second , we show that the proposed formulation inspires new minibucket heuristics and allows us to analyze existing heuristics from a new perspective . finally , we show that this new formulation allows mini-bucket approximations to benefit from recent advances in exact inference , allowing one to significantly increase the reach of these approximations .", "topics": ["approximation algorithm", "approximation"]}
{"title": "modeling label ambiguity for neural list-wise learning to rank", "abstract": "list-wise learning to rank methods are considered to be the state-of-the-art . one of the major problems with these methods is that the ambiguous nature of relevance labels in learning to rank data is ignored . ambiguity of relevance labels refers to the phenomenon that multiple documents may be assigned the same relevance label for a given query , so that no preference order should be learned for those documents . in this paper we propose a novel sampling technique for computing a list-wise loss that can take into account this ambiguity . we show the effectiveness of the proposed method by training a 3-layer deep neural network . we compare our new loss function to two strong baselines : listnet and listmle . we show that our method generalizes better and significantly outperforms other methods on the validation and test sets .", "topics": ["loss function", "relevance"]}
{"title": "preferred history semantics for iterated updates", "abstract": "we give a semantics to iterated update by a preference relation on possible developments . an iterated update is a sequence of formulas , giving ( incomplete ) information about successive states of the world . a development is a sequence of models , describing a possible trajectory through time . we assume a principle of inertia and prefer those developments , which are compatible with the information , and avoid unnecessary changes . the logical properties of the updates defined in this way are considered , and a representation result is proved .", "topics": ["iteration"]}
{"title": "quality classifiers for open source software repositories", "abstract": "open source software ( oss ) often relies on large repositories , like sourceforge , for initial incubation . the oss repositories offer a large variety of meta-data providing interesting information about projects and their success . in this paper we propose a data mining approach for training classifiers on the oss meta-data provided by such data repositories . the classifiers learn to predict the successful continuation of an oss project . the `successfulness ' of projects is defined in terms of the classifier confidence with which it predicts that they could be ported in popular oss projects ( such as freebsd , gentoo portage ) .", "topics": ["data mining"]}
{"title": "deep hyperspherical learning", "abstract": "convolution as inner product has been the founding basis of convolutional neural networks ( cnns ) and the key to end-to-end visual representation learning . benefiting from deeper architectures , recent cnns have demonstrated increasingly strong representation abilities . despite such improvement , the increased depth and larger parameter space have also led to challenges in properly training a network . in light of such challenges , we propose hyperspherical convolution ( sphereconv ) , a novel learning framework that gives angular representations on hyperspheres . we introduce spherenet , deep hyperspherical convolution networks that are distinct from conventional inner product based convolutional networks . in particular , spherenet adopts sphereconv as its basic convolution operator and is supervised by generalized angular softmax loss - a natural loss formulation under sphereconv . we show that spherenet can effectively encode discriminative representation and alleviate training difficulty , leading to easier optimization , faster convergence and comparable ( even better ) classification accuracy over convolutional counterparts . we also provide some theoretical insights for the advantages of learning on hyperspheres . in addition , we introduce the learnable sphereconv , i.e . , a natural improvement over prefixed sphereconv , and spherenorm , i.e . , hyperspherical learning as a normalization method . experiments have verified our conclusions .", "topics": ["feature learning", "convolution"]}
{"title": "creating a real-time , reproducible event dataset", "abstract": "the generation of political event data has remained much the same since the mid-1990s , both in terms of data acquisition and the process of coding text into data . since the 1990s , however , there have been significant improvements in open-source natural language processing software and in the availability of digitized news content . this paper presents a new , next-generation event dataset , named phoenix , that builds from these and other advances . this dataset includes improvements in the underlying news collection process and event coding software , along with the creation of a general processing pipeline necessary to produce daily-updated data . this paper provides a face validity checks by briefly examining the data for the conflict in syria , and a comparison between phoenix and the integrated crisis early warning system data .", "topics": ["natural language processing", "natural language"]}
{"title": "an $ \\mathcal { o } ( n\\log n ) $ projection operator for weighted $ \\ell_1 $ -norm regularization with sum constraint", "abstract": "we provide a simple and efficient algorithm for the projection operator for weighted $ \\ell_1 $ -norm regularization subject to a sum constraint , together with an elementary proof . the implementation of the proposed algorithm can be downloaded from the author 's homepage .", "topics": ["optimization problem", "matrix regularization"]}
{"title": "semantic video segmentation by gated recurrent flow propagation", "abstract": "semantic video segmentation is challenging due to the sheer amount of data that needs to be processed and labeled in order to construct accurate models . in this paper we present a deep , end-to-end trainable methodology to video segmentation that is capable of leveraging information present in unlabeled data in order to improve semantic estimates . our model combines a convolutional architecture and a spatio-temporal transformer recurrent layer that are able to temporally propagate labeling information by means of optical flow , adaptively gated based on its locally estimated uncertainty . the flow , the recognition and the gated temporal propagation modules can be trained jointly , end-to-end . the temporal , gated recurrent flow propagation component of our model can be plugged into any static semantic segmentation architecture and turn it into a weakly supervised video processing one . our extensive experiments in the challenging cityscapes and camvid datasets , and based on multiple deep architectures , indicate that the resulting model can leverage unlabeled temporal frames , next to a labeled one , in order to improve both the video segmentation accuracy and the consistency of its temporal labeling , at no additional annotation cost and with little extra computation .", "topics": ["computation", "end-to-end principle"]}
{"title": "max-value entropy search for efficient bayesian optimization", "abstract": "entropy search ( es ) and predictive entropy search ( pes ) are popular and empirically successful bayesian optimization techniques . both rely on a compelling information-theoretic motivation , and maximize the information gained about the $ \\arg\\max $ of the unknown function ; yet , both are plagued by the expensive computation for estimating entropies . we propose a new criterion , max-value entropy search ( mes ) , that instead uses the information about the maximum function value . we show relations of mes to other bayesian optimization methods , and establish a regret bound . we observe that mes maintains or improves the good empirical performance of es/pes , while tremendously lightening the computational burden . in particular , mes is much more robust to the number of samples used for computing the entropy , and hence more efficient for higher dimensional problems .", "topics": ["regret ( decision theory )", "computation"]}
{"title": "a hybrid approach to extract keyphrases from medical documents", "abstract": "keyphrases are the phrases , consisting of one or more words , representing the important concepts in the articles . keyphrases are useful for a variety of tasks such as text summarization , automatic indexing , clustering/classification , text mining etc . this paper presents a hybrid approach to keyphrase extraction from medical documents . the keyphrase extraction approach presented in this paper is an amalgamation of two methods : the first one assigns weights to candidate keyphrases based on an effective combination of features such as position , term frequency , inverse document frequency and the second one assign weights to candidate keyphrases using some knowledge about their similarities to the structure and characteristics of keyphrases available in the memory ( stored list of keyphrases ) . an efficient candidate keyphrase identification method as the first component of the proposed keyphrase extraction system has also been introduced in this paper . the experimental results show that the proposed hybrid approach performs better than some state-of-the art keyphrase extraction approaches .", "topics": ["statistical classification", "cluster analysis"]}
{"title": "example-based optimization of surface-generation tables", "abstract": "a method is given that `` inverts '' a logic grammar and displays it from the point of view of the logical form , rather than from that of the word string . lr-compiling techniques are used to allow a recursive-descent generation algorithm to perform `` functor merging '' much in the same way as an lr parser performs prefix merging . this is an improvement on the semantic-head-driven generator that results in a much smaller search space . the amount of semantic lookahead can be varied , and appropriate tradeoff points between table size and resulting nondeterminism can be found automatically . this can be done by removing all spurious nondeterminism for input sufficiently close to the examples of a training corpus , and large portions of it for other input , while preserving completeness .", "topics": ["parsing", "text corpus"]}
{"title": "a proximal newton framework for composite minimization : graph learning without cholesky decompositions and matrix inversions", "abstract": "we propose an algorithmic framework for convex minimization problems of a composite function with two terms : a self-concordant function and a possibly nonsmooth regularization term . our method is a new proximal newton algorithm that features a local quadratic convergence rate . as a specific instance of our framework , we consider the sparse inverse covariance matrix estimation in graph learning problems . via a careful dual formulation and a novel analytic step-size selection procedure , our approach for graph learning avoids cholesky decompositions and matrix inversions in its iteration making it attractive for parallel and distributed implementations .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "understanding grounded language learning agents", "abstract": "neural network-based systems can now learn to locate the referents of words and phrases in images , answer questions about visual scenes , and even execute symbolic instructions as first-person actors in partially-observable worlds . to achieve this so-called grounded language learning , models must overcome certain well-studied learning challenges that are also fundamental to infants learning their first words . while it is notable that models with no meaningful prior knowledge overcome these learning obstacles , ai researchers and practitioners currently lack a clear understanding of exactly how they do so . here we address this question as a way of achieving a clearer general understanding of grounded language learning , both to inform future research and to improve confidence in model predictions . for maximum control and generality , we focus on a simple neural network-based language learning agent trained via policy-gradient methods to interpret synthetic linguistic instructions in a simulated 3d world . we apply experimental paradigms from developmental psychology to this agent , exploring the conditions under which established human biases and learning effects emerge . we further propose a novel way to visualise and analyse semantic representation in grounded language learning agents that yields a plausible computational account of the observed effects .", "topics": ["synthetic data", "simulation"]}
{"title": "a subband-based svm front-end for robust asr", "abstract": "this work proposes a novel support vector machine ( svm ) based robust automatic speech recognition ( asr ) front-end that operates on an ensemble of the subband components of high-dimensional acoustic waveforms . the key issues of selecting the appropriate svm kernels for classification in frequency subbands and the combination of individual subband classifiers using ensemble methods are addressed . the proposed front-end is compared with state-of-the-art asr front-ends in terms of robustness to additive noise and linear filtering . experiments performed on the timit phoneme classification task demonstrate the benefits of the proposed subband based svm front-end : it outperforms the standard cepstral front-end in the presence of noise and linear filtering for signal-to-noise ratio ( snr ) below 12-db . a combination of the proposed front-end with a conventional front-end such as mfcc yields further improvements over the individual front ends across the full range of noise levels .", "topics": ["support vector machine", "speech recognition"]}
{"title": "robust and efficient relative pose with a multi-camera system for autonomous vehicle in highly dynamic environments", "abstract": "this paper studies the relative pose problem for autonomous vehicle driving in highly dynamic and possibly cluttered environments . this is a challenging scenario due to the existence of multiple , large , and independently moving objects in the environment , which often leads to excessive portion of outliers and results in erroneous motion estimation . existing algorithms can not cope with such situations well . this paper proposes a new algorithm for relative pose using a multi-camera system with multiple non-overlapping individual cameras . the method works robustly even when the numbers of outliers are overwhelming . by exploiting specific prior knowledge of driving scene we have developed an efficient 4-point algorithm for multi-camera relative pose , which admits analytic solutions by solving a polynomial root-finding equation , and runs extremely fast ( at about 0.5 $ u $ s per root ) . when the solver is used in combination with ransac , we are able to quickly prune unpromising hypotheses , significantly improve the chance of finding inliers . experiments on synthetic data have validated the performance of the proposed algorithm . tests on real data further confirm the method 's practical relevance .", "topics": ["synthetic data", "relevance"]}
{"title": "tight measurement bounds for exact recovery of structured sparse signals", "abstract": "standard compressive sensing results state that to exactly recover an s sparse signal in r^p , one requires o ( s. log ( p ) ) measurements . while this bound is extremely useful in practice , often real world signals are not only sparse , but also exhibit structure in the sparsity pattern . we focus on group-structured patterns in this paper . under this model , groups of signal coefficients are active ( or inactive ) together . the groups are predefined , but the particular set of groups that are active ( i.e . , in the signal support ) must be learned from measurements . we show that exploiting knowledge of groups can further reduce the number of measurements required for exact signal recovery , and derive universal bounds for the number of measurements needed . the bound is universal in the sense that it only depends on the number of groups under consideration , and not the particulars of the groups ( e.g . , compositions , sizes , extents , overlaps , etc . ) . experiments show that our result holds for a variety of overlapping group configurations .", "topics": ["sparse matrix", "coefficient"]}
{"title": "do deep nets really need to be deep ?", "abstract": "currently , deep neural networks are the state of the art on problems such as speech recognition and computer vision . in this extended abstract , we show that shallow feed-forward networks can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models . moreover , in some cases the shallow neural nets can learn these deep functions using a total number of parameters similar to the original deep model . we evaluate our method on the timit phoneme recognition task and are able to train shallow fully-connected nets that perform similarly to complex , well-engineered , deep convolutional architectures . our success in training shallow neural nets to mimic deeper models suggests that there probably exist better algorithms for training shallow feed-forward nets than those currently available .", "topics": ["computer vision", "speech recognition"]}
{"title": "coupled support vector machines for supervised domain adaptation", "abstract": "popular domain adaptation ( da ) techniques learn a classifier for the target domain by sampling relevant data points from the source and combining it with the target data . we present a support vector machine ( svm ) based supervised da technique , where the similarity between source and target domains is modeled as the similarity between their svm decision boundaries . we couple the source and target svms and reduce the model to a standard single svm . we test the coupled-svm on multiple datasets and compare our results with other popular svm based da approaches .", "topics": ["sampling ( signal processing )", "support vector machine"]}
{"title": "differentially private variational inference for non-conjugate models", "abstract": "many machine learning applications are based on data collected from people , such as their tastes and behaviour as well as biological traits and genetic data . regardless of how important the application might be , one has to make sure individuals ' identities or the privacy of the data are not compromised in the analysis . differential privacy constitutes a powerful framework that prevents breaching of data subject privacy from the output of a computation . differentially private versions of many important bayesian inference methods have been proposed , but there is a lack of an efficient unified approach applicable to arbitrary models . in this contribution , we propose a differentially private variational inference method with a very wide applicability . it is built on top of doubly stochastic variational inference , a recent advance which provides a variational solution to a large class of models . we add differential privacy into doubly stochastic variational inference by clipping and perturbing the gradients . the algorithm is made more efficient through privacy amplification from subsampling . we demonstrate the method can reach an accuracy close to non-private level under reasonably strong privacy guarantees , clearly improving over previous sampling-based alternatives especially in the strong privacy regime .", "topics": ["calculus of variations", "gradient descent"]}
{"title": "evolution of convolutional highway networks", "abstract": "convolutional highways are deep networks based on multiple stacked convolutional layers for feature preprocessing . we introduce an evolutionary algorithm ( ea ) for optimization of the structure and hyperparameters of convolutional highways and demonstrate the potential of this optimization setting on the well-known mnist data set . the ( 1+1 ) -ea employs rechenberg 's mutation rate control and a niching mechanism to overcome local optima adapts the optimization approach . an experimental study shows that the ea is capable of improving the state-of-the-art network contribution and of evolving highway networks from scratch .", "topics": ["mnist database"]}
{"title": "toward smart power grids : communication network design for power grids synchronization", "abstract": "in smart power grids , keeping the synchronicity of generators and the corresponding controls is of great importance . to do so , a simple model is employed in terms of swing equation to represent the interactions among dynamics of generators and feedback control . in case of having a communication network available , the control can be done based on the transmitted measurements by the communication network . the stability of system is denoted by the largest eigenvalue of the weighted sum of the laplacian matrices of the communication infrastructure and power network . in this work , we use graph theory to model the communication network as a graph problem . then , ant colony system ( acs ) is employed for optimum design of above graph for synchronization of power grids . performance evaluation of the proposed method for the 39-bus new england power system versus methods such as exhaustive search and rayleigh quotient approximation indicates feasibility and effectiveness of our method for even large scale smart power grids .", "topics": ["interaction"]}
{"title": "parallel machine scheduling with step deteriorating jobs and setup times by a hybrid discrete cuckoo search algorithm", "abstract": "this article considers the parallel machine scheduling problem with step-deteriorating jobs and sequence-dependent setup times . the objective is to minimize the total tardiness by determining the allocation and sequence of jobs on identical parallel machines . in this problem , the processing time of each job is a step function dependent upon its starting time . an individual extended time is penalized when the starting time of a job is later than a specific deterioration date . the possibility of deterioration of a job makes the parallel machine scheduling problem more challenging than ordinary ones . a mixed integer programming model for the optimal solution is derived . due to its np-hard nature , a hybrid discrete cuckoo search algorithm is proposed to solve this problem . in order to generate a good initial swarm , a modified heuristic named the mbhg is incorporated into the initialization of population . several discrete operators are proposed in the random walk of l\\ ' { e } vy flights and the crossover search . moreover , a local search procedure based on variable neighborhood descent is integrated into the algorithm as a hybrid strategy in order to improve the quality of elite solutions . computational experiments are executed on two sets of randomly generated test instances . the results show that the proposed hybrid algorithm can yield better solutions in comparison with the commercial solver cplex with one hour time limit , discrete cuckoo search algorithm and the existing variable neighborhood search algorithm .", "topics": ["optimization problem", "heuristic"]}
{"title": "sprite : a response model for multiple choice testing", "abstract": "item response theory ( irt ) models for categorical response data are widely used in the analysis of educational data , computerized adaptive testing , and psychological surveys . however , most irt models rely on both the assumption that categories are strictly ordered and the assumption that this ordering is known a priori . these assumptions are impractical in many real-world scenarios , such as multiple-choice exams where the levels of incorrectness for the distractor categories are often unknown . while a number of results exist on irt models for unordered categorical data , they tend to have restrictive modeling assumptions that lead to poor data fitting performance in practice . furthermore , existing unordered categorical models have parameters that are difficult to interpret . in this work , we propose a novel methodology for unordered categorical irt that we call sprite ( short for stochastic polytomous response item model ) that : ( i ) analyzes both ordered and unordered categories , ( ii ) offers interpretable outputs , and ( iii ) provides improved data fitting compared to existing models . we compare sprite to existing item response models and demonstrate its efficacy on both synthetic and real-world educational datasets .", "topics": ["synthetic data"]}
{"title": "ranking by dependence - a fair criteria", "abstract": "estimating the dependences between random variables , and ranking them accordingly , is a prevalent problem in machine learning . pursuing frequentist and information-theoretic approaches , we first show that the p-value and the mutual information can fail even in simplistic situations . we then propose two conditions for regularizing an estimator of dependence , which leads to a simple yet effective new measure . we discuss its advantages and compare it to well-established model-selection criteria . apart from that , we derive a simple constraint for regularizing parameter estimates in a graphical model . this results in an analytical approximation for the optimal value of the equivalent sample size , which agrees very well with the more involved bayesian approach in our experiments .", "topics": ["graphical model"]}
{"title": "jfleg : a fluency corpus and benchmark for grammatical error correction", "abstract": "we present a new parallel corpus , jhu fluency-extended gug corpus ( jfleg ) for developing and evaluating grammatical error correction ( gec ) . unlike other corpora , it represents a broad range of language proficiency levels and uses holistic fluency edits to not only correct grammatical errors but also make the original text more native sounding . we describe the types of corrections made and benchmark four leading gec systems on this corpus , identifying specific areas in which they do well and how they can improve . jfleg fulfills the need for a new gold standard to properly assess the current state of gec .", "topics": ["text corpus"]}
{"title": "structured sparsity-inducing norms through submodular functions", "abstract": "sparse methods for supervised learning aim at finding good linear predictors from as few variables as possible , i.e . , with small cardinality of their supports . this combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope ( tightest convex lower bound ) , in this case the l1-norm . in this paper , we investigate more general set-functions than the cardinality , that may incorporate prior knowledge or structural constraints which are common in many applications : namely , we show that for nondecreasing submodular set-functions , the corresponding convex envelope can be obtained from its \\lova extension , a common tool in submodular analysis . this defines a family of polyhedral norms , for which we provide generic algorithmic tools ( subgradients and proximal operators ) and theoretical results ( conditions for support recovery or high-dimensional inference ) . by selecting specific submodular functions , we can give a new interpretation to known norms , such as those based on rank-statistics or grouped norms with potentially overlapping groups ; we also define new norms , in particular ones that can be used as non-factorial priors for supervised learning .", "topics": ["optimization problem", "supervised learning"]}
{"title": "image-to-image translation with conditional adversarial networks", "abstract": "we investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems . these networks not only learn the mapping from input image to output image , but also learn a loss function to train this mapping . this makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations . we demonstrate that this approach is effective at synthesizing photos from label maps , reconstructing objects from edge maps , and colorizing images , among other tasks . indeed , since the release of the pix2pix software associated with this paper , a large number of internet users ( many of them artists ) have posted their own experiments with our system , further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking . as a community , we no longer hand-engineer our mapping functions , and this work suggests we can achieve reasonable results without hand-engineering our loss functions either .", "topics": ["loss function", "map"]}
{"title": "equations of states in statistical learning for a nonparametrizable and regular case", "abstract": "many learning machines that have hierarchical structure or hidden variables are now being used in information science , artificial intelligence , and bioinformatics . however , several learning machines used in such fields are not regular but singular statistical models , hence their generalization performance is still left unknown . to overcome these problems , in the previous papers , we proved new equations in statistical learning , by which we can estimate the bayes generalization loss from the bayes training loss and the functional variance , on the condition that the true distribution is a singularity contained in a learning machine . in this paper , we prove that the same equations hold even if a true distribution is not contained in a parametric model . also we prove that , the proposed equations in a regular case are asymptotically equivalent to the takeuchi information criterion . therefore , the proposed equations are always applicable without any condition on the unknown true distribution .", "topics": ["artificial intelligence", "eisenstein 's criterion"]}
{"title": "winqi : a system for 6d localization and slam augmentation using wideangle optics and coded light beacons", "abstract": "simultaneous localization and mapping ( slam ) systems use commodity visible/near visible digital sensors coupled with processing units that detect , recognize and track image points in a camera stream . these systems are cheap , fast and make use of readily available camera technologies . however , slam systems suffer from issues of drift as well as sensitivity to lighting variation such as shadows and changing brightness . beaconless slam systems will continue to suffer from this inherent drift problem irrespective of the improvements in on-board camera resolution , speed and inertial sensor precision . to cancel out destructive forms of drift , relocalization algorithms are used which use known detected landmarks together with loop closure processes to continually readjust the current location and orientation estimates to match `` known '' positions . however this is inherently problematic because these landmarks themselves may have been recorded with errors and they may also change under different illumination conditions . in this note we describe a unique beacon light coding system which is robust to desynchronized clock bit drift . the described beacons and codes are designed to be used in industrial or consumer environments for full standalone 6dof tracking or as known error free landmarks in a slam pipeline .", "topics": ["autonomous car", "robot"]}
{"title": "relevant ensemble of trees", "abstract": "tree ensembles are flexible predictive models that can capture relevant variables and to some extent their interactions in a compact and interpretable manner . most algorithms for obtaining tree ensembles are based on versions of boosting or random forest . previous work showed that boosting algorithms exhibit a cyclic behavior of selecting the same tree again and again due to the way the loss is optimized . at the same time , random forest is not based on loss optimization and obtains a more complex and less interpretable model . in this paper we present a novel method for obtaining compact tree ensembles by growing a large pool of trees in parallel with many independent boosting threads and then selecting a small subset and updating their leaf weights by loss optimization . we allow for the trees in the initial pool to have different depths which further helps with generalization . experiments on real datasets show that the obtained model has usually a smaller loss than boosting , which is also reflected in a lower misclassification error on the test set .", "topics": ["test set", "mathematical optimization"]}
{"title": "combining lexical and syntactic features for detecting content-dense texts in news", "abstract": "content-dense news report important factual information about an event in direct , succinct manner . information seeking applications such as information extraction , question answering and summarization normally assume all text they deal with is content-dense . here we empirically test this assumption on news articles from the business , u.s. international relations , sports and science journalism domains . our findings clearly indicate that about half of the news texts in our study are in fact not content-dense and motivate the development of a supervised content-density detector . we heuristically label a large training corpus for the task and train a two-layer classifying model based on lexical and unlexicalized syntactic features . on manually annotated data , we compare the performance of domain-specific classifiers , trained on data only from a given news domain and a general classifier in which data from all four domains is pooled together . our annotation and prediction experiments demonstrate that the concept of content density varies depending on the domain and that naive annotators provide judgement biased toward the stereotypical domain label . domain-specific classifiers are more accurate for domains in which content-dense texts are typically fewer . domain independent classifiers reproduce better naive crowdsourced judgements . classification prediction is high across all conditions , around 80 % .", "topics": ["text corpus", "heuristic"]}
{"title": "toward multilingual neural machine translation with universal encoder and decoder", "abstract": "in this paper , we present our first attempts in building a multilingual neural machine translation framework under a unified approach . we are then able to employ attention-based nmt for many-to-many multilingual translation tasks . our approach does not require any special treatment on the network architecture and it allows us to learn minimal number of free parameters in a standard way of training . our approach has shown its effectiveness in an under-resourced translation scenario with considerable improvements up to 2.6 bleu points . in addition , the approach has achieved interesting and promising results when applied in the translation task that there is no direct parallel corpus between source and target languages .", "topics": ["machine translation", "encoder"]}
{"title": "selling to a no-regret buyer", "abstract": "we consider the problem of a single seller repeatedly selling a single item to a single buyer ( specifically , the buyer has a value drawn fresh from known distribution $ d $ in every round ) . prior work assumes that the buyer is fully rational and will perfectly reason about how their bids today affect the seller 's decisions tomorrow . in this work we initiate a different direction : the buyer simply runs a no-regret learning algorithm over possible bids . we provide a fairly complete characterization of optimal auctions for the seller in this domain . specifically : - if the buyer bids according to exp3 ( or any `` mean-based '' learning algorithm ) , then the seller can extract expected revenue arbitrarily close to the expected welfare . this auction is independent of the buyer 's valuation $ d $ , but somewhat unnatural as it is sometimes in the buyer 's interest to overbid . - there exists a learning algorithm $ \\mathcal { a } $ such that if the buyer bids according to $ \\mathcal { a } $ then the optimal strategy for the seller is simply to post the myerson reserve for $ d $ every round . - if the buyer bids according to exp3 ( or any `` mean-based '' learning algorithm ) , but the seller is restricted to `` natural '' auction formats where overbidding is dominated ( e.g . generalized first-price or generalized second-price ) , then the optimal strategy for the seller is a pay-your-bid format with decreasing reserves over time . moreover , the seller 's optimal achievable revenue is characterized by a linear program , and can be unboundedly better than the best truthful auction yet simultaneously unboundedly worse than the expected welfare .", "topics": ["regret ( decision theory )", "value ( ethics )"]}
{"title": "rise of the humanbot", "abstract": "the accelerated path of technological development , particularly at the interface between hardware and biology has been suggested as evidence for future major technological breakthroughs associated to our potential to overcome biological constraints . this includes the potential of becoming immortal , having expanded cognitive capacities thanks to hardware implants or the creation of intelligent machines . here i argue that several relevant evolutionary and structural constraints might prevent achieving most ( if not all ) these innovations . instead , the coming future will bring novelties that will challenge many other aspects of our life and that can be seen as other feasible singularities . one particularly important one has to do with the evolving interactions between humans and non-intelligent robots capable of learning and communication . here i argue that a long term interaction can lead to a new class of `` agent '' ( the humanbot ) . the way shared memories get tangled over time will inevitably have important consequences for both sides of the pair , whose identity as separated entities might become blurred and ultimately vanish . understanding such hybrid systems requires a second-order neuroscience approach while posing serious conceptual challenges , including the definition of consciousness .", "topics": ["interaction", "entity"]}
{"title": "automatic construction of real-world datasets for 3d object localization using two cameras", "abstract": "unlike classification , position labels can not be assigned manually by humans . for this reason , generating supervision for precise object localization is a hard task . this paper details a method to create large datasets for 3d object localization , with real world images , using an industrial robot to generate position labels . by knowledge of the geometry of the robot , we are able to automatically synchronize the images of the two cameras and the object 3d position . we applied it to generate a screw-driver localization dataset with stereo images , using a kuka lbr iiwa robot . this dataset could then be used to train a cnn regressor to learn end-to-end stereo object localization from a set of two standard uncalibrated cameras .", "topics": ["end-to-end principle", "robot"]}
{"title": "planar object tracking in the wild : a benchmark", "abstract": "planar object tracking plays an important role in computer vision and related fields . while several benchmarks have been constructed for evaluating state-of-the-art algorithms , there is a lack of video sequences captured in the wild rather than in constrained laboratory environment . in this paper , we present a carefully designed planar object tracking benchmark containing 210 videos of 30 planar objects sampled in the natural environment . in particular , for each object , we shoot seven videos involving various challenging factors , namely scale change , rotation , perspective distortion , motion blur , occlusion , out-of-view , and unconstrained . the ground truth is carefully annotated semi-manually to ensure the quality . moreover , eleven state-of-the-art algorithms are evaluated on the benchmark using two evaluation metrics , with detailed analysis provided for the evaluation results . we expect the proposed benchmark to benefit future studies on planar object tracking .", "topics": ["ground truth", "robot"]}
{"title": "the loss surfaces of multilayer networks", "abstract": "we study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the hamiltonian of the spherical spin-glass model under the assumptions of : i ) variable independence , ii ) redundancy in network parametrization , and iii ) uniformity . these assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory . we show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum . the number of local minima outside that band diminishes exponentially with the size of the network . we empirically verify that the mathematical model exhibits similar behavior as the computer simulations , despite the presence of high dependencies in real networks . we conjecture that both simulated annealing and sgd converge to the band of low critical points , and that all critical points found there are local minima of high quality measured by the test error . this emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered . finally , we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting .", "topics": ["loss function", "simulation"]}
{"title": "mining to compact cnf propositional formulae", "abstract": "in this paper , we propose a first application of data mining techniques to propositional satisfiability . our proposed mining4sat approach aims to discover and to exploit hidden structural knowledge for reducing the size of propositional formulae in conjunctive normal form ( cnf ) . mining4sat combines both frequent itemset mining techniques and tseitin 's encoding for a compact representation of cnf formulae . the experiments of our mining4sat approach show interesting reductions of the sizes of many application instances taken from the last sat competitions .", "topics": ["data mining"]}
{"title": "reasoning in a hierarchical system with missing group size information", "abstract": "the paper analyzes the problem of judgments or preferences subsequent to initial analysis by autonomous agents in a hierarchical system where the higher level agents does not have access to group size information . we propose methods that reduce instances of preference reversal of the kind encountered in simpson 's paradox .", "topics": ["artificial intelligence", "autonomous car"]}
{"title": "reasonet : learning to stop reading in machine comprehension", "abstract": "teaching a computer to read and answer general questions pertaining to a document is a challenging yet unsolved problem . in this paper , we describe a novel neural network architecture called the reasoning network ( reasonet ) for machine comprehension tasks . reasonets make use of multiple turns to effectively exploit and then reason over the relation among queries , documents , and answers . different from previous approaches using a fixed number of turns during inference , reasonets introduce a termination state to relax this constraint on the reasoning depth . with the use of reinforcement learning , reasonets can dynamically determine whether to continue the comprehension process after digesting intermediate results , or to terminate reading when it concludes that existing information is adequate to produce an answer . reasonets have achieved exceptional performance in machine comprehension datasets , including unstructured cnn and daily mail datasets , the stanford squad dataset , and a structured graph reachability dataset .", "topics": ["reinforcement learning"]}
{"title": "generative adversarial networks in estimation of distribution algorithms for combinatorial optimization", "abstract": "estimation of distribution algorithms ( edas ) require flexible probability models that can be efficiently learned and sampled . generative adversarial networks ( gan ) are generative neural networks which can be trained to implicitly model the probability distribution of given data , and it is possible to sample this distribution . we integrate a gan into an eda and evaluate the performance of this system when solving combinatorial optimization problems with a single objective . we use several standard benchmark problems and compare the results to state-of-the-art multivariate edas . gan-eda doe not yield competitive results - the gan lacks the ability to quickly learn a good approximation of the probability distribution . a key reason seems to be the large amount of noise present in the first eda generations .", "topics": ["approximation"]}
{"title": "attend , adapt and transfer : attentive deep architecture for adaptive transfer from multiple sources in the same domain", "abstract": "transferring knowledge from prior source tasks in solving a new target task can be useful in several learning applications . the application of transfer poses two serious challenges which have not been adequately addressed . first , the agent should be able to avoid negative transfer , which happens when the transfer hampers or slows down the learning instead of helping it . second , the agent should be able to selectively transfer , which is the ability to select and transfer from different and multiple source tasks for different parts of the state space of the target task . we propose a2t ( attend , adapt and transfer ) , an attentive deep architecture which adapts and transfers from these source tasks . our model is generic enough to effect transfer of either policies or value functions . empirical evaluations on different learning algorithms show that a2t is an effective architecture for transfer by being able to avoid negative transfer while transferring selectively from multiple source tasks in the same domain .", "topics": ["reinforcement learning"]}
{"title": "sentence similarity measures for fine-grained estimation of topical relevance in learner essays", "abstract": "we investigate the task of assessing sentence-level prompt relevance in learner essays . various systems using word overlap , neural embeddings and neural compositional models are evaluated on two datasets of learner writing . we propose a new method for sentence-level similarity calculation , which learns to adjust the weights of pre-trained word embeddings for a specific task , achieving substantially higher accuracy compared to other relevant baselines .", "topics": ["relevance"]}
{"title": "neural conditional gradients", "abstract": "the move from hand-designed to learned optimizers in machine learning has been quite successful for gradient-based and -free optimizers . when facing a constrained problem , however , maintaining feasibility typically requires a projection step , which might be computationally expensive and not differentiable . we show how the design of projection-free convex optimization algorithms can be cast as a learning problem based on frank-wolfe networks : recurrent networks implementing the frank-wolfe algorithm aka . conditional gradients . this allows them to learn to exploit structure when , e.g . , optimizing over rank-1 matrices . our lstm-learned optimizers outperform hand-designed as well learned but unconstrained ones . we demonstrate this for training support vector machines and softmax classifiers .", "topics": ["support vector machine", "support vector machine"]}
{"title": "recurrent attentional networks for saliency detection", "abstract": "convolutional-deconvolution networks can be adopted to perform end-to-end saliency detection . but , they do not work well with objects of multiple scales . to overcome such a limitation , in this work , we propose a recurrent attentional convolutional-deconvolution network ( racdnn ) . using spatial transformer and recurrent network units , racdnn is able to iteratively attend to selected image sub-regions to perform saliency refinement progressively . besides tackling the scale problem , racdnn can also learn context-aware features from past iterations to enhance saliency refinement in future iterations . experiments on several challenging saliency detection datasets validate the effectiveness of racdnn , and show that racdnn outperforms state-of-the-art saliency detection methods .", "topics": ["recurrent neural network", "iteration"]}
{"title": "convex optimization : algorithms and complexity", "abstract": "this monograph presents the main complexity theorems in convex optimization and their corresponding algorithms . starting from the fundamental theory of black-box optimization , the material progresses towards recent advances in structural optimization and stochastic optimization . our presentation of black-box optimization , strongly influenced by nesterov 's seminal book and nemirovski 's lecture notes , includes the analysis of cutting plane methods , as well as ( accelerated ) gradient descent schemes . we also pay special attention to non-euclidean settings ( relevant algorithms include frank-wolfe , mirror descent , and dual averaging ) and discuss their relevance in machine learning . we provide a gentle introduction to structural optimization with fista ( to optimize a sum of a smooth and a simple non-smooth term ) , saddle-point mirror prox ( nemirovski 's alternative to nesterov 's smoothing ) , and a concise description of interior point methods . in stochastic optimization we discuss stochastic gradient descent , mini-batches , random coordinate descent , and sublinear algorithms . we also briefly touch upon convex relaxation of combinatorial problems and the use of randomness to round solutions , as well as random walks based methods .", "topics": ["gradient descent", "gradient"]}
{"title": "differentially private empirical risk minimization : efficient algorithms and tight error bounds", "abstract": "in this paper , we initiate a systematic investigation of differentially private algorithms for convex empirical risk minimization . various instantiations of this problem have been studied before . we provide new algorithms and matching lower bounds for private erm assuming only that each data point 's contribution to the loss function is lipschitz bounded and that the domain of optimization is bounded . we provide a separate set of algorithms and matching lower bounds for the setting in which the loss functions are known to also be strongly convex . our algorithms run in polynomial time , and in some cases even match the optimal non-private running time ( as measured by oracle complexity ) . we give separate algorithms ( and lower bounds ) for $ ( \\epsilon,0 ) $ - and $ ( \\epsilon , \\delta ) $ -differential privacy ; perhaps surprisingly , the techniques used for designing optimal algorithms in the two cases are completely different . our lower bounds apply even to very simple , smooth function families , such as linear and quadratic functions . this implies that algorithms from previous work can be used to obtain optimal error rates , under the additional assumption that the contributions of each data point to the loss function is smooth . we show that simple approaches to smoothing arbitrary loss functions ( in order to apply previous techniques ) do not yield optimal error rates . in particular , optimal algorithms were not previously known for problems such as training support vector machines and the high-dimensional median .", "topics": ["time complexity", "support vector machine"]}
{"title": "validation of neural spike sorting algorithms without ground-truth information", "abstract": "we describe a suite of validation metrics that assess the credibility of a given automatic spike sorting algorithm applied to a given electrophysiological recording , when ground-truth is unavailable . by rerunning the spike sorter two or more times , the metrics measure stability under various perturbations consistent with variations in the data itself , making no assumptions about the noise model , nor about the internal workings of the sorting algorithm . such stability is a prerequisite for reproducibility of results . we illustrate the metrics on standard sorting algorithms for both in vivo and ex vivo recordings . we believe that such metrics could reduce the significant human labor currently spent on validation , and should form an essential part of large-scale automated spike sorting and systematic benchmarking of algorithms .", "topics": ["time series", "feature vector"]}
{"title": "state of the art review for applying computational intelligence and machine learning techniques to portfolio optimisation", "abstract": "computational techniques have shown much promise in the field of finance , owing to their ability to extract sense out of dauntingly complex systems . this paper reviews the most promising of these techniques , from traditional computational intelligence methods to their machine learning siblings , with particular view to their application in optimising the management of a portfolio of financial instruments . the current state of the art is assessed , and prospective further work is assessed and recommended", "topics": ["mathematical optimization"]}
{"title": "object segmentation in depth maps with one user click and a synthetically trained fully convolutional network", "abstract": "with more and more household objects built on planned obsolescence and consumed by a fast-growing population , hazardous waste recycling has become a critical challenge . given the large variability of household waste , current recycling platforms mostly rely on human operators to analyze the scene , typically composed of many object instances piled up in bulk . helping them by robotizing the unitary extraction is a key challenge to speed up this tedious process . whereas supervised deep learning has proven very efficient for such object-level scene understanding , e.g . , generic object detection and segmentation in everyday scenes , it however requires large sets of per-pixel labeled images , that are hardly available for numerous application contexts , including industrial robotics . we thus propose a step towards a practical interactive application for generating an object-oriented robotic grasp , requiring as inputs only one depth map of the scene and one user click on the next object to extract . more precisely , we address in this paper the middle issue of object seg-mentation in top views of piles of bulk objects given a pixel location , namely seed , provided interactively by a human operator . we propose a twofold framework for generating edge-driven instance segments . first , we repurpose a state-of-the-art fully convolutional object contour detector for seed-based instance segmentation by introducing the notion of edge-mask duality with a novel patch-free and contour-oriented loss function . second , we train one model using only synthetic scenes , instead of manually labeled training data . our experimental results show that considering edge-mask duality for training an encoder-decoder network , as we suggest , outperforms a state-of-the-art patch-based network in the present application context .", "topics": ["test set", "object detection"]}
{"title": "deep learning of robotic tasks without a simulator using strong and weak human supervision", "abstract": "we propose a scheme for training a computerized agent to perform complex human tasks such as highway steering . the scheme is designed to follow a natural learning process whereby a human instructor teaches a computerized trainee . the learning process consists of five elements : ( i ) unsupervised feature learning ; ( ii ) supervised imitation learning ; ( iii ) supervised reward induction ; ( iv ) supervised safety module construction ; and ( v ) reinforcement learning . we implemented the last four elements of the scheme using deep convolutional networks and applied it to successfully create a computerized agent capable of autonomous highway steering over the well-known racing game assetto corsa . we demonstrate that the use of the last four elements is essential to effectively carry out the steering task using vision alone , without access to a driving simulator internals , and operating in wall-clock time . this is made possible also through the introduction of a safety network , a novel way for preventing the agent from performing catastrophic mistakes during the reinforcement learning stage .", "topics": ["feature learning", "unsupervised learning"]}
{"title": "direct and indirect effects", "abstract": "the direct effect of one eventon another can be defined and measured byholding constant all intermediate variables between the two.indirect effects present conceptual andpractical difficulties ( in nonlinear models ) , because they can not be isolated by holding certain variablesconstant . this paper shows a way of defining any path-specific effectthat does not invoke blocking the remainingpaths.this permits the assessment of a more naturaltype of direct and indirect effects , one thatis applicable in both linear and nonlinear models . the paper establishesconditions under which such assessments can be estimated consistentlyfrom experimental and nonexperimental data , and thus extends path-analytic techniques tononlinear and nonparametric models .", "topics": ["nonlinear system"]}
{"title": "convolutional neural knowledge graph learning", "abstract": "previous models for learning entity and relationship embeddings of knowledge graphs such as transe , transh , and transr aim to explore new links based on learned representations . however , these models interpret relationships as simple translations on entity embeddings . in this paper , we try to learn more complex connections between entities and relationships . in particular , we use a convolutional neural network ( cnn ) to learn entity and relationship representations in knowledge graphs . in our model , we treat entities and relationships as one-dimensional numerical sequences with the same length . after that , we combine each triplet of head , relationship , and tail together as a matrix with height 3 . cnn is applied to the triplets to get confidence scores . positive and manually corrupted negative triplets are used to train the embeddings and the cnn model simultaneously . experimental results on public benchmark datasets show that the proposed model outperforms state-of-the-art models on exploring unseen relationships , which proves that cnn is effective to learn complex interactive patterns between entities and relationships .", "topics": ["numerical analysis", "entity"]}
{"title": "variable selection for clustering with gaussian mixture models : state of the art", "abstract": "the mixture models have become widely used in clustering , given its probabilistic framework in which its based , however , for modern databases that are characterized by their large size , these models behave disappointingly in setting out the model , making essential the selection of relevant variables for this type of clustering . after recalling the basics of clustering based on a model , this article will examine the variable selection methods for model-based clustering , as well as presenting opportunities for improvement of these methods .", "topics": ["cluster analysis", "database"]}
{"title": "coupled clustering : a method for detecting structural correspondence", "abstract": "this paper proposes a new paradigm and computational framework for identification of correspondences between sub-structures of distinct composite systems . for this , we define and investigate a variant of traditional data clustering , termed coupled clustering , which simultaneously identifies corresponding clusters within two data sets . the presented method is demonstrated and evaluated for detecting topical correspondences in textual corpora .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "color and gradient features for text segmentation from video frames", "abstract": "text segmentation in a video is drawing attention of researchers in the field of image processing , pattern recognition and document image analysis because it helps in annotating and labeling video events accurately . we propose a novel idea of generating an enhanced frame from the r , g , and b channels of an input frame by grouping high and low values using min-max clustering criteria . we also perform sliding window on enhanced frame to group high and low values from the neighboring pixel values to further enhance the frame . subsequently , we use k-means with k=2 clustering algorithm to separate text and non-text regions . the fully connected components will be identified in the skeleton of the frame obtained by k-means clustering . concept of connected component analysis based on gradient feature has been adapted for the purpose of symmetry verification . the components which satisfy symmetric verification are selected to be the representatives of text regions and they are permitted to grow to cover their respective region fully containing text . the method is tested on variety of video frames to evaluate the performance of the method in terms of recall , precision and f-measure . the results show that method is promising and encouraging .", "topics": ["image processing", "cluster analysis"]}
{"title": "unified deep supervised domain adaptation and generalization", "abstract": "this work provides a unified framework for addressing the problem of visual supervised domain adaptation and generalization with deep models . the main idea is to exploit the siamese architecture to learn an embedding subspace that is discriminative , and where mapped visual domains are semantically aligned and yet maximally separated . the supervised setting becomes attractive especially when only few target data samples need to be labeled . in this scenario , alignment and separation of semantic probability distributions is difficult because of the lack of data . we found that by reverting to point-wise surrogates of distribution distances and similarities provides an effective solution . in addition , the approach has a high speed of adaptation , which requires an extremely low number of labeled target training samples , even one per category can be effective . the approach is extended to domain generalization . for both applications the experiments show very promising results .", "topics": ["supervised learning"]}
{"title": "blind deconvolution with non-local sparsity reweighting", "abstract": "blind deconvolution has made significant progress in the past decade . most successful algorithms are classified either as variational or maximum a-posteriori ( $ map $ ) . in spite of the superior theoretical justification of variational techniques , carefully constructed $ map $ algorithms have proven equally effective in practice . in this paper , we show that all successful $ map $ and variational algorithms share a common framework , relying on the following key principles : sparsity promotion in the gradient domain , $ l_2 $ regularization for kernel estimation , and the use of convex ( often quadratic ) cost functions . our observations lead to a unified understanding of the principles required for successful blind deconvolution . we incorporate these principles into a novel algorithm that improves significantly upon the state of the art .", "topics": ["calculus of variations", "matrix regularization"]}
{"title": "the curious robot : learning visual representations via physical interactions", "abstract": "what is the right supervisory signal to train visual representations ? current approaches in computer vision use category labels from datasets such as imagenet to train convnets . however , in case of biological agents , visual representation learning does not require millions of semantic labels . we argue that biological agents use physical interactions with the world to learn visual representations unlike current vision systems which just use passive observations ( images and videos downloaded from web ) . for example , babies push objects , poke them , put them in their mouth and throw them to learn representations . towards this goal , we build one of the first systems on a baxter platform that pushes , pokes , grasps and observes objects in a tabletop environment . it uses four different types of physical interactions to collect more than 130k datapoints , with each datapoint providing supervision to a shared convnet architecture allowing us to learn visual representations . we show the quality of learned representations by observing neuron activations and performing nearest neighbor retrieval on this learned representation . quantitatively , we evaluate our learned convnet on image classification tasks and show improvements compared to learning without external data . finally , on the task of instance retrieval , our network outperforms the imagenet network on recall @ 1 by 3 %", "topics": ["feature learning", "computer vision"]}
{"title": "learning runtime parameters in computer systems with delayed experience injection", "abstract": "learning effective configurations in computer systems without hand-crafting models for every parameter is a long-standing problem . this paper investigates the use of deep reinforcement learning for runtime parameters of cloud databases under latency constraints . cloud services serve up to thousands of concurrent requests per second and can adjust critical parameters by leveraging performance metrics . in this work , we use continuous deep reinforcement learning to learn optimal cache expirations for http caching in content delivery networks . to this end , we introduce a technique for asynchronous experience management called delayed experience injection , which facilitates delayed reward and next-state computation in concurrent environments where measurements are not immediately available . evaluation results show that our approach based on normalized advantage functions and asynchronous cpu-only training outperforms a statistical estimator .", "topics": ["reinforcement learning", "computation"]}
{"title": "cosface : large margin cosine loss for deep face recognition", "abstract": "face recognition has achieved revolutionary advancement owing to the advancement of the deep convolutional neural network ( cnn ) . the central task of face recognition , including face verification and identification , involves face feature discrimination . however , traditional softmax loss of deep cnn usually lacks the power of discrimination . to address this problem , recently several loss functions such as central loss \\cite { centerloss } , large margin softmax loss \\cite { lsoftmax } , and angular softmax loss \\cite { sphereface } have been proposed . all these improvement algorithms share the same idea : maximizing inter-class variance and minimizing intra-class variance . in this paper , we design a novel loss function , namely large margin cosine loss ( lmcl ) , to realize this idea from a different perspective . more specifically , we reformulate the softmax loss as cosine loss by l2 normalizing both features and weight vectors to remove radial variation , based on which a cosine margin term \\emph { $ m $ } is introduced to further maximize decision margin in angular space . as a result , minimum intra-class variance and maximum inter-class variance are achieved by normalization and cosine decision margin maximization . we refer to our model trained with lmcl as cosface . to test our approach , extensive experimental evaluations are conducted on the most popular public-domain face recognition datasets such as megaface challenge , youtube faces ( ytf ) and labeled face in the wild ( lfw ) . we achieve the state-of-the-art performance on these benchmark experiments , which confirms the effectiveness of our approach .", "topics": ["loss function"]}
{"title": "evolving intraday foreign exchange trading strategies utilizing multiple instruments price series", "abstract": "we propose a genetic programming architecture for the generation of foreign exchange trading strategies . the system 's principal features are the evolution of free-form strategies which do not rely on any prior models and the utilization of price series from multiple instruments as input data . this latter feature constitutes an innovation with respect to previous works documented in literature . in this article we utilize open , high , low , close bar data at a 5 minutes frequency for the aud.usd , eur.usd , gbp.usd and usd.jpy currency pairs . we will test the implementation analyzing the in-sample and out-of-sample performance of strategies for trading the usd.jpy obtained across multiple algorithm runs . we will also evaluate the differences between strategies selected according to two different criteria : one relies on the fitness obtained on the training set only , the second one makes use of an additional validation dataset . strategy activity and trade accuracy are remarkably stable between in and out of sample results . from a profitability aspect , the two criteria both result in strategies successful on out-of-sample data but exhibiting different characteristics . the overall best performing out-of-sample strategy achieves a yearly return of 19 % .", "topics": ["test set"]}
{"title": "improving orbit prediction accuracy through supervised machine learning", "abstract": "due to the lack of information such as the space environment condition and resident space objects ' ( rsos ' ) body characteristics , current orbit predictions that are solely grounded on physics-based models may fail to achieve required accuracy for collision avoidance and have led to satellite collisions already . this paper presents a methodology to predict rsos ' trajectories with higher accuracy than that of the current methods . inspired by the machine learning ( ml ) theory through which the models are learned based on large amounts of observed data and the prediction is conducted without explicitly modeling space objects and space environment , the proposed ml approach integrates physics-based orbit prediction algorithms with a learning-based process that focuses on reducing the prediction errors . using a simulation-based space catalog environment as the test bed , the paper demonstrates three types of generalization capability for the proposed ml approach : 1 ) the ml model can be used to improve the same rso 's orbit information that is not available during the learning process but shares the same time interval as the training data ; 2 ) the ml model can be used to improve predictions of the same rso at future epochs ; and 3 ) the ml model based on a rso can be applied to other rsos that share some common features .", "topics": ["test set", "simulation"]}
{"title": "self-supervised learning of motion capture", "abstract": "current state-of-the-art solutions for motion capture from a single camera are optimization driven : they optimize the parameters of a 3d human model so that its re-projection matches measurements in the video ( e.g . person segmentation , optical flow , keypoint detections etc . ) . optimization models are susceptible to local minima . this has been the bottleneck that forced using clean green-screen like backgrounds at capture time , manual initialization , or switching to multiple cameras as input resource . in this work , we propose a learning based motion capture model for single camera input . instead of optimizing mesh and skeleton parameters directly , our model optimizes neural network weights that predict 3d shape and skeleton configurations given a monocular rgb video . our model is trained using a combination of strong supervision from synthetic data , and self-supervision from differentiable rendering of ( a ) skeletal keypoints , ( b ) dense 3d mesh motion , and ( c ) human-background segmentation , in an end-to-end framework . empirically we show our model combines the best of both worlds of supervised learning and test-time optimization : supervised learning initializes the model parameters in the right regime , ensuring good pose and surface initialization at test time , without manual effort . self-supervision by back-propagating through differentiable rendering allows ( unsupervised ) adaptation of the model to the test data , and offers much tighter fit than a pretrained fixed model . we show that the proposed model improves with experience and converges to low-error solutions where previous optimization methods fail .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "a massive local rules search approach to the classification problem", "abstract": "an approach to the classification problem of machine learning , based on building local classification rules , is developed . the local rules are considered as projections of the global classification rules to the event we want to classify . a massive global optimization algorithm is used for optimization of quality criterion . the algorithm , which has polynomial complexity in typical case , is used to find all high -- quality local rules . the other distinctive feature of the algorithm is the integration of attributes levels selection ( for ordered attributes ) with rules searching and original conflicting rules resolution strategy . the algorithm is practical ; it was tested on a number of data sets from uci repository , and a comparison with the other predicting techniques is presented .", "topics": ["time complexity", "eisenstein 's criterion"]}
{"title": "learning dynamic hierarchical models for anytime scene labeling", "abstract": "with increasing demand for efficient image and video analysis , test-time cost of scene parsing becomes critical for many large-scale or time-sensitive vision applications . we propose a dynamic hierarchical model for anytime scene labeling that allows us to achieve flexible trade-offs between efficiency and accuracy in pixel-level prediction . in particular , our approach incorporates the cost of feature computation and model inference , and optimizes the model performance for any given test-time budget by learning a sequence of image-adaptive hierarchical models . we formulate this anytime representation learning as a markov decision process with a discrete-continuous state-action space . a high-quality policy of feature and model selection is learned based on an approximate policy iteration method with action proposal mechanism . we demonstrate the advantages of our dynamic non-myopic anytime scene parsing on three semantic segmentation datasets , which achieves $ 90\\ % $ of the state-of-the-art performances by using $ 15\\ % $ of their overall costs .", "topics": ["feature learning", "approximation algorithm"]}
{"title": "on optimistic versus randomized exploration in reinforcement learning", "abstract": "we discuss the relative merits of optimistic and randomized approaches to exploration in reinforcement learning . optimistic approaches presented in the literature apply an optimistic boost to the value estimate at each state-action pair and select actions that are greedy with respect to the resulting optimistic value function . randomized approaches sample from among statistically plausible value functions and select actions that are greedy with respect to the random sample . prior computational experience suggests that randomized approaches can lead to far more statistically efficient learning . we present two simple analytic examples that elucidate why this is the case . in principle , there should be optimistic approaches that fare well relative to randomized approaches , but that would require intractable computation . optimistic approaches that have been proposed in the literature sacrifice statistical efficiency for the sake of computational efficiency . randomized approaches , on the other hand , may enable simultaneous statistical and computational efficiency .", "topics": ["reinforcement learning", "computation"]}
{"title": "segmentation of skin lesions based on fuzzy classification of pixels and histogram thresholding", "abstract": "this paper proposes an innovative method for segmentation of skin lesions in dermoscopy images developed by the authors , based on fuzzy classification of pixels and histogram thresholding .", "topics": ["pixel"]}
{"title": "online learning to rank with top-k feedback", "abstract": "we consider two settings of online learning to rank where feedback is restricted to top ranked items . the problem is cast as an online game between a learner and sequence of users , over $ t $ rounds . in both settings , the learners objective is to present ranked list of items to the users . the learner 's performance is judged on the entire ranked list and true relevances of the items . however , the learner receives highly restricted feedback at end of each round , in form of relevances of only the top $ k $ ranked items , where $ k \\ll m $ . the first setting is \\emph { non-contextual } , where the list of items to be ranked is fixed . the second setting is \\emph { contextual } , where lists of items vary , in form of traditional query-document lists . no stochastic assumption is made on the generation process of relevances of items and contexts . we provide efficient ranking strategies for both the settings . the strategies achieve $ o ( t^ { 2/3 } ) $ regret , where regret is based on popular ranking measures in first setting and ranking surrogates in second setting . we also provide impossibility results for certain ranking measures and a certain class of surrogates , when feedback is restricted to the top ranked item , i.e . $ k=1 $ . we empirically demonstrate the performance of our algorithms on simulated and real world datasets .", "topics": ["regret ( decision theory )", "simulation"]}
{"title": "image representation learning using graph regularized auto-encoders", "abstract": "we consider the problem of image representation for the tasks of unsupervised learning and semi-supervised learning . in those learning tasks , the raw image vectors may not provide enough representation for their intrinsic structures due to their highly dense feature space . to overcome this problem , the raw image vectors should be mapped to a proper representation space which can capture the latent structure of the original data and represent the data explicitly for further learning tasks such as clustering . inspired by the recent research works on deep neural network and representation learning , in this paper , we introduce the multiple-layer auto-encoder into image representation , we also apply the locally invariant ideal to our image representation with auto-encoders and propose a novel method , called graph regularized auto-encoder ( gae ) . gae can provide a compact representation which uncovers the hidden semantics and simultaneously respects the intrinsic geometric structure . extensive experiments on image clustering show encouraging results of the proposed algorithm in comparison to the state-of-the-art algorithms on real-word cases .", "topics": ["feature learning", "nonlinear system"]}
{"title": "hierarchical clustering using randomly selected similarities", "abstract": "the problem of hierarchical clustering items from pairwise similarities is found across various scientific disciplines , from biology to networking . often , applications of clustering techniques are limited by the cost of obtaining similarities between pairs of items . while prior work has been developed to reconstruct clustering using a significantly reduced set of pairwise similarities via adaptive measurements , these techniques are only applicable when choice of similarities are available to the user . in this paper , we examine reconstructing hierarchical clustering under similarity observations at-random . we derive precise bounds which show that a significant fraction of the hierarchical clustering can be recovered using fewer than all the pairwise similarities . we find that the correct hierarchical clustering down to a constant fraction of the total number of items ( i.e . , clusters sized o ( n ) ) can be found using only o ( n log n ) randomly selected pairwise similarities in expectation .", "topics": ["cluster analysis"]}
{"title": "scalable and robust construction of topical hierarchies", "abstract": "automated generation of high-quality topical hierarchies for a text collection is a dream problem in knowledge engineering with many valuable applications . in this paper a scalable and robust algorithm is proposed for constructing a hierarchy of topics from a text collection . we divide and conquer the problem using a top-down recursive framework , based on a tensor orthogonal decomposition technique . we solve a critical challenge to perform scalable inference for our newly designed hierarchical topic model . experiments with various real-world datasets illustrate its ability to generate robust , high-quality hierarchies efficiently . our method reduces the time of construction by several orders of magnitude , and its robust feature renders it possible for users to interactively revise the hierarchy .", "topics": ["scalability"]}
{"title": "comparing several heuristics for a packing problem", "abstract": "packing problems are in general np-hard , even for simple cases . since now there are no highly efficient algorithms available for solving packing problems . the two-dimensional bin packing problem is about packing all given rectangular items , into a minimum size rectangular bin , without overlapping . the restriction is that the items can not be rotated . the current paper is comparing a greedy algorithm with a hybrid genetic algorithm in order to see which technique is better for the given problem . the algorithms are tested on different sizes data .", "topics": ["heuristic"]}
{"title": "shape characterization via boundary distortion", "abstract": "in this paper , we derive new shape descriptors based on a directional characterization . the main idea is to study the behavior of the shape neighborhood under family of transformations . we obtain a description invariant with respect to rotation , reflection , translation and scaling . a well-defined metric is then proposed on the associated feature space . we show the continuity of this metric . some results on shape retrieval are provided on two databases to show the accuracy of the proposed shape metric .", "topics": ["feature vector", "database"]}
{"title": "a regularization approach to blind deblurring and denoising of qr barcodes", "abstract": "qr bar codes are prototypical images for which part of the image is a priori known ( required patterns ) . open source bar code readers , such as zbar , are readily available . we exploit both these facts to provide and assess purely regularization-based methods for blind deblurring of qr bar codes in the presence of noise .", "topics": ["noise reduction", "matrix regularization"]}
{"title": "topic supervised non-negative matrix factorization", "abstract": "topic models have been extensively used to organize and interpret the contents of large , unstructured corpora of text documents . although topic models often perform well on traditional training vs. test set evaluations , it is often the case that the results of a topic model do not align with human interpretation . this interpretability fallacy is largely due to the unsupervised nature of topic models , which prohibits any user guidance on the results of a model . in this paper , we introduce a semi-supervised method called topic supervised non-negative matrix factorization ( ts-nmf ) that enables the user to provide labeled example documents to promote the discovery of more meaningful semantic structure of a corpus . in this way , the results of ts-nmf better match the intuition and desired labeling of the user . the core of ts-nmf relies on solving a non-convex optimization problem for which we derive an iterative algorithm that is shown to be monotonic and convergent to a local optimum . we demonstrate the practical utility of ts-nmf on the reuters and pubmed corpora , and find that ts-nmf is especially useful for conceptual or broad topics , where topic key terms are not well understood . although identifying an optimal latent structure for the data is not a primary objective of the proposed approach , we find that ts-nmf achieves higher weighted jaccard similarity scores than the contemporary methods , ( unsupervised ) nmf and latent dirichlet allocation , at supervision rates as low as 10 % to 20 % .", "topics": ["test set", "mathematical optimization"]}
{"title": "deep learning on fpgas : past , present , and future", "abstract": "the rapid growth of data size and accessibility in recent years has instigated a shift of philosophy in algorithm design for artificial intelligence . instead of engineering algorithms by hand , the ability to learn composable systems automatically from massive amounts of data has led to ground-breaking performance in important domains such as computer vision , speech recognition , and natural language processing . the most popular class of techniques used in these domains is called deep learning , and is seeing significant attention from industry . however , these models require incredible amounts of data and compute power to train , and are limited by the need for better hardware acceleration to accommodate scaling beyond current data and model sizes . while the current solution has been to use clusters of graphics processing units ( gpu ) as general purpose processors ( gpgpu ) , the use of field programmable gate arrays ( fpga ) provide an interesting alternative . current trends in design tools for fpgas have made them more compatible with the high-level software practices typically practiced in the deep learning community , making fpgas more accessible to those who build and deploy models . since fpga architectures are flexible , this could also allow researchers the ability to explore model-level optimizations beyond what is possible on fixed architectures such as gpus . as well , fpgas tend to provide high performance per watt of power consumption , which is of particular importance for application scientists interested in large scale server-based deployment or resource-limited embedded applications . this review takes a look at deep learning and fpgas from a hardware acceleration perspective , identifying trends and innovations that make these technologies a natural fit , and motivates a discussion on how fpgas may best serve the needs of the deep learning community moving forward .", "topics": ["natural language processing", "high- and low-level"]}
{"title": "multi-dimensional parametric mincuts for constrained map inference", "abstract": "in this paper , we propose novel algorithms for inferring the maximum a posteriori ( map ) solution of discrete pairwise random field models under multiple constraints . we show how this constrained discrete optimization problem can be formulated as a multi-dimensional parametric mincut problem via its lagrangian dual , and prove that our algorithm isolates all constraint instances for which the problem can be solved exactly . these multiple solutions enable us to even deal with `soft constraints ' ( higher order penalty functions ) . moreover , we propose two practical variants of our algorithm to solve problems with hard constraints . we also show how our method can be applied to solve various constrained discrete optimization problems such as submodular minimization and shortest path computation . experimental evaluation using the foreground-background image segmentation problem with statistic constraints reveals that our method is faster and its results are closer to the ground truth labellings compared with the popular continuous relaxation based methods .", "topics": ["optimization problem", "image segmentation"]}
{"title": "employing wikipedia 's natural intelligence for cross language information retrieval", "abstract": "in this paper we present a novel method for retrieving information in languages other than that of the query . we use this technique in combination with existing traditional cross language information retrieval ( clir ) techniques to improve their results . this method has a number of advantages over traditional techniques that rely on machine translation to translate the query and then search the target document space using a machine translation . this method is not limited to the availability of a machine translation algorithm for the desired language and uses already existing sources of readily available translated information on the internet as a `` middle-man '' approach . in this paper we use wikipedia ; however , any similar multilingual , cross referenced body of documents can be used . for evaluation and comparison purposes we also implemented a traditional machine translation approach separately as well as the wikipedia approach separately .", "topics": ["machine translation"]}
{"title": "the total variation on hypergraphs - learning on hypergraphs revisited", "abstract": "hypergraphs allow one to encode higher-order relationships in data and are thus a very flexible modeling tool . current learning methods are either based on approximations of the hypergraphs via graphs or on tensor methods which are only applicable under special conditions . in this paper , we present a new learning framework on hypergraphs which fully uses the hypergraph structure . the key element is a family of regularization functionals based on the total variation on hypergraphs .", "topics": ["matrix regularization", "approximation"]}
{"title": "online representation learning in recurrent neural language models", "abstract": "we investigate an extension of continuous online learning in recurrent neural network language models . the model keeps a separate vector representation of the current unit of text being processed and adaptively adjusts it after each prediction . the initial experiments give promising results , indicating that the method is able to increase language modelling accuracy , while also decreasing the parameters needed to store the model along with the computation required at each step .", "topics": ["recurrent neural network", "computation"]}
{"title": "determining points on handwritten mathematical symbols", "abstract": "in a variety of applications , such as handwritten mathematics and diagram labelling , it is common to have symbols of many different sizes in use and for the writing not to follow simple baselines . in order to understand the scale and relative positioning of individual characters , it is necessary to identify the location of certain expected features . these are typically identified by particular points in the symbols , for example , the baseline of a lower case `` p '' would be identified by the lowest part of the bowl , ignoring the descender . we investigate how to find these special points automatically so they may be used in a number of problems , such as improving two-dimensional mathematical recognition and in handwriting neatening , while preserving the original style .", "topics": ["baseline ( configuration management )"]}
{"title": "learning efficient convolutional networks through network slimming", "abstract": "the deployment of deep convolutional neural networks ( cnns ) in many real world applications is largely hindered by their high computational cost . in this paper , we propose a novel learning scheme for cnns to simultaneously 1 ) reduce the model size ; 2 ) decrease the run-time memory footprint ; and 3 ) lower the number of computing operations , without compromising accuracy . this is achieved by enforcing channel-level sparsity in the network in a simple but effective way . different from many existing approaches , the proposed method directly applies to modern cnn architectures , introduces minimum overhead to the training process , and requires no special software/hardware accelerators for the resulting models . we call our approach network slimming , which takes wide and large networks as input models , but during training insignificant channels are automatically identified and pruned afterwards , yielding thin and compact models with comparable accuracy . we empirically demonstrate the effectiveness of our approach with several state-of-the-art cnn models , including vggnet , resnet and densenet , on various image classification datasets . for vggnet , a multi-pass version of network slimming gives a 20x reduction in model size and a 5x reduction in computing operations .", "topics": ["computer vision", "sparse matrix"]}
{"title": "semi-supervised multi-task learning for lung cancer diagnosis", "abstract": "early detection of lung nodules is of great importance in lung cancer screening . existing research recognizes the critical role played by cad systems in early detection and diagnosis of lung nodules . however , many cad systems , which are used as cancer detection tools , produce a lot of false positives ( fp ) and require a further fp reduction step . furthermore , guidelines for early diagnosis and treatment of lung cancer are consist of different shape and volume measurements of abnormalities . segmentation is at the heart of our understanding of nodules morphology making it a major area of interest within the field of computer aided diagnosis systems . this study set out to test the hypothesis that joint learning of false positive ( fp ) nodule reduction and nodule segmentation can improve the computer aided diagnosis ( cad ) systems ' performance on both tasks . to support this hypothesis we propose a 3d deep multi-task cnn to tackle these two problems jointly . we tested our system on luna16 dataset and achieved an average dice similarity coefficient ( dsc ) of 91 % as segmentation accuracy and a score of nearly 92 % for fp reduction . as a proof of our hypothesis , we showed improvements of segmentation and fp reduction tasks over two baselines . our results support that joint training of these two tasks through a multi-task learning approach improves system performance on both . we also showed that a semi-supervised approach can be used to overcome the limitation of lack of labeled data for the 3d segmentation task .", "topics": ["baseline ( configuration management )"]}
{"title": "introducing machine learning for power system operation support", "abstract": "we address the problem of assisting human dispatchers in operating power grids in today 's changing context using machine learning , with theaim of increasing security and reducing costs . power networks are highly regulated systems , which at all times must meet varying demands of electricity with a complex production system , including conventional power plants , less predictable renewable energies ( such as wind or solar power ) , and the possibility of buying/selling electricity on the international market with more and more actors involved at a europeanscale . this problem is becoming ever more challenging in an aging network infrastructure . one of the primary goals of dispatchers is to protect equipment ( e.g . avoid that transmission lines overheat ) with few degrees of freedom : we are considering in this paper solely modifications in network topology , i.e . re-configuring the way in which lines , transformers , productions and loads are connected in sub-stations . using years of historical data collected by the french transmission service operator ( tso ) `` r\\'eseau de transport d'electricit\\'e '' ( rte ) , we develop novel machine learning techniques ( drawing on `` deep learning '' ) to mimic human decisions to devise `` remedial actions '' to prevent any line to violate power flow limits ( so-called `` thermal limits '' ) . the proposed technique is hybrid . it does not rely purely on machine learning : every action will be tested with actual simulators before being proposed to the dispatchers or implemented on the grid .", "topics": ["simulation"]}
{"title": "pronouncur : an urdu pronunciation lexicon generator", "abstract": "state-of-the-art speech recognition systems rely heavily on three basic components : an acoustic model , a pronunciation lexicon and a language model . to build these components , a researcher needs linguistic as well as technical expertise , which is a barrier in low-resource domains . techniques to construct these three components without having expert domain knowledge are in great demand . urdu , despite having millions of speakers all over the world , is a low-resource language in terms of standard publically available linguistic resources . in this paper , we present a grapheme-to-phoneme conversion tool for urdu that generates a pronunciation lexicon in a form suitable for use with speech recognition systems from a list of urdu words . the tool predicts the pronunciation of words using a lstm-based model trained on a handcrafted expert lexicon of around 39,000 words and shows an accuracy of 64 % upon internal evaluation . for external evaluation on a speech recognition task , we obtain a word error rate comparable to one achieved using a fully handcrafted expert lexicon .", "topics": ["speech recognition"]}
{"title": "recognizing involuntary actions from 3d skeleton data using body states", "abstract": "human action recognition has been one of the most active fields of research in computer vision for last years . two dimensional action recognition methods are facing serious challenges such as occlusion and missing the third dimension of data . development of depth sensors has made it feasible to track positions of human body joints over time . this paper proposes a novel method of action recognition which uses temporal 3d skeletal kinect data . this method introduces the definition of body states and then every action is modeled as a sequence of these states . the learning stage uses fisher linear discriminant analysis ( lda ) to construct discriminant feature space for discriminating the body states . moreover , this paper suggests the use of the mahalonobis distance as an appropriate distance metric for the classification of the states of involuntary actions . hidden markov model ( hmm ) is then used to model the temporal transition between the body states in each action . according to the results , this method significantly outperforms other popular methods , with recognition rate of 88.64 % for eight different actions and up to 96.18 % for classifying fall actions .", "topics": ["feature vector", "computer vision"]}
{"title": "principal differences analysis : interpretable characterization of differences between distributions", "abstract": "we introduce principal differences analysis ( pda ) for analyzing differences between high-dimensional distributions . the method operates by finding the projection that maximizes the wasserstein divergence between the resulting univariate populations . relying on the cramer-wold device , it requires no assumptions about the form of the underlying distributions , nor the nature of their inter-class differences . a sparse variant of the method is introduced to identify features responsible for the differences . we provide algorithms for both the original minimax formulation as well as its semidefinite relaxation . in addition to deriving some convergence results , we illustrate how the approach may be applied to identify differences between cell populations in the somatosensory cortex and hippocampus as manifested by single cell rna-seq . our broader framework extends beyond the specific choice of wasserstein divergence .", "topics": ["sparse matrix"]}
{"title": "improved asynchronous parallel optimization analysis for stochastic incremental methods", "abstract": "as datasets continue to increase in size and multi-core computer architectures are developed , asynchronous parallel optimization algorithms become more and more essential to the field of machine learning . unfortunately , conducting the theoretical analysis asynchronous methods is difficult , notably due to the introduction of delay and inconsistency in inherently sequential algorithms . handling these issues often requires resorting to simplifying but unrealistic assumptions . through a novel perspective , we revisit and clarify a subtle but important technical issue present in a large fraction of the recent convergence rate proofs for asynchronous parallel optimization algorithms , and propose a simplification of the recently introduced `` perturbed iterate '' framework that resolves it . we demonstrate the usefulness of our new framework by analyzing three distinct asynchronous parallel incremental optimization algorithms : hogwild ( asynchronous sgd ) , kromagnon ( asynchronous svrg ) and asaga , a novel asynchronous parallel version of the incremental gradient algorithm saga that enjoys fast linear convergence rates . we are able to both remove problematic assumptions and obtain better theoretical results . notably , we prove that asaga and kromagnon can obtain a theoretical linear speedup on multi-core systems even without sparsity assumptions . we present results of an implementation on a 40-core architecture illustrating the practical speedups as well as the hardware overhead . finally , we investigate the overlap constant , an ill-understood but central quantity for the theoretical analysis of asynchronous parallel algorithms . we find that it encompasses much more complexity than suggested in previous work , and often is order-of-magnitude bigger than traditionally thought .", "topics": ["sparse matrix", "gradient"]}
{"title": "on the organization of grid and place cells : neural de-noising via subspace learning", "abstract": "place cells in the hippocampus are active when an animal visits a certain locations ( referred to as place fields ) within an environment and remain silent otherwise . grid cells in the medial entorhinal cortex ( mec ) respond at multiple locations , with firing fields that exhibit a hexagonally symmetric periodic pattern . the joint activity of grid and place cell populations , as a function of location , forms a neural code for space . an ensemble of codes , for a given set of parameters , is generated by selecting grid and place cell population and tuning curve parameters . for each ensemble , codewords are generated by stimulating a network with a discrete set of locations . in this manuscript , we develop an understanding of the relationships between coding theoretic properties of these combined populations and code construction parameters . these observations are revisited by measuring the performances of biologically realizable algorithms ( e.g . neural bit-flipping ) implemented by a network of place and grid cell populations , as well as interneurons , which perform de-noising operations . simulations demonstrate that de-noising mechanisms analyzed here can significantly improve fidelity of this neural representation of space . further , patterns observed in connectivity of each population of simulated cells suggest the existence of heretofore unobserved neurobiological phenomena .", "topics": ["simulation"]}
{"title": "latent laplacian maximum entropy discrimination for detection of high-utility anomalies", "abstract": "data-driven anomaly detection methods suffer from the drawback of detecting all instances that are statistically rare , irrespective of whether the detected instances have real-world significance or not . in this paper , we are interested in the problem of specifically detecting anomalous instances that are known to have high real-world utility , while ignoring the low-utility statistically anomalous instances . to this end , we propose a novel method called latent laplacian maximum entropy discrimination ( latlapmed ) as a potential solution . this method uses the em algorithm to simultaneously incorporate the geometric entropy minimization principle for identifying statistical anomalies , and the maximum entropy discrimination principle to incorporate utility labels , in order to detect high-utility anomalies . we apply our method in both simulated and real datasets to demonstrate that it has superior performance over existing alternatives that independently pre-process with unsupervised anomaly detection algorithms before classifying .", "topics": ["unsupervised learning", "simulation"]}
{"title": "experimental analysis of data-driven control for a building heating system", "abstract": "driven by the opportunity to harvest the flexibility related to building climate control for demand response applications , this work presents a data-driven control approach building upon recent advancements in reinforcement learning . more specifically , model assisted batch reinforcement learning is applied to the setting of building climate control subjected to a dynamic pricing . the underlying sequential decision making problem is cast on a markov decision problem , after which the control algorithm is detailed . in this work , fitted q-iteration is used to construct a policy from a batch of experimental tuples . in those regions of the state space where the experimental sample density is low , virtual support samples are added using an artificial neural network . finally , the resulting policy is shaped using domain knowledge . the control approach has been evaluated quantitatively using a simulation and qualitatively in a living lab . from the quantitative analysis it has been found that the control approach converges in approximately 20 days to obtain a control policy with a performance within 90 % of the mathematical optimum . the experimental analysis confirms that within 10 to 20 days sensible policies are obtained that can be used for different outside temperature regimes .", "topics": ["reinforcement learning", "simulation"]}
{"title": "the bag semantics of ontology-based data access", "abstract": "ontology-based data access ( obda ) is a popular approach for integrating and querying multiple data sources by means of a shared ontology . the ontology is linked to the sources using mappings , which assign views over the data to ontology predicates . motivated by the need for obda systems supporting database-style aggregate queries , we propose a bag semantics for obda , where duplicate tuples in the views defined by the mappings are retained , as is the case in standard databases . we show that bag semantics makes conjunctive query answering in obda conp-hard in data complexity . to regain tractability , we consider a rather general class of queries and show its rewritability to a generalisation of the relational calculus to bags .", "topics": ["database"]}
{"title": "on the delays in spiking neural p systems", "abstract": "in this work we extend and improve the results done in a previous work on simulating spiking neural p systems ( snp systems in short ) with delays using snp systems without delays . we simulate the former with the latter over sequential , iteration , join , and split routing . our results provide constructions so that both systems halt at exactly the same time , start with only one spike , and produce the same number of spikes to the environment after halting .", "topics": ["simulation", "iteration"]}
{"title": "error estimation in approximate bayesian belief network inference", "abstract": "we can perform inference in bayesian belief networks by enumerating instantiations with high probability thus approximating the marginals . in this paper , we present a method for determining the fraction of instantiations that has to be considered such that the absolute error in the marginals does not exceed a predefined value . the method is based on extreme value theory . essentially , the proposed method uses the reversed generalized pareto distribution to model probabilities of instantiations below a given threshold . based on this distribution , an estimate of the maximal absolute error if instantiations with probability smaller than u are disregarded can be made .", "topics": ["approximation algorithm", "bayesian network"]}
{"title": "max-margin stacking and sparse regularization for linear classifier combination and selection", "abstract": "the main principle of stacked generalization ( or stacking ) is using a second-level generalizer to combine the outputs of base classifiers in an ensemble . in this paper , we investigate different combination types under the stacking framework ; namely weighted sum ( ws ) , class-dependent weighted sum ( cws ) and linear stacked generalization ( lsg ) . for learning the weights , we propose using regularized empirical risk minimization with the hinge loss . in addition , we propose using group sparsity for regularization to facilitate classifier selection . we performed experiments using two different ensemble setups with differing diversities on 8 real-world datasets . results show the power of regularized learning with the hinge loss function . using sparse regularization , we are able to reduce the number of selected classifiers of the diverse ensemble without sacrificing accuracy . with the non-diverse ensembles , we even gain accuracy on average by using sparse regularization .", "topics": ["matrix regularization", "loss function"]}
{"title": "a combinatorial algorithm to compute regularization paths", "abstract": "for a wide variety of regularization methods , algorithms computing the entire solution path have been developed recently . solution path algorithms do not only compute the solution for one particular value of the regularization parameter but the entire path of solutions , making the selection of an optimal parameter much easier . most of the currently used algorithms are not robust in the sense that they can not deal with general or degenerate input . here we present a new robust , generic method for parametric quadratic programming . our algorithm directly applies to nearly all machine learning applications , where so far every application required its own different algorithm . we illustrate the usefulness of our method by applying it to a very low rank problem which could not be solved by existing path tracking methods , namely to compute part-worth values in choice based conjoint analysis , a popular technique from market research to estimate consumers preferences on a class of parameterized options .", "topics": ["value ( ethics )", "matrix regularization"]}
{"title": "sum-product-quotient networks", "abstract": "we present a novel tractable generative model that extends sum-product networks ( spns ) and significantly boosts their power . we call it sum-product-quotient networks ( spqns ) , whose core concept is to incorporate conditional distributions into the model by direct computation using quotient nodes , e.g . $ p ( a|b ) = \\frac { p ( a , b ) } { p ( b ) } $ . we provide sufficient conditions for the tractability of spqns that generalize and relax the decomposable and complete tractability conditions of spns . these relaxed conditions give rise to an exponential boost to the expressive efficiency of our model , i.e . we prove that there are distributions which spqns can compute efficiently but require spns to be of exponential size . thus , we narrow the gap in expressivity between tractable graphical models and other neural network-based generative models .", "topics": ["generative model", "graphical model"]}
{"title": "potholes on the royal road", "abstract": "it is still unclear how an evolutionary algorithm ( ea ) searches a fitness landscape , and on what fitness landscapes a particular ea will do well . the validity of the building-block hypothesis , a major tenet of traditional genetic algorithm theory , remains controversial despite its continued use to justify claims about eas . this paper outlines a research program to begin to answer some of these open questions , by extending the work done in the royal road project . the short-term goal is to find a simple class of functions which the simple genetic algorithm optimizes better than other optimization methods , such as hillclimbers . a dialectical heuristic for searching for such a class is introduced . as an example of using the heuristic , the simple genetic algorithm is compared with a set of hillclimbers on a simple subset of the hyperplane-defined functions , the pothole functions .", "topics": ["heuristic"]}
{"title": "unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints", "abstract": "we present a novel approach for unsupervised learning of depth and ego-motion from monocular video . unsupervised learning removes the need for separate supervisory signals ( depth or ego-motion ground truth , or multi-view video ) . prior work in unsupervised depth learning uses pixel-wise or gradient-based losses , which only consider pixels in small local neighborhoods . our main contribution is to explicitly consider the inferred 3d geometry of the scene , enforcing consistency of the estimated 3d point clouds and ego-motion across consecutive frames . this is a challenging task and is solved by a novel ( approximate ) backpropagation algorithm for aligning 3d structures . we combine this novel 3d-based loss with 2d losses based on photometric quality of frame reconstructions using estimated depth and ego-motion from adjacent frames . we also incorporate validity masks to avoid penalizing areas in which no useful information exists . we test our algorithm on the kitti dataset and on a video dataset captured on an uncalibrated mobile phone camera . our proposed approach consistently improves depth estimates on both datasets , and outperforms the state-of-the-art for both depth and ego-motion . because we only require a simple video , learning depth and ego-motion on large and varied datasets becomes possible . we demonstrate this by training on the low quality uncalibrated video dataset and evaluating on kitti , ranking among top performing prior methods which are trained on kitti itself .", "topics": ["approximation algorithm", "unsupervised learning"]}
{"title": "an improved search algorithm for optimal multiple-sequence alignment", "abstract": "multiple sequence alignment ( msa ) is a ubiquitous problem in computational biology . although it is np-hard to find an optimal solution for an arbitrary number of sequences , due to the importance of this problem researchers are trying to push the limits of exact algorithms further . since msa can be cast as a classical path finding problem , it is attracting a growing number of ai researchers interested in heuristic search algorithms as a challenge with actual practical relevance . in this paper , we first review two previous , complementary lines of research . based on hirschbergs algorithm , dynamic programming needs o ( kn^ ( k-1 ) ) space to store both the search frontier and the nodes needed to reconstruct the solution path , for k sequences of length n . best first search , on the other hand , has the advantage of bounding the search space that has to be explored using a heuristic . however , it is necessary to maintain all explored nodes up to the final solution in order to prevent the search from re-expanding them at higher cost . earlier approaches to reduce the closed list are either incompatible with pruning methods for the open list , or must retain at least the boundary of the closed list . in this article , we present an algorithm that attempts at combining the respective advantages ; like a* it uses a heuristic for pruning the search space , but reduces both the maximum open and closed size to o ( kn^ ( k-1 ) ) , as in dynamic programming . the underlying idea is to conduct a series of searches with successively increasing upper bounds , but using the dp ordering as the key for the open priority queue . with a suitable choice of thresholds , in practice , a running time below four times that of a* can be expected . in our experiments we show that our algorithm outperforms one of the currently most successful algorithms for optimal multiple sequence alignments , partial expansion a* , both in time and memory . moreover , we apply a refined heuristic based on optimal alignments not only of pairs of sequences , but of larger subsets . this idea is not new ; however , to make it practically relevant we show that it is equally important to bound the heuristic computation appropriately , or the overhead can obliterate any possible gain . furthermore , we discuss a number of improvements in time and space efficiency with regard to practical implementations . our algorithm , used in conjunction with higher-dimensional heuristics , is able to calculate for the first time the optimal alignment for almost all of the problems in reference 1 of the benchmark database balibase .", "topics": ["time complexity", "optimization problem"]}
{"title": "leveraging deep neural networks and knowledge graphs for entity disambiguation", "abstract": "entity disambiguation aims to link mentions of ambiguous entities to a knowledge base ( e.g . , wikipedia ) . modeling topical coherence is crucial for this task based on the assumption that information from the same semantic context tends to belong to the same topic . this paper presents a novel deep semantic relatedness model ( dsrm ) based on deep neural networks ( dnn ) and semantic knowledge graphs ( kgs ) to measure entity semantic relatedness for topical coherence modeling . the dsrm is directly trained on large-scale kgs and it maps heterogeneous types of knowledge of an entity from kgs to numerical feature vectors in a latent space such that the distance between two semantically-related entities is minimized . compared with the state-of-the-art relatedness approach proposed by ( milne and witten , 2008a ) , the dsrm obtains 19.4 % and 24.5 % reductions in entity disambiguation errors on two publicly available datasets respectively .", "topics": ["feature vector", "entity"]}
{"title": "unsupervised learning segmentation for dynamic speckle activity images", "abstract": "this paper proposes the design of decision models based on computational intelligence techniques applied to image sequences of dynamic laser speckle . these models aim to identify image regions of biological specimens illuminated by a coherent beam coming from a laser . the field image is pseudo colored using a self organizing map projection . this process is carried out using a set of descriptors applied to the intensity variations along time in every pixel of an image sequence . the models use descriptors selected to improve effectiveness , depending on the specific application . we present two examples of the application of the proposed techniques to assess biological tissues . the results obtained are encouraging and significantly improve those obtained using a single descriptor .", "topics": ["unsupervised learning", "pixel"]}
{"title": "entropy production rate as a criterion for inconsistency in decision theory", "abstract": "evaluating pairwise comparisons breaks down complex decision problems into tractable ones . pairwise comparison matrices ( pcms ) are regularly used to solve multiple-criteria decision-making ( mcdm ) problems using saaty 's analytic hierarchy process ( ahp ) framework . there are two significant drawbacks of using pcms . first , humans evaluate pcm in an inconsistent manner . second , pcms of large problems often have missing entries . we address these two issues by first establishing a novel connection between pcms and time-irreversible markov processes . specifically , we show that every pcm induces a family of dissipative maximum path entropy random walks ( merw ) over the set of alternatives . we show that only `consistent ' pcms correspond to detailed balanced merws . we identify the non-equilibrium entropy production in the induced merws as a metric of inconsistency of the underlying pcms . notably , the entropy production satisfies all of the recently laid out criteria for reasonable consistency indices . we also propose an approach to use incompletely filled pcms in ahp . potential future avenues are discussed as well .", "topics": ["eisenstein 's criterion"]}
{"title": "generalizing the kelly strategy", "abstract": "prompted by a recent experiment by victor haghani and richard dewey , this note generalises the kelly strategy ( optimal for simple investment games with log utility ) to a large class of practical utility functions and including the effect of extraneous wealth . a counterintuitive result is proved : for any continuous , concave , differentiable utility function , the optimal choice at every point depends only on the probability of reaching that point . the practical calculation of the optimal action at every stage is made possible through use of the binomial expansion , reducing the problem size from exponential to quadratic . applications include ( better ) automatic investing and risk taking under uncertainty .", "topics": ["time complexity", "optimization problem"]}
{"title": "learning to compose neural networks for question answering", "abstract": "we describe a question answering model that applies to both images and structured knowledge bases . the model uses natural language strings to automatically assemble neural networks from a collection of composable modules . parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning , with only ( world , question , answer ) triples as supervision . our approach , which we term a dynamic neural model network , achieves state-of-the-art results on benchmark datasets in both visual and structured domains .", "topics": ["neural networks", "reinforcement learning"]}
{"title": "predicting foreground object ambiguity and efficiently crowdsourcing the segmentation ( s )", "abstract": "we propose the ambiguity problem for the foreground object segmentation task and motivate the importance of estimating and accounting for this ambiguity when designing vision systems . specifically , we distinguish between images which lead multiple annotators to segment different foreground objects ( ambiguous ) versus minor inter-annotator differences of the same object . taking images from eight widely used datasets , we crowdsource labeling the images as `` ambiguous '' or `` not ambiguous '' to segment in order to construct a new dataset we call static . using static , we develop a system that automatically predicts which images are ambiguous . experiments demonstrate the advantage of our prediction system over existing saliency-based methods on images from vision benchmarks and images taken by blind people who are trying to recognize objects in their environment . finally , we introduce a crowdsourcing system to achieve cost savings for collecting the diversity of all valid `` ground truth '' foreground object segmentations by collecting extra segmentations only when ambiguity is expected . experiments show our system eliminates up to 47 % of human effort compared to existing crowdsourcing methods with no loss in capturing the diversity of ground truths .", "topics": ["ground truth"]}
{"title": "achieving stable subspace clustering by post-processing generic clustering results", "abstract": "we propose an effective subspace selection scheme as a post-processing step to improve results obtained by sparse subspace clustering ( ssc ) . our method starts by the computation of stable subspaces using a novel random sampling scheme . thus constructed preliminary subspaces are used to identify the initially incorrectly clustered data points and then to reassign them to more suitable clusters based on their goodness-of-fit to the preliminary model . to improve the robustness of the algorithm , we use a dominant nearest subspace classification scheme that controls the level of sensitivity against reassignment . we demonstrate that our algorithm is convergent and superior to the direct application of a generic alternative such as principal component analysis . on several popular datasets for motion segmentation and face clustering pervasively used in the sparse subspace clustering literature the proposed method is shown to reduce greatly the incidence of clustering errors while introducing negligible disturbance to the data points already correctly clustered .", "topics": ["sampling ( signal processing )", "cluster analysis"]}
{"title": "estimating or propagating gradients through stochastic neurons", "abstract": "stochastic neurons can be useful for a number of reasons in deep learning models , but in many cases they pose a challenging problem : how to estimate the gradient of a loss function with respect to the input of such stochastic neurons , i.e . , can we `` back-propagate '' through these stochastic neurons ? we examine this question , existing approaches , and present two novel families of solutions , applicable in different settings . in particular , it is demonstrated that a simple biologically plausible formula gives rise to an an unbiased ( but noisy ) estimator of the gradient with respect to a binary stochastic neuron firing probability . unlike other estimators which view the noise as a small perturbation in order to estimate gradients by finite differences , this estimator is unbiased even without assuming that the stochastic perturbation is small . this estimator is also interesting because it can be applied in very general settings which do not allow gradient back-propagation , including the estimation of the gradient with respect to future rewards , as required in reinforcement learning setups . we also propose an approach to approximating this unbiased but high-variance estimator by learning to predict it using a biased estimator . the second approach we propose assumes that an estimator of the gradient can be back-propagated and it provides an unbiased estimator of the gradient , but can only work with non-linearities unlike the hard threshold , but like the rectifier , that are not flat for all of their range . this is similar to traditional sigmoidal units but has the advantage that for many inputs , a hard decision ( e.g . , a 0 output ) can be produced , which would be convenient for conditional computation and achieving sparse representations and sparse gradients .", "topics": ["approximation algorithm", "loss function"]}
{"title": "strassennets : deep learning with a multiplication budget", "abstract": "a large fraction of the arithmetic operations required to evaluate deep neural networks ( dnns ) consist of matrix multiplications , in both convolution and fully connected layers . we perform end-to-end learning of low-cost approximations of matrix multiplications in dnn layers by casting matrix multiplications as $ 2 $ -layer sum-product networks ( spns ) ( arithmetic circuits ) and learning their ( ternary ) edge weights from data . the spns disentangle multiplication and addition operations and enable us to impose a budget on the number of multiplication operations . combining our method with knowledge distillation and applying it image classification dnns ( trained on imagenet ) and language modeling dnns ( using lstms ) , we obtain a first-of-a-kind reduction in number of multiplications ( over 99.5 % ) while maintaining the predictive performance of the full precision models . finally , we demonstrate that the proposed framework is able to rediscover strassen 's matrix multiplication algorithm , learning to multiply $ 2 \\times 2 $ matrices using only $ 7 $ multiplications instead of $ 8 $ .", "topics": ["approximation", "end-to-end principle"]}
{"title": "parsing combinatory categorial grammar with answer set programming : preliminary report", "abstract": "combinatory categorial grammar ( ccg ) is a grammar formalism used for natural language parsing . ccg assigns structured lexical categories to words and uses a small set of combinatory rules to combine these categories to parse a sentence . in this work we propose and implement a new approach to ccg parsing that relies on a prominent knowledge representation formalism , answer set programming ( asp ) - a declarative programming paradigm . we formulate the task of ccg parsing as a planning problem and use an asp computational tool to compute solutions that correspond to valid parses . compared to other approaches , there is no need to implement a specific parsing algorithm using such a declarative method . our approach aims at producing all semantically distinct parse trees for a given sentence . from this goal , normalization and efficiency issues arise , and we deal with them by combining and extending existing strategies . we have implemented a ccg parsing tool kit - aspccgtk - that uses asp as its main computational means . the c & c supertagger can be used as a preprocessor within aspccgtk , which allows us to achieve wide-coverage natural language parsing .", "topics": ["natural language", "parsing"]}
{"title": "edinburgh neural machine translation systems for wmt 16", "abstract": "we participated in the wmt 2016 shared news translation task by building neural translation systems for four language pairs , each trained in both directions : english < - > czech , english < - > german , english < - > romanian and english < - > russian . our systems are based on an attentional encoder-decoder , using bpe subword segmentation for open-vocabulary translation with a fixed vocabulary . we experimented with using automatic back-translations of the monolingual news corpus as additional training data , pervasive dropout , and target-bidirectional models . all reported methods give substantial improvements , and we see improvements of 4.3 -- 11.2 bleu over our baseline systems . in the human evaluation , our systems were the ( tied ) best constrained system for 7 out of 8 translation directions in which we participated .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "gangs : generative adversarial network games", "abstract": "generative adversarial networks ( gan ) have become one of the most successful frameworks for unsupervised generative modeling . as gans are difficult to train much research has focused on this . however , very little of this research has directly exploited game-theoretic techniques . we introduce generative adversarial network games ( gangs ) , which explicitly model a finite zero-sum game between a generator ( $ g $ ) and classifier ( $ c $ ) that use mixed strategies . the size of these games precludes exact solution methods , therefore we define resource-bounded best responses ( rbbrs ) , and a resource-bounded nash equilibrium ( rb-ne ) as a pair of mixed strategies such that neither $ g $ or $ c $ can find a better rbbr . the rb-ne solution concept is richer than the notion of `local nash equilibria ' in that it captures not only failures of escaping local optima of gradient descent , but applies to any approximate best response computations , including methods with random restarts . to validate our approach , we solve gangs with the parallel nash memory algorithm , which provably monotonically converges to an rb-ne . we compare our results to standard gan setups , and demonstrate that our method deals well with typical gan problems such as mode collapse , partial mode coverage and forgetting .", "topics": ["approximation algorithm", "gradient descent"]}
{"title": "automated detection of smuggled high-risk security threats using deep learning", "abstract": "the security infrastructure is ill-equipped to detect and deter the smuggling of non-explosive devices that enable terror attacks such as those recently perpetrated in western europe . the detection of so-called `` small metallic threats '' ( smts ) in cargo containers currently relies on statistical risk analysis , intelligence reports , and visual inspection of x-ray images by security officers . the latter is very slow and unreliable due to the difficulty of the task : objects potentially spanning less than 50 pixels have to be detected in images containing more than 2 million pixels against very complex and cluttered backgrounds . in this contribution , we demonstrate for the first time the use of convolutional neural networks ( cnns ) , a type of deep learning , to automate the detection of smts in fullsize x-ray images of cargo containers . novel approaches for dataset augmentation allowed to train cnns from-scratch despite the scarcity of data available . we report fewer than 6 % false alarms when detecting 90 % smts synthetically concealed in stream-of-commerce images , which corresponds to an improvement of over an order of magnitude over conventional approaches such as bag-of-words ( bows ) . the proposed scheme offers potentially super-human performance for a fraction of the time it would take for a security officers to carry out visual inspection ( processing time is approximately 3.5s per container image ) .", "topics": ["pixel"]}
{"title": "semi-supervised kernel metric learning using relative comparisons", "abstract": "we consider the problem of metric learning subject to a set of constraints on relative-distance comparisons between the data items . such constraints are meant to reflect side-information that is not expressed directly in the feature vectors of the data items . the relative-distance constraints used in this work are particularly effective in expressing structures at finer level of detail than must-link ( ml ) and can not -link ( cl ) constraints , which are most commonly used for semi-supervised clustering . relative-distance constraints are thus useful in settings where providing an ml or a cl constraint is difficult because the granularity of the true clustering is unknown . our main contribution is an efficient algorithm for learning a kernel matrix using the log determinant divergence -- - a variant of the bregman divergence -- - subject to a set of relative-distance constraints . the learned kernel matrix can then be employed by many different kernel methods in a wide range of applications . in our experimental evaluations , we consider a semi-supervised clustering setting and show empirically that kernels found by our algorithm yield clusterings of higher quality than existing approaches that either use ml/cl constraints or a different means to implement the supervision using relative comparisons .", "topics": ["kernel ( operating system )", "cluster analysis"]}
{"title": "adaptive problem-solving for large-scale scheduling problems : a case study", "abstract": "although most scheduling problems are np-hard , domain specific techniques perform well in practice but are quite expensive to construct . in adaptive problem-solving solving , domain specific knowledge is acquired automatically for a general problem solver with a flexible control architecture . in this approach , a learning system explores a space of possible heuristic methods for one well-suited to the eccentricities of the given domain and problem distribution . in this article , we discuss an application of the approach to scheduling satellite communications . using problem distributions based on actual mission requirements , our approach identifies strategies that not only decrease the amount of cpu time required to produce schedules , but also increase the percentage of problems that are solvable within computational resource limitations .", "topics": ["heuristic"]}
{"title": "correction of noisy sentences using a monolingual corpus", "abstract": "correction of noisy natural language text is an important and well studied problem in natural language processing . it has a number of applications in domains like statistical machine translation , second language learning and natural language generation . in this work , we consider some statistical techniques for text correction . we define the classes of errors commonly found in text and describe algorithms to correct them . the data has been taken from a poorly trained machine translation system . the algorithms use only a language model in the target language in order to correct the sentences . we use phrase based correction methods in both the algorithms . the phrases are replaced and combined to give us the final corrected sentence . we also present the methods to model different kinds of errors , in addition to results of the working of the algorithms on the test set . we show that one of the approaches fail to achieve the desired goal , whereas the other succeeds well . in the end , we analyze the possible reasons for such a trend in performance .", "topics": ["test set", "natural language processing"]}
{"title": "control of memory , active perception , and action in minecraft", "abstract": "in this paper , we introduce a new set of reinforcement learning ( rl ) tasks in minecraft ( a flexible 3d world ) . we then use these tasks to systematically compare and contrast existing deep reinforcement learning ( drl ) architectures with our new memory-based drl architectures . these tasks are designed to emphasize , in a controllable manner , issues that pose challenges for rl methods including partial observability ( due to first-person visual observations ) , delayed rewards , high-dimensional visual observations , and the need to use active perception in a correct manner so as to perform well in the tasks . while these tasks are conceptually simple to describe , by virtue of having all of these challenges simultaneously they are difficult for current drl architectures . additionally , we evaluate the generalization performance of the architectures on environments not used during training . the experimental results show that our new architectures generalize to unseen environments better than existing drl architectures .", "topics": ["reinforcement learning"]}
{"title": "checkpoint ensembles : ensemble methods from a single training process", "abstract": "we present the checkpoint ensembles method that can learn ensemble models on a single training process . although checkpoint ensembles can be applied to any parametric iterative learning technique , here we focus on neural networks . neural networks ' composable and simple neurons make it possible to capture many individual and interaction effects among features . however , small sample sizes and sampling noise may result in patterns in the training data that are not representative of the true relationship between the features and the outcome . as a solution , regularization during training is often used ( e.g . dropout ) . however , regularization is no panacea -- it does not perfectly address overfitting . even with methods like dropout , two methodologies are commonly used in practice . first is to utilize a validation set independent to the training set as a way to decide when to stop training . second is to use ensemble methods to further reduce overfitting and take advantage of local optima ( i.e . averaging over the predictions of several models ) . in this paper , we explore checkpoint ensembles -- a simple technique that combines these two ideas in one training process . checkpoint ensembles improve performance by averaging the predictions from `` checkpoints '' of the best models within single training process . we use three real-world data sets -- text , image , and electronic health record data -- using three prediction models : a vanilla neural network , a convolutional neural network , and a long short term memory network to show that checkpoint ensembles outperform existing methods : a method that selects a model by minimum validation score , and two methods that average models by weights . our results also show that checkpoint ensembles capture a portion of the performance gains that traditional ensembles provide .", "topics": ["sampling ( signal processing )", "test set"]}
{"title": "real time implementation of spatial filtering on fpga", "abstract": "field programmable gate array ( fpga ) technology has gained vital importance mainly because of its parallel processing hardware which makes it ideal for image and video processing . in this paper , a step by step approach to apply a linear spatial filter on real time video frame sent by omnivision ov7670 camera using zynq evaluation and development board based on xilinx xc7z020 has been discussed . face detection application was chosen to explain above procedure . this procedure is applicable to most of the complex image processing algorithms which needs to be implemented using fpga .", "topics": ["image processing"]}
{"title": "contextual semantic parsing using crowdsourced spatial descriptions", "abstract": "we describe a contextual parser for the robot commands treebank , a new crowdsourced resource . in contrast to previous semantic parsers that select the most-probable parse , we consider the different problem of parsing using additional situational context to disambiguate between different readings of a sentence . we show that multiple semantic analyses can be searched using dynamic programming via interaction with a spatial planner , to guide the parsing process . we are able to parse sentences in near linear-time by ruling out analyses early on that are incompatible with spatial context . we report a 34 % upper bound on accuracy , as our planner correctly processes spatial context for 3,394 out of 10,000 sentences . however , our parser achieves a 96.53 % exact-match score for parsing within the subset of sentences recognized by the planner , compared to 82.14 % for a non-contextual parser .", "topics": ["time complexity", "parsing"]}
{"title": "bayesian reinforcement learning : a survey", "abstract": "bayesian methods for machine learning have been widely investigated , yielding principled methods for incorporating prior information into inference algorithms . in this survey , we provide an in-depth review of the role of bayesian methods for the reinforcement learning ( rl ) paradigm . the major incentives for incorporating bayesian reasoning in rl are : 1 ) it provides an elegant approach to action-selection ( exploration/exploitation ) as a function of the uncertainty in learning ; and 2 ) it provides a machinery to incorporate prior knowledge into the algorithms . we first discuss models and methods for bayesian inference in the simple single-step bandit model . we then review the extensive recent literature on bayesian methods for model-based rl , where prior information can be expressed on the parameters of the markov model . we also present bayesian methods for model-free rl , where priors are expressed over the value function or policy class . the objective of the paper is to provide a comprehensive survey on bayesian rl algorithms and their theoretical and empirical properties .", "topics": ["reinforcement learning"]}
{"title": "reading dependencies from covariance graphs", "abstract": "the covariance graph ( aka bi-directed graph ) of a probability distribution $ p $ is the undirected graph $ g $ where two nodes are adjacent iff their corresponding random variables are marginally dependent in $ p $ . in this paper , we present a graphical criterion for reading dependencies from $ g $ , under the assumption that $ p $ satisfies the graphoid properties as well as weak transitivity and composition . we prove that the graphical criterion is sound and complete in certain sense . we argue that our assumptions are not too restrictive . for instance , all the regular gaussian probability distributions satisfy them .", "topics": ["eisenstein 's criterion"]}
{"title": "digital ecosystems : ecosystem-oriented architectures", "abstract": "we view digital ecosystems to be the digital counterparts of biological ecosystems . here , we are concerned with the creation of these digital ecosystems , exploiting the self-organising properties of biological ecosystems to evolve high-level software applications . therefore , we created the digital ecosystem , a novel optimisation technique inspired by biological ecosystems , where the optimisation works at two levels : a first optimisation , migration of agents which are distributed in a decentralised peer-to-peer network , operating continuously in time ; this process feeds a second optimisation based on evolutionary computing that operates locally on single peers and is aimed at finding solutions to satisfy locally relevant constraints . the digital ecosystem was then measured experimentally through simulations , with measures originating from theoretical ecology , evaluating its likeness to biological ecosystems . this included its responsiveness to requests for applications from the user base , as a measure of the ecological succession ( ecosystem maturity ) . overall , we have advanced the understanding of digital ecosystems , creating ecosystem-oriented architectures where the word ecosystem is more than just a metaphor .", "topics": ["mathematical optimization", "high- and low-level"]}
{"title": "an approach for reducing outliers of non local means image denoising filter", "abstract": "we propose an adaptive approach for non local means ( nlm ) image filtering termed as non local adaptive clipped means ( nlacm ) , which reduces the effect of outliers and improves the denoising quality as compared to traditional nlm . common method to neglect outliers from a data population is computation of mean in a range defined by mean and standard deviation . in nlacm we perform the median within the defined range based on statistical estimation of the neighbourhood region of a pixel to be denoised . as parameters of the range are independent of any additional input and is based on local intensity values , hence the approach is adaptive . experimental results for nlacm show better estimation of true intensity from noisy neighbourhood observation as compared to nlm at high noise levels . we have verified the technique for speckle noise reduction and we have tested it on ultrasound ( us ) image of lumbar spine . these ultrasound images act as guidance for injection therapy for treatment of lumbar radiculopathy . we believe that the proposed approach for image denoising is first of its kind and its efficiency can be well justified as it shows better performance in image restoration .", "topics": ["noise reduction", "computation"]}
{"title": "survey : natural language parsing for indian languages", "abstract": "syntactic parsing is a necessary task which is required for nlp applications including machine translation . it is a challenging task to develop a qualitative parser for morphological rich and agglutinative languages . syntactic analysis is used to understand the grammatical structure of a natural language sentence . it outputs all the grammatical information of each word and its constituent . also issues related to it help us to understand the language in a more detailed way . this literature survey is groundwork to understand the different parser development for indian languages and various approaches that are used to develop such tools and techniques . this paper provides a survey of research papers from well known journals and conferences .", "topics": ["natural language processing", "machine translation"]}
{"title": "an efficient evolutionary based method for image segmentation", "abstract": "the goal of this paper is to present a new efficient image segmentation method based on evolutionary computation which is a model inspired from human behavior . based on this model , a four layer process for image segmentation is proposed using the split/merge approach . in the first layer , an image is split into numerous regions using the watershed algorithm . in the second layer , a co-evolutionary process is applied to form centers of finals segments by merging similar primary regions . in the third layer , a meta-heuristic process uses two operators to connect the residual regions to their corresponding determined centers . in the final layer , an evolutionary algorithm is used to combine the resulted similar and neighbor regions . different layers of the algorithm are totally independent , therefore for certain applications a specific layer can be changed without constraint of changing other layers . some properties of this algorithm like the flexibility of its method , the ability to use different feature vectors for segmentation ( grayscale , color , texture , etc ) , the ability to control uniformity and the number of final segments using free parameters and also maintaining small regions , makes it possible to apply the algorithm to different applications . moreover , the independence of each region from other regions in the second layer , and the independence of centers in the third layer , makes parallel implementation possible . as a result the algorithm speed will increase . the presented algorithm was tested on a standard dataset ( bsds 300 ) of images , and the region boundaries were compared with different people segmentation contours . results show the efficiency of the algorithm and its improvement to similar methods . as an instance , in 70 % of tested images , results are better than act algorithm , besides in 100 % of tested images , we had better results in comparison with vsp algorithm .", "topics": ["image segmentation", "feature vector"]}
{"title": "estimating an activity driven hidden markov model", "abstract": "we define a hidden markov model ( hmm ) in which each hidden state has time-dependent $ \\textit { activity levels } $ that drive transitions and emissions , and show how to estimate its parameters . our construction is motivated by the problem of inferring human mobility on sub-daily time scales from , for example , mobile phone records .", "topics": ["time series", "markov chain"]}
{"title": "a strategy for an uncompromising incremental learner", "abstract": "multi-class supervised learning systems require the knowledge of the entire range of labels they predict . often when learnt incrementally , they suffer from catastrophic forgetting . to avoid this , generous leeways have to be made to the philosophy of incremental learning that either forces a part of the machine to not learn , or to retrain the machine again with a selection of the historic data . while these hacks work to various degrees , they do not adhere to the spirit of incremental learning . in this article , we redefine incremental learning with stringent conditions that do not allow for any undesirable relaxations and assumptions . we design a strategy involving generative models and the distillation of dark knowledge as a means of hallucinating data along with appropriate targets from past distributions . we call this technique , phantom sampling.we show that phantom sampling helps avoid catastrophic forgetting during incremental learning . using an implementation based on deep neural networks , we demonstrate that phantom sampling dramatically avoids catastrophic forgetting . we apply these strategies to competitive multi-class incremental learning of deep neural networks . using various benchmark datasets and through our strategy , we demonstrate that strict incremental learning could be achieved . we further put our strategy to test on challenging cases , including cross-domain increments and incrementing on a novel label space . we also propose a trivial extension to unbounded-continual learning and identify potential for future development .", "topics": ["sampling ( signal processing )", "generative model"]}
{"title": "online tool condition monitoring based on parsimonious ensemble+", "abstract": "accurate diagnosis of tool wear in metal turning process remains an open challenge for both scientists and industrial practitioners because of inhomogeneities in workpiece material , nonstationary machining settings to suit production requirements , and nonlinear relations between measured variables and tool wear . common methodologies for tool condition monitoring still rely on batch approaches which can not cope with a fast sampling rate of metal cutting process . furthermore they require a retraining process to be completed from scratch when dealing with a new set of machining parameters . this paper presents an online tool condition monitoring approach based on parsimonious ensemble+ , pensemble+ . the unique feature of pensemble+ lies in its highly flexible principle where both ensemble structure and base-classifier structure can automatically grow and shrink on the fly based on the characteristics of data streams . moreover , the online feature selection scenario is integrated to actively sample relevant input attributes . the paper presents advancement of a newly developed ensemble learning algorithm , pensemble+ , where online active learning scenario is incorporated to reduce operator labelling effort . the ensemble merging scenario is proposed which allows reduction of ensemble complexity while retaining its diversity . experimental studies utilising real-world manufacturing data streams and comparisons with well known algorithms were carried out . furthermore , the efficacy of pensemble was examined using benchmark concept drift data streams . it has been found that pensemble+ incurs low structural complexity and results in a significant reduction of operator labelling effort .", "topics": ["sampling ( signal processing )", "nonlinear system"]}
{"title": "reflection invariance : an important consideration of image orientation", "abstract": "in this position paper , we consider the state of computer vision research with respect to invariance to the horizontal orientation of an image -- what we term reflection invariance . we describe why we consider reflection invariance to be an important property and provide evidence where the absence of this invariance produces surprising inconsistencies in state-of-the-art systems . we demonstrate inconsistencies in methods of object detection and scene classification when they are presented with images and the horizontal mirror of those images . finally , we examine where some of the invariance is exhibited in feature detection and descriptors , and make a case for future consideration of reflection invariance as a measure of quality in computer vision algorithms .", "topics": ["object detection", "computer vision"]}
{"title": "the matrix generalized inverse gaussian distribution : properties and applications", "abstract": "while the matrix generalized inverse gaussian ( $ \\mathcal { mgig } $ ) distribution arises naturally in some settings as a distribution over symmetric positive semi-definite matrices , certain key properties of the distribution and effective ways of sampling from the distribution have not been carefully studied . in this paper , we show that the $ \\mathcal { mgig } $ is unimodal , and the mode can be obtained by solving an algebraic riccati equation ( are ) equation [ 7 ] . based on the property , we propose an importance sampling method for the $ \\mathcal { mgig } $ where the mode of the proposal distribution matches that of the target . the proposed sampling method is more efficient than existing approaches [ 32 , 33 ] , which use proposal distributions that may have the mode far from the $ \\mathcal { mgig } $ 's mode . further , we illustrate that the the posterior distribution in latent factor models , such as probabilistic matrix factorization ( pmf ) [ 25 ] , when marginalized over one latent factor has the $ \\mathcal { mgig } $ distribution . the characterization leads to a novel collapsed monte carlo ( cmc ) inference algorithm for such latent factor models . we illustrate that cmc has a lower log loss or perplexity than mcmc , and needs fewer samples .", "topics": ["sampling ( signal processing )"]}
{"title": "an analytically tractable bayesian approximation to optimal point process filtering", "abstract": "the process of dynamic state estimation ( filtering ) based on point process observations is in general intractable . numerical sampling techniques are often practically useful , but lead to limited conceptual insight about optimal encoding/decoding strategies , which are of significant relevance to computational neuroscience . we develop an analytically tractable bayesian approximation to optimal filtering based on point process observations , which allows us to introduce distributional assumptions about sensory cell properties , that greatly facilitates the analysis of optimal encoding in situations deviating from common assumptions of uniform coding . the analytic framework leads to insights which are difficult to obtain from numerical algorithms , and is consistent with experiments about the distribution of tuning curve centers . interestingly , we find that the information gained from the absence of spikes may be crucial to performance .", "topics": ["sampling ( signal processing )", "numerical analysis"]}
{"title": "probabilistic forecasting and simulation of electricity markets via online dictionary learning", "abstract": "the problem of probabilistic forecasting and online simulation of real-time electricity market with stochastic generation and demand is considered . by exploiting the parametric structure of the direct current optimal power flow , a new technique based on online dictionary learning ( odl ) is proposed . the odl approach incorporates real-time measurements and historical traces to produce forecasts of joint and marginal probability distributions of future locational marginal prices , power flows , and dispatch levels , conditional on the system state at the time of forecasting . compared with standard monte carlo simulation techniques , the odl approach offers several orders of magnitude improvement in computation time , making it feasible for online forecasting of market operations . numerical simulations on large and moderate size power systems illustrate its performance and complexity features and its potential as a tool for system operators .", "topics": ["simulation", "dictionary"]}
{"title": "revisiting batch normalization for practical domain adaptation", "abstract": "deep neural networks ( dnn ) have shown unprecedented success in various computer vision applications such as image classification and object detection . however , it is still a common annoyance during the training phase , that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain . recent study ( tommasi et al . 2015 ) shows that a dnn has strong dependency towards the training dataset , and the learned features can not be easily transferred to a different but relevant task without fine-tuning . in this paper , we propose a simple yet powerful remedy , called adaptive batch normalization ( adabn ) to increase the generalization ability of a dnn . by modulating the statistics in all batch normalization layers across the network , our approach achieves deep adaptation effect for domain adaptation tasks . in contrary to other deep learning domain adaptation methods , our method does not require additional components , and is parameter-free . it archives state-of-the-art performance despite its surprising simplicity . furthermore , we demonstrate that our method is complementary with other existing methods . combining adabn with existing domain adaptation treatments may further improve model performance .", "topics": ["object detection", "computer vision"]}
{"title": "reinforcement learning via recurrent convolutional neural networks", "abstract": "deep reinforcement learning has enabled the learning of policies for complex tasks in partially observable environments , without explicitly learning the underlying model of the tasks . while such model-free methods achieve considerable performance , they often ignore the structure of task . we present a natural representation of to reinforcement learning ( rl ) problems using recurrent convolutional neural networks ( rcnns ) , to better exploit this inherent structure . we define 3 such rcnns , whose forward passes execute an efficient value iteration , propagate beliefs of state in partially observable environments , and choose optimal actions respectively . backpropagating gradients through these rcnns allows the system to explicitly learn the transition model and reward function associated with the underlying mdp , serving as an elegant alternative to classical model-based rl . we evaluate the proposed algorithms in simulation , considering a robot planning problem . we demonstrate the capability of our framework to reduce the cost of replanning , learn accurate mdp models , and finally re-plan with learnt models to achieve near-optimal policies .", "topics": ["reinforcement learning", "simulation"]}
{"title": "maximum-margin structured learning with deep networks for 3d human pose estimation", "abstract": "this paper focuses on structured-output learning using deep neural networks for 3d human pose estimation from monocular images . our network takes an image and 3d pose as inputs and outputs a score value , which is high when the image-pose pair matches and low otherwise . the network structure consists of a convolutional neural network for image feature extraction , followed by two sub-networks for transforming the image features and pose into a joint embedding . the score function is then the dot-product between the image and pose embeddings . the image-pose embedding and score function are jointly trained using a maximum-margin cost function . our proposed framework can be interpreted as a special form of structured support vector machines where the joint feature space is discriminatively learned using deep neural networks . we test our framework on the human3.6m dataset and obtain state-of-the-art results compared to other recent methods . finally , we present visualizations of the image-pose embedding space , demonstrating the network has learned a high-level embedding of body-orientation and pose-configuration .", "topics": ["feature vector", "recurrent neural network"]}
{"title": "zero-shot learning via latent space encoding", "abstract": "zero-shot learning ( zsl ) is typically achieved by resorting to a class semantic embedding space to transfer the knowledge from the seen classes to unseen ones . capturing the common semantic characteristics between the visual modality and the class semantic modality ( e.g . , attributes or word vector ) is a key to the success of zsl . in this paper , we present a novel approach called latent space encoding ( lse ) for zsl based on an encoder-decoder framework , which learns a highly effective latent space to well reconstruct both the visual space and the semantic embedding space . for each modality , the encoderdecoder framework jointly maximizes the recoverability of the original space from the latent space and the predictability of the latent space from the original space , thus making the latent space feature-aware . to relate the visual and class semantic modalities together , their features referring to the same concept are enforced to share the same latent codings . in this way , the semantic relations of different modalities are generalized with the latent representations . we also show that the proposed encoder-decoder framework is easily extended to more modalities . extensive experimental results on four benchmark datasets ( awa , cub , apy , and imagenet ) clearly demonstrate the superiority of the proposed approach on several zsl tasks , including traditional zsl , generalized zsl , and zero-shot retrieval ( zsr ) .", "topics": ["encoder"]}
{"title": "log-normal matrix completion for large scale link prediction", "abstract": "the ubiquitous proliferation of online social networks has led to the widescale emergence of relational graphs expressing unique patterns in link formation and descriptive user node features . matrix factorization and completion have become popular methods for link prediction due to the low rank nature of mutual node friendship information , and the availability of parallel computer architectures for rapid matrix processing . current link prediction literature has demonstrated vast performance improvement through the utilization of sparsity in addition to the low rank matrix assumption . however , the majority of research has introduced sparsity through the limited l1 or frobenius norms , instead of considering the more detailed distributions which led to the graph formation and relationship evolution . in particular , social networks have been found to express either pareto , or more recently discovered , log normal distributions . employing the convexity-inducing lovasz extension , we demonstrate how incorporating specific degree distribution information can lead to large scale improvements in matrix completion based link prediction . we introduce log-normal matrix completion ( lnmc ) , and solve the complex optimization problem by employing alternating direction method of multipliers . using data from three popular social networks , our experiments yield up to 5 % auc increase over top-performing non-structured sparsity based methods .", "topics": ["optimization problem", "sparse matrix"]}
{"title": "structural health monitoring using neural network based vibrational system identification", "abstract": "composite fabrication technologies now provide the means for producing high-strength , low-weight panels , plates , spars and other structural components which use embedded fiber optic sensors and piezoelectric transducers . these materials , often referred to as smart structures , make it possible to sense internal characteristics , such as delaminations or structural degradation . in this effort we use neural network based techniques for modeling and analyzing dynamic structural information for recognizing structural defects . this yields an adaptable system which gives a measure of structural integrity for composite structures .", "topics": ["nonlinear system", "neural networks"]}
{"title": "on the idea of a new artificial intelligence based optimization algorithm inspired from the nature of vortex", "abstract": "in this paper , the idea of a new artificial intelligence based optimization algorithm , which is inspired from the nature of vortex , has been provided briefly . as also a bio-inspired computation algorithm , the idea is generally focused on a typical vortex flow / behavior in nature and inspires from some dynamics that are occurred in the sense of vortex nature . briefly , the algorithm is also a swarm-oriented evolutional problem solution approach ; because it includes many methods related to elimination of weak swarm members and trying to improve the solution process by supporting the solution space via new swarm members . in order have better idea about success of the algorithm ; it has been tested via some benchmark functions . at this point , the obtained results show that the algorithm can be an alternative to the literature in terms of single-objective optimization solution ways . vortex optimization algorithm ( voa ) is the name suggestion by the authors ; for this new idea of intelligent optimization approach .", "topics": ["mathematical optimization", "computation"]}
{"title": "deep knowledge tracing", "abstract": "knowledge tracing -- -where a machine models the knowledge of a student as they interact with coursework -- -is a well established problem in computer supported education . though effectively modeling student knowledge would have high educational impact , the task has many inherent challenges . in this paper we explore the utility of using recurrent neural networks ( rnns ) to model student learning . the rnn family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge , and can capture more complex representations of student knowledge . using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets . moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks . these results suggest a promising new line of research for knowledge tracing and an exemplary application task for rnns .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "convolutional neural networks for facial expression recognition", "abstract": "we have developed convolutional neural networks ( cnn ) for a facial expression recognition task . the goal is to classify each facial image into one of the seven facial emotion categories considered in this study . we trained cnn models with different depth using gray-scale images . we developed our models in torch and exploited graphics processing unit ( gpu ) computation in order to expedite the training process . in addition to the networks performing based on raw pixel data , we employed a hybrid feature strategy by which we trained a novel cnn model with the combination of raw pixel data and histogram of oriented gradients ( hog ) features . to reduce the overfitting of the models , we utilized different techniques including dropout and batch normalization in addition to l2 regularization . we applied cross validation to determine the optimal hyper-parameters and evaluated the performance of the developed models by looking at their training histories . we also present the visualization of different layers of a network to show what features of a face can be learned by cnn models .", "topics": ["neural networks", "matrix regularization"]}
{"title": "handwritten and printed text separation in real document", "abstract": "the aim of the paper is to separate handwritten and printed text from a real document embedded with noise , graphics including annotations . relying on run-length smoothing algorithm ( rlsa ) , the extracted pseudo-lines and pseudo-words are used as basic blocks for classification . to handle this , a multi-class support vector machine ( svm ) with gaussian kernel performs a first labelling of each pseudo-word including the study of local neighbourhood . it then propagates the context between neighbours so that we can correct possible labelling errors . considering running time complexity issue , we propose linear complexity methods where we use k-nn with constraint . when using a kd-tree , it is almost linearly proportional to the number of pseudo-words . the performance of our system is close to 90 % , even when very small learning dataset where samples are basically composed of complex administrative documents .", "topics": ["support vector machine", "time complexity"]}
{"title": "auxiliary deep generative models", "abstract": "deep generative models parameterized by neural networks have recently achieved state-of-the-art performance in unsupervised and semi-supervised learning . we extend deep generative models with auxiliary variables which improves the variational approximation . the auxiliary variables leave the generative model unchanged but make the variational distribution more expressive . inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections . our findings suggest that more expressive and properly specified deep generative models converge faster with better results . we show state-of-the-art performance within semi-supervised learning on mnist , svhn and norb datasets .", "topics": ["generative model", "supervised learning"]}
{"title": "stochastic descent analysis of representation learning algorithms", "abstract": "although stochastic approximation learning methods have been widely used in the machine learning literature for over 50 years , formal theoretical analyses of specific machine learning algorithms are less common because stochastic approximation theorems typically possess assumptions which are difficult to communicate and verify . this paper presents a new stochastic approximation theorem for state-dependent noise with easily verifiable assumptions applicable to the analysis and design of important deep learning algorithms including : adaptive learning , contrastive divergence learning , stochastic descent expectation maximization , and active learning .", "topics": ["approximation algorithm", "reinforcement learning"]}
{"title": "particle swarm optimization based reactive power optimization", "abstract": "reactive power plays an important role in supporting the real power transfer by maintaining voltage stability and system reliability . it is a critical element for a transmission operator to ensure the reliability of an electric system while minimizing the cost associated with it . the traditional objectives of reactive power dispatch are focused on the technical side of reactive support such as minimization of transmission losses . reactive power cost compensation to a generator is based on the incurred cost of its reactive power contribution less the cost of its obligation to support the active power delivery . in this paper an efficient particle swarm optimization ( pso ) based reactive power optimization approach is presented . the optimal reactive power dispatch problem is a nonlinear optimization problem with several constraints . the objective of the proposed pso is to minimize the total support cost from generators and reactive compensators . it is achieved by maintaining the whole system power loss as minimum thereby reducing cost allocation . the purpose of reactive power dispatch is to determine the proper amount and location of reactive support . reactive optimal power flow ( ropf ) formulation is developed as an analysis tool and the validity of proposed method is examined using an ieee-14 bus system .", "topics": ["optimization problem", "nonlinear system"]}
{"title": "pyramidal gradient matching for optical flow estimation", "abstract": "initializing optical flow field by either sparse descriptor matching or dense patch matches has been proved to be particularly useful for capturing large displacements . in this paper , we present a pyramidal gradient matching approach that can provide dense matches for highly accurate and efficient optical flow estimation . a novel contribution of our method is that image gradient is used to describe image patches and proved to be able to produce robust matching . therefore , our method is more efficient than methods that adopt special features ( like sift ) or patch distance metric . moreover , we find that image gradient is scalable for optical flow estimation , which means we can use different levels of gradient feature ( for example , full gradients or only direction information of gradients ) to obtain different complexity without dramatic changes in accuracy . another contribution is that we uncover the secrets of limited patchmatch through a thorough analysis and design a pyramidal matching framework based these secrets . our pyramidal matching framework is aimed at robust gradient matching and effective to grow inliers and reject outliers . in this framework , we present some special enhancements for outlier filtering in gradient matching . by initializing epicflow with our matches , experimental results show that our method is efficient and robust ( ranking 1st on both clean pass and final pass of mpi sintel dataset among published methods ) .", "topics": ["sparse matrix", "gradient"]}
{"title": "competitive multi-agent inverse reinforcement learning with sub-optimal demonstrations", "abstract": "this paper considers the problem of inverse reinforcement learning in zero-sum stochastic games when expert demonstrations are known to be not optimal . compared to previous works that decouple agents in the game by assuming optimality in expert strategies , we introduce a new objective function that directly pits experts against nash equilibrium strategies , and we design an algorithm to solve for the reward function in the context of inverse reinforcement learning with deep neural networks as model approximations . in our setting the model and algorithm do not decouple by agent . in order to find nash equilibrium in large-scale games , we also propose an adversarial training algorithm for zero-sum stochastic games , and show the theoretical appeal of non-existence of local optima in its objective function . in our numerical experiments , we demonstrate that our nash equilibrium and inverse reinforcement learning algorithms address games that are not amenable to previous approaches using tabular representations . moreover , with sub-optimal expert demonstrations our algorithms recover both reward functions and strategies with good quality .", "topics": ["optimization problem", "reinforcement learning"]}
{"title": "distributed training of deep neural networks with theoretical analysis : under ssp setting", "abstract": "we propose a distributed approach to train deep neural networks ( dnns ) , which has guaranteed convergence theoretically and great scalability empirically : close to 6 times faster on instance of imagenet data set when run with 6 machines . the proposed scheme is close to optimally scalable in terms of number of machines , and guaranteed to converge to the same optima as the undistributed setting . the convergence and scalability of the distributed setting is shown empirically across different datasets ( timit and imagenet ) and machine learning tasks ( image classification and phoneme extraction ) . the convergence analysis provides novel insights into this complex learning scheme , including : 1 ) layerwise convergence , and 2 ) convergence of the weights in probability .", "topics": ["neural networks", "computer vision"]}
{"title": "dynamic template tracking and recognition", "abstract": "in this paper we address the problem of tracking non-rigid objects whose local appearance and motion changes as a function of time . this class of objects includes dynamic textures such as steam , fire , smoke , water , etc . , as well as articulated objects such as humans performing various actions . we model the temporal evolution of the object 's appearance/motion using a linear dynamical system ( lds ) . we learn such models from sample videos and use them as dynamic templates for tracking objects in novel videos . we pose the problem of tracking a dynamic non-rigid object in the current frame as a maximum a-posteriori estimate of the location of the object and the latent state of the dynamical system , given the current image features and the best estimate of the state in the previous frame . the advantage of our approach is that we can specify a-priori the type of texture to be tracked in the scene by using previously trained models for the dynamics of these textures . our framework naturally generalizes common tracking methods such as ssd and kernel-based tracking from static templates to dynamic templates . we test our algorithm on synthetic as well as real examples of dynamic textures and show that our simple dynamics-based trackers perform at par if not better than the state-of-the-art . since our approach is general and applicable to any image feature , we also apply it to the problem of human action tracking and build action-specific optical flow trackers that perform better than the state-of-the-art when tracking a human performing a particular action . finally , since our approach is generative , we can use a-priori trained trackers for different texture or action classes to simultaneously track and recognize the texture or action in the video .", "topics": ["synthetic data"]}
{"title": "averaged hausdorff approximations of pareto fronts based on multiobjective estimation of distribution algorithms", "abstract": "in the a posteriori approach of multiobjective optimization the pareto front is approximated by a finite set of solutions in the objective space . the quality of the approximation can be measured by different indicators that take into account the approximation 's closeness to the pareto front and its distribution along the pareto front . in particular , the averaged hausdorff indicator prefers an almost uniform distribution . an observed drawback of multiobjective estimation of distribution algorithms ( medas ) is that - as common for randomized metaheuristics - the final population usually is not uniformly distributed along the pareto front . therefore , we propose a postprocessing strategy which consists of applying the averaged hausdorff indicator to the complete archive of generated solutions after optimization in order to select a uniformly distributed subset of nondominated solutions from the archive . in this paper , we put forward a strategy for extracting the above described subset . the effectiveness of the proposal is contrasted in a series of experiments that involve different medas and filtering techniques .", "topics": ["approximation"]}
{"title": "evolution of central pattern generators for the control of a five-link bipedal walking mechanism", "abstract": "central pattern generators ( cpgs ) , with a basis is neurophysiological studies , are a type of neural network for the generation of rhythmic motion . while cpgs are being increasingly used in robot control , most applications are hand-tuned for a specific task and it is acknowledged in the field that generic methods and design principles for creating individual networks for a given task are lacking . this study presents an approach where the connectivity and oscillatory parameters of a cpg network are determined by an evolutionary algorithm with fitness evaluations in a realistic simulation with accurate physics . we apply this technique to a five-link planar walking mechanism to demonstrate its feasibility and performance . in addition , to see whether results from simulation can be acceptably transferred to real robot hardware , the best evolved cpg network is also tested on a real mechanism . our results also confirm that the biologically inspired cpg model is well suited for legged locomotion , since a diverse manifestation of networks have been observed to succeed in fitness simulations during evolution .", "topics": ["simulation", "heuristic"]}
{"title": "fast k-means based on knn graph", "abstract": "in the era of big data , k-means clustering has been widely adopted as a basic processing tool in various contexts . however , its computational cost could be prohibitively high as the data size and the cluster number are large . it is well known that the processing bottleneck of k-means lies in the operation of seeking closest centroid in each iteration . in this paper , a novel solution towards the scalability issue of k-means is presented . in the proposal , k-means is supported by an approximate k-nearest neighbors graph . in the k-means iteration , each data sample is only compared to clusters that its nearest neighbors reside . since the number of nearest neighbors we consider is much less than k , the processing cost in this step becomes minor and irrelevant to k. the processing bottleneck is therefore overcome . the most interesting thing is that k-nearest neighbor graph is constructed by iteratively calling the fast $ k $ -means itself . comparing with existing fast k-means variants , the proposed algorithm achieves hundreds to thousands times speed-up while maintaining high clustering quality . as it is tested on 10 million 512-dimensional data , it takes only 5.2 hours to produce 1 million clusters . in contrast , to fulfill the same scale of clustering , it would take 3 years for traditional k-means .", "topics": ["cluster analysis", "iteration"]}
{"title": "a notation for markov decision processes", "abstract": "this paper specifies a notation for markov decision processes .", "topics": ["reinforcement learning"]}
{"title": "if it ai n't broke , do n't fix it : sparse metric repair", "abstract": "many modern data-intensive computational problems either require , or benefit from distance or similarity data that adhere to a metric . the algorithms run faster or have better performance guarantees . unfortunately , in real applications , the data are messy and values are noisy . the distances between the data points are far from satisfying a metric . indeed , there are a number of different algorithms for finding the closest set of distances to the given ones that also satisfy a metric ( sometimes with the extra condition of being euclidean ) . these algorithms can have unintended consequences , they can change a large number of the original data points , and alter many other features of the data . the goal of sparse metric repair is to make as few changes as possible to the original data set or underlying distances so as to ensure the resulting distances satisfy the properties of a metric . in other words , we seek to minimize the sparsity ( or the $ \\ell_0 $ `` norm '' ) of the changes we make to the distances subject to the new distances satisfying a metric . we give three different combinatorial algorithms to repair a metric sparsely . in one setting the algorithm is guaranteed to return the sparsest solution and in the other settings , the algorithms repair the metric . without prior information , the algorithms run in time proportional to the cube of the number of input data points and , with prior information we can reduce the running time considerably .", "topics": ["time complexity", "sparse matrix"]}
{"title": "visual and semantic knowledge transfer for large scale semi-supervised object detection", "abstract": "deep cnn-based object detection systems have achieved remarkable success on several large-scale object detection benchmarks . however , training such detectors requires a large number of labeled bounding boxes , which are more difficult to obtain than image-level annotations . previous work addresses this issue by transforming image-level classifiers into object detectors . this is done by modeling the differences between the two on categories with both image-level and bounding box annotations , and transferring this information to convert classifiers to detectors for categories without bounding box annotations . we improve this previous work by incorporating knowledge about object similarities from visual and semantic domains during the transfer process . the intuition behind our proposed method is that visually and semantically similar categories should exhibit more common transferable properties than dissimilar categories , e.g . a better detector would result by transforming the differences between a dog classifier and a dog detector onto the cat class , than would by transforming from the violin class . experimental results on the challenging ilsvrc2013 detection dataset demonstrate that each of our proposed object similarity based knowledge transfer methods outperforms the baseline methods . we found strong evidence that visual similarity and semantic relatedness are complementary for the task , and when combined notably improve detection , achieving state-of-the-art detection performance in a semi-supervised setting .", "topics": ["baseline ( configuration management )", "object detection"]}
{"title": "image enhancement using a generalization of homographic function", "abstract": "this paper presents a new method of gray level image enhancement , based on point transforms . in order to define the transform function , it was used a generalization of the homographic function .", "topics": ["image processing"]}
{"title": "demystifying mmd gans", "abstract": "we investigate the training and performance of generative adversarial networks using the maximum mean discrepancy ( mmd ) as critic , termed mmd gans . as our main theoretical contribution , we clarify the situation with bias in gan loss functions raised by recent work : we show that gradient estimators used in the optimization process for both mmd gans and wasserstein gans are unbiased , but learning a discriminator based on samples leads to biased gradients for the generator parameters . we also discuss the issue of kernel choice for the mmd critic , and characterize the kernel corresponding to the energy distance used for the cramer gan critic . being an integral probability metric , the mmd benefits from training strategies recently developed for wasserstein gans . in experiments , the mmd gan is able to employ a smaller critic network than the wasserstein gan , resulting in a simpler and faster-training algorithm with matching performance . we also propose an improved measure of gan convergence , the kernel inception distance , and show how to use it to dynamically adapt learning rates during gan training .", "topics": ["kernel ( operating system )", "loss function"]}
{"title": "dynamic graph cnn for learning on point clouds", "abstract": "point clouds provide a flexible and scalable geometric representation suitable for countless applications in computer graphics ; they also comprise the raw output of most 3d data acquisition devices . hence , the design of intelligent computational models that act directly on point clouds is critical , especially when efficiency considerations or noise preclude the possibility of expensive denoising and meshing procedures . while hand-designed features on point clouds have long been proposed in graphics and vision , however , the recent overwhelming success of convolutional neural networks ( cnns ) for image analysis suggests the value of adapting insight from cnn to the point cloud world . to this end , we propose a new neural network module dubbed edgeconv suitable for cnn-based high-level tasks on point clouds including classification and segmentation . edgeconv is differentiable and can be plugged into existing architectures . compared to existing modules operating largely in extrinsic space or treating each point independently , edgeconv has several appealing properties : it incorporates local neighborhood information ; it can be stacked or recurrently applied to learn global shape properties ; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding . beyond proposing this module , we provide extensive evaluation and analysis revealing that edgeconv captures and exploits fine-grained geometric properties of point clouds . the proposed approach achieves state-of-the-art performance on standard benchmarks including modelnet40 and s3dis .", "topics": ["high- and low-level", "feature vector"]}
{"title": "a kinematic chain space for monocular motion capture", "abstract": "this paper deals with motion capture of kinematic chains ( e.g . human skeletons ) from monocular image sequences taken by uncalibrated cameras . we present a method based on projecting an observation into a kinematic chain space ( kcs ) . an optimization of the nuclear norm is proposed that implicitly enforces structural properties of the kinematic chain . unlike other approaches our method does not require specific camera or object motion and is not relying on training data or previously determined constraints such as particular body lengths . the proposed algorithm is able to reconstruct scenes with limited camera motion and previously unseen motions . it is not only applicable to human skeletons but also to other kinematic chains for instance animals or industrial robots . we achieve state-of-the-art results on different benchmark data bases and real world scenes .", "topics": ["test set", "database"]}
{"title": "beyond deep residual learning for image restoration : persistent homology-guided manifold simplification", "abstract": "the latest deep learning approaches perform better than the state-of-the-art signal processing approaches in various image restoration tasks . however , if an image contains many patterns and structures , the performance of these cnns is still inferior . to address this issue , here we propose a novel feature space deep residual learning algorithm that outperforms the existing residual learning . the main idea is originated from the observation that the performance of a learning algorithm can be improved if the input and/or label manifolds can be made topologically simpler by an analytic mapping to a feature space . our extensive numerical studies using denoising experiments and ntire single-image super-resolution ( sisr ) competition demonstrate that the proposed feature space residual learning outperforms the existing state-of-the-art approaches . moreover , our algorithm was ranked third in ntire competition with 5-10 times faster computational time compared to the top ranked teams . the source code is available on page : https : //github.com/iorism/cnn.git", "topics": ["feature vector", "time complexity"]}
{"title": "stochastic reformulations of linear systems : algorithms and convergence theory", "abstract": "we develop a family of reformulations of an arbitrary consistent linear system into a stochastic problem . the reformulations are governed by two user-defined parameters : a positive definite matrix defining a norm , and an arbitrary discrete or continuous distribution over random matrices . our reformulation has several equivalent interpretations , allowing for researchers from various communities to leverage their domain specific insights . in particular , our reformulation can be equivalently seen as a stochastic optimization problem , stochastic linear system , stochastic fixed point problem and a probabilistic intersection problem . we prove sufficient , and necessary and sufficient conditions for the reformulation to be exact . further , we propose and analyze three stochastic algorithms for solving the reformulated problem -- -basic , parallel and accelerated methods -- -with global linear convergence rates . the rates can be interpreted as condition numbers of a matrix which depends on the system matrix and on the reformulation parameters . this gives rise to a new phenomenon which we call stochastic preconditioning , and which refers to the problem of finding parameters ( matrix and distribution ) leading to a sufficiently small condition number . our basic method can be equivalently interpreted as stochastic gradient descent , stochastic newton method , stochastic proximal point method , stochastic fixed point method , and stochastic projection method , with fixed stepsize ( relaxation parameter ) , applied to the reformulations .", "topics": ["optimization problem", "gradient descent"]}
{"title": "an iterative approach to hough transform without re-voting", "abstract": "many bone shapes in the human skeleton are characterized by profiles that can be associated to equations of algebraic curves . fixing the parameters in the curve equation , by means of a classical pattern recognition procedure like the hough transform technique , it is then possible to associate an equation to a specific bone profile . however , most skeleton districts are more accurately described by piecewise defined curves . this paper utilizes an iterative approach of the hough transform without re-voting , to provide an efficient procedure for describing the profile of a bone in the human skeleton as a collection of different but continuously attached curves .", "topics": ["computational complexity theory"]}
{"title": "solving oscar regularization problems by proximal splitting algorithms", "abstract": "the oscar ( octagonal selection and clustering algorithm for regression ) regularizer consists of a l_1 norm plus a pair-wise l_inf norm ( responsible for its grouping behavior ) and was proposed to encourage group sparsity in scenarios where the groups are a priori unknown . the oscar regularizer has a non-trivial proximity operator , which limits its applicability . we reformulate this regularizer as a weighted sorted l_1 norm , and propose its grouping proximity operator ( gpo ) and approximate proximity operator ( apo ) , thus making state-of-the-art proximal splitting algorithms ( psas ) available to solve inverse problems with oscar regularization . the gpo is in fact the apo followed by additional grouping and averaging operations , which are costly in time and storage , explaining the reason why algorithms with apo are much faster than that with gpo . the convergences of psas with gpo are guaranteed since gpo is an exact proximity operator . although convergence of psas with apo is may not be guaranteed , we have experimentally found that apo behaves similarly to gpo when the regularization parameter of the pair-wise l_inf norm is set to an appropriately small value . experiments on recovery of group-sparse signals ( with unknown groups ) show that psas with apo are very fast and accurate .", "topics": ["cluster analysis", "matrix regularization"]}
{"title": "on the fusion of compton scatter and attenuation data for limited-view x-ray tomographic applications", "abstract": "in this paper we demonstrate the utility of fusing energy-resolved observations of compton scattered photons with traditional attenuation data for the joint recovery of mass density and photoelectric absorption in the context of limited view tomographic imaging applications . we begin with the development of a physical and associated numerical model for the compton scatter process . using this model , we propose a variational approach recovering these two material properties . in addition to the typical data-fidelity terms , the optimization functional includes regularization for both the mass density and photoelectric coefficients . we consider a novel edge-preserving method in the case of mass density . to aid in the recovery of the photoelectric information , we draw on our recent method in \\cite { r15 } and employ a non-local regularization scheme that builds on the fact that mass density is more stably imaged . simulation results demonstrate clear advantages associated with the use of both scattered photon data and energy resolved information in mapping the two material properties of interest . specifically , comparing images obtained using only conventional attenuation data with those where we employ only compton scatter photons and images formed from the combination of the two , shows that taking advantage of both types of data for reconstruction provides far more accurate results .", "topics": ["calculus of variations", "simulation"]}
{"title": "pipeline generative adversarial networks for facial images generation with multiple attributes", "abstract": "generative adversarial networks are proved to be efficient on various kinds of image generation tasks . however , it is still a challenge if we want to generate images precisely . many researchers focus on how to generate images with one attribute . but image generation under multiple attributes is still a tough work . in this paper , we try to generate a variety of face images under multiple constraints using a pipeline process . the pip-gan ( pipeline generative adversarial network ) we present employs a pipeline network structure which can generate a complex facial image step by step using a neutral face image . we applied our method on two face image databases and demonstrate its ability to generate convincing novel images of unseen identities under multiple conditions previously .", "topics": ["database"]}
{"title": "call attention to rumors : deep attention based recurrent neural networks for early rumor detection", "abstract": "the proliferation of social media in communication and information dissemination has made it an ideal platform for spreading rumors . automatically debunking rumors at their stage of diffusion is known as \\textit { early rumor detection } , which refers to dealing with sequential posts regarding disputed factual claims with certain variations and highly textual duplication over time . thus , identifying trending rumors demands an efficient yet flexible model that is able to capture long-range dependencies among postings and produce distinct representations for the accurate early detection . however , it is a challenging task to apply conventional classification algorithms to rumor detection in earliness since they rely on hand-crafted features which require intensive manual efforts in the case of large amount of posts . this paper presents a deep attention model on the basis of recurrent neural networks ( rnn ) to learn \\textit { selectively } temporal hidden representations of sequential posts for identifying rumors . the proposed model delves soft-attention into the recurrence to simultaneously pool out distinct features with particular focus and produce hidden representations that capture contextual variations of relevant posts over time . extensive experiments on real datasets collected from social media websites demonstrate that ( 1 ) the deep attention based rnn model outperforms state-of-the-arts that rely on hand-crafted features ; ( 2 ) the introduction of soft attention mechanism can effectively distill relevant parts to rumors from original posts in advance ; ( 3 ) the proposed method detects rumors more quickly and accurately than competitors .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "normface : l2 hypersphere embedding for face verification", "abstract": "thanks to the recent developments of convolutional neural networks , the performance of face verification methods has increased rapidly . in a typical face verification method , feature normalization is a critical step for boosting performance . this motivates us to introduce and study the effect of normalization during training . but we find this is non-trivial , despite normalization being differentiable . we identify and study four issues related to normalization through mathematical analysis , which yields understanding and helps with parameter settings . based on this analysis we propose two strategies for training using normalized features . the first is a modification of softmax loss , which optimizes cosine similarity instead of inner-product . the second is a reformulation of metric learning by introducing an agent vector for each class . we show that both strategies , and small variants , consistently improve performance by between 0.2 % to 0.4 % on the lfw dataset based on two models . this is significant because the performance of the two models on lfw dataset is close to saturation at over 98 % . codes and models are released on https : //github.com/happynear/normface", "topics": ["neural networks"]}
{"title": "zero-shot learning for semantic utterance classification", "abstract": "we propose a novel zero-shot learning method for semantic utterance classification ( suc ) . it learns a classifier $ f : x \\to y $ for problems where none of the semantic categories $ y $ are present in the training set . the framework uncovers the link between categories and utterances using a semantic space . we show that this semantic space can be learned by deep neural networks trained on large amounts of search engine query log data . more precisely , we propose a novel method that can learn discriminative semantic features without supervision . it uses the zero-shot learning framework to guide the learning of the semantic features . we demonstrate the effectiveness of the zero-shot semantic learning algorithm on the suc dataset collected by ( tur , 2012 ) . furthermore , we achieve state-of-the-art results by combining the semantic features with a supervised method .", "topics": ["statistical classification"]}
{"title": "shape from texture using locally scaled point processes", "abstract": "shape from texture refers to the extraction of 3d information from 2d images with irregular texture . this paper introduces a statistical framework to learn shape from texture where convex texture elements in a 2d image are represented through a point process . in a first step , the 2d image is preprocessed to generate a probability map corresponding to an estimate of the unnormalized intensity of the latent point process underlying the texture elements . the latent point process is subsequently inferred from the probability map in a non-parametric , model free manner . finally , the 3d information is extracted from the point pattern by applying a locally scaled point process model where the local scaling function represents the deformation caused by the projection of a 3d surface onto a 2d image .", "topics": ["scalability"]}
{"title": "adaption : toolbox and benchmark for training convolutional neural networks with reduced numerical precision weights and activation", "abstract": "deep neural networks ( dnns ) and convolutional neural networks ( cnns ) are useful for many practical tasks in machine learning . synaptic weights , as well as neuron activation functions within the deep network are typically stored with high-precision formats , e.g . 32 bit floating point . however , since storage capacity is limited and each memory access consumes power , both storage capacity and memory access are two crucial factors in these networks . here we present a method and present the adaption toolbox to extend the popular deep learning library caffe to support training of deep cnns with reduced numerical precision of weights and activations using fixed point notation . adaption includes tools to measure the dynamic range of weights and activations . using the adaption tools , we quantized several cnns including vgg16 down to 16-bit weights and activations with only 0.8 % drop in top-1 accuracy . the quantization , especially of the activations , leads to increase of up to 50 % of sparsity especially in early and intermediate layers , which we exploit to skip multiplications with zero , thus performing faster and computationally cheaper inference .", "topics": ["numerical analysis", "sparse matrix"]}
{"title": "hybrid transfer in an english-french spoken language translator", "abstract": "the paper argues the importance of high-quality translation for spoken language translation systems . it describes an architecture suitable for rapid development of high-quality limited-domain translation systems , which has been implemented within an advanced prototype english to french spoken language translator . the focus of the paper is the hybrid transfer model which combines unification-based rules and a set of trainable statistical preferences ; roughly , rules encode domain-independent grammatical information and preferences encode domain-dependent distributional information . the preferences are trained from sets of examples produced by the system , which have been annotated by human judges as correct or incorrect . an experiment is described in which the model was tested on a 2000 utterance sample of previously unseen data .", "topics": ["machine translation"]}
{"title": "pimper : piecewise dense 3d reconstruction from multi-view and multi-illumination images", "abstract": "in this paper , we address the problem of dense 3d reconstruction from multiple view images subject to strong lighting variations . in this regard , a new piecewise framework is proposed to explicitly take into account the change of illumination across several wide-baseline images . unlike multi-view stereo and multi-view photometric stereo methods , this pipeline deals with wide-baseline images that are uncalibrated , in terms of both camera parameters and lighting conditions . such a scenario is meant to avoid use of any specific imaging setup and provide a tool for normal users without any expertise . to the best of our knowledge , this paper presents the first work that deals with such unconstrained setting . we propose a coarse-to-fine approach , in which a coarse mesh is first created using a set of geometric constraints and , then , fine details are recovered by exploiting photometric properties of the scene . augmenting the fine details on the coarse mesh is done via a final optimization step . note that the method does not provide a generic solution for multi-view photometric stereo problem but it relaxes several common assumptions of this problem . the approach scales very well in size given its piecewise nature , dealing with large scale optimization and with severe missing data . experiments on a benchmark dataset robot data-set show the method performance against 3d ground truth .", "topics": ["baseline ( configuration management )", "ground truth"]}
{"title": "the landscape of empirical risk for non-convex losses", "abstract": "most high-dimensional estimation and prediction methods propose to minimize a cost function ( empirical risk ) that is written as a sum of losses associated to each data point . in this paper we focus on the case of non-convex losses , which is practically important but still poorly understood . classical empirical process theory implies uniform convergence of the empirical risk to the population risk . while uniform convergence implies consistency of the resulting m-estimator , it does not ensure that the latter can be computed efficiently . in order to capture the complexity of computing m-estimators , we propose to study the landscape of the empirical risk , namely its stationary points and their properties . we establish uniform convergence of the gradient and hessian of the empirical risk to their population counterparts , as soon as the number of samples becomes larger than the number of unknown parameters ( modulo logarithmic factors ) . consequently , good properties of the population risk can be carried to the empirical risk , and we can establish one-to-one correspondence of their stationary points . we demonstrate that in several problems such as non-convex binary classification , robust regression , and gaussian mixture model , this result implies a complete characterization of the landscape of the empirical risk , and of the convergence properties of descent algorithms . we extend our analysis to the very high-dimensional setting in which the number of parameters exceeds the number of samples , and provide a characterization of the empirical risk landscape under a nearly information-theoretically minimal condition . namely , if the number of samples exceeds the sparsity of the unknown parameters vector ( modulo logarithmic factors ) , then a suitable uniform convergence result takes place . we apply this result to non-convex binary classification and robust regression in very high-dimension .", "topics": ["loss function", "sparse matrix"]}
{"title": "on generalization and regularization in deep learning", "abstract": "why do large neural network generalize so well on complex tasks such as image classification or speech recognition ? what exactly is the role regularization for them ? these are arguably among the most important open questions in machine learning today . in a recent and thought provoking paper [ c. zhang et al . ] several authors performed a number of numerical experiments that hint at the need for novel theoretical concepts to account for this phenomenon . the paper stirred quit a lot of excitement among the machine learning community but at the same time it created some confusion as discussions on openreview.net testifies . the aim of this pedagogical paper is to make this debate accessible to a wider audience of data scientists without advanced theoretical knowledge in statistical learning . the focus here is on explicit mathematical definitions and on a discussion of relevant concepts , not on proofs for which we provide references .", "topics": ["numerical analysis", "computer vision"]}
{"title": "environmental sounds spectrogram classification using log-gabor filters and multiclass support vector machines", "abstract": "this paper presents novel approaches for efficient feature extraction using environmental sound magnitude spectrogram . we propose approach based on the visual domain . this approach included three methods . the first method is based on extraction for each spectrogram a single log-gabor filter followed by mutual information procedure . in the second method , the spectrogram is passed by the same steps of the first method but with an averaged bank of 12 log-gabor filter . the third method consists of spectrogram segmentation into three patches , and after that for each spectrogram patch we applied the second method . the classification results prove that the second method is the most efficient in our environmental sound classification system .", "topics": ["feature extraction", "support vector machine"]}
{"title": "statistical denoising for single molecule fluorescence microscopic images", "abstract": "single molecule fluorescence microscopy is a powerful technique for uncovering detailed information about biological systems , both in vitro and in vivo . in such experiments , the inherently low signal to noise ratios mean that accurate algorithms to separate true signal and background noise are essential to generate meaningful results . to this end , we have developed a new and robust method to reduce noise in single molecule fluorescence images by using a gaussian markov random field ( gmrf ) prior in a bayesian framework . two different strategies are proposed to build the prior - an intrinsic gmrf , with a stationary relationship between pixels and a heterogeneous intrinsic gmrf , with a differently weighted relationship between pixels classified as molecules and background . testing with synthetic and real experimental fluorescence images demonstrates that the heterogeneous intrinsic gmrf is superior to other conventional de-noising approaches .", "topics": ["noise reduction", "synthetic data"]}
{"title": "nmtpy : a flexible toolkit for advanced neural machine translation systems", "abstract": "in this paper , we present nmtpy , a flexible python toolkit based on theano for training neural machine translation and other neural sequence-to-sequence architectures . nmtpy decouples the specification of a network from the training and inference utilities to simplify the addition of a new architecture and reduce the amount of boilerplate code to be written . nmtpy has been used for lium 's top-ranked submissions to wmt multimodal machine translation and news translation tasks in 2016 and 2017 .", "topics": ["machine translation"]}
{"title": "a+d-net : shadow detection with adversarial shadow attenuation", "abstract": "single image shadow detection is a very challenging problem because of the limited amount of information available in one image , as well as the scarcity of annotated training data . in this work , we propose a novel adversarial training based framework that yields a high performance shadow detection network ( d-net ) . d-net is trained together with an attenuator network ( a-net ) that generates adversarial training examples . a-net performs shadow attenuation in original training images constrained by a simplified physical shadow model and focused on fooling d-net 's shadow predictions . hence , it is effectively augmenting the training data for d-net with hard to predict cases . experimental results on the most challenging shadow detection benchmark show that our method outperforms the state-of-the-art with a 38 % error reduction , measured in terms of balanced error rate ( ber ) . our proposed shadow detector also obtains state-of-the-art results on a cross-dataset task testing on ucf with a 14 % error reduction . furthermore , the proposed method can perform accurate close to real-time shadow detection at a rate of 13 frames per second .", "topics": ["test set"]}
{"title": "genetic algorithm ( ga ) in feature selection for crf based manipuri multiword expression ( mwe ) identification", "abstract": "this paper deals with the identification of multiword expressions ( mwes ) in manipuri , a highly agglutinative indian language . manipuri is listed in the eight schedule of indian constitution . mwe plays an important role in the applications of natural language processing ( nlp ) like machine translation , part of speech tagging , information retrieval , question answering etc . feature selection is an important factor in the recognition of manipuri mwes using conditional random field ( crf ) . the disadvantage of manual selection and choosing of the appropriate features for running crf motivates us to think of genetic algorithm ( ga ) . using ga we are able to find the optimal features to run the crf . we have tried with fifty generations in feature selection along with three fold cross validation as fitness function . this model demonstrated the recall ( r ) of 64.08 % , precision ( p ) of 86.84 % and f-measure ( f ) of 73.74 % , showing an improvement over the crf based manipuri mwe identification without ga application .", "topics": ["machine translation", "natural language"]}
{"title": "clustering learning for robotic vision", "abstract": "we present the clustering learning technique applied to multi-layer feedforward deep neural networks . we show that this unsupervised learning technique can compute network filters with only a few minutes and a much reduced set of parameters . the goal of this paper is to promote the technique for general-purpose robotic vision systems . we report its use in static image datasets and object tracking datasets . we show that networks trained with clustering learning can outperform large networks trained for many hours on complex datasets .", "topics": ["unsupervised learning", "cluster analysis"]}
{"title": "expectation-propogation for the generative aspect model", "abstract": "the generative aspect model is an extension of the multinomial model for text that allows word probabilities to vary stochastically across documents . previous results with aspect models have been promising , but hindered by the computational difficulty of carrying out inference and learning . this paper demonstrates that the simple variational methods of blei et al ( 2001 ) can lead to inaccurate inferences and biased learning for the generative aspect model . we develop an alternative approach that leads to higher accuracy at comparable cost . an extension of expectation-propagation is used for inference and then embedded in an em algorithm for learning . experimental results are presented for both synthetic and real data sets .", "topics": ["calculus of variations", "synthetic data"]}
{"title": "the meaning factory at semeval-2017 task 9 : producing amrs with neural semantic parsing", "abstract": "we evaluate a semantic parser based on a character-based sequence-to-sequence model in the context of the semeval-2017 shared task on semantic parsing for amrs . with data augmentation , super characters , and pos-tagging we gain major improvements in performance compared to a baseline character-level model . although we improve on previous character-based neural semantic parsing models , the overall accuracy is still lower than a state-of-the-art amr parser . an ensemble combining our neural semantic parser with an existing , traditional parser , yields a small gain in performance .", "topics": ["baseline ( configuration management )", "parsing"]}
{"title": "hi , how can i help you ? : automating enterprise it support help desks", "abstract": "question answering is one of the primary challenges of natural language understanding . in realizing such a system , providing complex long answers to questions is a challenging task as opposed to factoid answering as the former needs context disambiguation . the different methods explored in the literature can be broadly classified into three categories namely : 1 ) classification based , 2 ) knowledge graph based and 3 ) retrieval based . individually , none of them address the need of an enterprise wide assistance system for an it support and maintenance domain . in this domain the variance of answers is large ranging from factoid to structured operating procedures ; the knowledge is present across heterogeneous data sources like application specific documentation , ticket management systems and any single technique for a general purpose assistance is unable to scale for such a landscape . to address this , we have built a cognitive platform with capabilities adopted for this domain . further , we have built a general purpose question answering system leveraging the platform that can be instantiated for multiple products , technologies in the support domain . the system uses a novel hybrid answering model that orchestrates across a deep learning classifier , a knowledge graph based context disambiguation module and a sophisticated bag-of-words search system . this orchestration performs context switching for a provided question and also does a smooth hand-off of the question to a human expert if none of the automated techniques can provide a confident answer . this system has been deployed across 675 internal enterprise it support and maintenance projects .", "topics": ["natural language"]}
{"title": "a fast face detection method via convolutional neural network", "abstract": "current face or object detection methods via convolutional neural network ( such as overfeat , r-cnn and densenet ) explicitly extract multi-scale features based on an image pyramid . however , such a strategy increases the computational burden for face detection . in this paper , we propose a fast face detection method based on discriminative complete features ( dcfs ) extracted by an elaborately designed convolutional neural network , where face detection is directly performed on the complete feature maps . dcfs have shown the ability of scale invariance , which is beneficial for face detection with high speed and promising performance . therefore , extracting multi-scale features on an image pyramid employed in the conventional methods is not required in the proposed method , which can greatly improve its efficiency for face detection . experimental results on several popular face detection datasets show the efficiency and the effectiveness of the proposed method for face detection .", "topics": ["object detection", "map"]}
{"title": "restricted value iteration : theory and algorithms", "abstract": "value iteration is a popular algorithm for finding near optimal policies for pomdps . it is inefficient due to the need to account for the entire belief space , which necessitates the solution of large numbers of linear programs . in this paper , we study value iteration restricted to belief subsets . we show that , together with properly chosen belief subsets , restricted value iteration yields near-optimal policies and we give a condition for determining whether a given belief subset would bring about savings in space and time . we also apply restricted value iteration to two interesting classes of pomdps , namely informative pomdps and near-discernible pomdps .", "topics": ["iteration"]}
{"title": "convergence rates for pretraining and dropout : guiding learning parameters using network structure", "abstract": "unsupervised pretraining and dropout have been well studied , especially with respect to regularization and output consistency . however , our understanding about the explicit convergence rates of the parameter estimates , and their dependence on the learning ( like denoising and dropout rate ) and structural ( like depth and layer lengths ) aspects of the network is less mature . an interesting question in this context is to ask if the network structure could `` guide '' the choices of such learning parameters . in this work , we explore these gaps between network structure , the learning mechanisms and their interaction with parameter convergence rates . we present a way to address these issues based on the backpropagation convergence rates for general nonconvex objectives using first-order information . we then incorporate two learning mechanisms into this general framework -- denoising autoencoder and dropout , and subsequently derive the convergence rates of deep networks . building upon these bounds , we provide insights into the choices of learning parameters and network sizes that achieve certain levels of convergence accuracy . the results derived here support existing empirical observations , and we also conduct a set of experiments to evaluate them .", "topics": ["noise reduction", "matrix regularization"]}
{"title": "logarithmic-time updates and queries in probabilistic networks", "abstract": "in this paper we propose a dynamic data structure that supports efficient algorithms for updating and querying singly connected bayesian networks ( causal trees and polytrees ) . in the conventional algorithms , new evidence in absorbed in time o ( 1 ) and queries are processed in time o ( n ) , where n is the size of the network . we propose a practical algorithm which , after a preprocessing phase , allows us to answer queries in time o ( log n ) at the expense of o ( logn n ) time per evidence absorption . the usefulness of sub-linear processing time manifests itself in applications requiring ( near ) real-time response over large probabilistic databases .", "topics": ["time complexity", "bayesian network"]}
{"title": "dataset and neural recurrent sequence labeling model for open-domain factoid question answering", "abstract": "while question answering ( qa ) with neural network , i.e . neural qa , has achieved promising results in recent years , lacking of large scale real-word qa dataset is still a challenge for developing and evaluating neural qa system . to alleviate this problem , we propose a large scale human annotated real-world qa dataset webqa with more than 42k questions and 556k evidences . as existing neural qa methods resolve qa either as sequence generation or classification/ranking problem , they face challenges of expensive softmax computation , unseen answers handling or separate candidate answer generation component . in this work , we cast neural qa as a sequence labeling problem and propose an end-to-end sequence labeling model , which overcomes all the above challenges . experimental results on webqa show that our model outperforms the baselines significantly with an f1 score of 74.69 % with word-based input , and the performance drops only 3.72 f1 points with more challenging character-based input .", "topics": ["computation", "end-to-end principle"]}
{"title": "8-bit approximations for parallelism in deep learning", "abstract": "the creation of practical deep learning data-products often requires parallelization across processors and computers to make deep learning feasible on large data sets , but bottlenecks in communication bandwidth make it difficult to attain good speedups through parallelism . here we develop and test 8-bit approximation algorithms which make better use of the available bandwidth by compressing 32-bit gradients and nonlinear activations to 8-bit approximations . we show that these approximations do not decrease predictive performance on mnist , cifar10 , and imagenet for both model and data parallelism and provide a data transfer speedup of 2x relative to 32-bit parallelism . we build a predictive model for speedups based on our experimental data , verify its validity on known speedup data , and show that we can obtain a speedup of 50x and more on a system of 96 gpus compared to a speedup of 23x for 32-bit . we compare our data types with other methods and show that 8-bit approximations achieve state-of-the-art speedups for model parallelism . thus 8-bit approximation is an efficient method to parallelize convolutional networks on very large systems of gpus .", "topics": ["approximation algorithm", "nonlinear system"]}
{"title": "the computational theory of intelligence : information entropy", "abstract": "this paper presents an information theoretic approach to the concept of intelligence in the computational sense . we introduce a probabilistic framework from which computational intelligence is shown to be an entropy minimizing process at the local level . using this new scheme , we develop a simple data driven clustering example and discuss its applications .", "topics": ["cluster analysis"]}
{"title": "a pac rl algorithm for episodic pomdps", "abstract": "many interesting real world domains involve reinforcement learning ( rl ) in partially observable environments . efficient learning in such domains is important , but existing sample complexity bounds for partially observable rl are at least exponential in the episode length . we give , to our knowledge , the first partially observable rl algorithm with a polynomial bound on the number of episodes on which the algorithm may not achieve near-optimal performance . our algorithm is suitable for an important class of episodic pomdps . our approach builds on recent advances in method of moments for latent variable model estimation .", "topics": ["time complexity", "reinforcement learning"]}
{"title": "structured variational inference for coupled gaussian processes", "abstract": "sparse variational approximations allow for principled and scalable inference in gaussian process ( gp ) models . in settings where several gps are part of the generative model , theses gps are a posteriori coupled . for many applications such as regression where predictive accuracy is the quantity of interest , this coupling is not crucial . howewer if one is interested in posterior uncertainty , it can not be ignored . a key element of variational inference schemes is the choice of the approximate posterior parameterization . when the number of latent variables is large , mean field ( mf ) methods provide fast and accurate posterior means while more structured posterior lead to inference algorithm of greater computational complexity . here , we extend previous sparse gp approximations and propose a novel parameterization of variational posteriors in the multi-gp setting allowing for fast and scalable inference capturing posterior dependencies .", "topics": ["generative model", "approximation algorithm"]}
{"title": "towards ai-complete question answering : a set of prerequisite toy tasks", "abstract": "one long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language , in particular building an intelligent dialogue agent . to measure progress towards that goal , we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering . our tasks measure understanding in several ways : whether a system is able to answer questions via chaining facts , simple induction , deduction and many more . the tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human . we believe many existing learning systems can currently not solve them , and hence our aim is to classify these tasks into skill sets , so that researchers can identify ( and then rectify ) the failings of their systems . we also extend and improve the recently introduced memory networks model , and show it is able to solve some , but not all , of the tasks .", "topics": ["natural language"]}
{"title": "interpretable machine learning models for the digital clock drawing test", "abstract": "the clock drawing test ( cdt ) is a rapid , inexpensive , and popular neuropsychological screening tool for cognitive conditions . the digital clock drawing test ( dcdt ) uses novel software to analyze data from a digitizing ballpoint pen that reports its position with considerable spatial and temporal precision , making possible the analysis of both the drawing process and final product . we developed methodology to analyze pen stroke data from these drawings , and computed a large collection of features which were then analyzed with a variety of machine learning techniques . the resulting scoring systems were designed to be more accurate than the systems currently used by clinicians , but just as interpretable and easy to use . the systems also allow us to quantify the tradeoff between accuracy and interpretability . we created automated versions of the cdt scoring systems currently used by clinicians , allowing us to benchmark our models , which indicated that our machine learning models substantially outperformed the existing scoring systems .", "topics": ["value ( ethics )", "computational complexity theory"]}
{"title": "random forests for big data", "abstract": "big data is one of the major challenges of statistical science and has numerous consequences from algorithmic and theoretical viewpoints . big data always involve massive data but they also often include online data and data heterogeneity . recently some statistical methods have been adapted to process big data , like linear regression models , clustering methods and bootstrapping schemes . based on decision trees combined with aggregation and bootstrap ideas , random forests were introduced by breiman in 2001 . they are a powerful nonparametric statistical method allowing to consider in a single and versatile framework regression problems , as well as two-class and multi-class classification problems . focusing on classification problems , this paper proposes a selective review of available proposals that deal with scaling random forests to big data problems . these proposals rely on parallel environments or on online adaptations of random forests . we also describe how related quantities -- such as out-of-bag error and variable importance -- are addressed in these methods . then , we formulate various remarks for random forests in the big data context . finally , we experiment five variants on two massive datasets ( 15 and 120 millions of observations ) , a simulated one as well as real world data . one variant relies on subsampling while three others are related to parallel implementations of random forests and involve either various adaptations of bootstrap to big data or to `` divide-and-conquer '' approaches . the fifth variant relates on online learning of random forests . these numerical experiments lead to highlight the relative performance of the different variants , as well as some of their limitations .", "topics": ["cluster analysis", "simulation"]}
{"title": "monte carlo information geometry : the dually flat case", "abstract": "exponential families and mixture families are parametric probability models that can be geometrically studied as smooth statistical manifolds with respect to any statistical divergence like the kullback-leibler ( kl ) divergence or the hellinger divergence . when equipping a statistical manifold with the kl divergence , the induced manifold structure is dually flat , and the kl divergence between distributions amounts to an equivalent bregman divergence on their corresponding parameters . in practice , the corresponding bregman generators of mixture/exponential families require to perform definite integral calculus that can either be too time-consuming ( for exponentially large discrete support case ) or even do not admit closed-form formula ( for continuous support case ) . in these cases , the dually flat construction remains theoretical and can not be used by information-geometric algorithms . to bypass this problem , we consider performing stochastic monte carlo ( mc ) estimation of those integral-based mixture/exponential family bregman generators . we show that , under natural assumptions , these mc generators are almost surely bregman generators . we define a series of dually flat information geometries , termed monte carlo information geometries , that increasingly-finely approximate the untractable geometry . the advantage of this mcig is that it allows a practical use of the bregman algorithmic toolbox on a wide range of probability distribution families . we demonstrate our approach with a clustering task on a mixture family manifold .", "topics": ["approximation algorithm", "cluster analysis"]}
{"title": "on the equivalence of causal models", "abstract": "scientists often use directed acyclic graphs ( days ) to model the qualitative structure of causal theories , allowing the parameters to be estimated from observational data . two causal models are equivalent if there is no experiment which could distinguish one from the other . a canonical representation for causal models is presented which yields an efficient graphical criterion for deciding equivalence , and provides a theoretical basis for extracting causal structures from empirical data . this representation is then extended to the more general case of an embedded causal model , that is , a dag in which only a subset of the variables are observable . the canonical representation presented here yields an efficient algorithm for determining when two embedded causal models reflect the same dependency information . this algorithm leads to a model theoretic definition of causation in terms of statistical dependencies .", "topics": ["causality", "eisenstein 's criterion"]}
{"title": "automatically reinforcing a game ai", "abstract": "a recent research trend in artificial intelligence ( ai ) is the combination of several programs into one single , stronger , program ; this is termed portfolio methods . we here investigate the application of such methods to game playing programs ( gpps ) . in addition , we consider the case in which only one gpp is available - by decomposing this single gpp into several ones through the use of parameters or even simply random seeds . these portfolio methods are trained in a learning phase . we propose two different offline approaches . the simplest one , bestarm , is a straightforward optimization of seeds or parame- ters ; it performs quite well against the original gpp , but performs poorly against an opponent which repeats games and learns . the second one , namely nash-portfolio , performs similarly in a `` one game '' test , and is much more robust against an opponent who learns . we also propose an online learning portfolio , which tests several of the gpp repeatedly and progressively switches to the best one - using a bandit algorithm .", "topics": ["artificial intelligence"]}
{"title": "listen , interact and talk : learning to speak via interaction", "abstract": "one of the long-term goals of artificial intelligence is to build an agent that can communicate intelligently with human in natural language . most existing work on natural language learning relies heavily on training over a pre-collected dataset with annotated labels , leading to an agent that essentially captures the statistics of the fixed external training data . as the training data is essentially a static snapshot representation of the knowledge from the annotator , the agent trained this way is limited in adaptiveness and generalization of its behavior . moreover , this is very different from the language learning process of humans , where language is acquired during communication by taking speaking action and learning from the consequences of speaking action in an interactive manner . this paper presents an interactive setting for grounded natural language learning , where an agent learns natural language by interacting with a teacher and learning from feedback , thus learning and improving language skills while taking part in the conversation . to achieve this goal , we propose a model which incorporates both imitation and reinforcement by leveraging jointly sentence and reward feedbacks from the teacher . experiments are conducted to validate the effectiveness of the proposed approach .", "topics": ["test set", "natural language processing"]}
{"title": "speckle reduction with trained nonlinear diffusion filtering", "abstract": "speckle reduction is a prerequisite for many image processing tasks in synthetic aperture radar ( sar ) images , as well as all coherent images . in recent years , predominant state-of-the-art approaches for despeckling are usually based on nonlocal methods which mainly concentrate on achieving utmost image restoration quality , with relatively low computational efficiency . therefore , in this study we aim to propose an efficient despeckling model with both high computational efficiency and high recovery quality . to this end , we exploit a newly-developed trainable nonlinear reaction diffusion ( tnrd ) framework which has proven a simple and effective model for various image restoration problems . { in the original tnrd applications , the diffusion network is usually derived based on the direct gradient descent scheme . however , this approach will encounter some problem for the task of multiplicative noise reduction exploited in this study . to solve this problem , we employed a new architecture derived from the proximal gradient descent method . } { taking into account the speckle noise statistics , the diffusion process for the despeckling task is derived . we then retrain all the model parameters in the presence of speckle noise . finally , optimized nonlinear diffusion filtering models are obtained , which are specialized for despeckling with various noise levels . experimental results substantiate that the trained filtering models provide comparable or even better results than state-of-the-art nonlocal approaches . meanwhile , our proposed model merely contains convolution of linear filters with an image , which offers high level parallelism on gpus . as a consequence , for images of size $ 512 \\times 512 $ , our gpu implementation takes less than 0.1 seconds to produce state-of-the-art despeckling performance . }", "topics": ["image processing", "high- and low-level"]}
{"title": "weight uncertainty in neural networks", "abstract": "we introduce a new , efficient , principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network , called bayes by backprop . it regularises the weights by minimising a compression cost , known as the variational free energy or the expected lower bound on the marginal likelihood . we show that this principled kind of regularisation yields comparable performance to dropout on mnist classification . we then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems , and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning .", "topics": ["calculus of variations", "reinforcement learning"]}
{"title": "complexity of prioritized default logics", "abstract": "in default reasoning , usually not all possible ways of resolving conflicts between default rules are acceptable . criteria expressing acceptable ways of resolving the conflicts may be hardwired in the inference mechanism , for example specificity in inheritance reasoning can be handled this way , or they may be given abstractly as an ordering on the default rules . in this article we investigate formalizations of the latter approach in reiter 's default logic . our goal is to analyze and compare the computational properties of three such formalizations in terms of their computational complexity : the prioritized default logics of baader and hollunder , and brewka , and a prioritized default logic that is based on lexicographic comparison . the analysis locates the propositional variants of these logics on the second and third levels of the polynomial hierarchy , and identifies the boundary between tractable and intractable inference for restricted classes of prioritized default theories .", "topics": ["computational complexity theory", "polynomial"]}
{"title": "sparsity-based defense against adversarial attacks on linear classifiers", "abstract": "deep neural networks represent the state of the art in machine learning in a growing number of fields , including vision , speech and natural language processing . however , recent work raises important questions about the robustness of such architectures , by showing that it is possible to induce classification errors through tiny , almost imperceptible , perturbations . vulnerability to such `` adversarial attacks '' , or `` adversarial examples '' , has been conjectured to be due to the excessive linearity of deep networks . in this paper , we study this phenomenon in the setting of a linear classifier , and show that it is possible to exploit sparsity in natural data to combat $ \\ell_ { \\infty } $ -bounded adversarial perturbations . specifically , we demonstrate the efficacy of a sparsifying front end via an ensemble averaged analysis , and experimental results for the mnist handwritten digit database . to the best of our knowledge , this is the first work to show that sparsity provides a theoretically rigorous framework for defense against adversarial attacks .", "topics": ["natural language processing", "natural language"]}
{"title": "characterization of the convergence of stationary fokker-planck learning", "abstract": "the convergence properties of the stationary fokker-planck algorithm for the estimation of the asymptotic density of stochastic search processes is studied . theoretical and empirical arguments for the characterization of convergence of the estimation in the case of separable and nonseparable nonlinear optimization problems are given . some implications of the convergence of stationary fokker-planck learning for the inference of parameters in artificial neural network models are outlined .", "topics": ["nonlinear system"]}
{"title": "incremental clustering and expansion for faster optimal planning in dec-pomdps", "abstract": "this article presents the state-of-the-art in optimal solution methods for decentralized partially observable markov decision processes ( dec-pomdps ) , which are general models for collaborative multiagent planning under uncertainty . building off the generalized multiagent a* ( gmaa* ) algorithm , which reduces the problem to a tree of one-shot collaborative bayesian games ( cbgs ) , we describe several advances that greatly expand the range of dec-pomdps that can be solved optimally . first , we introduce lossless incremental clustering of the cbgs solved by gmaa* , which achieves exponential speedups without sacrificing optimality . second , we introduce incremental expansion of nodes in the gmaa* search tree , which avoids the need to expand all children , the number of which is in the worst case doubly exponential in the nodes depth . this is particularly beneficial when little clustering is possible . in addition , we introduce new hybrid heuristic representations that are more compact and thereby enable the solution of larger dec-pomdps . we provide theoretical guarantees that , when a suitable heuristic is used , both incremental clustering and incremental expansion yield algorithms that are both complete and search equivalent . finally , we present extensive empirical results demonstrating that gmaa*-ice , an algorithm that synthesizes these advances , can optimally solve dec-pomdps of unprecedented size .", "topics": ["cluster analysis", "mathematical optimization"]}
{"title": "a data-driven approach for semantic role labeling from induced grammar structures in language", "abstract": "semantic roles play an important role in extracting knowledge from text . current unsupervised approaches utilize features from grammar structures , to induce semantic roles . the dependence on these grammars , however , makes it difficult to adapt to noisy and new languages . in this paper we develop a data-driven approach to identifying semantic roles , the approach is entirely unsupervised up to the point where rules need to be learned to identify the position the semantic role occurs . specifically we develop a modified-adios algorithm based on adios solan et al . ( 2005 ) to learn grammar structures , and use these grammar structures to learn the rules for identifying the semantic roles based on the context in which the grammar structures appeared . the results obtained are comparable with the current state-of-art models that are inherently dependent on human annotated data .", "topics": ["unsupervised learning"]}
{"title": "logarithmic time online multiclass prediction", "abstract": "we study the problem of multiclass classification with an extremely large number of classes ( k ) , with the goal of obtaining train and test time complexity logarithmic in the number of classes . we develop top-down tree construction approaches for constructing logarithmic depth trees . on the theoretical front , we formulate a new objective function , which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure ( in terms of class labels ) and balanced . we demonstrate that under favorable conditions , we can construct logarithmic depth trees that have leaves with low label entropy . however , the objective function at the nodes is challenging to optimize computationally . we address the empirical problem with a new online decision tree construction procedure . experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches , which makes it a plausible method in computationally constrained large-k applications .", "topics": ["time complexity", "optimization problem"]}
{"title": "porting htm models to the heidelberg neuromorphic computing platform", "abstract": "hierarchical temporal memory ( htm ) is a computational theory of machine intelligence based on a detailed study of the neocortex . the heidelberg neuromorphic computing platform , developed as part of the human brain project ( hbp ) , is a mixed-signal ( analog and digital ) large-scale platform for modeling networks of spiking neurons . in this paper we present the first effort in porting htm networks to this platform . we describe a framework for simulating key htm operations using spiking network models . we then describe specific spatial pooling and temporal memory implementations , as well as simulations demonstrating that the fundamental properties are maintained . we discuss issues in implementing the full set of plasticity rules using spike-timing dependent plasticity ( stdp ) , and rough place and route calculations . although further work is required , our initial studies indicate that it should be possible to run large-scale htm networks ( including plasticity rules ) efficiently on the heidelberg platform . more generally the exercise of porting high level htm algorithms to biophysical neuron models promises to be a fruitful area of investigation for future studies .", "topics": ["simulation", "artificial intelligence"]}
{"title": "a version of geiringer-like theorem for decision making in the environments with randomness and incomplete information", "abstract": "purpose : in recent years monte-carlo sampling methods , such as monte carlo tree search , have achieved tremendous success in model free reinforcement learning . a combination of the so called upper confidence bounds policy to preserve the `` exploration vs. exploitation '' balance to select actions for sample evaluations together with massive computing power to store and to update dynamically a rather large pre-evaluated game tree lead to the development of software that has beaten the top human player in the game of go on a 9 by 9 board . much effort in the current research is devoted to widening the range of applicability of the monte-carlo sampling methodology to partially observable markov decision processes with non-immediate payoffs . the main challenge introduced by randomness and incomplete information is to deal with the action evaluation at the chance nodes due to drastic differences in the possible payoffs the same action could lead to . the aim of this article is to establish a version of a theorem that originated from population genetics and has been later adopted in evolutionary computation theory that will lead to novel monte-carlo sampling algorithms that provably increase the ai potential . due to space limitations the actual algorithms themselves will be presented in the sequel papers , however , the current paper provides a solid mathematical foundation for the development of such algorithms and explains why they are so promising .", "topics": ["sampling ( signal processing )", "reinforcement learning"]}
{"title": "comparison of echo state network output layer classification methods on noisy data", "abstract": "echo state networks are a recently developed type of recurrent neural network where the internal layer is fixed with random weights , and only the output layer is trained on specific data . echo state networks are increasingly being used to process spatiotemporal data in real-world settings , including speech recognition , event detection , and robot control . a strength of echo state networks is the simple method used to train the output layer - typically a collection of linear readout weights found using a least squares approach . although straightforward to train and having a low computational cost to use , this method may not yield acceptable accuracy performance on noisy data . this study compares the performance of three echo state network output layer methods to perform classification on noisy data : using trained linear weights , using sparse trained linear weights , and using trained low-rank approximations of reservoir states . the methods are investigated experimentally on both synthetic and natural datasets . the experiments suggest that using regularized least squares to train linear output weights is superior on data with low noise , but using the low-rank approximations may significantly improve accuracy on datasets contaminated with higher noise levels .", "topics": ["recurrent neural network", "synthetic data"]}
{"title": "tableaux for the lambek-grishin calculus", "abstract": "categorial type logics , pioneered by lambek , seek a proof-theoretic understanding of natural language syntax by identifying categories with formulas and derivations with proofs . we typically observe an intuitionistic bias : a structural configuration of hypotheses ( a constituent ) derives a single conclusion ( the category assigned to it ) . acting upon suggestions of grishin to dualize the logical vocabulary , moortgat proposed the lambek-grishin calculus ( lg ) with the aim of restoring symmetry between hypotheses and conclusions . we develop a theory of labeled modal tableaux for lg , inspired by the interpretation of its connectives as binary modal operators in the relational semantics of kurtonina and moortgat . as a linguistic application of our method , we show that grammars based on lg are context-free through use of an interpolation lemma . this result complements that of melissen , who proved that lg augmented by mixed associativity and -commutativity was exceeds ltag in expressive power .", "topics": ["natural language"]}
{"title": "an intelligent approach for negotiating between chains in supply chain management systems", "abstract": "holding commercial negotiations and selecting the best supplier in supply chain management systems are among weaknesses of producers in production process . therefore , applying intelligent systems may have an effective role in increased speed and improved quality in the selections .this paper introduces a system which tries to trade using multi-agents systems and holding negotiations between any agents . in this system , an intelligent agent is considered for each segment of chains which it tries to send order and receive the response with attendance in negotiation medium and communication with other agents .this paper introduces how to communicate between agents , characteristics of multi-agent and standard registration medium of each agent in the environment . jade ( java application development environment ) was used for implementation and simulation of agents cooperation .", "topics": ["simulation"]}
{"title": "mobile machine learning hardware at arm : a systems-on-chip ( soc ) perspective", "abstract": "machine learning is playing an increasingly significant role in emerging mobile application domains such as ar/vr , adas , etc . accordingly , hardware architects have designed customized hardware for machine learning algorithms , especially neural networks , to improve compute efficiency . however , machine learning is typically just one processing stage in complex end-to-end applications , involving multiple components in a mobile systems-on-a-chip ( soc ) . focusing only on ml accelerators loses bigger optimization opportunity at the system ( soc ) level . this paper argues that hardware architects should expand the optimization scope to the entire soc . we demonstrate one particular case-study in the domain of continuous computer vision where camera sensor , image signal processor ( isp ) , memory , and nn accelerator are synergistically co-designed to achieve optimal system-level efficiency .", "topics": ["computer vision", "end-to-end principle"]}
{"title": "the single machine total weighted tardiness problem - is it ( for metaheuristics ) a solved problem ?", "abstract": "the article presents a study of rather simple local search heuristics for the single machine total weighted tardiness problem ( smtwtp ) , namely hillclimbing and variable neighborhood search . in particular , we revisit these approaches for the smtwtp as there appears to be a lack of appropriate/challenging benchmark instances in this case . the obtained results are impressive indeed . only few instances remain unsolved , and even those are approximated within 1 % of the optimal/best known solutions . our experiments support the claim that metaheuristics for the smtwtp are very likely to lead to good results , and that , before refining search strategies , more work must be done with regard to the proposition of benchmark data . some recommendations for the construction of such data sets are derived from our investigations .", "topics": ["heuristic"]}
{"title": "learned features are better for ethnicity classification", "abstract": "ethnicity is a key demographic attribute of human beings and it plays a vital role in automatic facial recognition and have extensive real world applications such as human computer interaction ( hci ) ; demographic based classification ; biometric based recognition ; security and defense to name a few . in this paper we present a novel approach for extracting ethnicity from the facial images . the proposed method makes use of a pre trained convolutional neural network ( cnn ) to extract the features and then support vector machine ( svm ) with linear kernel is used as a classifier . this technique uses translational invariant hierarchical features learned by the network , in contrast to previous works , which use hand crafted features such as local binary pattern ( lbp ) ; gabor etc . thorough experiments are presented on ten different facial databases which strongly suggest that our approach is robust to different expressions and illuminations conditions . here we consider ethnicity classification as a three class problem including asian , african-american and caucasian . average classification accuracy over all databases is 98.28 % , 99.66 % and 99.05 % for asian , african-american and caucasian respectively .", "topics": ["support vector machine", "database"]}
{"title": "excess entropy in natural language : present state and perspectives", "abstract": "we review recent progress in understanding the meaning of mutual information in natural language . let us define words in a text as strings that occur sufficiently often . in a few previous papers , we have shown that a power-law distribution for so defined words ( a.k.a . herdan 's law ) is obeyed if there is a similar power-law growth of ( algorithmic ) mutual information between adjacent portions of texts of increasing length . moreover , the power-law growth of information holds if texts describe a complicated infinite ( algorithmically ) random object in a highly repetitive way , according to an analogous power-law distribution . the described object may be immutable ( like a mathematical or physical constant ) or may evolve slowly in time ( like cultural heritage ) . here we reflect on the respective mathematical results in a less technical way . we also discuss feasibility of deciding to what extent these results apply to the actual human communication .", "topics": ["natural language"]}
{"title": "reconstructing neural parameters and synapses of arbitrary interconnected neurons from their simulated spiking activity", "abstract": "to understand the behavior of a neural circuit it is a presupposition that we have a model of the dynamical system describing this circuit . this model is determined by several parameters , including not only the synaptic weights , but also the parameters of each neuron . existing works mainly concentrate on either the synaptic weights or the neural parameters . in this paper we present an algorithm to reconstruct all parameters including the synaptic weights of a spiking neuron model . the model based on works of eugene m. izhikevich ( izhikevich 2007 ) consists of two differential equations and covers different types of cortical neurons . it combines the dynamical properties of hodgkin-huxley-type dynamics with a high computational efficiency . the presented algorithm uses the recordings of the corresponding membrane potentials of the model for the reconstruction and consists of two main components . the first component is a rank based genetic algorithm ( ga ) which is used to find the neural parameters of the model . the second one is a least mean squares approach which computes the synaptic weights of all interconnected neurons by minimizing the squared error between the calculated and the measured membrane potentials for each time step . in preparation for the reconstruction of the neural parameters and of the synaptic weights from real measured membrane potentials , promising results based on simulated data generated with a randomly parametrized izhikevich model are presented . the reconstruction does not only converge to a global minimum of neural parameters , but also approximates the synaptic weights with high precision .", "topics": ["simulation"]}
{"title": "variants of rmsprop and adagrad with logarithmic regret bounds", "abstract": "adaptive gradient methods have become recently very popular , in particular as they have been shown to be useful in the training of deep neural networks . in this paper we have analyzed rmsprop , originally proposed for the training of deep neural networks , in the context of online convex optimization and show $ \\sqrt { t } $ -type regret bounds . moreover , we propose two variants sc-adagrad and sc-rmsprop for which we show logarithmic regret bounds for strongly convex functions . finally , we demonstrate in the experiments that these new variants outperform other adaptive gradient techniques or stochastic gradient descent in the optimization of strongly convex functions as well as in training of deep neural networks .", "topics": ["regret ( decision theory )", "gradient descent"]}
{"title": "visual place recognition with probabilistic vertex voting", "abstract": "we propose a novel scoring concept for visual place recognition based on nearest neighbor descriptor voting and demonstrate how the algorithm naturally emerges from the problem formulation . based on the observation that the number of votes for matching places can be evaluated using a binomial distribution model , loop closures can be detected with high precision . by casting the problem into a probabilistic framework , we not only remove the need for commonly employed heuristic parameters but also provide a powerful score to classify matching and non-matching places . we present methods for both a 2d-2d pose-graph vertex matching and a 2d-3d landmark matching based on the above scoring . the approach maintains accuracy while being efficient enough for online application through the use of compact ( low dimensional ) descriptors and fast nearest neighbor retrieval techniques . the proposed methods are evaluated on several challenging datasets in varied environments , showing state-of-the-art results with high precision and high recall .", "topics": ["heuristic"]}
{"title": "relnn : a deep neural model for relational learning", "abstract": "statistical relational ai ( starai ) aims at reasoning and learning in noisy domains described in terms of objects and relationships by combining probability with first-order logic . with huge advances in deep learning in the current years , combining deep networks with first-order logic has been the focus of several recent studies . many of the existing attempts , however , only focus on relations and ignore object properties . the attempts that do consider object properties are limited in terms of modelling power or scalability . in this paper , we develop relational neural networks ( relnns ) by adding hidden layers to relational logistic regression ( the relational counterpart of logistic regression ) . we learn latent properties for objects both directly and through general rules . back-propagation is used for training these models . a modular , layer-wise architecture facilitates utilizing the techniques developed within deep learning community to our architecture . initial experiments on eight tasks over three real-world datasets show that relnns are promising models for relational learning .", "topics": ["scalability"]}
{"title": "decision forests , convolutional networks and the models in-between", "abstract": "this paper investigates the connections between two state of the art classifiers : decision forests ( dfs , including decision jungles ) and convolutional neural networks ( cnns ) . decision forests are computationally efficient thanks to their conditional computation property ( computation is confined to only a small region of the tree , the nodes along a single branch ) . cnns achieve state of the art accuracy , thanks to their representation learning capabilities . we present a systematic analysis of how to fuse conditional computation with representation learning and achieve a continuum of hybrid models with different ratios of accuracy vs. efficiency . we call this new family of hybrid models conditional networks . conditional networks can be thought of as : i ) decision trees augmented with data transformation operators , or ii ) cnns , with block-diagonal sparse weight matrices , and explicit data routing functions . experimental validation is performed on the common task of image classification on both the cifar and imagenet datasets . compared to state of the art cnns , our hybrid models yield the same accuracy with a fraction of the compute cost and much smaller number of parameters .", "topics": ["feature learning", "computational complexity theory"]}
{"title": "analysis of vessel connectivities in retinal images by cortically inspired spectral clustering", "abstract": "retinal images provide early signs of diabetic retinopathy , glaucoma , and hypertension . these signs can be investigated based on microaneurysms or smaller vessels . the diagnostic biomarkers are the change of vessel widths and angles especially at junctions , which are investigated using the vessel segmentation or tracking . vessel paths may also be interrupted ; crossings and bifurcations may be disconnected . this paper addresses a novel contextual method based on the geometry of the primary visual cortex ( v1 ) to study these difficulties . we have analyzed the specific problems at junctions with a connectivity kernel obtained as the fundamental solution of the fokker-planck equation , which is usually used to represent the geometrical structure of multi-orientation cortical connectivity . using the spectral clustering on a large local affinity matrix constructed by both the connectivity kernel and the feature of intensity , the vessels are identified successfully in a hierarchical topology each representing an individual perceptual unit .", "topics": ["kernel ( operating system )", "cluster analysis"]}
{"title": "linear probability forecasting", "abstract": "multi-class classification is one of the most important tasks in machine learning . in this paper we consider two online multi-class classification problems : classification by a linear model and by a kernelized model . the quality of predictions is measured by the brier loss function . we suggest two computationally efficient algorithms to work with these problems and prove theoretical guarantees on their losses . we kernelize one of the algorithms and prove theoretical guarantees on its loss . we perform experiments and compare our algorithms with logistic regression .", "topics": ["computational complexity theory", "loss function"]}
{"title": "improved lossy image compression with priming and spatially adaptive bit rates for recurrent networks", "abstract": "we propose a method for lossy image compression based on recurrent , convolutional neural networks that outperforms bpg ( 4:2:0 ) , webp , jpeg2000 , and jpeg as measured by ms-ssim . we introduce three improvements over previous research that lead to this state-of-the-art result . first , we show that training with a pixel-wise loss weighted by ssim increases reconstruction quality according to several metrics . second , we modify the recurrent architecture to improve spatial diffusion , which allows the network to more effectively capture and propagate image information through the network 's hidden state . finally , in addition to lossless entropy coding , we use a spatially adaptive bit allocation algorithm to more efficiently use the limited number of bits to encode visually complex image regions . we evaluate our method on the kodak and tecnick image sets and compare against standard codecs as well recently published methods based on deep neural networks .", "topics": ["pixel"]}
{"title": "genetic algorithms for evolving computer chess programs", "abstract": "this paper demonstrates the use of genetic algorithms for evolving : 1 ) a grandmaster-level evaluation function , and 2 ) a search mechanism for a chess program , the parameter values of which are initialized randomly . the evaluation function of the program is evolved by learning from databases of ( human ) grandmaster games . at first , the organisms are evolved to mimic the behavior of human grandmasters , and then these organisms are further improved upon by means of coevolution . the search mechanism is evolved by learning from tactical test suites . our results show that the evolved program outperforms a two-time world computer chess champion and is at par with the other leading computer chess programs .", "topics": ["database"]}
{"title": "neural network-based clustering using pairwise constraints", "abstract": "this paper presents a neural network-based end-to-end clustering framework . we design a novel strategy to utilize the contrastive criteria for pushing data-forming clusters directly from raw data , in addition to learning a feature embedding suitable for such clustering . the network is trained with weak labels , specifically partial pairwise relationships between data instances . the cluster assignments and their probabilities are then obtained at the output layer by feed-forwarding the data . the framework has the interesting characteristic that no cluster centers need to be explicitly specified , thus the resulting cluster distribution is purely data-driven and no distance metrics need to be predefined . the experiments show that the proposed approach beats the conventional two-stage method ( feature embedding with k-means ) by a significant margin . it also compares favorably to the performance of the standard cross entropy loss for classification . robustness analysis also shows that the method is largely insensitive to the number of clusters . specifically , we show that the number of dominant clusters is close to the true number of clusters even when a large k is used for clustering .", "topics": ["cluster analysis", "mnist database"]}
{"title": "improving the performance of maxrpc", "abstract": "max restricted path consistency ( maxrpc ) is a local consistency for binary constraints that can achieve considerably stronger pruning than arc consistency . however , existing maxrrc algorithms suffer from overheads and redundancies as they can repeatedly perform many constraint checks without triggering any value deletions . in this paper we propose techniques that can boost the performance of maxrpc algorithms . these include the combined use of two data structures to avoid many redundant constraint checks , and heuristics for the efficient ordering and execution of certain operations . based on these , we propose two closely related algorithms . the first one which is a maxrpc algorithm with optimal o ( end^3 ) time complexity , displays good performance when used stand-alone , but is expensive to apply during search . the second one approximates maxrpc and has o ( en^2d^4 ) time complexity , but a restricted version with o ( end^4 ) complexity can be very efficient when used during search . both algorithms have o ( ed ) space complexity . experimental results demonstrate that the resulting methods constantly outperform previous algorithms for maxrpc , often by large margins , and constitute a more than viable alternative to arc consistency on many problems .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "radical-level ideograph encoder for rnn-based sentiment analysis of chinese and japanese", "abstract": "the character vocabulary can be very large in non-alphabetic languages such as chinese and japanese , which makes neural network models huge to process such languages . we explored a model for sentiment classification that takes the embeddings of the radicals of the chinese characters , i.e , hanzi of chinese and kanji of japanese . our model is composed of a cnn word feature encoder and a bi-directional rnn document feature encoder . the results achieved are on par with the character embedding-based models , and close to the state-of-the-art word embedding-based models , with 90 % smaller vocabulary , and at least 13 % and 80 % fewer parameters than the character embedding-based models and word embedding-based models respectively . the results suggest that the radical embedding-based approach is cost-effective for machine learning on chinese and japanese .", "topics": ["encoder"]}
{"title": "exploiting linguistic resources for neural machine translation using multi-task learning", "abstract": "linguistic resources such as part-of-speech ( pos ) tags have been extensively used in statistical machine translation ( smt ) frameworks and have yielded better performances . however , usage of such linguistic annotations in neural machine translation ( nmt ) systems has been left under-explored . in this work , we show that multi-task learning is a successful and a easy approach to introduce an additional knowledge into an end-to-end neural attentional model . by jointly training several natural language processing ( nlp ) tasks in one system , we are able to leverage common information and improve the performance of the individual task . we analyze the impact of three design decisions in multi-task learning : the tasks used in training , the training schedule , and the degree of parameter sharing across the tasks , which is defined by the network architecture . the experiments are conducted for an german to english translation task . as additional linguistic resources , we exploit pos information and named-entities ( ne ) . experiments show that the translation quality can be improved by up to 1.5 bleu points under the low-resource condition . the performance of the pos tagger is also improved using the multi-task learning scheme .", "topics": ["natural language processing", "machine translation"]}
{"title": "clustering with a reject option : interactive clustering as bayesian prior elicitation", "abstract": "a good clustering can help a data analyst to explore and understand a data set , but what constitutes a good clustering may depend on domain-specific and application-specific criteria . these criteria can be difficult to formalize , even when it is easy for an analyst to know a good clustering when she sees one . we present a new approach to interactive clustering for data exploration , called \\ciif , based on a particularly simple feedback mechanism , in which an analyst can choose to reject individual clusters and request new ones . the new clusters should be different from previously rejected clusters while still fitting the data well . we formalize this interaction in a novel bayesian prior elicitation framework . in each iteration , the prior is adapted to account for all the previous feedback , and a new clustering is then produced from the posterior distribution . to achieve the computational efficiency necessary for an interactive setting , we propose an incremental optimization method over data minibatches using lagrangian relaxation . experiments demonstrate that \\ciif can produce accurate and diverse clusterings .", "topics": ["cluster analysis"]}
{"title": "combining the best of graphical models and convnets for semantic segmentation", "abstract": "we present a two-module approach to semantic segmentation that incorporates convolutional networks ( cnns ) and graphical models . graphical models are used to generate a small ( 5-30 ) set of diverse segmentations proposals , such that this set has high recall . since the number of required proposals is so low , we can extract fairly complex features to rank them . our complex feature of choice is a novel cnn called segnet , which directly outputs a ( coarse ) semantic segmentation . importantly , segnet is specifically trained to optimize the corpus-level pascal iou loss function . to the best of our knowledge , this is the first cnn specifically designed for semantic segmentation . this two-module approach achieves $ 52.5\\ % $ on the pascal 2012 segmentation challenge .", "topics": ["graphical model", "loss function"]}
{"title": "three new methods for evaluating reference resolution", "abstract": "reference resolution on extended texts ( several thousand references ) can not be evaluated manually . an evaluation algorithm has been proposed for the muc tests , using equivalence classes for the coreference relation . however , we show here that this algorithm is too indulgent , yielding good scores even for poor resolution strategies . we elaborate on the same formalism to propose two new evaluation algorithms , comparing them first with the muc algorithm and giving then results on a variety of examples . a third algorithm using only distributional comparison of equivalence classes is finally described ; it assesses the relative importance of the recall vs. precision errors .", "topics": ["natural language processing", "entity"]}
{"title": "block dct filtering using vector processing", "abstract": "filtering is an important issue in signals and images processing . many images and videos are compressed using discrete cosine transform ( dct ) . for reducing the computation complexity , we are interested in filtering block and images directly in dct domain . this article proposed an efficient and yet very simple filtering method directly in dct domain for any symmetric , asymmetric , separable , inseparable and one or two dimensional filter . the proposed method is achieved by mathematical relations using vector processing for image filtering which it is equivalent to the spatial domain zero padding filtering . also to avoid the zero padding artifacts around the edge of the block , we prepare preliminary matrices in dct domain by implementation elements of selected mask which satisfies border replication for a block in the spatial domain . to evaluate the performance of the proposed algorithm , we compared the spatial domain filtering results with the results of the proposed method in dct domain . the experiments show that the results of our proposed method in dct are exactly the same as the spatial domain filtering .", "topics": ["computation"]}
{"title": "parameter-free online learning via model selection", "abstract": "we introduce an efficient algorithmic framework for model selection in online learning , also known as parameter-free online learning . departing from previous work , which has focused on highly structured function classes such as nested balls in hilbert space , we propose a generic meta-algorithm framework that achieves online model selection oracle inequalities under minimal structural assumptions . we give the first computationally efficient parameter-free algorithms that work in arbitrary banach spaces under mild smoothness assumptions ; previous results applied only to hilbert spaces . we further derive new oracle inequalities for matrix classes , non-nested convex sets , and $ \\mathbb { r } ^ { d } $ with generic regularizers . finally , we generalize these results by providing oracle inequalities for arbitrary non-linear classes in the online supervised learning model . these results are all derived through a unified meta-algorithm scheme using a novel `` multi-scale '' algorithm for prediction with expert advice based on random playout , which may be of independent interest .", "topics": ["supervised learning", "computational complexity theory"]}
{"title": "mining tree-query associations in graphs", "abstract": "new applications of data mining , such as in biology , bioinformatics , or sociology , are faced with large datasetsstructured as graphs . we introduce a novel class of tree-shapedpatterns called tree queries , and present algorithms for miningtree queries and tree-query associations in a large data graph . novel about our class of patterns is that they can containconstants , and can contain existential nodes which are not counted when determining the number of occurrences of the patternin the data graph . our algorithms have a number of provableoptimality properties , which are based on the theory of conjunctive database queries . we propose a practical , database-oriented implementation in sql , and show that the approach works in practice through experiments on data about food webs , protein interactions , and citation analysis .", "topics": ["data mining", "interaction"]}
{"title": "kernel sequential monte carlo", "abstract": "we propose kernel sequential monte carlo ( ksmc ) , a framework for sampling from static target densities . ksmc is a family of sequential monte carlo algorithms that are based on building emulator models of the current particle system in a reproducing kernel hilbert space . we here focus on modelling nonlinear covariance structure and gradients of the target . the emulator 's geometry is adaptively updated and subsequently used to inform local proposals . unlike in adaptive markov chain monte carlo , continuous adaptation does not compromise convergence of the sampler . ksmc combines the strengths of sequental monte carlo and kernel methods : superior performance for multimodal targets and the ability to estimate model evidence as compared to markov chain monte carlo , and the emulator 's ability to represent targets that exhibit high degrees of nonlinearity . as ksmc does not require access to target gradients , it is particularly applicable on targets whose gradients are unknown or prohibitively expensive . we describe necessary tuning details and demonstrate the benefits of the the proposed methodology on a series of challenging synthetic and real-world examples .", "topics": ["sampling ( signal processing )", "kernel ( operating system )"]}
{"title": "machine learning for neural decoding", "abstract": "while machine learning tools have been rapidly advancing , the majority of neural decoding approaches still use last century 's methods . improving the performance of neural decoding algorithms allows us to better understand what information is contained in the brain , and can help advance engineering applications such as brain machine interfaces . here , we apply modern machine learning techniques , including neural networks and gradient boosting , to decode from spiking activity in 1 ) motor cortex , 2 ) somatosensory cortex , and 3 ) hippocampus . we compare the predictive ability of these modern methods with traditional decoding methods such as wiener and kalman filters . modern methods , in particular neural networks and ensembles , significantly outperformed the traditional approaches . for instance , for all of the three brain areas , an lstm decoder explained over 40 % of the unexplained variance from a wiener filter . these results suggest that modern machine learning techniques should become the standard methodology for neural decoding . we provide code to facilitate wider implementation of these methods .", "topics": ["gradient"]}
{"title": "dxnn platform : the shedding of biological inefficiencies", "abstract": "this paper introduces a novel type of memetic algorithm based topology and weight evolving artificial neural network ( tweann ) system called dx neural network ( dxnn ) . dxnn implements a number of interesting features , amongst which is : a simple and database friendly tuple based encoding method , a 2 phase neuroevolutionary approach aimed at removing the need for speciation due to its intrinsic population diversification effects , a new `` targeted tuning phase '' aimed at dealing with `` the curse of dimensionality '' , and a new random intensity mutation ( rim ) method that removes the need for crossover algorithms . the paper will discuss dxnn 's architecture , mutation operators , and its built in feature selection method that allows for the evolved systems to expand and incorporate new sensors and actuators . i then compare dxnn to other state of the art tweanns on the standard double pole balancing benchmark , and demonstrate its superior ability to evolve highly compact solutions faster than its competitors . then a set of oblation experiments is performed to demonstrate how each feature of dxnn effects its performance , followed by a set of experiments which demonstrate the platform 's ability to create nn populations with exceptionally high diversity profiles . finally , dxnn is used to evolve artificial robots in a set of two dimensional open-ended food gathering and predator-prey simulations , demonstrating the system 's ability to produce ever more complex neural networks , and the system 's applicability to the domain of robotics , artificial life , and coevolution .", "topics": ["neural networks", "simulation"]}
{"title": "large-scale feature learning with spike-and-slab sparse coding", "abstract": "we consider the problem of object recognition with a large number of classes . in order to overcome the low amount of labeled examples available in this setting , we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding ( s3c ) . prior work on s3c has not prioritized the ability to exploit parallel architectures and scale s3c to the enormous problem sizes needed for object recognition . we present a novel inference procedure for appropriate for use with gpus which allows us to dramatically increase both the training set size and the amount of latent factors that s3c may be trained with . we demonstrate that this approach improves upon the supervised learning capabilities of both sparse coding and the spike-and-slab restricted boltzmann machine ( ssrbm ) on the cifar-10 dataset . we use the cifar-100 dataset to demonstrate that our method scales to large numbers of classes better than previous methods . finally , we use our method to win the nips 2011 workshop on challenges in learning hierarchical models ? transfer learning challenge .", "topics": ["feature learning", "supervised learning"]}
{"title": "optimal weighting for exam composition", "abstract": "a problem faced by many instructors is that of designing exams that accurately assess the abilities of the students . typically these exams are prepared several days in advance , and generic question scores are used based on rough approximation of the question difficulty and length . for example , for a recent class taught by the author , there were 30 multiple choice questions worth 3 points , 15 true/false with explanation questions worth 4 points , and 5 analytical exercises worth 10 points . we describe a novel framework where algorithms from machine learning are used to modify the exam question weights in order to optimize the exam scores , using the overall class grade as a proxy for a student 's true ability . we show that significant error reduction can be obtained by our approach over standard weighting schemes , and we make several new observations regarding the properties of the `` good '' and `` bad '' exam questions that can have impact on the design of improved future evaluation methods .", "topics": ["approximation"]}
{"title": "computational aspects of the mobius transform", "abstract": "in this paper we associate with every ( directed ) graph g a transformation called the mobius transformation of the graph g. the mobius transformation of the graph ( o ) is of major significance for dempster-shafer theory of evidence . however , because it is computationally very heavy , the mobius transformation together with dempster 's rule of combination is a major obstacle to the use of dempster-shafer theory for handling uncertainty in expert systems . the major contribution of this paper is the discovery of the 'fast mobius transformations ' of ( o ) . these 'fast mobius transformations ' are the fastest algorithms for computing the mobius transformation of ( o ) . as an easy but useful application , we provide , via the commonality function , an algorithm for computing dempster 's rule of combination which is much faster than the usual one .", "topics": ["computational complexity theory"]}
{"title": "efficient-ucbv : an almost optimal algorithm using variance estimates", "abstract": "we propose a novel variant of the ucb algorithm ( referred to as efficient-ucb-variance ( eucbv ) ) for minimizing cumulative regret in the stochastic multi-armed bandit ( mab ) setting . eucbv incorporates the arm elimination strategy proposed in ucb-improved \\citep { auer2010ucb } , while taking into account the variance estimates to compute the arms ' confidence bounds , similar to ucbv \\citep { audibert2009exploration } . through a theoretical analysis we establish that eucbv incurs a \\emph { gap-dependent } regret bound of { \\scriptsize $ o\\left ( \\dfrac { k\\sigma^2_ { \\max } \\log ( t\\delta^2 /k ) } { \\delta } \\right ) $ } after $ t $ trials , where $ \\delta $ is the minimal gap between optimal and sub-optimal arms ; the above bound is an improvement over that of existing state-of-the-art ucb algorithms ( such as ucb1 , ucb-improved , ucbv , moss ) . further , eucbv incurs a \\emph { gap-independent } regret bound of { \\scriptsize $ o\\left ( \\sqrt { kt } \\right ) $ } which is an improvement over that of ucb1 , ucbv and ucb-improved , while being comparable with that of moss and ocucb . through an extensive numerical study we show that eucbv significantly outperforms the popular ucb variants ( like moss , ocucb , etc . ) as well as thompson sampling and bayes-ucb algorithms .", "topics": ["regret ( decision theory )", "numerical analysis"]}
{"title": "a calculus for causal relevance", "abstract": "this paper presents a sound and completecalculus for causal relevance , based onpearl 's functional models semantics.the calculus consists of axioms and rulesof inference for reasoning about causalrelevance relationships.we extend the set of known axioms for causalrelevance with three new axioms , andintroduce two new rules of inference forreasoning about specific subclasses ofmodels.these subclasses give a more refinedcharacterization of causal models than the one given in halpern 's axiomatizationof counterfactual reasoning.finally , we show how the calculus for causalrelevance can be used in the task ofidentifying causal structure from non-observational data .", "topics": ["relevance", "causality"]}
{"title": "end-to-end global to local cnn learning for hand pose recovery in depth data", "abstract": "despite recent advances in 3d pose estimation of human hands , especially thanks to the advent of cnns and depth cameras , this task is still far from being solved . this is mainly due to the highly non-linear dynamics of fingers , which makes hand model training a challenging task . in this paper , we exploit a novel hierarchical tree-like structured cnn , in which branches are trained to become specialized in predefined subsets of hand joints , called local poses . we further fuse local pose features , extracted from hierarchical cnn branches , to learn higher order dependencies among joints in the final pose by end-to-end training . lastly , the loss function used is also defined to incorporate appearance and physical constraints about doable hand motion and deformation . experimental results suggest that feeding a tree-shaped cnn , specialized in local poses , into a fusion network for modeling joints correlations , helps to increase the precision of final estimations , outperforming state-of-the-art results on nyu and msra datasets .", "topics": ["loss function", "end-to-end principle"]}
{"title": "representation and reinforcement learning for personalized glycemic control in septic patients", "abstract": "glycemic control is essential for critical care . however , it is a challenging task because there has been no study on personalized optimal strategies for glycemic control . this work aims to learn personalized optimal glycemic trajectories for severely ill septic patients by learning data-driven policies to identify optimal targeted blood glucose levels as a reference for clinicians . we encoded patient states using a sparse autoencoder and adopted a reinforcement learning paradigm using policy iteration to learn the optimal policy from data . we also estimated the expected return following the policy learned from the recorded glycemic trajectories , which yielded a function indicating the relationship between real blood glucose values and 90-day mortality rates . this suggests that the learned optimal policy could reduce the patients ' estimated 90-day mortality rate by 6.3 % , from 31 % to 24.7 % . the result demonstrates that reinforcement learning with appropriate patient state encoding can potentially provide optimal glycemic trajectories and allow clinicians to design a personalized strategy for glycemic control in septic patients .", "topics": ["value ( ethics )", "reinforcement learning"]}
{"title": "online learning : sufficient statistics and the burkholder method", "abstract": "we uncover a fairly general principle in online learning : if regret can be ( approximately ) expressed as a function of certain `` sufficient statistics '' for the data sequence , then there exists a special burkholder function that 1 ) can be used algorithmically to achieve the regret bound and 2 ) only depends on these sufficient statistics , not the entire data sequence , so that the online strategy is only required to keep the sufficient statistics in memory . this characterization is achieved by bringing the full power of the burkholder method -- - originally developed for certifying probabilistic martingale inequalities -- - to bear on the online learning setting . to demonstrate the scope and effectiveness of the burkholder method , we develop a novel online strategy for matrix prediction that attains a regret bound corresponding to the variance term in matrix concentration inequalities . we also present a linear-time/space prediction strategy for parameter free supervised learning with linear classes and general smooth norms .", "topics": ["regret ( decision theory )", "supervised learning"]}
{"title": "metric learning for temporal sequence alignment", "abstract": "in this paper , we propose to learn a mahalanobis distance to perform alignment of multivariate time series . the learning examples for this task are time series for which the true alignment is known . we cast the alignment problem as a structured prediction task , and propose realistic losses between alignments for which the optimization is tractable . we provide experiments on real data in the audio to audio context , where we show that the learning of a similarity measure leads to improvements in the performance of the alignment task . we also propose to use this metric learning framework to perform feature selection and , from basic audio features , build a combination of these with better performance for the alignment .", "topics": ["time series"]}
{"title": "adversarial autoencoders", "abstract": "in this paper , we propose the `` adversarial autoencoder '' ( aae ) , which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks ( gan ) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution . matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples . as a result , the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution . we show how the adversarial autoencoder can be used in applications such as semi-supervised classification , disentangling style and content of images , unsupervised clustering , dimensionality reduction and data visualization . we performed experiments on mnist , street view house numbers and toronto face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks .", "topics": ["generative model", "supervised learning"]}
{"title": "end-to-end face detection and recognition", "abstract": "plenty of face detection and recognition methods have been proposed and got delightful results in decades . common face recognition pipeline consists of : 1 ) face detection , 2 ) face alignment , 3 ) feature extraction , 4 ) similarity calculation , which are separated and independent from each other . the separated face analyzing stages lead the model redundant calculation and are hard for end-to-end training . in this paper , we proposed a novel end-to-end trainable convolutional network framework for face detection and recognition , in which a geometric transformation matrix was directly learned to align the faces , instead of predicting the facial landmarks . in training stage , our single cnn model is supervised only by face bounding boxes and personal identities , which are publicly available from wider face \\cite { yang2016 } dataset and casia-webface \\cite { yi2014 } dataset . tested on face detection dataset and benchmark ( fddb ) \\cite { jain2010 } dataset and labeled face in the wild ( lfw ) \\cite { huang2007 } dataset , we have achieved 89.24\\ % recall for face detection task and 98.63\\ % verification accuracy for face recognition task simultaneously , which are comparable to state-of-the-art results .", "topics": ["feature extraction", "end-to-end principle"]}
{"title": "principal component projection without principal component analysis", "abstract": "we show how to efficiently project a vector onto the top principal components of a matrix , without explicitly computing these components . specifically , we introduce an iterative algorithm that provably computes the projection using few calls to any black-box routine for ridge regression . by avoiding explicit principal component analysis ( pca ) , our algorithm is the first with no runtime dependence on the number of top principal components . we show that it can be used to give a fast iterative method for the popular principal component regression problem , giving the first major runtime improvement over the naive method of combining pca with regression . to achieve our results , we first observe that ridge regression can be used to obtain a `` smooth projection '' onto the top principal components . we then sharpen this approximation to true projection using a low-degree polynomial approximation to the matrix step function . step function approximation is a topic of long-term interest in scientific computing . we extend prior theory by constructing polynomials with simple iterative structure and rigorously analyzing their behavior under limited precision .", "topics": ["approximation", "polynomial"]}
{"title": "self-organizing multilayered neural networks of optimal complexity", "abstract": "the principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set . the method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics .", "topics": ["neural networks"]}
{"title": "optimizing long short-term memory recurrent neural networks using ant colony optimization to predict turbine engine vibration", "abstract": "this article expands on research that has been done to develop a recurrent neural network ( rnn ) capable of predicting aircraft engine vibrations using long short-term memory ( lstm ) neurons . lstm rnns can provide a more generalizable and robust method for prediction over analytical calculations of engine vibration , as analytical calculations must be solved iteratively based on specific empirical engine parameters , making this approach ungeneralizable across multiple engines . in initial work , multiple lstm rnn architectures were proposed , evaluated and compared . this research improves the performance of the most effective lstm network design proposed in the previous work by using a promising neuroevolution method based on ant colony optimization ( aco ) to develop and enhance the lstm cell structure of the network . a parallelized version of the aco neuroevolution algorithm has been developed and the evolved lstm rnns were compared to the previously used fixed topology . the evolved networks were trained on a large database of flight data records obtained from an airline containing flights that suffered from excessive vibration . results were obtained using mpi ( message passing interface ) on a high performance computing ( hpc ) cluster , evolving 1000 different lstm cell structures using 168 cores over 4 days . the new evolved lstm cells showed an improvement of 1.35 % , reducing prediction error from 5.51 % to 4.17 % when predicting excessive engine vibrations 10 seconds in the future , while at the same time dramatically reducing the number of weights from 21,170 to 11,810 .", "topics": ["recurrent neural network", "database"]}
{"title": "neural summarization by extracting sentences and words", "abstract": "traditional approaches to extractive summarization rely heavily on human-engineered features . in this work we propose a data-driven approach based on neural networks and continuous sentence features . we develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor . this architecture allows us to develop different classes of summarization models which can extract sentences or words . we train our models on large scale corpora containing hundreds of thousands of document-summary pairs . experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation .", "topics": ["text corpus", "encoder"]}
{"title": "on the vocabulary of grammar-based codes and the logical consistency of texts", "abstract": "the article presents a new interpretation for zipf-mandelbrot 's law in natural language which rests on two areas of information theory . firstly , we construct a new class of grammar-based codes and , secondly , we investigate properties of strongly nonergodic stationary processes . the motivation for the joint discussion is to prove a proposition with a simple informal statement : if a text of length $ n $ describes $ n^\\beta $ independent facts in a repetitive way then the text contains at least $ n^\\beta/\\log n $ different words , under suitable conditions on $ n $ . in the formal statement , two modeling postulates are adopted . firstly , the words are understood as nonterminal symbols of the shortest grammar-based encoding of the text . secondly , the text is assumed to be emitted by a finite-energy strongly nonergodic source whereas the facts are binary iid variables predictable in a shift-invariant way .", "topics": ["natural language", "relevance"]}
{"title": "describing videos by exploiting temporal structure", "abstract": "recent progress in using recurrent neural networks ( rnns ) for image description has motivated the exploration of their application for video description . however , while images are static , working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description . in this context , we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions . first , our approach incorporates a spatial temporal 3-d convolutional neural network ( 3-d cnn ) representation of the short temporal dynamics . the 3-d cnn representation is trained on video action recognition tasks , so as to produce a representation that is tuned to human motion and behavior . second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating rnn . our approach exceeds the current state-of-art for both bleu and meteor metrics on the youtube2text dataset . we also present results on a new , larger and more challenging dataset of paired video and natural language descriptions .", "topics": ["recurrent neural network", "natural language"]}
{"title": "a primal-dual convergence analysis of boosting", "abstract": "boosting combines weak learners into a predictor with low empirical risk . its dual constructs a high entropy distribution upon which weak learners and training labels are uncorrelated . this manuscript studies this primal-dual relationship under a broad family of losses , including the exponential loss of adaboost and the logistic loss , revealing : - weak learnability aids the whole loss family : for any { \\epsilon } > 0 , o ( ln ( 1/ { \\epsilon } ) ) iterations suffice to produce a predictor with empirical risk { \\epsilon } -close to the infimum ; - the circumstances granting the existence of an empirical risk minimizer may be characterized in terms of the primal and dual problems , yielding a new proof of the known rate o ( ln ( 1/ { \\epsilon } ) ) ; - arbitrary instances may be decomposed into the above two , granting rate o ( 1/ { \\epsilon } ) , with a matching lower bound provided for the logistic loss .", "topics": ["iteration"]}
{"title": "fairness in learning : classic and contextual bandits", "abstract": "we introduce the study of fairness in multi-armed bandit problems . our fairness definition can be interpreted as demanding that given a pool of applicants ( say , for college admission or mortgages ) , a worse applicant is never favored over a better one , despite a learning algorithm 's uncertainty over the true payoffs . we prove results of two types . first , in the important special case of the classic stochastic bandits problem ( i.e . , in which there are no contexts ) , we provide a provably fair algorithm based on `` chained '' confidence intervals , and provide a cumulative regret bound with a cubic dependence on the number of arms . we further show that any fair algorithm must have such a dependence . when combined with regret bounds for standard non-fair algorithms such as ucb , this proves a strong separation between fair and unfair learning , which extends to the general contextual case . in the general contextual case , we prove a tight connection between fairness and the kwik ( knows what it knows ) learning model : a kwik algorithm for a class of functions can be transformed into a provably fair contextual bandit algorithm , and conversely any fair contextual bandit algorithm can be transformed into a kwik learning algorithm . this tight connection allows us to provide a provably fair algorithm for the linear contextual bandit problem with a polynomial dependence on the dimension , and to show ( for a different class of functions ) a worst-case exponential gap in regret between fair and non-fair learning algorithms", "topics": ["time complexity", "polynomial"]}
{"title": "divide-and-conquer learning by anchoring a conical hull", "abstract": "we reduce a broad class of machine learning problems , usually addressed by em or sampling , to the problem of finding the $ k $ extremal rays spanning the conical hull of a data point set . these $ k $ `` anchors '' lead to a global solution and a more interpretable model that can even outperform em and sampling on generalization error . to find the $ k $ anchors , we propose a novel divide-and-conquer learning scheme `` dca '' that distributes the problem to $ \\mathcal o ( k\\log k ) $ same-type sub-problems on different low-d random hyperplanes , each can be solved by any solver . for the 2d sub-problem , we present a non-iterative solver that only needs to compute an array of cosine values and its max/min entries . dca also provides a faster subroutine for other methods to check whether a point is covered in a conical hull , which improves algorithm design in multiple dimensions and brings significant speedup to learning . we apply our method to gmm , hmm , lda , nmf and subspace clustering , then show its competitive performance and scalability over other methods on rich datasets .", "topics": ["sampling ( signal processing )", "cluster analysis"]}
{"title": "achieving proportional representation via voting", "abstract": "proportional representation ( pr ) is often discussed in voting settings as a major desideratum . for the past century or so , it is common both in practice and in the academic literature to jump to stv ( single transferable vote ) as the solution for achieving pr . some of the most prominent electoral reform movements around the globe are pushing for the adoption of stv . it has been termed a major open problem to design a voting rule that satisfies the same pr properties as stv and better monotonicity properties . we present a rule called ear ( expanding approvals rule ) that satisfies properties stronger than the central pr axiom satisfied by stv , can handle indifferences in a convenient and computationally efficient manner , and also satisfies better candidate monotonicity properties . in view of this , our proposed rule seems to be a compelling solution for achieving proportional representation in voting settings .", "topics": ["computational complexity theory"]}
{"title": "computer vision systems in road vehicles : a review", "abstract": "the number of road vehicles significantly increased in recent decades . this trend accompanied a build-up of road infrastructure and development of various control systems to increase road traffic safety , road capacity and travel comfort . in traffic safety significant development has been made and today 's systems more and more include cameras and computer vision methods . cameras are used as part of the road infrastructure or in vehicles . in this paper a review on computer vision systems in vehicles from the stand point of traffic engineering is given . safety problems of road vehicles are presented , current state of the art in-vehicle vision systems is described and open problems with future research directions are discussed .", "topics": ["computer vision"]}
{"title": "pignistic probability transforms for mixes of low- and high-probability events", "abstract": "in some real world information fusion situations , time critical decisions must be made with an incomplete information set . belief function theories ( e.g . , dempster-shafer theory of evidence , transferable belief model ) have been shown to provide a reasonable methodology for processing or fusing the quantitative clues or information measurements that form the incomplete information set . for decision making , the pignistic ( from the latin pignus , a bet ) probability transform has been shown to be a good method of using beliefs or basic belief assignments ( bbas ) to make decisions . for many systems , one need only address the most-probable elements in the set . for some critical systems , one must evaluate the risk of wrong decisions and establish safe probability thresholds for decision making . this adds a greater complexity to decision making , since one must address all elements in the set that are above the risk decision threshold . the problem is greatly simplified if most of the probabilities fall below this threshold . finding a probability transform that properly represents mixes of low- and high-probability events is essential . this article introduces four new pignistic probability transforms with an implementation that uses the latest values of beliefs , plausibilities , or bbas to improve the pignistic probability estimates . some of them assign smaller values of probabilities for smaller values of beliefs or bbas than the smets pignistic transform . they also assign higher probability values for larger values of beliefs or bbas than the smets pignistic transform . these probability transforms will assign a value of probability that converges faster to the values below the risk threshold . a probability information content ( pic ) variable is also introduced that assigns an information content value to any set of probability . four operators are defined to help simplify the derivations .", "topics": ["value ( ethics )"]}
{"title": "embedded all relevant feature selection with random ferns", "abstract": "many machine learning methods can produce variable importance scores expressing the usability of each feature in context of the produced model ; those scores on their own are yet not sufficient to generate feature selection , especially when an all relevant selection is required . although there are wrapper methods aiming to solve this problem , they introduce a substantial increase in the required computational effort . in this paper i investigate an idea of incorporating all relevant selection within the training process by producing importance for implicitly generated shadows , attributes irrelevant by design . i propose and evaluate such a method in context of random ferns classifier . experiment results confirm the effectiveness of such approach , although show that fully stochastic nature of random ferns limits its applicability either to small dimensions or as a part of a broader feature selection procedure .", "topics": ["statistical classification", "relevance"]}
{"title": "an image processing based object counting approach for machine vision application", "abstract": "machine vision applications are low cost and high precision measurement systems which are frequently used in production lines . with these systems that provide contactless control and measurement , production facilities are able to reach high production numbers without errors . machine vision operations such as product counting , error control , dimension measurement can be performed through a camera . in this paper , a machine vision application is proposed , which can perform object-independent product counting . the proposed approach is based on otsu thresholding and hough transformation and performs automatic counting independently of product type and color . basically one camera is used in the system . through this camera , an image of the products passing through a conveyor is taken and various image processing algorithms are applied to these images . in this approach using images obtained from a real experimental setup , a real-time machine vision application was installed . as a result of the experimental studies performed , it has been determined that the proposed approach gives fast , accurate and reliable results .", "topics": ["image processing"]}
{"title": "counterfactual estimation and optimization of click metrics for search engines", "abstract": "optimizing an interactive system against a predefined online metric is particularly challenging , when the metric is computed from user feedback such as clicks and payments . the key challenge is the counterfactual nature : in the case of web search , any change to a component of the search engine may result in a different search result page for the same query , but we normally can not infer reliably from search log how users would react to the new result page . consequently , it appears impossible to accurately estimate online metrics that depend on user feedback , unless the new engine is run to serve users and compared with a baseline in an a/b test . this approach , while valid and successful , is unfortunately expensive and time-consuming . in this paper , we propose to address this problem using causal inference techniques , under the contextual-bandit framework . this approach effectively allows one to run ( potentially infinitely ) many a/b tests offline from search log , making it possible to estimate and optimize online metrics quickly and inexpensively . focusing on an important component in a commercial search engine , we show how these ideas can be instantiated and applied , and obtain very promising results that suggest the wide applicability of these techniques .", "topics": ["baseline ( configuration management )", "causality"]}
{"title": "generative multiple-instance learning models for quantitative electromyography", "abstract": "we present a comprehensive study of the use of generative modeling approaches for multiple-instance learning ( mil ) problems . in mil a learner receives training instances grouped together into bags with labels for the bags only ( which might not be correct for the comprised instances ) . our work was motivated by the task of facilitating the diagnosis of neuromuscular disorders using sets of motor unit potential trains ( mupts ) detected within a muscle which can be cast as a mil problem . our approach leads to a state-of-the-art solution to the problem of muscle classification . by introducing and analyzing generative models for mil in a general framework and examining a variety of model structures and components , our work also serves as a methodological guide to modelling mil tasks . we evaluate our proposed methods both on mupt datasets and on the musk1 dataset , one of the most widely used benchmarks for mil .", "topics": ["generative model"]}
{"title": "limited-memory matrix adaptation for large scale black-box optimization", "abstract": "the covariance matrix adaptation evolution strategy ( cma-es ) is a popular method to deal with nonconvex and/or stochastic optimization problems when the gradient information is not available . being based on the cma-es , the recently proposed matrix adaptation evolution strategy ( ma-es ) provides a rather surprising result that the covariance matrix and all associated operations ( e.g . , potentially unstable eigendecomposition ) can be replaced in the cma-es by a updated transformation matrix without any loss of performance . in order to further simplify ma-es and reduce its $ \\mathcal { o } \\big ( n^2\\big ) $ time and storage complexity to $ \\mathcal { o } \\big ( n\\log ( n ) \\big ) $ , we present the limited-memory matrix adaptation evolution strategy ( lm-ma-es ) for efficient zeroth order large-scale optimization . the algorithm demonstrates state-of-the-art performance on a set of established large-scale benchmarks . we explore the algorithm on the problem of generating adversarial inputs for a ( non-smooth ) random forest classifier , demonstrating a surprising vulnerability of the classifier .", "topics": ["gradient descent", "gradient"]}
{"title": "approximate bayesian inference in linear state space models for intermittent demand forecasting at scale", "abstract": "we present a scalable and robust bayesian inference method for linear state space models . the method is applied to demand forecasting in the context of a large e-commerce platform , paying special attention to intermittent and bursty target statistics . inference is approximated by the newton-raphson algorithm , reduced to linear-time kalman smoothing , which allows us to operate on several orders of magnitude larger problems than previous related work . in a study on large real-world sales datasets , our method outperforms competing approaches on fast and medium moving items .", "topics": ["time complexity", "scalability"]}
{"title": "disentangling by factorising", "abstract": "we define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation . we propose factorvae , a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions . we show that it improves upon $ \\beta $ -vae by providing a better trade-off between disentanglement and reconstruction quality . moreover , we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them .", "topics": ["calculus of variations"]}
{"title": "continuum directions for supervised dimension reduction", "abstract": "dimension reduction of multivariate data supervised by auxiliary information is considered . a series of basis for dimension reduction is obtained as minimizers of a novel criterion . the proposed method is akin to continuum regression , and the resulting basis is called continuum directions . with a presence of binary supervision data , these directions continuously bridge the principal component , mean difference and linear discriminant directions , thus ranging from unsupervised to fully supervised dimension reduction . high-dimensional asymptotic studies of continuum directions for binary supervision reveal several interesting facts . the conditions under which the sample continuum directions are inconsistent , but their classification performance is good , are specified . while the proposed method can be directly used for binary and multi-category classification , its generalizations to incorporate any form of auxiliary data are also presented . the proposed method enjoys fast computation , and the performance is better or on par with more computer-intensive alternatives .", "topics": ["numerical analysis", "eisenstein 's criterion"]}
{"title": "improving variational encoder-decoders in dialogue generation", "abstract": "variational encoder-decoders ( veds ) have shown promising results in dialogue generation . however , the latent variable distributions are usually approximated by a much simpler model than the powerful rnn structure used for encoding and decoding , yielding the kl-vanishing problem and inconsistent training objective . in this paper , we separate the training step into two phases : the first phase learns to autoencode discrete texts into continuous embeddings , from which the second phase learns to generalize latent representations by reconstructing the encoded embedding . in this case , latent variables are sampled by transforming gaussian noise through multi-layer perceptrons and are trained with a separate ved model , which has the potential of realizing a much more flexible distribution . we compare our model with current popular models and the experiment demonstrates substantial improvement in both metric-based and human evaluations .", "topics": ["encoder"]}
{"title": "masked conditional neural networks for audio classification", "abstract": "we present the conditional neural network ( clnn ) and the masked conditional neural network ( mclnn ) designed for temporal signal recognition . the clnn takes into consideration the temporal nature of the sound signal and the mclnn extends upon the clnn through a binary mask to preserve the spatial locality of the features and allows an automated exploration of the features combination analogous to hand-crafting the most relevant features for the recognition task . mclnn has achieved competitive recognition accuracies on the gtzan and the ismir2004 music datasets that surpass several state-of-the-art neural network based architectures and hand-crafted methods applied on both datasets .", "topics": ["neural networks"]}
{"title": "testing identifiability of causal effects", "abstract": "this paper concerns the probabilistic evaluation of the effects of actions in the presence of unmeasured variables . we show that the identification of causal effect between a singleton variable x and a set of variables y can be accomplished systematically , in time polynomial in the number of variables in the graph . when the causal effect is identifiable , a closed-form expression can be obtained for the probability that the action will achieve a specified goal , or a set of goals .", "topics": ["causality", "polynomial"]}
{"title": "extracting urban impervious surface from gf-1 imagery using one-class classifiers", "abstract": "impervious surface area is a direct consequence of the urbanization , which also plays an important role in urban planning and environmental management . with the rapidly technical development of remote sensing , monitoring urban impervious surface via high spatial resolution ( hsr ) images has attracted unprecedented attention recently . traditional multi-classes models are inefficient for impervious surface extraction because it requires labeling all needed and unneeded classes that occur in the image exhaustively . therefore , we need to find a reliable one-class model to classify one specific land cover type without labeling other classes . in this study , we investigate several one-class classifiers , such as presence and background learning ( pbl ) , positive unlabeled learning ( pul ) , ocsvm , bsvm and maxent , to extract urban impervious surface area using high spatial resolution imagery of gf-1 , china 's new generation of high spatial remote sensing satellite , and evaluate the classification accuracy based on artificial interpretation results . compared to traditional multi-classes classifiers ( ann and svm ) , the experimental results indicate that pbl and pul provide higher classification accuracy , which is similar to the accuracy provided by ann model . meanwhile , pbl and pul outperforms ocsvm , bsvm , maxent and svm models . hence , the one-class classifiers only need a small set of specific samples to train models without losing predictive accuracy , which is supposed to gain more attention on urban impervious surface extraction or other one specific land cover type .", "topics": ["support vector machine"]}
{"title": "bayesian compressed regression", "abstract": "as an alternative to variable selection or shrinkage in high dimensional regression , we propose to randomly compress the predictors prior to analysis . this dramatically reduces storage and computational bottlenecks , performing well when the predictors can be projected to a low dimensional linear subspace with minimal loss of information about the response . as opposed to existing bayesian dimensionality reduction approaches , the exact posterior distribution conditional on the compressed data is available analytically , speeding up computation by many orders of magnitude while also bypassing robustness issues due to convergence and mixing problems with mcmc . model averaging is used to reduce sensitivity to the random projection matrix , while accommodating uncertainty in the subspace dimension . strong theoretical support is provided for the approach by showing near parametric convergence rates for the predictive density in the large p small n asymptotic paradigm . practical performance relative to competitors is illustrated in simulations and real data applications .", "topics": ["simulation", "computation"]}
{"title": "imitation networks : few-shot learning of neural networks from scratch", "abstract": "in this paper , we propose imitation networks , a simple but effective method for training neural networks with a limited amount of training data . our approach inherits the idea of knowledge distillation that transfers knowledge from a deep or wide reference model to a shallow or narrow target model . the proposed method employs this idea to mimic predictions of reference estimators that are much more robust against overfitting than the network we want to train . different from almost all the previous work for knowledge distillation that requires a large amount of labeled training data , the proposed method requires only a small amount of training data . instead , we introduce pseudo training examples that are optimized as a part of model parameters . experimental results for several benchmark datasets demonstrate that the proposed method outperformed all the other baselines , such as naive training of the target model and standard knowledge distillation .", "topics": ["test set"]}
{"title": "gradient descent can take exponential time to escape saddle points", "abstract": "although gradient descent ( gd ) almost always escapes saddle points asymptotically [ lee et al . , 2016 ] , this paper shows that even with fairly natural random initialization schemes and non-pathological functions , gd can be significantly slowed down by saddle points , taking exponential time to escape . on the other hand , gradient descent with perturbations [ ge et al . , 2015 , jin et al . , 2017 ] is not slowed down by saddle points - it can find an approximate local minimizer in polynomial time . this result implies that gd is inherently slower than perturbed gd , and justifies the importance of adding perturbations for efficient non-convex optimization . while our focus is theoretical , we also present experiments that illustrate our theoretical findings .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "texture measures combination for improved meningioma classification of histopathological images", "abstract": "providing an improved technique which can assist pathologists in correctly classifying meningioma tumours with a significant accuracy is our main objective . the proposed technique , which is based on optimum texture measure combination , inspects the separability of the rgb colour channels and selects the channel which best segments the cell nuclei of the histopathological images . the morphological gradient was applied to extract the region of interest for each subtype and for elimination of possible noise ( e.g . cracks ) which might occur during biopsy preparation . meningioma texture features are extracted by four different texture measures ( two model-based and two statistical-based ) and then corresponding features are fused together in different combinations after excluding highly correlated features , and a bayesian classifier was used for meningioma subtype discrimination . the combined gaussian markov random field and run-length matrix texture measures outperformed all other combinations in terms of quantitatively characterising the meningioma tissue , achieving an overall classification accuracy of 92.50 % , improving from 83.75 % which is the best accuracy achieved if the texture measures are used individually .", "topics": ["feature extraction", "bayesian network"]}
{"title": "commonsense knowledge enhanced embeddings for solving pronoun disambiguation problems in winograd schema challenge", "abstract": "in this paper , we propose commonsense knowledge enhanced embeddings ( kee ) for solving the pronoun disambiguation problems ( pdp ) . the pdp task we investigate in this paper is a complex coreference resolution task which requires the utilization of commonsense knowledge . this task is a standard first round test set in the 2016 winograd schema challenge . in this task , traditional linguistic features that are useful for coreference resolution , e.g . context and gender information , are no longer effective anymore . therefore , the kee models are proposed to provide a general framework to make use of commonsense knowledge for solving the pdp problems . since the pdp task does n't have training data , the kee models would be used during the unsupervised feature extraction process . to evaluate the effectiveness of the kee models , we propose to incorporate various commonsense knowledge bases , including conceptnet , wordnet , and causecom , into the kee training process . we achieved the best performance by applying the proposed methods to the 2016 winograd schema challenge . in addition , experiments conducted on the standard pdp task indicate that , the proposed kee models could solve the pdp problems by achieving 66.7 % accuracy , which is a new state-of-the-art performance .", "topics": ["test set", "feature extraction"]}
{"title": "focus of attention for linear predictors", "abstract": "we present a method to stop the evaluation of a prediction process when the result of the full evaluation is obvious . this trait is highly desirable in prediction tasks where a predictor evaluates all its features for every example in large datasets . we observe that some examples are easier to classify than others , a phenomenon which is characterized by the event when most of the features agree on the class of an example . by stopping the feature evaluation when encountering an easy- to-classify example , the predictor can achieve substantial gains in computation . our method provides a natural attention mechanism for linear predictors where the predictor concentrates most of its computation on hard-to-classify examples and quickly discards easy-to-classify ones . by modifying a linear prediction algorithm such as an svm or adaboost to include our attentive method we prove that the average number of features computed is o ( sqrt ( n log 1/sqrt ( delta ) ) ) where n is the original number of features , and delta is the error rate incurred due to early stopping . we demonstrate the effectiveness of attentive prediction on mnist , real-sim , gisette , and synthetic datasets .", "topics": ["synthetic data", "computation"]}
{"title": "a non-parametric bayesian method for inferring hidden causes", "abstract": "we present a non-parametric bayesian approach to structure learning with hidden causes . previous bayesian treatments of this problem define a prior over the number of hidden causes and use algorithms such as reversible jump markov chain monte carlo to move between solutions . in contrast , we assume that the number of hidden causes is unbounded , but only a finite number influence observable variables . this makes it possible to use a gibbs sampler to approximate the distribution over causal structures . we evaluate the performance of both approaches in discovering hidden causes in simulated data , and use our non-parametric approach to discover hidden causes in a real medical dataset .", "topics": ["sampling ( signal processing )", "simulation"]}
{"title": "polynomial learning of distribution families", "abstract": "the question of polynomial learnability of probability distributions , particularly gaussian mixture distributions , has recently received significant attention in theoretical computer science and machine learning . however , despite major progress , the general question of polynomial learnability of gaussian mixture distributions still remained open . the current work resolves the question of polynomial learnability for gaussian mixtures in high dimension with an arbitrary fixed number of components . the result on learning gaussian mixtures relies on an analysis of distributions belonging to what we call `` polynomial families '' in low dimension . these families are characterized by their moments being polynomial in parameters and include almost all common probability distributions as well as their mixtures and products . using tools from real algebraic geometry , we show that parameters of any distribution belonging to such a family can be learned in polynomial time and using a polynomial number of sample points . the result on learning polynomial families is quite general and is of independent interest . to estimate parameters of a gaussian mixture distribution in high dimensions , we provide a deterministic algorithm for dimensionality reduction . this allows us to reduce learning a high-dimensional mixture to a polynomial number of parameter estimations in low dimension . combining this reduction with the results on polynomial families yields our result on learning arbitrary gaussian mixtures in high dimensions .", "topics": ["time complexity", "polynomial"]}
{"title": "introduction to the monogenic signal", "abstract": "the monogenic signal is an image analysis methodology that was introduced by felsberg and sommer in 2001 and has been employed for a variety of purposes in image processing and computer vision research . in particular , it has been found to be useful in the analysis of ultrasound imagery in several research scenarios mostly in work done within the biomedia lab at oxford . however , the literature on the monogenic signal can be difficult to penetrate due to the lack of a single resource to explain the various principles from basics . the purpose of this document is therefore to introduce the principles , purpose , applications , and limitations of the methodology . it assumes some background knowledge from the fields of image and signal processing , in particular a good knowledge of fourier transforms as applied to signals and images . we will not attempt to provide a thorough math- ematical description or derivation of the monogenic signal , but rather focus on developing an intuition for understanding and using the methodology and refer the reader elsewhere for a more mathematical treatment .", "topics": ["image processing", "computer vision"]}
{"title": "regularized orthogonal tensor decompositions for multi-relational learning", "abstract": "multi-relational learning has received lots of attention from researchers in various research communities . most existing methods either suffer from superlinear per-iteration cost , or are sensitive to the given ranks . to address both issues , we propose a scalable core tensor trace norm regularized orthogonal iteration decomposition ( roid ) method for full or incomplete tensor analytics , which can be generalized as a graph laplacian regularized version by using auxiliary information or a sparse higher-order orthogonal iteration ( shooi ) version . we first induce the equivalence relation of the schatten p-norm ( 0 < p < \\infty ) of a low multi-linear rank tensor and its core tensor . then we achieve a much smaller matrix trace norm minimization problem . finally , we develop two efficient augmented lagrange multiplier algorithms to solve our problems with convergence guarantees . extensive experiments using both real and synthetic datasets , even though with only a few observations , verified both the efficiency and effectiveness of our methods .", "topics": ["synthetic data", "sparse matrix"]}
{"title": "swarming around shellfish larvae", "abstract": "the collection of wild larvae seed as a source of raw material is a major sub industry of shellfish aquaculture . to predict when , where and in what quantities wild seed will be available , it is necessary to track the appearance and growth of planktonic larvae . one of the most difficult groups to identify , particularly at the species level are the bivalvia . this difficulty arises from the fact that fundamentally all bivalve larvae have a similar shape and colour . identification based on gross morphological appearance is limited by the time-consuming nature of the microscopic examination and by the limited availability of expertise in this field . molecular and immunological methods are also being studied . we describe the application of computational pattern recognition methods to the automated identification and size analysis of scallop larvae . for identification , the shape features used are binary invariant moments ; that is , the features are invariant to shift ( position within the image ) , scale ( induced either by growth or differential image magnification ) and rotation . images of a sample of scallop and non-scallop larvae covering a range of maturities have been analysed . in order to overcome the automatic identification , as well as to allow the system to receive new unknown samples at any moment , a self-organized and unsupervised ant-like clustering algorithm based on swarm intelligence is proposed , followed by simple k-nnr nearest neighbour classification on the final map . results achieve a full recognition rate of 100 % under several situations ( k =1 or 3 ) .", "topics": ["cluster analysis", "unsupervised learning"]}
{"title": "effective bayesian modeling of groups of related count time series", "abstract": "time series of counts arise in a variety of forecasting applications , for which traditional models are generally inappropriate . this paper introduces a hierarchical bayesian formulation applicable to count time series that can easily account for explanatory variables and share statistical strength across groups of related time series . we derive an efficient approximate inference technique , and illustrate its performance on a number of datasets from supply chain planning .", "topics": ["time series", "approximation algorithm"]}
{"title": "single-image superresolution through directional representations", "abstract": "we develop a mathematically-motivated algorithm for image superresolution , based on the discrete shearlet transform . the shearlet transform is strongly directional , and is known to provide near-optimally sparse representations for a broad class of images . this often leads to superior performance in edge detection and image representation , when compared to other isotropic frames . we justify the use of shearlet frames for superresolution mathematically before presenting a superresolution algorithm that combines the shearlet transform with the sparse mixing estimators ( sme ) approach pioneered by mallat and yu . our algorithm is compared with an isotropic superresolution method , a previous prototype of a shearlet superresolution algorithm , and sme superresolution with a discrete wavelet frame . our numerical results on a variety of image types show strong performance in terms of psnr .", "topics": ["numerical analysis", "sparse matrix"]}
{"title": "learning bayesian network equivalence classes with ant colony optimization", "abstract": "bayesian networks are a useful tool in the representation of uncertain knowledge . this paper proposes a new algorithm called aco-e , to learn the structure of a bayesian network . it does this by conducting a search through the space of equivalence classes of bayesian networks using ant colony optimization ( aco ) . to this end , two novel extensions of traditional aco techniques are proposed and implemented . firstly , multiple types of moves are allowed . secondly , moves can be given in terms of indices that are not based on construction graph nodes . the results of testing show that aco-e performs better than a greedy search and other state-of-the-art and metaheuristic algorithms whilst searching in the space of equivalence classes .", "topics": ["bayesian network"]}
{"title": "the computational complexity of sensitivity analysis and parameter tuning", "abstract": "while known algorithms for sensitivity analysis and parameter tuning in probabilistic networks have a running time that is exponential in the size of the network , the exact computational complexity of these problems has not been established as yet . in this paper we study several variants of the tuning problem and show that these problems are nppp-complete in general . we further show that the problems remain np-complete or pp-complete , for a number of restricted variants . these complexity results provide insight in whether or not recent achievements in sensitivity analysis and tuning can be extended to more general , practicable methods .", "topics": ["computational complexity theory", "time complexity"]}
{"title": "anatomical data augmentation for cnn based pixel-wise classification", "abstract": "in this work we propose a method for anatomical data augmentation that is based on using slices of computed tomography ( ct ) examinations that are adjacent to labeled slices as another resource of labeled data for training the network . the extended labeled data is used to train a u-net network for a pixel-wise classification into different hepatic lesions and normal liver tissues . our dataset contains ct examinations from 140 patients with 333 ct images annotated by an expert radiologist . we tested our approach and compared it to the conventional training process . results indicate superiority of our method . using the anatomical data augmentation we achieved an improvement of 3 % in the success rate , 5 % in the classification accuracy , and 4 % in dice .", "topics": ["pixel"]}
{"title": "sparse component analysis ( sca ) in random-valued and salt and pepper noise removal", "abstract": "in this paper , we propose a new method for impulse noise removal from images . it uses the sparsity of images in the discrete cosine transform ( dct ) domain . the zeros in this domain give us the exact mathematical equation to reconstruct the pixels that are corrupted by random-value impulse noises . the proposed method can also detect and correct the corrupted pixels . moreover , in a simpler case that salt and pepper noise is the brightest and darkest pixels in the image , we propose a simpler version of our method . in addition to the proposed method , we suggest a combination of the traditional median filter method with our method to yield better results when the percentage of the corrupted samples is high .", "topics": ["sparse matrix", "pixel"]}
{"title": "why do deep neural networks still not recognize these images ? : a qualitative analysis on failure cases of imagenet classification", "abstract": "in a recent decade , imagenet has become the most notable and powerful benchmark database in computer vision and machine learning community . as imagenet has emerged as a representative benchmark for evaluating the performance of novel deep learning models , its evaluation tends to include only quantitative measures such as error rate , rather than qualitative analysis . thus , there are few studies that analyze the failure cases of deep learning models in imagenet , though there are numerous works analyzing the networks themselves and visualizing them . in this abstract , we qualitatively analyze the failure cases of imagenet classification results from recent deep learning model , and categorize these cases according to the certain image patterns . through this failure analysis , we believe that it can be discovered what the final challenges are in imagenet database , which the current deep learning model is still vulnerable to .", "topics": ["neural networks", "computer vision"]}
{"title": "differentially private dropout", "abstract": "large data collections required for the training of neural networks often contain sensitive information such as the medical histories of patients , and the privacy of the training data must be preserved . in this paper , we introduce a dropout technique that provides an elegant bayesian interpretation to dropout , and show that the intrinsic noise added , with the primary goal of regularization , can be exploited to obtain a degree of differential privacy . the iterative nature of training neural networks presents a challenge for privacy-preserving estimation since multiple iterations increase the amount of noise added . we overcome this by using a relaxed notion of differential privacy , called concentrated differential privacy , which provides tighter estimates on the overall privacy loss . we demonstrate the accuracy of our privacy-preserving dropout algorithm on benchmark datasets .", "topics": ["test set", "matrix regularization"]}
{"title": "homotopy parametric simplex method for sparse learning", "abstract": "high dimensional sparse learning has imposed a great computational challenge to large scale data analysis . in this paper , we are interested in a broad class of sparse learning approaches formulated as linear programs parametrized by a { \\em regularization factor } , and solve them by the parametric simplex method ( psm ) . our parametric simplex method offers significant advantages over other competing methods : ( 1 ) psm naturally obtains the complete solution path for all values of the regularization parameter ; ( 2 ) psm provides a high precision dual certificate stopping criterion ; ( 3 ) psm yields sparse solutions through very few iterations , and the solution sparsity significantly reduces the computational cost per iteration . particularly , we demonstrate the superiority of psm over various sparse learning approaches , including dantzig selector for sparse linear regression , lad-lasso for sparse robust linear regression , clime for sparse precision matrix estimation , sparse differential network estimation , and sparse linear programming discriminant ( lpd ) analysis . we then provide sufficient conditions under which psm always outputs sparse solutions such that its computational performance can be significantly boosted . thorough numerical experiments are provided to demonstrate the outstanding performance of the psm method .", "topics": ["numerical analysis", "matrix regularization"]}
{"title": "contour detection using cost-sensitive convolutional neural networks", "abstract": "we address the problem of contour detection via per-pixel classifications of edge point . to facilitate the process , the proposed approach leverages with densenet , an efficient implementation of multiscale convolutional neural networks ( cnns ) , to extract an informative feature vector for each pixel and uses an svm classifier to accomplish contour detection . the main challenge lies in adapting a pre-trained per-image cnn model for yielding per-pixel image features . we propose to base on the densenet architecture to achieve pixelwise fine-tuning and then consider a cost-sensitive strategy to further improve the learning with a small dataset of edge and non-edge image patches . in the experiment of contour detection , we look into the effectiveness of combining per-pixel features from different cnn layers and obtain comparable performances to the state-of-the-art on bsds500 .", "topics": ["feature vector", "pixel"]}
{"title": "some apparently disjoint aims and requirements for grammar development environments : the case of natural language generation", "abstract": "grammar development environments ( gde 's ) for analysis and for generation have not yet come together . despite the fact that analysis-oriented gde 's ( such as alep ) may include some possibility of sentence generation , the development techniques and kinds of resources suggested are apparently not those required for practical , large-scale natural language generation work . indeed , there is no use of `standard ' ( i.e . , analysis-oriented ) gde 's in current projects/applications targetting the generation of fluent , coherent texts . this unsatisfactory situation requires some analysis and explanation , which this paper attempts using as an example an extensive gde for generation . the support provided for distributed large-scale grammar development , multilinguality , and resource maintenance are discussed and contrasted with analysis-oriented approaches .", "topics": ["natural language processing", "natural language"]}
{"title": "a cross entropy based optimization algorithm with global convergence guarantees", "abstract": "the cross entropy ( ce ) method is a model based search method to solve optimization problems where the objective function has minimal structure . the monte-carlo version of the ce method employs the naive sample averaging technique which is inefficient , both computationally and space wise . we provide a novel stochastic approximation version of the ce method , where the sample averaging is replaced with incremental geometric averaging . this approach can save considerable computational and storage costs . our algorithm is incremental in nature and possesses additional attractive features such as accuracy , stability , robustness and convergence to the global optimum for a particular class of objective functions . we evaluate the algorithm on a variety of global optimization benchmark problems and the results obtained corroborate our theoretical findings .", "topics": ["optimization problem", "mathematical optimization"]}
{"title": "real-time hand tracking using a sum of anisotropic gaussians model", "abstract": "real-time marker-less hand tracking is of increasing importance in human-computer interaction . robust and accurate tracking of arbitrary hand motion is a challenging problem due to the many degrees of freedom , frequent self-occlusions , fast motions , and uniform skin color . in this paper , we propose a new approach that tracks the full skeleton motion of the hand from multiple rgb cameras in real-time . the main contributions include a new generative tracking method which employs an implicit hand shape representation based on sum of anisotropic gaussians ( sag ) , and a pose fitting energy that is smooth and analytically differentiable making fast gradient based pose optimization possible . this shape representation , together with a full perspective projection model , enables more accurate hand modeling than a related baseline method from literature . our method achieves better accuracy than previous methods and runs at 25 fps . we show these improvements both qualitatively and quantitatively on publicly available datasets .", "topics": ["baseline ( configuration management )", "gradient"]}
{"title": "dash : dynamic approach for switching heuristics", "abstract": "complete tree search is a highly effective method for tackling mip problems , and over the years , a plethora of branching heuristics have been introduced to further refine the technique for varying problems . recently , portfolio algorithms have taken the process a step further , trying to predict the best heuristic for each instance at hand . however , the motivation behind algorithm selection can be taken further still , and used to dynamically choose the most appropriate algorithm for each encountered subproblem . in this paper we identify a feature space that captures both the evolution of the problem in the branching tree and the similarity among subproblems of instances from the same mip models . we show how to exploit these features to decide the best time to switch the branching heuristic and then show how such a system can be trained efficiently . experiments on a highly heterogeneous collection of mip instances show significant gains over the pure algorithm selection approach that for a given instance uses only a single heuristic throughout the search .", "topics": ["optimization problem", "value ( ethics )"]}
{"title": "semi-supervised qa with generative domain-adaptive nets", "abstract": "we study the problem of semi-supervised question answering -- -- utilizing unlabeled text to boost the performance of question answering models . we propose a novel training framework , the generative domain-adaptive nets . in this framework , we train a generative model to generate questions based on the unlabeled text , and combine model-generated questions with human-generated questions for training question answering models . we develop novel domain adaptation algorithms , based on reinforcement learning , to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution . experiments show that our proposed framework obtains substantial improvement from unlabeled text .", "topics": ["generative model", "reinforcement learning"]}
{"title": "machine comprehension using match-lstm and answer pointer", "abstract": "machine comprehension of text is an important problem in natural language processing . a recently released dataset , the stanford question answering dataset ( squad ) , offers a large number of real questions and their answers created by humans through crowdsourcing . squad provides a challenging testbed for evaluating machine comprehension algorithms , partly because compared with previous datasets , in squad the answers do not come from a small set of candidate answers and they have variable lengths . we propose an end-to-end neural architecture for the task . the architecture is based on match-lstm , a model we proposed previously for textual entailment , and pointer net , a sequence-to-sequence model proposed by vinyals et al . ( 2015 ) to constrain the output tokens to be from the input sequences . we propose two ways of using pointer net for our task . our experiments show that both of our two models substantially outperform the best results obtained by rajpurkar et al . ( 2016 ) using logistic regression and manually crafted features .", "topics": ["natural language processing", "end-to-end principle"]}
{"title": "a reactive tabu search algorithm for stimuli generation in psycholinguistics", "abstract": "the generation of meaningless `` words '' matching certain statistical and/or linguistic criteria is frequently needed for experimental purposes in psycholinguistics . such stimuli receive the name of pseudowords or nonwords in the cognitive neuroscience literatue . the process for building nonwords sometimes has to be based on linguistic units such as syllables or morphemes , resulting in a numerical explosion of combinations when the size of the nonwords is increased . in this paper , a reactive tabu search scheme is proposed to generate nonwords of variables size . the approach builds pseudowords by using a modified metaheuristic algorithm based on a local search procedure enhanced by a feedback-based scheme . experimental results show that the new algorithm is a practical and effective tool for nonword generation .", "topics": ["numerical analysis"]}
{"title": "understanding retail productivity by simulating management practise", "abstract": "intelligent agents offer a new and exciting way of understanding the world of work . in this paper we apply agent-based modeling and simulation to investigate a set of problems in a retail context . specifically , we are working to understand the relationship between human resource management practices and retail productivity . despite the fact we are working within a relatively novel and complex domain , it is clear that intelligent agents could offer potential for fostering sustainable organizational capabilities in the future . our research so far has led us to conduct case study work with a top ten uk retailer , collecting data in four departments in two stores . based on our case study data we have built and tested a first version of a department store simulator . in this paper we will report on the current development of our simulator which includes new features concerning more realistic data on the pattern of footfall during the day and the week , a more differentiated view of customers , and the evolution of customers over time . this allows us to investigate more complex scenarios and to analyze the impact of various management practices .", "topics": ["simulation"]}
{"title": "database of parliamentary speeches in ireland , 1919-2013", "abstract": "we present a database of parliamentary debates that contains the complete record of parliamentary speeches from d\\'ail \\'eireann , the lower house and principal chamber of the irish parliament , from 1919 to 2013 . in addition , the database contains background information on all tds ( teachta d\\'ala , members of parliament ) , such as their party affiliations , constituencies and office positions . the current version of the database includes close to 4.5 million speeches from 1,178 tds . the speeches were downloaded from the official parliament website and further processed and parsed with a python script . background information on tds was collected from the member database of the parliament website . data on cabinet positions ( ministers and junior ministers ) was collected from the official website of the government . a record linkage algorithm and human coders were used to match tds and ministers .", "topics": ["parsing", "database"]}
{"title": "recurrent ladder networks", "abstract": "we propose a recurrent extension of the ladder networks whose structure is motivated by the inference required in hierarchical latent variable models . we demonstrate that the recurrent ladder is able to handle a wide variety of complex learning tasks that benefit from iterative inference and temporal modeling . the architecture shows close-to-optimal results on temporal modeling of video data , competitive results on music modeling , and improved perceptual grouping based on higher order abstractions , such as stochastic textures and motion cues . we present results for fully supervised , semi-supervised , and unsupervised tasks . the results suggest that the proposed architecture and principles are powerful tools for learning a hierarchy of abstractions , learning iterative inference and handling temporal information .", "topics": ["unsupervised learning"]}
{"title": "modeling the dynamics of online learning activity", "abstract": "people are increasingly relying on the web and social media to find solutions to their problems in a wide range of domains . in this online setting , closely related problems often lead to the same characteristic learning pattern , in which people sharing these problems visit related pieces of information , perform almost identical queries or , more generally , take a series of similar actions . in this paper , we introduce a novel modeling framework for clustering continuous-time grouped streaming data , the hierarchical dirichlet hawkes process ( hdhp ) , which allows us to automatically uncover a wide variety of learning patterns from detailed traces of learning activity . our model allows for efficient inference , scaling to millions of actions taken by thousands of users . experiments on real data gathered from stack overflow reveal that our framework can recover meaningful learning patterns in terms of both content and temporal dynamics , as well as accurately track users ' interests and goals over time .", "topics": ["cluster analysis", "scalability"]}
{"title": "facial expression recognition based on complexity perception classification algorithm", "abstract": "facial expression recognition ( fer ) has always been a challenging issue in computer vision . the different expressions of emotion and uncontrolled environmental factors lead to inconsistencies in the complexity of fer and variability of between expression categories , which is often overlooked in most facial expression recognition systems . in order to solve this problem effectively , we presented a simple and efficient cnn model to extract facial features , and proposed a complexity perception classification ( cpc ) algorithm for fer . the cpc algorithm divided the dataset into an easy classification sample subspace and a complex classification sample subspace by evaluating the complexity of facial features that are suitable for classification . the experimental results of our proposed algorithm on fer2013 and ck-plus datasets demonstrated the algorithm 's effectiveness and superiority over other state-of-the-art approaches .", "topics": ["computer vision"]}
{"title": "personalized donor-recipient matching for organ transplantation", "abstract": "organ transplants can improve the life expectancy and quality of life for the recipient but carries the risk of serious post-operative complications , such as septic shock and organ rejection . the probability of a successful transplant depends in a very subtle fashion on compatibility between the donor and the recipient but current medical practice is short of domain knowledge regarding the complex nature of recipient-donor compatibility . hence a data-driven approach for learning compatibility has the potential for significant improvements in match quality . this paper proposes a novel system ( confidentmatch ) that is trained using data from electronic health records . confidentmatch predicts the success of an organ transplant ( in terms of the 3 year survival rates ) on the basis of clinical and demographic traits of the donor and recipient . confidentmatch captures the heterogeneity of the donor and recipient traits by optimally dividing the feature space into clusters and constructing different optimal predictive models to each cluster . the system controls the complexity of the learned predictive model in a way that allows for assuring more granular and confident predictions for a larger number of potential recipient-donor pairs , thereby ensuring that predictions are `` personalized '' and tailored to individual characteristics to the finest possible granularity . experiments conducted on the unos heart transplant dataset show the superiority of the prognostic value of confidentmatch to other competing benchmarks ; confidentmatch can provide predictions of success with 95 % confidence for 5,489 patients of a total population of 9,620 patients , which corresponds to 410 more patients than the most competitive benchmark algorithm ( deepboost ) .", "topics": ["feature vector"]}
{"title": "distributed regression in sensor networks : training distributively with alternating projections", "abstract": "wireless sensor networks ( wsns ) have attracted considerable attention in recent years and motivate a host of new challenges for distributed signal processing . the problem of distributed or decentralized estimation has often been considered in the context of parametric models . however , the success of parametric methods is limited by the appropriateness of the strong statistical assumptions made by the models . in this paper , a more flexible nonparametric model for distributed regression is considered that is applicable in a variety of wsn applications including field estimation . here , starting with the standard regularized kernel least-squares estimator , a message-passing algorithm for distributed estimation in wsns is derived . the algorithm can be viewed as an instantiation of the successive orthogonal projection ( sop ) algorithm . various practical aspects of the algorithm are discussed and several numerical simulations validate the potential of the approach .", "topics": ["numerical analysis", "simulation"]}
{"title": "recycling lingware in a multilingual mt system", "abstract": "we describe two methods relevant to multi-lingual machine translation systems , which can be used to port linguistic data ( grammars , lexicons and transfer rules ) between systems used for processing related languages . the methods are fully implemented within the spoken language translator system , and were used to create versions of the system for two new language pairs using only a month of expert effort .", "topics": ["machine translation"]}
{"title": "an information theory for preferences", "abstract": "recent literature in the last maximum entropy workshop introduced an analogy between cumulative probability distributions and normalized utility functions . based on this analogy , a utility density function can de defined as the derivative of a normalized utility function . a utility density function is non-negative and integrates to unity . these two properties form the basis of a correspondence between utility and probability . a natural application of this analogy is a maximum entropy principle to assign maximum entropy utility values . maximum entropy utility interprets many of the common utility functions based on the preference information needed for their assignment , and helps assign utility values based on partial preference information . this paper reviews maximum entropy utility and introduces further results that stem from the duality between probability and utility .", "topics": ["value ( ethics )"]}
{"title": "bayesian networks for dependability analysis : an application to digital control reliability", "abstract": "bayesian networks ( bn ) provide robust probabilistic methods of reasoning under uncertainty , but despite their formal grounds are strictly based on the notion of conditional dependence , not much attention has been paid so far to their use in dependability analysis . the aim of this paper is to propose bn as a suitable tool for dependability analysis , by challenging the formalism with basic issues arising in dependability tasks . we will discuss how both modeling and analysis issues can be naturally dealt with by bn . moreover , we will show how some limitations intrinsic to combinatorial dependability methods such as fault trees can be overcome using bn . this will be pursued through the study of a real-world example concerning the reliability analysis of a redundant digital programmable logic controller ( plc ) with majority voting 2:3", "topics": ["bayesian network"]}
{"title": "inferring graphs from cascades : a sparse recovery framework", "abstract": "in the network inference problem , one seeks to recover the edges of an unknown graph from the observations of cascades propagating over this graph . in this paper , we approach this problem from the sparse recovery perspective . we introduce a general model of cascades , including the voter model and the independent cascade model , for which we provide the first algorithm which recovers the graph 's edges with high probability and $ o ( s\\log m ) $ measurements where $ s $ is the maximum degree of the graph and $ m $ is the number of nodes . furthermore , we show that our algorithm also recovers the edge weights ( the parameters of the diffusion process ) and is robust in the context of approximate sparsity . finally we prove an almost matching lower bound of $ \\omega ( s\\log\\frac { m } { s } ) $ and validate our approach empirically on synthetic graphs .", "topics": ["approximation algorithm", "synthetic data"]}
{"title": "proje : embedding projection for knowledge graph completion", "abstract": "with the large volume of new information created every day , determining the validity of information in a knowledge graph and filling in its missing parts are crucial tasks for many researchers and practitioners . to address this challenge , a number of knowledge graph completion methods have been developed using low-dimensional graph embeddings . although researchers continue to improve these models using an increasingly complex feature space , we show that simple changes in the architecture of the underlying model can outperform state-of-the-art models without the need for complex feature engineering . in this work , we present a shared variable neural network model called proje that fills-in missing information in a knowledge graph by learning joint embeddings of the knowledge graph 's entities and edges , and through subtle , but important , changes to the standard loss function . in doing so , proje has a parameter size that is smaller than 11 out of 15 existing methods while performing $ 37\\ % $ better than the current-best method on standard datasets . we also show , via a new fact checking task , that proje is capable of accurately determining the veracity of many declarative statements .", "topics": ["data mining", "feature vector"]}
{"title": "functional principal component analysis and randomized sparse clustering algorithm for medical image analysis", "abstract": "due to advances in sensors , growing large and complex medical image data have the ability to visualize the pathological change in the cellular or even the molecular level or anatomical changes in tissues and organs . as a consequence , the medical images have the potential to enhance diagnosis of disease , prediction of clinical outcomes , characterization of disease progression , management of health care and development of treatments , but also pose great methodological and computational challenges for representation and selection of features in image cluster analysis . to address these challenges , we first extend one dimensional functional principal component analysis to the two dimensional functional principle component analyses ( 2dfpca ) to fully capture space variation of image signals . image signals contain a large number of redundant and irrelevant features which provide no additional or no useful information for cluster analysis . widely used methods for removing redundant and irrelevant features are sparse clustering algorithms using a lasso-type penalty to select the features . however , the accuracy of clustering using a lasso-type penalty depends on how to select penalty parameters and a threshold for selecting features . in practice , they are difficult to determine . recently , randomized algorithms have received a great deal of attention in big data analysis . this paper presents a randomized algorithm for accurate feature selection in image cluster analysis . the proposed method is applied to ovarian and kidney cancer histology image data from the tcga database . the results demonstrate that the randomized feature selection method coupled with functional principal component analysis substantially outperforms the current sparse clustering algorithms in image cluster analysis .", "topics": ["cluster analysis", "sparse matrix"]}
{"title": "the improvement of negative sentences translation in english-to-korean machine translation", "abstract": "this paper describes the algorithm for translating english negative sentences into korean in english-korean machine translation ( ekmt ) . the proposed algorithm is based on the comparative study of english and korean negative sentences . the earlier translation software can not translate english negative sentences into accurate korean equivalents . we established a new algorithm for the negative sentence translation and evaluated it .", "topics": ["machine translation"]}
{"title": "probabilistic planning by probabilistic programming", "abstract": "automated planning is a major topic of research in artificial intelligence , and enjoys a long and distinguished history . the classical paradigm assumes a distinguished initial state , comprised of a set of facts , and is defined over a set of actions which change that state in one way or another . planning in many real-world settings , however , is much more involved : an agent 's knowledge is almost never simply a set of facts that are true , and actions that the agent intends to execute never operate the way they are supposed to . thus , probabilistic planning attempts to incorporate stochastic models directly into the planning process . in this article , we briefly report on probabilistic planning through the lens of probabilistic programming : a programming paradigm that aims to ease the specification of structured probability distributions . in particular , we provide an overview of the features of two systems , hype and allegro , which emphasise different strengths of probabilistic programming that are particularly useful for complex modelling issues raised in probabilistic planning . among other things , with these systems , one can instantiate planning problems with growing and shrinking state spaces , discrete and continuous probability distributions , and non-unique prior distributions in a first-order setting .", "topics": ["artificial intelligence"]}
{"title": "fuzzy clustering based segmentation of vertebrae in t1-weighted spinal mr images", "abstract": "image segmentation in the medical domain is a challenging field owing to poor resolution and limited contrast . the predominantly used conventional segmentation techniques and the thresholding methods suffer from limitations because of heavy dependence on user interactions . uncertainties prevalent in an image can not be captured by these techniques . the performance further deteriorates when the images are corrupted by noise , outliers and other artifacts . the objective of this paper is to develop an effective robust fuzzy c- means clustering for segmenting vertebral body from magnetic resonance image owing to its unsupervised form of learning . the motivation for this work is detection of spine geometry and proper localisation and labelling will enhance the diagnostic output of a physician . the method is compared with otsu thresholding and k-means clustering to illustrate the robustness.the reference standard for validation was the annotated images from the radiologist , and the dice coefficient and hausdorff distance measures were used to evaluate the segmentation .", "topics": ["image segmentation", "cluster analysis"]}
{"title": "learning optimal parameters for multi-target tracking with contextual interactions", "abstract": "we describe an end-to-end framework for learning parameters of min-cost flow multi-target tracking problem with quadratic trajectory interactions including suppression of overlapping tracks and contextual cues about cooccurrence of different objects . our approach utilizes structured prediction with a tracking-specific loss function to learn the complete set of model parameters . in this learning framework , we evaluate two different approaches to finding an optimal set of tracks under a quadratic model objective , one based on an lp relaxation and the other based on novel greedy variants of dynamic programming that handle pairwise interactions . we find the greedy algorithms achieve almost equivalent accuracy to the lp relaxation while being up to 10x faster than a commercial lp solver . we evaluate trained models on three challenging benchmarks . surprisingly , we find that with proper parameter learning , our simple data association model without explicit appearance/motion reasoning is able to achieve comparable or better accuracy than many state-of-the-art methods that use far more complex motion features or appearance affinity metric learning .", "topics": ["loss function", "interaction"]}
{"title": "towards automating the generation of derivative nouns in sanskrit by simulating panini", "abstract": "about 1115 rules in astadhyayi from a.4.1.76 to a.5.4.160 deal with generation of derivative nouns , making it one of the largest topical sections in astadhyayi , called as the taddhita section owing to the head rule a.4.1.76 . this section is a systematic arrangement of rules that enumerates various affixes that are used in the derivation under specific semantic relations . we propose a system that automates the process of generation of derivative nouns as per the rules in astadhyayi . the proposed system follows a completely object oriented approach , that models each rule as a class of its own and then groups them as rule groups . the rule groups are decided on the basis of selective grouping of rules by virtue of anuvrtti . the grouping of rules results in an inheritance network of rules which is a directed acyclic graph . every rule group has a head rule and the head rule notifies all the direct member rules of the group about the environment which contains all the details about data entities , participating in the derivation process . the system implements this mechanism using multilevel inheritance and observer design patterns . the system focuses not only on generation of the desired final form , but also on the correctness of sequence of rules applied to make sure that the derivation has taken place in strict adherence to astadhyayi . the proposed system 's design allows to incorporate various conflict resolution methods mentioned in authentic texts and hence the effectiveness of those rules can be validated with the results from the system . we also present cases where we have checked the applicability of the system with the rules which are not specifically applicable to derivation of derivative nouns , in order to see the effectiveness of the proposed schema as a generic system for modeling astadhyayi .", "topics": ["entity"]}
{"title": "a multiagent urban traffic simulation . part ii : dealing with the extraordinary", "abstract": "in probabilistic risk management , risk is characterized by two quantities : the magnitude ( or severity ) of the adverse consequences that can potentially result from the given activity or action , and by the likelihood of occurrence of the given adverse consequences . but a risk seldom exists in isolation : chain of consequences must be examined , as the outcome of one risk can increase the likelihood of other risks . systemic theory must complement classic prm . indeed these chains are composed of many different elements , all of which may have a critical importance at many different levels . furthermore , when urban catastrophes are envisioned , space and time constraints are key determinants of the workings and dynamics of these chains of catastrophes : models must include a correct spatial topology of the studied risk . finally , literature insists on the importance small events can have on the risk on a greater scale : urban risks management models belong to self-organized criticality theory . we chose multiagent systems to incorporate this property in our model : the behavior of an agent can transform the dynamics of important groups of them .", "topics": ["simulation"]}
{"title": "generating a diverse set of high-quality clusterings", "abstract": "we provide a new framework for generating multiple good quality partitions ( clusterings ) of a single data set . our approach decomposes this problem into two components , generating many high-quality partitions , and then grouping these partitions to obtain k representatives . the decomposition makes the approach extremely modular and allows us to optimize various criteria that control the choice of representative partitions .", "topics": ["cluster analysis"]}
{"title": "hardware/software co-design for spike based recognition", "abstract": "the practical applications based on recurrent spiking neurons are limited due to their non-trivial learning algorithms . the temporal nature of spiking neurons is more favorable for hardware implementation where signals can be represented in binary form and communication can be done through the use of spikes . this work investigates the potential of recurrent spiking neurons implementations on reconfigurable platforms and their applicability in temporal based applications . a theoretical framework of reservoir computing is investigated for hardware/software implementation . in this framework , only readout neurons are trained which overcomes the burden of training at the network level . these recurrent neural networks are termed as microcircuits which are viewed as basic computational units in cortical computation . this paper investigates the potential of recurrent neural reservoirs and presents a novel hardware/software strategy for their implementation on fpgas . the design is implemented and the functionality is tested in the context of speech recognition application .", "topics": ["recurrent neural network", "computation"]}
{"title": "the infinite latent events model", "abstract": "we present the infinite latent events model , a nonparametric hierarchical bayesian distribution over infinite dimensional dynamic bayesian networks with binary state representations and noisy-or-like transitions . the distribution can be used to learn structure in discrete timeseries data by simultaneously inferring a set of latent events , which events fired at each timestep , and how those events are causally linked . we illustrate the model on a sound factorization task , a network topology identification task , and a video game task .", "topics": ["time series", "bayesian network"]}
{"title": "learning deep cnn denoiser prior for image restoration", "abstract": "model-based optimization methods and discriminative learning methods have been the two dominant strategies for solving various inverse problems in low-level vision . typically , those two kinds of methods have their respective merits and drawbacks , e.g . , model-based optimization methods are flexible for handling different inverse problems but are usually time-consuming with sophisticated priors for the purpose of good performance ; in the meanwhile , discriminative learning methods have fast testing speed but their application range is greatly restricted by the specialized task . recent works have revealed that , with the aid of variable splitting techniques , denoiser prior can be plugged in as a modular part of model-based optimization methods to solve other inverse problems ( e.g . , deblurring ) . such an integration induces considerable advantage when the denoiser is obtained via discriminative learning . however , the study of integration with fast discriminative denoiser prior is still lacking . to this end , this paper aims to train a set of fast and effective cnn ( convolutional neural network ) denoisers and integrate them into model-based optimization method to solve other inverse problems . experimental results demonstrate that the learned set of denoisers not only achieve promising gaussian denoising results but also can be used as prior to deliver good performance for various low-level vision applications .", "topics": ["high- and low-level", "noise reduction"]}
{"title": "bayesian policy gradients via alpha divergence dropout inference", "abstract": "policy gradient methods have had great success in solving continuous control tasks , yet the stochastic nature of such problems makes deterministic value estimation difficult . we propose an approach which instead estimates a distribution by fitting the value function with a bayesian neural network . we optimize an $ \\alpha $ -divergence objective with bayesian dropout approximation to learn and estimate this distribution . we show that using the monte carlo posterior mean of the bayesian value function distribution , rather than a deterministic network , improves stability and performance of policy gradient methods in continuous control mujoco simulations .", "topics": ["simulation", "bayesian network"]}
{"title": "representation learning for clustering : a statistical framework", "abstract": "we address the problem of communicating domain knowledge from a user to the designer of a clustering algorithm . we propose a protocol in which the user provides a clustering of a relatively small random sample of a data set . the algorithm designer then uses that sample to come up with a data representation under which $ k $ -means clustering results in a clustering ( of the full data set ) that is aligned with the user 's clustering . we provide a formal statistical model for analyzing the sample complexity of learning a clustering representation with this paradigm . we then introduce a notion of capacity of a class of possible representations , in the spirit of the vc-dimension , showing that classes of representations that have finite such dimension can be successfully learned with sample size error bounds , and end our discussion with an analysis of that dimension for classes of representations induced by linear embeddings .", "topics": ["cluster analysis"]}
{"title": "bitsim : an algebraic similarity measure for description logics concepts", "abstract": "in this paper , we propose an algebraic similarity measure { \\sigma } bs ( bs stands for bitsim ) for assigning semantic similarity score to concept definitions in alch+ an expressive fragment of description logics ( dl ) . we define an algebraic interpretation function , i_b , that maps a concept definition to a unique string ( { \\omega } _b ) called bit-code ) over an alphabet { \\sigma } _b of 11 symbols belonging to l_b - the language over p b. ib has semantic correspondence with conventional model-theoretic interpretation of dl . we then define { \\sigma } _bs on l_b . a detailed analysis of i_b and { \\sigma } _bs has been given .", "topics": ["map"]}
{"title": "training very deep networks", "abstract": "theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success . however , training becomes more difficult as depth increases , and training of very deep networks remains an open problem . here we introduce a new architecture designed to overcome this . our so-called highway networks allow unimpeded information flow across many layers on information highways . they are inspired by long short-term memory recurrent networks and use adaptive gating units to regulate the information flow . even with hundreds of layers , highway networks can be trained directly through simple gradient descent . this enables the study of extremely deep and efficient architectures .", "topics": ["gradient descent", "gradient"]}
{"title": "unconfused ultraconservative multiclass algorithms", "abstract": "we tackle the problem of learning linear classifiers from noisy datasets in a multiclass setting . the two-class version of this problem was studied a few years ago by , e.g . bylander ( 1994 ) and blum et al . ( 1996 ) : in these contributions , the proposed approaches to fight the noise revolve around a perceptron learning scheme fed with peculiar examples computed through a weighted average of points from the noisy training set . we propose to build upon these approaches and we introduce a new algorithm called uma ( for unconfused multiclass additive algorithm ) which may be seen as a generalization to the multiclass setting of the previous approaches . in order to characterize the noise we use the confusion matrix as a multiclass extension of the classification noise studied in the aforementioned literature . theoretically well-founded , uma furthermore displays very good empirical noise robustness , as evidenced by numerical simulations conducted on both synthetic and real data . keywords : multiclass classification , perceptron , noisy labels , confusion matrix", "topics": ["numerical analysis", "synthetic data"]}
{"title": "stance classification in rumours as a sequential task exploiting the tree structure of social media conversations", "abstract": "rumour stance classification , the task that determines if each tweet in a collection discussing a rumour is supporting , denying , questioning or simply commenting on the rumour , has been attracting substantial interest . here we introduce a novel approach that makes use of the sequence of transitions observed in tree-structured conversation threads in twitter . the conversation threads are formed by harvesting users ' replies to one another , which results in a nested tree-like structure . previous work addressing the stance classification task has treated each tweet as a separate unit . here we analyse tweets by virtue of their position in a sequence and test two sequential classifiers , linear-chain crf and tree crf , each of which makes different assumptions about the conversational structure . we experiment with eight twitter datasets , collected during breaking news , and show that exploiting the sequential structure of twitter conversations achieves significant improvements over the non-sequential methods . our work is the first to model twitter conversations as a tree structure in this manner , introducing a novel way of tackling nlp tasks on twitter conversations .", "topics": ["natural language processing"]}
{"title": "the interplay between stability and regret in online learning", "abstract": "this paper considers the stability of online learning algorithms and its implications for learnability ( bounded regret ) . we introduce a novel quantity called { \\em forward regret } that intuitively measures how good an online learning algorithm is if it is allowed a one-step look-ahead into the future . we show that given stability , bounded forward regret is equivalent to bounded regret . we also show that the existence of an algorithm with bounded regret implies the existence of a stable algorithm with bounded regret and bounded forward regret . the equivalence results apply to general , possibly non-convex problems . to the best of our knowledge , our analysis provides the first general connection between stability and regret in the online setting that is not restricted to a particular class of algorithms . our stability-regret connection provides a simple recipe for analyzing regret incurred by any online learning algorithm . using our framework , we analyze several existing online learning algorithms as well as the `` approximate '' versions of algorithms like rda that solve an optimization problem at each iteration . our proofs are simpler than existing analysis for the respective algorithms , show a clear trade-off between stability and forward regret , and provide tighter regret bounds in some cases . furthermore , using our recipe , we analyze `` approximate '' versions of several algorithms such as follow-the-regularized-leader ( ftrl ) that requires solving an optimization problem at each step .", "topics": ["regret ( decision theory )", "approximation algorithm"]}
{"title": "a detailed rubric for motion segmentation", "abstract": "motion segmentation is currently an active area of research in computer vision . the task of comparing different methods of motion segmentation is complicated by the fact that researchers may use subtly different definitions of the problem . questions such as `` which objects are moving ? `` , `` what is background ? `` , and `` how can we use motion of the camera to segment objects , whether they are static or moving ? '' are clearly related to each other , but lead to different algorithms , and imply different versions of the ground truth . this report has two goals . the first is to offer a precise definition of motion segmentation so that the intent of an algorithm is as well-defined as possible . the second is to report on new versions of three previously existing data sets that are compatible with this definition . we hope that this more detailed definition , and the three data sets that go with it , will allow more meaningful comparisons of certain motion segmentation methods .", "topics": ["computer vision", "ground truth"]}
{"title": "conjunctions of among constraints", "abstract": "many existing global constraints can be encoded as a conjunction of among constraints . an among constraint holds if the number of the variables in its scope whose value belongs to a prespecified set , which we call its range , is within some given bounds . it is known that domain filtering algorithms can benefit from reasoning about the interaction of among constraints so that values can be filtered out taking into consideration several among constraints simultaneously . the present pa- per embarks into a systematic investigation on the circumstances under which it is possible to obtain efficient and complete domain filtering algorithms for conjunctions of among constraints . we start by observing that restrictions on both the scope and the range of the among constraints are necessary to obtain meaningful results . then , we derive a domain flow-based filtering algorithm and present several applications . in particular , it is shown that the algorithm unifies and generalizes several previous existing results .", "topics": ["value ( ethics )"]}
{"title": "multi-channel encoder for neural machine translation", "abstract": "attention-based encoder-decoder has the effective architecture for neural machine translation ( nmt ) , which typically relies on recurrent neural networks ( rnn ) to build the blocks that will be lately called by attentive reader during the decoding process . this design of encoder yields relatively uniform composition on source sentence , despite the gating mechanism employed in encoding rnn . on the other hand , we often hope the decoder to take pieces of source sentence at varying levels suiting its own linguistic structure : for example , we may want to take the entity name in its raw form while taking an idiom as a perfectly composed unit . motivated by this demand , we propose multi-channel encoder ( mce ) , which enhances encoding components with different levels of composition . more specifically , in addition to the hidden state of encoding rnn , mce takes 1 ) the original word embedding for raw encoding with no composition , and 2 ) a particular design of external memory in neural turing machine ( ntm ) for more complex composition , while all three encoding strategies are properly blended during decoding . empirical study on chinese-english translation shows that our model can improve by 6.52 bleu points upon a strong open source nmt system : dl4mt1 . on the wmt14 english- french task , our single shallow system achieves bleu=38.8 , comparable with the state-of-the-art deep models .", "topics": ["recurrent neural network", "machine translation"]}
{"title": "spontaneous organization leads to robustness in evolutionary algorithms", "abstract": "the interaction networks of biological systems are known to take on several non-random structural properties , some of which are believed to positively influence system robustness . researchers are only starting to understand how these structural properties emerge , however suggested roles for component fitness and community development ( modularity ) have attracted interest from the scientific community . in this study , we apply some of these concepts to an evolutionary algorithm and spontaneously organize its population using information that the population receives as it moves over a fitness landscape . more precisely , we employ fitness and clustering based driving forces for guiding network structural dynamics , which in turn are controlled by the population dynamics of an evolutionary algorithm . to evaluate the effect this has on evolution , experiments are conducted on six engineering design problems and six artificial test functions and compared against cellular genetic algorithms and 16 other evolutionary algorithm designs . our results indicate that a self-organizing topology evolutionary algorithm exhibits surprisingly robust search behavior with promising performance observed over short and long time scales . after a careful analysis of these results , we conclude that the coevolution between a population and its topology represents a powerful new paradigm for designing robust search heuristics .", "topics": ["cluster analysis", "heuristic"]}
{"title": "parkinson 's disease digital biomarker discovery with optimized transitions and inferred markov emissions", "abstract": "we search for digital biomarkers from parkinson 's disease by observing approximate repetitive patterns matching hypothesized step and stride periodic cycles . these observations were modeled as a cycle of hidden states with randomness allowing deviation from a canonical pattern of transitions and emissions , under the hypothesis that the averaged features of hidden states would serve to informatively characterize classes of patients/controls . we propose a hidden semi-markov model ( hsmm ) , a latent-state model , emitting 3d-acceleration vectors . transitions and emissions are inferred from data . we fit separate models per unique device and training label . hidden markov models ( hmm ) force geometric distributions of the duration spent at each state before transition to a new state . instead , our hsmm allows us to specify the distribution of state duration . this modified version is more effective because we are interested more in each state 's duration than the sequence of distinct states , allowing inclusion of these durations the feature vector .", "topics": ["feature vector"]}
{"title": "em algorithms for weighted-data clustering with application to audio-visual scene analysis", "abstract": "data clustering has received a lot of attention and numerous methods , algorithms and software packages are available . among these techniques , parametric finite-mixture models play a central role due to their interesting mathematical properties and to the existence of maximum-likelihood estimators based on expectation-maximization ( em ) . in this paper we propose a new mixture model that associates a weight with each observed point . we introduce the weighted-data gaussian mixture and we derive two em algorithms . the first one considers a fixed weight for each observation . the second one treats each weight as a random variable following a gamma distribution . we propose a model selection method based on a minimum message length criterion , provide a weight initialization strategy , and validate the proposed algorithms by comparing them with several state of the art parametric and non-parametric clustering techniques . we also demonstrate the effectiveness and robustness of the proposed clustering technique in the presence of heterogeneous data , namely audio-visual scene analysis .", "topics": ["cluster analysis", "eisenstein 's criterion"]}
{"title": "evaluating race and sex diversity in the world 's largest companies using deep neural networks", "abstract": "diversity is one of the fundamental properties for the survival of species , populations , and organizations . recent advances in deep learning allow for the rapid and automatic assessment of organizational diversity and possible discrimination by race , sex , age and other parameters . automating the process of assessing the organizational diversity using the deep neural networks and eliminating the human factor may provide a set of real-time unbiased reports to all stakeholders . in this pilot study we applied the deep-learned predictors of race and sex to the executive management and board member profiles of the 500 largest companies from the 2016 forbes global 2000 list and compared the predicted ratios to the ratios within each company 's country of origin and ranked them by the sex- , age- and race- diversity index ( di ) . while the study has many limitations and no claims are being made concerning the individual companies , it demonstrates a method for the rapid and impartial assessment of organizational diversity using deep neural networks .", "topics": ["value ( ethics )", "numerical analysis"]}
{"title": "unsupervised domain adaptation : a multi-task learning-based method", "abstract": "this paper presents a novel multi-task learning-based method for unsupervised domain adaptation . specifically , the source and target domain classifiers are jointly learned by considering the geometry of target domain and the divergence between the source and target domains based on the concept of multi-task learning . two novel algorithms are proposed upon the method using regularized least squares and support vector machines respectively . experiments on both synthetic and real world cross domain recognition tasks have shown that the proposed methods outperform several state-of-the-art domain adaptation methods .", "topics": ["support vector machine", "synthetic data"]}
{"title": "transformation-based feature computation for algorithm portfolios", "abstract": "instance-specific algorithm configuration and algorithm portfolios have been shown to offer significant improvements over single algorithm approaches in a variety of application domains . in the sat and csp domains algorithm portfolios have consistently dominated the main competitions in these fields for the past five years . for a portfolio approach to be effective there are two crucial conditions that must be met . first , there needs to be a collection of complementary solvers with which to make a portfolio . second , there must be a collection of problem features that can accurately identify structural differences between instances . this paper focuses on the latter issue : feature representation , because , unlike sat , not every problem has well-studied features . we employ the well-known satzilla feature set , but compute alternative sets on different sat encodings of csps . we show that regardless of what encoding is used to convert the instances , adequate structural information is maintained to differentiate between problem instances , and that this can be exploited to make an effective portfolio-based csp solver .", "topics": ["computation"]}
{"title": "dynamic backtracking", "abstract": "because of their occasional need to return to shallow points in a search tree , existing backtracking methods can sometimes erase meaningful progress toward solving a search problem . in this paper , we present a method by which backtrack points can be moved deeper in the search space , thereby avoiding this difficulty . the technique developed is a variant of dependency-directed backtracking that uses only polynomial space while still providing useful control information and retaining the completeness guarantees provided by earlier approaches .", "topics": ["polynomial"]}
{"title": "complexity of representation and inference in compositional models with part sharing", "abstract": "this paper describes serial and parallel compositional models of multiple objects with part sharing . objects are built by part-subpart compositions and expressed in terms of a hierarchical dictionary of object parts . these parts are represented on lattices of decreasing sizes which yield an executive summary description . we describe inference and learning algorithms for these models . we analyze the complexity of this model in terms of computation time ( for serial computers ) and numbers of nodes ( e.g . , `` neurons '' ) for parallel computers . in particular , we compute the complexity gains by part sharing and its dependence on how the dictionary scales with the level of the hierarchy . we explore three regimes of scaling behavior where the dictionary size ( i ) increases exponentially with the level , ( ii ) is determined by an unsupervised compositional learning algorithm applied to real data , ( iii ) decreases exponentially with scale . this analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects . in other regimes part sharing has little advantage for serial computers but can give linear processing on parallel computers .", "topics": ["generative model", "time complexity"]}
{"title": "the munich rent advisor : a success for logic programming on the internet", "abstract": "most cities in germany regularly publish a booklet called the { \\em mietspiegel } . it basically contains a verbal description of an expert system . it allows the calculation of the estimated fair rent for a flat . by hand , one may need a weekend to do so . with our computerized version , the { \\em munich rent advisor } , the user just fills in a form in a few minutes and the rent is calculated immediately . we also extended the functionality and applicability of the { \\em mietspiegel } so that the user need not answer all questions on the form . the key to computing with partial information using high-level programming was to use constraint logic programming . we rely on the internet , and more specifically the world wide web , to provide this service to a broad user group . more than ten thousand people have used our service in the last three years . this article describes the experiences in implementing and using the { \\em munich rent advisor } . our results suggests that logic programming with constraints can be an important ingredient in intelligent internet systems .", "topics": ["high- and low-level"]}
{"title": "tell me where to look : guided attention inference network", "abstract": "weakly supervised learning with only coarse labels can obtain visual explanations of deep neural network such as attention maps by back-propagating gradients . these attention maps are then available as priors for tasks such as object localization and semantic segmentation . in one common framework we address three shortcomings of previous approaches in modeling such attention maps : we ( 1 ) first time make attention maps an explicit and natural component of the end-to-end training , ( 2 ) provide self-guidance directly on these maps by exploring supervision form the network itself to improve them , and ( 3 ) seamlessly bridge the gap between using weak and extra supervision if available . despite its simplicity , experiments on the semantic segmentation task demonstrate the effectiveness of our methods . we clearly surpass the state-of-the-art on pascal voc 2012 val . and test set . besides , the proposed framework provides a way not only explaining the focus of the learner but also feeding back with direct guidance towards specific tasks . under mild assumptions our method can also be understood as a plug-in to existing weakly supervised learners to improve their generalization performance .", "topics": ["test set", "supervised learning"]}
{"title": "supervised multiview learning based on simultaneous learning of multiview intact and single view classifier", "abstract": "multiview learning problem refers to the problem of learning a classifier from multiple view data . in this data set , each data points is presented by multiple different views . in this paper , we propose a novel method for this problem . this method is based on two assumptions . the first assumption is that each data point has an intact feature vector , and each view is obtained by a linear transformation from the intact vector . the second assumption is that the intact vectors are discriminative , and in the intact space , we have a linear classifier to separate the positive class from the negative class . we define an intact vector for each data point , and a view-conditional transformation matrix for each view , and propose to reconstruct the multiple view feature vectors by the product of the corresponding intact vectors and transformation matrices . moreover , we also propose a linear classifier in the intact space , and learn it jointly with the intact vectors . the learning problem is modeled by a minimization problem , and the objective function is composed of a cauchy error estimator-based view-conditional reconstruction term over all data points and views , and a classification error term measured by hinge loss over all the intact vectors of all the data points . some regularization terms are also imposed to different variables in the objective function . the minimization problem is solve by an iterative algorithm using alternate optimization strategy and gradient descent algorithm . the proposed algorithm shows it advantage in the compression to other multiview learning algorithms on benchmark data sets .", "topics": ["feature vector", "optimization problem"]}
{"title": "a characterization of deterministic sampling patterns for low-rank matrix completion", "abstract": "low-rank matrix completion ( lrmc ) problems arise in a wide variety of applications . previous theory mainly provides conditions for completion under missing-at-random samplings . this paper studies deterministic conditions for completion . an incomplete $ d \\times n $ matrix is finitely rank- $ r $ completable if there are at most finitely many rank- $ r $ matrices that agree with all its observed entries . finite completability is the tipping point in lrmc , as a few additional samples of a finitely completable matrix guarantee its unique completability . the main contribution of this paper is a deterministic sampling condition for finite completability . we use this to also derive deterministic sampling conditions for unique completability that can be efficiently verified . we also show that under uniform random sampling schemes , these conditions are satisfied with high probability if $ o ( \\max\\ { r , \\log d\\ } ) $ entries per column are observed . these findings have several implications on lrmc regarding lower bounds , sample and computational complexity , the role of coherence , adaptive settings and the validation of any completion algorithm . we complement our theoretical results with experiments that support our findings and motivate future analysis of uncharted sampling regimes .", "topics": ["sampling ( signal processing )"]}
{"title": "texture analysis using volume-radius fractal dimension", "abstract": "texture plays an important role in computer vision . it is one of the most important visual attributes used in image analysis , once it provides information about pixel organization at different regions of the image . this paper presents a novel approach for texture characterization , based on complexity analysis . the proposed approach expands the idea of the mass-radius fractal dimension , a method originally developed for shape analysis , to a set of coordinates in 3d-space that represents the texture under analysis in a signature able to characterize efficiently different texture classes in terms of complexity . an experiment using images from the brodatz album illustrates the method performance .", "topics": ["computer vision", "pixel"]}
{"title": "discriminative sparse coding on multi-manifold for data representation and classification", "abstract": "sparse coding has been popularly used as an effective data representation method in various applications , such as computer vision , medical imaging and bioinformatics , etc . however , the conventional sparse coding algorithms and its manifold regularized variants ( graph sparse coding and laplacian sparse coding ) , learn the codebook and codes in a unsupervised manner and neglect the class information available in the training set . to address this problem , in this paper we propose a novel discriminative sparse coding method based on multi-manifold , by learning discriminative class-conditional codebooks and sparse codes from both data feature space and class labels . first , the entire training set is partitioned into multiple manifolds according to the class labels . then , we formulate the sparse coding as a manifold-manifold matching problem and learn class-conditional codebooks and codes to maximize the manifold margins of different classes . lastly , we present a data point-manifold matching error based strategy to classify the unlabeled data point . experimental results on somatic mutations identification and breast tumors classification in ultrasonic images tasks demonstrate the efficacy of the proposed data representation-classification approach .", "topics": ["test set", "unsupervised learning"]}
{"title": "better mini-batch algorithms via accelerated gradient methods", "abstract": "mini-batch algorithms have been proposed as a way to speed-up stochastic convex optimization problems . we study how such algorithms can be improved using accelerated gradient methods . we provide a novel analysis , which shows how standard gradient methods may sometimes be insufficient to obtain a significant speed-up and propose a novel accelerated gradient algorithm , which deals with this deficiency , enjoys a uniformly superior guarantee and works well in practice .", "topics": ["gradient"]}
{"title": "timenet : pre-trained deep recurrent neural network for time series classification", "abstract": "inspired by the tremendous success of deep convolutional neural networks as generic feature extractors for images , we propose timenet : a deep recurrent neural network ( rnn ) trained on diverse time series in an unsupervised manner using sequence to sequence ( seq2seq ) models to extract features from time series . rather than relying on data from the problem domain , timenet attempts to generalize time series representation across domains by ingesting time series from several domains simultaneously . once trained , timenet can be used as a generic off-the-shelf feature extractor for time series . the representations or embeddings given by a pre-trained timenet are found to be useful for time series classification ( tsc ) . for several publicly available datasets from ucr tsc archive and an industrial telematics sensor data from vehicles , we observe that a classifier learned over the timenet embeddings yields significantly better performance compared to ( i ) a classifier learned over the embeddings given by a domain-specific rnn , as well as ( ii ) a nearest neighbor classifier based on dynamic time warping .", "topics": ["test set", "time series"]}
{"title": "on the sample complexity of the linear quadratic regulator", "abstract": "this paper addresses the optimal control problem known as the linear quadratic regulator in the case when the dynamics are unknown . we propose a multi-stage procedure , called coarse-id control , that estimates a model from a few experimental trials , estimates the error in that model with respect to the truth , and then designs a controller using both the model and uncertainty estimate . our technique uses contemporary tools from random matrix theory to bound the error in the estimation procedure . we also employ a recently developed approach to control synthesis called system level synthesis that enables robust control design by solving a convex optimization problem . we provide end-to-end bounds on the relative error in control cost that are nearly optimal in the number of parameters and that highlight salient properties of the system to be controlled such as closed-loop sensitivity and optimal control magnitude . we show experimentally that the coarse-id approach enables efficient computation of a stabilizing controller in regimes where simple control schemes that do not take the model uncertainty into account fail to stabilize the true system .", "topics": ["optimization problem", "computation"]}
{"title": "artificial intelligence and pediatrics : a synthetic mini review", "abstract": "the use of artificial intelligence intelligencein medicine can be traced back to 1968 when paycha published his paper le diagnostic a l'aide d'intelligences artificielle , presentation de la premiere machine diagnostri . few years later shortliffe et al . presented an expert system named mycin which was able to identify bacteria causing severe blood infections and to recommend antibiotics . despite the fact that mycin outperformed members of the stanford medical school in the reliability of diagnosis it was never used in practice due to a legal issue who do you sue if it gives a wrong diagnosis ? . however only in 2016 when the artificial intelligence software built into the ibm watson ai platform correctly diagnosed and proposed an effective treatment for a 60-year-old womans rare form of leukemia the ai use in medicine become really popular.on of first papers presenting the use of ai in paediatrics was published in 1984 . the paper introduced a computer-assisted medical decision making system called shelp .", "topics": ["synthetic data", "artificial intelligence"]}
{"title": "end-to-end learning of video super-resolution with motion compensation", "abstract": "learning approaches have shown great success in the task of super-resolving an image given a low resolution input . video super-resolution aims for exploiting additionally the information from multiple images . typically , the images are related via optical flow and consecutive image warping . in this paper , we provide an end-to-end video super-resolution network that , in contrast to previous works , includes the estimation of optical flow in the overall network architecture . we analyze the usage of optical flow for video super-resolution and find that common off-the-shelf image warping does not allow video super-resolution to benefit much from optical flow . we rather propose an operation for motion compensation that performs warping from low to high resolution directly . we show that with this network configuration , video super-resolution can benefit from optical flow and we obtain state-of-the-art results on the popular test sets . we also show that the processing of whole images rather than independent patches is responsible for a large increase in accuracy .", "topics": ["end-to-end principle"]}
{"title": "combining search with structured data to create a more engaging user experience in open domain dialogue", "abstract": "the greatest challenges in building sophisticated open-domain conversational agents arise directly from the potential for ongoing mixed-initiative multi-turn dialogues , which do not follow a particular plan or pursue a particular fixed information need . in order to make coherent conversational contributions in this context , a conversational agent must be able to track the types and attributes of the entities under discussion in the conversation and know how they are related . in some cases , the agent can rely on structured information sources to help identify the relevant semantic relations and produce a turn , but in other cases , the only content available comes from search , and it may be unclear which semantic relations hold between the search results and the discourse context . a further constraint is that the system must produce its contribution to the ongoing conversation in real-time . this paper describes our experience building slugbot for the 2017 alexa prize , and discusses how we leveraged search and structured data from different sources to help slugbot produce dialogic turns and carry on conversations whose length over the semi-finals user evaluation period averaged 8:17 minutes .", "topics": ["entity"]}
{"title": "multi-task regularization with covariance dictionary for linear classifiers", "abstract": "in this paper we propose a multi-task linear classifier learning problem called d-svm ( dictionary svm ) . d-svm uses a dictionary of parameter covariance shared by all tasks to do multi-task knowledge transfer among different tasks . we formally define the learning problem of d-svm and show two interpretations of this problem , from both the probabilistic and kernel perspectives . from the probabilistic perspective , we show that our learning formulation is actually a map estimation on all optimization variables . we also show its equivalence to a multiple kernel learning problem in which one is trying to find a re-weighting kernel for features from a dictionary of basis ( despite the fact that only linear classifiers are learned ) . finally , we describe an alternative optimization scheme to minimize the objective function and present empirical studies to valid our algorithm .", "topics": ["kernel ( operating system )", "optimization problem"]}
{"title": "robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching", "abstract": "this paper presents a robotic pick-and-place system that is capable of grasping and recognizing both known and novel objects in cluttered environments . the key new feature of the system is that it handles a wide range of object categories without needing any task-specific training data for novel objects . to achieve this , it first uses a category-agnostic affordance prediction algorithm to select and execute among four different grasping primitive behaviors . it then recognizes picked objects with a cross-domain image classification framework that matches observed images to product images . since product images are readily available for a wide range of objects ( e.g . , from the web ) , the system works out-of-the-box for novel objects without requiring any additional training data . exhaustive experimental results demonstrate that our multi-affordance grasping achieves high success rates for a wide variety of objects in clutter , and our recognition algorithm achieves high accuracy for both known and novel grasped objects . the approach was part of the mit-princeton team system that took 1st place in the stowing task at the 2017 amazon robotics challenge . all code , datasets , and pre-trained models are available online at http : //arc.cs.princeton.edu", "topics": ["test set", "computer vision"]}
{"title": "deep reinforcement learning using capsules in advanced game environments", "abstract": "reinforcement learning ( rl ) is a research area that has blossomed tremendously in recent years and has shown remarkable potential for artificial intelligence based opponents in computer games . this success is primarily due to vast capabilities of convolutional neural networks ( convnet ) , enabling algorithms to extract useful information from noisy environments . capsule network ( capsnet ) is a recent introduction to the deep learning algorithm group and has only barely begun to be explored . the network is an architecture for image classification , with superior performance for classification of the mnist dataset . capsnets have not been explored beyond image classification . this thesis introduces the use of capsnet for q-learning based game algorithms . to successfully apply capsnet in advanced game play , three main contributions follow . first , the introduction of four new game environments as frameworks for rl research with increasing complexity , namely flash rl , deep line wars , deep rts , and deep maze . these environments fill the gap between relatively simple and more complex game environments available for rl research and are in the thesis used to test and explore the capsnet behavior . second , the thesis introduces a generative modeling approach to produce artificial training data for use in deep learning models including capsnets . we empirically show that conditional generative modeling can successfully generate game data of sufficient quality to train a deep q-network well . third , we show that capsnet is a reliable architecture for deep q-learning based algorithms for game ai . a capsule is a group of neurons that determine the presence of objects in the data and is in the literature shown to increase the robustness of training and predictions while lowering the amount training data needed . it should , therefore , be ideally suited for game plays .", "topics": ["test set", "reinforcement learning"]}
{"title": "the latent structure of dictionaries", "abstract": "how many words ( and which ones ) are sufficient to define all other words ? when dictionaries are analyzed as directed graphs with links from defining words to defined words , they reveal a latent structure . recursively removing all words that are reachable by definition but that do not define any further words reduces the dictionary to a kernel of about 10 % . this is still not the smallest number of words that can define all the rest . about 75 % of the kernel turns out to be its core , a strongly connected subset of words with a definitional path to and from any pair of its words and no word 's definition depending on a word outside the set . but the core can not define all the rest of the dictionary . the 25 % of the kernel surrounding the core consists of small strongly connected subsets of words : the satellites . the size of the smallest set of words that can define all the rest ( the graph 's minimum feedback vertex set or minset ) is about 1 % of the dictionary , 15 % of the kernel , and half-core , half-satellite . but every dictionary has a huge number of minsets . the core words are learned earlier , more frequent , and less concrete than the satellites , which in turn are learned earlier and more frequent but more concrete than the rest of the dictionary . in principle , only one minset 's words would need to be grounded through the sensorimotor capacity to recognize and categorize their referents . in a dual-code sensorimotor-symbolic model of the mental lexicon , the symbolic code could do all the rest via re-combinatory definition .", "topics": ["kernel ( operating system )", "dictionary"]}
{"title": "regularized risk minimization by nesterov 's accelerated gradient methods : algorithmic extensions and empirical studies", "abstract": "nesterov 's accelerated gradient methods ( agm ) have been successfully applied in many machine learning areas . however , their empirical performance on training max-margin models has been inferior to existing specialized solvers . in this paper , we first extend agm to strongly convex and composite objective functions with bregman style prox-functions . our unifying framework covers both the $ \\infty $ -memory and 1-memory styles of agm , tunes the lipschiz constant adaptively , and bounds the duality gap . then we demonstrate various ways to apply this framework of methods to a wide range of machine learning problems . emphasis will be given on their rate of convergence and how to efficiently compute the gradient and optimize the models . the experimental results show that with our extensions agm outperforms state-of-the-art solvers on max-margin models .", "topics": ["gradient"]}
{"title": "detecting small , densely distributed objects with filter-amplifier networks and loss boosting", "abstract": "detecting small , densely distributed objects is a significant challenge : small objects often contain less distinctive information compared to larger ones , and finer-grained precision of bounding box boundaries are required . in this paper , we propose two techniques for addressing this problem . first , we estimate the likelihood that each pixel belongs to an object boundary rather than predicting coordinates of bounding boxes ( as yolo , faster-rcnn and ssd do ) , by proposing a new architecture called filter-amplifier networks ( fans ) . second , we introduce a technique called loss boosting ( lb ) which attempts to soften the loss imbalance problem on each image . we test our algorithm on the problem of detecting electrical components on a new , realistic , diverse dataset of printed circuit boards ( pcbs ) , as well as the problem of detecting vehicles in the vehicle detection in aerial imagery ( vedai ) dataset . experiments show that our method works significantly better than current state-of-the-art algorithms with respect to accuracy , recall and average iou .", "topics": ["pixel"]}
{"title": "feature extraction via recurrent random deep ensembles and its application in gruop-level happiness estimation", "abstract": "this paper presents a novel ensemble framework to extract highly discriminative feature representation of image and its application for group-level happpiness intensity prediction in wild . in order to generate enough diversity of decisions , n convolutional neural networks are trained by bootstrapping the training set and extract n features for each image from them . a recurrent neural network ( rnn ) is then used to remember which network extracts better feature and generate the final feature representation for one individual image . several group emotion models ( gem ) are used to aggregate face fea- tures in a group and use parameter-optimized support vector regressor ( svr ) to get the final results . through extensive experiments , the great effectiveness of the proposed recurrent random deep ensembles ( rrde ) is demonstrated in both structural and decisional ways . the best result yields a 0.55 root-mean-square error ( rmse ) on validation set of happei dataset , significantly better than the baseline of 0.78 .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "improved optimization of finite sums with minibatch stochastic variance reduced proximal iterations", "abstract": "we present novel minibatch stochastic optimization methods for empirical risk minimization problems , the methods efficiently leverage variance reduced first-order and sub-sampled higher-order information to accelerate the convergence speed . for quadratic objectives , we prove improved iteration complexity over state-of-the-art under reasonable assumptions . we also provide empirical evidence of the advantages of our method compared to existing approaches in the literature .", "topics": ["iteration"]}
{"title": "incorporating copying mechanism in sequence-to-sequence learning", "abstract": "we address an important problem in sequence-to-sequence ( seq2seq ) learning referred to as copying , in which certain segments in the input sequence are selectively replicated in the output sequence . a similar phenomenon is observable in human language communication . for example , humans tend to repeat entity names or even long phrases in conversation . the challenge with regard to copying in seq2seq is that new machinery is needed to decide when to perform the operation . in this paper , we incorporate copying into neural network-based seq2seq learning and propose a new model called copynet with encoder-decoder structure . copynet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence . our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of copynet . for example , copynet can outperform regular rnn-based model with remarkable margins on text summarization tasks .", "topics": ["synthetic data"]}
{"title": "transferring rich deep features for facial beauty prediction", "abstract": "feature extraction plays a significant part in computer vision tasks . in this paper , we propose a method which transfers rich deep features from a pretrained model on face verification task and feeds the features into bayesian ridge regression algorithm for facial beauty prediction . we leverage the deep neural networks that extracts more abstract features from stacked layers . through simple but effective feature fusion strategy , our method achieves improved or comparable performance on scut-fbp dataset and eccv hotornot dataset . our experiments demonstrate the effectiveness of the proposed method and clarify the inner interpretability of facial beauty perception .", "topics": ["feature extraction", "computer vision"]}
{"title": "post training in deep learning with last kernel", "abstract": "one of the main challenges of deep learning methods is the choice of an appropriate training strategy . in particular , additional steps , such as unsupervised pre-training , have been shown to greatly improve the performances of deep structures . in this article , we propose an extra training step , called post-training , which only optimizes the last layer of the network . we show that this procedure can be analyzed in the context of kernel theory , with the first layers computing an embedding of the data and the last layer a statistical model to solve the task based on this embedding . this step makes sure that the embedding , or representation , of the data is used in the best possible way for the considered task . this idea is then tested on multiple architectures with various data sets , showing that it consistently provides a boost in performance .", "topics": ["feature learning", "unsupervised learning"]}
{"title": "reasoning about independence in probabilistic models of relational data", "abstract": "we extend the theory of d-separation to cases in which data instances are not independent and identically distributed . we show that applying the rules of d-separation directly to the structure of probabilistic models of relational data inaccurately infers conditional independence . we introduce relational d-separation , a theory for deriving conditional independence facts from relational models . we provide a new representation , the abstract ground graph , that enables a sound , complete , and computationally efficient method for answering d-separation queries about relational models , and we present empirical results that demonstrate effectiveness .", "topics": ["computational complexity theory", "entity"]}
{"title": "online adaptation of deep architectures with reinforcement learning", "abstract": "online learning has become crucial to many problems in machine learning . as more data is collected sequentially , quickly adapting to changes in the data distribution can offer several competitive advantages such as avoiding loss of prior knowledge and more efficient learning . however , adaptation to changes in the data distribution ( also known as covariate shift ) needs to be performed without compromising past knowledge already built in into the model to cope with voluminous and dynamic data . in this paper , we propose an online stacked denoising autoencoder whose structure is adapted through reinforcement learning . our algorithm forces the network to exploit and explore favourable architectures employing an estimated utility function that maximises the accuracy of an unseen validation sequence . different actions , such as pool , increment and merge are available to modify the structure of the network . as we observe through a series of experiments , our approach is more responsive , robust , and principled than its counterparts for non-stationary as well as stationary data distributions . experimental results indicate that our algorithm performs better at preserving gained prior knowledge and responding to changes in the data distribution .", "topics": ["noise reduction", "reinforcement learning"]}
{"title": "approximate inference with amortised mcmc", "abstract": "we propose a novel approximate inference algorithm that approximates a target distribution by amortising the dynamics of a user-selected mcmc sampler . the idea is to initialise mcmc using samples from an approximation network , apply the mcmc operator to improve these samples , and finally use the samples to update the approximation network thereby improving its quality . this provides a new generic framework for approximate inference , allowing us to deploy highly complex , or implicitly defined approximation families with intractable densities , including approximations produced by warping a source of randomness through a deep neural network . experiments consider image modelling with deep generative models as a challenging test for the method . deep models trained using amortised mcmc are shown to generate realistic looking samples as well as producing diverse imputations for images with regions of missing pixels .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "tactics of adversarial attack on deep reinforcement learning agents", "abstract": "we introduce two tactics to attack agents trained by deep reinforcement learning algorithms using adversarial examples , namely the strategically-timed attack and the enchanting attack . in the strategically-timed attack , the adversary aims at minimizing the agent 's reward by only attacking the agent at a small subset of time steps in an episode . limiting the attack activity to this subset helps prevent detection of the attack by the agent . we propose a novel method to determine when an adversarial example should be crafted and applied . in the enchanting attack , the adversary aims at luring the agent to a designated target state . this is achieved by combining a generative model and a planning algorithm : while the generative model predicts the future states , the planning algorithm generates a preferred sequence of actions for luring the agent . a sequence of adversarial examples is then crafted to lure the agent to take the preferred sequence of actions . we apply the two tactics to the agents trained by the state-of-the-art deep reinforcement learning algorithm including dqn and a3c . in 5 atari games , our strategically timed attack reduces as much reward as the uniform attack ( i.e . , attacking at every time step ) does by attacking the agent 4 times less often . our enchanting attack lures the agent toward designated target states with a more than 70 % success rate . videos are available at http : //yclin.me/adversarial_attack_rl/", "topics": ["generative model", "reinforcement learning"]}
{"title": "improved estimation in time varying models", "abstract": "locally adapted parameterizations of a model ( such as locally weighted regression ) are expressive but often suffer from high variance . we describe an approach for reducing the variance , based on the idea of estimating simultaneously a transformed space for the model , as well as locally adapted parameterizations in this new space . we present a new problem formulation that captures this idea and illustrate it in the important context of time varying models . we develop an algorithm for learning a set of bases for approximating a time varying sparse network ; each learned basis constitutes an archetypal sparse network structure . we also provide an extension for learning task-driven bases . we present empirical results on synthetic data sets , as well as on a bci eeg classification task .", "topics": ["approximation algorithm", "synthetic data"]}
{"title": "tag-enhanced tree-structured neural networks for implicit discourse relation classification", "abstract": "identifying implicit discourse relations between text spans is a challenging task because it requires understanding the meaning of the text . to tackle this task , recent studies have tried several deep learning methods but few of them exploited the syntactic information . in this work , we explore the idea of incorporating syntactic parse tree into neural networks . specifically , we employ the tree-lstm model and tree-gru model , which are based on the tree structure , to encode the arguments in a relation . moreover , we further leverage the constituent tags to control the semantic composition process in these tree-structured neural networks . experimental results show that our method achieves state-of-the-art performance on pdtb corpus .", "topics": ["neural networks", "parsing"]}
{"title": "asymptotically exact , embarrassingly parallel mcmc", "abstract": "communication costs , resulting from synchronization requirements during learning , can greatly slow down many parallel machine learning algorithms . in this paper , we present a parallel markov chain monte carlo ( mcmc ) algorithm in which subsets of data are processed independently , with very little communication . first , we arbitrarily partition data onto multiple machines . then , on each machine , any classical mcmc method ( e.g . , gibbs sampling ) may be used to draw samples from a posterior distribution given the data subset . finally , the samples from each machine are combined to form samples from the full posterior . this embarrassingly parallel algorithm allows each machine to act independently on a subset of the data ( without communication ) until the final combination stage . we prove that our algorithm generates asymptotically exact samples and empirically demonstrate its ability to parallelize burn-in and sampling in several models .", "topics": ["sampling ( signal processing )", "markov chain"]}
{"title": "blood capillaries and vessels segmentation in optical coherence tomography angiogram using fuzzy c-means and curvelet transform", "abstract": "this paper has been removed from arxiv as the submitter did not have ownership of the data presented in this work .", "topics": ["cluster analysis", "unsupervised learning"]}
{"title": "incremental import vector machines for classifying hyperspectral data", "abstract": "in this paper we propose an incremental learning strategy for import vector machines ( ivm ) , which is a sparse kernel logistic regression approach . we use the procedure for the concept of self-training for sequential classification of hyperspectral data . the strategy comprises the inclusion of new training samples to increase the classification accuracy and the deletion of non-informative samples to be memory- and runtime-efficient . moreover , we update the parameters in the incremental ivm model without re-training from scratch . therefore , the incremental classifier is able to deal with large data sets . the performance of the ivm in comparison to support vector machines ( svm ) is evaluated in terms of accuracy and experiments are conducted to assess the potential of the probabilistic outputs of the ivm . experimental results demonstrate that the ivm and svm perform similar in terms of classification accuracy . however , the number of import vectors is significantly lower when compared to the number of support vectors and thus , the computation time during classification can be decreased . moreover , the probabilities provided by ivm are more reliable , when compared to the probabilistic information , derived from an svm 's output . in addition , the proposed self-training strategy can increase the classification accuracy . overall , the ivm and the its incremental version is worthwhile for the classification of hyperspectral data .", "topics": ["support vector machine", "time complexity"]}
{"title": "issues , challenges and tools of clustering algorithms", "abstract": "clustering is an unsupervised technique of data mining . it means grouping similar objects together and separating the dissimilar ones . each object in the data set is assigned a class label in the clustering process using a distance measure . this paper has captured the problems that are faced in real when clustering algorithms are implemented .it also considers the most extensively used tools which are readily available and support functions which ease the programming . once algorithms have been implemented , they also need to be tested for its validity . there exist several validation indexes for testing the performance and accuracy which have also been discussed here .", "topics": ["data mining", "cluster analysis"]}
{"title": "generating images part by part with composite generative adversarial networks", "abstract": "image generation remains a fundamental problem in artificial intelligence in general and deep learning in specific . the generative adversarial network ( gan ) was successful in generating high quality samples of natural images . we propose a model called composite generative adversarial network , that reveals the complex structure of images with multiple generators in which each generator generates some part of the image . those parts are combined by alpha blending process to create a new single image . it can generate , for example , background and face sequentially with two generators , after training on face dataset . training was done in an unsupervised way without any labels about what each generator should generate . we found possibilities of learning the structure by using this generative model empirically .", "topics": ["generative model", "unsupervised learning"]}
{"title": "digital ecosystems", "abstract": "we view digital ecosystems to be the digital counterparts of biological ecosystems , which are considered to be robust , self-organising and scalable architectures that can automatically solve complex , dynamic problems . so , this work is concerned with the creation , investigation , and optimisation of digital ecosystems , exploiting the self-organising properties of biological ecosystems . first , we created the digital ecosystem , a novel optimisation technique inspired by biological ecosystems , where the optimisation works at two levels : a first optimisation , migration of agents which are distributed in a decentralised peer-to-peer network , operating continuously in time ; this process feeds a second optimisation based on evolutionary computing that operates locally on single peers and is aimed at finding solutions to satisfy locally relevant constraints . we then investigated its self-organising aspects , starting with an extension to the definition of physical complexity to include evolving agent populations . next , we established stability of evolving agent populations over time , by extending the chli-dewilde definition of agent stability to include evolutionary dynamics . further , we evaluated the diversity of the software agents within evolving agent populations . to conclude , we considered alternative augmentations to optimise and accelerate our digital ecosystem , by studying the accelerating effect of a clustering catalyst on the evolutionary dynamics . we also studied the optimising effect of targeted migration on the ecological dynamics , through the indirect and emergent optimisation of the agent migration patterns . overall , we have advanced the understanding of creating digital ecosystems , the self-organisation that occurs within them , and the optimisation of their ecosystem-oriented architecture .", "topics": ["mathematical optimization", "cluster analysis"]}
{"title": "local similarities , global coding : an algorithm for feature coding and its applications", "abstract": "data coding as a building block of several image processing algorithms has been received great attention recently . indeed , the importance of the locality assumption in coding approaches is studied in numerous works and several methods are proposed based on this concept . we probe this assumption and claim that taking the similarity between a data point and a more global set of anchor points does not necessarily weaken the coding method as long as the underlying structure of the anchor points are taken into account . based on this fact , we propose to capture this underlying structure by assuming a random walker over the anchor points . we show that our method is a fast approximate learning algorithm based on the diffusion map kernel . the experiments on various datasets show that making different state-of-the-art coding algorithms aware of this structure boosts them in different learning tasks .", "topics": ["image processing", "approximation algorithm"]}
{"title": "fcn-rlstm : deep spatio-temporal neural networks for vehicle counting in city cameras", "abstract": "in this paper , we develop deep spatio-temporal neural networks to sequentially count vehicles from low quality videos captured by city cameras ( citycams ) . citycam videos have low resolution , low frame rate , high occlusion and large perspective , making most existing methods lose their efficacy . to overcome limitations of existing methods and incorporate the temporal information of traffic video , we design a novel fcn-rlstm network to jointly estimate vehicle density and vehicle count by connecting fully convolutional neural networks ( fcn ) with long short term memory networks ( lstm ) in a residual learning fashion . such design leverages the strengths of fcn for pixel-level prediction and the strengths of lstm for learning complex temporal dynamics . the residual learning connection reformulates the vehicle count regression as learning residual functions with reference to the sum of densities in each frame , which significantly accelerates the training of networks . to preserve feature map resolution , we propose a hyper-atrous combination to integrate atrous convolution in fcn and combine feature maps of different convolution layers . fcn-rlstm enables refined feature representation and a novel end-to-end trainable mapping from pixels to vehicle count . we extensively evaluated the proposed method on different counting tasks with three datasets , with experimental results demonstrating their effectiveness and robustness . in particular , fcn-rlstm reduces the mean absolute error ( mae ) from 5.31 to 4.21 on trancos , and reduces the mae from 2.74 to 1.53 on webcamt . training process is accelerated by 5 times on average .", "topics": ["neural networks", "map"]}
{"title": "scalable peaceman-rachford splitting method with proximal terms", "abstract": "along with developing of peaceman-rachford splittling method ( prsm ) , many batch algorithms based on it have been studied very deeply . but almost no algorithm focused on the performance of stochastic version of prsm . in this paper , we propose a new stochastic algorithm based on prsm , prove its convergence rate in ergodic sense , and test its performance on both artificial and real data . we show that our proposed algorithm , stochastic scalable prsm ( ss-prsm ) , enjoys the $ o ( 1/k ) $ convergence rate , which is the same as those newest stochastic algorithms that based on admm but faster than general stochastic admm ( which is $ o ( 1/\\sqrt { k } ) $ ) . our algorithm also owns wide flexibility , outperforms many state-of-the-art stochastic algorithms coming from admm , and has low memory cost in large-scale splitting optimization problems .", "topics": ["numerical analysis", "gradient"]}
{"title": "speeding-up graphical model optimization via a coarse-to-fine cascade of pruning classifiers", "abstract": "we propose a general and versatile framework that significantly speeds-up graphical model optimization while maintaining an excellent solution accuracy . the proposed approach relies on a multi-scale pruning scheme that is able to progressively reduce the solution space by use of a novel strategy based on a coarse-to-fine cascade of learnt classifiers . we thoroughly experiment with classic computer vision related mrf problems , where our framework constantly yields a significant time speed-up ( with respect to the most efficient inference methods ) and obtains a more accurate solution than directly optimizing the mrf .", "topics": ["graphical model", "mathematical optimization"]}
{"title": "divide and fuse : a re-ranking approach for person re-identification", "abstract": "as re-ranking is a necessary procedure to boost person re-identification ( re-id ) performance on large-scale datasets , the diversity of feature becomes crucial to person reid for its importance both on designing pedestrian descriptions and re-ranking based on feature fusion . however , in many circumstances , only one type of pedestrian feature is available . in this paper , we propose a `` divide and use '' re-ranking framework for person re-id . it exploits the diversity from different parts of a high-dimensional feature vector for fusion-based re-ranking , while no other features are accessible . specifically , given an image , the extracted feature is divided into sub-features . then the contextual information of each sub-feature is iteratively encoded into a new feature . finally , the new features from the same image are fused into one vector for re-ranking . experimental results on two person re-id benchmarks demonstrate the effectiveness of the proposed framework . especially , our method outperforms the state-of-the-art on the market-1501 dataset .", "topics": ["feature vector"]}
{"title": "learning convex regularizers for optimal bayesian denoising", "abstract": "we propose a data-driven algorithm for the maximum a posteriori ( map ) estimation of stochastic processes from noisy observations . the primary statistical properties of the sought signal is specified by the penalty function ( i.e . , negative logarithm of the prior probability density function ) . our alternating direction method of multipliers ( admm ) -based approach translates the estimation task into successive applications of the proximal mapping of the penalty function . capitalizing on this direct link , we define the proximal operator as a parametric spline curve and optimize the spline coefficients by minimizing the average reconstruction error for a given training set . the key aspects of our learning method are that the associated penalty function is constrained to be convex and the convergence of the admm iterations is proven . as a result of these theoretical guarantees , adaptation of the proposed framework to different levels of measurement noise is extremely simple and does not require any retraining . we apply our method to estimation of both sparse and non-sparse models of l\\ ' { e } vy processes for which the minimum mean square error ( mmse ) estimators are available . we carry out a single training session and perform comparisons at various signal-to-noise ratio ( snr ) values . simulations illustrate that the performance of our algorithm is practically identical to the one of the mmse estimator irrespective of the noise power .", "topics": ["test set", "noise reduction"]}
{"title": "speculation on graph computation architectures and computing via synchronization", "abstract": "a speculative overview of a future topic of research . the paper is a collection of ideas concerning two related areas : 1 ) graph computation machines ( `` computing with graphs '' ) . this is the class of models of computation in which the state of the computation is represented as a graph or network . 2 ) arc-based neural networks , which store information not as activation in the nodes , but rather by adding and deleting arcs . sometimes the arcs may be interpreted as synchronization . warnings to readers : this is not the sort of thing that one might submit to a journal or conference . no proofs are presented . the presentation is informal , and written at an introductory level . you 'll probably want to wait for a more concise presentation .", "topics": ["computation"]}
{"title": "simulation of optical flow and fuzzy based obstacle avoidance system for mobile robots", "abstract": "honey bees use optical flow to avoid obstacles effectively . in this research work similar methodology was tested on a simulated mobile robot . simulation framework was based on vrml and simulink in a 3d world . optical flow vectors were calculated from a video scene captured by a virtual camera which was used as inputs to a fuzzy logic controller . fuzzy logic controller decided the locomotion of the robot . different fuzzy logic rules were evaluated . the robot was able to navigate through complex static and dynamic environments effectively , avoiding obstacles on its path .", "topics": ["simulation", "robot"]}
{"title": "a theoretical analysis of noisy sparse subspace clustering on dimensionality-reduced data", "abstract": "subspace clustering is the problem of partitioning unlabeled data points into a number of clusters so that data points within one cluster lie approximately on a low-dimensional linear subspace . in many practical scenarios , the dimensionality of data points to be clustered are compressed due to constraints of measurement , computation or privacy . in this paper , we study the theoretical properties of a popular subspace clustering algorithm named sparse subspace clustering ( ssc ) and establish formal success conditions of ssc on dimensionality-reduced data . our analysis applies to the most general fully deterministic model where both underlying subspaces and data points within each subspace are deterministically positioned , and also a wide range of dimensionality reduction techniques ( e.g . , gaussian random projection , uniform subsampling , sketching ) that fall into a subspace embedding framework ( meng & mahoney , 2013 ; avron et al . , 2014 ) . finally , we apply our analysis to a differentially private ssc algorithm and established both privacy and utility guarantees of the proposed method .", "topics": ["cluster analysis", "sparse matrix"]}
{"title": "deep learning for predicting refractive error from retinal fundus images", "abstract": "refractive error , one of the leading cause of visual impairment , can be corrected by simple interventions like prescribing eyeglasses . we trained a deep learning algorithm to predict refractive error from the fundus photographs from participants in the uk biobank cohort , which were 45 degree field of view images and the areds clinical trial , which contained 30 degree field of view images . our model use the `` attention '' method to identify features that are correlated with refractive error . mean absolute error ( mae ) of the algorithm 's prediction compared to the refractive error obtained in the areds and uk biobank . the resulting algorithm had a mae of 0.56 diopters ( 95 % ci : 0.55-0.56 ) for estimating spherical equivalent on the uk biobank dataset and 0.91 diopters ( 95 % ci : 0.89-0.92 ) for the areds dataset . the baseline expected mae ( obtained by simply predicting the mean of this population ) was 1.81 diopters ( 95 % ci : 1.79-1.84 ) for uk biobank and 1.63 ( 95 % ci : 1.60-1.67 ) for areds . attention maps suggested that the foveal region was one of the most important areas used by the algorithm to make this prediction , though other regions also contribute to the prediction . the ability to estimate refractive error with high accuracy from retinal fundus photos has not been previously known and demonstrates that deep learning can be applied to make novel predictions from medical images . given that several groups have recently shown that it is feasible to obtain retinal fundus photos using mobile phones and inexpensive attachments , this work may be particularly relevant in regions of the world where autorefractors may not be readily available .", "topics": ["baseline ( configuration management )", "sampling ( signal processing )"]}
{"title": "on the min-cost traveling salesman problem with drone", "abstract": "over the past few years , unmanned aerial vehicles ( uav ) , also known as drones , have been adopted as part of a new logistic method in the commercial sector called `` last-mile delivery '' . in this novel approach , they are deployed alongside trucks to deliver goods to customers to improve the quality of service and reduce the transportation cost . this approach gives rise to a new variant of the traveling salesman problem ( tsp ) , called tsp with drone ( tsp-d ) . a variant of this problem that aims to minimize the time at which truck and drone finish the service ( or , in other words , to maximize the quality of service ) was studied in the work of murray and chu ( 2015 ) . in contrast , this paper considers a new variant of tsp-d in which the objective is to minimize operational costs including total transportation cost and one created by waste time a vehicle has to wait for the other . the problem is first formulated mathematically . then , two algorithms are proposed for the solution . the first algorithm ( tsp-ls ) was adapted from the approach proposed by murray and chu ( 2015 ) , in which an optimal tsp solution is converted to a feasible tsp-d solution by local searches . the second algorithm , a greedy randomized adaptive search procedure ( grasp ) , is based on a new split procedure that optimally splits any tsp tour into a tsp-d solution . after a tsp-d solution has been generated , it is then improved through local search operators . numerical results obtained on various instances of both objective functions with different sizes and characteristics are presented . the results show that grasp outperforms tsp-ls in terms of solution quality under an acceptable running time .", "topics": ["heuristic"]}
{"title": "an empirical approach to temporal reference resolution", "abstract": "this paper presents the results of an empirical investigation of temporal reference resolution in scheduling dialogs . the algorithm adopted is primarily a linear-recency based approach that does not include a model of global focus . a fully automatic system has been developed and evaluated on unseen test data with good results . this paper presents the results of an intercoder reliability study , a model of temporal reference resolution that supports linear recency and has very good coverage , the results of the system evaluated on unseen test data , and a detailed analysis of the dialogs assessing the viability of the approach .", "topics": ["machine translation", "natural language"]}
{"title": "a wavelet frame coefficient total variational model for image restoration", "abstract": "in this paper , we propose a vector total variation ( vtv ) of feature image model for image restoration . the vtv imposes different smoothing powers on different features ( e.g . edges and cartoons ) based on choosing various regularization parameters . thus , the model can simultaneously preserve edges and remove noises . next , the existence of solution for the model is proved and the split bregman algorithm is used to solve the model . at last , we use the wavelet filter banks to explicitly define the feature operator and present some experimental results to show its advantage over the related methods in both quality and efficiency .", "topics": ["calculus of variations", "matrix regularization"]}
{"title": "pcg-cut : graph driven segmentation of the prostate central gland", "abstract": "prostate cancer is the most abundant cancer in men , with over 200,000 expected new cases and around 28,000 deaths in 2012 in the us alone . in this study , the segmentation results for the prostate central gland ( pcg ) in mr scans are presented . the aim of this research study is to apply a graph-based algorithm to automated segmentation ( i.e . delineation ) of organ limits for the prostate central gland . the ultimate goal is to apply automated segmentation approach to facilitate efficient mr-guided biopsy and radiation treatment planning . the automated segmentation algorithm used is graph-driven based on a spherical template . therefore , rays are sent through the surface points of a polyhedron to sample the graph 's nodes . after graph construction - which only requires the center of the polyhedron defined by the user and located inside the prostate center gland - the minimal cost closed set on the graph is computed via a polynomial time s-t-cut , which results in the segmentation of the prostate center gland 's boundaries and volume . the algorithm has been realized as a c++ modul within the medical research platform mevislab and the ground truth of the central gland boundaries were manually extracted by clinical experts ( interventional radiologists ) with several years of experience in prostate treatment . for evaluation the automated segmentations of the proposed scheme have been compared with the manual segmentations , yielding an average dice similarity coefficient ( dsc ) of 78.94 +/- 10.85 % .", "topics": ["time complexity", "ground truth"]}
{"title": "a benchmark for endoluminal scene segmentation of colonoscopy images", "abstract": "colorectal cancer ( crc ) is the third cause of cancer death worldwide . currently , the standard approach to reduce crc-related mortality is to perform regular screening in search for polyps and colonoscopy is the screening tool of choice . the main limitations of this screening procedure are polyp miss-rate and inability to perform visual assessment of polyp malignancy . these drawbacks can be reduced by designing decision support systems ( dss ) aiming to help clinicians in the different stages of the procedure by providing endoluminal scene segmentation . thus , in this paper , we introduce an extended benchmark of colonoscopy image , with the hope of establishing a new strong benchmark for colonoscopy image analysis research . we provide new baselines on this dataset by training standard fully convolutional networks ( fcn ) for semantic segmentation and significantly outperforming , without any further post-processing , prior results in endoluminal scene segmentation .", "topics": ["image segmentation"]}
{"title": "a theory of goal-oriented mdps with dead ends", "abstract": "stochastic shortest path ( ssp ) mdps is a problem class widely studied in ai , especially in probabilistic planning . they describe a wide range of scenarios but make the restrictive assumption that the goal is reachable from any state , i.e . , that dead-end states do not exist . because of this , ssps are unable to model various scenarios that may have catastrophic events ( e.g . , an airplane possibly crashing if it flies into a storm ) . even though mdp algorithms have been used for solving problems with dead ends , a principled theory of ssp extensions that would allow dead ends , including theoretically sound algorithms for solving such mdps , has been lacking . in this paper , we propose three new mdp classes that admit dead ends under increasingly weaker assumptions . we present value iteration-based as well as the more efficient heuristic search algorithms for optimally solving each class , and explore theoretical relationships between these classes . we also conduct a preliminary empirical study comparing the performance of our algorithms on different mdp classes , especially on scenarios with unavoidable dead ends .", "topics": ["mathematical optimization", "heuristic"]}
{"title": "for solving linear equations recombination is a needless operation in time-variant adaptive hybrid algorithms", "abstract": "recently hybrid evolutionary computation ( ec ) techniques are successfully implemented for solving large sets of linear equations . all the recently developed hybrid evolutionary algorithms , for solving linear equations , contain both the recombination and the mutation operations . in this paper , two modified hybrid evolutionary algorithms contained time-variant adaptive evolutionary technique are proposed for solving linear equations in which recombination operation is absent . the effectiveness of the recombination operator has been studied for the time-variant adaptive hybrid algorithms for solving large set of linear equations . several experiments have been carried out using both the proposed modified hybrid evolutionary algorithms ( in which the recombination operation is absent ) and corresponding existing hybrid algorithms ( in which the recombination operation is present ) to solve large set of linear equations . it is found that the number of generations required by the existing hybrid algorithms ( i.e . the gauss-seidel-sr based time variant adaptive ( gsbtva ) hybrid algorithm and the jacobi-sr based time variant adaptive ( jbtva ) hybrid algorithm ) and modified hybrid algorithms ( i.e . the modified gauss-seidel-sr based time variant adaptive ( mgsbtva ) hybrid algorithm and the modified jacobi-sr based time variant adaptive ( mjbtva ) hybrid algorithm ) are comparable . also the proposed modified algorithms require less amount of computational time in comparison to the corresponding existing hybrid algorithms . as the proposed modified hybrid algorithms do not contain recombination operation , so they require less computational effort , and also they are more efficient , effective and easy to implement .", "topics": ["time complexity", "computation"]}
{"title": "domain adaptation for statistical classifiers", "abstract": "the most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution . unfortunately , in many applications , the `` in-domain '' test data is drawn from a distribution that is related , but not identical , to the `` out-of-domain '' distribution of the training data . we consider the common case in which labeled out-of-domain data is plentiful , but labeled in-domain data is scarce . we introduce a statistical formulation of this problem in terms of a simple mixture model and present an instantiation of this framework to maximum entropy classifiers and their linear chain counterparts . we present efficient inference algorithms for this special case based on the technique of conditional expectation maximization . our experimental results show that our approach leads to improved performance on three real world tasks on four different data sets from the natural language processing domain .", "topics": ["test set", "statistical classification"]}
{"title": "using a diathesis model for semantic parsing", "abstract": "this paper presents a semantic parsing approach for unrestricted texts . semantic parsing is one of the major bottlenecks of natural language understanding ( nlu ) systems and usually requires building expensive resources not easily portable to other domains . our approach obtains a case-role analysis , in which the semantic roles of the verb are identified . in order to cover all the possible syntactic realisations of a verb , our system combines their argument structure with a set of general semantic labelled diatheses models . combining them , the system builds a set of syntactic-semantic patterns with their own role-case representation . once the patterns are build , we use an approximate tree pattern-matching algorithm to identify the most reliable pattern for a sentence . the pattern matching is performed between the syntactic-semantic patterns and the feature-structure tree representing the morphological , syntactical and semantic information of the analysed sentence . for sentences assigned to the correct model , the semantic parsing system we are presenting identifies correctly more than 73 % of possible semantic case-roles .", "topics": ["approximation algorithm", "natural language"]}
{"title": "multispectral palmprint recognition using a hybrid feature", "abstract": "personal identification problem has been a major field of research in recent years . biometrics-based technologies that exploit fingerprints , iris , face , voice and palmprints , have been in the center of attention to solve this problem . palmprints can be used instead of fingerprints that have been of the earliest of these biometrics technologies . a palm is covered with the same skin as the fingertips but has a larger surface , giving us more information than the fingertips . the major features of the palm are palm-lines , including principal lines , wrinkles and ridges . using these lines is one of the most popular approaches towards solving the palmprint recognition problem . another robust feature is the wavelet energy of palms . in this paper we used a hybrid feature which combines both of these features . % moreover , multispectral analysis is applied to improve the performance of the system . at the end , minimum distance classifier is used to match test images with one of the training samples . the proposed algorithm has been tested on a well-known multispectral palmprint dataset and achieved an average accuracy of 98.8\\ % .", "topics": ["eisenstein 's criterion"]}
{"title": "oasis : adaptive column sampling for kernel matrix approximation", "abstract": "kernel matrices ( e.g . gram or similarity matrices ) are essential for many state-of-the-art approaches to classification , clustering , and dimensionality reduction . for large datasets , the cost of forming and factoring such kernel matrices becomes intractable . to address this challenge , we introduce a new adaptive sampling algorithm called accelerated sequential incoherence selection ( oasis ) that samples columns without explicitly computing the entire kernel matrix . we provide conditions under which oasis is guaranteed to exactly recover the kernel matrix with an optimal number of columns selected . numerical experiments on both synthetic and real-world datasets demonstrate that oasis achieves performance comparable to state-of-the-art adaptive sampling methods at a fraction of the computational cost . the low runtime complexity of oasis and its low memory footprint enable the solution of large problems that are simply intractable using other adaptive methods .", "topics": ["sampling ( signal processing )", "kernel ( operating system )"]}
{"title": "on the evaluation and comparison of taggers : the effect of noise in testing corpora", "abstract": "this paper addresses the issue of { \\sc pos } tagger evaluation . such evaluation is usually performed by comparing the tagger output with a reference test corpus , which is assumed to be error-free . currently used corpora contain noise which causes the obtained performance to be a distortion of the real value . we analyze to what extent this distortion may invalidate the comparison between taggers or the measure of the improvement given by a new system . the main conclusion is that a more rigorous testing experimentation setting/designing is needed to reliably evaluate and compare tagger accuracies .", "topics": ["text corpus"]}
{"title": "probabilistic data analysis with probabilistic programming", "abstract": "probabilistic techniques are central to data analysis , but different approaches can be difficult to apply , combine , and compare . this paper introduces composable generative population models ( cgpms ) , a computational abstraction that extends directed graphical models and can be used to describe and compose a broad class of probabilistic data analysis techniques . examples include hierarchical bayesian models , multivariate kernel methods , discriminative machine learning , clustering algorithms , dimensionality reduction , and arbitrary probabilistic programs . we also demonstrate the integration of cgpms into bayesdb , a probabilistic programming platform that can express data analysis tasks using a modeling language and a structured query language . the practical value is illustrated in two ways . first , cgpms are used in an analysis that identifies satellite data records which probably violate kepler 's third law , by composing causal probabilistic programs with non-parametric bayes in under 50 lines of probabilistic code . second , for several representative data analysis tasks , we report on lines of code and accuracy measurements of various cgpms , plus comparisons with standard baseline solutions from python and matlab libraries .", "topics": ["baseline ( configuration management )", "graphical model"]}
{"title": "learning of proto-object representations via fixations on low resolution", "abstract": "while previous researches in eye fixation prediction typically rely on integrating low-level features ( e.g . color , edge ) to form a saliency map , recently it has been found that the structural organization of these features into a proto-object representation can play a more significant role . in this work , we present a computational framework based on deep network to demonstrate that proto-object representations can be learned from low-resolution image patches from fixation regions . we advocate the use of low-resolution inputs in this work due to the following reasons : ( 1 ) proto-objects are computed in parallel over an entire visual field ( 2 ) people can perceive or recognize objects well even it is in low resolution . ( 3 ) fixations from lower resolution images can predict fixations on higher resolution images . in the proposed computational model , we extract multi-scale image patches on fixation regions from eye fixation datasets , resize them to low resolution and feed them into a hierarchical . with layer-wise unsupervised feature learning , we find that many proto-objects like features responsive to different shapes of object blobs are learned out . visualizations also show that these features are selective to potential objects in the scene and the responses of these features work well in predicting eye fixations on the images when combined with learned weights .", "topics": ["feature learning", "high- and low-level"]}
{"title": "optimistic simulated exploration as an incentive for real exploration", "abstract": "many reinforcement learning exploration techniques are overly optimistic and try to explore every state . such exploration is impossible in environments with the unlimited number of states . i propose to use simulated exploration with an optimistic model to discover promising paths for real exploration . this reduces the needs for the real exploration .", "topics": ["reinforcement learning", "simulation"]}
{"title": "end-to-end learning for structured prediction energy networks", "abstract": "structured prediction energy networks ( spens ) are a simple , yet expressive family of structured prediction models ( belanger and mccallum , 2016 ) . an energy function over candidate structured outputs is given by a deep network , and predictions are formed by gradient-based optimization . this paper presents end-to-end learning for spens , where the energy function is discriminatively trained by back-propagating through gradient-based prediction . in our experience , the approach is substantially more accurate than the structured svm method of belanger and mccallum ( 2016 ) , as it allows us to use more sophisticated non-convex energies . we provide a collection of techniques for improving the speed , accuracy , and memory requirements of end-to-end spens , and demonstrate the power of our method on 7-scenes image denoising and conll-2005 semantic role labeling tasks . in both , inexact minimization of non-convex spen energies is superior to baseline methods that use simplistic energy functions that can be minimized exactly .", "topics": ["baseline ( configuration management )", "mathematical optimization"]}
{"title": "novel sensor scheduling scheme for intruder tracking in energy efficient sensor networks", "abstract": "we consider the problem of tracking an intruder using a network of wireless sensors . for tracking the intruder at each instant , the optimal number and the right configuration of sensors has to be powered . as powering the sensors consumes energy , there is a trade off between accurately tracking the position of the intruder at each instant and the energy consumption of sensors . this problem has been formulated in the framework of partially observable markov decision process ( pomdp ) . even for the state-of-the-art algorithm in the literature , the curse of dimensionality renders the problem intractable . in this paper , we formulate the intrusion detection ( id ) problem with a suitable state-action space in the framework of pomdp and develop a reinforcement learning ( rl ) algorithm utilizing the upper confidence tree search ( uct ) method to solve the id problem . through simulations , we show that our algorithm performs and scales well with the increasing state and action spaces .", "topics": ["reinforcement learning", "simulation"]}
{"title": "a machine learning approach for evaluating creative artifacts", "abstract": "much work has been done in understanding human creativity and defining measures to evaluate creativity . this is necessary mainly for the reason of having an objective and automatic way of quantifying creative artifacts . in this work , we propose a regression-based learning framework which takes into account quantitatively the essential criteria for creativity like novelty , influence , value and unexpectedness . as it is often the case with most creative domains , there is no clear ground truth available for creativity . our proposed learning framework is applicable to all creative domains ; yet we evaluate it on a dataset of movies created from imdb and rotten tomatoes due to availability of audience and critic scores , which can be used as proxy ground truth labels for creativity . we report promising results and observations from our experiments in the following ways : 1 ) correlation of creative criteria with critic scores , 2 ) improvement in movie rating prediction with inclusion of various creative criteria , and 3 ) identification of creative movies .", "topics": ["ground truth"]}
{"title": "data programming : creating large training sets , quickly", "abstract": "large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques . for some applications , creating labeled training sets is the most time-consuming and expensive part of applying machine learning . we therefore propose a paradigm for the programmatic creation of training sets called data programming in which users express weak supervision strategies or domain heuristics as labeling functions , which are programs that label subsets of the data , but that are noisy and may conflict . we show that by explicitly representing this training set labeling process as a generative model , we can `` denoise '' the generated training set , and establish theoretically that we can recover the parameters of these generative models in a handful of settings . we then show how to modify a discriminative loss function to make it noise-aware , and demonstrate our method over a range of discriminative models including logistic regression and lstms . experimentally , on the 2014 tac-kbp slot filling challenge , we show that data programming would have led to a new winning score , and also show that applying data programming to an lstm model leads to a tac-kbp score almost 6 f1 points over a state-of-the-art lstm baseline ( and into second place in the competition ) . additionally , in initial user studies we observed that data programming may be an easier way for non-experts to create machine learning models when training data is limited or unavailable .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "adversarial vulnerability for any classifier", "abstract": "despite achieving impressive and often superhuman performance on multiple benchmarks , state-of-the-art deep networks remain highly vulnerable to perturbations : adding small , imperceptible , adversarial perturbations can lead to very high error rates . provided the data distribution is defined using a generative model mapping latent vectors to datapoints in the distribution , we prove that no classifier can be robust to adversarial perturbations when the latent space is sufficiently large and the generative model sufficiently smooth . under the same conditions , we prove the existence of adversarial perturbations that transfer well across different models with small risk . we conclude the paper with experiments validating the theoretical bounds .", "topics": ["generative model"]}
{"title": "highly scalable tensor factorization for prediction of drug-protein interaction type", "abstract": "the understanding of the type of inhibitory interaction plays an important role in drug design . therefore , researchers are interested to know whether a drug has competitive or non-competitive interaction to particular protein targets . method : to analyze the interaction types we propose factorization method macau which allows us to combine different measurement types into a single tensor together with proteins and compounds . the compounds are characterized by high dimensional 2d ecfp fingerprints . the novelty of the proposed method is that using a specially designed noise injection mcmc sampler it can incorporate high dimensional side information , i.e . , millions of unique 2d ecfp compound features , even for large scale datasets of millions of compounds . without the side information , in this case , the tensor factorization would be practically futile . results : using public ic50 and ki data from chembl we trained a model from where we can identify the latent subspace separating the two measurement types ( ic50 and ki ) . the results suggest the proposed method can detect the competitive inhibitory activity between compounds and proteins .", "topics": ["sampling ( signal processing )"]}
{"title": "bayesian structure learning for markov random fields with a spike and slab prior", "abstract": "in recent years a number of methods have been developed for automatically learning the ( sparse ) connectivity structure of markov random fields . these methods are mostly based on l1-regularized optimization which has a number of disadvantages such as the inability to assess model uncertainty and expensive crossvalidation to find the optimal regularization parameter . moreover , the model 's predictive performance may degrade dramatically with a suboptimal value of the regularization parameter ( which is sometimes desirable to induce sparseness ) . we propose a fully bayesian approach based on a `` spike and slab '' prior ( similar to l0 regularization ) that does not suffer from these shortcomings . we develop an approximate mcmc method combining langevin dynamics and reversible jump mcmc to conduct inference in this model . experiments show that the proposed model learns a good combination of the structure and parameter values without the need for separate hyper-parameter tuning . moreover , the model 's predictive performance is much more robust than l1-based methods with hyper-parameter settings that induce highly sparse model structures .", "topics": ["approximation algorithm", "value ( ethics )"]}
{"title": "aesthetic-driven image enhancement by adversarial learning", "abstract": "we introduce enhancegan , an adversarial learning based model that performs automatic image enhancement . traditional image enhancement frameworks involve training separate models for automatic cropping or color enhancement in a fully-supervised manner , which requires expensive annotations in the form of image pairs . in contrast to these approaches , our proposed enhancegan only requires weak supervision ( binary labels on image aesthetic quality ) and is able to learn enhancement parameters for tasks including image cropping and color enhancement . the full differentiability of our image enhancement modules enables training the proposed enhancegan in an end-to-end manner . a novel stage-wise learning scheme is further proposed to stabilize the training of each enhancement task and facilitate the extensibility for other image enhancement techniques . our weakly-supervised enhancegan reports competitive quantitative results against supervised models in automatic image cropping using standard benchmarking datasets , and a user study confirms that the images enhancement results are on par with or even preferred over professional enhancement .", "topics": ["image processing"]}
{"title": "fusion of hyperspectral and panchromatic images using spectral uumixing results", "abstract": "hyperspectral imaging , due to providing high spectral resolution images , is one of the most important tools in the remote sensing field . because of technological restrictions hyperspectral sensors has a limited spatial resolution . on the other hand panchromatic image has a better spatial resolution . combining this information together can provide a better understanding of the target scene . spectral unmixing of mixed pixels in hyperspectral images results in spectral signature and abundance fractions of endmembers but gives no information about their location in a mixed pixel . in this paper we have used spectral unmixing results of hyperspectral images and segmentation results of panchromatic image for data fusion . the proposed method has been applied on simulated data using avris indian pines datasets . results show that this method can effectively combine information in hyperspectral and panchromatic images .", "topics": ["simulation", "sensor"]}
{"title": "posterior concentration for sparse deep learning", "abstract": "spike-and-slab deep learning ( ss-dl ) is a fully bayesian alternative to dropout for improving generalizability of deep relu networks . this new type of regularization enables provable recovery of smooth input-output maps with unknown levels of smoothness . indeed , we show that the posterior distribution concentrates at the near minimax rate for $ \\alpha $ -h\\ '' older smooth maps , performing as well as if we knew the smoothness level $ \\alpha $ ahead of time . our result sheds light on architecture design for deep neural networks , namely the choice of depth , width and sparsity level . these network attributes typically depend on unknown smoothness in order to be optimal . we obviate this constraint with the fully bayes construction . as an aside , we show that ss-dl does not overfit in the sense that the posterior concentrates on smaller networks with fewer ( up to the optimal number of ) nodes and links . our results provide new theoretical justifications for deep relu networks from a bayesian point of view .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "inference and learning in probabilistic logic programs using weighted boolean formulas", "abstract": "probabilistic logic programs are logic programs in which some of the facts are annotated with probabilities . this paper investigates how classical inference and learning tasks known from the graphical model community can be tackled for probabilistic logic programs . several such tasks such as computing the marginals given evidence and learning from ( partial ) interpretations have not really been addressed for probabilistic logic programs before . the first contribution of this paper is a suite of efficient algorithms for various inference tasks . it is based on a conversion of the program and the queries and evidence to a weighted boolean formula . this allows us to reduce the inference tasks to well-studied tasks such as weighted model counting , which can be solved using state-of-the-art methods known from the graphical model and knowledge compilation literature . the second contribution is an algorithm for parameter estimation in the learning from interpretations setting . the algorithm employs expectation maximization , and is built on top of the developed inference algorithms . the proposed approach is experimentally evaluated . the results show that the inference algorithms improve upon the state-of-the-art in probabilistic logic programming and that it is indeed possible to learn the parameters of a probabilistic logic program from interpretations .", "topics": ["graphical model"]}
{"title": "the biodynamo project : a platform for computer simulations of biological dynamics", "abstract": "this paper is a brief update on developments in the biodynamo project , a new platform for computer simulations for biological research . we will discuss the new capabilities of the simulator , important new concepts simulation methodology as well as its numerous applications to the computational biology and nanoscience communities .", "topics": ["simulation"]}
{"title": "neural sentence ordering", "abstract": "sentence ordering is a general and critical task for natural language generation applications . previous works have focused on improving its performance in an external , downstream task , such as multi-document summarization . given its importance , we propose to study it as an isolated task . we collect a large corpus of academic texts , and derive a data driven approach to learn pairwise ordering of sentences , and validate the efficacy with extensive experiments . source codes and dataset of this paper will be made publicly available .", "topics": ["natural language"]}
{"title": "parsing for semidirectional lambek grammar is np-complete", "abstract": "we study the computational complexity of the parsing problem of a variant of lambek categorial grammar that we call { \\em semidirectional } . in semidirectional lambek calculus $ \\sdl $ there is an additional non-directional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent 's left-hand side , thus permitting non-peripheral extraction . $ \\sdl $ grammars are able to generate each context-free language and more than that . we show that the parsing problem for semidirectional lambek grammar is np-complete by a reduction of the 3-partition problem .", "topics": ["computational complexity theory", "parsing"]}
{"title": "belief hierarchical clustering", "abstract": "in the data mining field many clustering methods have been proposed , yet standard versions do not take into account uncertain databases . this paper deals with a new approach to cluster uncertain data by using a hierarchical clustering defined within the belief function framework . the main objective of the belief hierarchical clustering is to allow an object to belong to one or several clusters . to each belonging , a degree of belief is associated , and clusters are combined based on the pignistic properties . experiments with real uncertain data show that our proposed method can be considered as a propitious tool .", "topics": ["cluster analysis", "data mining"]}
{"title": "grad-cam++ : generalized gradient-based visual explanations for deep convolutional networks", "abstract": "over the last decade , convolutional neural network ( cnn ) models have been highly successful in solving complex vision based problems . however , deep models are perceived as `` black box '' methods considering the lack of understanding of their internal functioning . there has been a significant recent interest to develop explainable deep learning models , and this paper is an effort in this direction . building on a recently proposed method called grad-cam , we propose grad-cam++ to provide better visual explanations of cnn model predictions ( when compared to grad-cam ) , in terms of better localization of objects as well as explaining occurrences of multiple objects of a class in a single image . we provide a mathematical explanation for the proposed method , grad-cam++ , which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps with respect to a specific class score as weights to generate a visual explanation for the class label under consideration . our extensive experiments and evaluations , both subjective and objective , on standard datasets showed that grad-cam++ indeed provides better visual explanations for a given cnn architecture when compared to grad-cam .", "topics": ["map", "gradient"]}
{"title": "handwritten devanagari script segmentation : a non-linear fuzzy approach", "abstract": "the paper concentrates on improvement of segmentation accuracy by addressing some of the key challenges of handwritten devanagari word image segmentation technique . in the present work , we have developed a new feature based approach for identification of matra pixels from a word image , design of a non-linear fuzzy membership functions for headline estimation and finally design of a non-linear fuzzy functions for identifying segmentation points on the matra . the segmentation accuracy achieved by the current technique is 94.8 % . this shows an improvement of performance by 1.8 % over the previous technique [ 1 ] on a 300-word dataset , used for the current experiment .", "topics": ["image segmentation", "nonlinear system"]}
{"title": "on the runtime of randomized local search and simple evolutionary algorithms for dynamic makespan scheduling", "abstract": "evolutionary algorithms have been frequently used for dynamic optimization problems . with this paper , we contribute to the theoretical understanding of this research area . we present the first computational complexity analysis of evolutionary algorithms for a dynamic variant of a classical combinatorial optimization problem , namely makespan scheduling . we study the model of a strong adversary which is allowed to change one job at regular intervals . furthermore , we investigate the setting of random changes . our results show that randomized local search and a simple evolutionary algorithm are very effective in dynamically tracking changes made to the problem instance .", "topics": ["optimization problem", "computational complexity theory"]}
{"title": "enhanced factored three-way restricted boltzmann machines for speech detection", "abstract": "in this letter , we propose enhanced factored three way restricted boltzmann machines ( eftw-rbms ) for speech detection . the proposed model incorporates conditional feature learning by multiplying the dynamical state of the third unit , which allows a modulation over the visible-hidden node pairs . instead of stacking previous frames of speech as the third unit in a recursive manner , the correlation related weighting coefficients are assigned to the contextual neighboring frames . specifically , a threshold function is designed to capture the long-term features and blend the globally stored speech structure . a factored low rank approximation is introduced to reduce the parameters of the three-dimensional interaction tensor , on which non-negative constraint is imposed to address the sparsity characteristic . the validations through the area-under-roc-curve ( auc ) and signal distortion ratio ( sdr ) show that our approach outperforms several existing 1d and 2d ( i.e . , time and time-frequency domain ) speech detection algorithms in various noisy environments .", "topics": ["feature learning", "sparse matrix"]}
{"title": "k-modulus method for image transformation", "abstract": "in this paper , we propose a new algorithm to make a novel spatial image transformation . the proposed approach aims to reduce the bit depth used for image storage . the basic technique for the proposed transformation is based of the modulus operator . the goal is to transform the whole image into multiples of predefined integer . the division of the whole image by that integer will guarantee that the new image surely less in size from the original image . the k-modulus method could not be used as a stand alone transform for image compression because of its high compression ratio . it could be used as a scheme embedded in other image processing fields especially compression . according to its high psnr value , it could be amalgamated with other methods to facilitate the redundancy criterion .", "topics": ["image processing", "eisenstein 's criterion"]}
{"title": "ct image denoising with perceptive deep neural networks", "abstract": "increasing use of ct in modern medical practice has raised concerns over associated radiation dose . reduction of radiation dose associated with ct can increase noise and artifacts , which can adversely affect diagnostic confidence . denoising of low-dose ct images on the other hand can help improve diagnostic confidence , which however is a challenging problem due to its ill-posed nature , since one noisy image patch may correspond to many different output patches . in the past decade , machine learning based approaches have made quite impressive progress in this direction . however , most of those methods , including the recently popularized deep learning techniques , aim for minimizing mean-squared-error ( mse ) between a denoised ct image and the ground truth , which results in losing important structural details due to over-smoothing , although the psnr based performance measure looks great . in this work , we introduce a new perceptual similarity measure as the objective function for a deep convolutional neural network to facilitate ct image denoising . instead of directly computing mse for pixel-to-pixel intensity loss , we compare the perceptual features of a denoised output against those of the ground truth in a feature space . therefore , our proposed method is capable of not only reducing the image noise levels , but also keeping the critical structural information at the same time . promising results have been obtained in our experiments with a large number of ct images .", "topics": ["feature vector", "noise reduction"]}
{"title": "shape-based plagiarism detection for flowchart figures in texts", "abstract": "plagiarism detection is well known phenomenon in the academic arena . copying other people is considered as serious offence that needs to be checked . there are many plagiarism detection systems such as turn-it-in that has been developed to provide this checks . most , if not all , discard the figures and charts before checking for plagiarism . discarding the figures and charts results in look holes that people can take advantage . that means people can plagiarized figures and charts easily without the current plagiarism systems detecting it . there are very few papers which talks about flowcharts plagiarism detection . therefore , there is a need to develop a system that will detect plagiarism in figures and charts . this paper presents a method for detecting flow chart figure plagiarism based on shape-based image processing and multimedia retrieval . the method managed to retrieve flowcharts with ranked similarity according to different matching sets .", "topics": ["image processing"]}
{"title": "deep learning for real time crime forecasting", "abstract": "accurate real time crime prediction is a fundamental issue for public safety , but remains a challenging problem for the scientific community . crime occurrences depend on many complex factors . compared to many predictable events , crime is sparse . at different spatio-temporal scales , crime distributions display dramatically different patterns . these distributions are of very low regularity in both space and time . in this work , we adapt the state-of-the-art deep learning spatio-temporal predictor , st-resnet [ zhang et al , aaai , 2017 ] , to collectively predict crime distribution over the los angeles area . our models are two staged . first , we preprocess the raw crime data . this includes regularization in both space and time to enhance predictable signals . second , we adapt hierarchical structures of residual convolutional units to train multi-factor crime prediction models . experiments over a half year period in los angeles reveal highly accurate predictive power of our models .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "generating sentiment lexicons for german twitter", "abstract": "despite a substantial progress made in developing new sentiment lexicon generation ( slg ) methods for english , the task of transferring these approaches to other languages and domains in a sound way still remains open . in this paper , we contribute to the solution of this problem by systematically comparing semi-automatic translations of common english polarity lists with the results of the original automatic slg algorithms , which were applied directly to german data . we evaluate these lexicons on a corpus of 7,992 manually annotated tweets . in addition to that , we also collate the results of dictionary- and corpus-based slg methods in order to find out which of these paradigms is better suited for the inherently noisy domain of social media . our experiments show that semi-automatic translations notably outperform automatic systems ( reaching a macro-averaged f1-score of 0.589 ) , and that dictionary-based techniques produce much better polarity lists as compared to corpus-based approaches ( whose best f1-scores run up to 0.479 and 0.419 respectively ) even for the non-standard twitter genre .", "topics": ["text corpus", "dictionary"]}
{"title": "learning joint multilingual sentence representations with neural machine translation", "abstract": "in this paper , we use the framework of neural machine translation to learn joint sentence representations across six very different languages . our aim is that a representation which is independent of the language , is likely to capture the underlying semantics . we define a new cross-lingual similarity measure , compare up to 1.4m sentence representations and study the characteristics of close sentences . we provide experimental evidence that sentences that are close in embedding space are indeed semantically highly related , but often have quite different structure and syntax . these relations also hold when comparing sentences in different languages .", "topics": ["machine translation"]}
{"title": "heuristic search as evidential reasoning", "abstract": "bps , the bayesian problem solver , applies probabilistic inference and decision-theoretic control to flexible , resource-constrained problem-solving . this paper focuses on the bayesian inference mechanism in bps , and contrasts it with those of traditional heuristic search techniques . by performing sound inference , bps can outperform traditional techniques with significantly less computational effort . empirical tests on the eight puzzle show that after only a few hundred node expansions , bps makes better decisions than does the best existing algorithm after several million node expansions", "topics": ["heuristic"]}
{"title": "broad learning for healthcare", "abstract": "a broad spectrum of data from different modalities are generated in the healthcare domain every day , including scalar data ( e.g . , clinical measures collected at hospitals ) , tensor data ( e.g . , neuroimages analyzed by research institutes ) , graph data ( e.g . , brain connectivity networks ) , and sequence data ( e.g . , digital footprints recorded on smart sensors ) . capability for modeling information from these heterogeneous data sources is potentially transformative for investigating disease mechanisms and for informing therapeutic interventions . our works in this thesis attempt to facilitate healthcare applications in the setting of broad learning which focuses on fusing heterogeneous data sources for a variety of synergistic knowledge discovery and machine learning tasks . we are generally interested in computer-aided diagnosis , precision medicine , and mobile health by creating accurate user profiles which include important biomarkers , brain connectivity patterns , and latent representations . in particular , our works involve four different data mining problems with application to the healthcare domain : multi-view feature selection , subgraph pattern mining , brain network embedding , and multi-view sequence prediction .", "topics": ["data mining", "sensor"]}
{"title": "modeling latent variable uncertainty for loss-based learning", "abstract": "we consider the problem of parameter estimation using weakly supervised datasets , where a training sample consists of the input and a partially specified annotation , which we refer to as the output . the missing information in the annotation is modeled using latent variables . previous methods overburden a single distribution with two separate tasks : ( i ) modeling the uncertainty in the latent variables during training ; and ( ii ) making accurate predictions for the output and the latent variables during testing . we propose a novel framework that separates the demands of the two tasks using two distributions : ( i ) a conditional distribution to model the uncertainty of the latent variables for a given input-output pair ; and ( ii ) a delta distribution to predict the output and the latent variables for a given input . during learning , we encourage agreement between the two distributions by minimizing a loss-based dissimilarity coefficient . our approach generalizes latent svm in two important ways : ( i ) it models the uncertainty over latent variables instead of relying on a pointwise estimate ; and ( ii ) it allows the use of loss functions that depend on latent variables , which greatly increases its applicability . we demonstrate the efficacy of our approach on two challenging problems -- -object detection and action detection -- -using publicly available datasets .", "topics": ["supervised learning", "support vector machine"]}
{"title": "object detection through exploration with a foveated visual field", "abstract": "we present a foveated object detector ( fod ) as a biologically-inspired alternative to the sliding window ( sw ) approach which is the dominant method of search in computer vision object detection . similar to the human visual system , the fod has higher resolution at the fovea and lower resolution at the visual periphery . consequently , more computational resources are allocated at the fovea and relatively fewer at the periphery . the fod processes the entire scene , uses retino-specific object detection classifiers to guide eye movements , aligns its fovea with regions of interest in the input image and integrates observations across multiple fixations . our approach combines modern object detectors from computer vision with a recent model of peripheral pooling regions found at the v1 layer of the human visual system . we assessed various eye movement strategies on the pascal voc 2007 dataset and show that the fod performs on par with the sw detector while bringing significant computational cost savings .", "topics": ["object detection"]}
{"title": "benchmarking super-resolution algorithms on real data", "abstract": "over the past decades , various super-resolution ( sr ) techniques have been developed to enhance the spatial resolution of digital images . despite the great number of methodical contributions , there is still a lack of comparative validations of sr under practical conditions , as capturing real ground truth data is a challenging task . therefore , current studies are either evaluated 1 ) on simulated data or 2 ) on real data without a pixel-wise ground truth . to facilitate comprehensive studies , this paper introduces the publicly available super-resolution erlangen ( super ) database that includes real low-resolution images along with high-resolution ground truth data . our database comprises image sequences with more than 20k images captured from 14 scenes under various types of motions and photometric conditions . the datasets cover four spatial resolution levels using camera hardware binning . with this database , we benchmark 15 single-image and multi-frame sr algorithms . our experiments quantitatively analyze sr accuracy and robustness under realistic conditions including independent object and camera motion or photometric variations .", "topics": ["simulation", "ground truth"]}
{"title": "rancor : non-linear image registration with total variation regularization", "abstract": "optimization techniques have been widely used in deformable registration , allowing for the incorporation of similarity metrics with regularization mechanisms . these regularization mechanisms are designed to mitigate the effects of trivial solutions to ill-posed registration problems and to otherwise ensure the resulting deformation fields are well-behaved . this paper introduces a novel deformable registration algorithm , rancor , which uses iterative convexification to address deformable registration problems under total-variation regularization . initial comparative results against four state-of-the-art registration algorithms are presented using the internet brain segmentation repository ( ibsr ) database .", "topics": ["matrix regularization"]}
{"title": "big data regression using tree based segmentation", "abstract": "scaling regression to large datasets is a common problem in many application areas . we propose a two step approach to scaling regression to large datasets . using a regression tree ( cart ) to segment the large dataset constitutes the first step of this approach . the second step of this approach is to develop a suitable regression model for each segment . since segment sizes are not very large , we have the ability to apply sophisticated regression techniques if required . a nice feature of this two step approach is that it can yield models that have good explanatory power as well as good predictive performance . ensemble methods like gradient boosted trees can offer excellent predictive performance but may not provide interpretable models . in the experiments reported in this study , we found that the predictive performance of the proposed approach matched the predictive performance of gradient boosted trees .", "topics": ["gradient"]}
{"title": "a finite element computational framework for active contours on graphs", "abstract": "in this paper we present a new framework for the solution of active contour models on graphs . with the use of the finite element method we generalize active contour models on graphs and reduce the problem from a partial differential equation to the solution of a sparse non-linear system . additionally , we extend the proposed framework to solve models where the curve evolution is locally constrained around its current location . based on the previous extension , we propose a fast algorithm for the solution of a wide range active contour models . last , we present a supervised extension of geodesic active contours for image segmentation and provide experimental evidence for the effectiveness of our framework .", "topics": ["image segmentation", "nonlinear system"]}
{"title": "unsupervised learning and segmentation of complex activities from video", "abstract": "this paper presents a new method for unsupervised segmentation of complex activities from video into multiple steps , or sub-activities , without any textual input . we propose an iterative discriminative-generative approach which alternates between discriminatively learning the appearance of sub-activities from the videos ' visual features to sub-activity labels and generatively modelling the temporal structure of sub-activities using a generalized mallows model . in addition , we introduce a model for background to account for frames unrelated to the actual activities . our approach is validated on the challenging breakfast actions and inria instructional videos datasets and outperforms both unsupervised and weakly-supervised state of the art .", "topics": ["generative model", "unsupervised learning"]}
{"title": "ebic : an artificial intelligence-based parallel biclustering algorithm for pattern discovery", "abstract": "in this paper a novel biclustering algorithm based on artificial intelligence ( ai ) is introduced . the method called ebic aims to detect biologically meaningful , order-preserving patterns in complex data . the proposed algorithm is probably the first one capable of discovering with accuracy exceeding 50\\ % multiple complex patterns in real gene expression datasets . it is also one of the very few biclustering methods designed for parallel environments with multiple graphics processing units ( gpus ) . we demonstrate that ebic outperforms state-of-the-art biclustering methods , in terms of recovery and relevance , on both synthetic and genetic datasets . ebic also yields results over 12 times faster than the most accurate reference algorithms . the proposed algorithm is anticipated to be added to the repertoire of unsupervised machine learning algorithms for the analysis of datasets , including those from large-scale genomic studies .", "topics": ["synthetic data", "relevance"]}
{"title": "convolution by evolution : differentiable pattern producing networks", "abstract": "in this work we introduce a differentiable version of the compositional pattern producing network , called the dppn . unlike a standard cppn , the topology of a dppn is evolved but the weights are learned . a lamarckian algorithm , that combines evolution and learning , produces dppns to reconstruct an image . our main result is that dppns can be evolved/trained to compress the weights of a denoising autoencoder from 157684 to roughly 200 parameters , while achieving a reconstruction accuracy comparable to a fully connected network with more than two orders of magnitude more parameters . the regularization ability of the dppn allows it to rediscover ( approximate ) convolutional network architectures embedded within a fully connected architecture . such convolutional architectures are the current state of the art for many computer vision applications , so it is satisfying that dppns are capable of discovering this structure rather than having to build it in by design . dppns exhibit better generalization when tested on the omniglot dataset after being trained on mnist , than directly encoded fully connected autoencoders . dppns are therefore a new framework for integrating learning and evolution .", "topics": ["noise reduction", "computer vision"]}
{"title": "diffusion component analysis : unraveling functional topology in biological networks", "abstract": "complex biological systems have been successfully modeled by biochemical and genetic interaction networks , typically gathered from high-throughput ( htp ) data . these networks can be used to infer functional relationships between genes or proteins . using the intuition that the topological role of a gene in a network relates to its biological function , local or diffusion based `` guilt-by-association '' and graph-theoretic methods have had success in inferring gene functions . here we seek to improve function prediction by integrating diffusion-based methods with a novel dimensionality reduction technique to overcome the incomplete and noisy nature of network data . in this paper , we introduce diffusion component analysis ( dca ) , a framework that plugs in a diffusion model and learns a low-dimensional vector representation of each node to encode the topological properties of a network . as a proof of concept , we demonstrate dca 's substantial improvement over state-of-the-art diffusion-based approaches in predicting protein function from molecular interaction networks . moreover , our dca framework can integrate multiple networks from heterogeneous sources , consisting of genomic information , biochemical experiments and other resources , to even further improve function prediction . yet another layer of performance gain is achieved by integrating the dca framework with support vector machines that take our node vector representations as features . overall , our dca framework provides a novel representation of nodes in a network that can be used as a plug-in architecture to other machine learning algorithms to decipher topological properties of and obtain novel insights into interactomes .", "topics": ["support vector machine", "support vector machine"]}
{"title": "deep variational bayes filters : unsupervised learning of state space models from raw data", "abstract": "we introduce deep variational bayes filters ( dvbf ) , a new method for unsupervised learning and identification of latent markovian state space models . leveraging recent advances in stochastic gradient variational bayes , dvbf can overcome intractable inference distributions via variational inference . thus , it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge . our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding . this also enables realistic long-term prediction .", "topics": ["calculus of variations", "unsupervised learning"]}
{"title": "arabic language text classification using dependency syntax-based feature selection", "abstract": "we study the performance of arabic text classification combining various techniques : ( a ) tfidf vs. dependency syntax , for feature selection and weighting ; ( b ) class association rules vs. support vector machines , for classification . the arabic text is used in two forms : rootified and lightly stemmed . the results we obtain show that lightly stemmed text leads to better performance than rootified text ; that class association rules are better suited for small feature sets obtained by dependency syntax constraints ; and , finally , that support vector machines are better suited for large feature sets based on morphological feature selection criteria .", "topics": ["support vector machine"]}
{"title": "colour constancy : biologically-inspired contrast variant pooling mechanism", "abstract": "pooling is a ubiquitous operation in image processing algorithms that allows for higher-level processes to collect relevant low-level features from a region of interest . currently , max-pooling is one of the most commonly used operators in the computational literature . however , it can lack robustness to outliers due to the fact that it relies merely on the peak of a function . pooling mechanisms are also present in the primate visual cortex where neurons of higher cortical areas pool signals from lower ones . the receptive fields of these neurons have been shown to vary according to the contrast by aggregating signals over a larger region in the presence of low contrast stimuli . we hypothesise that this contrast-variant-pooling mechanism can address some of the shortcomings of max-pooling . we modelled this contrast variation through a histogram clipping in which the percentage of pooled signal is inversely proportional to the local contrast of an image . we tested our hypothesis by applying it to the phenomenon of colour constancy where a number of popular algorithms utilise a max-pooling step ( e.g . white-patch , grey-edge and double-opponency ) . for each of these methods , we investigated the consequences of replacing their original max-pooling by the proposed contrast-variant-pooling . our experiments on three colour constancy benchmark datasets suggest that previous results can significantly improve by adopting a contrast-variant-pooling mechanism .", "topics": ["image processing", "high- and low-level"]}
{"title": "teaching deep convolutional neural networks to play go", "abstract": "mastering the game of go has remained a long standing challenge to the field of ai . modern computer go systems rely on processing millions of possible future positions to play well , but intuitively a stronger and more 'humanlike ' way to play the game would be to rely on pattern recognition abilities rather then brute force computation . following this sentiment , we train deep convolutional neural networks to play go by training them to predict the moves made by expert go players . to solve this problem we introduce a number of novel techniques , including a method of tying weights in the network to 'hard code ' symmetries that are expect to exist in the target function , and demonstrate in an ablation study they considerably improve performance . our final networks are able to achieve move prediction accuracies of 41.1 % and 44.4 % on two different go datasets , surpassing previous state of the art on this task by significant margins . additionally , while previous move prediction programs have not yielded strong go playing programs , we show that the networks trained in this work acquired high levels of skill . our convolutional neural networks can consistently defeat the well known go program gnu go , indicating it is state of the art among programs that do not use monte carlo tree search . it is also able to win some games against state of the art go playing program fuego while using a fraction of the play time . this success at playing go indicates high level principles of the game were learned .", "topics": ["computation"]}
{"title": "relevant based structure learning for feature selection", "abstract": "feature selection is an important task in many problems occurring in pattern recognition , bioinformatics , machine learning and data mining applications . the feature selection approach enables us to reduce the computation burden and the falling accuracy effect of dealing with huge number of features in typical learning problems . there is a variety of techniques for feature selection in supervised learning problems based on different selection metrics . in this paper , we propose a novel unified framework for feature selection built on the graphical models and information theoretic tools . the proposed approach exploits the structure learning among features to select more relevant and less redundant features to the predictive modeling problem according to a primary novel likelihood based criterion . in line with the selection of the optimal subset of features through the proposed method , it provides us the bayesian network classifier without the additional cost of model training on the selected subset of features . the optimal properties of our method are established through empirical studies and computational complexity analysis . furthermore the proposed approach is evaluated on a bunch of benchmark datasets based on the well-known classification algorithms . extensive experiments confirm the significant improvement of the proposed approach compared to the earlier works .", "topics": ["data mining", "supervised learning"]}
{"title": "what do recurrent neural network grammars learn about syntax ?", "abstract": "recurrent neural network grammars ( rnng ) are a recently proposed probabilistic generative modeling family for natural language . they show state-of-the-art language modeling and parsing performance . we investigate what information they learn , from a linguistic perspective , through various ablations to the model and the data , and by augmenting the model with an attention mechanism ( ga-rnng ) to enable closer inspection . we find that explicit modeling of composition is crucial for achieving the best performance . through the attention mechanism , we find that headedness plays a central role in phrasal representation ( with the model 's latent attention largely agreeing with predictions made by hand-crafted head rules , albeit with some important differences ) . by training grammars without nonterminal labels , we find that phrasal representations depend minimally on nonterminals , providing support for the endocentricity hypothesis .", "topics": ["recurrent neural network", "natural language"]}
{"title": "demand-driven clustering in relational domains for predicting adverse drug events", "abstract": "learning from electronic medical records ( emr ) is challenging due to their relational nature and the uncertain dependence between a patient 's past and future health status . statistical relational learning is a natural fit for analyzing emrs but is less adept at handling their inherent latent structure , such as connections between related medications or diseases . one way to capture the latent structure is via a relational clustering of objects . we propose a novel approach that , instead of pre-clustering the objects , performs a demand-driven clustering during learning . we evaluate our algorithm on three real-world tasks where the goal is to use emrs to predict whether a patient will have an adverse reaction to a medication . we find that our approach is more accurate than performing no clustering , pre-clustering , and using expert-constructed medical heterarchies .", "topics": ["cluster analysis"]}
{"title": "reconstruction of enhanced ultrasound images from compressed measurements using simultaneous direction method of multipliers", "abstract": "high resolution ultrasound image reconstruction from a reduced number of measurements is of great interest in ultrasound imaging , since it could enhance both the frame rate and image resolution . compressive deconvolution , combining compressed sensing and image deconvolution , represents an interesting possibility to consider this challenging task . the model of compressive deconvolution includes , in addition to the compressive sampling matrix , a 2d convolution operator carrying the information on the system point spread function . through this model , the resolution of reconstructed ultrasound images from compressed measurements mainly depends on three aspects : the acquisition setup , i.e . the incoherence of the sampling matrix , the image regularization , i.e . the sparsity prior , and the optimization technique . in this paper , we mainly focused on the last two aspects . we proposed a novel simultaneous direction method of multipliers-based optimization scheme to invert the linear model , including two regularization terms expressing the sparsity of the rf images in a given basis and the generalized gaussian statistical assumption on tissue reflectivity functions . the performance of the method is evaluated on both simulated and in vivo data .", "topics": ["sampling ( signal processing )", "matrix regularization"]}
{"title": "action recognition with coarse-to-fine deep feature integration and asynchronous fusion", "abstract": "action recognition is an important yet challenging task in computer vision . in this paper , we propose a novel deep-based framework for action recognition , which improves the recognition accuracy by : 1 ) deriving more precise features for representing actions , and 2 ) reducing the asynchrony between different information streams . we first introduce a coarse-to-fine network which extracts shared deep features at different action class granularities and progressively integrates them to obtain a more accurate feature representation for input actions . we further introduce an asynchronous fusion network . it fuses information from different streams by asynchronously integrating stream-wise features at different time points , hence better leveraging the complementary information in different streams . experimental results on action recognition benchmarks demonstrate that our approach achieves the state-of-the-art performance .", "topics": ["computer vision"]}
{"title": "deviation based pooling strategies for full reference image quality assessment", "abstract": "the state-of-the-art pooling strategies for perceptual image quality assessment ( iqa ) are based on the mean and the weighted mean . they are robust pooling strategies which usually provide a moderate to high performance for different iqas . recently , standard deviation ( sd ) pooling was also proposed . although , this deviation pooling provides a very high performance for a few iqas , its performance is lower than mean poolings for many other iqas . in this paper , we propose to use the mean absolute deviation ( mad ) and show that it is a more robust and accurate pooling strategy for a wider range of iqas . in fact , mad pooling has the advantages of both mean pooling and sd pooling . the joint computation and use of the mad and sd pooling strategies is also considered in this paper . experimental results provide useful information on the choice of the proper deviation pooling strategy for different iqa models .", "topics": ["computation"]}
{"title": "the role of context selection in object detection", "abstract": "we investigate the reasons why context in object detection has limited utility by isolating and evaluating the predictive power of different context cues under ideal conditions in which context provided by an oracle . based on this study , we propose a region-based context re-scoring method with dynamic context selection to remove noise and emphasize informative context . we introduce latent indicator variables to select ( or ignore ) potential contextual regions , and learn the selection strategy with latent-svm . we conduct experiments to evaluate the performance of the proposed context selection method on the sun rgb-d dataset . the method achieves a significant improvement in terms of mean average precision ( map ) , compared with both appearance based detectors and a conventional context model without the selection scheme .", "topics": ["object detection"]}
{"title": "parallel belief revision", "abstract": "this paper describes a formal system of belief revision developed by wolfgang spohn and shows that this system has a parallel implementation that can be derived from an influence diagram in a manner similar to that in which bayesian networks are derived . the proof rests upon completeness results for an axiomatization of the notion of conditional independence , with the spohn system being used as a semantics for the relation of conditional independence .", "topics": ["iteration"]}
{"title": "case study : explaining diabetic retinopathy detection deep cnns via integrated gradients", "abstract": "in this report , we applied integrated gradients to explaining a neural network for diabetic retinopathy detection . the integrated gradient is an attribution method which measures the contributions of input to the quantity of interest . we explored some new ways for applying this method such as explaining intermediate layers , filtering out unimportant units by their attribution value and generating contrary samples . moreover , the visualization results extend the use of diabetic retinopathy detection model from merely predicting to assisting finding potential lesions .", "topics": ["gradient"]}
{"title": "filling knowledge gaps in a broad-coverage machine translation system", "abstract": "knowledge-based machine translation ( kbmt ) techniques yield high quality in domains with detailed semantic models , limited vocabulary , and controlled input grammar . scaling up along these dimensions means acquiring large knowledge resources . it also means behaving reasonably when definitive knowledge is not yet available . this paper describes how we can fill various kbmt knowledge gaps , often using robust statistical techniques . we describe quantitative and qualitative results from japangloss , a broad-coverage japanese-english mt system .", "topics": ["machine translation"]}
{"title": "learning from comparisons and choices", "abstract": "when tracking user-specific online activities , each user 's preference is revealed in the form of choices and comparisons . for example , a user 's purchase history tracks her choices , i.e . which item was chosen among a subset of offerings . a user 's comparisons are observed either explicitly as in movie ratings or implicitly as in viewing times of news articles . given such individualized ordinal data , we address the problem of collaboratively learning representations of the users and the items . the learned features can be used to predict a user 's preference of an unseen item to be used in recommendation systems . this also allows one to compute similarities among users and items to be used for categorization and search . motivated by the empirical successes of the multinomial logit ( mnl ) model in marketing and transportation , and also more recent successes in word embedding and crowdsourced image embedding , we pose this problem as learning the mnl model parameters that best explains the data . we propose a convex optimization for learning the mnl model , and show that it is minimax optimal up to a logarithmic factor by comparing its performance to a fundamental lower bound . this characterizes the minimax sample complexity of the problem , and proves that the proposed estimator can not be improved upon other than by a logarithmic factor . further , the analysis identifies how the accuracy depends on the topology of sampling via the spectrum of the sampling graph . this provides a guideline for designing surveys when one can choose which items are to be compared . this is accompanies by numerical simulations on synthetic and real datasets confirming our theoretical predictions .", "topics": ["sampling ( signal processing )", "computational complexity theory"]}
{"title": "flow segmentation in dense crowds", "abstract": "a framework is proposed in this paper that is used to segment flow of dense crowds . the flow field that is generated by the movement in the crowd is treated just like an aperiodic dynamic system . on this flow field a grid of particles is put over for particle advection by the use of a numerical integration scheme . then flow maps are generated which associates the initial position of the particles with final position . the gradient of the flow maps gives the amount of divergence of the neighboring particles . for forward integration and analysis forward finite time lyapunov exponent is calculated and backward finite time lyapunov exponent is also calculated it gives the lagrangian coherent structures of the flow in crowd . lagrangian coherent structures basically divides the flow in crowd into regions and these regions have different dynamics . these regions are then used to get the boundary in the different flow segments by using water shed algorithm . the experiment is conducted on the crowd dataset of ucf ( university of central florida ) .", "topics": ["numerical analysis", "map"]}
{"title": "smart `` predict , then optimize ''", "abstract": "many real-world analytics problems involve two significant challenges : prediction and optimization . due to the typically complex nature of each challenge , the standard paradigm is to predict , then optimize . by and large , machine learning tools are intended to minimize prediction error and do not account for how the predictions will be used in a downstream optimization problem . in contrast , we propose a new and very general framework , called smart `` predict , then optimize '' ( spo ) , which directly leverages the optimization problem structure , i.e . , its objective and constraints , for designing successful analytics tools . a key component of our framework is the spo loss function , which measures the quality of a prediction by comparing the objective values of the solutions generated using the predicted and observed parameters , respectively . training a model with respect to the spo loss is computationally challenging , and therefore we also develop a surrogate loss function , called the spo+ loss , which upper bounds the spo loss , has desirable convexity properties , and is statistically consistent under mild conditions . we also propose a stochastic gradient descent algorithm which allows for situations in which the number of training samples is large , model regularization is desired , and/or the optimization problem of interest is nonlinear or integer . finally , we perform computational experiments to empirically verify the success of our spo framework in comparison to the standard predict-then-optimize approach .", "topics": ["optimization problem", "nonlinear system"]}
{"title": "3-d rigid models from partial views - global factorization", "abstract": "the so-called factorization methods recover 3-d rigid structure from motion by factorizing an observation matrix that collects 2-d projections of features . these methods became popular due to their robustness - they use a large number of views , which constrains adequately the solution - and computational simplicity - the large number of unknowns is computed through an svd , avoiding non-linear optimization . however , they require that all the entries of the observation matrix are known . this is unlikely to happen in practice , due to self-occlusion and limited field of view . also , when processing long videos , regions that become occluded often appear again later . current factorization methods process these as new regions , leading to less accurate estimates of 3-d structure . in this paper , we propose a global factorization method that infers complete 3-d models directly from the 2-d projections in the entire set of available video frames . our method decides whether a region that has become visible is a region that was seen before , or a previously unseen region , in a global way , i.e . , by seeking the simplest rigid object that describes well the entire set of observations . this global approach increases significantly the accuracy of the estimates of the 3-d shape of the scene and the 3-d motion of the camera . experiments with artificial and real videos illustrate the good performance of our method .", "topics": ["nonlinear system"]}
{"title": "training l1-regularized models with orthant-wise passive descent algorithms", "abstract": "the $ l_1 $ -regularized models are widely used for sparse regression or classification tasks . in this paper , we propose the orthant-wise passive descent algorithm ( opda ) for optimizing $ l_1 $ -regularized models , as an improved substitute of proximal algorithms , which are the standard tools for optimizing the models nowadays . opda uses a stochastic variance-reduced gradient ( svrg ) to initialize the descent direction , then apply a novel alignment operator to encourage each element keeping the same sign after one iteration of update , so the parameter remains in the same orthant as before . it also explicitly suppresses the magnitude of each element to impose sparsity . the quasi-newton update can be utilized to incorporate curvature information and accelerate the speed . we prove a linear convergence rate for opda on general smooth and strongly-convex loss functions . by conducting experiments on $ l_1 $ -regularized logistic regression and convolutional neural networks , we show that opda outperforms state-of-the-art stochastic proximal algorithms , implying a wide range of applications in training sparse models .", "topics": ["loss function", "sparse matrix"]}
{"title": "deep cuboid detection : beyond 2d bounding boxes", "abstract": "we present a deep cuboid detector which takes a consumer-quality rgb image of a cluttered scene and localizes all 3d cuboids ( box-like objects ) . contrary to classical approaches which fit a 3d model from low-level cues like corners , edges , and vanishing points , we propose an end-to-end deep learning system to detect cuboids across many semantic categories ( e.g . , ovens , shipping boxes , and furniture ) . we localize cuboids with a 2d bounding box , and simultaneously localize the cuboid 's corners , effectively producing a 3d interpretation of box-like objects . we refine keypoints by pooling convolutional features iteratively , improving the baseline method significantly . our deep learning cuboid detector is trained in an end-to-end fashion and is suitable for real-time applications in augmented reality ( ar ) and robotics .", "topics": ["baseline ( configuration management )", "high- and low-level"]}
{"title": "hybrid image segmentation using discerner cluster in fcm and histogram thresholding", "abstract": "image thresholding has played an important role in image segmentation . this paper presents a hybrid approach for image segmentation based on the thresholding by fuzzy c-means ( thfcm ) algorithm for image segmentation . the goal of the proposed approach is to find a discerner cluster able to find an automatic threshold . the algorithm is formulated by applying the standard fcm clustering algorithm to the frequencies ( y-values ) on the smoothed histogram . hence , the frequencies of an image can be used instead of the conventional whole data of image . the cluster that has the highest peak which represents the maximum frequency in the image histogram will play as an excellent role in determining a discerner cluster to the grey level image . then , the pixels belong to the discerner cluster represent an object in the gray level histogram while the other clusters represent a background . experimental results with standard test images have been obtained through the proposed approach ( thfcm ) .", "topics": ["cluster analysis", "image segmentation"]}
{"title": "neural lattice-to-sequence models for uncertain inputs", "abstract": "the input to a neural sequence-to-sequence model is often determined by an up-stream system , e.g . a word segmenter , part of speech tagger , or speech recognizer . these up-stream models are potentially error-prone . representing inputs through word lattices allows making this uncertainty explicit by capturing alternative sequences and their posterior probabilities in a compact form . in this work , we extend the treelstm ( tai et al . , 2015 ) into a latticelstm that is able to consume word lattices , and can be used as encoder in an attentional encoder-decoder model . we integrate lattice posterior scores into this architecture by extending the treelstm 's child-sum and forget gates and introducing a bias term into the attention mechanism . we experiment with speech translation lattices and report consistent improvements over baselines that translate either the 1-best hypothesis or the lattice without posterior scores .", "topics": ["speech recognition", "encoder"]}
{"title": "handwritten recognition using svm , knn and neural network", "abstract": "handwritten recognition ( hwr ) is the ability of a computer to receive and interpret intelligible handwritten input from source such as paper documents , photographs , touch-screens and other devices . in this paper we will using three ( 3 ) classification t o re cognize the handwritten which is svm , knn and neural network .", "topics": ["support vector machine"]}
{"title": "deep learning from noisy image labels with quality embedding", "abstract": "there is an emerging trend to leverage noisy image datasets in many visual recognition tasks . however , the label noise among the datasets severely degenerates the \\mbox { performance of deep } learning approaches . recently , one mainstream is to introduce the latent label to handle label noise , which has shown promising improvement in the network designs . nevertheless , the mismatch between latent labels and noisy labels still affects the predictions in such methods . to address this issue , we propose a quality embedding model , which explicitly introduces a quality variable to represent the trustworthiness of noisy labels . our key idea is to identify the mismatch between the latent and noisy labels by embedding the quality variables into different subspaces , which effectively minimizes the noise effect . at the same time , the high-quality labels is still able to be applied for training . to instantiate the model , we further propose a contrastive-additive noise network ( can ) , which consists of two important layers : ( 1 ) the contrastive layer estimates the quality variable in the embedding space to reduce noise effect ; and ( 2 ) the additive layer aggregates the prior predictions and noisy labels as the posterior to train the classifier . moreover , to tackle the optimization difficulty , we deduce an sgd algorithm with the reparameterization tricks , which makes our method scalable to big data . we conduct the experimental evaluation of the proposed method over a range of noisy image datasets . comprehensive results have demonstrated can outperforms the state-of-the-art deep learning approaches .", "topics": ["scalability"]}
{"title": "analyzing tensor power method dynamics in overcomplete regime", "abstract": "we present a novel analysis of the dynamics of tensor power iterations in the overcomplete regime where the tensor cp rank is larger than the input dimension . finding the cp decomposition of an overcomplete tensor is np-hard in general . we consider the case where the tensor components are randomly drawn , and show that the simple power iteration recovers the components with bounded error under mild initialization conditions . we apply our analysis to unsupervised learning of latent variable models , such as multi-view mixture models and spherical gaussian mixtures . given the third order moment tensor , we learn the parameters using tensor power iterations . we prove it can correctly learn the model parameters when the number of hidden components $ k $ is much larger than the data dimension $ d $ , up to $ k = o ( d^ { 1.5 } ) $ . we initialize the power iterations with data samples and prove its success under mild conditions on the signal-to-noise ratio of the samples . our analysis significantly expands the class of latent variable models where spectral methods are applicable . our analysis also deals with noise in the input tensor leading to sample complexity result in the application to learning latent variable models .", "topics": ["unsupervised learning", "iteration"]}
{"title": "cross-lingual and multilingual speech emotion recognition on english and french", "abstract": "research on multilingual speech emotion recognition faces the problem that most available speech corpora differ from each other in important ways , such as annotation methods or interaction scenarios . these inconsistencies complicate building a multilingual system . we present results for cross-lingual and multilingual emotion recognition on english and french speech data with similar characteristics in terms of interaction ( human-human conversations ) . further , we explore the possibility of fine-tuning a pre-trained cross-lingual model with only a small number of samples from the target language , which is of great interest for low-resource languages . to gain more insights in what is learned by the deployed convolutional neural network , we perform an analysis on the attention mechanism inside the network .", "topics": ["text corpus"]}
{"title": "image-to-markup generation with coarse-to-fine attention", "abstract": "we present a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-fine attention mechanism . our method is evaluated in the context of image-to-latex generation , and we introduce a new dataset of real-world rendered mathematical expressions paired with latex markup . we show that unlike neural ocr techniques using ctc-based models , attention-based approaches can tackle this non-standard ocr task . our approach outperforms classical mathematical ocr systems by a large margin on in-domain rendered data , and , with pretraining , also performs well on out-of-domain handwritten data . to reduce the inference complexity associated with the attention-based approaches , we introduce a new coarse-to-fine attention layer that selects a support region before applying attention .", "topics": ["scalability", "encoder"]}
{"title": "bayesian hyperparameter optimization for ensemble learning", "abstract": "in this paper , we bridge the gap between hyperparameter optimization and ensemble learning by performing bayesian optimization of an ensemble with regards to its hyperparameters . our method consists in building a fixed-size ensemble , optimizing the configuration of one classifier of the ensemble at each iteration of the hyperparameter optimization algorithm , taking into consideration the interaction with the other models when evaluating potential performances . we also consider the case where the ensemble is to be reconstructed at the end of the hyperparameter optimization phase , through a greedy selection over the pool of models generated during the optimization . we study the performance of our proposed method on three different hyperparameter spaces , showing that our approach is better than both the best single model and a greedy ensemble construction over the models produced by a standard bayesian optimization .", "topics": ["iteration"]}
{"title": "recognition of documents in braille", "abstract": "visually impaired people are integral part of the society and it has been a must to provide them with means and system through which they may communicate with the world . in this work , i would like to address how computers can be made useful to read the scripts in braille . the importance of this work is to reduce communication gap between visually impaired people and the society . braille remains the most popular tactile reading code even in this century . there are numerous amount of literature locked up in braille . braille recognition not only reduces time in reading or extracting information from braille document but also helps people engaged in special education for correcting papers and other school related works . the availability of such a system will enhance communication and collaboration possibilities with visually impaired people . existing works supports only documents in white either bright or dull in colour . hardly any work could be traced on hand printed ordinary documents in braille .", "topics": ["speech recognition"]}
{"title": "analyzing the robustness of nearest neighbors to adversarial examples", "abstract": "motivated by safety-critical applications , test-time attacks on classifiers via adversarial examples has recently received a great deal of attention . however , there is a general lack of understanding on why adversarial examples arise ; whether they originate due to inherent properties of data or due to lack of training samples remains ill-understood . in this work , we introduce a theoretical framework analogous to bias-variance theory for understanding these effects . we use our framework to analyze the robustness of a canonical non-parametric classifier - the k-nearest neighbors . our analysis shows that its robustness properties depend critically on the value of k - the classifier may be inherently non-robust for small k , but its robustness approaches that of the bayes optimal classifier for fast-growing k. we propose a novel modified 1-nearest neighbor classifier , and guarantee its robustness in the large sample limit . our experiments suggest that this classifier may have good robustness properties even for reasonable data set sizes .", "topics": ["statistical classification", "autonomous car"]}
{"title": "triplet spike time dependent plasticity : a floating-gate implementation", "abstract": "synapse plays an important role of learning in a neural network ; the learning rules which modify the synaptic strength based on the timing difference between the pre- and post-synaptic spike occurrence is termed as spike time dependent plasticity ( stdp ) . the most commonly used rule posits weight change based on time difference between one pre- and one post spike and is hence termed doublet stdp ( dstdp ) . however , d-stdp could not reproduce results of many biological experiments ; a triplet stdp ( t-stdp ) that considers triplets of spikes as the fundamental unit has been proposed recently to explain these observations . this paper describes the compact implementation of a synapse using single floating-gate ( fg ) transistor that can store a weight in a nonvolatile manner and demonstrate the triplet stdp ( t-stdp ) learning rule by modifying drain voltages according to triplets of spikes . we describe a mathematical procedure to obtain control voltages for the fg device for t-stdp and also show measurement results from a fg synapse fabricated in tsmc 0.35um cmos process to support the theory . possible vlsi implementation of drain voltage waveform generator circuits are also presented with simulation results .", "topics": ["simulation"]}
{"title": "a feature-enriched neural model for joint chinese word segmentation and part-of-speech tagging", "abstract": "recently , neural network models for natural language processing tasks have been increasingly focused on for their ability of alleviating the burden of manual feature engineering . however , the previous neural models can not extract the complicated feature compositions as the traditional methods with discrete features . in this work , we propose a feature-enriched neural model for joint chinese word segmentation and part-of-speech tagging task . specifically , to simulate the feature templates of traditional discrete feature based models , we use different filters to model the complex compositional features with convolutional and pooling layer , and then utilize long distance dependency information with recurrent layer . experimental results on five different datasets show the effectiveness of our proposed model .", "topics": ["natural language processing", "natural language"]}
{"title": "bootstrapped thompson sampling and deep exploration", "abstract": "this technical note presents a new approach to carrying out the kind of exploration achieved by thompson sampling , but without explicitly maintaining or sampling from posterior distributions . the approach is based on a bootstrap technique that uses a combination of observed and artificially generated data . the latter serves to induce a prior distribution which , as we will demonstrate , is critical to effective exploration . we explain how the approach can be applied to multi-armed bandit and reinforcement learning problems and how it relates to thompson sampling . the approach is particularly well-suited for contexts in which exploration is coupled with deep learning , since in these settings , maintaining or generating samples from a posterior distribution becomes computationally infeasible .", "topics": ["sampling ( signal processing )", "computational complexity theory"]}
{"title": "online learning with predictable sequences", "abstract": "we present methods for online linear optimization that take advantage of benign ( as opposed to worst-case ) sequences . specifically if the sequence encountered by the learner is described well by a known `` predictable process '' , the algorithms presented enjoy tighter bounds as compared to the typical worst case bounds . additionally , the methods achieve the usual worst-case regret bounds if the sequence is not benign . our approach can be seen as a way of adding prior knowledge about the sequence within the paradigm of online learning . the setting is shown to encompass partial and side information . variance and path-length bounds can be seen as particular examples of online learning with simple predictable sequences . we further extend our methods and results to include competing with a set of possible predictable processes ( models ) , that is `` learning '' the predictable process itself concurrently with using it to obtain better regret guarantees . we show that such model selection is possible under various assumptions on the available feedback . our results suggest a promising direction of further research with potential applications to stock market and time series prediction .", "topics": ["regret ( decision theory )", "time series"]}
{"title": "learning with cross-kernels and ideal pca", "abstract": "we describe how cross-kernel matrices , that is , kernel matrices between the data and a custom chosen set of `feature spanning points ' can be used for learning . the main potential of cross-kernels lies in the fact that ( a ) only one side of the matrix scales with the number of data points , and ( b ) cross-kernels , as opposed to the usual kernel matrices , can be used to certify for the data manifold . our theoretical framework , which is based on a duality involving the feature space and vanishing ideals , indicates that cross-kernels have the potential to be used for any kind of kernel learning . we present a novel algorithm , ideal pca ( ipca ) , which cross-kernelizes pca . we demonstrate on real and synthetic data that ipca allows to ( a ) obtain pca-like features faster and ( b ) to extract novel and empirically validated features certifying for the data manifold .", "topics": ["kernel ( operating system )", "feature vector"]}
{"title": "parameter learning in prism programs with continuous random variables", "abstract": "probabilistic logic programming ( plp ) , exemplified by sato and kameya 's prism , poole 's icl , de raedt et al 's problog and vennekens et al 's lpad , combines statistical and logical knowledge representation and inference . inference in these languages is based on enumerative construction of proofs over logic programs . consequently , these languages permit very limited use of random variables with continuous distributions . in this paper , we extend prism with gaussian random variables and linear equality constraints , and consider the problem of parameter learning in the extended language . many statistical models such as finite mixture models and kalman filter can be encoded in extended prism . our em-based learning algorithm uses a symbolic inference procedure that represents sets of derivations without enumeration . this permits us to learn the distribution parameters of extended prism programs with discrete as well as gaussian variables . the learning algorithm naturally generalizes the ones used for prism and hybrid bayesian networks .", "topics": ["bayesian network"]}
{"title": "edge detection in radar images using weibull distribution", "abstract": "radar images can reveal information about the shape of the surface terrain as well as its physical and biophysical properties . radar images have long been used in geological studies to map structural features that are revealed by the shape of the landscape . radar imagery also has applications in vegetation and crop type mapping , landscape ecology , hydrology , and volcanology . image processing is using for detecting for objects in radar images . edge detection ; which is a method of determining the discontinuities in gray level images ; is a very important initial step in image processing . many classical edge detectors have been developed over time . some of the well-known edge detection operators based on the first derivative of the image are roberts , prewitt , sobel which is traditionally implemented by convolving the image with masks . also gaussian distribution has been used to build masks for the first and second derivative . however , this distribution has limit to only symmetric shape . this paper will use to construct the masks , the weibull distribution which was more general than gaussian because it has symmetric and asymmetric shape . the constructed masks are applied to images and we obtained good results .", "topics": ["image processing"]}
{"title": "understanding neural networks through representation erasure", "abstract": "while neural networks have been successfully applied to many natural language processing tasks , they come at the cost of interpretability . in this paper , we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation , such as input word-vector dimensions , intermediate hidden units , or input words . we present several approaches to analyzing the effects of such erasure , from computing the relative difference in evaluation metrics , to using reinforcement learning to erase the minimum set of input words in order to flip a neural model 's decision . in a comprehensive analysis of multiple nlp tasks , including linguistic feature classification , sentence-level sentiment analysis , and document level sentiment aspect prediction , we show that the proposed methodology not only offers clear explanations about neural model decisions , but also provides a way to conduct error analysis on neural models .", "topics": ["natural language processing", "neural networks"]}
{"title": "learning language representations for typology prediction", "abstract": "one central mystery of neural nlp is what neural models `` know '' about their subject matter . when a neural machine translation system learns to translate from one language to another , does it learn the syntax or semantics of the languages ? can this knowledge be extracted from the system to fill holes in human scientific knowledge ? existing typological databases contain relatively full feature specifications for only a few hundred languages . exploiting the existence of parallel texts in more than a thousand languages , we build a massive many-to-one neural machine translation ( nmt ) system from 1017 languages into english , and use this to predict information missing from typological databases . experiments show that the proposed method is able to infer not only syntactic , but also phonological and phonetic inventory features , and improves over a baseline that has access to information about the languages ' geographic and phylogenetic neighbors .", "topics": ["baseline ( configuration management )", "natural language processing"]}
{"title": "high dimensional sparse gaussian graphical mixture model", "abstract": "this paper considers the problem of networks reconstruction from heterogeneous data using a gaussian graphical mixture model ( ggmm ) . it is well known that parameter estimation in this context is challenging due to large numbers of variables coupled with the degeneracy of the likelihood . we propose as a solution a penalized maximum likelihood technique by imposing an $ l_ { 1 } $ penalty on the precision matrix . our approach shrinks the parameters thereby resulting in better identifiability and variable selection . we use the expectation maximization ( em ) algorithm which involves the graphical lasso to estimate the mixing coefficients and the precision matrices . we show that under certain regularity conditions the penalized maximum likelihood ( pml ) estimates are consistent . we demonstrate the performance of the pml estimator through simulations and we show the utility of our method for high dimensional data analysis in a genomic application .", "topics": ["simulation", "sparse matrix"]}
{"title": "learning rich features from rgb-d images for object detection and segmentation", "abstract": "in this paper we study the problem of object detection for rgb-d images using semantically rich image and depth features . we propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity . we demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks . our final object detection system achieves an average precision of 37.3 % , which is a 56 % relative improvement over existing methods . we then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector . for this task , we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and geocentric pose features . finally , we use the output from our object detectors in an existing superpixel classification framework for semantic scene segmentation and achieve a 24 % relative improvement over current state-of-the-art for the object categories that we study . we believe advances such as those represented in this paper will facilitate the use of perception in fields like robotics .", "topics": ["object detection", "pixel"]}
{"title": "efficient learning of optimal markov network topology with k-tree modeling", "abstract": "the seminal work of chow and liu ( 1968 ) shows that approximation of a finite probabilistic system by markov trees can achieve the minimum information loss with the topology of a maximum spanning tree . our current paper generalizes the result to markov networks of tree width $ \\leq k $ , for every fixed $ k\\geq 2 $ . in particular , we prove that approximation of a finite probabilistic system with such markov networks has the minimum information loss when the network topology is achieved with a maximum spanning $ k $ -tree . while constructing a maximum spanning $ k $ -tree is intractable for even $ k=2 $ , we show that polynomial algorithms can be ensured by a sufficient condition accommodated by many meaningful applications . in particular , we prove an efficient algorithm for learning the optimal topology of higher order correlations among random variables that belong to an underlying linear structure .", "topics": ["polynomial"]}
{"title": "multiple-instance , cascaded classification for keyword spotting in narrow-band audio", "abstract": "we propose using cascaded classifiers for a keyword spotting ( kws ) task on narrow-band ( nb ) , 8khz audio acquired in non-iid environments -- - a more challenging task than most state-of-the-art kws systems face . we present a model that incorporates deep neural networks ( dnns ) , cascading , multiple-feature representations , and multiple-instance learning . the cascaded classifiers handle the task 's class imbalance and reduce power consumption on computationally-constrained devices via early termination . the kws system achieves a false negative rate of 6 % at an hourly false positive rate of 0.75", "topics": ["neural networks"]}
{"title": "nestt : a nonconvex primal-dual splitting method for distributed and stochastic optimization", "abstract": "we study a stochastic and distributed algorithm for nonconvex problems whose objective consists of a sum of $ n $ nonconvex $ l_i/n $ -smooth functions , plus a nonsmooth regularizer . the proposed nonconvex primal-dual splitting ( nestt ) algorithm splits the problem into $ n $ subproblems , and utilizes an augmented lagrangian based primal-dual scheme to solve it in a distributed and stochastic manner . with a special non-uniform sampling , a version of nestt achieves $ \\epsilon $ -stationary solution using $ \\mathcal { o } ( ( \\sum_ { i=1 } ^n\\sqrt { l_i/n } ) ^2/\\epsilon ) $ gradient evaluations , which can be up to $ \\mathcal { o } ( n ) $ times better than the ( proximal ) gradient descent methods . it also achieves q-linear convergence rate for nonconvex $ \\ell_1 $ penalized quadratic problems with polyhedral constraints . further , we reveal a fundamental connection between primal-dual based methods and a few primal only methods such as iag/sag/saga .", "topics": ["optimization problem", "gradient descent"]}
{"title": "a general , sound and efficient natural language parsing algorithm based on syntactic constraints propagation", "abstract": "this paper presents a new context-free parsing algorithm based on a bidirectional strictly horizontal strategy which incorporates strong top-down predictions ( derivations and adjacencies ) . from a functional point of view , the parser is able to propagate syntactic constraints reducing parsing ambiguity . from a computational perspective , the algorithm includes different techniques aimed at the improvement of the manipulation and representation of the structures used .", "topics": ["parsing", "natural language"]}
{"title": "achieving non-discrimination in prediction", "abstract": "discrimination-aware classification is receiving an increasing attention in the data mining and machine learning fields . the data preprocessing methods for constructing a discrimination-free classifier remove discrimination from the training data , and learn the classifier from the cleaned data . however , there lacks of a theoretical guarantee for the performance of these methods . in this paper , we fill this theoretical gap by mathematically bounding the probability that the discrimination in predictions is within a given interval in terms of the given training data and classifier . in our analysis , we adopt the causal model for modeling the mechanisms in data generation , and formally defining discrimination in the population , in a dataset , and in the prediction . the theoretical results show that the fundamental assumption made by the data preprocessing methods is not correct . finally , we develop a framework for constructing a discrimination-free classifier with a theoretical guarantee .", "topics": ["test set", "statistical classification"]}
{"title": "identifying reliable annotations for large scale image segmentation", "abstract": "challenging computer vision tasks , in particular semantic image segmentation , require large training sets of annotated images . while obtaining the actual images is often unproblematic , creating the necessary annotation is a tedious and costly process . therefore , one often has to work with unreliable annotation sources , such as amazon mechanical turk or ( semi- ) automatic algorithmic techniques . in this work , we present a gaussian process ( gp ) based technique for simultaneously identifying which images of a training set have unreliable annotation and learning a segmentation model in which the negative effect of these images is suppressed . alternatively , the model can also just be used to identify the most reliably annotated images from the training set , which can then be used for training any other segmentation method . by relying on `` deep features '' in combination with a linear covariance function , our gp can be learned and its hyperparameter determined efficiently using only matrix operations and gradient-based optimization . this makes our method scalable even to large datasets with several million training instances .", "topics": ["image segmentation", "computer vision"]}
{"title": "provably fast and accurate recovery of evolutionary trees through harmonic greedy triplets", "abstract": "we give a greedy learning algorithm for reconstructing an evolutionary tree based on a certain harmonic average on triplets of terminal taxa . after the pairwise distances between terminal taxa are estimated from sequence data , the algorithm runs in o ( n^2 ) time using o ( n ) work space , where n is the number of terminal taxa . these time and space complexities are optimal in the sense that the size of an input distance matrix is n^2 and the size of an output tree is n. moreover , in the jukes-cantor model of evolution , the algorithm recovers the correct tree topology with high probability using sample sequences of length polynomial in ( 1 ) n , ( 2 ) the logarithm of the error probability , and ( 3 ) the inverses of two small parameters .", "topics": ["polynomial"]}
{"title": "using nuances of emotion to identify personality", "abstract": "past work on personality detection has shown that frequency of lexical categories such as first person pronouns , past tense verbs , and sentiment words have significant correlations with personality traits . in this paper , for the first time , we show that fine affect ( emotion ) categories such as that of excitement , guilt , yearning , and admiration are significant indicators of personality . additionally , we perform experiments to show that the gains provided by the fine affect categories are not obtained by using coarse affect categories alone or with specificity features alone . we employ these features in five svm classifiers for detecting five personality traits through essays . we find that the use of fine emotion features leads to statistically significant improvement over a competitive baseline , whereas the use of coarse affect and specificity features does not .", "topics": ["baseline ( configuration management )", "support vector machine"]}
{"title": "clustering signed networks with the geometric mean of laplacians", "abstract": "signed networks allow to model positive and negative relationships . we analyze existing extensions of spectral clustering to signed networks . it turns out that existing approaches do not recover the ground truth clustering in several situations where either the positive or the negative network structures contain no noise . our analysis shows that these problems arise as existing approaches take some form of arithmetic mean of the laplacians of the positive and negative part . as a solution we propose to use the geometric mean of the laplacians of positive and negative part and show that it outperforms the existing approaches . while the geometric mean of matrices is computationally expensive , we show that eigenvectors of the geometric mean can be computed efficiently , leading to a numerical scheme for sparse matrices which is of independent interest .", "topics": ["cluster analysis", "numerical analysis"]}
{"title": "cats & co : categorical time series coclustering", "abstract": "we suggest a novel method of clustering and exploratory analysis of temporal event sequences data ( also known as categorical time series ) based on three-dimensional data grid models . a data set of temporal event sequences can be represented as a data set of three-dimensional points , each point is defined by three variables : a sequence identifier , a time value and an event value . instantiating data grid models to the 3d-points turns the problem into 3d-coclustering . the sequences are partitioned into clusters , the time variable is discretized into intervals and the events are partitioned into clusters . the cross-product of the univariate partitions forms a multivariate partition of the representation space , i.e . , a grid of cells and it also represents a nonparametric estimator of the joint distribution of the sequences , time and events dimensions . thus , the sequences are grouped together because they have similar joint distribution of time and events , i.e . , similar distribution of events along the time dimension . the best data grid is computed using a parameter-free bayesian model selection approach . we also suggest several criteria for exploiting the resulting grid through agglomerative hierarchies , for interpreting the clusters of sequences and characterizing their components through insightful visualizations . extensive experiments on both synthetic and real-world data sets demonstrate that data grid models are efficient , effective and discover meaningful underlying patterns of categorical time series data .", "topics": ["time series", "cluster analysis"]}
{"title": "simple , fast semantic parsing with a tensor kernel", "abstract": "we describe a simple approach to semantic parsing based on a tensor product kernel . we extract two feature vectors : one for the query and one for each candidate logical form . we then train a classifier using the tensor product of the two vectors . using very simple features for both , our system achieves an average f1 score of 40.1 % on the webquestions dataset . this is comparable to more complex systems but is simpler to implement and runs faster .", "topics": ["kernel ( operating system )", "feature vector"]}
{"title": "arc-standard spinal parsing with stack-lstms", "abstract": "we present a neural transition-based parser for spinal trees , a dependency representation of constituent trees . the parser uses stack-lstms that compose constituent nodes with dependency-based derivations . in experiments , we show that this model adapts to different styles of dependency relations , but this choice has little effect for predicting constituent structure , suggesting that lstms induce useful states by themselves .", "topics": ["parsing"]}
{"title": "where to put the image in an image caption generator", "abstract": "when a recurrent neural network language model is used for caption generation , the image information can be fed to the neural network either by directly incorporating it in the rnn -- conditioning the language model by `injecting ' image features -- or in a layer following the rnn -- conditioning the language model by `merging ' image features . while both options are attested in the literature , there is as yet no systematic comparison between the two . in this paper we empirically show that it is not especially detrimental to performance whether one architecture is used or another . the merge architecture does have practical advantages , as conditioning by merging allows the rnn 's hidden state vector to shrink in size by up to four times . our results suggest that the visual and linguistic modalities for caption generation need not be jointly encoded by the rnn as that yields large , memory-intensive models with few tangible advantages in performance ; rather , the multimodal integration should be delayed to a subsequent stage .", "topics": ["recurrent neural network"]}
{"title": "open ended intelligence : the individuation of intelligent agents", "abstract": "artificial general intelligence is a field of research aiming to distill the principles of intelligence that operate independently of a specific problem domain or a predefined context and utilize these principles in order to synthesize systems capable of performing any intellectual task a human being is capable of and eventually go beyond that . while `` narrow '' artificial intelligence which focuses on solving specific problems such as speech recognition , text comprehension , visual pattern recognition , robotic motion , etc . has shown quite a few impressive breakthroughs lately , understanding general intelligence remains elusive . in the paper we offer a novel theoretical approach to understanding general intelligence . we start with a brief introduction of the current conceptual approach . our critique exposes a number of serious limitations that are traced back to the ontological roots of the concept of intelligence . we then propose a paradigm shift from intelligence perceived as a competence of individual agents defined in relation to an a priori given problem domain or a goal , to intelligence perceived as a formative process of self-organization by which intelligent agents are individuated . we call this process open-ended intelligence . open-ended intelligence is developed as an abstraction of the process of cognitive development so its application can be extended to general agents and systems . we introduce and discuss three facets of the idea : the philosophical concept of individuation , sense-making and the individuation of general cognitive agents . we further show how open-ended intelligence can be framed in terms of a distributed , self-organizing network of interacting elements and how such process is scalable . the framework highlights an important relation between coordination and intelligence and a new understanding of values . we conclude with a number of questions for future research .", "topics": ["value ( ethics )", "artificial intelligence"]}
{"title": "unsupervised domain adaptation with feature embeddings", "abstract": "representation learning is the dominant technique for unsupervised domain adaptation , but existing approaches often require the specification of `` pivot features '' that generalize across domains , which are selected by task-specific heuristics . we show that a novel but simple feature embedding approach provides better performance , by exploiting the feature template structure common in nlp problems .", "topics": ["natural language processing", "heuristic"]}
{"title": "covariance and pca for categorical variables", "abstract": "covariances from categorical variables are defined using a regular simplex expression for categories . the method follows the variance definition by gini , and it gives the covariance as a solution of simultaneous equations . the calculated results give reasonable values for test data . a method of principal component analysis ( rs-pca ) is also proposed using regular simplex expressions , which allows easy interpretation of the principal components . the proposed methods apply to variable selection problem of categorical data uscensus1990 data . the proposed methods give appropriate criterion for the variable selection problem of categorical", "topics": ["eisenstein 's criterion"]}
{"title": "iterative non-local shrinkage algorithm for mr image reconstruction", "abstract": "we introduce a fast iterative non-local shrinkage algorithm to recover mri data from undersampled fourier measurements . this approach is enabled by the reformulation of current non-local schemes as an alternating algorithm to minimize a global criterion . the proposed algorithm alternates between a non-local shrinkage step and a quadratic subproblem . we derive analytical shrinkage rules for several penalties that are relevant in non-local regularization . the redundancy in the searches used to evaluate the shrinkage steps are exploited using filtering operations . the resulting algorithm is observed to be considerably faster than current alternating non-local algorithms . the comparisons of the proposed scheme with state-of-the-art regularization schemes show a considerable reduction in alias artifacts and preservation of edges .", "topics": ["noise reduction", "matrix regularization"]}
{"title": "on considering uncertainty and alternatives in low-level vision", "abstract": "in this paper we address the uncertainty issues involved in the low-level vision task of image segmentation . researchers in computer vision have worked extensively on this problem , in which the goal is to partition ( or segment ) an image into regions that are homogeneous or uniform in some sense . this segmentation is often utilized by some higher level process , such as an object recognition system . we show that by considering uncertainty in a bayesian formalism , we can use statistical image models to build an approximate representation of a probability distribution over a space of alternative segmentations . we give detailed descriptions of the various levels of uncertainty associated with this problem , discuss the interaction of prior and posterior distributions , and provide the operations for constructing this representation .", "topics": ["image segmentation", "high- and low-level"]}
{"title": "self-organized stigmergic document maps : environment as a mechanism for context learning", "abstract": "social insect societies and more specifically ant colonies , are distributed systems that , in spite of the simplicity of their individuals , present a highly structured social organization . as a result of this organization , ant colonies can accomplish complex tasks that in some cases exceed the individual capabilities of a single ant . the study of ant colonies behavior and of their self-organizing capabilities is of interest to knowledge retrieval/management and decision support systems sciences , because it provides models of distributed adaptive organization which are useful to solve difficult optimization , classification , and distributed control problems , among others . in the present work we overview some models derived from the observation of real ants , emphasizing the role played by stigmergy as distributed communication paradigm , and we present a novel strategy to tackle unsupervised clustering as well as data retrieval problems . the present ant clustering system ( acluster ) avoids not only short-term memory based strategies , as well as the use of several artificial ant types ( using different speeds ) , present in some recent approaches . moreover and according to our knowledge , this is also the first application of ant systems into textual document clustering . keywords : swarm intelligence , ant systems , unsupervised clustering , data retrieval , data mining , distributed computing , document maps , textual document clustering .", "topics": ["data mining", "cluster analysis"]}
{"title": "tabletrans , multitrans , intertrans and treetrans : diverse tools built on the annotation graph toolkit", "abstract": "four diverse tools built on the annotation graph toolkit are described . each tool associates linguistic codes and structures with time-series data . all are based on the same software library and tool architecture . tabletrans is for observational coding , using a spreadsheet whose rows are aligned to a signal . multitrans is for transcribing multi-party communicative interactions recorded using multi-channel signals . intertrans is for creating interlinear text aligned to audio . treetrans is for creating and manipulating syntactic trees . this work demonstrates that the development of diverse tools and re-use of software components is greatly facilitated by a common high-level application programming interface for representing the data and managing input/output , together with a common architecture for managing the interaction of multiple components .", "topics": ["time series"]}
{"title": "correcting multi-focus images via simple standard deviation for image fusion", "abstract": "image fusion is one of the recent trends in image registration which is an essential field of image processing . the basic principle of this paper is to fuse multi-focus images using simple statistical standard deviation . firstly , the simple standard deviation for the k-by-k window inside each of the multi-focus images was computed . the contribution in this paper came from the idea that the focused part inside an image had high details rather than the unfocused part . hence , the dispersion between pixels inside the focused part is higher than the dispersion inside the unfocused part . secondly , a simple comparison between the standard deviation for each k-by-k window in the multi-focus images could be computed . the highest standard deviation between all the computed standard deviations for the multi-focus images could be treated as the optimal that is to be placed in the fused image . the experimental visual results show that the proposed method produces very satisfactory results in spite of its simplicity .", "topics": ["image processing", "pixel"]}
{"title": "derivative delay embedding : online modeling of streaming time series", "abstract": "the staggering amount of streaming time series coming from the real world calls for more efficient and effective online modeling solution . for time series modeling , most existing works make some unrealistic assumptions such as the input data is of fixed length or well aligned , which requires extra effort on segmentation or normalization of the raw streaming data . although some literature claim their approaches to be invariant to data length and misalignment , they are too time-consuming to model a streaming time series in an online manner . we propose a novel and more practical online modeling and classification scheme , dde-mgm , which does not make any assumptions on the time series while maintaining high efficiency and state-of-the-art performance . the derivative delay embedding ( dde ) is developed to incrementally transform time series to the embedding space , where the intrinsic characteristics of data is preserved as recursive patterns regardless of the stream length and misalignment . then , a non-parametric markov geographic model ( mgm ) is proposed to both model and classify the pattern in an online manner . experimental results demonstrate the effectiveness and superior classification accuracy of the proposed dde-mgm in an online setting as compared to the state-of-the-art .", "topics": ["time series"]}
{"title": "a survey of human activity recognition using wifi csi", "abstract": "in this article , we present a survey of recent advances in passive human behaviour recognition in indoor areas using the channel state information ( csi ) of commercial wifi systems . movement of human body causes a change in the wireless signal reflections , which results in variations in the csi . by analyzing the data streams of csis for different activities and comparing them against stored models , human behaviour can be recognized . this is done by extracting features from csi data streams and using machine learning techniques to build models and classifiers . the techniques from the literature that are presented herein have great performances , however , instead of the machine learning techniques employed in these works , we propose to use deep learning techniques such as long-short term memory ( lstm ) recurrent neural network ( rnn ) , and show the improved performance . we also discuss about different challenges such as environment change , frame rate selection , and multi-user scenario , and suggest possible directions for future work .", "topics": ["recurrent neural network"]}
{"title": "representational distance learning for deep neural networks", "abstract": "deep neural networks ( dnns ) provide useful models of visual representational transformations . we present a method that enables a dnn ( student ) to learn from the internal representational spaces of a reference model ( teacher ) , which could be another dnn or , in the future , a biological brain . representational spaces of the student and the teacher are characterized by representational distance matrices ( rdms ) . we propose representational distance learning ( rdl ) , a stochastic gradient descent method that drives the rdms of the student to approximate the rdms of the teacher . we demonstrate that rdl is competitive with other transfer learning techniques for two publicly available benchmark computer vision datasets ( mnist and cifar-100 ) , while allowing for architectural differences between student and teacher . by pulling the student 's rdms towards those of the teacher , rdl significantly improved visual classification performance when compared to baseline networks that did not use transfer learning . in the future , rdl may enable combined supervised training of deep neural networks using task constraints ( e.g . images and category labels ) and constraints from brain-activity measurements , so as to build models that replicate the internal representational spaces of biological brains .", "topics": ["baseline ( configuration management )", "neural networks"]}
{"title": "sample complexity analysis for learning overcomplete latent variable models through tensor methods", "abstract": "we provide guarantees for learning latent variable models emphasizing on the overcomplete regime , where the dimensionality of the latent space can exceed the observed dimensionality . in particular , we consider multiview mixtures , spherical gaussian mixtures , ica , and sparse coding models . we provide tight concentration bounds for empirical moments through novel covering arguments . we analyze parameter recovery through a simple tensor power update algorithm . in the semi-supervised setting , we exploit the label or prior information to get a rough estimate of the model parameters , and then refine it using the tensor method on unlabeled samples . we establish that learning is possible when the number of components scales as $ k=o ( d^ { p/2 } ) $ , where $ d $ is the observed dimension , and $ p $ is the order of the observed moment employed in the tensor method . our concentration bound analysis also leads to minimax sample complexity for semi-supervised learning of spherical gaussian mixtures . in the unsupervised setting , we use a simple initialization algorithm based on svd of the tensor slices , and provide guarantees under the stricter condition that $ k\\le \\beta d $ ( where constant $ \\beta $ can be larger than $ 1 $ ) , where the tensor method recovers the components under a polynomial running time ( and exponential in $ \\beta $ ) . our analysis establishes that a wide range of overcomplete latent variable models can be learned efficiently with low computational and sample complexity through tensor decomposition methods .", "topics": ["time complexity", "supervised learning"]}
{"title": "multi-dueling bandits and their application to online ranker evaluation", "abstract": "new ranking algorithms are continually being developed and refined , necessitating the development of efficient methods for evaluating these rankers . online ranker evaluation focuses on the challenge of efficiently determining , from implicit user feedback , which ranker out of a finite set of rankers is the best . online ranker evaluation can be modeled by dueling ban- dits , a mathematical model for online learning under limited feedback from pairwise comparisons . comparisons of pairs of rankers is performed by interleaving their result sets and examining which documents users click on . the dueling bandits model addresses the key issue of which pair of rankers to compare at each iteration , thereby providing a solution to the exploration-exploitation trade-off . recently , methods for simultaneously comparing more than two rankers have been developed . however , the question of which rankers to compare at each iteration was left open . we address this question by proposing a generalization of the dueling bandits model that uses simultaneous comparisons of an unrestricted number of rankers . we evaluate our algorithm on synthetic data and several standard large-scale online ranker evaluation datasets . our experimental results show that the algorithm yields orders of magnitude improvement in performance compared to stateof- the-art dueling bandit algorithms .", "topics": ["iteration"]}
{"title": "towards a mathematical foundation of immunology and amino acid chains", "abstract": "we attempt to set a mathematical foundation of immunology and amino acid chains . to measure the similarities of these chains , a kernel on strings is defined using only the sequence of the chains and a good amino acid substitution matrix ( e.g . blosum62 ) . the kernel is used in learning machines to predict binding affinities of peptides to human leukocyte antigens dr ( hla-dr ) molecules . on both fixed allele ( nielsen and lund 2009 ) and pan-allele ( nielsen et.al . 2010 ) benchmark databases , our algorithm achieves the state-of-the-art performance . the kernel is also used to define a distance on an hla-dr allele set based on which a clustering analysis precisely recovers the serotype classifications assigned by who ( nielsen and lund 2009 , and marsh et.al . 2010 ) . these results suggest that our kernel relates well the chain structure of both peptides and hla-dr molecules to their biological functions , and that it offers a simple , powerful and promising methodology to immunology and amino acid chain studies .", "topics": ["kernel ( operating system )", "cluster analysis"]}
{"title": "arbitrary discrete sequence anomaly detection with zero boundary lstm", "abstract": "we propose a simple mathematical definition and new neural architecture for finding anomalies within discrete sequence datasets . our model comprises of a modified lstm autoencoder and an array of one-class svms . the lstm takes in elements from a sequence and creates context vectors that are used to predict the probability distribution of the following element . these context vectors are then used to train an array of one-class svms . these svms are used to determine an outlier boundary in context space.we show that our method is consistently more stable and also outperforms standard lstm and sliding window anomaly detection systems on two generated datasets .", "topics": ["autoencoder"]}
{"title": "photographic home styles in congress : a computer vision approach", "abstract": "while members of congress now routinely communicate with constituents using images on a variety of internet platforms , little is known about how images are used as a means of strategic political communication . this is due primarily to computational limitations which have prevented large-scale , systematic analyses of image features . new developments in computer vision , however , are bringing the systematic study of images within reach . here , we develop a framework for understanding visual political communication by extending fenno 's analysis of home style ( fenno 1978 ) to images and introduce `` photographic '' home styles . using approximately 192,000 photographs collected from mcs facebook profiles , we build machine learning software with convolutional neural networks and conduct an image manipulation experiment to explore how the race of people that mcs pose with shape photographic home styles . we find evidence that electoral pressures shape photographic home styles and demonstrate that democratic and republican members of congress use images in very different ways .", "topics": ["computer vision"]}
{"title": "stochastic expectation propagation", "abstract": "expectation propagation ( ep ) is a deterministic approximation algorithm that is often used to perform approximate bayesian parameter learning . ep approximates the full intractable posterior distribution through a set of local approximations that are iteratively refined for each datapoint . ep can offer analytic and computational advantages over other approximations , such as variational inference ( vi ) , and is the method of choice for a number of models . the local nature of ep appears to make it an ideal candidate for performing bayesian learning on large models in large-scale dataset settings . however , ep has a crucial limitation in this context : the number of approximating factors needs to increase with the number of data-points , n , which often entails a prohibitively large memory overhead . this paper presents an extension to ep , called stochastic expectation propagation ( sep ) , that maintains a global posterior approximation ( like vi ) but updates it in a local way ( like ep ) . experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that sep performs almost as well as full ep , but reduces the memory consumption by a factor of $ n $ . sep is therefore ideally suited to performing approximate bayesian learning in the large model , large dataset setting .", "topics": ["approximation algorithm", "synthetic data"]}
{"title": "counterfactual equivalence for pomdps , and underlying deterministic environments", "abstract": "partially observable markov decision processes ( pomdps ) are rich environments often used in machine learning . but the issue of information and causal structures in pomdps has been relatively little studied . this paper presents the concepts of equivalent and counterfactually equivalent pomdps , where agents can not distinguish which environment they are in though any observations and actions . it shows that any pomdp is counterfactually equivalent , for any finite number of turns , to a deterministic pomdp with all uncertainty concentrated into the initial state . this allows a better understanding of pomdp uncertainty , information , and learning .", "topics": ["causality"]}
{"title": "measuring latent causal structure", "abstract": "discovering latent representations of the observed world has become increasingly more relevant in data analysis . much of the effort concentrates on building latent variables which can be used in prediction problems , such as classification and regression . a related goal of learning latent structure from data is that of identifying which hidden common causes generate the observations , such as in applications that require predicting the effect of policies . this will be the main problem tackled in our contribution : given a dataset of indicators assumed to be generated by unknown and unmeasured common causes , we wish to discover which hidden common causes are those , and how they generate our data . this is possible under the assumption that observed variables are linear functions of the latent causes with additive noise . previous results in the literature present solutions for the case where each observed variable is a noisy function of a single latent variable . we show how to extend the existing results for some cases where observed variables measure more than one latent variable .", "topics": ["statistical classification", "artificial intelligence"]}
{"title": "learning mixtures of submodular shells with application to document summarization", "abstract": "we introduce a method to learn a mixture of submodular `` shells '' in a large-margin setting . a submodular shell is an abstract submodular function that can be instantiated with a ground set and a set of parameters to produce a submodular function . a mixture of such shells can then also be so instantiated to produce a more complex submodular function . what our algorithm learns are the mixture weights over such shells . we provide a risk bound guarantee when learning in a large-margin structured-prediction setting using a projected subgradient method when only approximate submodular optimization is possible ( such as with submodular function maximization ) . we apply this method to the problem of multi-document summarization and produce the best results reported so far on the widely used nist duc-05 through duc-07 document summarization corpora .", "topics": ["approximation algorithm", "text corpus"]}
{"title": "non-convex projected gradient descent for generalized low-rank tensor regression", "abstract": "in this paper , we consider the problem of learning high-dimensional tensor regression problems with low-rank structure . one of the core challenges associated with learning high-dimensional models is computation since the underlying optimization problems are often non-convex . while convex relaxations could lead to polynomial-time algorithms they are often slow in practice . on the other hand , limited theoretical guarantees exist for non-convex methods . in this paper we provide a general framework that provides theoretical guarantees for learning high-dimensional tensor regression models under different low-rank structural assumptions using the projected gradient descent algorithm applied to a potentially non-convex constraint set $ \\theta $ in terms of its \\emph { localized gaussian width } . we juxtapose our theoretical results for non-convex projected gradient descent algorithms with previous results on regularized convex approaches . the two main differences between the convex and non-convex approach are : ( i ) from a computational perspective whether the non-convex projection operator is computable and whether the projection has desirable contraction properties and ( ii ) from a statistical upper bound perspective , the non-convex approach has a superior rate for a number of examples . we provide three concrete examples of low-dimensional structure which address these issues and explain the pros and cons for the non-convex and convex approaches . we supplement our theoretical results with simulations which show that , under several common settings of generalized low rank tensor regression , the projected gradient descent approach is superior both in terms of statistical error and run-time provided the step-sizes of the projected descent algorithm are suitably chosen .", "topics": ["gradient descent", "simulation"]}
{"title": "continuous multilinguality with language vectors", "abstract": "most existing models for multilingual natural language processing ( nlp ) treat language as a discrete category , and make predictions for either one language or the other . in contrast , we propose using continuous vector representations of language . we show that these can be learned efficiently with a character-based neural language model , and used to improve inference about language varieties not seen during training . in experiments with 1303 bible translations into 990 different languages , we empirically explore the capacity of multilingual language models , and also show that the language vectors capture genetic relationships between languages .", "topics": ["natural language processing", "natural language"]}
{"title": "proceedings of the twenty-first conference on uncertainty in artificial intelligence ( 2005 )", "abstract": "this is the proceedings of the twenty-first conference on uncertainty in artificial intelligence , which was held in edinburgh , scotland july 26 - 29 2005 .", "topics": ["artificial intelligence"]}
{"title": "on the use of reference points for the biobjective inventory routing problem", "abstract": "the article presents a study on the biobjective inventory routing problem . contrary to most previous research , the problem is treated as a true multi-objective optimization problem , with the goal of identifying pareto-optimal solutions . due to the hardness of the problem at hand , a reference point based optimization approach is presented and implemented into an optimization and decision support system , which allows for the computation of a true subset of the optimal outcomes . experimental investigation involving local search metaheuristics are conducted on benchmark data , and numerical results are reported and analyzed .", "topics": ["optimization problem", "numerical analysis"]}
{"title": "one model to learn them all", "abstract": "deep learning yields great results across many fields , from speech recognition , image classification , to translation . but for each problem , getting a deep model to work well involves research into the architecture and a long period of tuning . we present a single model that yields good results on a number of problems spanning multiple domains . in particular , this single model is trained concurrently on imagenet , multiple translation tasks , image captioning ( coco dataset ) , a speech recognition corpus , and an english parsing task . our model architecture incorporates building blocks from multiple domains . it contains convolutional layers , an attention mechanism , and sparsely-gated layers . each of these computational blocks is crucial for a subset of the tasks we train on . interestingly , even if a block is not crucial for a task , we observe that adding it never hurts performance and in most cases improves it on all tasks . we also show that tasks with less data benefit largely from joint training with other tasks , while performance on large tasks degrades only slightly if at all .", "topics": ["computer vision", "parsing"]}
{"title": "hole filling with multiple reference views in dibr view synthesis", "abstract": "depth-image-based rendering ( dibr ) oriented view synthesis has been widely employed in the current depth-based 3d video systems by synthesizing a virtual view from an arbitrary viewpoint . however , holes may appear in the synthesized view due to disocclusion , thus significantly degrading the quality . consequently , efforts have been made on developing effective and efficient hole filling algorithms . current hole filling techniques generally extrapolate/interpolate the hole regions with the neighboring information based on an assumption that the texture pattern in the holes is similar to that of the neighboring background information . however , in many scenarios especially of complex texture , the assumption may not hold . in other words , hole filling techniques can only provide an estimation for a hole which may not be good enough or may even be erroneous considering a wide variety of complex scene of images . in this paper , we first examine the view interpolation with multiple reference views , demonstrating that the problem of emerging holes in a target virtual view can be greatly alleviated by making good use of other neighboring complementary views in addition to its two ( commonly used ) most neighboring primary views . the effects of using multiple views for view extrapolation in reducing holes are also investigated in this paper . in view of the 3d video and ongoing free-viewpoint tv standardization , we propose a new view synthesis framework which employs multiple views to synthesize output virtual views . furthermore , a scheme of selective warping of complementary views is developed by efficiently locating a small number of useful pixels in the complementary views for hole reduction , to avoid a full warping of additional complementary views thus lowering greatly the warping complexity .", "topics": ["pixel"]}
{"title": "the role of semantics in mining frequent patterns from knowledge bases in description logics with rules", "abstract": "we propose a new method for mining frequent patterns in a language that combines both semantic web ontologies and rules . in particular we consider the setting of using a language that combines description logics with dl-safe rules . this setting is important for the practical application of data mining to the semantic web . we focus on the relation of the semantics of the representation formalism to the task of frequent pattern discovery , and for the core of our method , we propose an algorithm that exploits the semantics of the combined knowledge base . we have developed a proof-of-concept data mining implementation of this . using this we have empirically shown that using the combined knowledge base to perform semantic tests can make data mining faster by pruning useless candidate patterns before their evaluation . we have also shown that the quality of the set of patterns produced may be improved : the patterns are more compact , and there are fewer patterns . we conclude that exploiting the semantics of a chosen representation formalism is key to the design and application of ( onto- ) relational frequent pattern discovery methods . note : to appear in theory and practice of logic programming ( tplp )", "topics": ["data mining"]}
{"title": "natural language does not emerge 'naturally ' in multi-agent dialog", "abstract": "a number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations , and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents , all learned without any human supervision ! in this paper , using a task and tell reference game between two agents as a testbed , we present a sequence of 'negative ' results culminating in a 'positive ' one -- showing that while most agent-invented languages are effective ( i.e . achieve near-perfect task rewards ) , they are decidedly not interpretable or compositional . in essence , we find that natural language does not emerge 'naturally ' , despite the semblance of ease of natural-language-emergence that one may gather from recent literature . we discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate .", "topics": ["natural language", "end-to-end principle"]}
{"title": "joint stochastic approximation learning of helmholtz machines", "abstract": "though with progress , model learning and performing posterior inference still remains a common challenge for using deep generative models , especially for handling discrete hidden variables . this paper is mainly concerned with algorithms for learning helmholz machines , which is characterized by pairing the generative model with an auxiliary inference model . a common drawback of previous learning algorithms is that they indirectly optimize some bounds of the targeted marginal log-likelihood . in contrast , we successfully develop a new class of algorithms , based on stochastic approximation ( sa ) theory of the robbins-monro type , to directly optimize the marginal log-likelihood and simultaneously minimize the inclusive kl-divergence . the resulting learning algorithm is thus called joint sa ( jsa ) . moreover , we construct an effective mcmc operator for jsa . our results on the mnist datasets demonstrate that the jsa 's performance is consistently superior to that of competing algorithms like rws , for learning a range of difficult models .", "topics": ["generative model", "approximation"]}
{"title": "from imitation to prediction , data compression vs recurrent neural networks for natural language processing", "abstract": "in recent studies [ 1 ] [ 13 ] [ 12 ] recurrent neural networks were used for generative processes and their surprising performance can be explained by their ability to create good predictions . in addition , data compression is also based on predictions . what the problem comes down to is whether a data compressor could be used to perform as well as recurrent neural networks in natural language processing tasks . if this is possible , then the problem comes down to determining if a compression algorithm is even more intelligent than a neural network in specific tasks related to human language . in our journey we discovered what we think is the fundamental difference between a data compression algorithm and a recurrent neural network .", "topics": ["natural language processing", "recurrent neural network"]}
{"title": "belief flows of robust online learning", "abstract": "this paper introduces a new probabilistic model for online learning which dynamically incorporates information from stochastic gradients of an arbitrary loss function . similar to probabilistic filtering , the model maintains a gaussian belief over the optimal weight parameters . unlike traditional bayesian updates , the model incorporates a small number of gradient evaluations at locations chosen using thompson sampling , making it computationally tractable . the belief is then transformed via a linear flow field which optimally updates the belief distribution using rules derived from information theoretic principles . several versions of the algorithm are shown using different constraints on the flow field and compared with conventional online learning algorithms . results are given for several classification tasks including logistic regression and multilayer neural networks .", "topics": ["sampling ( signal processing )", "mathematical optimization"]}
{"title": "extending binary qualitative direction calculi with a granular distance concept : hidden feature attachment", "abstract": "in this paper we introduce a method for extending binary qualitative direction calculi with adjustable granularity like opram or the star calculus with a granular distance concept . this method is similar to the concept of extending points with an internal reference direction to get oriented points which are the basic entities in the opram calculus . even if the spatial objects are from a geometrical point of view infinitesimal small points locally available reference measures are attached . in the case of opram , a reference direction is attached . the same principle works also with local reference distances which are called elevations . the principle of attaching references features to a point is called hidden feature attachment .", "topics": ["entity"]}
{"title": "a user simulator for task-completion dialogues", "abstract": "despite widespread interests in reinforcement-learning for task-oriented dialogue systems , several obstacles can frustrate research and development progress . first , reinforcement learners typically require interaction with the environment , so conventional dialogue corpora can not be used directly . second , each task presents specific challenges , requiring separate corpus of task-specific annotated data . third , collecting and annotating human-machine or human-human conversations for task-oriented dialogues requires extensive domain knowledge . because building an appropriate dataset can be both financially costly and time-consuming , one popular approach is to build a user simulator based upon a corpus of example dialogues . then , one can train reinforcement learning agents in an online fashion as they interact with the simulator . dialogue agents trained on these simulators can serve as an effective starting point . once agents master the simulator , they may be deployed in a real environment to interact with humans , and continue to be trained online . to ease empirical algorithmic comparisons in dialogues , this paper introduces a new , publicly available simulation framework , where our simulator , designed for the movie-booking domain , leverages both rules and collected data . the simulator supports two tasks : movie ticket booking and movie seeking . finally , we demonstrate several agents and detail the procedure to add and test your own agent in the proposed framework .", "topics": ["reinforcement learning", "simulation"]}
{"title": "a survey of the trends in facial and expression recognition databases and methods", "abstract": "automated facial identification and facial expression recognition have been topics of active research over the past few decades . facial and expression recognition find applications in human-computer interfaces , subject tracking , real-time security surveillance systems and social networking . several holistic and geometric methods have been developed to identify faces and expressions using public and local facial image databases . in this work we present the evolution in facial image data sets and the methodologies for facial identification and recognition of expressions such as anger , sadness , happiness , disgust , fear and surprise . we observe that most of the earlier methods for facial and expression recognition aimed at improving the recognition rates for facial feature-based methods using static images . however , the recent methodologies have shifted focus towards robust implementation of facial/expression recognition from large image databases that vary with space ( gathered from the internet ) and time ( video recordings ) . the evolution trends in databases and methodologies for facial and expression recognition can be useful for assessing the next-generation topics that may have applications in security systems or personal identification systems that involve `` quantitative face '' assessments .", "topics": ["database"]}
{"title": "femoral rois and entropy for texture-based detection of osteoarthritis from high-resolution knee radiographs", "abstract": "the relationship between knee osteoarthritis progression and changes in tibial bone structure has long been recognized and various texture descriptors have been proposed to detect early osteoarthritis ( oa ) from radiographs . this work aims to investigate ( 1 ) femoral textures as an oa indicator and ( 2 ) the potential of entropy as a computationally efficient alternative to established texture descriptors . we design a robust semi-automatically placed layout for regions of interest ( roi ) , compute the hurst coefficient and the entropy in each roi , and employ statistical and machine learning methods to evaluate feature combinations . based on 153 high-resolution radiographs , our results identify medial femur as an effective univariate descriptor , with significance comparable to medial tibia . entropy is shown to contribute to classification performance . a linear five-feature classifier combining femur , entropic and standard texture descriptors , achieves auc of 0.85 , outperforming the state-of-the-art by roughly 0.1 .", "topics": ["computational complexity theory", "coefficient"]}
{"title": "a totally unimodular view of structured sparsity", "abstract": "this paper describes a simple framework for structured sparse recovery based on convex optimization . we show that many structured sparsity models can be naturally represented by linear matrix inequalities on the support of the unknown parameters , where the constraint matrix has a totally unimodular ( tu ) structure . for such structured models , tight convex relaxations can be obtained in polynomial time via linear programming . our modeling framework unifies the prevalent structured sparsity norms in the literature , introduces new interesting ones , and renders their tightness and tractability arguments transparent .", "topics": ["sparse matrix", "coefficient"]}
{"title": "the embedding dimension of laplacian eigenfunction maps", "abstract": "any closed , connected riemannian manifold $ m $ can be smoothly embedded by its laplacian eigenfunction maps into $ \\mathbb { r } ^m $ for some $ m $ . we call the smallest such $ m $ the maximal embedding dimension of $ m $ . we show that the maximal embedding dimension of $ m $ is bounded from above by a constant depending only on the dimension of $ m $ , a lower bound for injectivity radius , a lower bound for ricci curvature , and a volume bound . we interpret this result for the case of surfaces isometrically immersed in $ \\mathbb { r } ^3 $ , showing that the maximal embedding dimension only depends on bounds for the gaussian curvature , mean curvature , and surface area . furthermore , we consider the relevance of these results for shape registration .", "topics": ["map", "relevance"]}
{"title": "communication-efficient false discovery rate control via knockoff aggregation", "abstract": "the false discovery rate ( fdr ) -- -the expected fraction of spurious discoveries among all the discoveries -- -provides a popular statistical assessment of the reproducibility of scientific studies in various disciplines . in this work , we introduce a new method for controlling the fdr in meta-analysis of many decentralized linear models . our method targets the scenario where many research groups -- -possibly the number of which is random -- -are independently testing a common set of hypotheses and then sending summary statistics to a coordinating center in an online manner . built on the knockoffs framework introduced by barber and candes ( 2015 ) , our procedure starts by applying the knockoff filter to each linear model and then aggregates the summary statistics via one-shot communication in a novel way . this method gives exact fdr control non-asymptotically without any knowledge of the noise variances or making any assumption about sparsity of the signal . in certain settings , it has a communication complexity that is optimal up to a logarithmic factor .", "topics": ["sparse matrix"]}
{"title": "on wasserstein reinforcement learning and the fokker-planck equation", "abstract": "policy gradients methods often achieve better performance when the change in policy is limited to a small kullback-leibler divergence . we derive policy gradients where the change in policy is limited to a small wasserstein distance ( or trust region ) . this is done in the discrete and continuous multi-armed bandit settings with entropy regularisation . we show that in the small steps limit with respect to the wasserstein distance $ w_2 $ , policy dynamics are governed by the fokker-planck ( heat ) equation , following the jordan-kinderlehrer-otto result . this means that policies undergo diffusion and advection , concentrating near actions with high reward . this helps elucidate the nature of convergence in the probability matching setup , and provides justification for empirical practices such as gaussian policy priors and additive gradient noise .", "topics": ["reinforcement learning"]}
{"title": "identifying bengali multiword expressions using semantic clustering", "abstract": "one of the key issues in both natural language understanding and generation is the appropriate processing of multiword expressions ( mwes ) . mwes pose a huge problem to the precise language processing due to their idiosyncratic nature and diversity in lexical , syntactical and semantic properties . the semantics of a mwe can not be expressed after combining the semantics of its constituents . therefore , the formalism of semantic clustering is often viewed as an instrument for extracting mwes especially for resource constraint languages like bengali . the present semantic clustering approach contributes to locate clusters of the synonymous noun tokens present in the document . these clusters in turn help measure the similarity between the constituent words of a potentially candidate phrase using a vector space model and judge the suitability of this phrase to be a mwe . in this experiment , we apply the semantic clustering approach for noun-noun bigram mwes , though it can be extended to any types of mwes . in parallel , the well known statistical models , namely point-wise mutual information ( pmi ) , log likelihood ratio ( llr ) , significance function are also employed to extract mwes from the bengali corpus . the comparative evaluation shows that the semantic clustering approach outperforms all other competing statistical models . as a by-product of this experiment , we have started developing a standard lexicon in bengali that serves as a productive bengali linguistic thesaurus .", "topics": ["natural language processing", "cluster analysis"]}
{"title": "punjabi language interface to database : a brief review", "abstract": "unlike most user-computer interfaces , a natural language interface allows users to communicate fluently with a computer system with very little preparation . databases are often hard to use in cooperating with the users because of their rigid interface . a good nlidb allows a user to enter commands and ask questions in native language and then after interpreting respond to the user in native language . for a large number of applications requiring interaction between humans and the computer systems , it would be convenient to provide the end-user friendly interface . punjabi language interface to database would proof fruitful to native people of punjab , as it provides ease to them to use various e-governance applications like punjab sewa , suwidha , online public utility forms , online grievance cell , land records management system , legacy matters , e-district , agriculture , etc . punjabi is the mother tongue of more than 110 million people all around the world . according to available information , punjabi ranks 10th from top out of a total of 6,900 languages recognized internationally by the united nations . this paper covers a brief overview of the natural language interface to database , its different components , its advantages , disadvantages , approaches and techniques used . the paper ends with the work done on punjabi language interface to database and future enhancements that can be done .", "topics": ["natural language"]}
{"title": "attractor metadynamics in adapting neural networks", "abstract": "slow adaption processes , like synaptic and intrinsic plasticity , abound in the brain and shape the landscape for the neural dynamics occurring on substantially faster timescales . at any given time the network is characterized by a set of internal parameters , which are adapting continuously , albeit slowly . this set of parameters defines the number and the location of the respective adiabatic attractors . the slow evolution of network parameters hence induces an evolving attractor landscape , a process which we term attractor metadynamics . we study the nature of the metadynamics of the attractor landscape for several continuous-time autonomous model networks . we find both first- and second-order changes in the location of adiabatic attractors and argue that the study of the continuously evolving attractor landscape constitutes a powerful tool for understanding the overall development of the neural dynamics .", "topics": ["neural networks", "autonomous car"]}
{"title": "scenenet : understanding real world indoor scenes with synthetic data", "abstract": "scene understanding is a prerequisite to many high level tasks for any automated intelligent machine operating in real world environments . recent attempts with supervised learning have shown promise in this direction but also highlighted the need for enormous quantity of supervised data -- - performance increases in proportion to the amount of data used . however , this quickly becomes prohibitive when considering the manual labour needed to collect such data . in this work , we focus our attention on depth based semantic per-pixel labelling as a scene understanding problem and show the potential of computer graphics to generate virtually unlimited labelled data from synthetic 3d scenes . by carefully synthesizing training data with appropriate noise models we show comparable performance to state-of-the-art rgbd systems on nyuv2 dataset despite using only depth data as input and set a benchmark on depth-based segmentation on sun rgb-d dataset . additionally , we offer a route to generating synthesized frame or video data , and understanding of different factors influencing performance gains .", "topics": ["test set", "supervised learning"]}
{"title": "reinforcement learning for bandit neural machine translation with simulated human feedback", "abstract": "machine translation is a natural candidate problem for reinforcement learning from human feedback : users provide quick , dirty ratings on candidate translations to guide a system to improve . yet , current neural machine translation training focuses on expensive human-generated reference translations . we describe a reinforcement learning algorithm that improves neural machine translation systems from simulated human feedback . our algorithm combines the advantage actor-critic algorithm ( mnih et al . , 2016 ) with the attention-based neural encoder-decoder architecture ( luong et al . , 2015 ) . this algorithm ( a ) is well-designed for problems with a large action space and delayed rewards , ( b ) effectively optimizes traditional corpus-level machine translation metrics , and ( c ) is robust to skewed , high-variance , granular feedback modeled after actual human behaviors .", "topics": ["machine translation", "reinforcement learning"]}
{"title": "disambiguating bilingual nominal entries against wordnet", "abstract": "this paper explores the acquisition of conceptual knowledge from bilingual dictionaries ( french/english , spanish/english and english/spanish ) using a pre-existing broad coverage lexical knowledge base ( lkb ) wordnet . bilingual nominal entries are disambiguated agains wordnet , therefore linking the bilingual dictionaries to wordnet yielding a multilingual lkb ( mlkb ) . the resulting mlkb has the same structure as wordnet , but some nodes are attached additionally to disambiguated vocabulary of other languages . two different , complementary approaches are explored . in one of the approaches each entry of the dictionary is taken in turn , exploiting the information in the entry itself . the inferential capability for disambiguating the translation is given by semantic density over wordnet . in the other approach , the bilingual dictionary was merged with wordnet , exploiting mainly synonymy relations . each of the approaches was used in a different dictionary . both approaches attain high levels of precision on their own , showing that disambiguating bilingual nominal entries , and therefore linking bilingual dictionaries to wordnet is a feasible task .", "topics": ["natural language processing", "machine translation"]}
{"title": "syntagma lexical database", "abstract": "this paper discusses the structure of syntagma 's lexical database ( focused on italian ) . the basic database consists in four tables . table forms contains word inflections , used by the pos-tagger for the identification of input-words . forms is related to lemma . table lemma stores all kinds of grammatical features of words , word-level semantic data and restrictions . in the table meanings meaning-related data are stored : definition , examples , domain , and semantic information . table valency contains the argument structure of each meaning , with syntactic and semantic features for each argument . the extended version of sld contains the links to syntagma 's semantic net and to the wordnet synsets of other languages .", "topics": ["database", "dictionary"]}
{"title": "rapid feature learning with stacked linear denoisers", "abstract": "we investigate unsupervised pre-training of deep architectures as feature generators for `` shallow '' classifiers . stacked denoising autoencoders ( sda ) , when used as feature pre-processing tools for svm classification , can lead to significant improvements in accuracy - however , at the price of a substantial increase in computational cost . in this paper we create a simple algorithm which mimics the layer by layer training of sdas . however , in contrast to sdas , our algorithm requires no training through gradient descent as the parameters can be computed in closed-form . it can be implemented in less than 20 lines of matlabtmand reduces the computation time from several hours to mere seconds . we show that our feature transformation reliably improves the results of svm classification significantly on all our data sets - often outperforming sdas and even deep neural networks in three out of four deep learning benchmarks .", "topics": ["feature learning", "time complexity"]}
{"title": "optimal and adaptive algorithms for online boosting", "abstract": "we study online boosting , the task of converting any weak online learner into a strong online learner . based on a novel and natural definition of weak online learnability , we develop two online boosting algorithms . the first algorithm is an online version of boost-by-majority . by proving a matching lower bound , we show that this algorithm is essentially optimal in terms of the number of weak learners and the sample complexity needed to achieve a specified accuracy . this optimal algorithm is not adaptive however . using tools from online loss minimization , we derive an adaptive online boosting algorithm that is also parameter-free , but not optimal . both algorithms work with base learners that can handle example importance weights directly , as well as by rejection sampling examples with probability defined by the booster . results are complemented with an extensive experimental study .", "topics": ["sampling ( signal processing )"]}
{"title": "addressing limited data for textual entailment across domains", "abstract": "we seek to address the lack of labeled data ( and high cost of annotation ) for textual entailment in some domains . to that end , we first create ( for experimental purposes ) an entailment dataset for the clinical domain , and a highly competitive supervised entailment system , ent , that is effective ( out of the box ) on two domains . we then explore self-training and active learning strategies to address the lack of labeled data . with self-training , we successfully exploit unlabeled data to improve over ent by 15 % f-score on the newswire domain , and 13 % f-score on clinical data . on the other hand , our active learning experiments demonstrate that we can match ( and even beat ) ent using only 6.6 % of the training data in the clinical domain , and only 5.8 % of the training data in the newswire domain .", "topics": ["test set"]}
{"title": "optimal stochastic package delivery planning with deadline : a cardinality minimization in routing", "abstract": "vehicle routing problem with private fleet and common carrier ( vrppc ) has been proposed to help a supplier manage package delivery services from a single depot to multiple customers . most of the existing vrppc works consider deterministic parameters which may not be practical and uncertainty has to be taken into account . in this paper , we propose the optimal stochastic delivery planning with deadline ( odpd ) to help a supplier plan and optimize the package delivery . the aim of odpd is to service all customers within a given deadline while considering the randomness in customer demands and traveling time . we formulate the odpd as a stochastic integer programming , and use the cardinality minimization approach for calculating the deadline violation probability . to accelerate computation , the l-shaped decomposition method is adopted . we conduct extensive performance evaluation based on real customer locations and traveling time from google map .", "topics": ["computation"]}
{"title": "real time ridge orientation estimation for fingerprint images", "abstract": "fingerprint verification is an important bio-metric technique for personal identification . most of the automatic verification systems are based on matching of fingerprint minutiae . extraction of minutiae is an essential process which requires estimation of orientation of the lines in an image . most of the existing methods involve intense mathematical computations and hence are performed through software means . in this paper a hardware scheme to perform real time orientation estimation is presented which is based on pipelined architecture . synthesized circuits proved the functionality and accuracy of the suggested method .", "topics": ["computation"]}
{"title": "fusing bird view lidar point cloud and front view camera image for deep object detection", "abstract": "we propose a new method for fusing a lidar point cloud and camera-captured images in the deep convolutional neural network ( cnn ) . the proposed method constructs a new layer called non-homogeneous pooling layer to transform features between bird view map and front view map . the sparse lidar point cloud is used to construct the mapping between the two maps . the pooling layer allows efficient fusion of the bird view and front view features at any stage of the network . this is favorable for the 3d-object detection using camera-lidar fusion in autonomous driving scenarios . a corresponding deep cnn is designed and tested on the kitti bird view object detection dataset , which produces 3d bounding boxes from the bird view map . the fusion method shows particular benefit for detection of pedestrians in the bird view compared to other fusion-based object detection networks .", "topics": ["object detection", "map"]}
{"title": "comparison of smt and rbmt ; the requirement of hybridization for marathi-hindi mt", "abstract": "we present in this paper our work on comparison between statistical machine translation ( smt ) and rule-based machine translation for translation from marathi to hindi . rule based systems although robust take lots of time to build . on the other hand statistical machine translation systems are easier to create , maintain and improve upon . we describe the development of a basic marathi-hindi smt system and evaluate its performance . through a detailed error analysis , we , point out the relative strengths and weaknesses of both systems . effectively , we shall see that even with a small amount of training corpus a statistical machine translation system has many advantages for high quality domain specific machine translation over that of a rule-based counterpart .", "topics": ["machine translation", "text corpus"]}
{"title": "solving non-parametric inverse problem in continuous markov random field using loopy belief propagation", "abstract": "in this paper , we address the inverse problem , or the statistical machine learning problem , in markov random fields with a non-parametric pair-wise energy function with continuous variables . the inverse problem is formulated by maximum likelihood estimation . the exact treatment of maximum likelihood estimation is intractable because of two problems : ( 1 ) it includes the evaluation of the partition function and ( 2 ) it is formulated in the form of functional optimization . we avoid problem ( 1 ) by using bethe approximation . bethe approximation is an approximation technique equivalent to the loopy belief propagation . problem ( 2 ) can be solved by using orthonormal function expansion . orthonormal function expansion can reduce a functional optimization problem to a function optimization problem . our method can provide an analytic form of the solution of the inverse problem within the framework of bethe approximation .", "topics": ["optimization problem", "mathematical optimization"]}
{"title": "pac-bayes-bernstein inequality for martingales and its application to multiarmed bandits", "abstract": "we develop a new tool for data-dependent analysis of the exploration-exploitation trade-off in learning under limited feedback . our tool is based on two main ingredients . the first ingredient is a new concentration inequality that makes it possible to control the concentration of weighted averages of multiple ( possibly uncountably many ) simultaneously evolving and interdependent martingales . the second ingredient is an application of this inequality to the exploration-exploitation trade-off via importance weighted sampling . we apply the new tool to the stochastic multiarmed bandit problem , however , the main importance of this paper is the development and understanding of the new tool rather than improvement of existing algorithms for stochastic multiarmed bandits . in the follow-up work we demonstrate that the new tool can improve over state-of-the-art in structurally richer problems , such as stochastic multiarmed bandits with side information ( seldin et al . , 2011a ) .", "topics": ["regret ( decision theory )"]}
{"title": "diversifying sparsity using variational determinantal point processes", "abstract": "we propose a novel diverse feature selection method based on determinantal point processes ( dpps ) . our model enables one to flexibly define diversity based on the covariance of features ( similar to orthogonal matching pursuit ) or alternatively based on side information . we introduce our approach in the context of bayesian sparse regression , employing a dpp as a variational approximation to the true spike and slab posterior distribution . we subsequently show how this variational dpp approximation generalizes and extends mean-field approximation , and can be learned efficiently by exploiting the fast sampling properties of dpps . our motivating application comes from bioinformatics , where we aim to identify a diverse set of genes whose expression profiles predict a tumor type where the diversity is defined with respect to a gene-gene interaction network . we also explore an application in spatial statistics . in both cases , we demonstrate that the proposed method yields significantly more diverse feature sets than classic sparse methods , without compromising accuracy .", "topics": ["calculus of variations", "approximation"]}
{"title": "efficient variational inference in large-scale bayesian compressed sensing", "abstract": "we study linear models under heavy-tailed priors from a probabilistic viewpoint . instead of computing a single sparse most probable ( map ) solution as in standard deterministic approaches , the focus in the bayesian compressed sensing framework shifts towards capturing the full posterior distribution on the latent variables , which allows quantifying the estimation uncertainty and learning model parameters using maximum likelihood . the exact posterior distribution under the sparse linear model is intractable and we concentrate on variational bayesian techniques to approximate it . repeatedly computing gaussian variances turns out to be a key requisite and constitutes the main computational bottleneck in applying variational techniques in large-scale problems . we leverage on the recently proposed perturb-and-map algorithm for drawing exact samples from gaussian markov random fields ( gmrf ) . the main technical contribution of our paper is to show that estimating gaussian variances using a relatively small number of such efficiently drawn random samples is much more effective than alternative general-purpose variance estimation techniques . by reducing the problem of variance estimation to standard optimization primitives , the resulting variational algorithms are fully scalable and parallelizable , allowing bayesian computations in extremely large-scale problems with the same memory and time complexity requirements as conventional point estimation techniques . we illustrate these ideas with experiments in image deblurring .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "nearest descent , in-tree , and clustering", "abstract": "in this paper , we propose a physically inspired graph-theoretical clustering method , which first makes the data points organized into an attractive graph , called in-tree , via a physically inspired rule , called nearest descent ( nd ) . in particular , the rule of nd works to select the nearest node in the descending direction of potential as the parent node of each node , which is in essence different from the classical gradient descent or steepest descent . the constructed in-tree proves a very good candidate for clustering due to its particular features and properties . in the in-tree , the original clustering problem is reduced to a problem of removing a very few of undesired edges from this graph . pleasingly , the undesired edges in in-tree are so distinguishable that they can be easily determined in either automatic or interactive way , which is in stark contrast to the cases in the widely used minimal spanning tree and k-nearest-neighbor graph . the cluster number in the proposed method can be easily determined based on some intermediate plots , and the cluster assignment for each node is easily made by quickly searching its root node in each sub-graph ( also an in-tree ) . the proposed method is extensively evaluated on both synthetic and real-world datasets . overall , the proposed clustering method is a density-based one , but shows significant differences and advantages in comparison to the traditional ones . the proposed method is simple yet efficient and reliable , and is applicable to various datasets with diverse shapes , attributes and any high dimensionality", "topics": ["cluster analysis", "approximation algorithm"]}
{"title": "a formal calculus for international relations computation and evaluation", "abstract": "this publication presents a relation computation or calculus for international relations using a mathematical modeling . it examined trust for international relations and its calculus , which related to bayesian inference , dempster-shafer theory and subjective logic . based on an observation in the literature , we found no literature discussing the calculus method for the international relations . to bridge this research gap , we propose a relation algebra method for international relations computation . the proposed method will allow a relation computation which is previously subjective and incomputable . we also present three international relations as case studies to demonstrate the proposed method is a real-world scenario . the method will deliver the relation computation for the international relations that to support decision makers in a government such as foreign ministry , defense ministry , presidential or prime minister office . the department of defense ( dod ) may use our method to determine a nation that can be identified as a friendly , neutral or hostile nation .", "topics": ["computation"]}
{"title": "predicting online user behaviour using deep learning algorithms", "abstract": "we propose a robust classifier to predict buying intentions based on user behaviour within a large e-commerce website . in this work we compare traditional machine learning techniques with the most advanced deep learning approaches . we show that both deep belief networks and stacked denoising auto-encoders achieved a substantial improvement by extracting features from high dimensional data during the pre-train phase . they prove also to be more convenient to deal with severe class imbalance .", "topics": ["mathematical optimization", "noise reduction"]}
{"title": "online learning for changing environments using coin betting", "abstract": "a key challenge in online learning is that classical algorithms can be slow to adapt to changing environments . recent studies have proposed `` meta '' algorithms that convert any online learning algorithm to one that is adaptive to changing environments , where the adaptivity is analyzed in a quantity called the strongly-adaptive regret . this paper describes a new meta algorithm that has a strongly-adaptive regret bound that is a factor of $ \\sqrt { \\log ( t ) } $ better than other algorithms with the same time complexity , where $ t $ is the time horizon . we also extend our algorithm to achieve a first-order ( i.e . , dependent on the observed losses ) strongly-adaptive regret bound for the first time , to our knowledge . at its heart is a new parameter-free algorithm for the learning with expert advice ( lea ) problem in which experts sometimes do not output advice for consecutive time steps ( i.e . , \\emph { sleeping } experts ) . this algorithm is derived by a reduction from optimal algorithms for the so-called coin betting problem . empirical results show that our algorithm outperforms state-of-the-art methods in both learning with expert advice and metric learning scenarios .", "topics": ["regret ( decision theory )", "time complexity"]}
{"title": "bayesian segnet : model uncertainty in deep convolutional encoder-decoder architectures for scene understanding", "abstract": "we present a deep learning framework for probabilistic pixel-wise semantic segmentation , which we term bayesian segnet . semantic segmentation is an important tool for visual scene understanding and a meaningful measure of uncertainty is essential for decision making . our contribution is a practical system which is able to predict pixel-wise class labels with a measure of model uncertainty . we achieve this by monte carlo sampling with dropout at test time to generate a posterior distribution of pixel class labels . in addition , we show that modelling uncertainty improves segmentation performance by 2-3 % across a number of state of the art architectures such as segnet , fcn and dilation network , with no additional parametrisation . we also observe a significant improvement in performance for smaller datasets where modelling uncertainty is more effective . we benchmark bayesian segnet on the indoor sun scene understanding and outdoor camvid driving scenes datasets .", "topics": ["sampling ( signal processing )", "encoder"]}
{"title": "fast bounded online gradient descent algorithms for scalable kernel-based online learning", "abstract": "kernel-based online learning has often shown state-of-the-art performance for many online learning tasks . it , however , suffers from a major shortcoming , that is , the unbounded number of support vectors , making it non-scalable and unsuitable for applications with large-scale datasets . in this work , we study the problem of bounded kernel-based online learning that aims to constrain the number of support vectors by a predefined budget . although several algorithms have been proposed in literature , they are neither computationally efficient due to their intensive budget maintenance strategy nor effective due to the use of simple perceptron algorithm . to overcome these limitations , we propose a framework for bounded kernel-based online learning based on an online gradient descent approach . we propose two efficient algorithms of bounded online gradient descent ( bogd ) for scalable kernel-based online learning : ( i ) bogd by maintaining support vectors using uniform sampling , and ( ii ) bogd++ by maintaining support vectors using non-uniform sampling . we present theoretical analysis of regret bound for both algorithms , and found promising empirical performance in terms of both efficacy and efficiency by comparing them to several well-known algorithms for bounded kernel-based online learning on large-scale datasets .", "topics": ["sampling ( signal processing )", "regret ( decision theory )"]}
{"title": "neobility at semeval-2017 task 1 : an attention-based sentence similarity model", "abstract": "this paper describes a neural-network model which performed competitively ( top 6 ) at the semeval 2017 cross-lingual semantic textual similarity ( sts ) task . our system employs an attention-based recurrent neural network model that optimizes the sentence similarity . in this paper , we describe our participation in the multilingual sts task which measures similarity across english , spanish , and arabic .", "topics": ["recurrent neural network"]}
{"title": "uncovering the dynamics of crowdlearning and the value of knowledge", "abstract": "learning from the crowd has become increasingly popular in the web and social media . there is a wide variety of crowdlearning sites in which , on the one hand , users learn from the knowledge that other users contribute to the site , and , on the other hand , knowledge is reviewed and curated by the same users using assessment measures such as upvotes or likes . in this paper , we present a probabilistic modeling framework of crowdlearning , which uncovers the evolution of a user 's expertise over time by leveraging other users ' assessments of her contributions . the model allows for both off-site and on-site learning and captures forgetting of knowledge . we then develop a scalable estimation method to fit the model parameters from millions of recorded learning and contributing events . we show the effectiveness of our model by tracing activity of ~25 thousand users in stack overflow over a 4.5 year period . we find that answers with high knowledge value are rare . newbies and experts tend to acquire less knowledge than users in the middle range . prolific learners tend to be also proficient contributors that post answers with high knowledge value .", "topics": ["scalability"]}
{"title": "attentive explanations : justifying decisions and pointing to the evidence", "abstract": "deep models are the defacto standard in visual decision models due to their impressive performance on a wide array of visual tasks . however , they are frequently seen as opaque and are unable to explain their decisions . in contrast , humans can justify their decisions with natural language and point to the evidence in the visual world which led to their decisions . we postulate that deep models can do this as well and propose our pointing and justification ( pj-x ) model which can justify its decision with a sentence and point to the evidence by introspecting its decision and explanation process using an attention mechanism . unfortunately there is no dataset available with reference explanations for visual decision making . we thus collect two datasets in two domains where it is interesting and challenging to explain decisions . first , we extend the visual question answering task to not only provide an answer but also a natural language explanation for the answer . second , we focus on explaining human activities which is traditionally more challenging than object classification . we extensively evaluate our pj-x model , both on the justification and pointing tasks , by comparing it to prior models and ablations using both automatic and human evaluations .", "topics": ["natural language"]}
{"title": "fast clustering for scalable statistical analysis on structured images", "abstract": "the use of brain images as markers for diseases or behavioral differences is challenged by the small effects size and the ensuing lack of power , an issue that has incited researchers to rely more systematically on large cohorts . coupled with resolution increases , this leads to very large datasets . a striking example in the case of brain imaging is that of the human connectome project : 20 terabytes of data and growing . the resulting data deluge poses severe challenges regarding the tractability of some processing steps ( discriminant analysis , multivariate models ) due to the memory demands posed by these data . in this work , we revisit dimension reduction approaches , such as random projections , with the aim of replacing costly function evaluations by cheaper ones while decreasing the memory requirements . specifically , we investigate the use of alternate schemes , based on fast clustering , that are well suited for signals exhibiting a strong spatial structure , such as anatomical and functional brain images . our contribution is twofold : i ) we propose a linear-time clustering scheme that bypasses the percolation issues inherent in these algorithms and thus provides compressions nearly as good as traditional quadratic-complexity variance-minimizing clustering schemes , ii ) we show that cluster-based compression can have the virtuous effect of removing high-frequency noise , actually improving subsequent estimations steps . as a consequence , the proposed approach yields very accurate models on several large-scale problems yet with impressive gains in computational efficiency , making it possible to analyze large datasets .", "topics": ["cluster analysis", "time complexity"]}
{"title": "an online learning approach to generative adversarial networks", "abstract": "we consider the problem of training generative models with a generative adversarial network ( gan ) . although gans can accurately model complex distributions , they are known to be difficult to train due to instabilities caused by a difficult minimax optimization problem . in this paper , we view the problem of training gans as finding a mixed strategy in a zero-sum game . building on ideas from online learning we propose a novel training method named chekhov gan 1 . on the theory side , we show that our method provably converges to an equilibrium for semi-shallow gan architectures , i.e . architectures where the discriminator is a one layer network and the generator is arbitrary . on the practical side , we develop an efficient heuristic guided by our theoretical results , which we apply to commonly used deep gan architectures . on several real world tasks our approach exhibits improved stability and performance compared to standard gan training .", "topics": ["generative model", "optimization problem"]}
{"title": "sapa : a multi-objective metric temporal planner", "abstract": "sapa is a domain-independent heuristic forward chaining planner that can handle durative actions , metric resource constraints , and deadline goals . it is designed to be capable of handling the multi-objective nature of metric temporal planning . our technical contributions include ( i ) planning-graph based methods for deriving heuristics that are sensitive to both cost and makespan ( ii ) techniques for adjusting the heuristic estimates to take action interactions and metric resource limitations into account and ( iii ) a linear time greedy post-processing technique to improve execution flexibility of the solution plans . an implementation of sapa using many of the techniques presented in this paper was one of the best domain independent planners for domains with metric and temporal constraints in the third international planning competition , held at aips-02 . we describe the technical details of extracting the heuristics and present an empirical evaluation of the current implementation of sapa .", "topics": ["time complexity", "interaction"]}
{"title": "nfl play prediction", "abstract": "based on nfl game data we try to predict the outcome of a play in multiple different ways . an application of this is the following : by plugging in various play options one could determine the best play for a given situation in real time . while the outcome of a play can be described in many ways we had the most promising results with a newly defined measure that we call `` progress '' . we see this work as a first step to include predictive analysis into nfl playcalling .", "topics": ["support vector machine", "neural networks"]}
{"title": "identification of refugee influx patterns in greece via model-theoretic analysis of daily arrivals", "abstract": "the refugee crisis is perhaps the single most challenging problem for europe today . hundreds of thousands of people have already traveled across dangerous sea passages from turkish shores to greek islands , resulting in thousands of dead and missing , despite the best rescue efforts from both sides . one of the main reasons is the total lack of any early warning-alerting system , which could provide some preparation time for the prompt and effective deployment of resources at the hot zones . this work is such an attempt for a systemic analysis of the refugee influx in greece , aiming at ( a ) the statistical and signal-level characterization of the smuggling networks and ( b ) the formulation and preliminary assessment of such models for predictive purposes , i.e . , as the basis of such an early warning-alerting protocol . to our knowledge , this is the first-ever attempt to design such a system , since this refugee crisis itself and its geographical properties are unique ( intense event handling , little or no warning ) . the analysis employs a wide range of statistical , signal-based and matrix factorization ( decomposition ) techniques , including linear & linear-cosine regression , spectral analysis , arma , svd , probabilistic pca , ica , k-svd for dictionary learning , as well as fractal dimension analysis . it is established that the behavioral patterns of the smuggling networks closely match ( as expected ) the regular burst and pause periods of store-and-forward networks in digital communications . there are also major periodic trends in the range of 6.2-6.5 days and strong correlations in lags of four or more days , with distinct preference in the sunday-monday 48-hour time frame . these results show that such models can be used successfully for short-term forecasting of the influx intensity , producing an invaluable operational asset for planners , decision-makers and first-responders .", "topics": ["dictionary"]}
{"title": "vehicle detection and tracking techniques : a concise review", "abstract": "vehicle detection and tracking applications play an important role for civilian and military applications such as in highway traffic surveillance control , management and urban traffic planning . vehicle detection process on road are used for vehicle tracking , counts , average speed of each individual vehicle , traffic analysis and vehicle categorizing objectives and may be implemented under different environments changes . in this review , we present a concise overview of image processing methods and analysis tools which used in building these previous mentioned applications that involved developing traffic surveillance systems . more precisely and in contrast with other reviews , we classified the processing methods under three categories for more clarification to explain the traffic systems .", "topics": ["image processing"]}
{"title": "the zipml framework for training models with end-to-end low precision : the cans , the cannots , and a little bit of deep learning", "abstract": "recently there has been significant interest in training machine-learning models at low precision : by reducing precision , one can reduce computation and communication by one order of magnitude . we examine training at reduced precision , both from a theoretical and practical perspective , and ask : is it possible to train models at end-to-end low precision with provable guarantees ? can this lead to consistent order-of-magnitude speedups ? we present a framework called zipml to answer these questions . for linear models , the answer is yes . we develop a simple framework based on one simple but novel strategy called double sampling . our framework is able to execute training at low precision with no bias , guaranteeing convergence , whereas naive quantization would introduce significant bias . we validate our framework across a range of applications , and show that it enables an fpga prototype that is up to 6.5x faster than an implementation using full 32-bit precision . we further develop a variance-optimal stochastic quantization strategy and show that it can make a significant difference in a variety of settings . when applied to linear models together with double sampling , we save up to another 1.7x in data movement compared with uniform quantization . when training deep networks with quantized models , we achieve higher accuracy than the state-of-the-art xnor-net . finally , we extend our framework through approximation to non-linear models , such as svm . we show that , although using low-precision data induces bias , we can appropriately bound and control the bias . we find in practice 8-bit precision is often sufficient to converge to the correct solution . interestingly , however , in practice we notice that our framework does not always outperform the naive rounding approach . we discuss this negative result in detail .", "topics": ["approximation algorithm", "gradient descent"]}
{"title": "structured control nets for deep reinforcement learning", "abstract": "in recent years , deep reinforcement learning has made impressive advances in solving several important benchmark problems for sequential decision making . many control applications use a generic multilayer perceptron ( mlp ) for non-vision parts of the policy network . in this work , we propose a new neural network architecture for the policy network representation that is simple yet effective . the proposed structured control net ( scn ) splits the generic mlp into two separate sub-modules : a nonlinear control module and a linear control module . intuitively , the nonlinear control is for forward-looking and global control , while the linear control stabilizes the local dynamics around the residual of global control . we hypothesize that this will bring together the benefits of both linear and nonlinear policies : improve training sample efficiency , final episodic reward , and generalization of learned policy , while requiring a smaller network and being generally applicable to different training methods . we validated our hypothesis with competitive results on simulations from openai mujoco , roboschool , atari , and a custom 2d urban driving environment , with various ablation and generalization tests , trained with multiple black-box and policy gradient training methods . the proposed architecture has the potential to improve upon broader control tasks by incorporating problem specific priors into the architecture . as a case study , we demonstrate much improved performance for locomotion tasks by emulating the biological central pattern generators ( cpgs ) as the nonlinear part of the architecture .", "topics": ["nonlinear system", "reinforcement learning"]}
{"title": "mass displacement networks", "abstract": "despite the large improvements in performance attained by using deep learning in computer vision , one can often further improve results with some additional post-processing that exploits the geometric nature of the underlying task . this commonly involves displacing the posterior distribution of a cnn in a way that makes it more appropriate for the task at hand , e.g . better aligned with local image features , or more compact . in this work we integrate this geometric post-processing within a deep architecture , introducing a differentiable and probabilistically sound counterpart to the common geometric voting technique used for evidence accumulation in vision . we refer to the resulting neural models as mass displacement networks ( mdns ) , and apply them to human pose estimation in two distinct setups : ( a ) landmark localization , where we collapse a distribution to a point , allowing for precise localization of body keypoints and ( b ) communication across body parts , where we transfer evidence from one part to the other , allowing for a globally consistent pose estimate . we evaluate on large-scale pose estimation benchmarks , such as mpii human pose and coco datasets , and report systematic improvements when compared to strong baselines .", "topics": ["baseline ( configuration management )", "computer vision"]}
{"title": "connectivity in bag generation", "abstract": "this paper presents a pruning technique which can be used to reduce the number of paths searched in rule-based bag generators of the type proposed by \\cite { poznanskietal95 } and \\cite { popowich95 } . pruning the search space in these generators is important given the computational cost of bag generation . the technique relies on a connectivity constraint between the semantic indices associated with each lexical sign in a bag . testing the algorithm on a range of sentences shows reductions in the generation time and the number of edges constructed .", "topics": ["mathematical optimization", "natural language"]}
{"title": "a tale of two bases : local-nonlocal regularization on image patches with convolution framelets", "abstract": "we propose an image representation scheme combining the local and nonlocal characterization of patches in an image . our representation scheme can be shown to be equivalent to a tight frame constructed from convolving local bases ( e.g . wavelet frames , discrete cosine transforms , etc . ) with nonlocal bases ( e.g . spectral basis induced by nonlinear dimension reduction on patches ) , and we call the resulting frame elements { \\it convolution framelets } . insight gained from analyzing the proposed representation leads to a novel interpretation of a recent high-performance patch-based image inpainting algorithm using point integral method ( pim ) and low dimension manifold model ( ldmm ) [ osher , shi and zhu , 2016 ] . in particular , we show that ldmm is a weighted $ \\ell_2 $ -regularization on the coefficients obtained by decomposing images into linear combinations of convolution framelets ; based on this understanding , we extend the original ldmm to a reweighted version that yields further improved inpainting results . in addition , we establish the energy concentration property of convolution framelet coefficients for the setting where the local basis is constructed from a given nonlocal basis via a linear reconstruction framework ; a generalization of this framework to unions of local embeddings can provide a natural setting for interpreting bm3d , one of the state-of-the-art image denoising algorithms .", "topics": ["image processing", "noise reduction"]}
{"title": "multi channel-kernel canonical correlation analysis for cross-view person re-identification", "abstract": "in this paper we introduce a method to overcome one of the main challenges of person re-identification in multi-camera networks , namely cross-view appearance changes . the proposed solution addresses the extreme variability of person appearance in different camera views by exploiting multiple feature representations . for each feature , kernel canonical correlation analysis ( kcca ) with different kernels is exploited to learn several projection spaces in which the appearance correlation between samples of the same person observed from different cameras is maximized . an iterative logistic regression is finally used to select and weigh the contributions of each feature projections and perform the matching between the two views . experimental evaluation shows that the proposed solution obtains comparable performance on viper and prid 450s datasets and improves on prid and cuhk01 datasets with respect to the state of the art .", "topics": ["kernel ( operating system )"]}
{"title": "efficiently bounding optimal solutions after small data modification in large-scale empirical risk minimization", "abstract": "we study large-scale classification problems in changing environments where a small part of the dataset is modified , and the effect of the data modification must be quickly incorporated into the classifier . when the entire dataset is large , even if the amount of the data modification is fairly small , the computational cost of re-training the classifier would be prohibitively large . in this paper , we propose a novel method for efficiently incorporating such a data modification effect into the classifier without actually re-training it . the proposed method provides bounds on the unknown optimal classifier with the cost only proportional to the size of the data modification . we demonstrate through numerical experiments that the proposed method provides sufficiently tight bounds with negligible computational costs , especially when a small part of the dataset is modified in a large-scale classification problem .", "topics": ["numerical analysis"]}
{"title": "hierarchical structure-and-motion recovery from uncalibrated images", "abstract": "this paper addresses the structure-and-motion problem , that requires to find camera motion and 3d struc- ture from point matches . a new pipeline , dubbed samantha , is presented , that departs from the prevailing sequential paradigm and embraces instead a hierarchical approach . this method has several advantages , like a provably lower computational complexity , which is necessary to achieve true scalability , and better error containment , leading to more stability and less drift . moreover , a practical autocalibration procedure allows to process images without ancillary information . experiments with real data assess the accuracy and the computational efficiency of the method .", "topics": ["computational complexity theory", "scalability"]}
{"title": "abc-sg : a new artificial bee colony algorithm-based distance of sequential data using sigma grams", "abstract": "the problem of similarity search is one of the main problems in computer science . this problem has many applications in text-retrieval , web search , computational biology , bioinformatics and others . similarity between two data objects can be depicted using a similarity measure or a distance metric . there are numerous distance metrics in the literature , some are used for a particular data type , and others are more general . in this paper we present a new distance metric for sequential data which is based on the sum of n-grams . the novelty of our distance is that these n-grams are weighted using artificial bee colony ; a recent optimization algorithm based on the collective intelligence of a swarm of bees on their search for nectar . this algorithm has been used in optimizing a large number of numerical problems . we validate the new distance experimentally .", "topics": ["numerical analysis"]}
{"title": "ga-pso-optimized neural-based control scheme for adaptive congestion control to improve performance in multimedia applications", "abstract": "active queue control aims to improve the overall communication network throughput while providing lower delay and small packet loss rate . the basic idea is to actively trigger packet dropping ( or marking provided by explicit congestion notification ( ecn ) ) before buffer overflow . in this paper , two artificial neural networks ( ann ) -based control schemes are proposed for adaptive queue control in tcp communication networks . the structure of these controllers is optimized using genetic algorithm ( ga ) and the output weights of anns are optimized using particle swarm optimization ( pso ) algorithm . the controllers are radial bias function ( rbf ) -based , but to improve the robustness of rbf controller , an error-integral term is added to rbf equation in the second scheme . experimental results show that ga- pso-optimized improved rbf ( i-rbf ) model controls network congestion effectively in terms of link utilization with a low packet loss rate and outperform drop tail , proportional-integral ( pi ) , random exponential marking ( rem ) , and adaptive random early detection ( ared ) controllers .", "topics": ["time complexity"]}
{"title": "query complexity of clustering with side information", "abstract": "suppose , we are given a set of $ n $ elements to be clustered into $ k $ ( unknown ) clusters , and an oracle/expert labeler that can interactively answer pair-wise queries of the form , `` do two elements $ u $ and $ v $ belong to the same cluster ? '' . the goal is to recover the optimum clustering by asking the minimum number of queries . in this paper , we initiate a rigorous theoretical study of this basic problem of query complexity of interactive clustering , and provide strong information theoretic lower bounds , as well as nearly matching upper bounds . most clustering problems come with a similarity matrix , which is used by an automated process to cluster similar points together . our main contribution in this paper is to show the dramatic power of side information aka similarity matrix on reducing the query complexity of clustering . a similarity matrix represents noisy pair-wise relationships such as one computed by some function on attributes of the elements . a natural noisy model is where similarity values are drawn independently from some arbitrary probability distribution $ f_+ $ when the underlying pair of elements belong to the same cluster , and from some $ f_- $ otherwise . we show that given such a similarity matrix , the query complexity reduces drastically from $ \\theta ( nk ) $ ( no similarity matrix ) to $ o ( \\frac { k^2\\log { n } } { \\ch^2 ( f_+\\|f_- ) } ) $ where $ \\ch^2 $ denotes the squared hellinger divergence . moreover , this is also information-theoretic optimal within an $ o ( \\log { n } ) $ factor . our algorithms are all efficient , and parameter free , i.e . , they work without any knowledge of $ k , f_+ $ and $ f_- $ , and only depend logarithmically with $ n $ . along the way , our work also reveals intriguing connection to popular community detection models such as the { \\em stochastic block model } , significantly generalizes them , and opens up many venues for interesting future research .", "topics": ["cluster analysis", "heuristic"]}
{"title": "gram : graph-based attention model for healthcare representation learning", "abstract": "deep learning methods exhibit promising performance for predictive modeling in healthcare , but two important challenges remain : -data insufficiency : often in healthcare predictive modeling , the sample size is insufficient for deep learning methods to achieve satisfactory results . -interpretation : the representations learned by deep learning methods should align with medical knowledge . to address these challenges , we propose a graph-based attention model , gram that supplements electronic health records ( ehr ) with hierarchical information inherent to medical ontologies . based on the data volume and the ontology structure , gram represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism . we compared predictive performance ( i.e . accuracy , data needs , interpretability ) of gram to various methods including the recurrent neural network ( rnn ) in two sequential diagnoses prediction tasks and one heart failure prediction task . compared to the basic rnn , gram achieved 10 % higher accuracy for predicting diseases rarely observed in the training data and 3 % improved area under the roc curve for predicting heart failure using an order of magnitude less training data . additionally , unlike other methods , the medical concept representations learned by gram are well aligned with the medical ontology . finally , gram exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts .", "topics": ["test set", "recurrent neural network"]}
{"title": "accelerating learning in constructive predictive frameworks with the successor representation", "abstract": "here we propose using the successor representation ( sr ) to accelerate learning in a constructive knowledge system based on general value functions ( gvfs ) . in real-world settings like robotics for unstructured and dynamic environments , it is infeasible to model all meaningful aspects of a system and its environment by hand due to both complexity and size . instead , robots must be capable of learning and adapting to changes in their environment and task , incrementally constructing models from their own experience . gvfs , taken from the field of reinforcement learning ( rl ) , are a way of modeling the world as predictive questions . one approach to such models proposes a massive network of interconnected and interdependent gvfs , which are incrementally added over time . it is reasonable to expect that new , incrementally added predictions can be learned more swiftly if the learning process leverages knowledge gained from past experience . the sr provides such a means of separating the dynamics of the world from the prediction targets and thus capturing regularities that can be reused across multiple gvfs . as a primary contribution of this work , we show that using sr-based predictions can improve sample efficiency and learning speed in a continual learning setting where new predictions are incrementally added and learned over time . we analyze our approach in a grid-world and then demonstrate its potential on data from a physical robot arm .", "topics": ["value ( ethics )", "reinforcement learning"]}
{"title": "separating reflection and transmission images in the wild", "abstract": "the reflections caused by common semi-reflectors , such as glass windows , can severely impact the performance of computer vision algorithms . state-of-the-art works can successfully remove reflections on synthetic data and in controlled scenarios . however , they are based on strong assumptions and fail to generalize to real-world images -- -even when they leverage polarization . we present a deep learning approach to separate the reflected and the transmitted components of the recorded irradiance . key to our approach is our synthetic data generation , which accurately simulates reflections , including those generated by curved and non-ideal surfaces , and non-static scenes . we extensively validate our method against a number of related works on a new dataset of images captured in the wild .", "topics": ["synthetic data", "computer vision"]}
{"title": "value-directed belief state approximation for pomdps", "abstract": "we consider the problem belief-state monitoring for the purposes of implementing a policy for a partially-observable markov decision process ( pomdp ) , specifically how one might approximate the belief state . other schemes for belief-state approximation ( e.g . , based on minimixing a measures such as kl-diveregence between the true and estimated state ) are not necessarily appropriate for pomdps . instead we propose a framework for analyzing value-directed approximation schemes , where approximation quality is determined by the expected error in utility rather than by the error in the belief state itself . we propose heuristic methods for finding good projection schemes for belief state estimation - exhibiting anytime characteristics - given a pomdp value fucntion . we also describe several algorithms for constructing bounds on the error in decision quality ( expected utility ) associated with acting in accordance with a given belief state approximation .", "topics": ["approximation algorithm", "approximation"]}
{"title": "large-sample learning of bayesian networks is np-hard", "abstract": "in this paper , we provide new complexity results for algorithms that learn discrete-variable bayesian networks from data . our results apply whenever the learning algorithm uses a scoring criterion that favors the simplest model able to represent the generative distribution exactly . our results therefore hold whenever the learning algorithm uses a consistent scoring criterion and is applied to a sufficiently large dataset . we show that identifying high-scoring structures is hard , even when we are given an independence oracle , an inference oracle , and/or an information oracle . our negative results also apply to the learning of discrete-variable bayesian networks in which each node has at most k parents , for all k > 3 .", "topics": ["bayesian network", "eisenstein 's criterion"]}
{"title": "enhanced mixtures of part model for human pose estimation", "abstract": "mixture of parts model has been successfully applied to 2d human pose estimation problem either as explicitly trained body part model or as latent variables for the whole human body model . mixture of parts model usually utilize tree structure for representing relations between body parts . tree structures facilitate training and referencing of the model but could not deal with double counting problems , which hinder its applications in 3d pose estimation . while most of work targeted to solve these problems tend to modify the tree models or the optimization target . we incorporate other cues from input features . for example , in surveillance environments , human silhouettes can be extracted relative easily although not flawlessly . in this condition , we can combine extracted human blobs with histogram of gradient feature , which is commonly used in mixture of parts model for training body part templates . the method can be easily extend to other candidate features under our generalized framework . we show 2d body part detection results on a public available dataset : humaneva dataset . furthermore , a 2d to 3d pose estimator is trained with gaussian process regression model and 2d body part detections from the proposed method is fed to the estimator , thus 3d poses are predictable given new 2d body part detections . we also show results of 3d pose estimation on humaneva dataset .", "topics": ["gradient", "sensor"]}
{"title": "improving zero-shot learning by mitigating the hubness problem", "abstract": "the zero-shot paradigm exploits vector-based word representations extracted from text corpora with unsupervised methods to learn general mapping functions from other feature spaces onto word space , where the words associated to the nearest neighbours of the mapped vectors are used as their linguistic labels . we show that the neighbourhoods of the mapped elements are strongly polluted by hubs , vectors that tend to be near a high proportion of items , pushing their correct labels down the neighbour list . after illustrating the problem empirically , we propose a simple method to correct it by taking the proximity distribution of potential neighbours across many mapped vectors into account . we show that this correction leads to consistent improvements in realistic zero-shot experiments in the cross-lingual , image labeling and image retrieval domains .", "topics": ["text corpus"]}
{"title": "characterizing implicit bias in terms of optimization geometry", "abstract": "we study the bias of generic optimization methods , including mirror descent , natural gradient descent and steepest descent with respect to different potentials and norms , when optimizing underdetermined linear regression or separable linear classification problems . we ask the question of whether the global minimum ( among the many possible global minima ) reached by optimization algorithms can be characterized in terms of the potential or norm , and independently of hyperparameter choices such as step size and momentum .", "topics": ["mathematical optimization", "gradient descent"]}
{"title": "beating the minimax rate of active learning with prior knowledge", "abstract": "active learning refers to the learning protocol where the learner is allowed to choose a subset of instances for labeling . previous studies have shown that , compared with passive learning , active learning is able to reduce the label complexity exponentially if the data are linearly separable or satisfy the tsybakov noise condition with parameter $ \\kappa=1 $ . in this paper , we propose a novel active learning algorithm using a convex surrogate loss , with the goal to broaden the cases for which active learning achieves an exponential improvement . we make use of a convex loss not only because it reduces the computational cost , but more importantly because it leads to a tight bound for the empirical process ( i.e . , the difference between the empirical estimation and the expectation ) when the current solution is close to the optimal one . under the assumption that the norm of the optimal classifier that minimizes the convex risk is available , our analysis shows that the introduction of the convex surrogate loss yields an exponential reduction in the label complexity even when the parameter $ \\kappa $ of the tsybakov noise is larger than $ 1 $ . to the best of our knowledge , this is the first work that improves the minimax rate of active learning by utilizing certain priori knowledge .", "topics": ["time complexity"]}
{"title": "linear , deterministic , and order-invariant initialization methods for the k-means clustering algorithm", "abstract": "over the past five decades , k-means has become the clustering algorithm of choice in many application domains primarily due to its simplicity , time/space efficiency , and invariance to the ordering of the data points . unfortunately , the algorithm 's sensitivity to the initial selection of the cluster centers remains to be its most serious drawback . numerous initialization methods have been proposed to address this drawback . many of these methods , however , have time complexity superlinear in the number of data points , which makes them impractical for large data sets . on the other hand , linear methods are often random and/or sensitive to the ordering of the data points . these methods are generally unreliable in that the quality of their results is unpredictable . therefore , it is common practice to perform multiple runs of such methods and take the output of the run that produces the best results . such a practice , however , greatly increases the computational requirements of the otherwise highly efficient k-means algorithm . in this chapter , we investigate the empirical performance of six linear , deterministic ( non-random ) , and order-invariant k-means initialization methods on a large and diverse collection of data sets from the uci machine learning repository . the results demonstrate that two relatively unknown hierarchical initialization methods due to su and dy outperform the remaining four methods with respect to two objective effectiveness criteria . in addition , a recent method due to erisoglu et al . performs surprisingly poorly .", "topics": ["cluster analysis", "time complexity"]}
{"title": "cause identification from aviation safety incident reports via weakly supervised semantic lexicon construction", "abstract": "the aviation safety reporting system collects voluntarily submitted reports on aviation safety incidents to facilitate research work aiming to reduce such incidents . to effectively reduce these incidents , it is vital to accurately identify why these incidents occurred . more precisely , given a set of possible causes , or shaping factors , this task of cause identification involves identifying all and only those shaping factors that are responsible for the incidents described in a report . we investigate two approaches to cause identification . both approaches exploit information provided by a semantic lexicon , which is automatically constructed via thelen and riloffs basilisk framework augmented with our linguistic and algorithmic modifications . the first approach labels a report using a simple heuristic , which looks for the words and phrases acquired during the semantic lexicon learning process in the report . the second approach recasts cause identification as a text classification problem , employing supervised and transductive text classification algorithms to learn models from incident reports labeled with shaping factors and using the models to label unseen reports . our experiments show that both the heuristic-based approach and the learning-based approach ( when given sufficient training data ) outperform the baseline system significantly .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "discriminative models for robust image classification", "abstract": "a variety of real-world tasks involve the classification of images into pre-determined categories . designing image classification algorithms that exhibit robustness to acquisition noise and image distortions , particularly when the available training data are insufficient to learn accurate models , is a significant challenge . this dissertation explores the development of discriminative models for robust image classification that exploit underlying signal structure , via probabilistic graphical models and sparse signal representations . probabilistic graphical models are widely used in many applications to approximate high-dimensional data in a reduced complexity set-up . learning graphical structures to approximate probability distributions is an area of active research . recent work has focused on learning graphs in a discriminative manner with the goal of minimizing classification error . in the first part of the dissertation , we develop a discriminative learning framework that exploits the complementary yet correlated information offered by multiple representations ( or projections ) of a given signal/image . specifically , we propose a discriminative tree-based scheme for feature fusion by explicitly learning the conditional correlations among such multiple projections in an iterative manner . experiments reveal the robustness of the resulting graphical model classifier to training insufficiency .", "topics": ["graphical model", "test set"]}
{"title": "deep q-networks for accelerating the training of deep neural networks", "abstract": "in this paper , we propose a principled deep reinforcement learning ( rl ) approach that is able to accelerate the convergence rate of general deep neural networks ( dnns ) . with our approach , a deep rl agent ( synonym for optimizer in this work ) is used to automatically learn policies about how to schedule learning rates during the optimization of a dnn . the state features of the agent are learned from the weight statistics of the optimizee during training . the reward function of this agent is designed to learn policies that minimize the optimizee 's training time given a certain performance goal . the actions of the agent correspond to changing the learning rate for the optimizee during training . as far as we know , this is the first attempt to use deep rl to learn how to optimize a large-sized dnn . we perform extensive experiments on a standard benchmark dataset and demonstrate the effectiveness of the policies learned by our approach .", "topics": ["reinforcement learning", "neural networks"]}
{"title": "online sequence training of recurrent neural networks with connectionist temporal classification", "abstract": "connectionist temporal classification ( ctc ) based supervised sequence training of recurrent neural networks ( rnns ) has shown great success in many machine learning areas including end-to-end speech and handwritten character recognition . for the ctc training , however , it is required to unroll ( or unfold ) the rnn by the length of an input sequence . this unrolling requires a lot of memory and hinders a small footprint implementation of online learning or adaptation . furthermore , the length of training sequences is usually not uniform , which makes parallel training with multiple sequences inefficient on shared memory models such as graphics processing units ( gpus ) . in this work , we introduce an expectation-maximization ( em ) based online ctc algorithm that enables unidirectional rnns to learn sequences that are longer than the amount of unrolling . the rnns can also be trained to process an infinitely long input sequence without pre-segmentation or external reset . moreover , the proposed approach allows efficient parallel training on gpus . for evaluation , phoneme recognition and end-to-end speech recognition examples are presented on the timit and wall street journal ( wsj ) corpora , respectively . our online model achieves 20.7 % phoneme error rate ( per ) on the very long input sequence that is generated by concatenating all 192 utterances in the timit core test set . on wsj , a network can be trained with only 64 times of unrolling while sacrificing 4.5 % relative word error rate ( wer ) .", "topics": ["test set", "recurrent neural network"]}
{"title": "provable tensor methods for learning mixtures of generalized linear models", "abstract": "we consider the problem of learning mixtures of generalized linear models ( glm ) which arise in classification and regression problems . typical learning approaches such as expectation maximization ( em ) or variational bayes can get stuck in spurious local optima . in contrast , we present a tensor decomposition method which is guaranteed to correctly recover the parameters . the key insight is to employ certain feature transformations of the input , which depend on the input generative model . specifically , we employ score function tensors of the input and compute their cross-correlation with the response variable . we establish that the decomposition of this tensor consistently recovers the parameters , under mild non-degeneracy conditions . we demonstrate that the computational and sample complexity of our method is a low order polynomial of the input and the latent dimensions .", "topics": ["artificial intelligence"]}
{"title": "on geometric algebra representation of binary spatter codes", "abstract": "kanerva 's binary spatter codes are reformulated in terms of geometric algebra . the key ingredient of the construction is the representation of xor binding in terms of geometric product .", "topics": ["computation", "computer vision"]}
{"title": "recurrent additive networks", "abstract": "we introduce recurrent additive networks ( rans ) , a new gated rnn which is distinguished by the use of purely additive latent state updates . at every time step , the new state is computed as a gated component-wise sum of the input and the previous state , without any of the non-linearities commonly used in rnn transition dynamics . we formally show that ran states are weighted sums of the input vectors , and that the gates only contribute to computing the weights of these sums . despite this relatively simple functional form , experiments demonstrate that rans perform on par with lstms on benchmark language modeling problems . this result shows that many of the non-linear computations in lstms and related networks are not essential , at least for the problems we consider , and suggests that the gates are doing more of the computational work than previously understood .", "topics": ["nonlinear system", "computation"]}
{"title": "finite first hitting time versus stochastic convergence in particle swarm optimisation", "abstract": "we reconsider stochastic convergence analyses of particle swarm optimisation , and point out that previously obtained parameter conditions are not always sufficient to guarantee mean square convergence to a local optimum . we show that stagnation can in fact occur for non-trivial configurations in non-optimal parts of the search space , even for simple functions like sphere . the convergence properties of the basic pso may in these situations be detrimental to the goal of optimisation , to discover a sufficiently good solution within reasonable time . to characterise optimisation ability of algorithms , we suggest the expected first hitting time ( fht ) , i.e . , the time until a search point in the vicinity of the optimum is visited . it is shown that a basic pso may have infinite expected fht , while an algorithm introduced here , the noisy pso , has finite expected fht on some functions .", "topics": ["mathematical optimization"]}
{"title": "silnet : single- and multi-view reconstruction by learning from silhouettes", "abstract": "the objective of this paper is 3d shape understanding from single and multiple images . to this end , we introduce a new deep-learning architecture and loss function , silnet , that can handle multiple views in an order-agnostic manner . the architecture is fully convolutional , and for training we use a proxy task of silhouette prediction , rather than directly learning a mapping from 2d images to 3d shape as has been the target in most recent work . we demonstrate that with the silnet architecture there is generalisation over the number of views -- for example , silnet trained on 2 views can be used with 3 or 4 views at test-time ; and performance improves with more views . we introduce two new synthetics datasets : a blobby object dataset useful for pre-training , and a challenging and realistic sculpture dataset ; and demonstrate on these datasets that silnet has indeed learnt 3d shape . finally , we show that silnet exceeds the state of the art on the shapenet benchmark dataset , and use silnet to generate novel views of the sculpture dataset .", "topics": ["loss function"]}
{"title": "pac-bayesian policy evaluation for reinforcement learning", "abstract": "bayesian priors offer a compact yet general means of incorporating domain knowledge into many learning tasks . the correctness of the bayesian analysis and inference , however , largely depends on accuracy and correctness of these priors . pac-bayesian methods overcome this problem by providing bounds that hold regardless of the correctness of the prior distribution . this paper introduces the first pac-bayesian bound for the batch reinforcement learning problem with function approximation . we show how this bound can be used to perform model-selection in a transfer learning scenario . our empirical results confirm that pac-bayesian policy evaluation is able to leverage prior distributions when they are informative and , unlike standard bayesian rl approaches , ignore them when they are misleading .", "topics": ["reinforcement learning", "bayesian network"]}
{"title": "steering social activity : a stochastic optimal control point of view", "abstract": "user engagement in online social networking depends critically on the level of social activity in the corresponding platform -- the number of online actions , such as posts , shares or replies , taken by their users . can we design data-driven algorithms to increase social activity ? at a user level , such algorithms may increase activity by helping users decide when to take an action to be more likely to be noticed by their peers . at a network level , they may increase activity by incentivizing a few influential users to take more actions , which in turn will trigger additional actions by other users . in this paper , we model social activity using the framework of marked temporal point processes , derive an alternate representation of these processes using stochastic differential equations ( sdes ) with jumps and , exploiting this alternate representation , develop two efficient online algorithms with provable guarantees to steer social activity both at a user and at a network level . in doing so , we establish a previously unexplored connection between optimal control of jump sdes and doubly stochastic marked temporal point processes , which is of independent interest . finally , we experiment both with synthetic and real data gathered from twitter and show that our algorithms consistently steer social activity more effectively than the state of the art .", "topics": ["synthetic data"]}
{"title": "self-transfer learning for fully weakly supervised object localization", "abstract": "recent advances of deep learning have achieved remarkable performances in various challenging computer vision tasks . especially in object localization , deep convolutional neural networks outperform traditional approaches based on extraction of data/task-driven features instead of hand-crafted features . although location information of region-of-interests ( rois ) gives good prior for object localization , it requires heavy annotation efforts from human resources . thus a weakly supervised framework for object localization is introduced . the term `` weakly '' means that this framework only uses image-level labeled datasets to train a network . with the help of transfer learning which adopts weight parameters of a pre-trained network , the weakly supervised learning framework for object localization performs well because the pre-trained network already has well-trained class-specific features . however , those approaches can not be used for some applications which do not have pre-trained networks or well-localized large scale images . medical image analysis is a representative among those applications because it is impossible to obtain such pre-trained networks . in this work , we present a `` fully '' weakly supervised framework for object localization ( `` semi '' -weakly is the counterpart which uses pre-trained filters for weakly supervised localization ) named as self-transfer learning ( stl ) . it jointly optimizes both classification and localization networks simultaneously . by controlling a supervision level of the localization network , stl helps the localization network focus on correct rois without any types of priors . we evaluate the proposed stl framework using two medical image datasets , chest x-rays and mammograms , and achieve signiticantly better localization performance compared to previous weakly supervised approaches .", "topics": ["supervised learning", "statistical classification"]}
{"title": "policy gradients for cvar-constrained mdps", "abstract": "we study a risk-constrained version of the stochastic shortest path ( ssp ) problem , where the risk measure considered is conditional value-at-risk ( cvar ) . we propose two algorithms that obtain a locally risk-optimal policy by employing four tools : stochastic approximation , mini batches , policy gradients and importance sampling . both the algorithms incorporate a cvar estimation procedure , along the lines of bardou et al . [ 2009 ] , which in turn is based on rockafellar-uryasev 's representation for cvar and utilize the likelihood ratio principle for estimating the gradient of the sum of one cost function ( objective of the ssp ) and the gradient of the cvar of the sum of another cost function ( in the constraint of ssp ) . the algorithms differ in the manner in which they approximate the cvar estimates/necessary gradients - the first algorithm uses stochastic approximation , while the second employ mini-batches in the spirit of monte carlo methods . we establish asymptotic convergence of both the algorithms . further , since estimating cvar is related to rare-event simulation , we incorporate an importance sampling based variance reduction scheme into our proposed algorithms .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "collaborative deep reinforcement learning", "abstract": "besides independent learning , human learning process is highly improved by summarizing what has been learned , communicating it with peers , and subsequently fusing knowledge from different sources to assist the current learning goal . this collaborative learning procedure ensures that the knowledge is shared , continuously refined , and concluded from different perspectives to construct a more profound understanding . the idea of knowledge transfer has led to many advances in machine learning and data mining , but significant challenges remain , especially when it comes to reinforcement learning , heterogeneous model structures , and different learning tasks . motivated by human collaborative learning , in this paper we propose a collaborative deep reinforcement learning ( cdrl ) framework that performs adaptive knowledge transfer among heterogeneous learning agents . specifically , the proposed cdrl conducts a novel deep knowledge distillation method to address the heterogeneity among different learning tasks with a deep alignment network . furthermore , we present an efficient collaborative asynchronous advantage actor-critic ( ca3c ) algorithm to incorporate deep knowledge distillation into the online training of agents , and demonstrate the effectiveness of the cdrl framework using extensive empirical evaluation on openai gym .", "topics": ["data mining", "reinforcement learning"]}
{"title": "latent self-exciting point process model for spatial-temporal networks", "abstract": "we propose a latent self-exciting point process model that describes geographically distributed interactions between pairs of entities . in contrast to most existing approaches that assume fully observable interactions , here we consider a scenario where certain interaction events lack information about participants . instead , this information needs to be inferred from the available observations . we develop an efficient approximate algorithm based on variational expectation-maximization to infer unknown participants in an event given the location and the time of the event . we validate the model on synthetic as well as real-world data , and obtain very promising results on the identity-inference task . we also use our model to predict the timing and participants of future events , and demonstrate that it compares favorably with baseline approaches .", "topics": ["baseline ( configuration management )", "calculus of variations"]}
{"title": "how will the internet of things enable augmented personalized health ?", "abstract": "internet-of-things ( iot ) is profoundly redefining the way we create , consume , and share information . health aficionados and citizens are increasingly using iot technologies to track their sleep , food intake , activity , vital body signals , and other physiological observations . this is complemented by iot systems that continuously collect health-related data from the environment and inside the living quarters . together , these have created an opportunity for a new generation of healthcare solutions . however , interpreting data to understand an individual 's health is challenging . it is usually necessary to look at that individual 's clinical record and behavioral information , as well as social and environmental information affecting that individual . interpreting how well a patient is doing also requires looking at his adherence to respective health objectives , application of relevant clinical knowledge and the desired outcomes . we resort to the vision of augmented personalized healthcare ( aph ) to exploit the extensive variety of relevant data and medical knowledge using artificial intelligence ( ai ) techniques to extend and enhance human health to presents various stages of augmented health management strategies : self-monitoring , self-appraisal , self-management , intervention , and disease progress tracking and prediction . khealth technology , a specific incarnation of aph , and its application to asthma and other diseases are used to provide illustrations and discuss alternatives for technology-assisted health management . several prominent efforts involving iot and patient-generated health data ( pghd ) with respect converting multimodal data into actionable information ( big data to smart data ) are also identified . roles of three components in an evidence-based semantic perception approach- contextualization , abstraction , and personalization are discussed .", "topics": ["artificial intelligence"]}
{"title": "tasnet : time-domain audio separation network for real-time , single-channel speech separation", "abstract": "robust speech processing in multi-talker environments requires effective speech separation . recent deep learning systems have made significant progress toward solving this problem , yet it remains challenging particularly in real-time , short latency applications . most methods attempt to construct a mask for each source in time-frequency representation of the mixture signal which is not necessarily an optimal representation for speech separation . in addition , time-frequency decomposition results in inherent problems such as phase/magnitude decoupling and long time window which is required to achieve sufficient frequency resolution . we propose time-domain audio separation network ( tasnet ) to overcome these limitations . we directly model the signal in the time-domain using encoder-decoder framework and perform the source separation on nonnegative encoder outputs . this method removes the frequency decomposition step and reduces the separation problem to estimation of source masks on encoder outputs which is then synthesized by the decoder . our system outperforms the current state-of-the-art causal speech separation algorithms , reduces the computational cost of speech separation , and significantly reduces the minimum required latency of the output . this makes tasnet suitable for applications where low-power , real-time implementation is desirable such as in hearable and telecommunication devices .", "topics": ["encoder", "causality"]}
{"title": "empirical methods for compound splitting", "abstract": "compounded words are a challenge for nlp applications such as machine translation ( mt ) . we introduce methods to learn splitting rules from monolingual and parallel corpora . we evaluate them against a gold standard and measure their impact on performance of statistical mt systems . results show accuracy of 99.1 % and performance gains for mt of 0.039 bleu on a german-english noun phrase translation task .", "topics": ["natural language processing", "machine translation"]}
{"title": "towards automatic learning of heuristics for mechanical transformations of procedural code", "abstract": "the current trends in next-generation exascale systems go towards integrating a wide range of specialized ( co- ) processors into traditional supercomputers . due to the efficiency of heterogeneous systems in terms of watts and flops per surface unit , opening the access of heterogeneous platforms to a wider range of users is an important problem to be tackled . however , heterogeneous platforms limit the portability of the applications and increase development complexity due to the programming skills required . program transformation can help make programming heterogeneous systems easier by defining a step-wise transformation process that translates a given initial code into a semantically equivalent final code , but adapted to a specific platform . program transformation systems require the definition of efficient transformation strategies to tackle the combinatorial problem that emerges due to the large set of transformations applicable at each step of the process . in this paper we propose a machine learning-based approach to learn heuristics to define program transformation strategies . our approach proposes a novel combination of reinforcement learning and classification methods to efficiently tackle the problems inherent to this type of systems . preliminary results demonstrate the suitability of this approach .", "topics": ["reinforcement learning", "heuristic"]}
{"title": "honeyfaces : increasing the security and privacy of authentication using synthetic facial images", "abstract": "one of the main challenges faced by biometric-based authentication systems is the need to offer secure authentication while maintaining the privacy of the biometric data . previous solutions , such as secure sketch and fuzzy extractors , rely on assumptions that can not be guaranteed in practice , and often affect the authentication accuracy . in this paper , we introduce honeyfaces : the concept of adding a large set of synthetic faces ( indistinguishable from real ) into the biometric `` password file '' . this password inflation protects the privacy of users and increases the security of the system without affecting the accuracy of the authentication . in particular , privacy for the real users is provided by `` hiding '' them among a large number of fake users ( as the distributions of synthetic and real faces are equal ) . in addition to maintaining the authentication accuracy , and thus not affecting the security of the authentication process , honeyfaces offer several security improvements : increased exfiltration hardness , improved leakage detection , and the ability to use a two-server setting like in honeywords . finally , honeyfaces can be combined with other security and privacy mechanisms for biometric data . we implemented the honeyfaces system and tested it with a password file composed of 270 real users . the `` password file '' was then inflated to accommodate up to $ 2^ { 36.5 } $ users ( resulting in a 56.6 tb `` password file '' ) . at the same time , the inclusion of additional faces does not affect the true acceptance rate or false acceptance rate which were 93.33\\ % and 0.01\\ % , respectively .", "topics": ["synthetic data"]}
{"title": "robust learning bayesian networks for prior belief", "abstract": "recent reports have described that learning bayesian networks are highly sensitive to the chosen equivalent sample size ( ess ) in the bayesian dirichlet equivalence uniform ( bdeu ) . this sensitivity often engenders some unstable or undesirable results . this paper describes some asymptotic analyses of bdeu to explain the reasons for the sensitivity and its effects . furthermore , this paper presents a proposal for a robust learning score for ess by eliminating the sensitive factors from the approximation of log-bdeu .", "topics": ["bayesian network"]}
{"title": "second-order kernel online convex optimization with adaptive sketching", "abstract": "kernel online convex optimization ( koco ) is a framework combining the expressiveness of non-parametric kernel models with the regret guarantees of online learning . first-order koco methods such as functional gradient descent require only $ \\mathcal { o } ( t ) $ time and space per iteration , and , when the only information on the losses is their convexity , achieve a minimax optimal $ \\mathcal { o } ( \\sqrt { t } ) $ regret . nonetheless , many common losses in kernel problems , such as squared loss , logistic loss , and squared hinge loss posses stronger curvature that can be exploited . in this case , second-order koco methods achieve $ \\mathcal { o } ( \\log ( \\text { det } ( \\boldsymbol { k } ) ) ) $ regret , which we show scales as $ \\mathcal { o } ( d_ { \\text { eff } } \\log t ) $ , where $ d_ { \\text { eff } } $ is the effective dimension of the problem and is usually much smaller than $ \\mathcal { o } ( \\sqrt { t } ) $ . the main drawback of second-order methods is their much higher $ \\mathcal { o } ( t^2 ) $ space and time complexity . in this paper , we introduce kernel online newton step ( kons ) , a new second-order koco method that also achieves $ \\mathcal { o } ( d_ { \\text { eff } } \\log t ) $ regret . to address the computational complexity of second-order methods , we introduce a new matrix sketching algorithm for the kernel matrix $ \\boldsymbol { k } _t $ , and show that for a chosen parameter $ \\gamma \\leq 1 $ our sketched-kons reduces the space and time complexity by a factor of $ \\gamma^2 $ to $ \\mathcal { o } ( t^2\\gamma^2 ) $ space and time per iteration , while incurring only $ 1/\\gamma $ times more regret .", "topics": ["kernel ( operating system )", "regret ( decision theory )"]}
{"title": "learning item trees for probabilistic modelling of implicit feedback", "abstract": "user preferences for items can be inferred from either explicit feedback , such as item ratings , or implicit feedback , such as rental histories . research in collaborative filtering has concentrated on explicit feedback , resulting in the development of accurate and scalable models . however , since explicit feedback is often difficult to collect it is important to develop effective models that take advantage of the more widely available implicit feedback . we introduce a probabilistic approach to collaborative filtering with implicit feedback based on modelling the user 's item selection process . in the interests of scalability , we restrict our attention to tree-structured distributions over items and develop a principled and efficient algorithm for learning item trees from data . we also identify a problem with a widely used protocol for evaluating implicit feedback models and propose a way of addressing it using a small quantity of explicit feedback data .", "topics": ["scalability"]}
{"title": "restricted parallelism in object-oriented lexical parsing", "abstract": "we present an approach to parallel natural language parsing which is based on a concurrent , object-oriented model of computation . a depth-first , yet incomplete parsing algorithm for a dependency grammar is specified and several restrictions on the degree of its parallelization are discussed .", "topics": ["parsing", "natural language"]}
{"title": "1-hkust : object detection in ilsvrc 2014", "abstract": "the imagenet large scale visual recognition challenge ( ilsvrc ) is the one of the most important big data challenges to date . we participated in the object detection track of ilsvrc 2014 and received the fourth place among the 38 teams . we introduce in our object detection system a number of novel techniques in localization and recognition . for localization , initial candidate proposals are generated using selective search , and a novel bounding boxes regression method is used for better object localization . for recognition , to represent a candidate proposal , we adopt three features , namely , rcnn feature , ifv feature , and dpm feature . given these features , category-specific combination functions are learned to improve the object recognition rate . in addition , object context in the form of background priors and object interaction priors are learned and applied in our system . our ilsvrc 2014 results are reported alongside with the results of other participating teams .", "topics": ["object detection"]}
{"title": "convolutional sparse coding with overlapping group norms", "abstract": "the most widely used form of convolutional sparse coding uses an $ \\ell_1 $ regularization term . while this approach has been successful in a variety of applications , a limitation of the $ \\ell_1 $ penalty is that it is homogeneous across the spatial and filter index dimensions of the sparse representation array , so that sparsity can not be separately controlled across these dimensions . the present paper considers the consequences of replacing the $ \\ell_1 $ penalty with a mixed group norm , motivated by recent theoretical results for convolutional sparse representations . algorithms are developed for solving the resulting problems , which are quite challenging , and the impact on the performance of the denoising problem is evaluated . the mixed group norms are found to perform very poorly in this application . while their performance is greatly improved by introducing a weighting strategy , such a strategy also improves the performance obtained from the much simpler and computationally cheaper $ \\ell_1 $ norm .", "topics": ["noise reduction", "matrix regularization"]}
{"title": "roml : a robust feature correspondence approach for matching objects in a set of images", "abstract": "feature-based object matching is a fundamental problem for many applications in computer vision , such as object recognition , 3d reconstruction , tracking , and motion segmentation . in this work , we consider simultaneously matching object instances in a set of images , where both inlier and outlier features are extracted . the task is to identify the inlier features and establish their consistent correspondences across the image set . this is a challenging combinatorial problem , and the problem complexity grows exponentially with the image number . to this end , we propose a novel framework , termed roml , to address this problem . roml optimizes simultaneously a partial permutation matrix ( ppm ) for each image , and feature correspondences are established by the obtained ppms . two of our key contributions are summarized as follows . ( 1 ) we formulate the problem as rank and sparsity minimization for ppm optimization , and treat simultaneous optimization of multiple ppms as a regularized consensus problem in the context of distributed optimization . ( 2 ) we use the admm method to solve the thus formulated roml problem , in which a subproblem associated with a single ppm optimization appears to be a difficult integer quadratic program ( iqp ) . we prove that under wildly applicable conditions , this iqp is equivalent to a linear sum assignment problem ( lsap ) , which can be efficiently solved to an exact solution . extensive experiments on rigid/non-rigid object matching , matching instances of a common object category , and common object localization show the efficacy of our proposed method .", "topics": ["computer vision", "sparse matrix"]}
{"title": "on abruptly-changing and slowly-varying multiarmed bandit problems", "abstract": "we study the non-stationary stochastic multiarmed bandit ( mab ) problem and propose two generic algorithms , namely , the limited memory deterministic sequencing of exploration and exploitation ( lm-dsee ) and the sliding-window upper confidence bound # ( sw-ucb # ) . we rigorously analyze these algorithms in abruptly-changing and slowly-varying environments and characterize their performance . we show that the expected cumulative regret for these algorithms under either of the environments is upper bounded by sublinear functions of time , i.e . , the time average of the regret asymptotically converges to zero . we complement our analytic results with numerical illustrations .", "topics": ["regret ( decision theory )", "numerical analysis"]}
{"title": "bag of tricks for efficient text classification", "abstract": "this paper explores a simple and efficient baseline for text classification . our experiments show that our fast text classifier fasttext is often on par with deep learning classifiers in terms of accuracy , and many orders of magnitude faster for training and evaluation . we can train fasttext on more than one billion words in less than ten minutes using a standard multicore~cpu , and classify half a million sentences among~312k classes in less than a minute .", "topics": ["baseline ( configuration management )"]}
{"title": "learning video object segmentation with visual memory", "abstract": "this paper addresses the task of segmenting moving objects in unconstrained videos . we introduce a novel two-stream neural network with an explicit memory module to achieve this . the two streams of the network encode spatial and temporal features in a video sequence respectively , while the memory module captures the evolution of objects over time . the module to build a `` visual memory '' in video , i.e . , a joint representation of all the video frames , is realized with a convolutional recurrent unit learned from a small number of training video sequences . given a video frame as input , our approach assigns each pixel an object or background label based on the learned spatio-temporal features as well as the `` visual memory '' specific to the video , acquired automatically without any manually-annotated frames . the visual memory is implemented with convolutional gated recurrent units , which allows to propagate spatial information over time . we evaluate our method extensively on two benchmarks , davis and freiburg-berkeley motion segmentation datasets , and show state-of-the-art results . for example , our approach outperforms the top method on the davis dataset by nearly 6 % . we also provide an extensive ablative analysis to investigate the influence of each component in the proposed framework .", "topics": ["pixel"]}
{"title": "anytime planning for decentralized pomdps using expectation maximization", "abstract": "decentralized pomdps provide an expressive framework for multi-agent sequential decision making . while fnite-horizon decpomdps have enjoyed signifcant success , progress remains slow for the infnite-horizon case mainly due to the inherent complexity of optimizing stochastic controllers representing agent policies . we present a promising new class of algorithms for the infnite-horizon case , which recasts the optimization problem as inference in a mixture of dbns . an attractive feature of this approach is the straightforward adoption of existing inference techniques in dbns for solving dec-pomdps and supporting richer representations such as factored or continuous states and actions . we also derive the expectation maximization ( em ) algorithm to optimize the joint policy represented as dbns . experiments on benchmark domains show that em compares favorably against the state-of-the-art solvers .", "topics": ["optimization problem"]}
{"title": "a bayesian multiresolution independence test for continuous variables", "abstract": "in this paper we present a method ofcomputing the posterior probability ofconditional independence of two or morecontinuous variables from data , examined at several resolutions . ourapproach is motivated by theobservation that the appearance ofcontinuous data varies widely atvarious resolutions , producing verydifferent independence estimatesbetween the variablesinvolved . therefore , it is difficultto ascertain independence withoutexamining data at several carefullyselected resolutions . in our paper , weaccomplish this using the exactcomputation of the posteriorprobability of independence , calculatedanalytically given a resolution . ateach examined resolution , we assume amultinomial distribution with dirichletpriors for the discretized tableparameters , and compute the posteriorusing bayesian integration . acrossresolutions , we use a search procedureto approximate the bayesian integral ofprobability over an exponential numberof possible histograms . our methodgeneralizes to an arbitrary numbervariables in a straightforward manner.the test is suitable for bayesiannetwork learning algorithms that useindependence tests to infer the networkstructure , in domains that contain anymix of continuous , ordinal andcategorical variables .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "data structuring for the ontological modelling of wind energy systems", "abstract": "small wind projects encounter difficulties to be efficiently deployed , partly because wrong way data and information are managed . ontologies can overcome the drawbacks of partially available , noisy , inconsistent , and heterogeneous data sources , by providing a semantic middleware between low level data and more general knowledge . in this paper , we engineer an ontology for the wind energy domain using description logic as technical instrumentation . we aim to integrate corpus of heterogeneous knowledge , both digital and human , in order to help the interested user to speed-up the initialization of a small-scale wind project . we exemplify one use case scenario of our ontology , that consists of automatically checking whether a planned wind project is compliant or not with the active regulations .", "topics": ["high- and low-level", "text corpus"]}
{"title": "efficient diverse ensemble for discriminative co-tracking", "abstract": "ensemble discriminative tracking utilizes a committee of classifiers , to label data samples , which are in turn , used for retraining the tracker to localize the target using the collective knowledge of the committee . committee members could vary in their features , memory update schemes , or training data , however , it is inevitable to have committee members that excessively agree because of large overlaps in their version space . to remove this redundancy and have an effective ensemble learning , it is critical for the committee to include consistent hypotheses that differ from one-another , covering the version space with minimum overlaps . in this study , we propose an online ensemble tracker that directly generates a diverse committee by generating an efficient set of artificial training . the artificial data is sampled from the empirical distribution of the samples taken from both target and background , whereas the process is governed by query-by-committee to shrink the overlap between classifiers . the experimental results demonstrate that the proposed scheme outperforms conventional ensemble trackers on public benchmarks .", "topics": ["test set"]}
{"title": "space and camera path reconstruction for omni-directional vision", "abstract": "in this paper , we address the inverse problem of reconstructing a scene as well as the camera motion from the image sequence taken by an omni-directional camera . our structure from motion results give sharp conditions under which the reconstruction is unique . for example , if there are three points in general position and three omni-directional cameras in general position , a unique reconstruction is possible up to a similarity . we then look at the reconstruction problem with m cameras and n points , where n and m can be large and the over-determined system is solved by least square methods . the reconstruction is robust and generalizes to the case of a dynamic environment where landmarks can move during the movie capture . possible applications of the result are computer assisted scene reconstruction , 3d scanning , autonomous robot navigation , medical tomography and city reconstructions .", "topics": ["autonomous car"]}
{"title": "adaptive learning rate via covariance matrix based preconditioning for deep neural networks", "abstract": "adaptive learning rate algorithms such as rmsprop are widely used for training deep neural networks . rmsprop offers efficient training since it uses first order gradients to approximate hessian-based preconditioning . however , since the first order gradients include noise caused by stochastic optimization , the approximation may be inaccurate . in this paper , we propose a novel adaptive learning rate algorithm called sdprop . its key idea is effective handling of the noise by preconditioning based on covariance matrix . for various neural networks , our approach is more efficient and effective than rmsprop and its variant .", "topics": ["approximation algorithm", "gradient descent"]}
{"title": "a discrete state transition algorithm for generalized traveling salesman problem", "abstract": "generalized traveling salesman problem ( gtsp ) is an extension of classical traveling salesman problem ( tsp ) , which is a combinatorial optimization problem and an np-hard problem . in this paper , an efficient discrete state transition algorithm ( dsta ) for gtsp is proposed , where a new local search operator named \\textit { k-circle } , directed by neighborhood information in space , has been introduced to dsta to shrink search space and strengthen search ability . a novel robust update mechanism , restore in probability and risk in probability ( double r-probability ) , is used in our work to escape from local minima . the proposed algorithm is tested on a set of gtsp instances . compared with other heuristics , experimental results have demonstrated the effectiveness and strong adaptability of dsta and also show that dsta has better search ability than its competitors .", "topics": ["optimization problem", "heuristic"]}
{"title": "foundations of coupled nonlinear dimensionality reduction", "abstract": "in this paper we introduce and analyze the learning scenario of \\emph { coupled nonlinear dimensionality reduction } , which combines two major steps of machine learning pipeline : projection onto a manifold and subsequent supervised learning . first , we present new generalization bounds for this scenario and , second , we introduce an algorithm that follows from these bounds . the generalization error bound is based on a careful analysis of the empirical rademacher complexity of the relevant hypothesis set . in particular , we show an upper bound on the rademacher complexity that is in $ \\widetilde o ( \\sqrt { \\lambda_ { ( r ) } /m } ) $ , where $ m $ is the sample size and $ \\lambda_ { ( r ) } $ the upper bound on the ky-fan $ r $ -norm of the associated kernel matrix . we give both upper and lower bound guarantees in terms of that ky-fan $ r $ -norm , which strongly justifies the definition of our hypothesis set . to the best of our knowledge , these are the first learning guarantees for the problem of coupled dimensionality reduction . our analysis and learning guarantees further apply to several special cases , such as that of using a fixed kernel with supervised dimensionality reduction or that of unsupervised learning of a kernel for dimensionality reduction followed by a supervised learning algorithm . based on theoretical analysis , we suggest a structural risk minimization algorithm consisting of the coupled fitting of a low dimensional manifold and a separation function on that manifold .", "topics": ["kernel ( operating system )", "supervised learning"]}
{"title": "multiple instance curriculum learning for weakly supervised object detection", "abstract": "when supervising an object detector with weakly labeled data , most existing approaches are prone to trapping in the discriminative object parts , e.g . , finding the face of a cat instead of the full body , due to lacking the supervision on the extent of full objects . to address this challenge , we incorporate object segmentation into the detector training , which guides the model to correctly localize the full objects . we propose the multiple instance curriculum learning ( micl ) method , which injects curriculum learning ( cl ) into the multiple instance learning ( mil ) framework . the micl method starts by automatically picking the easy training examples , where the extent of the segmentation masks agree with detection bounding boxes . the training set is gradually expanded to include harder examples to train strong detectors that handle complex images . the proposed micl method with segmentation in the loop outperforms the state-of-the-art weakly supervised object detectors by a substantial margin on the pascal voc datasets .", "topics": ["object detection"]}
{"title": "r3mc : a riemannian three-factor algorithm for low-rank matrix completion", "abstract": "we exploit the versatile framework of riemannian optimization on quotient manifolds to develop r3mc , a nonlinear conjugate-gradient method for low-rank matrix completion . the underlying search space of fixed-rank matrices is endowed with a novel riemannian metric that is tailored to the least-squares cost . numerical comparisons suggest that r3mc robustly outperforms state-of-the-art algorithms across different problem instances , especially those that combine scarcely sampled and ill-conditioned data .", "topics": ["nonlinear system", "gradient"]}
{"title": "a supervised goal directed algorithm in economical choice behaviour : an actor-critic approach", "abstract": "this paper aims to find an algorithmic structure that affords to predict and explain economical choice behaviour particularly under uncertainty ( random policies ) by manipulating the prevalent actor-critic learning method to comply with the requirements we have been entrusted ever since the field of neuroeconomics dawned on us . whilst skimming some basics of neuroeconomics that seem relevant to our discussion , we will try to outline some of the important works which have so far been done to simulate choice making processes . concerning neurological findings that suggest the existence of two specific functions that are executed through basal ganglia all the way up to sub- cortical areas , namely 'rewards ' and 'beliefs ' , we will offer a modified version of actor/critic algorithm to shed a light on the relation between these functions and most importantly resolve what is referred to as a challenge for actor-critic algorithms , that is , the lack of inheritance or hierarchy which avoids the system being evolved in continuous time tasks whence the convergence might not be emerged .", "topics": ["reinforcement learning", "simulation"]}
{"title": "comparing k-nearest neighbors and potential energy method in classification problem . a case study using knn applet by e.m. mirkes and real life benchmark data sets", "abstract": "k-nearest neighbors ( knn ) method is used in many supervised learning classification problems . potential energy ( pe ) method is also developed for classification problems based on its physical metaphor . the energy potential used in the experiments are yukawa potential and gaussian potential . in this paper , i use both applet and matlab program with real life benchmark data to analyze the performances of knn and pe method in classification problems . the results show that in general , knn and pe methods have similar performance . in particular , pe with yukawa potential has worse performance than knn when the density of the data is higher in the distribution of the database . when the gaussian potential is applied , the results from pe and knn have similar behavior . the indicators used are correlation coefficients and information gain .", "topics": ["supervised learning"]}
{"title": "optimizing differentiable relaxations of coreference evaluation metrics", "abstract": "coreference evaluation metrics are hard to optimize directly as they are non-differentiable functions , not easily decomposable into elementary decisions . consequently , most approaches optimize objectives only indirectly related to the end goal , resulting in suboptimal performance . instead , we propose a differentiable relaxation that lends itself to gradient-based optimisation , thus bypassing the need for reinforcement learning or heuristic modification of cross-entropy . we show that by modifying the training objective of a competitive neural coreference system , we obtain a substantial gain in performance . this suggests that our approach can be regarded as a viable alternative to using reinforcement learning or more computationally expensive imitation learning .", "topics": ["mathematical optimization", "reinforcement learning"]}
{"title": "nlp and call : integration is working", "abstract": "in the first part of this article , we explore the background of computer-assisted learning from its beginnings in the early xixth century and the first teaching machines , founded on theories of learning , at the start of the xxth century . with the arrival of the computer , it became possible to offer language learners different types of language activities such as comprehension tasks , simulations , etc . however , these have limits that can not be overcome without some contribution from the field of natural language processing ( nlp ) . in what follows , we examine the challenges faced and the issues raised by integrating nlp into call . we hope to demonstrate that the key to success in integrating nlp into call is to be found in multidisciplinary work between computer experts , linguists , language teachers , didacticians and nlp specialists .", "topics": ["natural language processing", "natural language"]}
{"title": "deep dictionary learning : a parametric network approach", "abstract": "deep dictionary learning seeks multiple dictionaries at different image scales to capture complementary coherent characteristics . we propose a method for learning a hierarchy of synthesis dictionaries with an image classification goal . the dictionaries and classification parameters are trained by a classification objective , and the sparse features are extracted by reducing a reconstruction loss in each layer . the reconstruction objectives in some sense regularize the classification problem and inject source signal information in the extracted features . the performance of the proposed hierarchical method increases by adding more layers , which consequently makes this model easier to tune and adapt . the proposed algorithm furthermore , shows remarkably lower fooling rate in presence of adversarial perturbation . the validation of the proposed approach is based on its classification performance using four benchmark datasets and is compared to a cnn of similar size .", "topics": ["computer vision", "sparse matrix"]}
{"title": "expnet : landmark-free , deep , 3d facial expressions", "abstract": "we describe a deep learning based method for estimating 3d facial expression coefficients . unlike previous work , our process does not relay on facial landmark detection methods as a proxy step . recent methods have shown that a cnn can be trained to regress accurate and discriminative 3d morphable model ( 3dmm ) representations , directly from image intensities . by foregoing facial landmark detection , these methods were able to estimate shapes for occluded faces appearing in unprecedented in-the-wild viewing conditions . we build on those methods by showing that facial expressions can also be estimated by a robust , deep , landmark-free approach . our expnet cnn is applied directly to the intensities of a face image and regresses a 29d vector of 3d expression coefficients . we propose a unique method for collecting data to train this network , leveraging on the robustness of deep networks to training label noise . we further offer a novel means of evaluating the accuracy of estimated expression coefficients : by measuring how well they capture facial emotions on the ck+ and emotiw-17 emotion recognition benchmarks . we show that our expnet produces expression coefficients which better discriminate between facial emotions than those obtained using state of the art , facial landmark detection techniques . moreover , this advantage grows as image scales drop , demonstrating that our expnet is more robust to scale changes than landmark detection methods . finally , at the same level of accuracy , our expnet is orders of magnitude faster than its alternatives .", "topics": ["coefficient"]}
{"title": "dependency sensitive convolutional neural networks for modeling sentences and documents", "abstract": "the goal of sentence and document modeling is to accurately represent the meaning of sentences and documents for various natural language processing tasks . in this work , we present dependency sensitive convolutional neural networks ( dscnn ) as a general-purpose classification system for both sentences and documents . dscnn hierarchically builds textual representations by processing pretrained word embeddings via long short-term memory networks and subsequently extracting features with convolution operators . compared with existing recursive neural models with tree structures , dscnn does not rely on parsers and expensive phrase labeling , and thus is not restricted to sentence-level tasks . moreover , unlike other cnn-based models that analyze sentences locally by sliding windows , our system captures both the dependency information within each sentence and relationships across sentences in the same document . experiment results demonstrate that our approach is achieving state-of-the-art performance on several tasks , including sentiment analysis , question type classification , and subjectivity classification .", "topics": ["natural language processing", "natural language"]}
{"title": "an experimental comparison of numerical and qualitative probabilistic reasoning", "abstract": "qualitative and infinitesimal probability schemes are consistent with the axioms of probability theory , but avoid the need for precise numerical probabilities . using qualitative probabilities could substantially reduce the effort for knowledge engineering and improve the robustness of results . we examine experimentally how well infinitesimal probabilities ( the kappa-calculus of goldszmidt and pearl ) perform a diagnostic task - troubleshooting a car that will not start by comparison with a conventional numerical belief network . we found the infinitesimal scheme to be as good as the numerical scheme in identifying the true fault . the performance of the infinitesimal scheme worsens significantly for prior fault probabilities greater than 0.03 . these results suggest that infinitesimal probability methods may be of substantial practical value for machine diagnosis with small prior fault probabilities .", "topics": ["numerical analysis", "bayesian network"]}
{"title": "additive gaussian processes", "abstract": "we introduce a gaussian process model of functions which are additive . an additive function is one which decomposes into a sum of low-dimensional functions , each depending on only a subset of the input variables . additive gps generalize both generalized additive models , and the standard gp models which use squared-exponential kernels . hyperparameter learning in this model can be seen as bayesian hierarchical kernel learning ( hkl ) . we introduce an expressive but tractable parameterization of the kernel function , which allows efficient evaluation of all input interaction terms , whose number is exponential in the input dimension . the additional structure discoverable by this model results in increased interpretability , as well as state-of-the-art predictive power in regression tasks .", "topics": ["time complexity"]}
{"title": "computational complexity results for genetic programming and the sorting problem", "abstract": "genetic programming ( gp ) has found various applications . understanding this type of algorithm from a theoretical point of view is a challenging task . the first results on the computational complexity of gp have been obtained for problems with isolated program semantics . with this paper , we push forward the computational complexity analysis of gp on a problem with dependent program semantics . we study the well-known sorting problem in this context and analyze rigorously how gp can deal with different measures of sortedness .", "topics": ["computational complexity theory", "computation"]}
{"title": "doppler-radar based hand gesture recognition system using convolutional neural networks", "abstract": "hand gesture recognition has long been a hot topic in human computer interaction . traditional camera-based hand gesture recognition systems can not work properly under dark circumstances . in this paper , a doppler radar based hand gesture recognition system using convolutional neural networks is proposed . a cost-effective doppler radar sensor with dual receiving channels at 5.8ghz is used to acquire a big database of four standard gestures . the received hand gesture signals are then processed with time-frequency analysis . convolutional neural networks are used to classify different gestures . experimental results verify the effectiveness of the system with an accuracy of 98 % . besides , related factors such as recognition distance and gesture scale are investigated .", "topics": ["neural networks"]}
{"title": "predicting opponent team activity in a robocup environment", "abstract": "the goal of this project is to predict the opponent 's configuration in a robocup ssl environment . for simplicity , a markov model assumption is made such that the predicted formation of the opponent team only depends on its current formation . the field is divided into a grid and a robot state per player is created with information about its position and its velocity . to gather a more general sense of what the opposing team is doing , the state also incorporates the team 's average position ( centroid ) . all possible state transitions are stored in a hash table that requires minimum storage space . the table is populated with transition probabilities that are learned by reading vision packages and counting the state transitions regardless of the specific robot player . therefore , the computation during the game is reduced to interpreting a given vision package to assign each player to a state , and looking for the most likely state it will transition to . the confidence of the predicted team 's formation is the product of each individual player 's probability . the project is noteworthy in that it minimizes the time and space complexity requirements for opponent 's moves prediction .", "topics": ["computation", "robot"]}
{"title": "forecasting across time series databases using long short-term memory networks on groups of similar series", "abstract": "with the advent of big data , nowadays in many applications databases containing large quantities of similar time series are available . forecasting time series in these domains with traditional univariate forecasting procedures leaves great potentials for producing accurate forecasts untapped . recurrent neural networks , and in particular long short-term memory ( lstm ) networks have proven recently that they are able to outperform state-of-the-art univariate time series forecasting methods in this context , when trained across all available time series . however , if the time series database is heterogeneous accuracy may degenerate , so that on the way towards fully automatic forecasting methods in this space , a notion of similarity between the time series needs to be built into the methods . to this end , we present a prediction model using lstms on subgroups of similar time series , which are identified by time series clustering techniques . the proposed methodology is able to consistently outperform the baseline lstm model , and it achieves competitive results on benchmarking datasets , in particular outperforming all other methods on the cif2016 dataset .", "topics": ["baseline ( configuration management )", "time series"]}
{"title": "single-model encoder-decoder with explicit morphological representation for reinflection", "abstract": "morphological reinflection is the task of generating a target form given a source form , a source tag and a target tag . we propose a new way of modeling this task with neural encoder-decoder models . our approach reduces the amount of required training data for this architecture and achieves state-of-the-art results , making encoder-decoder models applicable to morphological reinflection even for low-resource languages . we further present a new automatic correction method for the outputs based on edit trees .", "topics": ["test set", "encoder"]}
{"title": "koniq-10k : towards an ecologically valid and large-scale iqa database", "abstract": "the main challenge in applying state-of-the-art deep learning methods to predict image quality in-the-wild is the relatively small size of existing quality scored datasets . the reason for the lack of larger datasets is the massive resources required in generating diverse and publishable content . we present a new systematic and scalable approach to create large-scale , authentic and diverse image datasets for image quality assessment ( iqa ) . we show how we built an iqa database , koniq-10k , consisting of 10,073 images , on which we performed very large scale crowdsourcing experiments in order to obtain reliable quality ratings from 1,467 crowd workers ( 1.2 million ratings ) . we argue for its ecological validity by analyzing the diversity of the dataset , by comparing it to state-of-the-art iqa databases , and by checking the reliability of our user studies .", "topics": ["scalability", "database"]}
{"title": "unbiasing truncated backpropagation through time", "abstract": "truncated backpropagation through time ( truncated bptt ) is a widespread method for learning recurrent computational graphs . truncated bptt keeps the computational benefits of backpropagation through time ( bptt ) while relieving the need for a complete backtrack through the whole data sequence at every step . however , truncation favors short-term dependencies : the gradient estimate of truncated bptt is biased , so that it does not benefit from the convergence guarantees from stochastic gradient theory . we introduce anticipated reweighted truncated backpropagation ( artbp ) , an algorithm that keeps the computational benefits of truncated bptt , while providing unbiasedness . artbp works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation . we check the viability of artbp on two tasks . first , a simple synthetic task where careful balancing of temporal dependencies at different scales is needed : truncated bptt displays unreliable performance , and in worst case scenarios , divergence , while artbp converges reliably . second , on penn treebank character-level language modelling , artbp slightly outperforms truncated bptt .", "topics": ["recurrent neural network", "synthetic data"]}
{"title": "online pairwise learning algorithms with kernels", "abstract": "pairwise learning usually refers to a learning task which involves a loss function depending on pairs of examples , among which most notable ones include ranking , metric learning and auc maximization . in this paper , we study an online algorithm for pairwise learning with a least-square loss function in an unconstrained setting of a reproducing kernel hilbert space ( rkhs ) , which we refer to as the online pairwise learning algorithm ( opera ) . in contrast to existing works \\cite { kar , wang } which require that the iterates are restricted to a bounded domain or the loss function is strongly-convex , opera is associated with a non-strongly convex objective function and learns the target function in an unconstrained rkhs . specifically , we establish a general theorem which guarantees the almost surely convergence for the last iterate of opera without any assumptions on the underlying distribution . explicit convergence rates are derived under the condition of polynomially decaying step sizes . we also establish an interesting property for a family of widely-used kernels in the setting of pairwise learning and illustrate the above convergence results using such kernels . our methodology mainly depends on the characterization of rkhss using its associated integral operators and probability inequalities for random variables with values in a hilbert space .", "topics": ["loss function"]}
{"title": "gradient-based hyperparameter optimization through reversible learning", "abstract": "tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable . we compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure . these gradients allow us to optimize thousands of hyperparameters , including step-size and momentum schedules , weight initialization distributions , richly parameterized regularization schemes , and neural network architectures . we compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum .", "topics": ["gradient descent", "reinforcement learning"]}
{"title": "parsing inside-out", "abstract": "the inside-outside probabilities are typically used for reestimating probabilistic context free grammars ( pcfgs ) , just as the forward-backward probabilities are typically used for reestimating hmms . i show several novel uses , including improving parser accuracy by matching parsing algorithms to evaluation criteria ; speeding up dop parsing by 500 times ; and 30 times faster pcfg thresholding at a given accuracy level . i also give an elegant , state-of-the-art grammar formalism , which can be used to compute inside-outside probabilities ; and a parser description formalism , which makes it easy to derive inside-outside formulas and many others .", "topics": ["approximation algorithm", "value ( ethics )"]}
{"title": "salgan : visual saliency prediction with generative adversarial networks", "abstract": "we introduce salgan , a deep convolutional neural network for visual saliency prediction trained with adversarial examples . the first stage of the network consists of a generator model whose weights are learned by back-propagation computed from a binary cross entropy ( bce ) loss over downsampled versions of the saliency maps . the resulting prediction is processed by a discriminator network trained to solve a binary classification task between the saliency maps generated by the generative stage and the ground truth ones . our experiments show how adversarial training allows reaching state-of-the-art performance across different metrics when combined with a widely-used loss function like bce .", "topics": ["loss function", "map"]}
{"title": "microsoft coco : common objects in context", "abstract": "we present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding . this is achieved by gathering images of complex everyday scenes containing common objects in their natural context . objects are labeled using per-instance segmentations to aid in precise object localization . our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old . with a total of 2.5 million labeled instances in 328k images , the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection , instance spotting and instance segmentation . we present a detailed statistical analysis of the dataset in comparison to pascal , imagenet , and sun . finally , we provide baseline performance analysis for bounding box and segmentation detection results using a deformable parts model .", "topics": ["baseline ( configuration management )", "image segmentation"]}
{"title": "prospective algorithms for quantum evolutionary computation", "abstract": "this effort examines the intersection of the emerging field of quantum computing and the more established field of evolutionary computation . the goal is to understand what benefits quantum computing might offer to computational intelligence and how computational intelligence paradigms might be implemented as quantum programs to be run on a future quantum computer . we critically examine proposed algorithms and methods for implementing computational intelligence paradigms , primarily focused on heuristic optimization methods including and related to evolutionary computation , with particular regard for their potential for eventual implementation on quantum computing hardware .", "topics": ["computation", "heuristic"]}
{"title": "skopus : mining top-k sequential patterns under leverage", "abstract": "this paper presents a framework for exact discovery of the top-k sequential patterns under leverage . it combines ( 1 ) a novel definition of the expected support for a sequential pattern - a concept on which most interestingness measures directly rely - with ( 2 ) skopus : a new branch-and-bound algorithm for the exact discovery of top-k sequential patterns under a given measure of interest . our interestingness measure employs the partition approach . a pattern is interesting to the extent that it is more frequent than can be explained by assuming independence between any of the pairs of patterns from which it can be composed . the larger the support compared to the expectation under independence , the more interesting is the pattern . we build on these two elements to exactly extract the k sequential patterns with highest leverage , consistent with our definition of expected support . we conduct experiments on both synthetic data with known patterns and real-world datasets ; both experiments confirm the consistency and relevance of our approach with regard to the state of the art . this article was published in data mining and knowledge discovery and is accessible at http : //dx.doi.org/10.1007/s10618-016-0467-9 .", "topics": ["synthetic data", "relevance"]}
{"title": "measurements of collective machine intelligence", "abstract": "independent from the still ongoing research in measuring individual intelligence , we anticipate and provide a framework for measuring collective intelligence . collective intelligence refers to the idea that several individuals can collaborate in order to achieve high levels of intelligence . we present thus some ideas on how the intelligence of a group can be measured and simulate such tests . we will however focus here on groups of artificial intelligence agents ( i.e . , machines ) . we will explore how a group of agents is able to choose the appropriate problem and to specialize for a variety of tasks . this is a feature which is an important contributor to the increase of intelligence in a group ( apart from the addition of more agents and the improvement due to common decision making ) . our results reveal some interesting results about how ( collective ) intelligence can be modeled , about how collective intelligence tests can be designed and about the underlying dynamics of collective intelligence . as it will be useful for our simulations , we provide also some improvements of the threshold allocation model originally used in the area of swarm intelligence but further generalized here .", "topics": ["simulation", "artificial intelligence"]}
{"title": "recognition of handwritten roman script using tesseract open source ocr engine", "abstract": "in the present work , we have used tesseract 2.01 open source optical character recognition ( ocr ) engine under apache license 2.0 for recognition of handwriting samples of lower case roman script . handwritten isolated and free-flow text samples were collected from multiple users . tesseract is trained to recognize user-specific handwriting samples of both the categories of document pages . on a single user model , the system is trained with 1844 isolated handwritten characters and the performance is tested on 1133 characters , taken form the test set . the overall character-level accuracy of the system is observed as 83.5 % . the system fails to segment 5.56 % characters and erroneously classifies 10.94 % characters .", "topics": ["test set"]}
{"title": "graphical models : an extension to random graphs , trees , and other objects", "abstract": "in this work , we consider an extension of graphical models to random graphs , trees , and other objects . to do this , many fundamental concepts for multivariate random variables ( e.g . , marginal variables , gibbs distribution , markov properties ) must be extended to other mathematical objects ; it turns out that this extension is possible , as we will discuss , if we have a consistent , complete system of projections on a given object . each projection defines a marginal random variable , allowing one to specify independence assumptions between them . furthermore , these independencies can be specified in terms of a small subset of these marginal variables ( which we call the atomic variables ) , allowing the compact representation of independencies by a directed graph . projections also define factors , functions on the projected object space , and hence a projection family defines a set of possible factorizations for a distribution ; these can be compactly represented by an undirected graph . the invariances used in graphical models are essential for learning distributions , not just on multivariate random variables , but also on other objects . when they are applied to random graphs and random trees , the result is a general class of models that is applicable to a broad range of problems , including those in which the graphs and trees have complicated edge structures . these models need not be conditioned on a fixed number of vertices , as is often the case in the literature for random graphs , and can be used for problems in which attributes are associated with vertices and edges . for graphs , applications include the modeling of molecules , neural networks , and relational real-world scenes ; for trees , applications include the modeling of infectious diseases , cell fusion , the structure of language , and the structure of objects in visual scenes . many classic models are particular instances of this framework .", "topics": ["graphical model"]}
{"title": "approximations for decision making in the dempster-shafer theory of evidence", "abstract": "the computational complexity of reasoning within the dempster-shafer theory of evidence is one of the main points of criticism this formalism has to face . to overcome this difficulty various approximation algorithms have been suggested that aim at reducing the number of focal elements in the belief functions involved . besides introducing a new algorithm using this method , this paper describes an empirical study that examines the appropriateness of these approximation procedures in decision making situations . it presents the empirical findings and discusses the various tradeoffs that have to be taken into account when actually applying one of these methods .", "topics": ["computational complexity theory", "approximation algorithm"]}
{"title": "recursive binary neural network learning model with 2.28b/weight storage requirement", "abstract": "this paper presents a storage-efficient learning model titled recursive binary neural networks for sensing devices having a limited amount of on-chip data storage such as < 100 's kilo-bytes . the main idea of the proposed model is to recursively recycle data storage of synaptic weights ( parameters ) during training . this enables a device with a given storage constraint to train and instantiate a neural network classifier with a larger number of weights on a chip and with a less number of off-chip storage accesses . this enables higher classification accuracy , shorter training time , less energy dissipation , and less on-chip storage requirement . we verified the training model with deep neural network classifiers and the permutation-invariant mnist benchmark . our model uses only 2.28 bits/weight while for the same data storage constraint achieving ~1 % lower classification error as compared to the conventional binary-weight learning model which yet has to use 8 to 16 bit storage per weight . to achieve the similar classification error , the conventional binary model requires ~4x more data storage for weights than the proposed model .", "topics": ["neural networks", "mnist database"]}
{"title": "flecs : planning with a flexible commitment strategy", "abstract": "there has been evidence that least-commitment planners can efficiently handle planning problems that involve difficult goal interactions . this evidence has led to the common belief that delayed-commitment is the `` best '' possible planning strategy . however , we recently found evidence that eager-commitment planners can handle a variety of planning problems more efficiently , in particular those with difficult operator choices . resigned to the futility of trying to find a universally successful planning strategy , we devised a planner that can be used to study which domains and problems are best for which planning strategies . in this article we introduce this new planning algorithm , flecs , which uses a flexible commitment strategy with respect to plan-step orderings . it is able to use any strategy from delayed-commitment to eager-commitment . the combination of delayed and eager operator-ordering commitments allows flecs to take advantage of the benefits of explicitly using a simulated execution state and reasoning about planning constraints . flecs can vary its commitment strategy across different problems and domains , and also during the course of a single planning problem . flecs represents a novel contribution to planning in that it explicitly provides the choice of which commitment strategy to use while planning . flecs provides a framework to investigate the mapping from planning domains and problems to efficient planning strategies .", "topics": ["simulation", "interaction"]}
{"title": "adaptive specular reflection detection and inpainting in colonoscopy video frames", "abstract": "colonoscopy video frames might be contaminated by bright spots with unsaturated values known as specular reflection . detection and removal of such reflections could enhance the quality of colonoscopy images and facilitate diagnosis procedure . in this paper we propose a novel two-phase method for this purpose , consisting of detection and removal phases . in the detection phase , we employ both hsv and rgb color space information for segmentation of specular reflections . we first train a non-linear svm for selecting a color space based on image statistical features extracted from each channel of the color spaces . then , a cost function for detection of specular reflections is introduced . in the removal phase , we propose a two-step inpainting method which consists of appropriate replacement patch selection and removal of the blockiness effects . the proposed method is evaluated by testing on an available colonoscopy image database where accuracy and dice score of 99.68 % and 71.79 % are achieved respectively .", "topics": ["nonlinear system", "loss function"]}
{"title": "lattice structures of fixed points of the lower approximations of two types of covering-based rough sets", "abstract": "covering is a common type of data structure and covering-based rough set theory is an efficient tool to process this data . lattice is an important algebraic structure and used extensively in investigating some types of generalized rough sets . in this paper , we propose two family of sets and study the conditions that these two sets become some lattice structures . these two sets are consisted by the fixed point of the lower approximations of the first type and the sixth type of covering-based rough sets , respectively . these two sets are called the fixed point set of neighborhoods and the fixed point set of covering , respectively . first , for any covering , the fixed point set of neighborhoods is a complete and distributive lattice , at the same time , it is also a double p-algebra . especially , when the neighborhood forms a partition of the universe , the fixed point set of neighborhoods is both a boolean lattice and a double stone algebra . second , for any covering , the fixed point set of covering is a complete lattice.when the covering is unary , the fixed point set of covering becomes a distributive lattice and a double p-algebra . a distributive lattice and a double p-algebra when the covering is unary . especially , when the reduction of the covering forms a partition of the universe , the fixed point set of covering is both a boolean lattice and a double stone algebra .", "topics": ["approximation"]}
{"title": "co-occurrence filter", "abstract": "co-occurrence filter ( cof ) is a boundary preserving filter . it is based on the bilateral filter ( bf ) but instead of using a gaussian on the range values to preserve edges it relies on a co-occurrence matrix . pixel values that co-occur frequently in the image ( i.e . , inside textured regions ) will have a high weight in the co-occurrence matrix . this , in turn , means that such pixel pairs will be averaged and hence smoothed , regardless of their intensity differences . on the other hand , pixel values that rarely co-occur ( i.e . , across texture boundaries ) will have a low weight in the co-occurrence matrix . as a result , they will not be averaged and the boundary between them will be preserved . the cof therefore extends the bf to deal with boundaries , not just edges . it learns co-occurrences directly from the image . we can achieve various filtering results by directing it to learn the co-occurrence matrix from a part of the image , or a different image . we give the definition of the filter , discuss how to use it with color images and show several use cases .", "topics": ["pixel"]}
{"title": "the power of asymmetry in binary hashing", "abstract": "when approximating binary similarity using the hamming distance between short binary hashes , we show that even if the similarity is symmetric , we can have shorter and more accurate hashes by using two distinct code maps . i.e . by approximating the similarity between $ x $ and $ x ' $ as the hamming distance between $ f ( x ) $ and $ g ( x ' ) $ , for two distinct binary codes $ f , g $ , rather than as the hamming distance between $ f ( x ) $ and $ f ( x ' ) $ .", "topics": ["approximation algorithm"]}
{"title": "billion-scale commodity embedding for e-commerce recommendation in alibaba", "abstract": "recommender systems ( rss ) have been the most important technology for increasing the business in taobao , the largest online consumer-to-consumer ( c2c ) platform in china . the billion-scale data in taobao creates three major challenges to taobao 's rs : scalability , sparsity and cold start . in this paper , we present our technical solutions to address these three challenges . the methods are based on the graph embedding framework . we first construct an item graph from users ' behavior history . each item is then represented as a vector using graph embedding . the item embeddings are employed to compute pairwise similarities between all items , which are then used in the recommendation process . to alleviate the sparsity and cold start problems , side information is incorporated into the embedding framework . we propose two aggregation methods to integrate the embeddings of items and the corresponding side information . experimental results from offline experiments show that methods incorporating side information are superior to those that do not . further , we describe the platform upon which the embedding methods are deployed and the workflow to process the billion-scale data in taobao . using online a/b test , we show that the online click-through-rate ( ctrs ) are improved comparing to the previous recommendation methods widely used in taobao , further demonstrating the effectiveness and feasibility of our proposed methods in taobao 's live production environment .", "topics": ["sparse matrix", "scalability"]}
{"title": "what a nerd ! beating students and vector cosine in the esl and toefl datasets", "abstract": "in this paper , we claim that vector cosine , which is generally considered one of the most efficient unsupervised measures for identifying word similarity in vector space models , can be outperformed by a completely unsupervised measure that evaluates the extent of the intersection among the most associated contexts of two target words , weighting such intersection according to the rank of the shared contexts in the dependency ranked lists . this claim comes from the hypothesis that similar words do not simply occur in similar contexts , but they share a larger portion of their most relevant contexts compared to other related words . to prove it , we describe and evaluate apsyn , a variant of average precision that , independently of the adopted parameters , outperforms the vector cosine and the co-occurrence on the esl and toefl test sets . in the best setting , apsyn reaches 0.73 accuracy on the esl dataset and 0.70 accuracy in the toefl dataset , beating therefore the non-english us college applicants ( whose average , as reported in the literature , is 64.50 % ) and several state-of-the-art approaches .", "topics": ["unsupervised learning"]}
{"title": "f-score driven max margin neural network for named entity recognition in chinese social media", "abstract": "we focus on named entity recognition ( ner ) for chinese social media . with massive unlabeled text and quite limited labelled corpus , we propose a semi-supervised learning model based on b-lstm neural network . to take advantage of traditional methods in ner such as crf , we combine transition probability with deep learning in our model . to bridge the gap between label accuracy and f-score of ner , we construct a model which can be directly trained on f-score . when considering the instability of f-score driven method and meaningful information provided by label accuracy , we propose an integrated method to train on both f-score and label accuracy . our integrated model yields 7.44\\ % improvement over previous state-of-the-art result .", "topics": ["markov chain"]}
{"title": "labeling of query words using conditional random field", "abstract": "this paper describes our approach on query word labeling as an attempt in the shared task on mixed script information retrieval at forum for information retrieval evaluation ( fire ) 2015 . the query is written in roman script and the words were in english or transliterated from indian regional languages . a total of eight indian languages were present in addition to english . we also identified the named entities and special symbols as part of our task . a crf based machine learning framework was used for labeling the individual words with their corresponding language labels . we used a dictionary based approach for language identification . we also took into account the context of the word while identifying the language . our system demonstrated an overall accuracy of 75.5 % for token level language identification . the strict f-measure scores for the identification of token level language labels for bengali , english and hindi are 0.7486 , 0.892 and 0.7972 respectively . the overall weighted f-measure of our system was 0.7498 .", "topics": ["dictionary"]}
{"title": "image quality assessment for performance evaluation of focus measure operators", "abstract": "this paper presents the performance evaluation of eight focus measure operators namely image curv ( curvature ) , grae ( gradient energy ) , hise ( histogram entropy ) , lapm ( modified laplacian ) , lapv ( variance of laplacian ) , lapd ( diagonal laplacian ) , lap3 ( laplacian in 3d window ) and wavs ( sum of wavelet coefficients ) . statistical matrics such as mse ( mean squared error ) , pnsr ( peak signal to noise ratio ) , sc ( structural content ) , ncc ( normalized cross correlation ) , md ( maximum difference ) and nae ( normalized absolute error ) are used to evaluate stated focus measures in this research . . fr ( full reference ) method of the image quality assessment is utilized in this paper . results indicate that lapd method is comparatively better than other seven focus operators at typical imaging conditions .", "topics": ["gradient"]}
{"title": "blind construction of optimal nonlinear recursive predictors for discrete sequences", "abstract": "we present a new method for nonlinear prediction of discrete random sequences under minimal structural assumptions . we give a mathematical construction for optimal predictors of such processes , in the form of hidden markov models . we then describe an algorithm , cssr ( causal-state splitting reconstruction ) , which approximates the ideal predictor from data . we discuss the reliability of cssr , its data requirements , and its performance in simulations . finally , we compare our approach to existing methods using variablelength markov models and cross-validated hidden markov models , and show theoretically and experimentally that our method delivers results superior to the former and at least comparable to the latter .", "topics": ["nonlinear system", "simulation"]}
{"title": "a deterministic and generalized framework for unsupervised learning with restricted boltzmann machines", "abstract": "restricted boltzmann machines ( rbms ) are energy-based neural-networks which are commonly used as the building blocks for deep architectures neural architectures . in this work , we derive a deterministic framework for the training , evaluation , and use of rbms based upon the thouless-anderson-palmer ( tap ) mean-field approximation of widely-connected systems with weak interactions coming from spin-glass theory . while the tap approach has been extensively studied for fully-visible binary spin systems , our construction is generalized to latent-variable models , as well as to arbitrarily distributed real-valued spin systems with bounded support . in our numerical experiments , we demonstrate the effective deterministic training of our proposed models and are able to show interesting features of unsupervised learning which could not be directly observed with sampling . additionally , we demonstrate how to utilize our tap-based framework for leveraging trained rbms as joint priors in denoising problems .", "topics": ["unsupervised learning", "numerical analysis"]}
{"title": "inception recurrent convolutional neural network for object recognition", "abstract": "deep convolutional neural networks ( dcnns ) are an influential tool for solving various problems in the machine learning and computer vision fields . in this paper , we introduce a new deep learning model called an inception- recurrent convolutional neural network ( ircnn ) , which utilizes the power of an inception network combined with recurrent layers in dcnn architecture . we have empirically evaluated the recognition performance of the proposed ircnn model using different benchmark datasets such as mnist , cifar-10 , cifar- 100 , and svhn . experimental results show similar or higher recognition accuracy when compared to most of the popular dcnns including the rcnn . furthermore , we have investigated ircnn performance against equivalent inception networks and inception-residual networks using the cifar-100 dataset . we report about 3.5 % , 3.47 % and 2.54 % improvement in classification accuracy when compared to the rcnn , equivalent inception networks , and inception- residual networks on the augmented cifar- 100 dataset respectively .", "topics": ["computer vision", "mnist database"]}
{"title": "iterative multi-domain regularized deep learning for anatomical structure detection and segmentation from ultrasound images", "abstract": "accurate detection and segmentation of anatomical structures from ultrasound images are crucial for clinical diagnosis and biometric measurements . although ultrasound imaging has been widely used with superiorities such as low cost and portability , the fuzzy border definition and existence of abounding artifacts pose great challenges for automatically detecting and segmenting the complex anatomical structures . in this paper , we propose a multi-domain regularized deep learning method to address this challenging problem . by leveraging the transfer learning from cross domains , the feature representations are effectively enhanced . the results are further improved by the iterative refinement . moreover , our method is quite efficient by taking advantage of a fully convolutional network , which is formulated as an end-to-end learning framework of detection and segmentation . extensive experimental results on a large-scale database corroborated that our method achieved a superior detection and segmentation accuracy , outperforming other methods by a significant margin and demonstrating competitive capability even compared to human performance .", "topics": ["iteration", "end-to-end principle"]}
{"title": "robust image segmentation in low depth of field images", "abstract": "in photography , low depth of field ( dof ) is an important technique to emphasize the object of interest ( ooi ) within an image . thus , low dof images are widely used in the application area of macro , portrait or sports photography . when viewing a low dof image , the viewer implicitly concentrates on the regions that are sharper regions of the image and thus segments the image into regions of interest and non regions of interest which has a major impact on the perception of the image . thus , a robust algorithm for the fully automatic detection of the ooi in low dof images provides valuable information for subsequent image processing and image retrieval . in this paper we propose a robust and parameterless algorithm for the fully automatic segmentation of low dof images . we compare our method with three similar methods and show the superior robustness even though our algorithm does not require any parameters to be set by hand . the experiments are conducted on a real world data set with high and low dof images .", "topics": ["image segmentation"]}
{"title": "detecting truth , just on parts", "abstract": "we introduce and discuss , through a computational algebraic geometry approach , the automatic reasoning handling of propositions that are simultaneously true and false over some relevant collections of instances . a rigorous , algorithmic criterion is presented for detecting such cases , and its performance is exemplified through the implementation of this test on the dynamic geometry program geogebra .", "topics": ["eisenstein 's criterion"]}
{"title": "from kernel machines to ensemble learning", "abstract": "ensemble methods such as boosting combine multiple learners to obtain better prediction than could be obtained from any individual learner . here we propose a principled framework for directly constructing ensemble learning methods from kernel methods . unlike previous studies showing the equivalence between boosting and support vector machines ( svms ) , which needs a translation procedure , we show that it is possible to design boosting-like procedure to solve the svm optimization problems . in other words , it is possible to design ensemble methods directly from svm without any middle procedure . this finding not only enables us to design new ensemble learning methods directly from kernel methods , but also makes it possible to take advantage of those highly-optimized fast linear svm solvers for ensemble learning . we exemplify this framework for designing binary ensemble learning as well as a new multi-class ensemble learning methods . experimental results demonstrate the flexibility and usefulness of the proposed framework .", "topics": ["support vector machine"]}
{"title": "bat algorithm is better than intermittent search strategy", "abstract": "the efficiency of any metaheuristic algorithm largely depends on the way of balancing local intensive exploitation and global diverse exploration . studies show that bat algorithm can provide a good balance between these two key components with superior efficiency . in this paper , we first review some commonly used metaheuristic algorithms , and then compare the performance of bat algorithm with the so-called intermittent search strategy . from simulations , we found that bat algorithm is better than the optimal intermittent search strategy . we also analyse the comparison results and their implications for higher dimensional optimization problems . in addition , we also apply bat algorithm in solving business optimization and engineering design problems .", "topics": ["optimization problem", "simulation"]}
{"title": "real-time end-to-end action detection with two-stream networks", "abstract": "two-stream networks have been very successful for solving the problem of action detection . however , prior work using two-stream networks train both streams separately , which prevents the network from exploiting regularities between the two streams . moreover , unlike the visual stream , the dominant forms of optical flow computation typically do not maximally exploit gpu parallelism . we present a real-time end-to-end trainable two-stream network for action detection . first , we integrate the optical flow computation in our framework by using flownet2 . second , we apply early fusion for the two streams and train the whole pipeline jointly end-to-end . finally , for better network initialization , we transfer from the task of action recognition to action detection by pre-training our framework using the recently released large-scale kinetics dataset . our experimental results show that training the pipeline jointly end-to-end with fine-tuning the optical flow for the objective of action detection improves detection performance significantly . additionally , we observe an improvement when initializing with parameters pre-trained using kinetics . last , we show that by integrating the optical flow computation , our framework is more efficient , running at real-time speeds ( up to 31 fps ) .", "topics": ["computation", "end-to-end principle"]}
{"title": "viterbi algorithm generalized for n-tape best-path search", "abstract": "we present a generalization of the viterbi algorithm for identifying the path with minimal ( resp . maximal ) weight in a n-tape weighted finite-state machine ( n-wfsm ) , that accepts a given n-tuple of input strings ( s_1 , ... s_n ) . it also allows us to compile the best transduction of a given input n-tuple by a weighted ( n+m ) -wfsm ( transducer ) with n input and m output tapes . our algorithm has a worst-case time complexity of o ( |s|^n |e| log ( |s|^n |q| ) ) , where n and |s| are the number and average length of the strings in the n-tuple , and |q| and |e| the number of states and transitions in the n-wfsm , respectively . a straight forward alternative , consisting in intersection followed by classical shortest-distance search , operates in o ( |s|^n ( |e|+|q| ) log ( |s|^n |q| ) ) time .", "topics": ["time complexity"]}
{"title": "object segmentation in images using eeg signals", "abstract": "this paper explores the potential of brain-computer interfaces in segmenting objects from images . our approach is centered around designing an effective method for displaying the image parts to the users such that they generate measurable brain reactions . when an image region , specifically a block of pixels , is displayed we estimate the probability of the block containing the object of interest using a score based on eeg activity . after several such blocks are displayed , the resulting probability map is binarized and combined with the grabcut algorithm to segment the image into object and background regions . this study shows that bci and simple eeg analysis are useful in locating object boundaries in images .", "topics": ["pixel"]}
{"title": "accelerated mini-batch stochastic dual coordinate ascent", "abstract": "stochastic dual coordinate ascent ( sdca ) is an effective technique for solving regularized loss minimization problems in machine learning . this paper considers an extension of sdca under the mini-batch setting that is often used in practice . our main contribution is to introduce an accelerated mini-batch version of sdca and prove a fast convergence rate for this method . we discuss an implementation of our method over a parallel computing system , and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of \\cite { nesterov2007gradient } .", "topics": ["gradient descent", "gradient"]}
{"title": "a novel model of working set selection for smo decomposition methods", "abstract": "in the process of training support vector machines ( svms ) by decomposition methods , working set selection is an important technique , and some exciting schemes were employed into this field . to improve working set selection , we propose a new model for working set selection in sequential minimal optimization ( smo ) decomposition methods . in this model , it selects b as working set without reselection . some properties are given by simple proof , and experiments demonstrate that the proposed method is in general faster than existing methods .", "topics": ["support vector machine"]}
{"title": "text coherence analysis based on deep neural network", "abstract": "in this paper , we propose a novel deep coherence model ( dcm ) using a convolutional neural network architecture to capture the text coherence . the text coherence problem is investigated with a new perspective of learning sentence distributional representation and text coherence modeling simultaneously . in particular , the model captures the interactions between sentences by computing the similarities of their distributional representations . further , it can be easily trained in an end-to-end fashion . the proposed model is evaluated on a standard sentence ordering task . the experimental results demonstrate its effectiveness and promise in coherence assessment showing a significant improvement over the state-of-the-art by a wide margin .", "topics": ["interaction", "end-to-end principle"]}
{"title": "predicting patient state-of-health using sliding window and recurrent classifiers", "abstract": "bedside monitors in intensive care units ( icus ) frequently sound incorrectly , slowing response times and desensitising nurses to alarms ( chambrin , 2001 ) , causing true alarms to be missed ( hug et al . , 2011 ) . we compare sliding window predictors with recurrent predictors to classify patient state-of-health from icu multivariate time series ; we report slightly improved performance for the rnn for three out of four targets .", "topics": ["time series"]}
{"title": "shiva : a framework for graph based ontology matching", "abstract": "since long , corporations are looking for knowledge sources which can provide structured description of data and can focus on meaning and shared understanding . structures which can facilitate open world assumptions and can be flexible enough to incorporate and recognize more than one name for an entity . a source whose major purpose is to facilitate human communication and interoperability . clearly , databases fail to provide these features and ontologies have emerged as an alternative choice , but corporations working on same domain tend to make different ontologies . the problem occurs when they want to share their data/knowledge . thus we need tools to merge ontologies into one . this task is termed as ontology matching . this is an emerging area and still we have to go a long way in having an ideal matcher which can produce good results . in this paper we have shown a framework to matching ontologies using graphs .", "topics": ["database"]}
{"title": "better mixing via deep representations", "abstract": "it has previously been hypothesized , and supported with some experimental evidence , that deeper representations , when well trained , tend to do a better job at disentangling the underlying factors of variation . we study the following related conjecture : better representations , in the sense of better disentangling , can be exploited to produce faster-mixing markov chains . consequently , mixing would be more efficient at higher levels of representation . to better understand why and how this is happening , we propose a secondary conjecture : the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels . the paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing and interpolating between samples .", "topics": ["markov chain"]}
{"title": "graph-grammar assistance for automated generation of influence diagrams", "abstract": "one of the most difficult aspects of modeling complex dilemmas in decision-analytic terms is composing a diagram of relevance relations from a set of domain concepts . decision models in domains such as medicine , however , exhibit certain prototypical patterns that can guide the modeling process . medical concepts can be classified according to semantic types that have characteristic positions and typical roles in an influence-diagram model . we have developed a graph-grammar production system that uses such inherent interrelationships among medical terms to facilitate the modeling of medical decisions .", "topics": ["relevance"]}
{"title": "robust sparse blind source separation", "abstract": "blind source separation is a widely used technique to analyze multichannel data . in many real-world applications , its results can be significantly hampered by the presence of unknown outliers . in this paper , a novel algorithm coined rgmca ( robust generalized morphological component analysis ) is introduced to retrieve sparse sources in the presence of outliers . it explicitly estimates the sources , the mixing matrix , and the outliers . it also takes advantage of the estimation of the outliers to further implement a weighting scheme , which provides a highly robust separation procedure . numerical experiments demonstrate the efficiency of rgmca to estimate the mixing matrix in comparison with standard bss techniques .", "topics": ["numerical analysis", "sparse matrix"]}
{"title": "detailed derivations of small-variance asymptotics for some hierarchical bayesian nonparametric models", "abstract": "in this note we provide detailed derivations of two versions of small-variance asymptotics for hierarchical dirichlet process ( hdp ) mixture models and the hdp hidden markov model ( hdp-hmm , a.k.a . the infinite hmm ) . we include derivations for the probabilities of certain crp and crf partitions , which are of more general interest .", "topics": ["cluster analysis", "time series"]}
{"title": "throwing fuel on the embers : probability or dichotomy , cognitive or linguistic ?", "abstract": "prof. robert berwick 's abstract for his forthcoming invited talk at the acl2016 workshop on cognitive aspects of computational language learning revives an ancient debate . entitled `` why take a chance ? `` , berwick seems to refer implicitly to chomsky 's critique of the statistical approach of harris as well as the currently dominant paradigms in conll . berwick avoids chomsky 's use of `` innate '' but states that `` the debate over the existence of sophisticated mental grammars was settled with chomsky 's logical structure of linguistic theory ( 1957/1975 ) '' , acknowledging that `` this debate has often been revived '' . this paper agrees with the view that this debate has long since been settled , but with the opposite outcome ! given the embers have not yet died away , and the questions remain fundamental , perhaps it is appropriate to refuel the debate , so i would like to join bob in throwing fuel on this fire by reviewing the evidence against the chomskian position !", "topics": ["approximation"]}
{"title": "the paralleleye dataset : constructing large-scale artificial scenes for traffic vision research", "abstract": "video image datasets are playing an essential role in design and evaluation of traffic vision algorithms . nevertheless , a longstanding inconvenience concerning image datasets is that manually collecting and annotating large-scale diversified datasets from real scenes is time-consuming and prone to error . for that virtual datasets have begun to function as a proxy of real datasets . in this paper , we propose to construct large-scale artificial scenes for traffic vision research and generate a new virtual dataset called `` paralleleye '' . first of all , the street map data is used to build 3d scene model of zhongguancun area , beijing . then , the computer graphics , virtual reality , and rule modeling technologies are utilized to synthesize large-scale , realistic virtual urban traffic scenes , in which the fidelity and geography match the real world well . furthermore , the unity3d platform is used to render the artificial scenes and generate accurate ground-truth labels , e.g . , semantic/instance segmentation , object bounding box , object tracking , optical flow , and depth . the environmental conditions in artificial scenes can be controlled completely . as a result , we present a viable implementation pipeline for constructing large-scale artificial scenes for traffic vision research . the experimental results demonstrate that this pipeline is able to generate photorealistic virtual datasets with low modeling time and high accuracy labeling .", "topics": ["ground truth"]}
{"title": "sequential ensemble learning for outlier detection : a bias-variance perspective", "abstract": "ensemble methods for classification and clustering have been effectively used for decades , while ensemble learning for outlier detection has only been studied recently . in this work , we design a new ensemble approach for outlier detection in multi-dimensional point data , which provides improved accuracy by reducing error through both bias and variance . although classification and outlier detection appear as different problems , their theoretical underpinnings are quite similar in terms of the bias-variance trade-off [ 1 ] , where outlier detection is considered as a binary classification task with unobserved labels but a similar bias-variance decomposition of error . in this paper , we propose a sequential ensemble approach called care that employs a two-phase aggregation of the intermediate results in each iteration to reach the final outcome . unlike existing outlier ensembles which solely incorporate a parallel framework by aggregating the outcomes of independent base detectors to reduce variance , our ensemble incorporates both the parallel and sequential building blocks to reduce bias as well as variance by ( $ i $ ) successively eliminating outliers from the original dataset to build a better data model on which outlierness is estimated ( sequentially ) , and ( $ ii $ ) combining the results from individual base detectors and across iterations ( parallelly ) . through extensive experiments on sixteen real-world datasets mainly from the uci machine learning repository [ 2 ] , we show that care performs significantly better than or at least similar to the individual baselines . we also compare care with the state-of-the-art outlier ensembles where it also provides significant improvement when it is the winner and remains close otherwise .", "topics": ["iteration"]}
{"title": "proceedings of the seventh conference on uncertainty in artificial intelligence ( 1991 )", "abstract": "this is the proceedings of the seventh conference on uncertainty in artificial intelligence , which was held in los angeles , ca , july 13-15 , 1991", "topics": ["artificial intelligence"]}
{"title": "performance analysis of unsupervised feature selection methods", "abstract": "feature selection ( fs ) is a process which attempts to select more informative features . in some cases , too many redundant or irrelevant features may overpower main features for classification . feature selection can remedy this problem and therefore improve the prediction accuracy and reduce the computational overhead of classification algorithms . the main aim of feature selection is to determine a minimal feature subset from a problem domain while retaining a suitably high accuracy in representing the original features . in this paper , principal component analysis ( pca ) , rough pca , unsupervised quick reduct ( usqr ) algorithm and empirical distribution ranking ( edr ) approaches are applied to discover discriminative features that will be the most adequate ones for classification . efficiency of the approaches is evaluated using standard classification metrics .", "topics": ["relevance"]}
{"title": "xnorbin : a 95 top/s/w hardware accelerator for binary convolutional neural networks", "abstract": "deploying state-of-the-art cnns requires power-hungry processors and off-chip memory . this precludes the implementation of cnns in low-power embedded systems . recent research shows cnns sustain extreme quantization , binarizing their weights and intermediate feature maps , thereby saving 8-32\\x memory and collapsing energy-intensive sum-of-products into xnor-and-popcount operations . we present xnorbin , an accelerator for binary cnns with computation tightly coupled to memory for aggressive data reuse . implemented in umc 65nm technology xnorbin achieves an energy efficiency of 95 top/s/w and an area efficiency of 2.0 top/s/mge at 0.8 v .", "topics": ["computation"]}
{"title": "unsupervised clustering under the union of polyhedral cones ( uopc ) model", "abstract": "in this paper , we consider clustering data that is assumed to come from one of finitely many pointed convex polyhedral cones . this model is referred to as the union of polyhedral cones ( uopc ) model . similar to the union of subspaces ( uos ) model where each data from each subspace is generated from a ( unknown ) basis , in the uopc model each data from each cone is assumed to be generated from a finite number of ( unknown ) \\emph { extreme rays } .to cluster data under this model , we consider several algorithms - ( a ) sparse subspace clustering by non-negative constraints lasso ( ncl ) , ( b ) least squares approximation ( lsa ) , and ( c ) k-nearest neighbor ( knn ) algorithm to arrive at affinity between data points . spectral clustering ( sc ) is then applied on the resulting affinity matrix to cluster data into different polyhedral cones . we show that on an average knn outperforms both ncl and lsa and for this algorithm we provide the deterministic conditions for correct clustering . for an affinity measure between the cones it is shown that as long as the cones are not very coherent and as long as the density of data within each cone exceeds a threshold , knn leads to accurate clustering . finally , simulation results on real datasets ( mnist and yaleface datasets ) depict that the proposed algorithm works well on real data indicating the utility of the uopc model and the proposed algorithm .", "topics": ["cluster analysis"]}
{"title": "agglomerative clustering and collectiveness measure via exponent generating function", "abstract": "the key in agglomerative clustering is to define the affinity measure between two sets . a novel agglomerative clustering method is proposed by utilizing the path integral to define the affinity measure . firstly , the path integral descriptor of an edge , a node and a set is computed by path integral and exponent generating function . then , the affinity measure between two sets is obtained by path integral descriptor of sets . several good properties of the path integral descriptor is proposed in this paper . in addition , we give the physical interpretation of the proposed path integral descriptor of a set . the proposed path integral descriptor of a set can be regard as the collectiveness measure of a set , which can be a moving system such as human crowd , sheep herd and so on . self-driven particle ( sdp ) model is used to test the ability of the proposed method in measuring collectiveness .", "topics": ["cluster analysis"]}
{"title": "deciding morality of graphs is np-complete", "abstract": "in order to find a causal explanation for data presented in the form of covariance and concentration matrices it is necessary to decide if the graph formed by such associations is a projection of a directed acyclic graph ( dag ) . we show that the general problem of deciding whether such a dag exists is np-complete .", "topics": ["causality"]}
{"title": "clustering under perturbation resilience", "abstract": "motivated by the fact that distances between data points in many real-world clustering instances are often based on heuristic measures , bilu and linial~\\cite { bl } proposed analyzing objective based clustering problems under the assumption that the optimum clustering to the objective is preserved under small multiplicative perturbations to distances between points . the hope is that by exploiting the structure in such instances , one can overcome worst case hardness results . in this paper , we provide several results within this framework . for center-based objectives , we present an algorithm that can optimally cluster instances resilient to perturbations of factor $ ( 1 + \\sqrt { 2 } ) $ , solving an open problem of awasthi et al.~\\cite { abs10 } . for $ k $ -median , a center-based objective of special interest , we additionally give algorithms for a more relaxed assumption in which we allow the optimal solution to change in a small $ \\epsilon $ fraction of the points after perturbation . we give the first bounds known for $ k $ -median under this more realistic and more general assumption . we also provide positive results for min-sum clustering which is typically a harder objective than center-based objectives from approximability standpoint . our algorithms are based on new linkage criteria that may be of independent interest . additionally , we give sublinear-time algorithms , showing algorithms that can return an implicit clustering from only access to a small random sample .", "topics": ["cluster analysis", "optimization problem"]}
{"title": "introduction to intelligent computing unit 1", "abstract": "this brief note highlights some basic concepts required toward understanding the evolution of machine learning and deep learning models . the note starts with an overview of artificial intelligence and its relationship to biological neuron that ultimately led to the evolution of todays intelligent models .", "topics": ["artificial intelligence"]}
{"title": "extending term subsumption systems for uncertainty management", "abstract": "a major difficulty in developing and maintaining very large knowledge bases originates from the variety of forms in which knowledge is made available to the kb builder . the objective of this research is to bring together two complementary knowledge representation schemes : term subsumption languages , which represent and reason about defining characteristics of concepts , and proximate reasoning models , which deal with uncertain knowledge and data in expert systems . previous works in this area have primarily focused on probabilistic inheritance . in this paper , we address two other important issues regarding the integration of term subsumption-based systems and approximate reasoning models . first , we outline a general architecture that specifies the interactions between the deductive reasoner of a term subsumption system and an approximate reasoner . second , we generalize the semantics of terminological language so that terminological knowledge can be used to make plausible inferences . the architecture , combined with the generalized semantics , forms the foundation of a synergistic tight integration of term subsumption systems and approximate reasoning models .", "topics": ["approximation algorithm", "interaction"]}
{"title": "learning architectures based on quantum entanglement : a simple matrix product state algorithm for image recognition", "abstract": "it is a fundamental , but still elusive question whether methods based on quantum mechanics , in particular on quantum entanglement , can be used for classical information processing and machine learning . even partial answer to this question would bring important insights to both fields of both machine learning and quantum mechanics . in this work , we implement simple numerical experiments , related to pattern/images classification , in which we represent the classifiers by quantum matrix product states ( mps ) . classical machine learning algorithm is then applied to these quantum states . we explicitly show how quantum features ( i.e . , single-site and bipartite entanglement ) can emerge in such represented images ; entanglement characterizes here the importance of data , and this information can be practically used to improve the learning procedures . thanks to the low demands on the dimensions and number of the unitary matrices , necessary to construct the mps , we expect such numerical experiments could open new paths in classical machine learning , and shed at same time lights on generic quantum simulations/computations .", "topics": ["numerical analysis", "computer vision"]}
{"title": "online object tracking , learning and parsing with and-or graphs", "abstract": "this paper presents a method , called aogtracker , for simultaneously tracking , learning and parsing ( tlp ) of unknown objects in video sequences with a hierarchical and compositional and-or graph ( aog ) representation . % the aog captures both structural and appearance variations of a target object in a principled way . the tlp method is formulated in the bayesian framework with a spatial and a temporal dynamic programming ( dp ) algorithms inferring object bounding boxes on-the-fly . during online learning , the aog is discriminatively learned using latent svm to account for appearance ( e.g . , lighting and partial occlusion ) and structural ( e.g . , different poses and viewpoints ) variations of a tracked object , as well as distractors ( e.g . , similar objects ) in background . three key issues in online inference and learning are addressed : ( i ) maintaining purity of positive and negative examples collected online , ( ii ) controling model complexity in latent structure learning , and ( iii ) identifying critical moments to re-learn the structure of aog based on its intrackability . the intrackability measures uncertainty of an aog based on its score maps in a frame . in experiments , our aogtracker is tested on two popular tracking benchmarks with the same parameter setting : the tb-100/50/cvpr2013 benchmarks , and the vot benchmarks -- - vot 2013 , 2014 , 2015 and tir2015 ( thermal imagery tracking ) . in the former , our aogtracker outperforms state-of-the-art tracking algorithms including two trackers based on deep convolutional network . in the latter , our aogtracker outperforms all other trackers in vot2013 and is comparable to the state-of-the-art methods in vot2014 , 2015 and tir2015 .", "topics": ["parsing", "map"]}
{"title": "`` i have a feeling trump will win ... ... ... ... ... ... '' : forecasting winners and losers from user predictions on twitter", "abstract": "social media users often make explicit predictions about upcoming events . such statements vary in the degree of certainty the author expresses toward the outcome : '' leonardo dicaprio will win best actor '' vs. `` leonardo dicaprio may win '' or `` no way leonardo wins ! '' . can popular beliefs on social media predict who will win ? to answer this question , we build a corpus of tweets annotated for veridicality on which we train a log-linear classifier that detects positive veridicality with high precision . we then forecast uncertain outcomes using the wisdom of crowds , by aggregating users ' explicit predictions . our method for forecasting winners is fully automated , relying only on a set of contenders as input . it requires no training data of past outcomes and outperforms sentiment and tweet volume baselines on a broad range of contest prediction tasks . we further demonstrate how our approach can be used to measure the reliability of individual accounts ' predictions and retrospectively identify surprise outcomes .", "topics": ["test set"]}
{"title": "relative pairwise relationship constrained non-negative matrix factorisation", "abstract": "non-negative matrix factorisation ( nmf ) has been extensively used in machine learning and data analytics applications . most existing variations of nmf only consider how each row/column vector of factorised matrices should be shaped , and ignore the relationship among pairwise rows or columns . in many cases , such pairwise relationship enables better factorisation , for example , image clustering and recommender systems . in this paper , we propose an algorithm named , relative pairwise relationship constrained non-negative matrix factorisation ( rpr-nmf ) , which places constraints over relative pairwise distances amongst features by imposing penalties in a triplet form . two distance measures , squared euclidean distance and symmetric divergence , are used , and exponential and hinge loss penalties are adopted for the two measures respectively . it is well known that the so-called `` multiplicative update rules '' result in a much faster convergence than gradient descend for matrix factorisation . however , applying such update rules to rpr-nmf and also proving its convergence is not straightforward . thus , we use reasonable approximations to relax the complexity brought by the penalties , which are practically verified . experiments on both synthetic datasets and real datasets demonstrate that our algorithms have advantages on gaining close approximation , satisfying a high proportion of expected constraints , and achieving superior performance compared with other algorithms .", "topics": ["cluster analysis", "time complexity"]}
{"title": "distributional reinforcement learning with quantile regression", "abstract": "in reinforcement learning an agent interacts with the environment by taking actions and observing the next state and reward . when sampled probabilistically , these state transitions , rewards , and actions can all induce randomness in the observed long-term return . traditionally , reinforcement learning algorithms average over this randomness to estimate the value function . in this paper , we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean . that is , we examine methods of learning the value distribution instead of the value function . we give results that close a number of gaps between the theoretical and algorithmic results given by bellemare , dabney , and munos ( 2017 ) . first , we extend existing results to the approximate distribution setting . second , we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation . finally , we evaluate this new algorithm on the atari 2600 games , observing that it significantly outperforms many of the recent improvements on dqn , including the related distributional algorithm c51 .", "topics": ["approximation algorithm", "reinforcement learning"]}
{"title": "propositional abduction with implicit hitting sets", "abstract": "logic-based abduction finds important applications in artificial intelligence and related areas . one application example is in finding explanations for observed phenomena . propositional abduction is a restriction of abduction to the propositional domain , and complexity-wise is in the second level of the polynomial hierarchy . recent work has shown that exploiting implicit hitting sets and propositional satisfiability ( sat ) solvers provides an efficient approach for propositional abduction . this paper investigates this earlier work and proposes a number of algorithmic improvements . these improvements are shown to yield exponential reductions in the number of sat solver calls . more importantly , the experimental results show significant performance improvements compared to the the best approaches for propositional abduction .", "topics": ["time complexity", "artificial intelligence"]}
{"title": "context related derivation of word senses", "abstract": "real applications of natural language document processing are very often confronted with domain specific lexical gaps during the analysis of documents of a new domain . this paper describes an approach for the derivation of domain specific concepts for the extension of an existing ontology . as resources we need an initial ontology and a partially processed corpus of a domain . we exploit the specific characteristic of the sublanguage in the corpus . our approach is based on syntactical structures ( noun phrases ) and compound analyses to extract information required for the extension of germanet 's lexical resources .", "topics": ["natural language processing", "natural language"]}
{"title": "learning features for relational data", "abstract": "feature engineering is one of the most important but tedious tasks in data science projects . this work studies automation of feature learning for relational data . we first theoretically proved that learning relevant features from relational data for a given predictive analytics problem is np-hard . however , it is possible to empirically show that an efficient rule based approach predefining transformations as a priori based on heuristics can extract very useful features from relational data . indeed , the proposed approach outperformed the state of the art solutions with a significant margin . we further introduce a deep neural network which automatically learns appropriate transformations of relational data into a representation that predicts the target variable well instead of being predefined as a priori by users . in an extensive experiment with kaggle competitions , the proposed methods could win late medals . to the best of our knowledge , this is the first time an automation system could win medals in kaggle competitions with complex relational data .", "topics": ["feature learning", "heuristic"]}
{"title": "margins , shrinkage , and boosting", "abstract": "this manuscript shows that adaboost and its immediate variants can produce approximate maximum margin classifiers simply by scaling step size choices with a fixed small constant . in this way , when the unscaled step size is an optimal choice , these results provide guarantees for friedman 's empirically successful `` shrinkage '' procedure for gradient boosting ( friedman , 2000 ) . guarantees are also provided for a variety of other step sizes , affirming the intuition that increasingly regularized line searches provide improved margin guarantees . the results hold for the exponential loss and similar losses , most notably the logistic loss .", "topics": ["time complexity", "gradient"]}
{"title": "topic aware neural response generation", "abstract": "we consider incorporating topic information into the sequence-to-sequence framework to generate informative and interesting responses for chatbots . to this end , we propose a topic aware sequence-to-sequence ( ta-seq2seq ) model . the model utilizes topics to simulate prior knowledge of human that guides them to form informative and interesting responses in conversation , and leverages the topic information in generation by a joint attention mechanism and a biased generation probability . the joint attention mechanism summarizes the hidden vectors of an input message as context vectors by message attention , synthesizes topic vectors by topic attention from the topic words of the message obtained from a pre-trained lda model , and let these vectors jointly affect the generation of words in decoding . to increase the possibility of topic words appearing in responses , the model modifies the generation probability of topic words by adding an extra probability item to bias the overall distribution . empirical study on both automatic evaluation metrics and human annotations shows that ta-seq2seq can generate more informative and interesting responses , and significantly outperform the-state-of-the-art response generation models .", "topics": ["simulation"]}
{"title": "stochastic gradient descent as approximate bayesian inference", "abstract": "stochastic gradient descent with a constant learning rate ( constant sgd ) simulates a markov chain with a stationary distribution . with this perspective , we derive several new results . ( 1 ) we show that constant sgd can be used as an approximate bayesian posterior inference algorithm . specifically , we show how to adjust the tuning parameters of constant sgd to best match the stationary distribution to a posterior , minimizing the kullback-leibler divergence between these two distributions . ( 2 ) we demonstrate that constant sgd gives rise to a new variational em algorithm that optimizes hyperparameters in complex probabilistic models . ( 3 ) we also propose sgd with momentum for sampling and show how to adjust the damping coefficient accordingly . ( 4 ) we analyze mcmc algorithms . for langevin dynamics and stochastic gradient fisher scoring , we quantify the approximation errors due to finite learning rates . finally ( 5 ) , we use the stochastic process perspective to give a short proof of why polyak averaging is optimal . based on this idea , we propose a scalable approximate mcmc algorithm , the averaged stochastic gradient sampler .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "feature-distributed svrg for high-dimensional linear classification", "abstract": "linear classification has been widely used in many high-dimensional applications like text classification . to perform linear classification for large-scale tasks , we often need to design distributed learning methods on a cluster of multiple machines . in this paper , we propose a new distributed learning method , called feature-distributed stochastic variance reduced gradient ( fd-svrg ) for high-dimensional linear classification . unlike most existing distributed learning methods which are instance-distributed , fd-svrg is feature-distributed . fd-svrg has lower communication cost than other instance-distributed methods when the data dimensionality is larger than the number of data instances . experimental results on real data demonstrate that fd-svrg can outperform other state-of-the-art distributed methods for high-dimensional linear classification in terms of both communication cost and wall-clock time , when the dimensionality is larger than the number of instances in training data .", "topics": ["test set"]}
{"title": "reified unit resolution and the failed literal rule", "abstract": "unit resolution can simplify a cnf formula or detect an inconsistency by repeatedly assign the variables occurring in unit clauses . given any cnf formula sigma , we show that there exists a satisfiable cnf formula psi with size polynomially related to the size of sigma such that applying unit resolution to psi simulates all the effects of applying it to sigma . the formula psi is said to be the reified counterpart of sigma . this approach can be used to prove that the failed literal rule , which is an inference rule used by some sat solvers , can be entirely simulated by unit resolution . more generally , it sheds new light on the expressive power of unit resolution .", "topics": ["simulation"]}
{"title": "deterministic autopoietic automata", "abstract": "this paper studies two issues related to the paper on computing by self-reproduction : autopoietic automata by jiri wiedermann . it is shown that all results presented there extend to deterministic computations . in particular , nondeterminism is not needed for a lineage to generate all autopoietic automata .", "topics": ["computation"]}
{"title": "bandit models of human behavior : reward processing in mental disorders", "abstract": "drawing an inspiration from behavioral studies of human decision making , we propose here a general parametric framework for multi-armed bandit problem , which extends the standard thompson sampling approach to incorporate reward processing biases associated with several neurological and psychiatric conditions , including parkinson 's and alzheimer 's diseases , attention-deficit/hyperactivity disorder ( adhd ) , addiction , and chronic pain . we demonstrate empirically that the proposed parametric approach can often outperform the baseline thompson sampling on a variety of datasets . moreover , from the behavioral modeling perspective , our parametric framework can be viewed as a first step towards a unifying computational model capturing reward processing abnormalities across multiple mental conditions .", "topics": ["baseline ( configuration management )"]}
{"title": "a sequential matching framework for multi-turn response selection in retrieval-based chatbots", "abstract": "we study the problem of response selection for multi-turn conversation in retrieval-based chatbots . the task requires matching a response candidate with a conversation context , whose challenges include how to recognize important parts of the context , and how to model the relationships among utterances in the context . existing matching methods may lose important information in contexts as we can interpret them with a unified framework in which contexts are transformed to fixed-length vectors without any interaction with responses before matching . the analysis motivates us to propose a new matching framework that can sufficiently carry the important information in contexts to matching and model the relationships among utterances at the same time . the new framework , which we call a sequential matching framework ( smf ) , lets each utterance in a context interacts with a response candidate at the first step and transforms the pair to a matching vector . the matching vectors are then accumulated following the order of the utterances in the context with a recurrent neural network ( rnn ) which models the relationships among the utterances . the context-response matching is finally calculated with the hidden states of the rnn . under smf , we propose a sequential convolutional network and sequential attention network and conduct experiments on two public data sets to test their performance . experimental results show that both models can significantly outperform the state-of-the-art matching methods . we also show that the models are interpretable with visualizations that provide us insights on how they capture and leverage the important information in contexts for matching .", "topics": ["recurrent neural network"]}
{"title": "off-line handwritten signature retrieval using curvelet transforms", "abstract": "in this paper , a new method for offline handwritten signature retrieval is based on curvelet transform is proposed . many applications in image processing require similarity retrieval of an image from a large collection of images . in such cases , image indexing becomes important for efficient organization and retrieval of images . this paper addresses this issue in the context of a database of handwritten signature images and describes a system for similarity retrieval . the proposed system uses a curvelet based texture features extraction . the performance of the system has been tested with an image database of 180 signatures . the results obtained indicate that the proposed system is able to identify signatures with great with accuracy even when a part of a signature is missing .", "topics": ["image processing"]}
{"title": "beyond pixels : leveraging geometry and shape cues for online multi-object tracking", "abstract": "this paper introduces geometry and object shape and pose costs for multi-object tracking in urban driving scenarios . using images from a monocular camera alone , we devise pairwise costs for object tracks , based on several 3d cues such as object pose , shape , and motion . the proposed costs are agnostic to the data association method and can be incorporated into any optimization framework to output the pairwise data associations . these costs are easy to implement , can be computed in real-time , and complement each other to account for possible errors in a tracking-by-detection framework . we perform an extensive analysis of the designed costs and empirically demonstrate consistent improvement over the state-of-the-art under varying conditions that employ a range of object detectors , exhibit a variety in camera and object motions , and , more importantly , are not reliant on the choice of the association framework . we also show that , by using the simplest of associations frameworks ( two-frame hungarian assignment ) , we surpass the state-of-the-art in multi-object-tracking on road scenes . more qualitative and quantitative results can be found at the following url : https : //junaidcs032.github.io/geometry_objectshape_mot/ .", "topics": ["pixel"]}
{"title": "transcut : transparent object segmentation from a light-field image", "abstract": "the segmentation of transparent objects can be very useful in computer vision applications . however , because they borrow texture from their background and have a similar appearance to their surroundings , transparent objects are not handled well by regular image segmentation methods . we propose a method that overcomes these problems using the consistency and distortion properties of a light-field image . graph-cut optimization is applied for the pixel labeling problem . the light-field linearity is used to estimate the likelihood of a pixel belonging to the transparent object or lambertian background , and the occlusion detector is used to find the occlusion boundary . we acquire a light field dataset for the transparent object , and use this dataset to evaluate our method . the results demonstrate that the proposed method successfully segments transparent objects from the background .", "topics": ["image segmentation", "computer vision"]}
{"title": "recommendation with k-anonymized ratings", "abstract": "recommender systems are widely used to predict personalized preferences of goods or services using users ' past activities , such as item ratings or purchase histories . if collections of such personal activities were made publicly available , they could be used to personalize a diverse range of services , including targeted advertisement or recommendations . however , there would be an accompanying risk of privacy violations . the pioneering work of narayanan et al.\\ demonstrated that even if the identifiers are eliminated , the public release of user ratings can allow for the identification of users by those who have only a small amount of data on the users ' past ratings . in this paper , we assume the following setting . a collector collects user ratings , then anonymizes and distributes them . a recommender constructs a recommender system based on the anonymized ratings provided by the collector . based on this setting , we exhaustively list the models of recommender systems that use anonymized ratings . for each model , we then present an item-based collaborative filtering algorithm for making recommendations based on anonymized ratings . our experimental results show that an item-based collaborative filtering based on anonymized ratings can perform better than collaborative filterings based on 5 -- 10 non-anonymized ratings . this surprising result indicates that , in some settings , privacy protection does not necessarily reduce the usefulness of recommendations . from the experimental analysis of this counterintuitive result , we observed that the sparsity of the ratings can be reduced by anonymization and the variance of the prediction can be reduced if $ k $ , the anonymization parameter , is appropriately tuned . in this way , the predictive performance of recommendations based on anonymized ratings can be improved in some settings .", "topics": ["sparse matrix"]}
{"title": "deepfm : a factorization-machine based neural network for ctr prediction", "abstract": "learning sophisticated feature interactions behind user behaviors is critical in maximizing ctr for recommender systems . despite great progress , existing methods seem to have a strong bias towards low- or high-order interactions , or require expertise feature engineering . in this paper , we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions . the proposed model , deepfm , combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture . compared to the latest wide \\ & deep model from google , deepfm has a shared input to its `` wide '' and `` deep '' parts , with no need of feature engineering besides raw features . comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of deepfm over the existing models for ctr prediction , on both benchmark data and commercial data .", "topics": ["feature learning", "interaction"]}
{"title": "distributed gaussian processes", "abstract": "to scale gaussian processes ( gps ) to large data sets we introduce the robust bayesian committee machine ( rbcm ) , a practical and scalable product-of-experts model for large-scale distributed gp regression . unlike state-of-the-art sparse gp approximations , the rbcm is conceptually simple and does not rely on inducing or variational parameters . the key idea is to recursively distribute computations to independent computational units and , subsequently , recombine them to form an overall result . efficient closed-form inference allows for straightforward parallelisation and distributed computations with a small memory footprint . the rbcm is independent of the computational graph and can be used on heterogeneous computing infrastructures , ranging from laptops to clusters . with sufficient computing resources our distributed gp model can handle arbitrarily large data sets .", "topics": ["calculus of variations", "computation"]}
{"title": "high order recurrent neural networks for acoustic modelling", "abstract": "vanishing long-term gradients are a major issue in training standard recurrent neural networks ( rnns ) , which can be alleviated by long short-term memory ( lstm ) models with memory cells . however , the extra parameters associated with the memory cells mean an lstm layer has four times as many parameters as an rnn with the same hidden vector size . this paper addresses the vanishing gradient problem using a high order rnn ( hornn ) which has additional connections from multiple previous time steps . speech recognition experiments using british english multi-genre broadcast ( mgb3 ) data showed that the proposed hornn architectures for rectified linear unit and sigmoid activation functions reduced word error rates ( wer ) by 4.2 % and 6.3 % over the corresponding rnns , and gave similar wers to a ( projected ) lstm while using only 20 % -- 50 % of the recurrent layer parameters and computation .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "mantime : temporal expression identification and normalization in the tempeval-3 challenge", "abstract": "this paper describes a temporal expression identification and normalization system , mantime , developed for the tempeval-3 challenge . the identification phase combines the use of conditional random fields along with a post-processing identification pipeline , whereas the normalization phase is carried out using norma , an open-source rule-based temporal normalizer . we investigate the performance variation with respect to different feature types . specifically , we show that the use of wordnet-based features in the identification task negatively affects the overall performance , and that there is no statistically significant difference in using gazetteers , shallow parsing and propositional noun phrases labels on top of the morphological features . on the test data , the best run achieved 0.95 ( p ) , 0.85 ( r ) and 0.90 ( f1 ) in the identification phase . normalization accuracies are 0.84 ( type attribute ) and 0.77 ( value attribute ) . surprisingly , the use of the silver data ( alone or in addition to the gold annotated ones ) does not improve the performance .", "topics": ["parsing"]}
{"title": "high-speed real-time single-pixel microscopy based on fourier sampling", "abstract": "single-pixel cameras based on the concepts of compressed sensing ( cs ) leverage the inherent structure of images to retrieve them with far fewer measurements and operate efficiently over a significantly broader spectral range than conventional silicon-based cameras . recently , photonic time-stretch ( pts ) technique facilitates the emergence of high-speed single-pixel cameras . a significant breakthrough in imaging speed of single-pixel cameras enables observation of fast dynamic phenomena . however , according to cs theory , image reconstruction is an iterative process that consumes enormous amounts of computational time and can not be performed in real time . to address this challenge , we propose a novel single-pixel imaging technique that can produce high-quality images through rapid acquisition of their effective spatial fourier spectrum . we employ phase-shifting sinusoidal structured illumination instead of random illumination for spectrum acquisition and apply inverse fourier transform to the obtained spectrum for image restoration . we evaluate the performance of our prototype system by recognizing quick response ( qr ) codes and flow cytometric screening of cells . a frame rate of 625 khz and a compression ratio of 10 % are experimentally demonstrated in accordance with the recognition rate of the qr code . an imaging flow cytometer enabling high-content screening with an unprecedented throughput of 100,000 cells/s is also demonstrated . for real-time imaging applications , the proposed single-pixel microscope can significantly reduce the time required for image reconstruction by two orders of magnitude , which can be widely applied in industrial quality control and label-free biomedical imaging .", "topics": ["sampling ( signal processing )", "pixel"]}
{"title": "areas of attention for image captioning", "abstract": "we propose `` areas of attention '' , a novel attention-based model for automatic image captioning . our approach models the dependencies between image regions , caption words , and the state of an rnn language model , using three pairwise interactions . in contrast to previous attention-based approaches that associate image regions only to the rnn state , our method allows a direct association between caption words and image regions . during training these associations are inferred from image-level captions , akin to weakly-supervised object detector training . these associations help to improve captioning by localizing the corresponding regions during testing . we also propose and compare different ways of generating attention areas : cnn activation grids , object proposals , and spatial transformers nets applied in a convolutional fashion . spatial transformers give the best results . they allow for image specific attention areas , and can be trained jointly with the rest of the network . our attention mechanism and spatial transformer attention areas together yield state-of-the-art results on the mscoco dataset.o meaningful latent semantic structure in the generated captions .", "topics": ["supervised learning"]}
{"title": "a corpus-based approach for building semantic lexicons", "abstract": "semantic knowledge can be a great asset to natural language processing systems , but it is usually hand-coded for each application . although some semantic information is available in general-purpose knowledge bases such as wordnet and cyc , many applications require domain-specific lexicons that represent words and categories for a particular topic . in this paper , we present a corpus-based method that can be used to build semantic lexicons for specific categories . the input to the system is a small set of seed words for a category and a representative text corpus . the output is a ranked list of words that are associated with the category . a user then reviews the top-ranked words and decides which ones should be entered in the semantic lexicon . in experiments with five categories , users typically found about 60 words per category in 10-15 minutes to build a core semantic lexicon .", "topics": ["natural language processing", "text corpus"]}
{"title": "automatic classification of bengali sentences based on sense definitions present in bengali wordnet", "abstract": "based on the sense definition of words available in the bengali wordnet , an attempt is made to classify the bengali sentences automatically into different groups in accordance with their underlying senses . the input sentences are collected from 50 different categories of the bengali text corpus developed in the tdil project of the govt . of india , while information about the different senses of particular ambiguous lexical item is collected from bengali wordnet . in an experimental basis we have used naive bayes probabilistic model as a useful classifier of sentences . we have applied the algorithm over 1747 sentences that contain a particular bengali lexical item which , because of its ambiguous nature , is able to trigger different senses that render sentences in different meanings . in our experiment we have achieved around 84 % accurate result on the sense classification over the total input sentences . we have analyzed those residual sentences that did not comply with our experiment and did affect the results to note that in many cases , wrong syntactic structures and less semantic information are the main hurdles in semantic classification of sentences . the applicational relevance of this study is attested in automatic text classification , machine learning , information extraction , and word sense disambiguation .", "topics": ["text corpus", "relevance"]}
{"title": "supervised quantile normalisation", "abstract": "quantile normalisation is a popular normalisation method for data subject to unwanted variations such as images , speech , or genomic data . it applies a monotonic transformation to the feature values of each sample to ensure that after normalisation , they follow the same target distribution for each sample . choosing a `` good '' target distribution remains however largely empirical and heuristic , and is usually done independently of the subsequent analysis of normalised data . we propose instead to couple the quantile normalisation step with the subsequent analysis , and to optimise the target distribution jointly with the other parameters in the analysis . we illustrate this principle on the problem of estimating a linear model over normalised data , and show that it leads to a particular low-rank matrix regression problem that can be solved efficiently . we illustrate the potential of our method , which we term suquan , on simulated data , images and genomic data , where it outperforms standard quantile normalisation .", "topics": ["simulation", "heuristic"]}
{"title": "application of fuzzy system in segmentation of mri brain tumor", "abstract": "segmentation of images holds an important position in the area of image processing . it becomes more important whi le typically dealing with medical images where presurgery and post surgery decisions are required for the purpose of initiating and speeding up the recovery process . segmentation of 3-d tumor structures from magnetic resonance images ( mri ) is a very challenging problem due to the variability of tumor geometry and intensity patterns . level set evolution combining global smoothness with the flexibility of topology changes offers significant advantages over the conventional statistical classification followed by mathematical morphology . level set evolution with constant propagation needs to be initialized either completely inside or outside the tumor and can leak through weak or missing boundary parts . replacing the constant propagation term by a statistical force overcomes these limitations and results in a convergence to a stable solution . using mr images presenting tumors , probabilities for background and tumor regions are calculated from a pre- and post-contrast difference image and mixture modeling fit of the histogram . the whole image is used for initialization of the level set evolution to segment the tumor boundaries .", "topics": ["image processing", "statistical classification"]}
{"title": "piecewise latent variables for neural variational text processing", "abstract": "advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables , such as variational autoencoders . the hope is that such models will learn to represent rich , multi-modal latent factors in real-world data , such as natural language text . however , current models often assume simplistic priors on the latent variables - such as the uni-modal gaussian distribution - which are incapable of representing complex latent factors efficiently . to overcome this restriction , we propose the simple , but highly flexible , piecewise constant distribution . this distribution has the capacity to represent an exponential number of modes of a latent target distribution , while remaining mathematically tractable . our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and natural language generation for dialogue .", "topics": ["graphical model", "calculus of variations"]}
{"title": "offloading cognition onto cognitive technology", "abstract": "`` cognizing '' ( e.g . , thinking , understanding , and knowing ) is a mental state . systems without mental states , such as cognitive technology , can sometimes contribute to human cognition , but that does not make them cognizers . cognizers can offload some of their cognitive functions onto cognitive technology , thereby extending their performance capacity beyond the limits of their own brain power . language itself is a form of cognitive technology that allows cognizers to offload some of their cognitive functions onto the brains of other cognizers . language also extends cognizers ' individual and joint performance powers , distributing the load through interactive and collaborative cognition . reading , writing , print , telecommunications and computing further extend cognizers ' capacities . and now the web , with its network of cognizers , digital databases and software agents , all accessible anytime , anywhere , has become our 'cognitive commons , ' in which distributed cognizers and cognitive technology can interoperate globally with a speed , scope and degree of interactivity inconceivable through local individual cognition alone . and as with language , the cognitive tool par excellence , such technological changes are not merely instrumental and quantitative : they can have profound effects on how we think and encode information , on how we communicate with one another , on our mental states , and on our very nature .", "topics": ["approximation algorithm", "interaction"]}
{"title": "linxgboost : extension of xgboost to generalized local linear models", "abstract": "xgboost is often presented as the algorithm that wins every ml competition . surprisingly , this is true even though predictions are piecewise constant . this might be justified in high dimensional input spaces , but when the number of features is low , a piecewise linear model is likely to perform better . xgboost was extended into linxgboost that stores at each leaf a linear model . this extension , equivalent to piecewise regularized least-squares , is particularly attractive for regression of functions that exhibits jumps or discontinuities . those functions are notoriously hard to regress . our extension is compared to the vanilla xgboost and random forest in experiments on both synthetic and real-world data sets .", "topics": ["synthetic data"]}
{"title": "automatic extraction of protein interaction in literature", "abstract": "protein-protein interaction extraction is the key precondition of the construction of protein knowledge network , and it is very important for the research in the biomedicine . this paper extracted directional protein-protein interaction from the biological text , using the svm-based method . experiments were evaluated on the lll05 corpus with good results . the results show that dependency features are import for the protein-protein interaction extraction and features related to the interaction word are effective for the interaction direction judgment . at last , we analyzed the effects of different features and planed for the next step .", "topics": ["support vector machine"]}
{"title": "searnn : training rnns with global-local losses", "abstract": "we propose searnn , a novel training algorithm for recurrent neural networks ( rnns ) inspired by the `` learning to search '' ( l2s ) approach to structured prediction . rnns have been widely successful in structured prediction applications such as machine translation or parsing , and are commonly trained using maximum likelihood estimation ( mle ) . unfortunately , this training loss is not always an appropriate surrogate for the test error : by only maximizing the ground truth probability , it fails to exploit the wealth of information offered by structured losses . further , it introduces discrepancies between training and predicting ( such as exposure bias ) that may hurt test performance . instead , searnn leverages test-alike search space exploration to introduce global-local losses that are closer to the test error . we first demonstrate improved performance over mle on two different tasks : ocr and spelling correction . then , we propose a subsampling strategy to enable searnn to scale to large vocabulary sizes . this allows us to validate the benefits of our approach on a machine translation task .", "topics": ["recurrent neural network", "machine translation"]}
{"title": "is epicurus the father of reinforcement learning ?", "abstract": "the epicurean philosophy is commonly thought as simplistic and hedonistic . here i discuss how this is a misconception and explore its link to reinforcement learning . based on the letters of epicurus , i construct an objective function for hedonism which turns out to be equivalent of the reinforcement learning objective function when omitting the discount factor . i then discuss how plato and aristotle 's views that can be also loosely linked to reinforcement learning , as well as their weaknesses in relationship to it . finally , i emphasise the close affinity of the epicurean views and the bellman equation .", "topics": ["optimization problem", "reinforcement learning"]}
{"title": "spatio-temporal data mining : a survey of problems and methods", "abstract": "large volumes of spatio-temporal data are increasingly collected and studied in diverse domains including , climate science , social sciences , neuroscience , epidemiology , transportation , mobile health , and earth sciences . spatio-temporal data differs from relational data for which computational approaches are developed in the data mining community for multiple decades , in that both spatial and temporal attributes are available in addition to the actual measurements/attributes . the presence of these attributes introduces additional challenges that needs to be dealt with . approaches for mining spatio-temporal data have been studied for over a decade in the data mining community . in this article we present a broad survey of this relatively young field of spatio-temporal data mining . we discuss different types of spatio-temporal data and the relevant data mining questions that arise in the context of analyzing each of these datasets . based on the nature of the data mining problem studied , we classify literature on spatio-temporal data mining into six major categories : clustering , predictive learning , change detection , frequent pattern mining , anomaly detection , and relationship mining . we discuss the various forms of spatio-temporal data mining problems in each of these categories .", "topics": ["data mining", "cluster analysis"]}
{"title": "max-plus statistical leverage scores", "abstract": "the statistical leverage scores of a complex matrix $ a\\in\\mathbb { c } ^ { n\\times d } $ record the degree of alignment between col $ ( a ) $ and the coordinate axes in $ \\mathbb { c } ^n $ . these score are used in random sampling algorithms for solving certain numerical linear algebra problems . in this paper we present a max-plus algebraic analogue for statistical leverage scores . we show that max-plus statistical leverage scores can be used to calculate the exact asymptotic behavior of the conventional statistical leverage scores of a generic matrices of puiseux series and also provide a novel way to approximate the conventional statistical leverage scores of a fixed or complex matrix . the advantage of approximating a complex matrices scores with max-plus scores is that the max-plus scores can be computed very quickly . this approximation is typically accurate to within an order or magnitude and should be useful in practical problems where the true scores are known to vary widely .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "large-scale probabilistic predictors with and without guarantees of validity", "abstract": "this paper studies theoretically and empirically a method of turning machine-learning algorithms into probabilistic predictors that automatically enjoys a property of validity ( perfect calibration ) and is computationally efficient . the price to pay for perfect calibration is that these probabilistic predictors produce imprecise ( in practice , almost precise for large data sets ) probabilities . when these imprecise probabilities are merged into precise probabilities , the resulting predictors , while losing the theoretical property of perfect calibration , are consistently more accurate than the existing methods in empirical studies .", "topics": ["computational complexity theory"]}
{"title": "sensory anticipation of optical flow in mobile robotics", "abstract": "in order to anticipate dangerous events , like a collision , an agent needs to make long-term predictions . however , those are challenging due to uncertainties in internal and external variables and environment dynamics . a sensorimotor model is acquired online by the mobile robot using a state-of-the-art method that learns the optical flow distribution in images , both in space and time . the learnt model is used to anticipate the optical flow up to a given time horizon and to predict an imminent collision by using reinforcement learning . we demonstrate that multi-modal predictions reduce to simpler distributions once actions are taken into account .", "topics": ["reinforcement learning"]}
{"title": "a geometric descriptor for cell-division detection", "abstract": "we describe a method for cell-division detection based on a geometric-driven descriptor that can be represented as a 5-layers processing network , based mainly on wavelet filtering and a test for mirror symmetry between pairs of pixels . after the centroids of the descriptors are computed for a sequence of frames , the two-steps piecewise constant function that best fits the sequence of centroids determines the frame where the division occurs .", "topics": ["pixel"]}
{"title": "corpus-based learning of analogies and semantic relations", "abstract": "we present an algorithm for learning from unlabeled text , based on the vector space model ( vsm ) of information retrieval , that can solve verbal analogy questions of the kind found in the sat college entrance exam . a verbal analogy has the form a : b : : c : d , meaning `` a is to b as c is to d '' ; for example , mason : stone : : carpenter : wood . sat analogy questions provide a word pair , a : b , and the problem is to select the most analogous word pair , c : d , from a set of five choices . the vsm algorithm correctly answers 47 % of a collection of 374 college-level analogy questions ( random guessing would yield 20 % correct ; the average college-bound senior high school student answers about 57 % correctly ) . we motivate this research by applying it to a difficult problem in natural language processing , determining semantic relations in noun-modifier pairs . the problem is to classify a noun-modifier pair , such as `` laser printer '' , according to the semantic relation between the noun ( printer ) and the modifier ( laser ) . we use a supervised nearest-neighbour algorithm that assigns a class to a given noun-modifier pair by finding the most analogous noun-modifier pair in the training data . with 30 classes of semantic relations , on a collection of 600 labeled noun-modifier pairs , the learning algorithm attains an f value of 26.5 % ( random guessing : 3.3 % ) . with 5 classes of semantic relations , the f value is 43.2 % ( random : 20 % ) . the performance is state-of-the-art for both verbal analogies and noun-modifier relations .", "topics": ["test set", "natural language processing"]}
{"title": "improved image captioning via policy gradient optimization of spider", "abstract": "current image captioning methods are usually trained via ( penalized ) maximum likelihood estimation . however , the log-likelihood score of a caption does not correlate well with human assessments of quality . standard syntactic evaluation metrics , such as bleu , meteor and rouge , are also not well correlated . the newer spice and cider metrics are better correlated , but have traditionally been hard to optimize for . in this paper , we show how to use a policy gradient ( pg ) method to directly optimize a linear combination of spice and cider ( a combination we call spider ) : the spice score ensures our captions are semantically faithful to the image , while cider score ensures our captions are syntactically fluent . the pg method we propose improves on the prior mixer approach , by using monte carlo rollouts instead of mixing mle training with pg . we show empirically that our algorithm leads to easier optimization and improved results compared to mixer . finally , we show that using our pg method we can optimize any of the metrics , including the proposed spider metric which results in image captions that are strongly preferred by human raters compared to captions generated by the same model but trained to optimize mle or the coco metrics .", "topics": ["mathematical optimization", "gradient"]}
{"title": "fuzzy ontology-based sentiment analysis of transportation and city feature reviews for safe traveling", "abstract": "traffic congestion is rapidly increasing in urban areas , particularly in mega cities . to date , there exist a few sensor network based systems to address this problem . however , these techniques are not suitable enough in terms of monitoring an entire transportation system and delivering emergency services when needed . these techniques require real-time data and intelligent ways to quickly determine traffic activity from useful information . in addition , these existing systems and websites on city transportation and travel rely on rating scores for different factors ( e.g . , safety , low crime rate , cleanliness , etc . ) . these rating scores are not efficient enough to deliver precise information , whereas reviews or tweets are significant , because they help travelers and transportation administrators to know about each aspect of the city . however , it is difficult for travelers to read , and for transportation systems to process , all reviews and tweets to obtain expressive sentiments regarding the needs of the city . the optimum solution for this kind of problem is analyzing the information available on social network platforms and performing sentiment analysis . on the other hand , crisp ontology-based frameworks can not extract blurred information from tweets and reviews ; therefore , they produce inadequate results . in this regard , this paper proposes fuzzy ontology-based sentiment analysis and swrl rule-based decision-making to monitor transportation activities and to make a city- feature polarity map for travelers . this system retrieves reviews and tweets related to city features and transportation activities . the feature opinions are extracted from these retrieved data , and then fuzzy ontology is used to determine the transportation and city-feature polarity . a fuzzy ontology and an intelligent system prototype are developed by using prot\\'eg\\'e owl and java , respectively .", "topics": ["artificial intelligence"]}
{"title": "a generalized parsing framework for abstract grammars", "abstract": "this technical report presents a general framework for parsing a variety of grammar formalisms . we develop a grammar formalism , called an abstract grammar , which is general enough to represent grammars at many levels of the hierarchy , including context free grammars , minimalist grammars , and generalized context-free grammars . we then develop a single parsing framework which is capable of parsing grammars which are at least up to gcfgs on the hierarchy . our parsing framework exposes a grammar interface , so that it can parse any particular grammar formalism that can be reduced to an abstract grammar .", "topics": ["natural language", "parsing"]}
{"title": "ab initio algorithmic causal deconvolution of intertwined programs and networks by generative mechanism", "abstract": "to extract and learn representations leading to generative mechanisms from data , especially without making arbitrary decisions and biased assumptions , is a central challenge in most areas of scientific research particularly in connection to current major limitations of influential topics and methods of machine and deep learning as they have often lost sight of the model component . complex data is usually produced by interacting sources with different mechanisms . here we introduce a parameter-free model-based approach , based upon the seminal concept of algorithmic probability , that decomposes an observation and signal into its most likely algorithmic generative mechanisms . our methods use a causal calculus to infer model representations . we demonstrate the method ability to distinguish interacting mechanisms and deconvolve them , regardless of whether the objects produce strings , space-time evolution diagrams , images or networks . we numerically test and evaluate our method and find that it can disentangle observations from discrete dynamic systems , random and complex networks . we think that these causal inference techniques can contribute as key pieces of information for estimations of probability distributions complementing other more statistical-oriented techniques that otherwise lack model inference capabilities .", "topics": ["numerical analysis", "causality"]}
{"title": "past , present , future : a computational investigation of the typology of tense in 1000 languages", "abstract": "we present superpivot , an analysis method for low-resource languages that occur in a superparallel corpus , i.e . , in a corpus that contains an order of magnitude more languages than parallel corpora currently in use . we show that superpivot performs well for the crosslingual analysis of the linguistic phenomenon of tense . we produce analysis results for more than 1000 languages , conducting - to the best of our knowledge - the largest crosslingual computational study performed to date . we extend existing methodology for leveraging parallel corpora for typological analysis by overcoming a limiting assumption of earlier work : we only require that a linguistic feature is overtly marked in a few of thousands of languages as opposed to requiring that it be marked in all languages under investigation .", "topics": ["text corpus"]}
{"title": "classification of autism spectrum disorder using supervised learning of brain connectivity measures extracted from synchrostates", "abstract": "objective . the paper investigates the presence of autism using the functional brain connectivity measures derived from electro-encephalogram ( eeg ) of children during face perception tasks . approach . phase synchronized patterns from 128-channel eeg signals are obtained for typical children and children with autism spectrum disorder ( asd ) . the phase synchronized states or synchrostates temporally switch amongst themselves as an underlying process for the completion of a particular cognitive task . we used 12 subjects in each group ( asd and typical ) for analyzing their eeg while processing fearful , happy and neutral faces . the minimal and maximally occurring synchrostates for each subject are chosen for extraction of brain connectivity features , which are used for classification between these two groups of subjects . among different supervised learning techniques , we here explored the discriminant analysis and support vector machine both with polynomial kernels for the classification task . main results . the leave one out cross-validation of the classification algorithm gives 94.7 % accuracy as the best performance with corresponding sensitivity and specificity values as 85.7 % and 100 % respectively . significance . the proposed method gives high classification accuracies and outperforms other contemporary research results . the effectiveness of the proposed method for classification of autistic and typical children suggests the possibility of using it on a larger population to validate it for clinical practice .", "topics": ["supervised learning", "support vector machine"]}
{"title": "fast recurrent fully convolutional networks for direct perception in autonomous driving", "abstract": "deep convolutional neural networks ( cnns ) have been shown to perform extremely well at a variety of tasks including subtasks of autonomous driving such as image segmentation and object classification . however , networks designed for these tasks typically require vast quantities of training data and long training periods to converge . we investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive deep end-to-end neural network models that generate driving control signals directly from input images . in contrast to prior work that segments the autonomous driving task , our models take on a novel approach to the autonomous driving problem by utilizing deep and thin fully convolutional nets ( fcns ) with recurrent neural nets and low parameter counts to tackle a complex end-to-end regression task predicting both steering and acceleration commands . in addition , we include layers optimized for classification to allow the networks to implicitly learn image semantics . we show that the resulting networks use 3x fewer parameters than the most recent comparable end-to-end driving network and 500x fewer parameters than the alexnet variations and converge both faster and to lower losses while maintaining robustness against overfitting .", "topics": ["test set", "image segmentation"]}
{"title": "fully point-wise convolutional neural network for modeling statistical regularities in natural images", "abstract": "modeling statistical regularities is the problem of representing the pixel distributions in natural images , and usually applied to solve the ill-posed image processing problems . in this paper , we present an extremely efficient cnn architecture for modeling statistical regularities . our method is based on the observation that , by random sampling the pixels in natural images , we can obtain a set of pixel ensembles in which the pixel value is independent identically distributed . this leads to the idea of using 1*1 ( point-wise ) convolution kernel instead of k*k convolution kernel to learn the feature representation efficiently . accordingly , we design a novel architecture with fully point-wise convolutions to greatly reduce the model complexity while maintaining the representation ability . experiments on three applications : color constancy , image dehazing and underwater image enhancement demonstrate the superior performance of our proposed network over the existing architectures , i.e . , using 1/10-1/100 network parameters and computational cost over the state-of-the-art networks while achieving comparable accuracy . codes and models will be made publicly available .", "topics": ["sampling ( signal processing )", "image processing"]}
{"title": "reinforcement learning to adapt speech enhancement to instantaneous input signal quality", "abstract": "today , the optimal performance of existing noise-suppression algorithms , both data-driven and those based on classic statistical methods , is range bound to specific levels of instantaneous input signal-to-noise ratios . in this paper , we present a new approach to improve the adaptivity of such algorithms enabling them to perform robustly across a wide range of input signal and noise types . our methodology is based on the dynamic control of algorithmic parameters via reinforcement learning . specifically , we model the noise-suppression module as a black box , requiring no knowledge of the algorithmic mechanics except a simple feedback from the output . we utilize this feedback as the reward signal for a reinforcement-learning agent that learns a policy to adapt the algorithmic parameters for every incoming audio frame ( 16 ms of data ) . our preliminary results show that such a control mechanism can substantially increase the overall performance of the underlying noise-suppression algorithm ; 42 % and 16 % improvements in output snr and mse , respectively , when compared to no adaptivity .", "topics": ["reinforcement learning"]}
{"title": "anomaly detection and localisation using mixed graphical models", "abstract": "we propose a method that performs anomaly detection and localisation within heterogeneous data using a pairwise undirected mixed graphical model . the data are a mixture of categorical and quantitative variables , and the model is learned over a dataset that is supposed not to contain any anomaly . we then use the model over temporal data , potentially a data stream , using a version of the two-sided cusum algorithm . the proposed decision statistic is based on a conditional likelihood ratio computed for each variable given the others . our results show that this function allows to detect anomalies variable by variable , and thus to localise the variables involved in the anomalies more precisely than univariate methods based on simple marginals .", "topics": ["graphical model"]}
{"title": "exploring speech enhancement with generative adversarial networks for robust speech recognition", "abstract": "we investigate the effectiveness of generative adversarial networks ( gans ) for speech enhancement , in the context of improving noise robustness of automatic speech recognition ( asr ) systems . prior work demonstrates that gans can effectively suppress additive noise in raw waveform speech signals , improving perceptual quality metrics ; however this technique was not justified in the context of asr . in this work , we conduct a detailed study to measure the effectiveness of gans in enhancing speech contaminated by both additive and reverberant noise . motivated by recent advances in image processing , we propose operating gans on log-mel filterbank spectra instead of waveforms , which requires less computation and is more robust to reverberant noise . while gan enhancement improves the performance of a clean-trained asr system on noisy speech , it falls short of the performance achieved by conventional multi-style training ( mtr ) . by appending the gan-enhanced features to the noisy inputs and retraining , we achieve a 7 % wer improvement relative to the mtr system .", "topics": ["image processing", "speech recognition"]}
{"title": "generating different story tellings from semantic representations of narrative", "abstract": "in order to tell stories in different voices for different audiences , interactive story systems require : ( 1 ) a semantic representation of story structure , and ( 2 ) the ability to automatically generate story and dialogue from this semantic representation using some form of natural language generation ( nlg ) . however , there has been limited research on methods for linking story structures to narrative descriptions of scenes and story events . in this paper we present an automatic method for converting from scheherazade 's story intention graph , a semantic representation , to the input required by the personage nlg engine . using 36 aesop fables distributed in dramabank , a collection of story encodings , we train translation rules on one story and then test these rules by generating text for the remaining 35 . the results are measured in terms of the string similarity metrics levenshtein distance and bleu score . the results show that we can generate the 35 stories with correct content : the test set stories on average are close to the output of the scheherazade realizer , which was customized to this semantic representation . we provide some examples of story variations generated by personage . in future work , we will experiment with measuring the quality of the same stories generated in different voices , and with techniques for making storytelling interactive .", "topics": ["test set", "natural language"]}
{"title": "algorithmically probable mutations reproduce aspects of evolution such as convergence rate , genetic memory , modularity , diversity explosions , and mass extinction", "abstract": "natural selection explains how life has evolved over millions of years from more primitive forms . the speed at which this happens , however , has sometimes defied explanations based on random ( uniformly distributed ) mutations . here we investigate the application of algorithmic mutations ( no recombination ) to binary matrices drawn from numerical approximations to algorithmic probability in order to compare evolutionary convergence rates against the null hypothesis ( uniformly distributed mutations ) . results both on synthetic and a small biological examples lead to an accelerated rate of convergence when using the algorithmic probability . we also show that algorithmically evolved modularity provides an advantage that produces a genetic memory . we demonstrate that regular structures are preserved and carried on when they first occur and can lead to an accelerated production of diversity and extinction , possibly explaining naturally occurring phenomena such as diversity explosions ( e.g . the cambrian ) and massive extinctions ( e.g . the end triassic ) whose causes have eluded researchers and are a cause for debate . the approach introduced here appears to be a better approximation to biological evolution than models based exclusively upon random uniform mutations , and it also approaches better a formal version of open-ended evolution based on previous results . the results validate the motivations and results of chaitin 's metabiology programme and previous suggestions that computation may be an equally important driver of evolution together , and even before , the action and result of natural selection . we also show that inducing the method on problems of optimization , such as genetic algorithms , has the potential to accelerate convergence of artificial evolutionary algorithms .", "topics": ["numerical analysis", "synthetic data"]}
{"title": "computing dialogue acts from features with transformation-based learning", "abstract": "to interpret natural language at the discourse level , it is very useful to accurately recognize dialogue acts , such as suggest , in identifying speaker intentions . our research explores the utility of a machine learning method called transformation-based learning ( tbl ) in computing dialogue acts , because tbl has a number of advantages over alternative approaches for this application . we have identified some extensions to tbl that are necessary in order to address the limitations of the original algorithm and the particular demands of discourse processing . we use a monte carlo strategy to increase the applicability of the tbl method , and we select features of utterances that can be used as input to improve the performance of tbl . our system is currently being tested on the verbmobil corpora of spoken dialogues , producing promising preliminary results .", "topics": ["natural language", "text corpus"]}
{"title": "efficient constrained regret minimization", "abstract": "online learning constitutes a mathematical and compelling framework to analyze sequential decision making problems in adversarial environments . the learner repeatedly chooses an action , the environment responds with an outcome , and then the learner receives a reward for the played action . the goal of the learner is to maximize his total reward . however , there are situations in which , in addition to maximizing the cumulative reward , there are some additional constraints on the sequence of decisions that must be satisfied on average by the learner . in this paper we study an extension to the online learning where the learner aims to maximize the total reward given that some additional constraints need to be satisfied . by leveraging on the theory of lagrangian method in constrained optimization , we propose lagrangian exponentially weighted average ( lewa ) algorithm , which is a primal-dual variant of the well known exponentially weighted average algorithm , to efficiently solve constrained online decision making problems . using novel theoretical analysis , we establish the regret and the violation of the constraint bounds in full information and bandit feedback models .", "topics": ["regret ( decision theory )"]}
{"title": "machine learning at scale", "abstract": "it takes skill to build a meaningful predictive model even with the abundance of implementations of modern machine learning algorithms and readily available computing resources . building a model becomes challenging if hundreds of terabytes of data need to be processed to produce the training data set . in a digital advertising technology setting , we are faced with the need to build thousands of such models that predict user behavior and power advertising campaigns in a 24/7 chaotic real-time production environment . as data scientists , we also have to convince other internal departments critical to implementation success , our management , and our customers that our machine learning system works . in this paper , we present the details of the design and implementation of an automated , robust machine learning platform that impacts billions of advertising impressions monthly . this platform enables us to continuously optimize thousands of campaigns over hundreds of millions of users , on multiple continents , against varying performance objectives .", "topics": ["test set"]}
{"title": "a new type-ii fuzzy logic based controller for non-linear dynamical systems with application to a 3-psp parallel robot", "abstract": "the concept of uncertainty is posed in almost any complex system including parallel robots as an outstanding instance of dynamical robotics systems . as suggested by the name , uncertainty , is some missing information that is beyond the knowledge of human thus we may tend to handle it properly to minimize the side-effects through the control process . type-ii fuzzy logic has shown its superiority over traditional fuzzy logic when dealing with uncertainty . type-ii fuzzy logic controllers are however newer and more promising approaches that have been recently applied to various fields due to their significant contribution especially when noise ( as an important instance of uncertainty ) emerges . during the design of type-i fuzzy logic systems , we presume that we are almost certain about the fuzzy membership functions which is not true in many cases . thus t2fls as a more realistic approach dealing with practical applications might have a lot to offer . type-ii fuzzy logic takes into account a higher level of uncertainty , in other words , the membership grade for a type-ii fuzzy variable is no longer a crisp number but rather is itself a type-i linguistic term . in this thesis the effects of uncertainty in dynamic control of a parallel robot is considered . more specifically , it is intended to incorporate the type-ii fuzzy logic paradigm into a model based controller , the so-called computed torque control method , and apply the result to a 3 degrees of freedom parallel manipulator . ...", "topics": ["computational complexity theory", "computation"]}
{"title": "analysis of the entropy-guided switching trimmed mean deviation-based anisotropic diffusion filter", "abstract": "this report describes the experimental analysis of a proposed switching filter-anisotropic diffusion hybrid for the filtering of the fixed value ( salt and pepper ) impulse noise ( fvin ) . the filter works well at both low and high noise densities though it was specifically designed for high noise density levels . the filter combines the switching mechanism of decision-based filters and the partial differential equation-based formulation to yield a powerful system capable of recovering the image signals at very high noise levels . experimental results indicate that the filter surpasses other filters , especially at very high noise levels . additionally , its adaptive nature ensures that the performance is guided by the metrics obtained from the noisy input image . the filter algorithm is of both global and local nature , where the former is chosen to reduce computation time and complexity , while the latter is used for best results .", "topics": ["time complexity", "computation"]}
{"title": "empirical analysis of predictive algorithms for collaborative filtering", "abstract": "collaborative filtering or recommender systems use a database about user preferences to predict additional topics or products a new user might like . in this paper we describe several algorithms designed for this task , including techniques based on correlation coefficients , vector-based similarity calculations , and statistical bayesian methods . we compare the predictive accuracy of the various methods in a set of representative problem domains . we use two basic classes of evaluation metrics . the first characterizes accuracy over a set of individual predictions in terms of average absolute deviation . the second estimates the utility of a ranked list of suggested items . this metric uses an estimate of the probability that a user will see a recommendation in an ordered list . experiments were run for datasets associated with 3 application areas , 4 experimental protocols , and the 2 evaluation metrics for the various algorithms . results indicate that for a wide range of conditions , bayesian networks with decision trees at each node and correlation methods outperform bayesian-clustering and vector-similarity methods . between correlation and bayesian networks , the preferred method depends on the nature of the dataset , nature of the application ( ranked versus one-by-one presentation ) , and the availability of votes with which to make predictions . other considerations include the size of database , speed of predictions , and learning time .", "topics": ["cluster analysis", "bayesian network"]}
{"title": "reinforcement learning : a survey", "abstract": "this paper surveys the field of reinforcement learning from a computer-science perspective . it is written to be accessible to researchers familiar with machine learning . both the historical basis of the field and a broad selection of current work are summarized . reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment . the work described here has a resemblance to work in psychology , but differs considerably in the details and in the use of the word `` reinforcement . '' the paper discusses central issues of reinforcement learning , including trading off exploration and exploitation , establishing the foundations of the field via markov decision theory , learning from delayed reinforcement , constructing empirical models to accelerate learning , making use of generalization and hierarchy , and coping with hidden state . it concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning .", "topics": ["reinforcement learning", "interaction"]}
{"title": "roi segmentation for feature extraction from human facial images", "abstract": "human computer interaction ( hci ) is the biggest goal of computer vision researchers . features form the different facial images are able to provide a very deep knowledge about the activities performed by the different facial movements . in this paper we presented a technique for feature extraction from various regions of interest with the help of skin color segmentation technique , thresholding , knowledge based technique for face recognition .", "topics": ["image segmentation", "feature extraction"]}
{"title": "gibbs sampling in open-universe stochastic languages", "abstract": "languages for open-universe probabilistic models ( oupms ) can represent situations with an unknown number of objects and iden- tity uncertainty . while such cases arise in a wide range of important real-world appli- cations , existing general purpose inference methods for oupms are far less efficient than those available for more restricted lan- guages and model classes . this paper goes some way to remedying this deficit by in- troducing , and proving correct , a generaliza- tion of gibbs sampling to partial worlds with possibly varying model structure . our ap- proach draws on and extends previous generic oupm inference methods , as well as aux- iliary variable samplers for nonparametric mixture models . it has been implemented for blog , a well-known oupm language . combined with compile-time optimizations , the resulting algorithm yields very substan- tial speedups over existing methods on sev- eral test cases , and substantially improves the practicality of oupm languages generally .", "topics": ["sampling ( signal processing )"]}
{"title": "barista - a graphical tool for designing and training deep neural networks", "abstract": "in recent years , the importance of deep learning has significantly increased in pattern recognition , computer vision , and artificial intelligence research , as well as in industry . however , despite the existence of multiple deep learning frameworks , there is a lack of comprehensible and easy-to-use high-level tools for the design , training , and testing of deep neural networks ( dnns ) . in this paper , we introduce barista , an open-source graphical high-level interface for the caffe deep learning framework . while caffe is one of the most popular frameworks for training dnns , editing prototext files in order to specify the net architecture and hyper parameters can become a cumbersome and error-prone task . instead , barista offers a fully graphical user interface with a graph-based net topology editor and provides an end-to-end training facility for dnns , which allows researchers to focus on solving their problems without having to write code , edit text files , or manually parse logged data .", "topics": ["high- and low-level", "neural networks"]}
{"title": "multilingual knowledge graph embeddings for cross-lingual knowledge alignment", "abstract": "many recent works have demonstrated the benefits of knowledge graph embeddings in completing monolingual knowledge graphs . inasmuch as related knowledge bases are built in several different languages , achieving cross-lingual knowledge alignment will help people in constructing a coherent knowledge base , and assist machines in dealing with different expressions of entity relationships across diverse human languages . unfortunately , achieving this highly desirable crosslingual alignment by human labor is very costly and errorprone . thus , we propose mtranse , a translation-based model for multilingual knowledge graph embeddings , to provide a simple and automated solution . by encoding entities and relations of each language in a separated embedding space , mtranse provides transitions for each embedding vector to its cross-lingual counterparts in other spaces , while preserving the functionalities of monolingual embeddings . we deploy three different techniques to represent cross-lingual transitions , namely axis calibration , translation vectors , and linear transformations , and derive five variants for mtranse using different loss functions . our models can be trained on partially aligned graphs , where just a small portion of triples are aligned with their cross-lingual counterparts . the experiments on cross-lingual entity matching and triple-wise alignment verification show promising results , with some variants consistently outperforming others on different tasks . we also explore how mtranse preserves the key properties of its monolingual counterpart transe .", "topics": ["entity", "loss function"]}
{"title": "long-term ensemble learning of visual place classifiers", "abstract": "this paper addresses the problem of cross-season visual place classification ( vpc ) from a novel perspective of long-term map learning . our goal is to enable transfer learning efficiently from one season to the next , at a small constant cost , and without wasting the robot 's available long-term-memory by memorizing very large amounts of training data . to realize a good tradeoff between generalization and specialization abilities , we employ an ensemble of convolutional neural network ( dcn ) classifiers and consider the task of scheduling ( when and which classifiers to retrain ) , given a previous season 's dcn classifiers as the sole prior knowledge . we present a unified framework for retraining scheduling and discuss practical implementation strategies . furthermore , we address the task of partitioning a robot 's workspace into places to define place classes in an unsupervised manner , rather than using uniform partitioning , so as to maximize vpc performance . experiments using the publicly available nclt dataset revealed that retraining scheduling of a dcn classifier ensemble is crucial and performance is significantly increased by using planned scheduling .", "topics": ["test set"]}
{"title": "a hybrid both filter and wrapper feature selection method for microarray classification", "abstract": "gene expression data is widely used in disease analysis and cancer diagnosis . however , since gene expression data could contain thousands of genes simultaneously , successful microarray classification is rather difficult . feature selection is an important pre-treatment for any classification process . selecting a useful gene subset as a classifier not only decreases the computational time and cost , but also increases classification accuracy . in this study , we applied the information gain method as a filter approach , and an improved binary particle swarm optimization as a wrapper approach to implement feature selection ; selected gene subsets were used to evaluate the performance of classification . experimental results show that by employing the proposed method fewer gene subsets needed to be selected and better classification accuracy could be obtained .", "topics": ["time complexity"]}
{"title": "an adaptive matrix factorization approach for personalized recommender systems", "abstract": "given a set $ u $ of users and a set of items $ i $ , a dataset of recommendations can be viewed as a sparse rectangular matrix $ a $ of size $ |u|\\times |i| $ such that $ a_ { u , i } $ contains the rating the user $ u $ assigns to item $ i $ , $ a_ { u , i } = ? $ if the user $ u $ has not rated the item $ i $ . the goal of a recommender system is to predict replacements to the missing observations $ ? $ in $ a $ in order to make personalized recommendations meeting the user 's tastes . a promising approach is the one based on the low-rank nonnegative matrix factorization of $ a $ where items and users are represented in terms of a few vectors . these vector can be used for estimating the missing evaluations and to produce new recommendations . in this paper we propose an algorithm based on the nonnegative matrix factorization approach for predicting the missing entries . numerical test have been performed to estimate the accuracy in predicting the missing entries and in the recommendations provided and we have compared our technique with others in the literature . we have tested the algorithm on the movielens databases containing ratings of users on movies .", "topics": ["numerical analysis", "sparse matrix"]}
{"title": "diffusion nets", "abstract": "non-linear manifold learning enables high-dimensional data analysis , but requires out-of-sample-extension methods to process new data points . in this paper , we propose a manifold learning algorithm based on deep learning to create an encoder , which maps a high-dimensional dataset and its low-dimensional embedding , and a decoder , which takes the embedded data back to the high-dimensional space . stacking the encoder and decoder together constructs an autoencoder , which we term a diffusion net , that performs out-of-sample-extension as well as outlier detection . we introduce new neural net constraints for the encoder , which preserves the local geometry of the points , and we prove rates of convergence for the encoder . also , our approach is efficient in both computational complexity and memory requirements , as opposed to previous methods that require storage of all training points in both the high-dimensional and the low-dimensional spaces to calculate the out-of-sample-extension and the pre-image .", "topics": ["computational complexity theory", "map"]}
{"title": "efficient two-dimensional sparse coding using tensor-linear combination", "abstract": "sparse coding ( sc ) is an automatic feature extraction and selection technique that is widely used in unsupervised learning . however , conventional sc vectorizes the input images , which breaks apart the local proximity of pixels and destructs the elementary object structures of images . in this paper , we propose a novel two-dimensional sparse coding ( 2dsc ) scheme that represents the input images as the tensor-linear combinations under a novel algebraic framework . 2dsc learns much more concise dictionaries because it uses the circular convolution operator , since the shifted versions of atoms learned by conventional sc are treated as the same ones . we apply 2dsc to natural images and demonstrate that 2dsc returns meaningful dictionaries for large patches . moreover , for mutli-spectral images denoising , the proposed 2dsc reduces computational costs with competitive performance in comparison with the state-of-the-art algorithms .", "topics": ["feature extraction", "unsupervised learning"]}
{"title": "3d point cloud classification and segmentation using 3d modified fisher vector representation for convolutional neural networks", "abstract": "the point cloud is gaining prominence as a method for representing 3d shapes , but its irregular format poses a challenge for deep learning methods . the common solution of transforming the data into a 3d voxel grid introduces its own challenges , mainly large memory size . in this paper we propose a novel 3d point cloud representation called 3d modified fisher vectors ( 3dmfv ) . our representation is hybrid as it combines the discrete structure of a grid with continuous generalization of fisher vectors , in a compact and computationally efficient way . using the grid enables us to design a new cnn architecture for point cloud classification and part segmentation . in a series of experiments we demonstrate competitive performance or even better than state-of-the-art on challenging benchmark datasets .", "topics": ["computational complexity theory"]}
{"title": "spike synchronization dynamics of small-world networks", "abstract": "in this research report , we examine the effects of small-world network organization on spike synchronization dynamics in networks of izhikevich spiking units . we interpolate network organizations from regular ring lattices , through the small-world region , to random networks , and measure global spike synchronization dynamics . we examine how average path length and clustering effect the dynamics of global and neighborhood clique spike organization and propagation . we show that the emergence of global synchronization undergoes a phase transition in the small-world region , between the clustering and path length phase transitions that are known to exist . we add additional realistic constraints on the dynamics by introducing propagation delays of spiking signals proportional to wiring length . the addition of delays interferes with the ability of random networks to sustain global synchronization , in relation to the breakdown of clustering in the networks . the addition of delays further enhances the finding that small-world organization is beneficial for balancing neighborhood synchronized waves of organization with global synchronization dynamics .", "topics": ["cluster analysis"]}
{"title": "evaluation of croatian word embeddings", "abstract": "croatian is poorly resourced and highly inflected language from slavic language family . nowadays , research is focusing mostly on english . we created a new word analogy corpus based on the original english word2vec word analogy corpus and added some of the specific linguistic aspects from croatian language . next , we created croatian wordsim353 and rg65 corpora for a basic evaluation of word similarities . we compared created corpora on two popular word representation models , based on word2vec tool and fasttext tool . models has been trained on 1.37b tokens training data corpus and tested on a new robust croatian word analogy corpus . results show that models are able to create meaningful word representation . this research has shown that free word order and the higher morphological complexity of croatian language influences the quality of resulting word embeddings .", "topics": ["test set", "text corpus"]}
{"title": "weight initialization of deep neural networks ( dnns ) using data statistics", "abstract": "deep neural networks ( dnns ) form the backbone of almost every state-of-the-art technique in the fields such as computer vision , speech processing , and text analysis . the recent advances in computational technology have made the use of dnns more practical . despite the overwhelming performances by dnn and the advances in computational technology , it is seen that very few researchers try to train their models from the scratch . training of dnns still remains a difficult and tedious job . the main challenges that researchers face during training of dnns are the vanishing/exploding gradient problem and the highly non-convex nature of the objective function which has up to million variables . the approaches suggested in he and xavier solve the vanishing gradient problem by providing a sophisticated initialization technique . these approaches have been quite effective and have achieved good results on standard datasets , but these same approaches do not work very well on more practical datasets . we think the reason for this is not making use of data statistics for initializing the network weights . optimizing such a high dimensional loss function requires careful initialization of network weights . in this work , we propose a data dependent initialization and analyze its performance against the standard initialization techniques such as he and xavier . we performed our experiments on some practical datasets and the results show our algorithm 's superior classification accuracy .", "topics": ["optimization problem", "computer vision"]}
{"title": "a particle swarm optimization-based flexible convolutional auto-encoder for image classification", "abstract": "convolutional auto-encoders have shown their remarkable performance in stacking to deep convolutional neural networks for classifying image data during past several years . however , they are unable to construct the state-of-the-art convolutional neural networks due to their intrinsic architectures . in this regard , we propose a flexible convolutional auto-encoder by eliminating the constraints on the numbers of convolutional layers and pooling layers from the traditional convolutional auto-encoder . we also design an architecture discovery method by using particle swarm optimization , which is capable of automatically searching for the optimal architectures of the proposed flexible convolutional auto-encoder with much less computational resource and without any manual intervention . we use the designed architecture optimization algorithm to test the proposed flexible convolutional auto-encoder through utilizing one graphic processing unit card on four extensively used image classification datasets . experimental results show that our work in this paper significantly outperform the peer competitors including the state-of-the-art algorithm .", "topics": ["computer vision", "encoder"]}
{"title": "recognizing objects in-the-wild : where do we stand ?", "abstract": "the ability to recognize objects is an essential skill for a robotic system acting in human-populated environments . despite decades of effort from the robotic and vision research communities , robots are still missing good visual perceptual systems , preventing the use of autonomous agents for real-world applications . the progress is slowed down by the lack of a testbed able to accurately represent the world perceived by the robot in-the-wild . in order to fill this gap , we introduce a large-scale , multi-view object dataset collected with an rgb-d camera mounted on a mobile robot . the dataset embeds the challenges faced by a robot in a real-life application and provides a useful tool for validating object recognition algorithms . besides describing the characteristics of the dataset , the paper evaluates the performance of a collection of well-established deep convolutional networks on the new dataset and analyzes the transferability of deep representations from web images to robotic data . despite the promising results obtained with such representations , the experiments demonstrate that object classification with real-life robotic data is far from being solved . finally , we provide a comparative study to analyze and highlight the open challenges in robot vision , explaining the discrepancies in the performance .", "topics": ["autonomous car", "robot"]}
{"title": "finding density functionals with machine learning", "abstract": "machine learning is used to approximate density functionals . for the model problem of the kinetic energy of non-interacting fermions in 1d , mean absolute errors below 1 kcal/mol on test densities similar to the training set are reached with fewer than 100 training densities . a predictor identifies if a test density is within the interpolation region . via principal component analysis , a projected functional derivative finds highly accurate self-consistent densities . challenges for application of our method to real electronic structure problems are discussed .", "topics": ["test set"]}
{"title": "deep regression bayesian network and its applications", "abstract": "deep directed generative models have attracted much attention recently due to their generative modeling nature and powerful data representation ability . in this paper , we review different structures of deep directed generative models and the learning and inference algorithms associated with the structures . we focus on a specific structure that consists of layers of bayesian networks due to the property of capturing inherent and rich dependencies among latent variables . the major difficulty of learning and inference with deep directed models with many latent variables is the intractable inference due to the dependencies among the latent variables and the exponential number of latent variable configurations . current solutions use variational methods often through an auxiliary network to approximate the posterior probability inference . in contrast , inference can also be performed directly without using any auxiliary network to maximally preserve the dependencies among the latent variables . specifically , by exploiting the sparse representation with the latent space , max-max instead of max-sum operation can be used to overcome the exponential number of latent configurations . furthermore , the max-max operation and augmented coordinate ascent are applied to both supervised and unsupervised learning as well as to various inference . quantitative evaluations on benchmark datasets of different models are given for both data representation and feature learning tasks .", "topics": ["generative model", "feature learning"]}
{"title": "variational latent gaussian process for recovering single-trial dynamics from population spike trains", "abstract": "when governed by underlying low-dimensional dynamics , the interdependence of simultaneously recorded population of neurons can be explained by a small number of shared factors , or a low-dimensional trajectory . recovering these latent trajectories , particularly from single-trial population recordings , may help us understand the dynamics that drive neural computation . however , due to the biophysical constraints and noise in the spike trains , inferring trajectories from data is a challenging statistical problem in general . here , we propose a practical and efficient inference method , called the variational latent gaussian process ( vlgp ) . the vlgp combines a generative model with a history-dependent point process observation together with a smoothness prior on the latent trajectories . the vlgp improves upon earlier methods for recovering latent trajectories , which assume either observation models inappropriate for point processes or linear dynamics . we compare and validate vlgp on both simulated datasets and population recordings from the primary visual cortex . in the v1 dataset , we find that vlgp achieves substantially higher performance than previous methods for predicting omitted spike trains , as well as capturing both the toroidal topology of visual stimuli space , and the noise-correlation . these results show that vlgp is a robust method with a potential to reveal hidden neural dynamics from large-scale neural recordings .", "topics": ["generative model", "calculus of variations"]}
{"title": "a theory of interpretive clustering in free recall", "abstract": "a stochastic model of short-term verbal memory is proposed , in which the psychological state of the subject is encoded as the instantaneous position of a particle diffusing over a semantic graph with a probabilistic structure . the model is particularly suitable for studying the dependence of free-recall observables on semantic properties of the words to be recalled . besides predicting some well-known experimental features ( contiguity effect , forward asymmetry , word-length effect ) , a novel prediction is obtained on the relationship between the contiguity effect and the syllabic length of words ; shorter words , by way of their wider semantic range , are predicted to be characterized by stronger forward contiguity . a fresh analysis of archival data allows to confirm this prediction .", "topics": ["cluster analysis"]}
{"title": "towards verifiably ethical robot behaviour", "abstract": "ensuring that autonomous systems work ethically is both complex and difficult . however , the idea of having an additional `governor ' that assesses options the system has , and prunes them to select the most ethical choices is well understood . recent work has produced such a governor consisting of a `consequence engine ' that assesses the likely future outcomes of actions then applies a safety/ethical logic to select actions . although this is appealing , it is impossible to be certain that the most ethical options are actually taken . in this paper we extend and apply a well-known agent verification approach to our consequence engine , allowing us to verify the correctness of its ethical decision-making .", "topics": ["robot"]}
{"title": "a new approach to constraint weight learning for variable ordering in csps", "abstract": "a constraint satisfaction problem ( csp ) is a framework used for modeling and solving constrained problems . tree-search algorithms like backtracking try to construct a solution to a csp by selecting the variables of the problem one after another . the order in which these algorithm select the variables potentially have significant impact on the search performance . various heuristics have been proposed for choosing good variable ordering . many powerful variable ordering heuristics weigh the constraints first and then utilize the weights for selecting good order of the variables . constraint weighting are basically employed to identify global bottlenecks in a csp . in this paper , we propose a new approach for learning weights for the constraints using competitive coevolutionary genetic algorithm ( ga ) . weights learned by the coevolutionary ga later help to make better choices for the first few variables in a search . in the competitive coevolutionary ga , constraints and candidate solutions for a csp evolve together through an inverse fitness interaction process . we have conducted experiments on several random , quasi-random and patterned instances to measure the efficiency of the proposed approach . the results and analysis show that the proposed approach is good at learning weights to distinguish the hard constraints for quasi-random instances and forced satisfiable random instances generated with the model rb . for other type of instances , rndi still seems to be the best approach as our experiments show .", "topics": ["approximation algorithm", "heuristic"]}
{"title": "batch normalization : accelerating deep network training by reducing internal covariate shift", "abstract": "training deep neural networks is complicated by the fact that the distribution of each layer 's inputs changes during training , as the parameters of the previous layers change . this slows down the training by requiring lower learning rates and careful parameter initialization , and makes it notoriously hard to train models with saturating nonlinearities . we refer to this phenomenon as internal covariate shift , and address the problem by normalizing layer inputs . our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch . batch normalization allows us to use much higher learning rates and be less careful about initialization . it also acts as a regularizer , in some cases eliminating the need for dropout . applied to a state-of-the-art image classification model , batch normalization achieves the same accuracy with 14 times fewer training steps , and beats the original model by a significant margin . using an ensemble of batch-normalized networks , we improve upon the best published result on imagenet classification : reaching 4.9 % top-5 validation error ( and 4.8 % test error ) , exceeding the accuracy of human raters .", "topics": ["neural networks", "computer vision"]}
{"title": "recognizing abnormal heart sounds using deep learning", "abstract": "the work presented here applies deep learning to the task of automated cardiac auscultation , i.e . recognizing abnormalities in heart sounds . we describe an automated heart sound classification algorithm that combines the use of time-frequency heat map representations with a deep convolutional neural network ( cnn ) . given the cost-sensitive nature of misclassification , our cnn architecture is trained using a modified loss function that directly optimizes the trade-off between sensitivity and specificity . we evaluated our algorithm at the 2016 physionet computing in cardiology challenge where the objective was to accurately classify normal and abnormal heart sounds from single , short , potentially noisy recordings . our entry to the challenge achieved a final specificity of 0.95 , sensitivity of 0.73 and overall score of 0.84 . we achieved the greatest specificity score out of all challenge entries and , using just a single cnn , our algorithm differed in overall score by only 0.02 compared to the top place finisher , which used an ensemble approach .", "topics": ["loss function"]}
{"title": "parallelizing word2vec in shared and distributed memory", "abstract": "word2vec is a widely used algorithm for extracting low-dimensional vector representations of words . it generated considerable excitement in the machine learning and natural language processing ( nlp ) communities recently due to its exceptional performance in many nlp applications such as named entity recognition , sentiment analysis , machine translation and question answering . state-of-the-art algorithms including those by mikolov et al . have been parallelized for multi-core cpu architectures but are based on vector-vector operations that are memory-bandwidth intensive and do not efficiently use computational resources . in this paper , we improve reuse of various data structures in the algorithm through the use of minibatching , hence allowing us to express the problem using matrix multiply operations . we also explore different techniques to distribute word2vec computation across nodes in a compute cluster , and demonstrate good strong scalability up to 32 nodes . in combination , these techniques allow us to scale up the computation near linearly across cores and nodes , and process hundreds of millions of words per second , which is the fastest word2vec implementation to the best of our knowledge .", "topics": ["natural language processing", "machine translation"]}
{"title": "ensemble-compression : a new method for parallel training of deep neural networks", "abstract": "parallelization framework has become a necessity to speed up the training of deep neural networks ( dnn ) recently . such framework typically employs the model average approach , denoted as ma-dnn , in which parallel workers conduct respective training based on their own local data while the parameters of local models are periodically communicated and averaged to obtain a global model which serves as the new start of local models . however , since dnn is a highly non-convex model , averaging parameters can not ensure that such global model can perform better than those local models . to tackle this problem , we introduce a new parallel training framework called ensemble-compression , denoted as ec-dnn . in this framework , we propose to aggregate the local models by ensemble , i.e . , averaging the outputs of local models instead of the parameters . as most of prevalent loss functions are convex to the output of dnn , the performance of ensemble-based global model is guaranteed to be at least as good as the average performance of local models . however , a big challenge lies in the explosion of model size since each round of ensemble can give rise to multiple times size increment . thus , we carry out model compression after each ensemble , specialized by a distillation based method in this paper , to reduce the size of the global model to be the same as the local ones . our experimental results demonstrate the prominent advantage of ec-dnn over ma-dnn in terms of both accuracy and speedup .", "topics": ["loss function"]}
{"title": "optimal low-rank tensor recovery from separable measurements : four contractions suffice", "abstract": "tensors play a central role in many modern machine learning and signal processing applications . in such applications , the target tensor is usually of low rank , i.e . , can be expressed as a sum of a small number of rank one tensors . this motivates us to consider the problem of low rank tensor recovery from a class of linear measurements called separable measurements . as specific examples , we focus on two distinct types of separable measurement mechanisms ( a ) random projections , where each measurement corresponds to an inner product of the tensor with a suitable random tensor , and ( b ) the completion problem where measurements constitute revelation of a random set of entries . we present a computationally efficient algorithm , with rigorous and order-optimal sample complexity results ( upto logarithmic factors ) for tensor recovery . our method is based on reduction to matrix completion sub-problems and adaptation of leurgans ' method for tensor decomposition . we extend the methodology and sample complexity results to higher order tensors , and experimentally validate our theoretical results .", "topics": ["computational complexity theory"]}
{"title": "joint intermodal and intramodal label transfers for extremely rare or unseen classes", "abstract": "in this paper , we present a label transfer model from texts to images for image classification tasks . the problem of image classification is often much more challenging than text classification . on one hand , labeled text data is more widely available than the labeled images for classification tasks . on the other hand , text data tends to have natural semantic interpretability , and they are often more directly related to class labels . on the contrary , the image features are not directly related to concepts inherent in class labels . one of our goals in this paper is to develop a model for revealing the functional relationships between text and image features as to directly transfer intermodal and intramodal labels to annotate the images . this is implemented by learning a transfer function as a bridge to propagate the labels between two multimodal spaces . however , the intermodal label transfers could be undermined by blindly transferring the labels of noisy texts to annotate images . to mitigate this problem , we present an intramodal label transfer process , which complements the intermodal label transfer by transferring the image labels instead when relevant text is absent from the source corpus . in addition , we generalize the inter-modal label transfer to zero-shot learning scenario where there are only text examples available to label unseen classes of images without any positive image examples . we evaluate our algorithm on an image classification task and show the effectiveness with respect to the other compared algorithms .", "topics": ["computer vision", "text corpus"]}
{"title": "efficient algorithms for parsing the dop model ? a reply to joshua goodman", "abstract": "this note is a reply to joshua goodman 's paper `` efficient algorithms for parsing the dop model '' ( goodman , 1996 ; cmp-lg/9604008 ) . in his paper , goodman makes a number of claims about ( my work on ) the data-oriented parsing model ( bod , 1992-1996 ) . this note shows that some of these claims must be mistaken .", "topics": ["time complexity", "parsing"]}
{"title": "faith in the algorithm , part 1 : beyond the turing test", "abstract": "since the turing test was first proposed by alan turing in 1950 , the primary goal of artificial intelligence has been predicated on the ability for computers to imitate human behavior . however , the majority of uses for the computer can be said to fall outside the domain of human abilities and it is exactly outside of this domain where computers have demonstrated their greatest contribution to intelligence . another goal for artificial intelligence is one that is not predicated on human mimicry , but instead , on human amplification . this article surveys various systems that contribute to the advancement of human and social intelligence .", "topics": ["artificial intelligence"]}
{"title": "truth maintenance under uncertainty", "abstract": "this paper addresses the problem of resolving errors under uncertainty in a rule-based system . a new approach has been developed that reformulates this problem as a neural-network learning problem . the strength and the fundamental limitations of this approach are explored and discussed . the main result is that neural heuristics can be applied to solve some but not all problems in rule-based systems .", "topics": ["heuristic"]}
{"title": "a statistical test for joint distributions equivalence", "abstract": "we provide a distribution-free test that can be used to determine whether any two joint distributions $ p $ and $ q $ are statistically different by inspection of a large enough set of samples . following recent efforts from long et al . [ 1 ] , we rely on joint kernel distribution embedding to extend the kernel two-sample test of gretton et al . [ 2 ] to the case of joint probability distributions . our main result can be directly applied to verify if a dataset-shift has occurred between training and test distributions in a learning framework , without further assuming the shift has occurred only in the input , in the target or in the conditional distribution .", "topics": ["kernel ( operating system )"]}
{"title": "deep reinforcement learning for improving downlink mmwave communication performance", "abstract": "we propose a method to improve the dl sinr for a single cell indoor base station operating in the millimeter wave frequency range using deep reinforcement learning . in this paper , we use the deep reinforcement learning model to arrive at optimal sequences of actions to improve the cellular network sinr value from a starting to a feasible target value . while deep reinforcement learning has been discussed extensively in literature , its applications in the cellular networks in general and in mmwave propagations are new and starting to gain attention . we have run simulations and have shown that an optimal action sequence is feasible even against the randomness of the network actions .", "topics": ["reinforcement learning", "simulation"]}
{"title": "evidential supplier selection based on interval data fusion", "abstract": "supplier selection is a typical multi-criteria decision making ( mcdm ) problem and lots of uncertain information exist inevitably . to address this issue , a new method was proposed based on interval data fusion . our method follows the original way to generate classical basic probability assignment ( bpa ) determined by the distance among the evidences . however , the weights of criteria are kept as interval numbers to generate interval bpas and do the fusion of interval bpas . finally , the order is ranked and the decision is made according to the obtained interval bpas . in this paper , a numerical example of supplier selection is applied to verify the feasibility and validity of our method . the new method is presented aiming at solving multiple-criteria decision-making problems in which the weights of criteria or experts are described in fuzzy data like linguistic terms or interval data .", "topics": ["numerical analysis"]}
{"title": "learning document-level semantic properties from free-text annotations", "abstract": "this paper presents a new method for inferring the semantic properties of documents by leveraging free-text keyphrase annotations . such annotations are becoming increasingly abundant due to the recent dramatic growth in semi-structured , user-generated online content . one especially relevant domain is product reviews , which are often annotated by their authors with pros/cons keyphrases such as a real bargain or good value . these annotations are representative of the underlying semantic properties ; however , unlike expert annotations , they are noisy : lay authors may use different labels to denote the same property , and some labels may be missing . to learn using such noisy annotations , we find a hidden paraphrase structure which clusters the keyphrases . the paraphrase structure is linked with a latent topic model of the review texts , enabling the system to predict the properties of unannotated documents and to effectively aggregate the semantic properties of multiple reviews . our approach is implemented as a hierarchical bayesian model with joint inference . we find that joint inference increases the robustness of the keyphrase clustering and encourages the latent topics to correlate with semantically meaningful properties . multiple evaluations demonstrate that our model substantially outperforms alternative approaches for summarizing single and multiple documents into a set of semantically salient keyphrases .", "topics": ["cluster analysis", "bayesian network"]}
{"title": "neural machine translation with gumbel-greedy decoding", "abstract": "previous neural machine translation models used some heuristic search algorithms ( e.g . , beam search ) in order to avoid solving the maximum a posteriori problem over translation sentences at test time . in this paper , we propose the gumbel-greedy decoding which trains a generative network to predict translation under a trained model . we solve such a problem using the gumbel-softmax reparameterization , which makes our generative network differentiable and trainable through standard stochastic gradient methods . we empirically demonstrate that our proposed model is effective for generating sequences of discrete words .", "topics": ["machine translation", "gradient"]}
{"title": "attentional pooling for action recognition", "abstract": "we introduce a simple yet surprisingly powerful model to incorporate attention in action recognition and human object interaction tasks . our proposed attention module can be trained with or without extra supervision , and gives a sizable boost in accuracy while keeping the network size and computational cost nearly the same . it leads to significant improvements over state of the art base architecture on three standard action recognition benchmarks across still images and videos , and establishes new state of the art on mpii dataset with 12.5 % relative improvement . we also perform an extensive analysis of our attention module both empirically and analytically . in terms of the latter , we introduce a novel derivation of bottom-up and top-down attention as low-rank approximations of bilinear pooling methods ( typically used for fine-grained classification ) . from this perspective , our attention formulation suggests a novel characterization of action recognition as a fine-grained recognition problem .", "topics": ["approximation"]}
{"title": "semi-supervised fusedgan for conditional image generation", "abstract": "we present fusedgan , a deep network for conditional image synthesis with controllable sampling of diverse images . fidelity , diversity and controllable sampling are the main quality measures of a good image generation model . most existing models are insufficient in all three aspects . the fusedgan can perform controllable sampling of diverse images with very high fidelity . we argue that controllability can be achieved by disentangling the generation process into various stages . in contrast to stacked gans , where multiple stages of gans are trained separately with full supervision of labeled intermediate images , the fusedgan has a single stage pipeline with a built-in stacking of gans . unlike existing methods , which requires full supervision with paired conditions and images , the fusedgan can effectively leverage more abundant images without corresponding conditions in training , to produce more diverse samples with high fidelity . we achieve this by fusing two generators : one for unconditional image generation , and the other for conditional image generation , where the two partly share a common latent space thereby disentangling the generation . we demonstrate the efficacy of the fusedgan in fine grained image generation tasks such as text-to-image , and attribute-to-face generation .", "topics": ["sampling ( signal processing )"]}
{"title": "densereg : fully convolutional dense shape regression in-the-wild", "abstract": "in this paper we propose to learn a mapping from image pixels into a dense template grid through a fully convolutional network . we formulate this task as a regression problem and train our network by leveraging upon manually annotated facial landmarks `` in-the-wild '' . we use such landmarks to establish a dense correspondence field between a three-dimensional object template and the input image , which then serves as the ground-truth for training our regression system . we show that we can combine ideas from semantic segmentation with regression networks , yielding a highly-accurate `` quantized regression '' architecture . our system , called densereg , allows us to estimate dense image-to-template correspondences in a fully convolutional manner . as such our network can provide useful correspondence information as a stand-alone system , while when used as an initialization for statistical deformable models we obtain landmark localization results that largely outperform the current state-of-the-art on the challenging 300w benchmark . we thoroughly evaluate our method on a host of facial analysis tasks and also provide qualitative results for dense human body correspondence . we make our code available at http : //alpguler.com/densereg.html along with supplementary materials .", "topics": ["pixel"]}
{"title": "efficient inference in occlusion-aware generative models of images", "abstract": "we present a generative model of images based on layering , in which image layers are individually generated , then composited from front to back . we are thus able to factor the appearance of an image into the appearance of individual objects within the image -- - and additionally for each individual object , we can factor content from pose . unlike prior work on layered models , we learn a shape prior for each object/layer , allowing the model to tease out which object is in front by looking for a consistent shape , without needing access to motion cues or any labeled data . we show that ordinary stochastic gradient variational bayes ( sgvb ) , which optimizes our fully differentiable lower-bound on the log-likelihood , is sufficient to learn an interpretable representation of images . finally we present experiments demonstrating the effectiveness of the model for inferring foreground and background objects in images .", "topics": ["generative model", "calculus of variations"]}
{"title": "alternating maximization : unifying framework for 8 sparse pca formulations and efficient parallel codes", "abstract": "given a multivariate data set , sparse principal component analysis ( spca ) aims to extract several linear combinations of the variables that together explain the variance in the data as much as possible , while controlling the number of nonzero loadings in these combinations . in this paper we consider 8 different optimization formulations for computing a single sparse loading vector ; these are obtained by combining the following factors : we employ two norms for measuring variance ( l2 , l1 ) and two sparsity-inducing norms ( l0 , l1 ) , which are used in two different ways ( constraint , penalty ) . three of our formulations , notably the one with l0 constraint and l1 variance , have not been considered in the literature . we give a unifying reformulation which we propose to solve via a natural alternating maximization ( am ) method . we show the the am method is nontrivially equivalent to gpower ( journ\\ ' { e } e et al ; jmlr 11:517 -- 553 , 2010 ) for all our formulations . besides this , we provide 24 efficient parallel spca implementations : 3 codes ( multi-core , gpu and cluster ) for each of the 8 problems . parallelism in the methods is aimed at i ) speeding up computations ( our gpu code can be 100 times faster than an efficient serial code written in c++ ) , ii ) obtaining solutions explaining more variance and iii ) dealing with big data problems ( our cluster code is able to solve a 357 gb problem in about a minute ) .", "topics": ["sparse matrix", "computation"]}
{"title": "bayesian compression for deep learning", "abstract": "compression and computational efficiency in deep learning have become a problem of great significance . in this work , we argue that the most principled and effective way to attack this problem is by adopting a bayesian point of view , where through sparsity inducing priors we prune large parts of the network . we introduce two novelties in this paper : 1 ) we use hierarchical priors to prune nodes instead of individual weights , and 2 ) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights . both factors significantly contribute to achieving the state of the art in terms of compression rates , while still staying competitive with methods designed to optimize for speed or energy efficiency .", "topics": ["sparse matrix"]}
{"title": "tractable and scalable schatten quasi-norm approximations for rank minimization", "abstract": "the schatten quasi-norm was introduced to bridge the gap between the trace norm and rank function . however , existing algorithms are too slow or even impractical for large-scale problems . motivated by the equivalence relation between the trace norm and its bilinear spectral penalty , we define two tractable schatten norms , i.e.\\ the bi-trace and tri-trace norms , and prove that they are in essence the schatten- $ 1/2 $ and $ 1/3 $ quasi-norms , respectively . by applying the two defined schatten quasi-norms to various rank minimization problems such as mc and rpca , we only need to solve much smaller factor matrices . we design two efficient linearized alternating minimization algorithms to solve our problems and establish that each bounded sequence generated by our algorithms converges to a critical point . we also provide the restricted strong convexity ( rsc ) based and mc error bounds for our algorithms . our experimental results verified both the efficiency and effectiveness of our algorithms compared with the state-of-the-art methods .", "topics": ["approximation"]}
{"title": "deep learning for unsupervised insider threat detection in structured cybersecurity data streams", "abstract": "analysis of an organization 's computer network activity is a key component of early detection and mitigation of insider threat , a growing concern for many organizations . raw system logs are a prototypical example of streaming data that can quickly scale beyond the cognitive power of a human analyst . as a prospective filter for the human analyst , we present an online unsupervised deep learning approach to detect anomalous network activity from system logs in real time . our models decompose anomaly scores into the contributions of individual user behavior features for increased interpretability to aid analysts reviewing potential cases of insider threat . using the cert insider threat dataset v6.2 and threat detection recall as our performance metric , our novel deep and recurrent neural network models outperform principal component analysis , support vector machine and isolation forest based anomaly detection baselines . for our best model , the events labeled as insider threat activity in our dataset had an average anomaly score in the 95.53 percentile , demonstrating our approach 's potential to greatly reduce analyst workloads .", "topics": ["support vector machine", "recurrent neural network"]}
{"title": "approximate decentralized bayesian inference", "abstract": "this paper presents an approximate method for performing bayesian inference in models with conditional independence over a decentralized network of learning agents . the method first employs variational inference on each individual learning agent to generate a local approximate posterior , the agents transmit their local posteriors to other agents in the network , and finally each agent combines its set of received local posteriors . the key insight in this work is that , for many bayesian models , approximate inference schemes destroy symmetry and dependencies in the model that are crucial to the correct application of bayes ' rule when combining the local posteriors . the proposed method addresses this issue by including an additional optimization step in the combination procedure that accounts for these broken dependencies . experiments on synthetic and real data demonstrate that the decentralized method provides advantages in computational performance and predictive test likelihood over previous batch and distributed methods .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "human pose forecasting via deep markov models", "abstract": "human pose forecasting is an important problem in computer vision with applications to human-robot interaction , visual surveillance , and autonomous driving . usually , forecasting algorithms use 3d skeleton sequences and are trained to forecast for a few milliseconds into the future . long-range forecasting is challenging due to the difficulty of estimating how long a person continues an activity . to this end , our contributions are threefold : ( i ) we propose a generative framework for poses using variational autoencoders based on deep markov models ( dmms ) ; ( ii ) we evaluate our pose forecasts using a pose-based action classifier , which we argue better reflects the subjective quality of pose forecasts than distance in coordinate space ; ( iii ) last , for evaluation of the new model , we introduce a 480,000-frame video dataset called ikea furniture assembly ( ikea fa ) , which depicts humans repeatedly assembling and disassembling furniture . we demonstrate promising results for our approach on both ikea fa and the existing ntu rgb+d dataset .", "topics": ["calculus of variations", "computer vision"]}
{"title": "fast gaussian process regression for big data", "abstract": "gaussian processes are widely used for regression tasks . a known limitation in the application of gaussian processes to regression tasks is that the computation of the solution requires performing a matrix inversion . the solution also requires the storage of a large matrix in memory . these factors restrict the application of gaussian process regression to small and moderate size data sets . we present an algorithm that combines estimates from models developed using subsets of the data obtained in a manner similar to the bootstrap . the sample size is a critical parameter for this algorithm . guidelines for reasonable choices of algorithm parameters , based on detailed experimental study , are provided . various techniques have been proposed to scale gaussian processes to large scale regression tasks . the most appropriate choice depends on the problem context . the proposed method is most appropriate for problems where an additive model works well and the response depends on a small number of features . the minimax rate of convergence for such problems is attractive and we can build effective models with a small subset of the data . the stochastic variational gaussian process and the sparse gaussian process are also appropriate choices for such problems . these methods pick a subset of data based on theoretical considerations . the proposed algorithm uses bagging and random sampling . results from experiments conducted as part of this study indicate that the algorithm presented in this work can be as effective as these methods . model stacking can be used to combine the model developed with the proposed method with models from other methods for large scale regression such as gradient boosted trees . this can yield performance gains .", "topics": ["time complexity", "synthetic data"]}
{"title": "understanding black-box predictions via influence functions", "abstract": "how can we explain the predictions of a black-box model ? in this paper , we use influence functions -- a classic technique from robust statistics -- to trace a model 's prediction through the learning algorithm and back to its training data , thereby identifying training points most responsible for a given prediction . to scale up influence functions to modern machine learning settings , we develop a simple , efficient implementation that requires only oracle access to gradients and hessian-vector products . we show that even on non-convex and non-differentiable models where the theory breaks down , approximations to influence functions can still provide valuable information . on linear models and convolutional neural networks , we demonstrate that influence functions are useful for multiple purposes : understanding model behavior , debugging models , detecting dataset errors , and even creating visually-indistinguishable training-set attacks .", "topics": ["test set", "approximation"]}
{"title": "a more globally accurate dimensionality reduction method using triplets", "abstract": "we first show that the commonly used dimensionality reduction ( dr ) methods such as t-sne and largevis poorly capture the global structure of the data in the low dimensional embedding . we show this via a number of tests for the dr methods that can be easily applied by any practitioner to the dataset at hand . surprisingly enough , t-sne performs the best w.r.t . the commonly used measures that reward the local neighborhood accuracy such as precision-recall while having the worst performance in our tests for global structure . we then contrast the performance of these two dr method against our new method called trimap . the main idea behind trimap is to capture higher orders of structure with triplet information ( instead of pairwise information used by t-sne and largevis ) , and to minimize a robust loss function for satisfying the chosen triplets . we provide compelling experimental evidence on large natural datasets for the clear advantage of the trimap dr results . as largevis , trimap scales linearly with the number of data points .", "topics": ["loss function"]}
{"title": "metric learning with dynamically generated pairwise constraints for ear recognition", "abstract": "ear recognition task is known as predicting whether two ear images belong to the same person or not . in this paper , we present a novel metric learning method for ear recognition . this method is formulated as a pairwise constrained optimization problem . in each training cycle , this method selects the nearest similar and dissimilar neighbors of each sample to construct the pairwise constraints , and then solve the optimization problem by the iterated bregman projections . experiments are conducted on ami , ustb ii and wput databases . the results show that the proposed approach can achieve promising recognition rates in ear recognition , and its training process is much more efficient than the other competing metric learning methods .", "topics": ["optimization problem", "database"]}
{"title": "cdvae : co-embedding deep variational auto encoder for conditional variational generation", "abstract": "problems such as predicting a new shading field ( y ) for an image ( x ) are ambiguous : many very distinct solutions are good . representing this ambiguity requires building a conditional model p ( y|x ) of the prediction , conditioned on the image . such a model is difficult to train , because we do not usually have training data containing many different shadings for the same image . as a result , we need different training examples to share data to produce good models . this presents a danger we call `` code space collapse '' - the training procedure produces a model that has a very good loss score , but which represents the conditional distribution poorly . we demonstrate an improved method for building conditional models by exploiting a metric constraint on training data that prevents code space collapse . we demonstrate our model on two example tasks using real data : image saturation adjustment , image relighting . we describe quantitative metrics to evaluate ambiguous generation results . our results quantitatively and qualitatively outperform different strong baselines .", "topics": ["test set", "encoder"]}
{"title": "compact neural networks based on the multiscale entanglement renormalization ansatz", "abstract": "the goal of this paper is to demonstrate a method for tensorizing neural networks based upon an efficient way of approximating scale invariant quantum states , the multi-scale entanglement renormalization ansatz ( mera ) . we employ mera as a replacement for linear layers in a neural network and test this implementation on the cifar-10 dataset . the proposed method outperforms factorization using tensor trains , providing greater compression for the same level of accuracy and greater accuracy for the same level of compression . we demonstrate mera-layers with 3900 times fewer parameters and a reduction in accuracy of less than 1 % compared to the equivalent fully connected layers .", "topics": ["approximation algorithm"]}
{"title": "paris-lille-3d : a large and high-quality ground truth urban point cloud dataset for automatic segmentation and classification", "abstract": "this paper introduces a new urban point cloud dataset for automatic segmentation and classification acquired by mobile laser scanning ( mls ) . we describe how the dataset is obtained from acquisition to post-processing and labeling . this dataset can be used to learn classification algorithm , however , given that a great attention has been paid to the split between the different objects , this dataset can also be used to learn the segmentation . the dataset consists of around 2km of mls point cloud acquired in two cities . the number of points and range of classes make us consider that it can be used to train deep-learning methods . besides we show some results of automatic segmentation and classification . the dataset is available at : http : //caor-mines-paristech.fr/fr/paris-lille-3d-dataset/", "topics": ["statistical classification", "ground truth"]}
{"title": "gaussian process networks", "abstract": "in this paper we address the problem of learning the structure of a bayesian network in domains with continuous variables . this task requires a procedure for comparing different candidate structures . in the bayesian framework , this is done by evaluating the { em marginal likelihood/ } of the data given a candidate structure . this term can be computed in closed-form for standard parametric families ( e.g . , gaussians ) , and can be approximated , at some computational cost , for some semi-parametric families ( e.g . , mixtures of gaussians ) . we present a new family of continuous variable probabilistic networks that are based on { em gaussian process/ } priors . these priors are semi-parametric in nature and can learn almost arbitrary noisy functional relations . using these priors , we can directly compute marginal likelihoods for structure learning . the resulting method can discover a wide range of functional dependencies in multivariate data . we develop the bayesian score of gaussian process networks and describe how to learn them from data . we present empirical results on artificial data as well as on real-life domains with non-linear dependencies .", "topics": ["bayesian network"]}
{"title": "semantically conditioned lstm-based natural language generation for spoken dialogue systems", "abstract": "natural language generation ( nlg ) is a critical component of spoken dialogue and it has a significant impact both on usability and perceived quality . most nlg systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language . they are also not easily scaled to systems covering multiple domains and languages . this paper presents a statistical language generator based on a semantically controlled long short-term memory ( lstm ) structure . the lstm generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion , and language variation can be easily achieved by sampling from output candidates . with fewer heuristics , an objective evaluation in two differing test domains showed the proposed method improved performance compared to previous methods . human judges scored the lstm system higher on informativeness and naturalness and overall preferred it to the other systems .", "topics": ["sampling ( signal processing )", "natural language"]}
{"title": "learning-theoretic foundations of algorithm configuration for combinatorial partitioning problems", "abstract": "max-cut , clustering , and many other partitioning problems that are of significant importance to machine learning and other scientific fields are np-hard , a reality that has motivated researchers to develop a wealth of approximation algorithms and heuristics . although the best algorithm to use typically depends on the specific application domain , a worst-case analysis is often used to compare algorithms . this may be misleading if worst-case instances occur infrequently , and thus there is a demand for optimization methods which return the algorithm configuration best suited for the given application 's typical inputs . we address this problem for clustering , max-cut , and other partitioning problems , such as integer quadratic programming , by designing computationally efficient and sample efficient learning algorithms which receive samples from an application-specific distribution over problem instances and learn a partitioning algorithm with high expected performance . our algorithms learn over common integer quadratic programming and clustering algorithm families : sdp rounding algorithms and agglomerative clustering algorithms with dynamic programming . for our sample complexity analysis , we provide tight bounds on the pseudodimension of these algorithm classes , and show that surprisingly , even for classes of algorithms parameterized by a single parameter , the pseudo-dimension is superconstant . in this way , our work both contributes to the foundations of algorithm configuration and pushes the boundaries of learning theory , since the algorithm classes we analyze consist of multi-stage optimization procedures and are significantly more complex than classes typically studied in learning theory .", "topics": ["approximation algorithm", "cluster analysis"]}
{"title": "tree-cnn : a deep convolutional neural network for lifelong learning", "abstract": "in recent years , convolutional neural networks ( cnns ) have shown remarkable performance in many computer vision tasks such as object recognition and detection . however , complex training issues , such as `` catastrophic forgetting '' and hyper-parameter tuning , make incremental learning in cnns a difficult challenge . in this paper , we propose a hierarchical deep neural network , with cnns at multiple levels , and a corresponding training method for lifelong learning . the network grows in a tree-like manner to accommodate the new classes of data without losing the ability to identify the previously trained classes . the proposed network was tested on cifar-10 and cifar-100 datasets , and compared against the method of fine tuning specific layers of a conventional cnn . we obtained comparable accuracies and achieved 40 % and 20 % reduction in training effort in cifar-10 and cifar 100 respectively . the network was able to organize the incoming classes of data into feature-driven super-classes . our model improves upon existing hierarchical cnn models by adding the capability of self-growth and also yields important observations on feature selective classification .", "topics": ["neural networks", "computer vision"]}
{"title": "language to logical form with neural attention", "abstract": "semantic parsing aims at mapping natural language to machine interpretable meaning representations . traditional approaches rely on high-quality lexicons , manually-built templates , and linguistic features which are either domain- or representation-specific . in this paper we present a general method based on an attention-enhanced encoder-decoder model . we encode input utterances into vector representations , and generate their logical forms by conditioning the output sequences or trees on the encoding vectors . experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations .", "topics": ["natural language", "parsing"]}
{"title": "preference-based search using example-critiquing with suggestions", "abstract": "we consider interactive tools that help users search for their most preferred item in a large collection of options . in particular , we examine example-critiquing , a technique for enabling users to incrementally construct preference models by critiquing example options that are presented to them . we present novel techniques for improving the example-critiquing technology by adding suggestions to its displayed options . such suggestions are calculated based on an analysis of users current preference model and their potential hidden preferences . we evaluate the performance of our model-based suggestion techniques with both synthetic and real users . results show that such suggestions are highly attractive to users and can stimulate them to express more preferences to improve the chance of identifying their most preferred item by up to 78 % .", "topics": ["synthetic data"]}
{"title": "assessing the reach and impact of game-based learning approaches to cultural competency and behavioural change", "abstract": "as digital games continue to be explored as solutions to educational and behavioural challenges , the need for evaluation methodologies which support both the unique nature of the format and the need for comparison with other approaches continues to increase . in this workshop paper , a range of challenges are described related specifically to the case of cultural learning using digital games , in terms of how it may best be assessed , understood , and sustained through an iterative process supported by research . an evaluation framework is proposed , identifying metrics for reach and impact and their associated challenges , as well as presenting ethical considerations and the means to utilize evaluation outcomes within an iterative cycle , and to provide feedback to learners . presenting as a case study a serious game from the mobile assistance for social inclusion and empowerment of immigrants with persuasive learning technologies and social networks ( maseltov ) project , the use of the framework in the context of an integrative project is discussed , with emphasis on the need to view game-based learning as a blended component of the cultural learning process , rather than a standalone solution . the particular case of mobile gaming is also considered within this case study , providing a platform by which to deliver and update content in response to evaluation outcomes . discussion reflects upon the general challenges related to the assessment of cultural learning , and behavioural change in more general terms , suggesting future work should address the need to provide sustainable , research-driven platforms for game-based learning content .", "topics": ["iteration"]}
{"title": "deep de-aliasing for fast compressive sensing mri", "abstract": "fast magnetic resonance imaging ( mri ) is highly in demand for many clinical applications in order to reduce the scanning cost and improve the patient experience . this can also potentially increase the image quality by reducing the motion artefacts and contrast washout . however , once an image field of view and the desired resolution are chosen , the minimum scanning time is normally determined by the requirement of acquiring sufficient raw data to meet the nyquist-shannon sampling criteria . compressive sensing ( cs ) theory has been perfectly matched to the mri scanning sequence design with much less required raw data for the image reconstruction . inspired by recent advances in deep learning for solving various inverse problems , we propose a conditional generative adversarial networks-based deep learning framework for de-aliasing and reconstructing mri images from highly undersampled data with great promise to accelerate the data acquisition process . by coupling an innovative content loss with the adversarial loss our de-aliasing results are more realistic . furthermore , we propose a refinement learning procedure for training the generator network , which can stabilise the training with fast convergence and less parameter tuning . we demonstrate that the proposed framework outperforms state-of-the-art cs-mri methods , in terms of reconstruction error and perceptual image quality . in addition , our method can reconstruct each image in 0.22ms -- 0.37ms , which is promising for real-time applications .", "topics": ["sampling ( signal processing )"]}
{"title": "can image-level labels replace pixel-level labels for image parsing", "abstract": "this paper presents a weakly supervised sparse learning approach to the problem of noisily tagged image parsing , or segmenting all the objects within a noisily tagged image and identifying their categories ( i.e . tags ) . different from the traditional image parsing that takes pixel-level labels as strong supervisory information , our noisily tagged image parsing is provided with noisy tags of all the images ( i.e . image-level labels ) , which is a natural setting for social image collections ( e.g . flickr ) . by oversegmenting all the images into regions , we formulate noisily tagged image parsing as a weakly supervised sparse learning problem over all the regions , where the initial labels of each region are inferred from image-level labels . furthermore , we develop an efficient algorithm to solve such weakly supervised sparse learning problem . the experimental results on two benchmark datasets show the effectiveness of our approach . more notably , the reported surprising results shed some light on answering the question : can image-level labels replace pixel-level labels ( hard to access ) as supervisory information for image parsing .", "topics": ["parsing", "pixel"]}
{"title": "regret-based reward elicitation for markov decision processes", "abstract": "the specification of amarkov decision process ( mdp ) can be difficult . reward function specification is especially problematic ; in practice , it is often cognitively complex and time-consuming for users to precisely specify rewards . this work casts the problem of specifying rewards as one of preference elicitation and aims to minimize the degree of precision with which a reward function must be specified while still allowing optimal or near-optimal policies to be produced . we first discuss how robust policies can be computed for mdps given only partial reward information using the minimax regret criterion . we then demonstrate how regret can be reduced by efficiently eliciting reward information using bound queries , using regret-reduction as a means for choosing suitable queries . empirical results demonstrate that regret-based reward elicitation offers an effective way to produce near-optimal policies without resorting to the precise specification of the entire reward function .", "topics": ["regret ( decision theory )", "reinforcement learning"]}
{"title": "deep learning for decoding of linear codes - a syndrome-based approach", "abstract": "we present a novel framework for applying deep neural networks ( dnn ) to soft decoding of linear codes at arbitrary block lengths . unlike other approaches , our framework allows unconstrained dnn design , enabling the free application of powerful designs that were developed in other contexts . our method is robust to overfitting that inhibits many competing methods , which follows from the exponentially large number of codewords required for their training . we achieve this by transforming the channel output before feeding it to the network , extracting only the syndrome of the hard decisions and the channel output reliabilities . we prove analytically that this approach does not involve any intrinsic performance penalty , and guarantees the generalization of performance obtained during training . our best results are obtained using a recurrent neural network ( rnn ) architecture combined with simple preprocessing by permutation . we provide simulation results that demonstrate performance that sometimes approaches that of the ordered statistics decoding ( osd ) algorithm .", "topics": ["recurrent neural network", "simulation"]}
{"title": "hidden structure and function in the lexicon", "abstract": "how many words are needed to define all the words in a dictionary ? graph-theoretic analysis reveals that about 10 % of a dictionary is a unique kernel of words that define one another and all the rest , but this is not the smallest such subset . the kernel consists of one huge strongly connected component ( scc ) , about half its size , the core , surrounded by many small sccs , the satellites . core words can define one another but not the rest of the dictionary . the kernel also contains many overlapping minimal grounding sets ( mgss ) , each about the same size as the core , each part-core , part-satellite . mgs words can define all the rest of the dictionary . they are learned earlier , more concrete and more frequent than the rest of the dictionary . satellite words , not correlated with age or frequency , are less concrete ( more abstract ) words that are also needed for full lexical power .", "topics": ["kernel ( operating system )", "dictionary"]}
{"title": "an analytical piecewise radial distortion model for precision camera calibration", "abstract": "the common approach to radial distortion is by the means of polynomial approximation , which introduces distortion-specific parameters into the camera model and requires estimation of these distortion parameters . the task of estimating radial distortion is to find a radial distortion model that allows easy undistortion as well as satisfactory accuracy . this paper presents a new piecewise radial distortion model with easy analytical undistortion formula . the motivation for seeking a piecewise radial distortion model is that , when a camera is resulted in a low quality during manufacturing , the nonlinear radial distortion can be complex . using low order polynomials to approximate the radial distortion might not be precise enough . on the other hand , higher order polynomials suffer from the inverse problem . with the new piecewise radial distortion function , more flexibility is obtained and the radial undistortion can be performed analytically . experimental results are presented to show that with this new piecewise radial distortion model , better performance is achieved than that using the single function . furthermore , a comparable performance with the conventional polynomial model using 2 coefficients can also be accomplished .", "topics": ["approximation algorithm", "nonlinear system"]}
{"title": "a deep language model for software code", "abstract": "existing language models such as n-grams for software code often fail to capture a long context where dependent code elements scatter far apart . in this paper , we propose a novel approach to build a language model for software code to address this particular issue . our language model , partly inspired by human memory , is built upon the powerful deep learning-based long short term memory architecture that is capable of learning long-term dependencies which occur frequently in software code . results from our intrinsic evaluation on a corpus of java projects have demonstrated the effectiveness of our language model . this work contributes to realizing our vision for deepsoft , an end-to-end , generic deep learning-based framework for modeling software and its development process .", "topics": ["natural language processing", "end-to-end principle"]}
{"title": "trannsformer : neural network transformation for memristive crossbar based neuromorphic system design", "abstract": "implementation of neuromorphic systems using post complementary metal-oxide-semiconductor ( cmos ) technology based memristive crossbar array ( mca ) has emerged as a promising solution to enable low-power acceleration of neural networks . however , the recent trend to design deep neural networks ( dnns ) for achieving human-like cognitive abilities poses significant challenges towards the scalable design of neuromorphic systems ( due to the increase in computation/storage demands ) . network pruning [ 7 ] is a powerful technique to remove redundant connections for designing optimally connected ( maximally sparse ) dnns . however , such pruning techniques induce irregular connections that are incoherent to the crossbar structure . eventually they produce dnns with highly inefficient hardware realizations ( in terms of area and energy ) . in this work , we propose trannsformer - an integrated training framework that transforms dnns to enable their efficient realization on mca-based systems . trannsformer first prunes the connectivity matrix while forming clusters with the remaining connections . subsequently , it retrains the network to fine tune the connections and reinforce the clusters . this is done iteratively to transform the original connectivity into an optimally pruned and maximally clustered mapping . without accuracy loss , trannsformer reduces the area ( energy ) consumption by 28 % - 55 % ( 49 % - 67 % ) with respect to the original network . compared to network pruning , trannsformer achieves 28 % - 49 % ( 15 % - 29 % ) area ( energy ) savings . furthermore , trannsformer is a technology-aware framework that allows mapping a given dnn to any mca size permissible by the memristive technology for reliable operations .", "topics": ["mathematical optimization", "neural networks"]}
{"title": "senns : sparse extraction neural networks for feature extraction", "abstract": "by drawing on ideas from optimisation theory , artificial neural networks ( ann ) , graph embeddings and sparse representations , i develop a novel technique , termed senns ( sparse extraction neural networks ) , aimed at addressing the feature extraction problem . the proposed method uses ( preferably deep ) anns for projecting input attribute vectors to an output space wherein pairwise distances are maximized for vectors belonging to different classes , but minimized for those belonging to the same class , while simultaneously enforcing sparsity on the ann outputs . the vectors that result from the projection can then be used as features in any classifier of choice . mathematically , i formulate the proposed method as the minimisation of an objective function which can be interpreted , in the ann output space , as a negative factor of the sum of the squares of the pair-wise distances between output vectors belonging to different classes , added to a positive factor of the sum of squares of the pair-wise distances between output vectors belonging to the same classes , plus sparsity and weight decay terms . to derive an algorithm for minimizing the objective function via gradient descent , i use the multi-variate version of the chain rule to obtain the partial derivatives of the function with respect to ann weights and biases , and find that each of the required partial derivatives can be expressed as a sum of six terms . as it turns out , four of those six terms can be computed using the standard back propagation algorithm ; the fifth can be computed via a slight modification of the standard backpropagation algorithm ; while the sixth one can be computed via simple arithmetic . finally , i propose experiments on the arabase arabic corpora of digits and letters , the cmu pie database of faces , the mnist digits database , and other standard machine learning databases .", "topics": ["mathematical optimization", "feature extraction"]}
{"title": "syntax-directed variational autoencoder for structured data", "abstract": "deep generative models have been enjoying success in modeling continuous data . however it remains challenging to capture the representations for discrete structures with formal grammars and semantics , e.g . , computer programs and molecular structures . how to generate both syntactically and semantically correct data still remains largely an open problem . inspired by the theory of compiler where the syntax and semantics check is done via syntax-directed translation ( sdt ) , we propose a novel syntax-directed variational autoencoder ( sd-vae ) by introducing stochastic lazy attributes . this approach converts the offline sdt check into on-the-fly generated guidance for constraining the decoder . comparing to the state-of-the-art methods , our approach enforces constraints on the output space so that the output will be not only syntactically valid , but also semantically reasonable . we evaluate the proposed model with applications in programming language and molecules , including reconstruction and program/molecule optimization . the results demonstrate the effectiveness in incorporating syntactic and semantic constraints in discrete generative models , which is significantly better than current state-of-the-art approaches .", "topics": ["generative model", "calculus of variations"]}
{"title": "small sample learning of superpixel classifiers for em segmentation- extended version", "abstract": "pixel and superpixel classifiers have become essential tools for em segmentation algorithms . training these classifiers remains a major bottleneck primarily due to the requirement of completely annotating the dataset which is tedious , error-prone and costly . in this paper , we propose an interactive learning scheme for the superpixel classifier for em segmentation . our algorithm is `` active semi-supervised '' because it requests the labels of a small number of examples from user and applies label propagation technique to generate these queries . using only a small set ( $ < 20\\ % $ ) of all datapoints , the proposed algorithm consistently generates a classifier almost as accurate as that estimated from a complete groundtruth . we provide segmentation results on multiple datasets to show the strength of these classifiers .", "topics": ["pixel"]}
{"title": "delight-net : decomposing reflectance maps into specular materials and natural illumination", "abstract": "in this paper we are extracting surface reflectance and natural environmental illumination from a reflectance map , i.e . from a single 2d image of a sphere of one material under one illumination . this is a notoriously difficult problem , yet key to various re-rendering applications . with the recent advances in estimating reflectance maps from 2d images their further decomposition has become increasingly relevant . to this end , we propose a convolutional neural network ( cnn ) architecture to reconstruct both material parameters ( i.e . phong ) as well as illumination ( i.e . high-resolution spherical illumination maps ) , that is solely trained on synthetic data . we demonstrate that decomposition of synthetic as well as real photographs of reflectance maps , both in high dynamic range ( hdr ) , and , for the first time , on low dynamic range ( ldr ) as well . results are compared to previous approaches quantitatively as well as qualitatively in terms of re-renderings where illumination , material , view or shape are changed .", "topics": ["synthetic data", "map"]}
{"title": "word , graph and manifold embedding from markov processes", "abstract": "continuous vector representations of words and objects appear to carry surprisingly rich semantic content . in this paper , we advance both the conceptual and theoretical understanding of word embeddings in three ways . first , we ground embeddings in semantic spaces studied in cognitive-psychometric literature and introduce new evaluation tasks . second , in contrast to prior work , we take metric recovery as the key object of study , unify existing algorithms as consistent metric recovery methods based on co-occurrence counts from simple markov random walks , and propose a new recovery algorithm . third , we generalize metric recovery to graphs and manifolds , relating co-occurence counts on random walks in graphs and random processes on manifolds to the underlying metric to be recovered , thereby reconciling manifold estimation and embedding algorithms . we compare embedding algorithms across a range of tasks , from nonlinear dimensionality reduction to three semantic language tasks , including analogies , sequence completion , and classification .", "topics": ["natural language processing", "computer vision"]}
{"title": "deep inside convolutional networks : visualising image classification models and saliency maps", "abstract": "this paper addresses the visualisation of image classification models , learnt using deep convolutional networks ( convnets ) . we consider two visualisation techniques , based on computing the gradient of the class score with respect to the input image . the first one generates an image , which maximises the class score [ erhan et al . , 2009 ] , thus visualising the notion of the class , captured by a convnet . the second technique computes a class saliency map , specific to a given image and class . we show that such maps can be employed for weakly supervised object segmentation using classification convnets . finally , we establish the connection between the gradient-based convnet visualisation methods and deconvolutional networks [ zeiler et al . , 2013 ] .", "topics": ["computer vision", "map"]}
{"title": "gp-sum . gaussian processes filtering of non-gaussian beliefs", "abstract": "this work studies the problem of stochastic dynamic filtering and state propagation with complex beliefs . the main contribution is gp-sum , a filtering algorithm tailored to dynamic systems and observation models expressed as gaussian processes ( gp ) , that does not rely on linearizations or unimodal gaussian approximations of the belief . the algorithm can be seen as a combination of a sampling-based filter and a probabilistic bayes filter . gp-sum operates by sampling the state distribution and propagating each sample through the dynamic system and observation models . effective sampling and accurate probabilistic propagation are possible by relying on the gp form of the system , and a gaussian mixture form of the belief . we show that gp-sum outperforms several gp-bayes and particle filters on a standard benchmark . we also illustrate its practical use in a pushing task , and demonstrate that it predicts heteroscedasticity , i.e . , different amounts of uncertainty , and multi-modality when naturally occurring in pushing .", "topics": ["sampling ( signal processing )", "approximation"]}
{"title": "pairwise choice markov chains", "abstract": "as datasets capturing human choices grow in richness and scale -- -particularly in online domains -- -there is an increasing need for choice models that escape traditional choice-theoretic axioms such as regularity , stochastic transitivity , and luce 's choice axiom . in this work we introduce the pairwise choice markov chain ( pcmc ) model of discrete choice , an inferentially tractable model that does not assume any of the above axioms while still satisfying the foundational axiom of uniform expansion , a considerably weaker assumption than luce 's choice axiom . we show that the pcmc model significantly outperforms the multinomial logit ( mnl ) model in prediction tasks on both synthetic and empirical datasets known to exhibit violations of luce 's axiom . our analysis also synthesizes several recent observations connecting the multinomial logit model and markov chains ; the pcmc model retains the multinomial logit model as a special case .", "topics": ["synthetic data", "markov chain"]}
{"title": "attacking binarized neural networks", "abstract": "neural networks with low-precision weights and activations offer compelling efficiency advantages over their full-precision equivalents . the two most frequently discussed benefits of quantization are reduced memory consumption , and a faster forward pass when implemented with efficient bitwise operations . we propose a third benefit of very low-precision neural networks : improved robustness against some adversarial attacks , and in the worst case , performance that is on par with full-precision models . we focus on the very low-precision case where weights and activations are both quantized to $ \\pm $ 1 , and note that stochastically quantizing weights in just one layer can sharply reduce the impact of iterative attacks . we observe that non-scaled binary neural networks exhibit a similar effect to the original defensive distillation procedure that led to gradient masking , and a false notion of security . we address this by conducting both black-box and white-box experiments with binary models that do not artificially mask gradients .", "topics": ["neural networks", "gradient"]}
{"title": "convex formulation for kernel pca and its use in semi-supervised learning", "abstract": "in this paper , kernel pca is reinterpreted as the solution to a convex optimization problem . actually , there is a constrained convex problem for each principal component , so that the constraints guarantee that the principal component is indeed a solution , and not a mere saddle point . although these insights do not imply any algorithmic improvement , they can be used to further understand the method , formulate possible extensions and properly address them . as an example , a new convex optimization problem for semi-supervised classification is proposed , which seems particularly well-suited whenever the number of known labels is small . our formulation resembles a least squares svm problem with a regularization parameter multiplied by a negative sign , combined with a variational principle for kernel pca . our primal optimization principle for semi-supervised learning is solved in terms of the lagrange multipliers . numerical experiments in several classification tasks illustrate the performance of the proposed model in problems with only a few labeled data .", "topics": ["optimization problem", "calculus of variations"]}
{"title": "adaptive plant propagation algorithm for solving economic load dispatch problem", "abstract": "optimization problems in design engineering are complex by nature , often because of the involvement of critical objective functions accompanied by a number of rigid constraints associated with the products involved . one such problem is economic load dispatch ( ed ) problem which focuses on the optimization of the fuel cost while satisfying some system constraints . classical optimization algorithms are not sufficient and also inefficient for the ed problem involving highly nonlinear , and non-convex functions both in the objective and in the constraints . this led to the development of metaheuristic optimization approaches which can solve the ed problem almost efficiently . this paper presents a novel robust plant intelligence based adaptive plant propagation algorithm ( appa ) which is used to solve the classical ed problem . the application of the proposed method to the 3-generator and 6-generator systems shows the efficiency and robustness of the proposed algorithm . a comparative study with another state-of-the-art algorithm ( apso ) demonstrates the quality of the solution achieved by the proposed method along with the convergence characteristics of the proposed approach .", "topics": ["mathematical optimization", "nonlinear system"]}
{"title": "cognitive database : a step towards endowing relational databases with artificial intelligence capabilities", "abstract": "we propose cognitive databases , an approach for transparently enabling artificial intelligence ( ai ) capabilities in relational databases . a novel aspect of our design is to first view the structured data source as meaningful unstructured text , and then use the text to build an unsupervised neural network model using a natural language processing ( nlp ) technique called word embedding . this model captures the hidden inter-/intra-column relationships between database tokens of different types . for each database token , the model includes a vector that encodes contextual semantic relationships . we seamlessly integrate the word embedding model into existing sql query infrastructure and use it to enable a new class of sql-based analytics queries called cognitive intelligence ( ci ) queries . ci queries use the model vectors to enable complex queries such as semantic matching , inductive reasoning queries such as analogies , predictive queries using entities not present in a database , and , more generally , using knowledge from external sources . we demonstrate unique capabilities of cognitive databases using an apache spark based prototype to execute inductive reasoning ci queries over a multi-modal database containing text and images . we believe our first-of-a-kind system exemplifies using ai functionality to endow relational databases with capabilities that were previously very hard to realize in practice .", "topics": ["natural language processing", "natural language"]}
{"title": "doing better than uct : rational monte carlo sampling in trees", "abstract": "uct , a state-of-the art algorithm for monte carlo tree sampling ( mcts ) , is based on ucb , a sampling policy for the multi-armed bandit problem ( mab ) that minimizes the accumulated regret . however , mcts differs from mab in that only the final choice , rather than all arm pulls , brings a reward , that is , the simple regret , as opposite to the cumulative regret , must be minimized . this ongoing work aims at applying meta-reasoning techniques to mcts , which is non-trivial . we begin by introducing policies for multi-armed bandits with lower simple regret than ucb , and an algorithm for mcts which combines cumulative and simple regret minimization and outperforms uct . we also develop a sampling scheme loosely based on a myopic version of perfect value of information . finite-time and asymptotic analysis of the policies is provided , and the algorithms are compared empirically .", "topics": ["regret ( decision theory )"]}
{"title": "attack planning in the real world", "abstract": "assessing network security is a complex and difficult task . attack graphs have been proposed as a tool to help network administrators understand the potential weaknesses of their network . however , a problem has not yet been addressed by previous work on this subject ; namely , how to actually execute and validate the attack paths resulting from the analysis of the attack graph . in this paper we present a complete pddl representation of an attack model , and an implementation that integrates a planner into a penetration testing tool . this allows to automatically generate attack paths for penetration testing scenarios , and to validate these attacks by executing the corresponding actions -including exploits- against the real target network . we present an algorithm for transforming the information present in the penetration testing tool to the planning domain , and show how the scalability issues of attack graphs can be solved using current planners . we include an analysis of the performance of our solution , showing how our model scales to medium-sized networks and the number of actions available in current penetration testing tools .", "topics": ["scalability"]}
{"title": "defense against universal adversarial perturbations", "abstract": "recent advances in deep learning show the existence of image-agnostic quasi-imperceptible perturbations that when applied to `any ' image can fool a state-of-the-art network classifier to change its prediction about the image label . these `universal adversarial perturbations ' pose a serious threat to the success of deep learning in practice . we present the first dedicated framework to effectively defend the networks against such perturbations . our approach learns a perturbation rectifying network ( prn ) as `pre-input ' layers to a targeted model , such that the targeted model needs no modification . the prn is learned from real and synthetic image-agnostic perturbations , where an efficient method to compute the latter is also proposed . a perturbation detector is separately trained on the discrete cosine transform of the input-output difference of the prn . a query image is first passed through the prn and verified by the detector . if a perturbation is detected , the output of the prn is used for label prediction instead of the actual image . a rigorous evaluation shows that our framework can defend the network classifiers against unseen adversarial perturbations in the real-world scenarios with up to 97.5 % success rate . the prn also generalizes well in the sense that training for one targeted network defends another network with a comparable success rate .", "topics": ["synthetic data"]}
{"title": "sylvester normalizing flows for variational inference", "abstract": "variational inference relies on flexible approximate posterior distributions . normalizing flows provide a general recipe to construct flexible variational posteriors . we introduce sylvester normalizing flows , which can be seen as a generalization of planar flows . sylvester normalizing flows remove the well-known single-unit bottleneck from planar flows , making a single transformation much more flexible . we compare the performance of sylvester normalizing flows against planar flows and inverse autoregressive flows and demonstrate that they compare favorably on several datasets .", "topics": ["calculus of variations", "approximation algorithm"]}
{"title": "how far are we from solving the 2d & 3d face alignment problem ? ( and a dataset of 230,000 3d facial landmarks )", "abstract": "this paper investigates how far a very deep neural network is from attaining close to saturating performance on existing 2d and 3d face alignment datasets . to this end , we make the following 5 contributions : ( a ) we construct , for the first time , a very strong baseline by combining a state-of-the-art architecture for landmark localization with a state-of-the-art residual block , train it on a very large yet synthetically expanded 2d facial landmark dataset and finally evaluate it on all other 2d facial landmark datasets . ( b ) we create a guided by 2d landmarks network which converts 2d landmark annotations to 3d and unifies all existing datasets , leading to the creation of ls3d-w , the largest and most challenging 3d facial landmark dataset to date ~230,000 images . ( c ) following that , we train a neural network for 3d face alignment and evaluate it on the newly introduced ls3d-w. ( d ) we further look into the effect of all `` traditional '' factors affecting face alignment performance like large pose , initialization and resolution , and introduce a `` new '' one , namely the size of the network . ( e ) we show that both 2d and 3d face alignment networks achieve performance of remarkable accuracy which is probably close to saturating the datasets used . training and testing code as well as the dataset can be downloaded from https : //www.adrianbulat.com/face-alignment/", "topics": ["baseline ( configuration management )"]}
{"title": "the structure of bayes nets for vision recognition", "abstract": "this paper is part of a study whose goal is to show the effciency of using bayes networks to carry out model based vision calculations . [ binford et al . 1987 ] recognition proceeds by drawing up a network model from the object 's geometric and functional description that predicts the appearance of an object . then this network is used to find the object within a photographic image . many existing and proposed techniques for vision recognition resemble the uncertainty calculations of a bayes net . in contrast , though , they lack a derivation from first principles , and tend to rely on arbitrary parameters that we hope to avoid by a network model . the connectedness of the network depends on what independence considerations can be identified in the vision problem . greater independence leads to easier calculations , at the expense of the net 's expressiveness . once this trade-off is made and the structure of the network is determined , it should be possible to tailor a solution technique for it . this paper explores the use of a network with multiply connected paths , drawing on both techniques of belief networks [ pearl 86 ] and influence diagrams . we then demonstrate how one formulation of a multiply connected network can be solved .", "topics": ["computer vision", "bayesian network"]}
{"title": "optimizing kernel machines using deep learning", "abstract": "building highly non-linear and non-parametric models is central to several state-of-the-art machine learning systems . kernel methods form an important class of techniques that induce a reproducing kernel hilbert space ( rkhs ) for inferring non-linear models through the construction of similarity functions from data . these methods are particularly preferred in cases where the training data sizes are limited and when prior knowledge of the data similarities is available . despite their usefulness , they are limited by the computational complexity and their inability to support end-to-end learning with a task-specific objective . on the other hand , deep neural networks have become the de facto solution for end-to-end inference in several learning paradigms . in this article , we explore the idea of using deep architectures to perform kernel machine optimization , for both computational efficiency and end-to-end inferencing . to this end , we develop the dkmo ( deep kernel machine optimization ) framework , that creates an ensemble of dense embeddings using nystrom kernel approximations and utilizes deep learning to generate task-specific representations through the fusion of the embeddings . intuitively , the filters of the network are trained to fuse information from an ensemble of linear subspaces in the rkhs . furthermore , we introduce the kernel dropout regularization to enable improved training convergence . finally , we extend this framework to the multiple kernel case , by coupling a global fusion layer with pre-trained deep kernel machines for each of the constituent kernels . using case studies with limited training data , and lack of explicit feature sources , we demonstrate the effectiveness of our framework over conventional model inferencing techniques .", "topics": ["kernel ( operating system )", "test set"]}
{"title": "conceptual spaces for cognitive architectures : a lingua franca for different levels of representation", "abstract": "during the last decades , many cognitive architectures ( cas ) have been realized adopting different assumptions about the organization and the representation of their knowledge level . some of them ( e.g . soar [ laird ( 2012 ) ] ) adopt a classical symbolic approach , some ( e.g . leabra [ o'reilly and munakata ( 2000 ) ] ) are based on a purely connectionist model , while others ( e.g . clarion [ sun ( 2006 ) ] adopt a hybrid approach combining connectionist and symbolic representational levels . additionally , some attempts ( e.g . bisoar ) trying to extend the representational capacities of cas by integrating diagrammatical representations and reasoning are also available [ kurup and chandrasekaran ( 2007 ) ] . in this paper we propose a reflection on the role that conceptual spaces , a framework developed by peter g\u007f\\ '' ardenfors [ g\u007f\\ '' ardenfors ( 2000 ) ] more than fifteen years ago , can play in the current development of the knowledge level in cognitive systems and architectures . in particular , we claim that conceptual spaces offer a lingua franca that allows to unify and generalize many aspects of the symbolic , sub-symbolic and diagrammatic approaches ( by overcoming some of their typical problems ) and to integrate them on a common ground . in doing so we extend and detail some of the arguments explored by g\u007f\\ '' ardenfors [ g\u007f\\ '' ardenfors ( 1997 ) ] for defending the need of a conceptual , intermediate , representation level between the symbolic and the sub-symbolic one .", "topics": ["artificial intelligence"]}
{"title": "how part-of-speech tags affect text retrieval and filtering performance", "abstract": "natural language processing ( nlp ) applied to information retrieval ( ir ) and filtering problems may assign part-of-speech tags to terms and , more generally , modify queries and documents . analytic models can predict the performance of a text filtering system as it incorporates changes suggested by nlp , allowing us to make precise statements about the average effect of nlp operations on ir . here we provide a model of retrieval and tagging that allows us to both compute the performance change due to syntactic parsing and to allow us to understand what factors affect performance and how . in addition to a prediction of performance with tags , upper and lower bounds for retrieval performance are derived , giving the best and worst effects of including part-of-speech tags . empirical grounds for selecting sets of tags are considered .", "topics": ["natural language processing", "natural language"]}
{"title": "quantization of fully convolutional networks for accurate biomedical image segmentation", "abstract": "with pervasive applications of medical imaging in health-care , biomedical image segmentation plays a central role in quantitative analysis , clinical diagno- sis , and medical intervention . since manual anno- tation su ers limited reproducibility , arduous e orts , and excessive time , automatic segmentation is desired to process increasingly larger scale histopathological data . recently , deep neural networks ( dnns ) , par- ticularly fully convolutional networks ( fcns ) , have been widely applied to biomedical image segmenta- tion , attaining much improved performance . at the same time , quantization of dnns has become an ac- tive research topic , which aims to represent weights with less memory ( precision ) to considerably reduce memory and computation requirements of dnns while maintaining acceptable accuracy . in this paper , we apply quantization techniques to fcns for accurate biomedical image segmentation . unlike existing litera- ture on quantization which primarily targets memory and computation complexity reduction , we apply quan- tization as a method to reduce over tting in fcns for better accuracy . speci cally , we focus on a state-of- the-art segmentation framework , suggestive annotation [ 22 ] , which judiciously extracts representative annota- tion samples from the original training dataset , obtain- ing an e ective small-sized balanced training dataset . we develop two new quantization processes for this framework : ( 1 ) suggestive annotation with quantiza- tion for highly representative training samples , and ( 2 ) network training with quantization for high accuracy . extensive experiments on the miccai gland dataset show that both quantization processes can improve the segmentation performance , and our proposed method exceeds the current state-of-the-art performance by up to 1 % . in addition , our method has a reduction of up to 6.4x on memory usage .", "topics": ["image segmentation", "computation"]}
{"title": "label-free supervision of neural networks with physics and domain knowledge", "abstract": "in many machine learning applications , labeled data is scarce and obtaining more labels is expensive . we introduce a new approach to supervising neural networks by specifying constraints that should hold over the output space , rather than direct examples of input-output pairs . these constraints are derived from prior domain knowledge , e.g . , from known laws of physics . we demonstrate the effectiveness of this approach on real world and simulated computer vision tasks . we are able to train a convolutional neural network to detect and track objects without any labeled examples . our approach can significantly reduce the need for labeled training data , but introduces new challenges for encoding prior knowledge into appropriate loss functions .", "topics": ["test set", "neural networks"]}
{"title": "how slow is slow ? sfa detects signals that are slower than the driving force", "abstract": "slow feature analysis ( sfa ) is a method for extracting slowly varying driving forces from quickly varying nonstationary time series . we show here that it is possible for sfa to detect a component which is even slower than the driving force itself ( e.g . the envelope of a modulated sine wave ) . it is shown that it depends on circumstances like the embedding dimension , the time series predictability , or the base frequency , whether the driving force itself or a slower subcomponent is detected . we observe a phase transition from one regime to the other and it is the purpose of this work to quantify the influence of various parameters on this phase transition . we conclude that what is percieved as slow by sfa varies and that a more or less fast switching from one regime to the other occurs , perhaps showing some similarity to human perception .", "topics": ["time series"]}
{"title": "iterative judgment aggregation", "abstract": "judgment aggregation problems form a class of collective decision-making problems represented in an abstract way , subsuming some well known problems such as voting . a collective decision can be reached in many ways , but a direct one-step aggregation of individual decisions is arguably most studied . another way to reach collective decisions is by iterative consensus building -- allowing each decision-maker to change their individual decision in response to the choices of the other agents until a consensus is reached . iterative consensus building has so far only been studied for voting problems . here we propose an iterative judgment aggregation algorithm , based on movements in an undirected graph , and we study for which instances it terminates with a consensus . we also compare the computational complexity of our iterative procedure with that of related judgment aggregation operators .", "topics": ["computational complexity theory", "iteration"]}
{"title": "accelerating stochastic gradient descent", "abstract": "there is widespread sentiment that it is not possible to effectively utilize fast gradient methods ( e.g . nesterov 's acceleration , conjugate gradient , heavy ball ) for the purposes of stochastic optimization due to their instability and error accumulation , a notion made precise in d'aspremont 2008 and devolder , glineur , and nesterov 2014 . this work considers these issues for the special case of stochastic approximation for the least squares regression problem , and our main result refutes the conventional wisdom by showing that acceleration can be made robust to statistical errors . in particular , this work introduces an accelerated stochastic gradient method that provably achieves the minimax optimal statistical risk faster than stochastic gradient descent . critical to the analysis is a sharp characterization of accelerated stochastic gradient descent as a stochastic process . we hope this characterization gives insights towards the broader question of designing simple and effective accelerated stochastic methods for more general convex and non-convex optimization problems .", "topics": ["gradient descent", "gradient"]}
{"title": "a generic deep architecture for single image reflection removal and image smoothing", "abstract": "this paper proposes a deep neural network structure that exploits edge information in addressing representative low-level vision tasks such as layer separation and image filtering . unlike most other deep learning strategies applied in this context , our approach tackles these challenging problems by estimating edges and reconstructing images using only cascaded convolutional layers arranged such that no handcrafted or application-specific image-processing components are required . we apply the resulting transferrable pipeline to two different problem domains that are both sensitive to edges , namely , single image reflection removal and image smoothing . for the former , using a mild reflection smoothness assumption and a novel synthetic data generation method that acts as a type of weak supervision , our network is able to solve much more difficult reflection cases that can not be handled by previous methods . for the latter , we also exceed the state-of-the-art quantitative and qualitative results by wide margins . in all cases , the proposed framework is simple , fast , and easy to transfer across disparate domains .", "topics": ["image processing", "high- and low-level"]}
{"title": "multi-class svms : from tighter data-dependent generalization bounds to novel algorithms", "abstract": "this paper studies the generalization performance of multi-class classification algorithms , for which we obtain , for the first time , a data-dependent generalization error bound with a logarithmic dependence on the class size , substantially improving the state-of-the-art linear dependence in the existing data-dependent generalization analysis . the theoretical analysis motivates us to introduce a new multi-class classification machine based on $ \\ell_p $ -norm regularization , where the parameter $ p $ controls the complexity of the corresponding bounds . we derive an efficient optimization algorithm based on fenchel duality theory . benchmarks on several real-world datasets show that the proposed algorithm can achieve significant accuracy gains over the state of the art .", "topics": ["matrix regularization"]}
{"title": "( bandit ) convex optimization with biased noisy gradient oracles", "abstract": "algorithms for bandit convex optimization and online learning often rely on constructing noisy gradient estimates , which are then used in appropriately adjusted first-order algorithms , replacing actual gradients . depending on the properties of the function to be optimized and the nature of `` noise '' in the bandit feedback , the bias and variance of gradient estimates exhibit various tradeoffs . in this paper we propose a novel framework that replaces the specific gradient estimation methods with an abstract oracle . with the help of the new framework we unify previous works , reproducing their results in a clean and concise fashion , while , perhaps more importantly , the framework also allows us to formally show that to achieve the optimal root- $ n $ rate either the algorithms that use existing gradient estimators , or the proof techniques used to analyze them have to go beyond what exists today .", "topics": ["gradient"]}
{"title": "learning to navigate by growing deep networks", "abstract": "adaptability is central to autonomy . intuitively , for high-dimensional learning problems such as navigating based on vision , internal models with higher complexity allow to accurately encode the information available . however , most learning methods rely on models with a fixed structure and complexity . in this paper , we present a self-supervised framework for robots to learn to navigate , without any prior knowledge of the environment , by incrementally building the structure of a deep network as new data becomes available . our framework captures images from a monocular camera and self labels the images to continuously train and predict actions from a computationally efficient adaptive deep architecture based on autoencoders ( ae ) , in a self-supervised fashion . the deep architecture , named reinforced adaptive denoising autoencoders ( ra-dae ) , uses reinforcement learning to dynamically change the network structure by adding or removing neurons . experiments were conducted in simulation and real-world indoor and outdoor environments to assess the potential of self-supervised navigation . ra-dae demonstrates better performance than equivalent non-adaptive deep learning alternatives and can continue to expand its knowledge , trading-off past and present information .", "topics": ["computational complexity theory", "reinforcement learning"]}
{"title": "why do linear svms trained on hog features perform so well ?", "abstract": "linear support vector machines trained on hog features are now a de facto standard across many visual perception tasks . their popularisation can largely be attributed to the step-change in performance they brought to pedestrian detection , and their subsequent successes in deformable parts models . this paper explores the interactions that make the hog-svm symbiosis perform so well . by connecting the feature extraction and learning processes rather than treating them as disparate plugins , we show that hog features can be viewed as doing two things : ( i ) inducing capacity in , and ( ii ) adding prior to a linear svm trained on pixels . from this perspective , preserving second-order statistics and locality of interactions are key to good performance . we demonstrate surprising accuracy on expression recognition and pedestrian detection tasks , by assuming only the importance of preserving such local second-order interactions .", "topics": ["support vector machine", "feature extraction"]}
{"title": "fast orthonormal sparsifying transforms based on householder reflectors", "abstract": "dictionary learning is the task of determining a data-dependent transform that yields a sparse representation of some observed data . the dictionary learning problem is non-convex , and usually solved via computationally complex iterative algorithms . furthermore , the resulting transforms obtained generally lack structure that permits their fast application to data . to address this issue , this paper develops a framework for learning orthonormal dictionaries which are built from products of a few householder reflectors . two algorithms are proposed to learn the reflector coefficients : one that considers a sequential update of the reflectors and one with a simultaneous update of all reflectors that imposes an additional internal orthogonal constraint . the proposed methods have low computational complexity and are shown to converge to local minimum points which can be described in terms of the spectral properties of the matrices involved . the resulting dictionaries balance between the computational complexity and the quality of the sparse representations by controlling the number of householder reflectors in their product . simulations of the proposed algorithms are shown in the image processing setting where well-known fast transforms are available for comparisons . the proposed algorithms have favorable reconstruction error and the advantage of a fast implementation relative to the classical , unstructured , dictionaries .", "topics": ["image processing", "computational complexity theory"]}
{"title": "analysis of fast alternating minimization for structured dictionary learning", "abstract": "methods exploiting sparsity have been popular in imaging and signal processing applications including compression , denoising , and imaging inverse problems . data-driven approaches such as dictionary learning and transform learning enable one to discover complex image features from datasets and provide promising performance over analytical models . alternating minimization algorithms have been particularly popular in dictionary or transform learning . in this work , we study the properties of alternating minimization for structured ( unitary ) sparsifying operator learning . while the algorithm converges to the stationary points of the non-convex problem in general , we prove rapid local linear convergence to the underlying generative model under mild assumptions . our experiments show that the unitary operator learning algorithm is robust to initialization .", "topics": ["generative model", "noise reduction"]}
{"title": "soccer league optimization : a heuristic algorithm inspired by the football system in european countries", "abstract": "in this paper a new heuristic optimization algorithm has been introduced based on the performance of the major football leagues within each season in eu countries . the algorithm starts with an initial population including three different groups of teams : the wealthiest ( strongest ) , the regular , the poorest ( weakest ) . each individual of population constitute a football team while each player is an indication of a player in a post . the optimization can hopefully occurs when the competition among the teams in all the leagues is imitated as the strongest teams usually purchase the best players of the regular teams and in turn , regular teams purchase the best players of the weakest who should always discover young players instead of buying professionals . it has been shown that the algorithm can hopefully converge to an acceptable solution solving various benchmarks . key words : heuristic algorithms", "topics": ["mathematical optimization", "heuristic"]}
{"title": "complex networks and human language", "abstract": "this paper introduces how human languages can be studied in light of recent development of network theories . there are two directions of exploration . one is to study networks existing in the language system . various lexical networks can be built based on different relationships between words , being semantic or syntactic . recent studies have shown that these lexical networks exhibit small-world and scale-free features . the other direction of exploration is to study networks of language users ( i.e . social networks of people in the linguistic community ) , and their role in language evolution . social networks also show small-world and scale-free features , which can not be captured by random or regular network models . in the past , computational models of language change and language emergence often assume a population to have a random or regular structure , and there has been little discussion how network structures may affect the dynamics . in the second part of the paper , a series of simulation models of diffusion of linguistic innovation are used to illustrate the importance of choosing realistic conditions of population structure for modeling language change . four types of social networks are compared , which exhibit two categories of diffusion dynamics . while the questions about which type of networks are more appropriate for modeling still remains , we give some preliminary suggestions for choosing the type of social networks for modeling .", "topics": ["natural language", "simulation"]}
{"title": "more accurate tests for the statistical significance of result differences", "abstract": "statistical significance testing of differences in values of metrics like recall , precision and balanced f-score is a necessary part of empirical natural language processing . unfortunately , we find in a set of experiments that many commonly used tests often underestimate the significance and so are less likely to detect differences that exist between different techniques . this underestimation comes from an independence assumption that is often violated . we point out some useful tests that do not make this assumption , including computationally-intensive randomization tests .", "topics": ["natural language processing"]}
{"title": "improved memory-bounded dynamic programming for decentralized pomdps", "abstract": "memory-bounded dynamic programming ( mbdp ) has proved extremely effective in solving decentralized pomdps with large horizons . we generalize the algorithm and improve its scalability by reducing the complexity with respect to the number of observations from exponential to polynomial . we derive error bounds on solution quality with respect to this new approximation and analyze the convergence behavior . to evaluate the effectiveness of the improvements , we introduce a new , larger benchmark problem . experimental results show that despite the high complexity of decentralized pomdps , scalable solution techniques such as mbdp perform surprisingly well .", "topics": ["time complexity", "scalability"]}
{"title": "unsupervised methods for determining object and relation synonyms on the web", "abstract": "the task of identifying synonymous relations and objects , or synonym resolution , is critical for high-quality information extraction . this paper investigates synonym resolution in the context of unsupervised information extraction , where neither hand-tagged training examples nor domain knowledge is available . the paper presents a scalable , fully-implemented system that runs in o ( kn log n ) time in the number of extractions , n , and the maximum number of synonyms per word , k. the system , called resolver , introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them . on a set of two million assertions extracted from the web , resolver resolves objects with 78 % precision and 68 % recall , and resolves relations with 90 % precision and 35 % recall . several variations of resolvers probabilistic model are explored , and experiments demonstrate that under appropriate conditions these variations can improve f1 by 5 % . an extension to the basic resolver system allows it to handle polysemous names with 97 % precision and 95 % recall on a data set from the trec corpus .", "topics": ["unsupervised learning", "scalability"]}
{"title": "adaptive seeding for gaussian mixture models", "abstract": "we present new initialization methods for the expectation-maximization algorithm for multivariate gaussian mixture models . our methods are adaptions of the well-known $ k $ -means++ initialization and the gonzalez algorithm . thereby we aim to close the gap between simple random , e.g . uniform , and complex methods , that crucially depend on the right choice of hyperparameters . our extensive experiments indicate the usefulness of our methods compared to common techniques and methods , which e.g . apply the original $ k $ -means++ and gonzalez directly , with respect to artificial as well as real-world data sets .", "topics": ["data mining"]}
{"title": "shared representation learning for heterogeneous face recognition", "abstract": "after intensive research , heterogenous face recognition is still a challenging problem . the main difficulties are owing to the complex relationship between heterogenous face image spaces . the heterogeneity is always tightly coupled with other variations , which makes the relationship of heterogenous face images highly nonlinear . many excellent methods have been proposed to model the nonlinear relationship , but they apt to overfit to the training set , due to limited samples . inspired by the unsupervised algorithms in deep learning , this paper proposes an novel framework for heterogeneous face recognition . we first extract gabor features at some localized facial points , and then use restricted boltzmann machines ( rbms ) to learn a shared representation locally to remove the heterogeneity around each facial point . finally , the shared representations of local rbms are connected together and processed by pca . two problems ( sketch-photo and nir-vis ) and three databases are selected to evaluate the proposed method . for sketch-photo problem , we obtain perfect results on the cufs database . for nir-vis problem , we produce new state-of-the-art performance on the casia hfb and nir-vis 2.0 databases .", "topics": ["unsupervised learning", "nonlinear system"]}
{"title": "identification of relevant subtypes via preweighted sparse clustering", "abstract": "cluster analysis methods are used to identify homogeneous subgroups in a data set . in biomedical applications , one frequently applies cluster analysis in order to identify biologically interesting subgroups . in particular , one may wish to identify subgroups that are associated with a particular outcome of interest . conventional clustering methods generally do not identify such subgroups , particularly when there are a large number of high-variance features in the data set . conventional methods may identify clusters associated with these high-variance features when one wishes to obtain secondary clusters that are more interesting biologically or more strongly associated with a particular outcome of interest . a modification of sparse clustering can be used to identify such secondary clusters or clusters associated with an outcome of interest . this method correctly identifies such clusters of interest in several simulation scenarios . the method is also applied to a large prospective cohort study of temporomandibular disorders and a leukemia microarray data set .", "topics": ["cluster analysis", "simulation"]}
{"title": "unregularized online learning algorithms with general loss functions", "abstract": "in this paper , we consider unregularized online learning algorithms in a reproducing kernel hilbert spaces ( rkhs ) . firstly , we derive explicit convergence rates of the unregularized online learning algorithms for classification associated with a general gamma-activating loss ( see definition 1 in the paper ) . our results extend and refine the results in ying and pontil ( 2008 ) for the least-square loss and the recent result in bach and moulines ( 2011 ) for the loss function with a lipschitz-continuous gradient . moreover , we establish a very general condition on the step sizes which guarantees the convergence of the last iterate of such algorithms . secondly , we establish , for the first time , the convergence of the unregularized pairwise learning algorithm with a general loss function and derive explicit rates under the assumption of polynomially decaying step sizes . concrete examples are used to illustrate our main results . the main techniques are tools from convex analysis , refined inequalities of gaussian averages , and an induction approach .", "topics": ["loss function", "gradient descent"]}
{"title": "information bottleneck in control tasks with recurrent spiking neural networks", "abstract": "the nervous system encodes continuous information from the environment in the form of discrete spikes , and then decodes these to produce smooth motor actions . understanding how spikes integrate , represent , and process information to produce behavior is one of the greatest challenges in neuroscience . information theory has the potential to help us address this challenge . informational analyses of deep and feed-forward artificial neural networks solving static input-output tasks , have led to the proposal of the \\emph { information bottleneck } principle , which states that deeper layers encode more relevant yet minimal information about the inputs . such an analyses on networks that are recurrent , spiking , and perform control tasks is relatively unexplored . here , we present results from a mutual information analysis of a recurrent spiking neural network that was evolved to perform the classic pole-balancing task . our results show that these networks deviate from the \\emph { information bottleneck } principle prescribed for feed-forward networks .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "citlab argus for arabic handwriting", "abstract": "in the recent years it turned out that multidimensional recurrent neural networks ( mdrnn ) perform very well for offline handwriting recognition tasks like the openhart 2013 evaluation dir . with suitable writing preprocessing and dictionary lookup , our argus software completed this task with an error rate of 26.27 % in its primary setup .", "topics": ["recurrent neural network", "computer vision"]}
{"title": "biometric signature processing & recognition using radial basis function network", "abstract": "automatic recognition of signature is a challenging problem which has received much attention during recent years due to its many applications in different fields . signature has been used for long time for verification and authentication purpose . earlier methods were manual but nowadays they are getting digitized . this paper provides an efficient method to signature recognition using radial basis function network . the network is trained with sample images in database . feature extraction is performed before using them for training . for testing purpose , an image is made to undergo rotation-translation-scaling correction and then given to network . the network successfully identifies the original image and gives correct output for stored database images also . the method provides recognition rate of approximately 80 % for 200 samples .", "topics": ["feature extraction", "database"]}
{"title": "the complexity of graph-based reductions for reachability in markov decision processes", "abstract": "we study the never-worse relation ( nwr ) for markov decision processes with an infinite-horizon reachability objective . a state q is never worse than a state p if the maximal probability of reaching the target set of states from p is at most the same value from q , regard- less of the probabilities labelling the transitions . extremal-probability states , end components , and essential states are all special cases of the equivalence relation induced by the nwr . using the nwr , states in the same equivalence class can be collapsed . then , actions leading to sub- optimal states can be removed . we show the natural decision problem associated to computing the nwr is conp-complete . finally , we ex- tend a previously known incomplete polynomial-time iterative algorithm to under-approximate the nwr .", "topics": ["time complexity"]}
{"title": "continuous relaxation of map inference : a nonconvex perspective", "abstract": "in this paper , we study a nonconvex continuous relaxation of map inference in discrete markov random fields ( mrfs ) . we show that for arbitrary mrfs , this relaxation is tight , and a discrete stationary point of it can be easily reached by a simple block coordinate descent algorithm . in addition , we study the resolution of this relaxation using popular gradient methods , and further propose a more effective solution using a multilinear decomposition framework based on the alternating direction method of multipliers ( admm ) . experiments on many real-world problems demonstrate that the proposed admm significantly outperforms other nonconvex relaxation based methods , and compares favorably with state of the art mrf optimization algorithms in different settings .", "topics": ["gradient descent", "gradient"]}
{"title": "forecasting sleep apnea with dynamic network models", "abstract": "dynamic network models ( dnms ) are belief networks for temporal reasoning . the dnm methodology combines techniques from time series analysis and probabilistic reasoning to provide ( 1 ) a knowledge representation that integrates noncontemporaneous and contemporaneous dependencies and ( 2 ) methods for iteratively refining these dependencies in response to the effects of exogenous influences . we use belief-network inference algorithms to perform forecasting , control , and discrete event simulation on dnms . the belief network formulation allows us to move beyond the traditional assumptions of linearity in the relationships among time-dependent variables and of normality in their probability distributions . we demonstrate the dnm methodology on an important forecasting problem in medicine . we conclude with a discussion of how the methodology addresses several limitations found in traditional time series analyses .", "topics": ["time series", "simulation"]}
{"title": "learning the probabilistic structure of cumulative phenomena with suppes-bayes causal networks", "abstract": "one of the critical issues when adopting bayesian networks ( bns ) to model dependencies among random variables is to `` learn '' their structure , given the huge search space of possible solutions , i.e . , all the possible direct acyclic graphs . this is a well-known np-hard problem , which is also complicated by known pitfalls such as the issue of i-equivalence among different structures . in this work we restrict the investigations on bn structure learning to a specific class of networks , i.e . , those representing the dynamics of phenomena characterized by the monotonic accumulation of events . such phenomena allow to set specific structural constraints based on suppes ' theory of probabilistic causation and , accordingly , to define constrained bns , named suppes-bayes causal networks ( sbcns ) . we here investigate the structure learning of sbcns via extensive simulations with various state-of-the-art search strategies , such as canonical local search techniques and genetic algorithms . among the main results we show that suppes ' constraints deeply simplify the learning task , by reducing the solution search space and providing a temporal ordering on the variables .", "topics": ["simulation", "bayesian network"]}
{"title": "gait pattern recognition using accelerometers", "abstract": "motion ability is one of the most important human properties , including gait as a basis of human transitional movement . gait , as a biometric for recognizing human identities , can be non-intrusively captured signals using wearable or portable smart devices . in this study gait patterns is collected using a wireless platform of two sensors located at chest and right ankle of the subjects . then the raw data has undergone some preprocessing methods and segmented into 5 seconds windows . some time and frequency domain features is extracted and the performance evaluated by 5 different classifiers . decision tree ( with all features ) and k-nearest neighbors ( with 10 selected features ) classifiers reached 99.4 % and 100 % respectively .", "topics": ["sensor"]}
{"title": "comparison of training methods for deep neural networks", "abstract": "this report describes the difficulties of training neural networks and in particular deep neural networks . it then provides a literature review of training methods for deep neural networks , with a focus on pre-training . it focuses on deep belief networks composed of restricted boltzmann machines and stacked autoencoders and provides an outreach on further and alternative approaches . it also includes related practical recommendations from the literature on training them . in the second part , initial experiments using some of the covered methods are performed on two databases . in particular , experiments are performed on the mnist hand-written digit dataset and on facial emotion data from a kaggle competition . the results are discussed in the context of results reported in other research papers . an error rate lower than the best contribution to the kaggle competition is achieved using an optimized stacked autoencoder .", "topics": ["neural networks", "bayesian network"]}
{"title": "condorcet 's jury theorem for consensus clustering and its implications for diversity", "abstract": "condorcet 's jury theorem has been invoked for ensemble classifiers to indicate that the combination of many classifiers can have better predictive performance than a single classifier . such a theoretical underpinning is unknown for consensus clustering . this article extends condorcet 's jury theorem to the mean partition approach under the additional assumptions that a unique ground-truth partition exists and sample partitions are drawn from a sufficiently small ball containing the ground-truth . as an implication of practical relevance , we question the claim that the quality of consensus clustering depends on the diversity of the sample partitions . instead , we conjecture that limiting the diversity of the mean partitions is necessary for controlling the quality .", "topics": ["cluster analysis", "relevance"]}
{"title": "inducing domain-specific sentiment lexicons from unlabeled corpora", "abstract": "a word 's sentiment depends on the domain in which it is used . computational social science research thus requires sentiment lexicons that are specific to the domains being studied . we combine domain-specific word embeddings with a label propagation framework to induce accurate domain-specific sentiment lexicons using small sets of seed words , achieving state-of-the-art performance competitive with approaches that rely on hand-curated resources . using our framework we perform two large-scale empirical studies to quantify the extent to which sentiment varies across time and between communities . we induce and release historical sentiment lexicons for 150 years of english and community-specific sentiment lexicons for 250 online communities from the social media forum reddit . the historical lexicons show that more than 5 % of sentiment-bearing ( non-neutral ) english words completely switched polarity during the last 150 years , and the community-specific lexicons highlight how sentiment varies drastically between different communities .", "topics": ["text corpus"]}
{"title": "deepgestalt - identifying rare genetic syndromes using deep learning", "abstract": "facial analysis technologies have recently measured up to the capabilities of expert clinicians in syndrome identification . to date , these technologies could only identify phenotypes of a few diseases , limiting their role in clinical settings where hundreds of diagnoses must be considered . we developed a facial analysis framework , deepgestalt , using computer vision and deep learning algorithms , that quantifies similarities to hundreds of genetic syndromes based on unconstrained 2d images . deepgestalt is currently trained with over 26,000 patient cases from a rapidly growing phenotype-genotype database , consisting of tens of thousands of validated clinical cases , curated through a community-driven platform . deepgestalt currently achieves 91 % top-10-accuracy in identifying over 215 different genetic syndromes and has outperformed clinical experts in three separate experiments . we suggest that this form of artificial intelligence is ready to support medical genetics in clinical and laboratory practices and will play a key role in the future of precision medicine .", "topics": ["computer vision", "artificial intelligence"]}
{"title": "efficient exploration through bayesian deep q-networks", "abstract": "we propose bayesian deep q-network ( bdqn ) , a practical thompson sampling based reinforcement learning ( rl ) algorithm . thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive . we address this limitation by introducing uncertainty only at the output layer of the network through a bayesian linear regression ( blr ) model . this layer can be trained with fast closed-form updates and its samples can be drawn efficiently through the gaussian distribution . we apply our method to a wide range of atari games in arcade learning environments . since bdqn carries out more efficient exploration , it is able to reach higher rewards substantially faster than a key baseline , the double deep q network ( ddqn ) .", "topics": ["sampling ( signal processing )", "baseline ( configuration management )"]}
{"title": "improving a strong neural parser with conjunction-specific features", "abstract": "while dependency parsers reach very high overall accuracy , some dependency relations are much harder than others . in particular , dependency parsers perform poorly in coordination construction ( i.e . , correctly attaching the `` conj '' relation ) . we extend a state-of-the-art dependency parser with conjunction-specific features , focusing on the similarity between the conjuncts head words . training the extended parser yields an improvement in `` conj '' attachment as well as in overall dependency parsing accuracy on the stanford dependency conversion of the penn treebank .", "topics": ["parsing"]}
{"title": "neon2 : finding local minima via first-order oracles", "abstract": "we propose a reduction for non-convex optimization that can ( 1 ) turn an stationary-point finding algorithm into an local-minimum finding one , and ( 2 ) replace the hessian-vector product computations with only gradient computations . it works both in the stochastic and the deterministic settings , without hurting the algorithm 's performance . as applications , our reduction turns natasha2 into a first-order method without hurting its performance . it also converts sgd , gd , scsg , and svrg into algorithms finding approximate local minima , outperforming some best known results .", "topics": ["approximation algorithm", "computation"]}
{"title": "classification with sparse overlapping groups", "abstract": "classification with a sparsity constraint on the solution plays a central role in many high dimensional machine learning applications . in some cases , the features can be grouped together so that entire subsets of features can be selected or not selected . in many applications , however , this can be too restrictive . in this paper , we are interested in a less restrictive form of structured sparse feature selection : we assume that while features can be grouped according to some notion of similarity , not all features in a group need be selected for the task at hand . when the groups are comprised of disjoint sets of features , this is sometimes referred to as the `` sparse group '' lasso , and it allows for working with a richer class of models than traditional group lasso methods . our framework generalizes conventional sparse group lasso further by allowing for overlapping groups , an additional flexiblity needed in many applications and one that presents further challenges . the main contribution of this paper is a new procedure called sparse overlapping group ( sog ) lasso , a convex optimization program that automatically selects similar features for classification in high dimensions . we establish model selection error bounds for soglasso classification problems under a fairly general setting . in particular , the error bounds are the first such results for classification using the sparse group lasso . furthermore , the general soglasso bound specializes to results for the lasso and the group lasso , some known and some new . the soglasso is motivated by multi-subject fmri studies in which functional activity is classified using brain voxels as features , source localization problems in magnetoencephalography ( meg ) , and analyzing gene activation patterns in microarray data analysis . experiments with real and synthetic data demonstrate the advantages of soglasso compared to the lasso and group lasso .", "topics": ["synthetic data", "sparse matrix"]}
{"title": "causal learning and explanation of deep neural networks via autoencoded activations", "abstract": "deep neural networks are complex and opaque . as they enter application in a variety of important and safety critical domains , users seek methods to explain their output predictions . we develop an approach to explaining deep neural networks by constructing causal models on salient concepts contained in a cnn . we develop methods to extract salient concepts throughout a target network by using autoencoders trained to extract human-understandable representations of network activations . we then build a bayesian causal model using these extracted concepts as variables in order to explain image classification . finally , we use this causal model to identify and visualize features with significant causal influence on final classification .", "topics": ["neural networks", "computer vision"]}
{"title": "multi-instance dynamic ordinal random fields for weakly-supervised facial behavior analysis", "abstract": "we propose a multi-instance-learning ( mil ) approach for weakly-supervised learning problems , where a training set is formed by bags ( sets of feature vectors or instances ) and only labels at bag-level are provided . specifically , we consider the multi-instance dynamic-ordinal-regression ( mi-dor ) setting , where the instance labels are naturally represented as ordinal variables and bags are structured as temporal sequences . to this end , we propose multi-instance dynamic ordinal random fields ( mi-dorf ) . in this framework , we treat instance-labels as temporally-dependent latent variables in an undirected graphical model . different mil assumptions are modelled via newly introduced high-order potentials relating bag and instance-labels within the energy function of the model . we also extend our framework to address the partially-observed mi-dor problems , where a subset of instance labels are available during training . we show on the tasks of weakly-supervised facial behavior analysis , facial action unit ( disfa dataset ) and pain ( unbc dataset ) intensity estimation , that the proposed framework outperforms alternative learning approaches . furthermore , we show that midorf can be employed to reduce the data annotation efforts in this context by large-scale .", "topics": ["test set", "graphical model"]}
{"title": "the variational garrote", "abstract": "in this paper , we present a new variational method for sparse regression using $ l_0 $ regularization . the variational parameters appear in the approximate model in a way that is similar to breiman 's garrote model . we refer to this method as the variational garrote ( vg ) . we show that the combination of the variational approximation and $ l_0 $ regularization has the effect of making the problem effectively of maximal rank even when the number of samples is small compared to the number of variables . the vg is compared numerically with the lasso method , ridge regression and the recently introduced paired mean field method ( pmf ) ( m. titsias & m . l\\'azaro-gredilla . , nips 2012 ) . numerical results show that the vg and pmf yield more accurate predictions and more accurately reconstruct the true model than the other methods . it is shown that the vg finds correct solutions when the lasso solution is inconsistent due to large input correlations . globally , vg is significantly faster than pmf and tends to perform better as the problems become denser and in problems with strongly correlated inputs . the naive implementation of the vg scales cubic with the number of features . by introducing lagrange multipliers we obtain a dual formulation of the problem that scales cubic in the number of samples , but close to linear in the number of features .", "topics": ["calculus of variations", "matrix regularization"]}
{"title": "a non-technical survey on deep convolutional neural network architectures", "abstract": "artificial neural networks have recently shown great results in many disciplines and a variety of applications , including natural language understanding , speech processing , games and image data generation . one particular application in which the strong performance of artificial neural networks was demonstrated is the recognition of objects in images , where deep convolutional neural networks are commonly applied . in this survey , we give a comprehensive introduction to this topic ( object recognition with deep convolutional neural networks ) , with a strong focus on the evolution of network architectures . therefore , we aim to compress the most important concepts in this field in a simple and non-technical manner to allow for future researchers to have a quick general understanding . this work is structured as follows : 1 . we will explain the basic ideas of ( convolutional ) neural networks and deep learning and examine their usage for three object recognition tasks : image classification , object localization and object detection . 2 . we give a review on the evolution of deep convolutional neural networks by providing an extensive overview of the most important network architectures presented in chronological order of their appearances .", "topics": ["object detection", "computer vision"]}
{"title": "range loss for deep face recognition with long-tail", "abstract": "convolutional neural networks have achieved great improvement on face recognition in recent years because of its extraordinary ability in learning discriminative features of people with different identities . to train such a well-designed deep network , tremendous amounts of data is indispensable . long tail distribution specifically refers to the fact that a small number of generic entities appear frequently while other objects far less existing . considering the existence of long tail distribution of the real world data , large but uniform distributed data are usually hard to retrieve . empirical experiences and analysis show that classes with more samples will pose greater impact on the feature learning process and inversely cripple the whole models feature extracting ability on tail part data . contrary to most of the existing works that alleviate this problem by simply cutting the tailed data for uniform distributions across the classes , this paper proposes a new loss function called range loss to effectively utilize the whole long tailed data in training process . more specifically , range loss is designed to reduce overall intra-personal variations while enlarging inter-personal differences within one mini-batch simultaneously when facing even extremely unbalanced data . the optimization objective of range loss is the $ k $ greatest range 's harmonic mean values in one class and the shortest inter-class distance within one batch . extensive experiments on two famous and challenging face recognition benchmarks ( labeled faces in the wild ( lfw ) and youtube faces ( ytf ) not only demonstrate the effectiveness of the proposed approach in overcoming the long tail effect but also show the good generalization ability of the proposed approach .", "topics": ["feature learning", "mathematical optimization"]}
{"title": "insights from classifying visual concepts with multiple kernel learning", "abstract": "combining information from various image features has become a standard technique in concept recognition tasks . however , the optimal way of fusing the resulting kernel functions is usually unknown in practical applications . multiple kernel learning ( mkl ) techniques allow to determine an optimal linear combination of such similarity matrices . classical approaches to mkl promote sparse mixtures . unfortunately , so-called 1-norm mkl variants are often observed to be outperformed by an unweighted sum kernel . the contribution of this paper is twofold : we apply a recently developed non-sparse mkl variant to state-of-the-art concept recognition tasks within computer vision . we provide insights on benefits and limits of non-sparse mkl and compare it against its direct competitors , the sum kernel svm and the sparse mkl . we report empirical results for the pascal voc 2009 classification and imageclef2010 photo annotation challenge data sets . about to be submitted to plos one .", "topics": ["kernel ( operating system )", "computer vision"]}
{"title": "deepiso : a deep learning model for peptide feature detection", "abstract": "liquid chromatography with tandem mass spectrometry ( lc-ms/ms ) based proteomics is a well-established research field with major applications such as identification of disease biomarkers , drug discovery , drug design and development . in proteomics , protein identification and quantification is a fundamental task , which is done by first enzymatically digesting it into peptides , and then analyzing peptides by lc-ms/ms instruments . the peptide feature detection and quantification from an lc-ms map is the first step in typical analysis workflows . in this paper we propose a novel deep learning based model , deepiso , that uses convolutional neural networks ( cnns ) to scan an lc-ms map to detect peptide features and estimate their abundance . existing tools are often designed with limited engineered features based on domain knowledge , and depend on pretrained parameters which are hardly updated despite huge amount of new coming proteomic data . our proposed model , on the other hand , is capable of learning multiple levels of representation of high dimensional data through its many layers of neurons and continuously evolving with newly acquired data . to evaluate our proposed model , we use an antibody dataset including a heavy and a light chain , each digested by asp-n , chymotrypsin , trypsin , thus giving six lc-ms maps for the experiment . our model achieves 93.21 % sensitivity with specificity of 99.44 % on this dataset . our results demonstrate that novel deep learning tools are desirable to advance the state-of-the-art in protein identification and quantification .", "topics": ["map"]}
{"title": "sparse-posterior gaussian processes for general likelihoods", "abstract": "gaussian processes ( gps ) provide a probabilistic nonparametric representation of functions in regression , classification , and other problems . unfortunately , exact learning with gps is intractable for large datasets . a variety of approximate gp methods have been proposed that essentially map the large dataset into a small set of basis points . among them , two state-of-the-art methods are sparse pseudo-input gaussian process ( spgp ) ( snelson and ghahramani , 2006 ) and variablesigma gp ( vsgp ) walder et al . ( 2008 ) , which generalizes spgp and allows each basis point to have its own length scale . however , vsgp was only derived for regression . in this paper , we propose a new sparse gp framework that uses expectation propagation to directly approximate general gp likelihoods using a sparse and smooth basis . it includes both spgp and vsgp for regression as special cases . plus as an ep algorithm , it inherits the ability to process data online . as a particular choice of approximating family , we blur each basis point with a gaussian distribution that has a full covariance matrix representing the data distribution around that basis point ; as a result , we can summarize local data manifold information with a small set of basis points . our experiments demonstrate that this framework outperforms previous gp classification methods on benchmark datasets in terms of minimizing divergence to the non-sparse gp solution as well as lower misclassification rate .", "topics": ["approximation algorithm", "statistical classification"]}
{"title": "dd-eba : an algorithm for determining the number of neighbors in cost estimation by analogy using distance distributions", "abstract": "case based reasoning and particularly estimation by analogy , has been used in a number of problem-solving areas , such as cost estimation . conventional methods , despite the lack of a sound criterion for choosing nearest projects , were based on estimation using a fixed and predetermined number of neighbors from the entire set of historical instances . this approach puts boundaries to the estimation ability of such algorithms , for they do not take into consideration that every project under estimation is unique and requires different handling . the notion of distributions of distances together with a distance metric for distributions help us to adapt the proposed method ( we call it dd-eba ) each time to a specific case that is to be estimated without loosing in prediction power or computational cost . the results of this paper show that the proposed technique achieves the above idea in a very efficient way .", "topics": ["eisenstein 's criterion"]}
{"title": "efficient recurrent neural networks using structured matrices in fpgas", "abstract": "recurrent neural networks ( rnns ) are becoming increasingly important for time series-related applications which require efficient and real-time implementations . the recent pruning based work ese suffers from degradation of performance/energy efficiency due to the irregular network structure after pruning . we propose block-circulant matrices for weight matrix representation in rnns , thereby achieving simultaneous model compression and acceleration . we aim to implement rnns in fpga with highest performance and energy efficiency , with certain accuracy requirement ( negligible accuracy degradation ) . experimental results on actual fpga deployments shows that the proposed framework achieves a maximum energy efficiency improvement of 35.7 $ \\times $ compared with ese .", "topics": ["recurrent neural network", "time series"]}
{"title": "implementation and comparison of solution methods for decision processes with non-markovian rewards", "abstract": "this paper examines a number of solution methods for decision processes with non-markovian rewards ( nmrdps ) . they all exploit a temporal logic specification of the reward function to automatically translate the nmrdp into an equivalent markov decision process ( mdp ) amenable to well-known mdp solution methods . they differ however in the representation of the target mdp and the class of mdp solution methods to which they are suited . as a result , they adopt different temporal logics and different translations . unfortunately , no implementation of these methods nor experimental let alone comparative results have ever been reported . this paper is the first step towards filling this gap . we describe an integrated system for solving nmrdps which implements these methods and several variants under a common interface ; we use it to compare the various approaches and identify the problem features favoring one over the other .", "topics": ["reinforcement learning"]}
{"title": "semi-supervised bayesian deep multi-modal emotion recognition", "abstract": "in emotion recognition , it is difficult to recognize human 's emotional states using just a single modality . besides , the annotation of physiological emotional data is particularly expensive . these two aspects make the building of effective emotion recognition model challenging . in this paper , we first build a multi-view deep generative model to simulate the generative process of multi-modality emotional data . by imposing a mixture of gaussians assumption on the posterior approximation of the latent variables , our model can learn the shared deep representation from multiple modalities . to solve the labeled-data-scarcity problem , we further extend our multi-view model to semi-supervised learning scenario by casting the semi-supervised classification problem as a specialized missing data imputation task . our semi-supervised multi-view deep generative framework can leverage both labeled and unlabeled data from multiple modalities , where the weight factor for each modality can be learned automatically . compared with previous emotion recognition methods , our method is more robust and flexible . the experiments conducted on two real multi-modal emotion datasets have demonstrated the superiority of our framework over a number of competitors .", "topics": ["generative model", "supervised learning"]}
{"title": "ajile movement prediction : multimodal deep learning for natural human neural recordings and video", "abstract": "developing useful interfaces between brains and machines is a grand challenge of neuroengineering . an effective interface has the capacity to not only interpret neural signals , but predict the intentions of the human to perform an action in the near future ; prediction is made even more challenging outside well-controlled laboratory experiments . this paper describes our approach to detect and to predict natural human arm movements in the future , a key challenge in brain computer interfacing that has never before been attempted . we introduce the novel annotated joints in long-term ecog ( ajile ) dataset ; ajile includes automatically annotated poses of 7 upper body joints for four human subjects over 670 total hours ( more than 72 million frames ) , along with the corresponding simultaneously acquired intracranial neural recordings . the size and scope of ajile greatly exceeds all previous datasets with movements and electrocorticography ( ecog ) , making it possible to take a deep learning approach to movement prediction . we propose a multimodal model that combines deep convolutional neural networks ( cnn ) with long short-term memory ( lstm ) blocks , leveraging both ecog and video modalities . we demonstrate that our models are able to detect movements and predict future movements up to 800 msec before movement initiation . further , our multimodal movement prediction models exhibit resilience to simulated ablation of input neural signals . we believe a multimodal approach to natural neural decoding that takes context into account is critical in advancing bioelectronic technologies and human neuroscience .", "topics": ["simulation"]}
{"title": "testing visual attention in dynamic environments", "abstract": "we investigate attention as the active pursuit of useful information . this contrasts with attention as a mechanism for the attenuation of irrelevant information . we also consider the role of short-term memory , whose use is critical to any model incapable of simultaneously perceiving all information on which its output depends . we present several simple synthetic tasks , which become considerably more interesting when we impose strong constraints on how a model can interact with its input , and on how long it can take to produce its output . we develop a model with a different structure from those seen in previous work , and we train it using stochastic variational inference with a learned proposal distribution .", "topics": ["calculus of variations", "synthetic data"]}
{"title": "sentiment identification in code-mixed social media text", "abstract": "sentiment analysis is the natural language processing ( nlp ) task dealing with the detection and classification of sentiments in texts . while some tasks deal with identifying the presence of sentiment in the text ( subjectivity analysis ) , other tasks aim at determining the polarity of the text categorizing them as positive , negative and neutral . whenever there is a presence of sentiment in the text , it has a source ( people , group of people or any entity ) and the sentiment is directed towards some entity , object , event or person . sentiment analysis tasks aim to determine the subject , the target and the polarity or valence of the sentiment . in our work , we try to automatically extract sentiment ( positive or negative ) from facebook posts using a machine learning approach.while some works have been done in code-mixed social media data and in sentiment analysis separately , our work is the first attempt ( as of now ) which aims at performing sentiment analysis of code-mixed social media text . we have used extensive pre-processing to remove noise from raw text . multilayer perceptron model has been used to determine the polarity of the sentiment . we have also developed the corpus for this task by manually labeling facebook posts with their associated sentiments .", "topics": ["natural language processing", "natural language"]}
{"title": "improvements to context based self-supervised learning", "abstract": "we develop a set of methods to improve on the results of self-supervised learning using context . we start with a baseline of patch based arrangement context learning and go from there . our methods address some overt problems such as chromatic aberration as well as other potential problems such as spatial skew and mid-level feature neglect . we prevent problems with testing generalization on common self-supervised benchmark tests by using different datasets during our development . the results of our methods combined yield top scores on all standard self-supervised benchmarks , including classification and detection on pascal voc 2007 , segmentation on pascal voc 2012 , and `` linear tests '' on the imagenet and csail places datasets . we obtain an improvement over our baseline method of between 4.0 to 7.1 percentage points on transfer learning classification tests . we also show results on different standard network architectures to demonstrate generalization as well as portability .", "topics": ["baseline ( configuration management )", "supervised learning"]}
{"title": "visualizing and exploring dynamic high-dimensional datasets with lion-tsne", "abstract": "t-distributed stochastic neighbor embedding ( tsne ) is a popular and prize-winning approach for dimensionality reduction and visualizing high-dimensional data . however , tsne is non-parametric : once visualization is built , tsne is not designed to incorporate additional data into existing representation . it highly limits the applicability of tsne to the scenarios where data are added or updated over time ( like dashboards or series of data snapshots ) . in this paper we propose , analyze and evaluate lion-tsne ( local interpolation with outlier control ) - a novel approach for incorporating new data into tsne representation . lion-tsne is based on local interpolation in the vicinity of training data , outlier detection and a special outlier mapping algorithm . we show that lion-tsne method is robust both to outliers and to new samples from existing clusters . we also discuss multiple possible improvements for special cases . we compare lion-tsne to a comprehensive list of possible benchmark approaches that include multiple interpolation techniques , gradient descent for new data , and neural network approximation .", "topics": ["test set", "gradient descent"]}
{"title": "chexnet : radiologist-level pneumonia detection on chest x-rays with deep learning", "abstract": "we develop an algorithm that can detect pneumonia from chest x-rays at a level exceeding practicing radiologists . our algorithm , chexnet , is a 121-layer convolutional neural network trained on chestx-ray14 , currently the largest publicly available chest x-ray dataset , containing over 100,000 frontal-view x-ray images with 14 diseases . four practicing academic radiologists annotate a test set , on which we compare the performance of chexnet to that of radiologists . we find that chexnet exceeds average radiologist performance on the f1 metric . we extend chexnet to detect all 14 diseases in chestx-ray14 and achieve state of the art results on all 14 diseases .", "topics": ["test set"]}
{"title": "better text understanding through image-to-text transfer", "abstract": "generic text embeddings are successfully used in a variety of tasks . however , they are often learnt by capturing the co-occurrence structure from pure text corpora , resulting in limitations of their ability to generalize . in this paper , we explore models that incorporate visual information into the text representation . based on comprehensive ablation studies , we propose a conceptually simple , yet well performing architecture . it outperforms previous multimodal approaches on a set of well established benchmarks . we also improve the state-of-the-art results for image-related text datasets , using orders of magnitude less data .", "topics": ["text corpus"]}
{"title": "interpretable deep convolutional neural networks via meta-learning", "abstract": "model interpretability is a requirement in many applications in which crucial decisions are made by users relying on a model 's outputs . the recent movement for `` algorithmic fairness '' also stipulates explainability , and therefore interpretability of learning models . and yet the most successful contemporary machine learning approaches , the deep neural networks , produce models that are highly non-interpretable . we attempt to address this challenge by proposing a technique called cnn-inte to interpret deep convolutional neural networks ( cnn ) via meta-learning . in this work , we interpret a specific hidden layer of the deep cnn model on the mnist image dataset . we use a clustering algorithm in a two-level structure to find the meta-level training data and random forest as base learning algorithms to generate the meta-level test data . the interpretation results are displayed visually via diagrams , which clearly indicates how a specific test instance is classified . our method achieves global interpretation for all the test instances without sacrificing the accuracy obtained by the original deep cnn model . this means our model is faithful to the deep cnn model , which leads to reliable interpretations .", "topics": ["test set", "cluster analysis"]}
{"title": "an experiment on using bayesian networks for process mining", "abstract": "process mining is a technique that performs an automatic analysis of business processes from a log of events with the promise of understanding how processes are executed in an organisation . several models have been proposed to address this problem , however , here we propose a different approach to deal with uncertainty . by uncertainty , we mean estimating the probability of some sequence of tasks occurring in a business process , given that only a subset of tasks may be observable . in this sense , this work proposes a new approach to perform process mining using bayesian networks . these structures can take into account the probability of a task being present or absent in the business process . moreover , bayesian networks are able to automatically learn these probabilities through mechanisms such as the maximum likelihood estimate and em clustering . experiments made over a loan application case study suggest that bayesian networks are adequate structures for process mining and enable a deep analysis of the business process model that can be used to answer queries about that process .", "topics": ["cluster analysis", "bayesian network"]}
{"title": "biomedical term normalization of ehrs with umls", "abstract": "this paper presents a novel prototype for biomedical term normalization of electronic health record excerpts with the unified medical language system ( umls ) metathesaurus . despite being multilingual and cross-lingual by design , we first focus on processing clinical text in spanish because there is no existing tool for this language and for this specific purpose . the tool is based on apache lucene to index the metathesaurus and generate mapping candidates from input text . it uses the ixa pipeline for basic language processing and resolves ambiguities with the ukb toolkit . it has been evaluated by measuring its agreement with metamap in two english-spanish parallel corpora . in addition , we present a web-based interface for the tool .", "topics": ["text corpus"]}
{"title": "label noise reduction in entity typing by heterogeneous partial-label embedding", "abstract": "current systems of fine-grained entity typing use distant supervision in conjunction with existing knowledge bases to assign categories ( type labels ) to entity mentions . however , the type labels so obtained from knowledge bases are often noisy ( i.e . , incorrect for the entity mention 's local context ) . we define a new task , label noise reduction in entity typing ( lnr ) , to be the automatic identification of correct type labels ( type-paths ) for training examples , given the set of candidate type labels obtained by distant supervision with a given type hierarchy . the unknown type labels for individual entity mentions and the semantic similarity between entity types pose unique challenges for solving the lnr task . we propose a general framework , called ple , to jointly embed entity mentions , text features and entity types into the same low-dimensional space where , in that space , objects whose types are semantically close have similar representations . then we estimate the type-path for each training example in a top-down manner using the learned embeddings . we formulate a global objective for learning the embeddings from text corpora and knowledge bases , which adopts a novel margin-based loss that is robust to noisy labels and faithfully models type correlation derived from knowledge bases . our experiments on three public typing datasets demonstrate the effectiveness and robustness of ple , with an average of 25 % improvement in accuracy compared to next best method .", "topics": ["noise reduction", "text corpus"]}
{"title": "ubiquitous wlan/camera positioning using inverse intensity chromaticity space-based feature detection and matching : a preliminary result", "abstract": "this paper present our new intensity chromaticity space-based feature detection and matching algorithm . this approach utilizes hybridization of wireless local area network and camera internal sensor which to receive signal strength from a access point and the same time retrieve interest point information from hallways . this information is combined by model fitting approach in order to find the absolute of user target position . no conventional searching algorithm is required , thus it is expected reducing the computational complexity . finally we present pre-experimental results to illustrate the performance of the localization system for an indoor environment set-up .", "topics": ["computational complexity theory"]}
{"title": "image encryption with dynamic chaotic look-up table", "abstract": "in this paper we propose a novel image encryption scheme . the proposed method is based on the chaos theory . our cryptosystem uses the chaos theory to define a dynamic chaotic look-up table ( lut ) to compute the new value of the current pixel to cipher . applying this process on each pixel of the plain image , we generate the encrypted image . the results of different experimental tests , such as key space analysis , information entropy and histogram analysis , show that the proposed encryption image scheme seems to be protected against various attacks . a comparison between the plain and encrypted image , in terms of correlation coefficient , proves that the plain image is very different from the encrypted one .", "topics": ["pixel", "coefficient"]}
{"title": "statistical analysis based hypothesis testing method in biological knowledge discovery", "abstract": "the correlation and interactions among different biological entities comprise the biological system . although already revealed interactions contribute to the understanding of different existing systems , researchers face many questions everyday regarding inter-relationships among entities . their queries have potential role in exploring new relations which may open up a new area of investigation . in this paper , we introduce a text mining based method for answering the biological queries in terms of statistical computation such that researchers can come up with new knowledge discovery . it facilitates user to submit their query in natural linguistic form which can be treated as hypothesis . our proposed approach analyzes the hypothesis and measures the p-value of the hypothesis with respect to the existing literature . based on the measured value , the system either accepts or rejects the hypothesis from statistical point of view . moreover , even it does not find any direct relationship among the entities of the hypothesis , it presents a network to give an integral overview of all the entities through which the entities might be related . this is also congenial for the researchers to widen their view and thus think of new hypothesis for further investigation . it assists researcher to get a quantitative evaluation of their assumptions such that they can reach a logical conclusion and thus aids in relevant re-searches of biological knowledge discovery . the system also provides the researchers a graphical interactive interface to submit their hypothesis for assessment in a more convenient way .", "topics": ["interaction", "entity"]}
{"title": "multifaceted feature visualization : uncovering the different types of features learned by each neuron in deep neural networks", "abstract": "we can better understand deep neural networks by identifying which features each of their neurons have learned to detect . to do so , researchers have created deep visualization techniques including activation maximization , which synthetically generates inputs ( e.g . images ) that maximally activate each neuron . a limitation of current techniques is that they assume each neuron detects only one type of feature , but we know that neurons can be multifaceted , in that they fire in response to many different types of features : for example , a grocery store class neuron must activate either for rows of produce or for a storefront . previous activation maximization techniques constructed images without regard for the multiple different facets of a neuron , creating inappropriate mixes of colors , parts of objects , scales , orientations , etc . here , we introduce an algorithm that explicitly uncovers the multiple facets of each neuron by producing a synthetic visualization of each of the types of images that activate a neuron . we also introduce regularization methods that produce state-of-the-art results in terms of the interpretability of images obtained by activation maximization . by separately synthesizing each type of image a neuron fires in response to , the visualizations have more appropriate colors and coherent global structure . multifaceted feature visualization thus provides a clearer and more comprehensive description of the role of each neuron .", "topics": ["synthetic data", "neural networks"]}
{"title": "fssd : feature fusion single shot multibox detector", "abstract": "ssd ( single shot multibox detetor ) is one of the best object detection algorithms with both high accuracy and fast speed . however , ssd 's feature pyramid detection method makes it hard to fuse the features from different scales . in this paper , we proposed fssd ( feature fusion single shot multibox detector ) , an enhanced ssd with a novel and lightweight feature fusion module which can improve the performance significantly over ssd with just a little speed drop . in the feature fusion module , features from different layers with different scales are concatenated together , followed by some down-sampling blocks to generate new feature pyramid , which will be fed to multibox detectors to predict the final detection results . on the pascal voc 2007 test , our network can achieve 82.7 map ( mean average precision ) at the speed of 65.8 fps ( frame per second ) with the input size 300 $ \\times $ 300 using a single nvidia 1080ti gpu . in addition , our result on coco is also better than the conventional ssd with a large margin . our fssd outperforms a lot of state-of-the-art object detection algorithms in both aspects of accuracy and speed . code is available at https : //github.com/lzx1413/caffe_ssd/tree/fssd .", "topics": ["sampling ( signal processing )", "object detection"]}
{"title": "a bayesian method for constructing bayesian belief networks from databases", "abstract": "this paper presents a bayesian method for constructing bayesian belief networks from a database of cases . potential applications include computer-assisted hypothesis testing , automated scientific discovery , and automated construction of probabilistic expert systems . results are presented of a preliminary evaluation of an algorithm for constructing a belief network from a database of cases . we relate the methods in this paper to previous work , and we discuss open problems .", "topics": ["bayesian network"]}
{"title": "hierarchical clustering for finding symmetries and other patterns in massive , high dimensional datasets", "abstract": "data analysis and data mining are concerned with unsupervised pattern finding and structure determination in data sets . `` structure '' can be understood as symmetry and a range of symmetries are expressed by hierarchy . such symmetries directly point to invariants , that pinpoint intrinsic properties of the data and of the background empirical domain of interest . we review many aspects of hierarchy here , including ultrametric topology , generalized ultrametric , linkages with lattices and other discrete algebraic structures and with p-adic number representations . by focusing on symmetries in data we have a powerful means of structuring and analyzing massive , high dimensional data stores . we illustrate the powerfulness of hierarchical clustering in case studies in chemistry and finance , and we provide pointers to other published case studies .", "topics": ["data mining", "cluster analysis"]}
{"title": "learning criticality in an embodied boltzmann machine", "abstract": "many biological and cognitive systems do not operate deep into one or other regime of activity . instead , they exploit critical surfaces poised at transitions in their parameter space . the pervasiveness of criticality in natural systems suggests that there may be general principles inducing this behaviour . however , there is a lack of conceptual models explaining how embodied agents propel themselves towards these critical points . in this paper , we present a learning model driving an embodied boltzmann machine towards critical behaviour by maximizing the heat capacity of the network . we test and corroborate the model implementing an embodied agent in the mountain car benchmark , controlled by a boltzmann machine that adjust its weights according to the model . we find that the neural controller reaches a point of criticality , which coincides with a transition point of the behaviour of the agent between two regimes of behaviour , maximizing the synergistic information between its sensors and the hidden and motor neurons . finally , we discuss the potential of our learning model to study the contribution of criticality to the behaviour of embodied living systems in scenarios not necessarily constrained by biological restrictions of the examples of criticality we find in nature .", "topics": ["artificial intelligence", "sensor"]}
{"title": "on the existence and multiplicity of extensions in dialectical argumentation", "abstract": "in the present paper , the existence and multiplicity problems of extensions are addressed . the focus is on extension of the stable type . the main result of the paper is an elegant characterization of the existence and multiplicity of extensions in terms of the notion of dialectical justification , a close cousin of the notion of admissibility . the characterization is given in the context of the particular logic for dialectical argumentation deflog . the results are of direct relevance for several well-established models of defeasible reasoning ( like default logic , logic programming and argumentation frameworks ) , since elsewhere dialectical argumentation has been shown to have close formal connections with these models .", "topics": ["relevance"]}
{"title": "structured sparsity through convex optimization", "abstract": "sparse estimation methods are aimed at using or obtaining parsimonious representations of data or models . while naturally cast as a combinatorial optimization problem , variable or feature selection admits a convex relaxation through the regularization by the $ \\ell_1 $ -norm . in this paper , we consider situations where we are not only interested in sparsity , but where some structural prior knowledge is available as well . we show that the $ \\ell_1 $ -norm can then be extended to structured norms built on either disjoint or overlapping groups of variables , leading to a flexible framework that can deal with various structures . we present applications to unsupervised learning , for structured sparse principal component analysis and hierarchical dictionary learning , and to supervised learning in the context of non-linear variable selection .", "topics": ["supervised learning", "optimization problem"]}
{"title": "learning factored representations in a deep mixture of experts", "abstract": "mixtures of experts combine the outputs of several `` expert '' networks , each of which specializes in a different part of the input space . this is achieved by training a `` gating '' network that maps each input to a distribution over the experts . such models show promise for building larger networks that are still cheap to compute at test time , and more parallelizable at training time . in this this work , we extend the mixture of experts to a stacked model , the deep mixture of experts , with multiple sets of gating and experts . this exponentially increases the number of effective experts by associating each input with a combination of experts at each layer , yet maintains a modest model size . on a randomly translated version of the mnist dataset , we find that the deep mixture of experts automatically learns to develop location-dependent ( `` where '' ) experts at the first layer , and class-specific ( `` what '' ) experts at the second layer . in addition , we see that the different combinations are in use when the model is applied to a dataset of speech monophones . these demonstrate effective use of all expert combinations .", "topics": ["map", "speech recognition"]}
{"title": "kernel cross-view collaborative representation based classification for person re-identification", "abstract": "person re-identification aims at the maintenance of a global identity as a person moves among non-overlapping surveillance cameras . it is a hard task due to different illumination conditions , viewpoints and the small number of annotated individuals from each pair of cameras ( small-sample-size problem ) . collaborative representation based classification ( crc ) has been employed successfully to address the small-sample-size problem in computer vision . however , the original crc formulation is not well-suited for person re-identification since it does not consider that probe and gallery samples are from different cameras . furthermore , it is a linear model , while appearance changes caused by different camera conditions indicate a strong nonlinear transition between cameras . to overcome such limitations , we propose the kernel cross-view collaborative representation based classification ( kernel x-crc ) that represents probe and gallery images by balancing representativeness and similarity nonlinearly . it assumes that a probe and its corresponding gallery image are represented with similar coding vectors using individuals from the training set . experimental results demonstrate that our assumption is true when using a high-dimensional feature vector and becomes more compelling when dealing with a low-dimensional and discriminative representation computed using a common subspace learning method . we achieve state-of-the-art for rank-1 matching rates in two person re-identification datasets ( prid450s and grid ) and the second best results on viper and cuhk01 datasets .", "topics": ["kernel ( operating system )", "feature vector"]}
{"title": "live blog corpus for summarization", "abstract": "live blogs are an increasingly popular news format to cover breaking news and live events in online journalism . online news websites around the world are using this medium to give their readers a minute by minute update on an event . good summaries enhance the value of the live blogs for a reader but are often not available . in this paper , we study a way of collecting corpora for automatic live blog summarization . in an empirical evaluation using well-known state-of-the-art summarization systems , we show that live blogs corpus poses new challenges in the field of summarization . we make our tools publicly available to reconstruct the corpus to encourage the research community and replicate our results .", "topics": ["text corpus"]}
{"title": "neural dynamic programming for musical self similarity", "abstract": "we present a neural sequence model designed specifically for symbolic music . the model is based on a learned edit distance mechanism which generalises a classic recursion from computer science , leading to a neural dynamic program . repeated motifs are detected by learning the transformations between them . we represent the arising computational dependencies using a novel data structure , the edit tree ; this perspective suggests natural approximations which afford the scaling up of our otherwise cubic time algorithm . we demonstrate our model on real and synthetic data ; in all cases it out-performs a strong stacked long short-term memory benchmark .", "topics": ["computational complexity theory", "synthetic data"]}
{"title": "superimposition of eye fundus images for longitudinal analysis from large public health databases", "abstract": "in this paper , a method is presented for superimposition ( i.e . registration ) of eye fundus images from persons with diabetes screened over many years for diabetic retinopathy . the method is fully automatic and robust to camera changes and colour variations across the images both in space and time . all the stages of the process are designed for longitudinal analysis of cohort public health databases where retinal examinations are made at approximately yearly intervals . the method relies on a model correcting two radial distortions and an affine transformation between pairs of images which is robustly fitted on salient points . each stage involves linear estimators followed by non-linear optimisation . the model of image warping is also invertible for fast computation . the method has been validated ( 1 ) on a simulated montage and ( 2 ) on public health databases with 69 patients with high quality images ( 271 pairs acquired mostly with different types of camera and 268 pairs acquired mostly with the same type of camera ) with success rates of 92 % and 98 % , and five patients ( 20 pairs ) with low quality images with a success rate of 100 % . compared to two state-of-the-art methods , ours gives better results .", "topics": ["nonlinear system", "mathematical optimization"]}
{"title": "estimating or propagating gradients through stochastic neurons for conditional computation", "abstract": "stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models , but in many cases they pose a challenging problem : how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons ? i.e . , can we `` back-propagate '' through these stochastic neurons ? we examine this question , existing approaches , and compare four families of solutions , applicable in different settings . one of them is the minimum variance unbiased gradient estimator for stochatic binary neurons ( a special case of the reinforce algorithm ) . a second approach , introduced here , decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part , which approximates the expected effect of the pure stochatic binary neuron to first order . a third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable . a fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument ( we call this the straight-through estimator ) . to explore a context where these estimators are useful , we consider a small-scale version of { \\em conditional computation } , where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network . in this case , it is important that the gating units produce an actual 0 most of the time . the resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful .", "topics": ["loss function", "sparse matrix"]}
{"title": "predictive business process monitoring with lstm neural networks", "abstract": "predictive business process monitoring methods exploit logs of completed cases of a process in order to make predictions about running cases thereof . existing methods in this space are tailor-made for specific prediction tasks . moreover , their relative accuracy is highly sensitive to the dataset at hand , thus requiring users to engage in trial-and-error and tuning when applying them in a specific setting . this paper investigates long short-term memory ( lstm ) neural networks as an approach to build consistently accurate models for a wide range of predictive process monitoring tasks . first , we show that lstms outperform existing techniques to predict the next event of a running case and its timestamp . next , we show how to use models for predicting the next task in order to predict the full continuation of a running case . finally , we apply the same approach to predict the remaining time , and show that this approach outperforms existing tailor-made methods .", "topics": ["neural networks"]}
{"title": "linear additive markov processes", "abstract": "we introduce lamp : the linear additive markov process . transitions in lamp may be influenced by states visited in the distant history of the process , but unlike higher-order markov processes , lamp retains an efficient parametrization . lamp also allows the specific dependence on history to be learned efficiently from data . we characterize some theoretical properties of lamp , including its steady-state and mixing time . we then give an algorithm based on alternating minimization to learn lamp models from data . finally , we perform a series of real-world experiments to show that lamp is more powerful than first-order markov processes , and even holds its own against deep sequential models ( lstms ) with a negligible increase in parameter complexity .", "topics": ["markov chain"]}
{"title": "end-to-end kernel learning with supervised convolutional kernel networks", "abstract": "in this paper , we introduce a new image representation based on a multilayer kernel machine . unlike traditional kernel methods where data representation is decoupled from the prediction task , we learn how to shape the kernel with supervision . we proceed by first proposing improvements of the recently-introduced convolutional kernel networks ( ckns ) in the context of unsupervised learning ; then , we derive backpropagation rules to take advantage of labeled training data . the resulting model is a new type of convolutional neural network , where optimizing the filters at each layer is equivalent to learning a linear subspace in a reproducing kernel hilbert space ( rkhs ) . we show that our method achieves reasonably competitive performance for image classification on some standard `` deep learning '' datasets such as cifar-10 and svhn , and also for image super-resolution , demonstrating the applicability of our approach to a large variety of image-related tasks .", "topics": ["kernel ( operating system )", "test set"]}
{"title": "mirror , mirror on the wall , tell me , is the error small ?", "abstract": "do object part localization methods produce bilaterally symmetric results on mirror images ? surprisingly not , even though state of the art methods augment the training set with mirrored images . in this paper we take a closer look into this issue . we first introduce the concept of mirrorability as the ability of a model to produce symmetric results in mirrored images and introduce a corresponding measure , namely the \\textit { mirror error } that is defined as the difference between the detection result on an image and the mirror of the detection result on its mirror image . we evaluate the mirrorability of several state of the art algorithms in two of the most intensively studied problems , namely human pose estimation and face alignment . our experiments lead to several interesting findings : 1 ) surprisingly , most of state of the art methods struggle to preserve the mirror symmetry , despite the fact that they do have very similar overall performance on the original and mirror images ; 2 ) the low mirrorability is not caused by training or testing sample bias - all algorithms are trained on both the original images and their mirrored versions ; 3 ) the mirror error is strongly correlated to the localization/alignment error ( with correlation coefficients around 0.7 ) . since the mirror error is calculated without knowledge of the ground truth , we show two interesting applications - in the first it is used to guide the selection of difficult samples and in the second to give feedback in a popular cascaded pose regression method for face alignment .", "topics": ["ground truth", "coefficient"]}
{"title": "dimension of marginals of kronecker product models", "abstract": "a kronecker product model is the set of visible marginal probability distributions of an exponential family whose sufficient statistics matrix factorizes as a kronecker product of two matrices , one for the visible variables and one for the hidden variables . we estimate the dimension of these models by the maximum rank of the jacobian in the limit of large parameters . the limit is described by the tropical morphism ; a piecewise linear map with pieces corresponding to slicings of the visible matrix by the normal fan of the hidden matrix . we obtain combinatorial conditions under which the model has the expected dimension , equal to the minimum of the number of natural parameters and the dimension of the ambient probability simplex . additionally , we prove that the binary restricted boltzmann machine always has the expected dimension .", "topics": ["time complexity"]}
{"title": "functional connectivity patterns of autism spectrum disorder identified by deep feature learning", "abstract": "autism spectrum disorder ( asd ) is regarded as a brain disease with globally disrupted neuronal networks . even though fmri studies have revealed abnormal functional connectivity in asd , they have not reached a consensus of the disrupted patterns . here , a deep learning-based feature extraction method identifies multivariate and nonlinear functional connectivity patterns of asd . resting-state fmri data of 972 subjects ( 465 asd 507 normal controls ) acquired from the autism brain imaging data exchange were used . a functional connectivity matrix of each subject was generated using 90 predefined brain regions . as a data-driven feature extraction method without prior knowledge such as subjects diagnosis , variational autoencoder ( vae ) summarized the functional connectivity matrix into 2 features . those feature values of asd patients were statistically compared with those of controls . a feature was significantly different between asd and normal controls . the extracted features were visualized by vae-based generator which can produce virtual functional connectivity matrices . the asd-related feature was associated with frontoparietal connections , interconnections of the dorsal medial frontal cortex and corticostriatal connections . it also showed a trend of negative correlation with full-scale iq . a data-driven feature extraction based on deep learning could identify complex patterns of functional connectivity of asd . this approach will help discover complex patterns of abnormalities in brain connectivity in various brain disorders .", "topics": ["feature learning", "calculus of variations"]}
{"title": "learning with submodular functions : a convex optimization perspective", "abstract": "submodular functions are relevant to machine learning for at least two reasons : ( 1 ) some problems may be expressed directly as the optimization of submodular functions and ( 2 ) the lovasz extension of submodular functions provides a useful set of regularization functions for supervised and unsupervised learning . in this monograph , we present the theory of submodular functions from a convex analysis perspective , presenting tight links between certain polyhedra , combinatorial optimization and convex optimization problems . in particular , we show how submodular function minimization is equivalent to solving a wide variety of convex optimization problems . this allows the derivation of new efficient algorithms for approximate and exact submodular function minimization with theoretical guarantees and good practical performance . by listing many examples of submodular functions , we review various applications to machine learning , such as clustering , experimental design , sensor placement , graphical model structure learning or subset selection , as well as a family of structured sparsity-inducing norms that can be derived and used from submodular functions .", "topics": ["approximation algorithm", "graphical model"]}
{"title": "human activity recognition from mobile inertial sensors using recurrence plots", "abstract": "inertial sensors are present in most mobile devices nowadays and such devices are used by people during most of their daily activities . in this paper , we present an approach for human activity recognition based on inertial sensors by employing recurrence plots ( rp ) and visual descriptors . the pipeline of the proposed approach is the following : compute rps from sensor data , compute visual features from rps and use them in a machine learning protocol . as rps generate texture visual patterns , we transform the problem of sensor data classification to a problem of texture classification . experiments for classifying human activities based on accelerometer data showed that the proposed approach obtains the highest accuracies , outperforming time- and frequency-domain features directly extracted from sensor data . the best results are obtained when using rgb rps , in which each rgb channel corresponds to the rp of an independent accelerometer axis .", "topics": ["sensor"]}
{"title": "learning to price with reference effects", "abstract": "as a firm varies the price of a product , consumers exhibit reference effects , making purchase decisions based not only on the prevailing price but also the product 's price history . we consider the problem of learning such behavioral patterns as a monopolist releases , markets , and prices products . this context calls for pricing decisions that intelligently trade off between maximizing revenue generated by a current product and probing to gain information for future benefit . due to dependence on price history , realized demand can reflect delayed consequences of earlier pricing decisions . as such , inference entails attribution of outcomes to prior decisions and effective exploration requires planning price sequences that yield informative future outcomes . despite the considerable complexity of this problem , we offer a tractable systematic approach . in particular , we frame the problem as one of reinforcement learning and leverage thompson sampling . we also establish a regret bound that provides graceful guarantees on how performance improves as data is gathered and how this depends on the complexity of the demand model . we illustrate merits of the approach through simulations .", "topics": ["regret ( decision theory )", "reinforcement learning"]}
{"title": "facial expression recognition using enhanced deep 3d convolutional neural networks", "abstract": "deep neural networks ( dnns ) have shown to outperform traditional methods in various visual recognition tasks including facial expression recognition ( fer ) . in spite of efforts made to improve the accuracy of fer systems using dnn , existing methods still are not generalizable enough in practical applications . this paper proposes a 3d convolutional neural network method for fer in videos . this new network architecture consists of 3d inception-resnet layers followed by an lstm unit that together extracts the spatial relations within facial images as well as the temporal relations between different frames in the video . facial landmark points are also used as inputs to our network which emphasize on the importance of facial components rather than the facial regions that may not contribute significantly to generating facial expressions . our proposed method is evaluated using four publicly available databases in subject-independent and cross-database tasks and outperforms state-of-the-art methods .", "topics": ["neural networks", "database"]}
{"title": "learning residual images for face attribute manipulation", "abstract": "face attributes are interesting due to their detailed description of human faces . unlike prior researches working on attribute prediction , we address an inverse and more challenging problem called face attribute manipulation which aims at modifying a face image according to a given attribute value . instead of manipulating the whole image , we propose to learn the corresponding residual image defined as the difference between images before and after the manipulation . in this way , the manipulation can be operated efficiently with modest pixel modification . the framework of our approach is based on the generative adversarial network . it consists of two image transformation networks and a discriminative network . the transformation networks are responsible for the attribute manipulation and its dual operation and the discriminative network is used to distinguish the generated images from real images . we also apply dual learning to allow transformation networks to learn from each other . experiments show that residual images can be effectively learned and used for attribute manipulations . the generated images remain most of the details in attribute-irrelevant areas .", "topics": ["relevance", "pixel"]}
{"title": "deep reinforcement learning with model learning and monte carlo tree search in minecraft", "abstract": "deep reinforcement learning has been successfully applied to several visual-input tasks using model-free methods . in this paper , we propose a model-based approach that combines learning a dnn-based transition model with monte carlo tree search to solve a block-placing task in minecraft . our learned transition model predicts the next frame and the rewards one step ahead given the last four frames of the agent 's first-person-view image and the current action . then a monte carlo tree search algorithm uses this model to plan the best sequence of actions for the agent to perform . on the proposed task in minecraft , our model-based approach reaches the performance comparable to the deep q-network 's , but learns faster and , thus , is more training sample efficient .", "topics": ["reinforcement learning"]}
{"title": "tag-weighted topic model for large-scale semi-structured documents", "abstract": "to date , there have been massive semi-structured documents ( ssds ) during the evolution of the internet . these ssds contain both unstructured features ( e.g . , plain text ) and metadata ( e.g . , tags ) . most previous works focused on modeling the unstructured text , and recently , some other methods have been proposed to model the unstructured text with specific tags . to build a general model for ssds remains an important problem in terms of both model fitness and efficiency . we propose a novel method to model the ssds by a so-called tag-weighted topic model ( twtm ) . twtm is a framework that leverages both the tags and words information , not only to learn the document-topic and topic-word distributions , but also to infer the tag-topic distributions for text mining tasks . we present an efficient variational inference method with an em algorithm for estimating the model parameters . meanwhile , we propose three large-scale solutions for our model under the mapreduce distributed computing platform for modeling large-scale ssds . the experimental results show the effectiveness , efficiency and the robustness by comparing our model with the state-of-the-art methods in document modeling , tags prediction and text classification . we also show the performance of the three distributed solutions in terms of time and accuracy on document modeling .", "topics": ["calculus of variations"]}
{"title": "segmentation and classification of cine-mr images using fully convolutional networks and handcrafted features", "abstract": "three-dimensional cine-mri is of crucial importance for assessing the cardiac function . features that describe the anatomy and function of cardiac structures ( e.g . left ventricle ( lv ) , right ventricle ( rv ) , and myocardium ( mc ) ) are known to have significant diagnostic value and can be computed from 3d cine-mr images . however , these features require precise segmentation of cardiac structures . among the fully automated segmentation methods , fully convolutional networks ( fcn ) with skip connections have shown robustness in medical segmentation problems . in this study , we develop a complete pipeline for classification of subjects with cardiac conditions based on 3d cine-mri . for the segmentation task , we develop a 2d fcn and introduce parallel paths ( pp ) as a way to exploit the 3d information of the cine-mr image . for the classification task , 125 features were extracted from the segmented structures , describing their anatomy and function . next , a two-stage pipeline for feature selection using the lasso method is developed . a subset of 20 features is selected for classification . each subject is classified using an ensemble of logistic regression , multi-layer perceptron , and support vector machine classifiers through majority voting . the dice coefficient for segmentation was 0.95+-0.03 , 0.89+-0.13 , and 0.90+-0.03 for lv , rv , and mc respectively . the 8-fold cross validation accuracy for the classification task was 95.05 % and 92.77 % based on ground truth and the proposed methods segmentations respectively . the results show that the pps increase the segmentation accuracy , by exploiting the spatial relations . moreover , the classification algorithm and the features showed discriminability while keeping the sensitivity to segmentation error as low as possible .", "topics": ["statistical classification", "support vector machine"]}
{"title": "deep lda-pruned nets for efficient facial gender classification", "abstract": "many real-time tasks , such as human-computer interaction , require fast and efficient facial gender classification . although deep cnn nets have been very effective for a multitude of classification tasks , their high space and time demands make them impractical for personal computers and mobile devices without a powerful gpu . in this paper , we develop a 16-layer , yet lightweight , neural network which boosts efficiency while maintaining high accuracy . our net is pruned from the vgg-16 model starting from the last convolutional ( conv ) layer where we find neuron activations are highly uncorrelated given the gender . through fisher 's linear discriminant analysis ( lda ) , we show that this high decorrelation makes it safe to discard directly last conv layer neurons with high within-class variance and low between-class variance . combined with either support vector machines ( svm ) or bayesian classification , the reduced cnns are capable of achieving comparable ( or even higher ) accuracies on the lfw and celeba datasets than the original net with fully connected layers . on lfw , only four conv5_3 neurons are able to maintain a comparably high recognition accuracy , which results in a reduction of total network size by a factor of 70x with a 11 fold speedup . comparisons with a state-of-the-art pruning method as well as two smaller nets in terms of accuracy loss and convolutional layers pruning rate are also provided .", "topics": ["support vector machine"]}
{"title": "the causality/repair connection in databases : causality-programs", "abstract": "in this work , answer-set programs that specify repairs of databases are used as a basis for solving computational and reasoning problems about causes for query answers from databases .", "topics": ["database", "causality"]}
{"title": "born to learn : the inspiration , progress , and future of evolved plastic artificial neural networks", "abstract": "biological plastic neural networks are systems of extraordinary computational capabilities shaped by evolution , development , and lifetime learning . the interplay of these elements leads to the emergence of adaptive behavior and intelligence . inspired by such intricate natural phenomena , evolved plastic artificial neural networks ( epanns ) use simulated evolution in-silico to breed plastic neural networks with a large variety of dynamics , architectures , and plasticity rules : these artificial systems are composed of inputs , outputs , and plastic components that change in response to experiences in an environment . these systems may autonomously discover novel adaptive algorithms , and lead to hypotheses on the emergence of biological adaptation . epanns have seen considerable progress over the last two decades . current scientific and technological advances in artificial neural networks are now setting the conditions for radically new approaches and results . in particular , the limitations of hand-designed networks could be overcome by more flexible and innovative solutions . this paper brings together a variety of inspiring ideas that define the field of epanns . the main methods and results are reviewed . finally , new opportunities and developments are presented .", "topics": ["neural networks", "simulation"]}
{"title": "an empirical evaluation of possible variations of lazy propagation", "abstract": "as real-world bayesian networks continue to grow larger and more complex , it is important to investigate the possibilities for improving the performance of existing algorithms of probabilistic inference . motivated by examples , we investigate the dependency of the performance of lazy propagation on the message computation algorithm . we show how symbolic probabilistic inference ( spi ) and arc-reversal ( ar ) can be used for computation of clique to clique messages in the addition to the traditional use of variable elimination ( ve ) . in addition , the paper resents the results of an empirical evaluation of the performance of lazy propagation using ve , spi , and ar as the message computation algorithm . the results of the empirical evaluation show that for most networks , the performance of inference did not depend on the choice of message computation algorithm , but for some randomly generated networks the choice had an impact on both space and time performance . in the cases where the choice had an impact , ar produced the best results .", "topics": ["computation", "bayesian network"]}
{"title": "predicting contextual sequences via submodular function maximization", "abstract": "sequence optimization , where the items in a list are ordered to maximize some reward has many applications such as web advertisement placement , search , and control libraries in robotics . previous work in sequence optimization produces a static ordering that does not take any features of the item or context of the problem into account . in this work , we propose a general approach to order the items within the sequence based on the context ( e.g . , perceptual information , environment description , and goals ) . we take a simple , efficient , reduction-based approach where the choice and order of the items is established by repeatedly learning simple classifiers or regressors for each `` slot '' in the sequence . our approach leverages recent work on submodular function maximization to provide a formal regret reduction from submodular sequence optimization to simple cost-sensitive prediction . we apply our contextual sequence prediction algorithm to optimize control libraries and demonstrate results on two robotics problems : manipulator trajectory prediction and mobile robot path planning .", "topics": ["regret ( decision theory )"]}
{"title": "a mathematical trust algebra for international nation relations computation and evaluation", "abstract": "this paper presents a trust computation for international relations and its calculus , which related to bayesian inference , dempster shafer theory and subjective logic . we proposed a method that allows a trust computation which is previously subjective and incomputable . an example of case study for the trust computation is the united states of america great britain relations . the method supports decision makers in a government such as foreign ministry , defense ministry , presidential or prime minister office . the department of defense ( dod ) may use our method to determine a nation that can be known as a friendly , neutral or hostile nation .", "topics": ["computation"]}
{"title": "database transposition for constrained ( closed ) pattern mining", "abstract": "recently , different works proposed a new way to mine patterns in databases with pathological size . for example , experiments in genome biology usually provide databases with thousands of attributes ( genes ) but only tens of objects ( experiments ) . in this case , mining the `` transposed '' database runs through a smaller search space , and the galois connection allows to infer the closed patterns of the original database . we focus here on constrained pattern mining for those unusual databases and give a theoretical framework for database and constraint transposition . we discuss the properties of constraint transposition and look into classical constraints . we then address the problem of generating the closed patterns of the original database satisfying the constraint , starting from those mined in the `` transposed '' database . finally , we show how to generate all the patterns satisfying the constraint from the closed ones .", "topics": ["data mining", "database"]}
{"title": "spectral convolution networks", "abstract": "previous research has shown that computation of convolution in the frequency domain provides a significant speedup versus traditional convolution network implementations . however , this performance increase comes at the expense of repeatedly computing the transform and its inverse in order to apply other network operations such as activation , pooling , and dropout . we show , mathematically , how convolution and activation can both be implemented in the frequency domain using either the fourier or laplace transformation . the main contributions are a description of spectral activation under the fourier transform and a further description of an efficient algorithm for computing both convolution and activation under the laplace transform . by computing both the convolution and activation functions in the frequency domain , we can reduce the number of transforms required , as well as reducing overall complexity . our description of a spectral activation function , together with existing spectral analogs of other network functions may then be used to compose a fully spectral implementation of a convolution network .", "topics": ["computational complexity theory", "statistical classification"]}
{"title": "keyphrase based evaluation of automatic text summarization", "abstract": "the development of methods to deal with the informative contents of the text units in the matching process is a major challenge in automatic summary evaluation systems that use fixed n-gram matching . the limitation causes inaccurate matching between units in a peer and reference summaries . the present study introduces a new keyphrase based summary evaluator kpeval for evaluating automatic summaries . the kpeval relies on the keyphrases since they convey the most important concepts of a text . in the evaluation process , the keyphrases are used in their lemma form as the matching text unit . the system was applied to evaluate different summaries of arabic multi-document data set presented at tac2011 . the results showed that the new evaluation technique correlates well with the known evaluation systems : rouge1 , rouge2 , rougesu4 , and autosummeng memog . kpeval has the strongest correlation with autosummeng memog , pearson and spearman correlation coefficient measures are 0.8840 , 0.9667 respectively .", "topics": ["coefficient"]}
{"title": "generating multi-sentence lingual descriptions of indoor scenes", "abstract": "this paper proposes a novel framework for generating lingual descriptions of indoor scenes . whereas substantial efforts have been made to tackle this problem , previous approaches focusing primarily on generating a single sentence for each image , which is not sufficient for describing complex scenes . we attempt to go beyond this , by generating coherent descriptions with multiple sentences . our approach is distinguished from conventional ones in several aspects : ( 1 ) a 3d visual parsing system that jointly infers objects , attributes , and relations ; ( 2 ) a generative grammar learned automatically from training text ; and ( 3 ) a text generation algorithm that takes into account the coherence among sentences . experiments on the augmented nyu-v2 dataset show that our framework can generate natural descriptions with substantially higher rogue scores compared to those produced by the baseline .", "topics": ["baseline ( configuration management )", "parsing"]}
{"title": "non-linear learning for statistical machine translation", "abstract": "modern statistical machine translation ( smt ) systems usually use a linear combination of features to model the quality of each translation hypothesis . the linear combination assumes that all the features are in a linear relationship and constrains that each feature interacts with the rest features in an linear manner , which might limit the expressive power of the model and lead to a under-fit model on the current data . in this paper , we propose a non-linear modeling for the quality of translation hypotheses based on neural networks , which allows more complex interaction between features . a learning framework is presented for training the non-linear models . we also discuss possible heuristics in designing the network structure which may improve the non-linear learning performance . experimental results show that with the basic features of a hierarchical phrase-based machine translation system , our method produce translations that are better than a linear model .", "topics": ["machine translation", "nonlinear system"]}
{"title": "deepnav : learning to navigate large cities", "abstract": "we present deepnav , a convolutional neural network ( cnn ) based algorithm for navigating large cities using locally visible street-view images . the deepnav agent learns to reach its destination quickly by making the correct navigation decisions at intersections . we collect a large-scale dataset of street-view images organized in a graph where nodes are connected by roads . this dataset contains 10 city graphs and more than 1 million street-view images . we propose 3 supervised learning approaches for the navigation task and show how a* search in the city graph can be used to generate supervision for the learning . our annotation process is fully automated using publicly available mapping services and requires no human input . we evaluate the proposed deepnav models on 4 held-out cities for navigating to 5 different types of destinations . our algorithms outperform previous work that uses hand-crafted features and support vector regression ( svr ) [ 19 ] .", "topics": ["supervised learning", "support vector machine"]}
{"title": "clustering words by projection entropy", "abstract": "we apply entropy agglomeration ( ea ) , a recently introduced algorithm , to cluster the words of a literary text . ea is a greedy agglomerative procedure that minimizes projection entropy ( pe ) , a function that can quantify the segmentedness of an element set . to apply it , the text is reduced to a feature allocation , a combinatorial object to represent the word occurences in the text 's paragraphs . the experiment results demonstrate that ea , despite its reduction and simplicity , is useful in capturing significant relationships among the words in the text . this procedure was implemented in python and published as a free software : rebus .", "topics": ["cluster analysis"]}
{"title": "semantic segmentation of colon glands with deep convolutional neural networks and total variation segmentation", "abstract": "segmentation of histopathology sections is an ubiquitous requirement in digital pathology and due to the large variability of biological tissue , machine learning techniques have shown superior performance over standard image processing methods . as part of the glas @ miccai2015 colon gland segmentation challenge , we present a learning-based algorithm to segment glands in tissue of benign and malignant colorectal cancer . images are preprocessed according to the hematoxylin-eosin staining protocol and two deep convolutional neural networks ( cnn ) are trained as pixel classifiers . the cnn predictions are then regularized using a figure-ground segmentation based on weighted total variation to produce the final segmentation result . on two test sets , our approach achieves a tissue classification accuracy of 98 % and 94 % , making use of the inherent capability of our system to distinguish between benign and malignant tissue .", "topics": ["test set", "image processing"]}
{"title": "dense human body correspondences using convolutional networks", "abstract": "we propose a deep learning approach for finding dense correspondences between 3d scans of people . our method requires only partial geometric information in the form of two depth maps or partial reconstructed surfaces , works for humans in arbitrary poses and wearing any clothing , does not require the two people to be scanned from similar viewpoints , and runs in real time . we use a deep convolutional neural network to train a feature descriptor on depth map pixels , but crucially , rather than training the network to solve the shape correspondence problem directly , we train it to solve a body region classification problem , modified to increase the smoothness of the learned descriptors near region boundaries . this approach ensures that nearby points on the human body are nearby in feature space , and vice versa , rendering the feature descriptor suitable for computing dense correspondences between the scans . we validate our method on real and synthetic data for both clothed and unclothed humans , and show that our correspondences are more robust than is possible with state-of-the-art unsupervised methods , and more accurate than those found using methods that require full watertight 3d geometry .", "topics": ["feature vector", "unsupervised learning"]}
{"title": "gaussian process models with parallelization and gpu acceleration", "abstract": "in this work , we present an extension of gaussian process ( gp ) models with sophisticated parallelization and gpu acceleration . the parallelization scheme arises naturally from the modular computational structure w.r.t . datapoints in the sparse gaussian process formulation . additionally , the computational bottleneck is implemented with gpu acceleration for further speed up . combining both techniques allows applying gaussian process models to millions of datapoints . the efficiency of our algorithm is demonstrated with a synthetic dataset . its source code has been integrated into our popular software library gpy .", "topics": ["synthetic data", "sparse matrix"]}
{"title": "pedestrian alignment network for large-scale person re-identification", "abstract": "person re-identification ( person re-id ) is mostly viewed as an image retrieval problem . this task aims to search a query person in a large image pool . in practice , person re-id usually adopts automatic detectors to obtain cropped pedestrian images . however , this process suffers from two types of detector errors : excessive background and part missing . both errors deteriorate the quality of pedestrian alignment and may compromise pedestrian matching due to the position and scale variances . to address the misalignment problem , we propose that alignment can be learned from an identification procedure . we introduce the pedestrian alignment network ( pan ) which allows discriminative embedding learning and pedestrian alignment without extra annotations . our key observation is that when the convolutional neural network ( cnn ) learns to discriminate between different identities , the learned feature maps usually exhibit strong activations on the human body rather than the background . the proposed network thus takes advantage of this attention mechanism to adaptively locate and align pedestrians within a bounding box . visual examples show that pedestrians are better aligned with pan . experiments on three large-scale re-id datasets confirm that pan improves the discriminative ability of the feature embeddings and yields competitive accuracy with the state-of-the-art methods .", "topics": ["map"]}
{"title": "do convnets learn correspondence ?", "abstract": "convolutional neural nets ( convnets ) trained from massive labeled datasets have substantially improved the state-of-the-art in image classification and object detection . however , visual understanding requires establishing correspondence on a finer level than object category . given their large pooling regions and training from whole-image labels , it is not clear that convnets derive their success from an accurate correspondence model which could be used for precise localization . in this paper , we study the effectiveness of convnet activation features for tasks requiring correspondence . we present evidence that convnet features localize at a much finer scale than their receptive field sizes , that they can be used to perform intraclass alignment as well as conventional hand-engineered features , and that they outperform conventional features in keypoint prediction on objects from pascal voc 2011 .", "topics": ["object detection", "computer vision"]}
{"title": "weakly supervised cascaded convolutional networks", "abstract": "object detection is a challenging task in visual understanding domain , and even more so if the supervision is to be weak . recently , few efforts to handle the task without expensive human annotations is established by promising deep neural network . a new architecture of cascaded networks is proposed to learn a convolutional neural network ( cnn ) under such conditions . we introduce two such architectures , with either two cascade stages or three which are trained in an end-to-end pipeline . the first stage of both architectures extracts best candidate of class specific region proposals by training a fully convolutional network . in the case of the three stage architecture , the middle stage provides object segmentation , using the output of the activation maps of first stage . the final stage of both architectures is a part of a convolutional neural network that performs multiple instance learning on proposals extracted in the previous stage ( s ) . our experiments on the pascal voc 2007 , 2010 , 2012 and large scale object datasets , ilsvrc 2013 , 2014 datasets show improvements in the areas of weakly-supervised object detection , classification and localization .", "topics": ["statistical classification", "object detection"]}
{"title": "matching neural paths : transfer from recognition to correspondence search", "abstract": "many machine learning tasks require finding per-part correspondences between objects . in this work we focus on low-level correspondences - a highly ambiguous matching problem . we propose to use a hierarchical semantic representation of the objects , coming from a convolutional neural network , to solve this ambiguity . training it for low-level correspondence prediction directly might not be an option in some domains where the ground-truth correspondences are hard to obtain . we show how transfer from recognition can be used to avoid such training . our idea is to mark parts as `` matching '' if their features are close to each other at all the levels of convolutional feature hierarchy ( neural paths ) . although the overall number of such paths is exponential in the number of layers , we propose a polynomial algorithm for aggregating all of them in a single backward pass . the empirical validation is done on the task of stereo correspondence and demonstrates that we achieve competitive results among the methods which do not use labeled target domain data .", "topics": ["high- and low-level", "time complexity"]}
{"title": "towards instance segmentation with object priority : prominent object detection and recognition", "abstract": "this manuscript introduces the problem of prominent object detection and recognition inspired by the fact that human seems to priorities perception of scene elements . the problem deals with finding the most important region of interest , segmenting the relevant item/object in that area , and assigning it an object class label . in other words , we are solving the three problems of saliency modeling , saliency detection , and object recognition under one umbrella . the motivation behind such a problem formulation is ( 1 ) the benefits to the knowledge representation-based vision pipelines , and ( 2 ) the potential improvements in emulating bio-inspired vision systems by solving these three problems together . we are foreseeing extending this problem formulation to fully semantically segmented scenes with instance object priority for high-level inferences in various applications including assistive vision . along with a new problem definition , we also propose a method to achieve such a task . the proposed model predicts the most important area in the image , segments the associated objects , and labels them . the proposed problem and method are evaluated against human fixations , annotated segmentation masks , and object class categories . we define a chance level for each of the evaluation criterion to compare the proposed algorithm with . despite the good performance of the proposed baseline , the overall evaluations indicate that the problem of prominent object detection and recognition is a challenging task that is still worth investigating further .", "topics": ["baseline ( configuration management )", "object detection"]}
{"title": "end-to-end task-completion neural dialogue systems", "abstract": "one of the major drawbacks of modularized task-completion dialogue systems is that each module is trained individually , which presents several challenges . for example , downstream modules are affected by earlier modules , and the performance of the entire system is not robust to the accumulated errors . this paper presents a novel end-to-end learning framework for task-completion dialogue systems to tackle such issues . our neural dialogue system can directly interact with a structured database to assist users in accessing information and accomplishing certain tasks . the reinforcement learning based dialogue manager offers robust capabilities to handle noises caused by other components of the dialogue system . our experiments in a movie-ticket booking domain show that our end-to-end system not only outperforms modularized dialogue system baselines for both objective and subjective evaluation , but also is robust to noises as demonstrated by several systematic experiments with different error granularity and rates specific to the language understanding module .", "topics": ["supervised learning", "reinforcement learning"]}
{"title": "on planning with preferences in htn", "abstract": "in this paper , we address the problem of generating preferred plans by combining the procedural control knowledge specified by hierarchical task networks ( htns ) with rich qualitative user preferences . the outcome of our work is a language for specifyin user preferences , tailored to htn planning , together with a provably optimal preference-based planner , htnplan , that is implemented as an extension of shop2 . to compute preferred plans , we propose an approach based on forward-chaining heuristic search . our heuristic uses an admissible evaluation function measuring the satisfaction of preferences over partial plans . our empirical evaluation demonstrates the effectiveness of our htnplan heuristics . we prove our approach sound and optimal with respect to the plans it generates by appealing to a situation calculus semantics of our preference language and of htn planning . while our implementation builds on shop2 , the language and techniques proposed here are relevant to a broad range of htn planners .", "topics": ["heuristic"]}
{"title": "adadnns : adaptive ensemble of deep neural networks for scene text recognition", "abstract": "recognizing text in the wild is a really challenging task because of complex backgrounds , various illuminations and diverse distortions , even with deep neural networks ( convolutional neural networks and recurrent neural networks ) . in the end-to-end training procedure for scene text recognition , the outputs of deep neural networks at different iterations are always demonstrated with diversity and complementarity for the target object ( text ) . here , a simple but effective deep learning method , an adaptive ensemble of deep neural networks ( adadnns ) , is proposed to simply select and adaptively combine classifier components at different iterations from the whole learning system . furthermore , the ensemble is formulated as a bayesian framework for classifier weighting and combination . a variety of experiments on several typical acknowledged benchmarks , i.e . , icdar robust reading competition ( challenge 1 , 2 and 4 ) datasets , verify the surprised improvement from the baseline dnns , and the effectiveness of adadnns compared with the recent state-of-the-art methods .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "an analysis of random projections in cancelable biometrics", "abstract": "with increasing concerns about security , the need for highly secure physical biometrics-based authentication systems utilizing \\emph { cancelable biometric } technologies is on the rise . because the problem of cancelable template generation deals with the trade-off between template security and matching performance , many state-of-the-art algorithms successful in generating high quality cancelable biometrics all have random projection as one of their early processing steps . this paper therefore presents a formal analysis of why random projections is an essential step in cancelable biometrics . by formally defining the notion of an \\textit { independent subspace structure } for datasets , it can be shown that random projection preserves the subspace structure of data vectors generated from a union of independent linear subspaces . the bound on the minimum number of random vectors required for this to hold is also derived and is shown to depend logarithmically on the number of data samples , not only in independent subspaces but in disjoint subspace settings as well . the theoretical analysis presented is supported in detail with empirical results on real-world face recognition datasets .", "topics": ["supervised learning", "time complexity"]}
{"title": "training deep networks with structured layers by matrix backpropagation", "abstract": "deep neural network architectures have recently produced excellent results in a variety of areas in artificial intelligence and visual recognition , well surpassing traditional shallow architectures trained using hand-designed features . the power of deep networks stems both from their ability to perform local computations followed by pointwise non-linearities over increasingly larger receptive fields , and from the simplicity and scalability of the gradient-descent training procedure based on backpropagation . an open problem is the inclusion of layers that perform global , structured matrix computations like segmentation ( e.g . normalized cuts ) or higher-order pooling ( e.g . log-tangent space metrics defined over the manifold of symmetric positive definite matrices ) while preserving the validity and efficiency of an end-to-end deep training framework . in this paper we propose a sound mathematical apparatus to formally integrate global structured computation into deep computation architectures . at the heart of our methodology is the development of the theory and practice of backpropagation that generalizes to the calculus of adjoint matrix variations . the proposed matrix backpropagation methodology applies broadly to a variety of problems in machine learning or computational perception . here we illustrate it by performing visual segmentation experiments using the bsds and mscoco benchmarks , where we show that deep networks relying on second-order pooling and normalized cuts layers , trained end-to-end using matrix backpropagation , outperform counterparts that do not take advantage of such global layers .", "topics": ["computation", "gradient descent"]}
{"title": "sparse code formation with linear inhibition", "abstract": "sparse code formation in the primary visual cortex ( v1 ) has been inspiration for many state-of-the-art visual recognition systems . to stimulate this behavior , networks are trained networks under mathematical constraint of sparsity or selectivity . in this paper , the authors exploit another approach which uses lateral interconnections in feature learning networks . however , instead of adding direct lateral interconnections among neurons , we introduce an inhibitory layer placed right after normal encoding layer . this idea overcomes the challenge of computational cost and complexity on lateral networks while preserving crucial objective of sparse code formation . to demonstrate this idea , we use sparse autoencoder as normal encoding layer and apply inhibitory layer . early experiments in visual recognition show relative improvements over traditional approach on cifar-10 dataset . moreover , simple installment and training process using hebbian rule allow inhibitory layer to be integrated into existing networks , which enables further analysis in the future .", "topics": ["computer vision", "sparse matrix"]}
{"title": "quantum aspects of semantic analysis and symbolic artificial intelligence", "abstract": "modern approaches to semanic analysis if reformulated as hilbert-space problems reveal formal structures known from quantum mechanics . similar situation is found in distributed representations of cognitive structures developed for the purposes of neural networks . we take a closer look at similarites and differences between the above two fields and quantum information theory .", "topics": ["artificial intelligence"]}
{"title": "a survey of current datasets for vision and language research", "abstract": "integrating vision and language has long been a dream in work on artificial intelligence ( ai ) . in the past two years , we have witnessed an explosion of work that brings together vision and language from images to videos and beyond . the available corpora have played a crucial role in advancing this area of research . in this paper , we propose a set of quality metrics for evaluating and analyzing the vision & language datasets and categorize them accordingly . our analyses show that the most recent datasets have been using more complex language and more abstract concepts , however , there are different strengths and weaknesses in each .", "topics": ["text corpus", "artificial intelligence"]}
{"title": "clusternet : detecting small objects in large scenes by exploiting spatio-temporal information", "abstract": "object detection in wide area motion imagery ( wami ) has drawn the attention of the computer vision research community for a number of years . wami proposes a number of unique challenges including extremely small object sizes , both sparse and densely-packed objects , and extremely large search spaces ( large video frames ) . nearly all state-of-the-art methods in wami object detection report that appearance-based classifiers fail in this challenging data and instead rely almost entirely on motion information in the form of background subtraction or frame-differencing . in this work , we experimentally verify the failure of appearance-based classifiers in wami , such as faster r-cnn and a heatmap-based fully convolutional neural network ( cnn ) , and propose a novel two-stage spatio-temporal cnn which effectively and efficiently combines both appearance and motion information to significantly surpass the state-of-the-art in wami object detection . to reduce the large search space , the first stage ( clusternet ) takes in a set of extremely large video frames , combines the motion and appearance information within the convolutional architecture , and proposes regions of objects of interest ( roobi ) . these roobi can contain from one to clusters of several hundred objects due to the large video frame size and varying object density in wami . the second stage ( foveanet ) then estimates the centroid location of all objects in that given roobi simultaneously via heatmap estimation . the proposed method exceeds state-of-the-art results on the wpafb 2009 dataset by 5-16 % for moving objects and nearly 50 % for stopped objects , as well as being the first proposed method in wide area motion imagery to detect completely stationary objects .", "topics": ["object detection", "computer vision"]}
{"title": "coco : performance assessment", "abstract": "we present an any-time performance assessment for benchmarking numerical optimization algorithms in a black-box scenario , applied within the coco benchmarking platform . the performance assessment is based on runtimes measured in number of objective function evaluations to reach one or several quality indicator target values . we argue that runtime is the only available measure with a generic , meaningful , and quantitative interpretation . we discuss the choice of the target values , runlength-based targets , and the aggregation of results by using simulated restarts , averages , and empirical distribution functions .", "topics": ["mathematical optimization", "numerical analysis"]}
{"title": "curvature integration in a 5d kernel for extracting vessel connections in retinal images", "abstract": "tree-like structures such as retinal images are widely studied in computer-aided diagnosis systems for large-scale screening programs . despite several segmentation and tracking methods proposed in the literature , there still exist several limitations specifically when two or more curvilinear structures cross or bifurcate , or in the presence of interrupted lines or highly curved blood vessels . in this paper , we propose a novel approach based on multi-orientation scores augmented with a contextual affinity matrix , which both are inspired by the geometry of the primary visual cortex ( v1 ) and their contextual connections . the connectivity is described with a five-dimensional kernel obtained as the fundamental solution of the fokker-planck equation modelling the cortical connectivity in the lifted space of positions , orientations , curvatures and intensity . it is further used in a self-tuning spectral clustering step to identify the main perceptual units in the stimuli . the proposed method has been validated on several easy and challenging structures in a set of artificial images and actual retinal patches . supported by quantitative and qualitative results , the method is capable of overcoming the limitations of current state-of-the-art techniques .", "topics": ["kernel ( operating system )", "cluster analysis"]}
{"title": "face super-resolution through wasserstein gans", "abstract": "generative adversarial networks ( gans ) have received a tremendous amount of attention in the past few years , and have inspired applications addressing a wide range of problems . despite its great potential , gans are difficult to train . recently , a series of papers ( arjovsky & bottou , 2017a ; arjovsky et al . 2017b ; and gulrajani et al . 2017 ) proposed using wasserstein distance as the training objective and promised easy , stable gan training across architectures with minimal hyperparameter tuning . in this paper , we compare the performance of wasserstein distance with other training objectives on a variety of gan architectures in the context of single image super-resolution . our results agree that wasserstein gan with gradient penalty ( wgan-gp ) provides stable and converging gan training and that wasserstein distance is an effective metric to gauge training progress .", "topics": ["gradient"]}
{"title": "adaptive lambda least-squares temporal difference learning", "abstract": "temporal difference learning or td ( $ \\lambda $ ) is a fundamental algorithm in the field of reinforcement learning . however , setting td 's $ \\lambda $ parameter , which controls the timescale of td updates , is generally left up to the practitioner . we formalize the $ \\lambda $ selection problem as a bias-variance trade-off where the solution is the value of $ \\lambda $ that leads to the smallest mean squared value error ( msve ) . to solve this trade-off we suggest applying leave-one-trajectory-out cross-validation ( loto-cv ) to search the space of $ \\lambda $ values . unfortunately , this approach is too computationally expensive for most practical applications . for least squares td ( lstd ) we show that loto-cv can be implemented efficiently to automatically tune $ \\lambda $ and apply function optimization methods to efficiently search the space of $ \\lambda $ values . the resulting algorithm , allstd , is parameter free and our experiments demonstrate that allstd is significantly computationally faster than the na\\ '' { i } ve loto-cv implementation while achieving similar performance .", "topics": ["mathematical optimization", "reinforcement learning"]}
{"title": "nonparametric online regression while learning the metric", "abstract": "we study algorithms for online nonparametric regression that learn the directions along which the regression function is smoother . our algorithm learns the mahalanobis metric based on the gradient outer product matrix $ \\boldsymbol { g } $ of the regression function ( automatically adapting to the effective rank of this matrix ) , while simultaneously bounding the regret -- -on the same data sequence -- - in terms of the spectrum of $ \\boldsymbol { g } $ . as a preliminary step in our analysis , we extend a nonparametric online learning algorithm by hazan and megiddo enabling it to compete against functions whose lipschitzness is measured with respect to an arbitrary mahalanobis metric .", "topics": ["regret ( decision theory )", "gradient"]}
{"title": "cortical microcircuits as gated-recurrent neural networks", "abstract": "cortical circuits exhibit intricate recurrent architectures that are remarkably similar across different brain areas . such stereotyped structure suggests the existence of common computational principles . however , such principles have remained largely elusive . inspired by gated-memory networks , namely long short-term memory networks ( lstms ) , we introduce a recurrent neural network in which information is gated through inhibitory cells that are subtractive ( sublstm ) . we propose a natural mapping of sublstms onto known canonical excitatory-inhibitory cortical microcircuits . our empirical evaluation across sequential image classification and language modelling tasks shows that sublstm units can achieve similar performance to lstm units . these results suggest that cortical circuits can be optimised to solve complex contextual problems and proposes a novel view on their computational function . overall our work provides a step towards unifying recurrent networks as used in machine learning with their biological counterparts .", "topics": ["recurrent neural network", "computer vision"]}
{"title": "a neural model for multi-expert architectures", "abstract": "we present a generalization of conventional artificial neural networks that allows for a functional equivalence to multi-expert systems . the new model provides an architectural freedom going beyond existing multi-expert models and an integrative formalism to compare and combine various techniques of learning . ( we consider gradient , em , reinforcement , and unsupervised learning . ) its uniform representation aims at a simple genetic encoding and evolutionary structure optimization of multi-expert systems . this paper contains a detailed description of the model and learning rules , empirically validates its functionality , and discusses future perspectives .", "topics": ["unsupervised learning", "reinforcement learning"]}
{"title": "reading scene text with attention convolutional sequence modeling", "abstract": "reading text in the wild is a challenging task in the field of computer vision . existing approaches mainly adopted connectionist temporal classification ( ctc ) or attention models based on recurrent neural network ( rnn ) , which is computationally expensive and hard to train . in this paper , we present an end-to-end attention convolutional network for scene text recognition . firstly , instead of rnn , we adopt the stacked convolutional layers to effectively capture the contextual dependencies of the input sequence , which is characterized by lower computational complexity and easier parallel computation . compared to the chain structure of recurrent networks , the convolutional neural network ( cnn ) provides a natural way to capture long-term dependencies between elements , which is 9 times faster than bidirectional long short-term memory ( blstm ) . furthermore , in order to enhance the representation of foreground text and suppress the background noise , we incorporate the residual attention modules into a small densely connected network to improve the discriminability of cnn features . we validate the performance of our approach on the standard benchmarks , including the street view text , iiit5k and icdar datasets . as a result , state-of-the-art or highly-competitive performance and efficiency show the superiority of the proposed approach .", "topics": ["recurrent neural network", "computational complexity theory"]}
{"title": "asynchronous multi-context systems", "abstract": "in this work , we present asynchronous multi-context systems ( amcss ) , which provide a framework for loosely coupling different knowledge representation formalisms that allows for online reasoning in a dynamic environment . systems of this kind may interact with the outside world via input and output streams and may therefore react to a continuous flow of external information . in contrast to recent proposals , contexts in an amcs communicate with each other in an asynchronous way which fits the needs of many application domains and is beneficial for scalability . the federal semantics of amcss renders our framework an integration approach rather than a knowledge representation formalism itself . we illustrate the introduced concepts by means of an example scenario dealing with rescue services . in addition , we compare amcss to reactive multi-context systems and describe how to simulate the latter with our novel approach .", "topics": ["scalability"]}
{"title": "deepgauge : comprehensive and multi-granularity testing criteria for gauging the robustness of deep learning systems", "abstract": "deep learning defines a new data-driven programming paradigm that constructs the internal system logic of a crafted neuron network through a set of training data . deep learning ( dl ) has been widely adopted in many safety-critical scenarios . however , a plethora of studies have shown that the state-of-the-art dl systems suffer from various vulnerabilities which can lead to severe consequences when applied to real-world applications . currently , the robustness of a dl system against adversarial attacks is usually measured by the accuracy of test data . considering the limitation of accessible test data , good performance on test data can hardly guarantee the robustness and generality of dl systems . different from traditional software systems which have clear and controllable logic and functionality , a dl system is trained with data and lacks thorough understanding . this makes it difficult for system analysis and defect detection , which could potentially hinder its real-world deployment without safety guarantees . in this paper , we propose deepgauge , a comprehensive and multi-granularity testing criteria for dl systems , which renders a complete and multi-faceted portrayal of the testbed . the in-depth evaluation of our proposed testing criteria is demonstrated on two well-known datasets , five dl systems , with four state-of-the-art adversarial data generation techniques . the effectiveness of deepgauge sheds light on the construction of robust dl systems .", "topics": ["test set"]}
{"title": "an unsupervised method for detection and validation of the optic disc and the fovea", "abstract": "in this work , we have presented a novel method for detection of retinal image features , the optic disc and the fovea , from colour fundus photographs of dilated eyes for computer-aided diagnosis ( cad ) system . a saliency map based method was used to detect the optic disc followed by an unsupervised probabilistic latent semantic analysis for detection validation . the validation concept is based on distinct vessels structures in the optic disc . by using the clinical information of standard location of the fovea with respect to the optic disc , the macula region is estimated . accuracy of 100\\ % detection is achieved for the optic disc and the macula on messidor and diaretdb1 and 98.8\\ % detection accuracy on stare dataset .", "topics": ["unsupervised learning"]}
{"title": "mapping the economic crisis : some preliminary investigations", "abstract": "in this paper we describe our contribution to the poliinformatics 2014 challenge on the 2007-2008 financial crisis . we propose a state of the art technique to extract information from texts and provide different representations , giving first a static overview of the domain and then a dynamic representation of its main evolutions . we show that this strategy provides a practical solution to some recent theories in social sciences that are facing a lack of methods and tools to automatically extract information from natural language texts .", "topics": ["natural language"]}
{"title": "question relevance in vqa : identifying non-visual and false-premise questions", "abstract": "visual question answering ( vqa ) is the task of answering natural-language questions about images . we introduce the novel problem of determining the relevance of questions to images in vqa . current vqa models do not reason about whether a question is even related to the given image ( e.g . what is the capital of argentina ? ) or if it requires information from external resources to answer correctly . this can break the continuity of a dialogue in human-machine interaction . our approaches for determining relevance are composed of two stages . given an image and a question , ( 1 ) we first determine whether the question is visual or not , ( 2 ) if visual , we determine whether the question is relevant to the given image or not . our approaches , based on lstm-rnns , vqa model uncertainty , and caption-question similarity , are able to outperform strong baselines on both relevance tasks . we also present human studies showing that vqa models augmented with such question relevance reasoning are perceived as more intelligent , reasonable , and human-like .", "topics": ["baseline ( configuration management )", "natural language"]}
{"title": "a semi-automated statistical algorithm for object separation", "abstract": "we explicate a semi-automated statistical algorithm for object identification and segregation in both gray scale and color images . the algorithm makes optimal use of the observation that definite objects in an image are typically represented by pixel values having narrow gaussian distributions about characteristic mean values . furthermore , for visually distinct objects , the corresponding gaussian distributions have negligible overlap with each other and hence the mahalanobis distance between these distributions are large . these statistical facts enable one to sub-divide images into multiple thresholds of variable sizes , each segregating similar objects . the procedure incorporates the sensitivity of human eye to the gray pixel values into the variable threshold size , while mapping the gaussian distributions into localized \\delta-functions , for object separation . the effectiveness of this recursive statistical algorithm is demonstrated using a wide variety of images .", "topics": ["pixel"]}
{"title": "ranking sentences for extractive summarization with reinforcement learning", "abstract": "single document summarization is the task of producing a shorter version of a document while preserving its principal information content . in this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the rouge evaluation metric through a reinforcement learning objective . we use our algorithm to train a neural summarization model on the cnn and dailymail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and abstractive systems when evaluated automatically and by humans .", "topics": ["reinforcement learning"]}
{"title": "learning latent sub-events in activity videos using temporal attention filters", "abstract": "in this paper , we newly introduce the concept of temporal attention filters , and describe how they can be used for human activity recognition from videos . many high-level activities are often composed of multiple temporal parts ( e.g . , sub-events ) with different duration/speed , and our objective is to make the model explicitly learn such temporal structure using multiple attention filters and benefit from them . our temporal filters are designed to be fully differentiable , allowing end-of-end training of the temporal filters together with the underlying frame-based or segment-based convolutional neural network architectures . this paper presents an approach of learning a set of optimal static temporal attention filters to be shared across different videos , and extends this approach to dynamically adjust attention filters per testing video using recurrent long short-term memory networks ( lstms ) . this allows our temporal attention filters to learn latent sub-events specific to each activity . we experimentally confirm that the proposed concept of temporal attention filters benefits the activity recognition , and we visualize the learned latent sub-events .", "topics": ["high- and low-level"]}
{"title": "quickedit : editing text & translations via simple delete actions", "abstract": "we propose a framework for computer-assisted text editing . it applies to translation post-editing and to paraphrasing and relies on very simple interactions : a human editor modifies a sentence by marking tokens they would like the system to change . our model then generates a new sentence which reformulates the initial sentence by avoiding the words from the marked tokens . our approach builds upon neural sequence-to-sequence modeling and introduces a neural network which takes as input a sentence along with deleted token markers . our model is trained on translation bi-text by simulating post-edits . our results on post-editing for machine translation and paraphrasing evaluate the performance of our approach . we show +11.4 bleu with limited post-editing effort on the wmt-14 english-german translation task ( 25.2 to 36.6 ) , which represents +5.9 bleu over the post-editing baseline ( 30.7 to 36.6 ) .", "topics": ["baseline ( configuration management )", "machine translation"]}
{"title": "joint gaussian processes for biophysical parameter retrieval", "abstract": "solving inverse problems is central to geosciences and remote sensing . radiative transfer models ( rtms ) represent mathematically the physical laws which govern the phenomena in remote sensing applications ( forward models ) . the numerical inversion of the rtm equations is a challenging and computationally demanding problem , and for this reason , often the application of a nonlinear statistical regression is preferred . in general , regression models predict the biophysical parameter of interest from the corresponding received radiance . however , this approach does not employ the physical information encoded in the rtms . an alternative strategy , which attempts to include the physical knowledge , consists in learning a regression model trained using data simulated by an rtm code . in this work , we introduce a nonlinear nonparametric regression model which combines the benefits of the two aforementioned approaches . the inversion is performed taking into account jointly both real observations and rtm-simulated data . the proposed joint gaussian process ( jgp ) provides a solid framework for exploiting the regularities between the two types of data . the jgp automatically detects the relative quality of the simulated and real data , and combines them accordingly . this occurs by learning an additional hyper-parameter w.r.t . a standard gp model , and fitting parameters through maximizing the pseudo-likelihood of the real observations . the resulting scheme is both simple and robust , i.e . , capable of adapting to different scenarios . the advantages of the jgp method compared to benchmark strategies are shown considering rtm-simulated and real observations in different experiments . specifically , we consider leaf area index ( lai ) retrieval from landsat data combined with simulated data generated by the prosail model .", "topics": ["nonlinear system", "numerical analysis"]}
{"title": "efficient batchwise dropout training using submatrices", "abstract": "dropout is a popular technique for regularizing artificial neural networks . dropout networks are generally trained by minibatch gradient descent with a dropout mask turning off some of the units -- -a different pattern of dropout is applied to every sample in the minibatch . we explore a very simple alternative to the dropout mask . instead of masking dropped out units by setting them to zero , we perform matrix multiplication using a submatrix of the weight matrix -- -unneeded hidden units are never calculated . performing dropout batchwise , so that one pattern of dropout is used for each sample in a minibatch , we can substantially reduce training times . batchwise dropout can be used with fully-connected and convolutional neural networks .", "topics": ["test set", "nonlinear system"]}
{"title": "agreement-based learning of parallel lexicons and phrases from non-parallel corpora", "abstract": "we introduce an agreement-based approach to learning parallel lexicons and phrases from non-parallel corpora . the basic idea is to encourage two asymmetric latent-variable translation models ( i.e . , source-to-target and target-to-source ) to agree on identifying latent phrase and word alignments . the agreement is defined at both word and phrase levels . we develop a viterbi em algorithm for jointly training the two unidirectional models efficiently . experiments on the chinese-english dataset show that agreement-based learning significantly improves both alignment and translation performance .", "topics": ["text corpus"]}
{"title": "training conditional random fields with natural gradient descent", "abstract": "we propose a novel parameter estimation procedure that works efficiently for conditional random fields ( crf ) . this algorithm is an extension to the maximum likelihood estimation ( mle ) , using loss functions defined by bregman divergences which measure the proximity between the model expectation and the empirical mean of the feature vectors . this leads to a flexible training framework from which multiple update strategies can be derived using natural gradient descent ( ngd ) . we carefully choose the convex function inducing the bregman divergence so that the types of updates are reduced , while making the optimization procedure more effective by transforming the gradients of the log-likelihood loss function . the derived algorithms are very simple and can be easily implemented on top of the existing stochastic gradient descent ( sgd ) optimization procedure , yet it is very effective as illustrated by experimental results .", "topics": ["loss function", "gradient descent"]}
{"title": "simulation optimization of the crossdock door assignment problem", "abstract": "the purpose of this report is to present the crossdock door assignment problem , which involves assigning destinations to outbound dock doors of crossdock centres such that travel distance by material handling equipment is minimized . we propose a two fold solution ; simulation and optimization of the simulation model simulation optimization . the novel aspect of our solution approach is that we intend to use simulation to derive a more realistic objective function and use memetic algorithms to find an optimal solution . the main advantage of using memetic algorithms is that it combines a local search with genetic algorithms . the crossdock door assignment problem is a new domain application to memetic algorithms and it is yet unknown how it will perform .", "topics": ["approximation algorithm", "optimization problem"]}
{"title": "redundancy techniques for straggler mitigation in distributed optimization and learning", "abstract": "performance of distributed optimization and learning systems is bottlenecked by `` straggler '' nodes and slow communication links , which significantly delay computation . we propose a distributed optimization framework where the dataset is `` encoded '' to have an over-complete representation with built-in redundancy , and the straggling nodes in the system are dynamically left out of the computation at every iteration , whose loss is compensated by the embedded redundancy . we show that oblivious application of several popular optimization algorithms on encoded data , including gradient descent , l-bfgs , proximal gradient under data parallelism , and coordinate descent under model parallelism , converge to either approximate or exact solutions of the original problem when stragglers are treated as erasures . these convergence results are deterministic , i.e . , they establish sample path convergence for arbitrary sequences of delay patterns or distributions on the nodes , and are independent of the tail behavior of the delay distribution . we demonstrate that equiangular tight frames have desirable properties as encoding matrices , and propose efficient mechanisms for encoding large-scale data . we implement the proposed technique on amazon ec2 clusters , and demonstrate its performance over several learning problems , including matrix factorization , lasso , ridge regression and logistic regression , and compare the proposed method with uncoded , asynchronous , and data replication strategies .", "topics": ["approximation algorithm", "cluster analysis"]}
{"title": "component-based distributed framework for coherent and real-time video dehazing", "abstract": "traditional dehazing techniques , as a well studied topic in image processing , are now widely used to eliminate the haze effects from individual images . however , even the state-of-the-art dehazing algorithms may not provide sufficient support to video analytics , as a crucial pre-processing step for video-based decision making systems ( e.g . , robot navigation ) , due to the limitations of these algorithms on poor result coherence and low processing efficiency . this paper presents a new framework , particularly designed for video dehazing , to output coherent results in real time , with two novel techniques . firstly , we decompose the dehazing algorithms into three generic components , namely transmission map estimator , atmospheric light estimator and haze-free image generator . they can be simultaneously processed by multiple threads in the distributed system , such that the processing efficiency is optimized by automatic cpu resource allocation based on the workloads . secondly , a cross-frame normalization scheme is proposed to enhance the coherence among consecutive frames , by sharing the parameters of atmospheric light from consecutive frames in the distributed computation platform . the combination of these techniques enables our framework to generate highly consistent and accurate dehazing results in real-time , by using only 3 pcs connected by ethernet .", "topics": ["image processing"]}
{"title": "picanet : learning pixel-wise contextual attention in convnets and its application in saliency detection", "abstract": "context plays an important role in many computer vision tasks . previous models usually construct contextual information from the whole context region . however , not all context locations are helpful and some of them may be detrimental to the final task . to solve this problem , we propose a novel pixel-wise contextual attention network , i.e . , the picanet , to learn to selectively attend to informative context locations for each pixel . specifically , it can generate an attention map over the context region for each pixel , where each attention weight corresponds to the contextual relevance of each context location w.r.t . the specified pixel location . thus , an attended contextual feature can be constructed by using the attention map to aggregate the contextual features . we formulate picanet in a global form and a local form to attend to global contexts and local contexts , respectively . our designs for the two forms are both fully differentiable . thus they can be embedded into any cnn architectures for various computer vision tasks in an end-to-end manner . we take saliency detection as an example application to demonstrate the effectiveness of the proposed picanets . specifically , we embed global and local picanets into an encoder-decoder convnet hierarchically . thorough analyses indicate that the global picanet helps to construct global contrast while the local picanets help to enhance the feature maps to be more homogenous , thus making saliency detection results more accurate and uniform . as a result , our proposed saliency model achieves state-of-the-art results on 4 benchmark datasets .", "topics": ["computer vision", "map"]}
{"title": "stability of matrix factorization for collaborative filtering", "abstract": "we study the stability vis a vis adversarial noise of matrix factorization algorithm for matrix completion . in particular , our results include : ( i ) we bound the gap between the solution matrix of the factorization method and the ground truth in terms of root mean square error ; ( ii ) we treat the matrix factorization as a subspace fitting problem and analyze the difference between the solution subspace and the ground truth ; ( iii ) we analyze the prediction error of individual users based on the subspace stability . we apply these results to the problem of collaborative filtering under manipulator attack , which leads to useful insights and guidelines for collaborative filtering system design .", "topics": ["ground truth"]}
{"title": "non-markovian control with gated end-to-end memory policy networks", "abstract": "partially observable environments present an important open challenge in the domain of sequential control learning with delayed rewards . despite numerous attempts during the two last decades , the majority of reinforcement learning algorithms and associated approximate models , applied to this context , still assume markovian state transitions . in this paper , we explore the use of a recently proposed attention-based model , the gated end-to-end memory network , for sequential control . we call the resulting model the gated end-to-end memory policy network . more precisely , we use a model-free value-based algorithm to learn policies for partially observed domains using this memory-enhanced neural network . this model is end-to-end learnable and it features unbounded memory . indeed , because of its attention mechanism and associated non-parametric memory , the proposed model allows us to define an attention mechanism over the observation stream unlike recurrent models . we show encouraging results that illustrate the capability of our attention-based model in the context of the continuous-state non-stationary control problem of stock trading . we also present an openai gym environment for simulated stock exchange and explain its relevance as a benchmark for the field of non-markovian decision process learning .", "topics": ["approximation algorithm", "reinforcement learning"]}
{"title": "a probabilistic methodology for multilabel classification", "abstract": "multilabel classification is a relatively recent subfield of machine learning . unlike to the classical approach , where instances are labeled with only one category , in multilabel classification , an arbitrary number of categories is chosen to label an instance . due to the problem complexity ( the solution is one among an exponential number of alternatives ) , a very common solution ( the binary method ) is frequently used , learning a binary classifier for every category , and combining them all afterwards . the assumption taken in this solution is not realistic , and in this work we give examples where the decisions for all the labels are not taken independently , and thus , a supervised approach should learn those existing relationships among categories to make a better classification . therefore , we show here a generic methodology that can improve the results obtained by a set of independent probabilistic binary classifiers , by using a combination procedure with a classifier trained on the co-occurrences of the labels . we show an exhaustive experimentation in three different standard corpora of labeled documents ( reuters-21578 , ohsumed-23 and rcv1 ) , which present noticeable improvements in all of them , when using our methodology , in three probabilistic base classifiers .", "topics": ["statistical classification", "supervised learning"]}
{"title": "theoretical insights into the optimization landscape of over-parameterized shallow neural networks", "abstract": "in this paper we study the problem of learning a shallow artificial neural network that best fits a training data set . we study this problem in the over-parameterized regime where the number of observations are fewer than the number of parameters in the model . we show that with quadratic activations the optimization landscape of training such shallow neural networks has certain favorable characteristics that allow globally optimal models to be found efficiently using a variety of local search heuristics . this result holds for an arbitrary training data of input/output pairs . for differentiable activation functions we also show that gradient descent , when suitably initialized , converges at a linear rate to a globally optimal model . this result focuses on a realizable model where the inputs are chosen i.i.d . from a gaussian distribution and the labels are generated according to planted weight coefficients .", "topics": ["test set", "mathematical optimization"]}
{"title": "binarized convolutional neural networks with separable filters for efficient hardware acceleration", "abstract": "state-of-the-art convolutional neural networks are enormously costly in both compute and memory , demanding massively parallel gpus for execution . such networks strain the computational capabilities and energy available to embedded and mobile processing platforms , restricting their use in many important applications . in this paper , we push the boundaries of hardware-effective cnn design by proposing bcnn with separable filters ( bcnnw/sf ) , which applies singular value decomposition ( svd ) on bcnn kernels to further reduce computational and storage complexity . to enable its implementation , we provide a closed form of the gradient over svd to calculate the exact gradient with respect to every binarized weight in backward propagation . we verify bcnnw/sf on the mnist , cifar-10 , and svhn datasets , and implement an accelerator for cifar-10 on fpga hardware . our bcnnw/sf accelerator realizes memory savings of 17 % and execution time reduction of 31.3 % compared to bcnn with only minor accuracy sacrifices .", "topics": ["gradient", "mnist database"]}
{"title": "take and took , gaggle and goose , book and read : evaluating the utility of vector differences for lexical relation learning", "abstract": "recent work on word embeddings has shown that simple vector subtraction over pre-trained embeddings is surprisingly effective at capturing different lexical relations , despite lacking explicit supervision . prior work has evaluated this intriguing result using a word analogy prediction formulation and hand-selected relations , but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated . in this paper , we carry out such an evaluation in two learning settings : ( 1 ) spectral clustering to induce word relations , and ( 2 ) supervised learning to classify vector differences into relation types . we find that word embeddings capture a surprising amount of information , and that , under suitable supervised training , vector subtraction generalises well to a broad range of relations , including over unseen lexical items .", "topics": ["supervised learning", "cluster analysis"]}
{"title": "incorporating causal prior knowledge as path-constraints in bayesian networks and maximal ancestral graphs", "abstract": "we consider the incorporation of causal knowledge about the presence or absence of ( possibly indirect ) causal relations into a causal model . such causal relations correspond to directed paths in a causal model . this type of knowledge naturally arises from experimental data , among others . specifically , we consider the formalisms of causal bayesian networks and maximal ancestral graphs and their markov equivalence classes : partially directed acyclic graphs and partially oriented ancestral graphs . we introduce sound and complete procedures which are able to incorporate causal prior knowledge in such models . in simulated experiments , we show that often considering even a few causal facts leads to a significant number of new inferences . in a case study , we also show how to use real experimental data to infer causal knowledge and incorporate it into a real biological causal network . the code is available at mensxmachina.org .", "topics": ["simulation", "bayesian network"]}
{"title": "bregman iteration for correspondence problems : a study of optical flow", "abstract": "bregman iterations are known to yield excellent results for denoising , deblurring and compressed sensing tasks , but so far this technique has rarely been used for other image processing problems . in this paper we give a thorough description of the bregman iteration , unifying thereby results of different authors within a common framework . then we show how to adapt the split bregman iteration , originally developed by goldstein and osher for image restoration purposes , to optical flow which is a fundamental correspondence problem in computer vision . we consider some classic and modern optical flow models and present detailed algorithms that exhibit the benefits of the bregman iteration . by making use of the results of the bregman framework , we address the issues of convergence and error estimation for the algorithms . numerical examples complement the theoretical part .", "topics": ["image processing", "numerical analysis"]}
{"title": "causal rule sets for identifying subgroups with enhanced treatment effect", "abstract": "we introduce a novel generative model for interpretable subgroup analysis for causal inference applications , causal rule sets ( crs ) . a crs model uses a small set of short rules to capture a subgroup where the average treatment effect is elevated compared to the entire population . we present a bayesian framework for learning a causal rule set . the bayesian framework consists of a prior that favors simpler models and a bayesian logistic regression that characterizes the relation between outcomes , attributes and subgroup membership . we find maximum a posteriori models using discrete monte carlo steps in the joint solution space of rules sets and parameters . we provide theoretically grounded heuristics and bounding strategies to improve search efficiency . experiments show that the search algorithm can efficiently recover a true underlying subgroup and crs shows consistently competitive performance compared to other state-of-the-art baseline methods .", "topics": ["baseline ( configuration management )", "generative model"]}
{"title": "development and evolution of neural networks in an artificial chemistry", "abstract": "we present a model of decentralized growth for artificial neural networks ( anns ) inspired by the development and the physiology of real nervous systems . in this model , each individual artificial neuron is an autonomous unit whose behavior is determined only by the genetic information it harbors and local concentrations of substrates modeled by a simple artificial chemistry . gene expression is manifested as axon and dendrite growth , cell division and differentiation , substrate production and cell stimulation . we demonstrate the model 's power with a hand-written genome that leads to the growth of a simple network which performs classical conditioning . to evolve more complex structures , we implemented a platform-independent , asynchronous , distributed genetic algorithm ( ga ) that allows users to participate in evolutionary experiments via the world wide web .", "topics": ["neural networks", "autonomous car"]}
{"title": "computational mapping of the ground reflectivity with laser scanners", "abstract": "in this investigation we focus on the problem of mapping the ground reflectivity with multiple laser scanners mounted on mobile robots/vehicles . the problem originates because regions of the ground become populated with a varying number of reflectivity measurements whose value depends on the observer and its corresponding perspective . here , we propose a novel automatic , data-driven computational mapping framework specifically aimed at preserving edge sharpness in the map reconstruction process and that considers the sources of measurement variation . our new formulation generates map-perspective gradients and applies sub-set selection fusion and de-noising operators to these through iterative algorithms that minimize an $ \\ell_1 $ sparse regularized least squares formulation . reconstruction of the ground reflectivity is then carried out based on poisson 's formulation posed as an $ \\ell_2 $ term promoting consistency with the fused gradient of map-perspectives and a term that ensures equality constraints with reference measurement data . we demonstrate our new framework outperforms the capabilities of existing ones with experiments realized on ford 's fleet of autonomous vehicles . for example , we show we can achieve map enhancement ( i.e . , contrast enhancement ) , artifact removal , de-noising and map-stitching without requiring an additional reflectivity adjustment to calibrate sensors to the specific mounting and robot/vehicle motion .", "topics": ["sparse matrix", "gradient"]}
{"title": "search improves label for active learning", "abstract": "we investigate active learning with access to two distinct oracles : label ( which is standard ) and search ( which is not ) . the search oracle models the situation where a human searches a database to seed or counterexample an existing solution . search is stronger than label while being natural to implement in many situations . we show that an algorithm using both oracles can provide exponentially large problem-dependent improvements over label alone .", "topics": ["supervised learning", "time complexity"]}
{"title": "assessing threat of adversarial examples on deep neural networks", "abstract": "deep neural networks are facing a potential security threat from adversarial examples , inputs that look normal but cause an incorrect classification by the deep neural network . for example , the proposed threat could result in hand-written digits on a scanned check being incorrectly classified but looking normal when humans see them . this research assesses the extent to which adversarial examples pose a security threat , when one considers the normal image acquisition process . this process is mimicked by simulating the transformations that normally occur in acquiring the image in a real world application , such as using a scanner to acquire digits for a check amount or using a camera in an autonomous car . these small transformations negate the effect of the carefully crafted perturbations of adversarial examples , resulting in a correct classification by the deep neural network . thus just acquiring the image decreases the potential impact of the proposed security threat . we also show that the already widely used process of averaging over multiple crops neutralizes most adversarial examples . normal preprocessing , such as text binarization , almost completely neutralizes adversarial examples . this is the first paper to show that for text driven classification , adversarial examples are an academic curiosity , not a security threat .", "topics": ["neural networks", "simulation"]}
{"title": "weighted radial variation for node feature classification", "abstract": "connections created from a node-edge matrix have been traditionally difficult to visualize and analyze because of the number of flows to be rendered in a limited feature or cartographic space . because analyzing connectivity patterns is useful for understanding the complex dynamics of human and information flow that connect non-adjacent space , techniques that allow for visual data mining or static representations of system dynamics are a growing field of research . here , we create a weighted radial variation ( wrv ) technique to classify a set of nodes based on the configuration of their radially-emanating vector flows . each entity 's vector is syncopated in terms of cardinality , direction , length , and flow magnitude . the wrv process unravels each star-like entity 's individual flow vectors on a 0-360 { \\deg } spectrum , to form a unique signal whose distribution depends on the flow presence at each step around the entity , and is further characterized by flow distance and magnitude . the signals are processed with an unsupervised classification method that clusters entities with similar signatures in order to provide a typology for each node in the system of spatial flows . we use a case study of u.s. county-to-county human incoming and outgoing migration data to test our method .", "topics": ["data mining", "unsupervised learning"]}
{"title": "quasi-bayesian strategies for efficient plan generation : application to the planning to observe problem", "abstract": "quasi-bayesian theory uses convex sets of probability distributions and expected loss to represent preferences about plans . the theory focuses on decision robustness , i.e . , the extent to which plans are affected by deviations in subjective assessments of probability . the present work presents solutions for plan generation when robustness of probability assessments must be included : plans contain information about the robustness of certain actions . the surprising result is that some problems can be solved faster in the quasi-bayesian framework than within usual bayesian theory . we investigate this on the planning to observe problem , i.e . , an agent must decide whether to take new observations or not . the fundamental question is : how , and how much , to search for a `` best '' plan , based on the robustness of probability assessments ? plan generation algorithms are derived in the context of material classification with an acoustic robotic probe . a package that constructs quasi-bayesian plans is available through anonymous ftp .", "topics": ["robot"]}
{"title": "factoring nonnegative matrices with linear programs", "abstract": "this paper describes a new approach , based on linear programming , for computing nonnegative matrix factorizations ( nmfs ) . the key idea is a data-driven model for the factorization where the most salient features in the data are used to express the remaining features . more precisely , given a data matrix x , the algorithm identifies a matrix c such that x approximately equals cx and some linear constraints . the constraints are chosen to ensure that the matrix c selects features ; these features can then be used to find a low-rank nmf of x . a theoretical analysis demonstrates that this approach has guarantees similar to those of the recent nmf algorithm of arora et al . ( 2012 ) . in contrast with this earlier work , the proposed method extends to more general noise models and leads to efficient , scalable algorithms . experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice . an optimized c++ implementation can factor a multigigabyte matrix in a matter of minutes .", "topics": ["synthetic data", "scalability"]}
{"title": "bayesian linear regression with student-t assumptions", "abstract": "as an automatic method of determining model complexity using the training data alone , bayesian linear regression provides us a principled way to select hyperparameters . but one often needs approximation inference if distribution assumption is beyond gaussian distribution . in this paper , we propose a bayesian linear regression model with student-t assumptions ( blrs ) , which can be inferred exactly . in this framework , both conjugate prior and expectation maximization ( em ) algorithm are generalized . meanwhile , we prove that the maximum likelihood solution is equivalent to the standard bayesian linear regression with gaussian assumptions ( blrg ) . the $ q $ -em algorithm for blrs is nearly identical to the em algorithm for blrg . it is showed that $ q $ -em for blrs can converge faster than em for blrg for the task of predicting online news popularity .", "topics": ["test set"]}
{"title": "information-theoretic approach to efficient adaptive path planning for mobile robotic environmental sensing", "abstract": "recent research in robot exploration and mapping has focused on sampling environmental hotspot fields . this exploration task is formalized by low , dolan , and khosla ( 2008 ) in a sequential decision-theoretic planning under uncertainty framework called masp . the time complexity of solving masp approximately depends on the map resolution , which limits its use in large-scale , high-resolution exploration and mapping . to alleviate this computational difficulty , this paper presents an information-theoretic approach to masp ( imasp ) for efficient adaptive path planning ; by reformulating the cost-minimizing imasp as a reward-maximizing problem , its time complexity becomes independent of map resolution and is less sensitive to increasing robot team size as demonstrated both theoretically and empirically . using the reward-maximizing dual , we derive a novel adaptive variant of maximum entropy sampling , thus improving the induced exploration policy performance . it also allows us to establish theoretical bounds quantifying the performance advantage of optimal adaptive over non-adaptive policies and the performance quality of approximately optimal vs. optimal adaptive policies . we show analytically and empirically the superior performance of imasp-based policies for sampling the log-gaussian process to that of policies for the widely-used gaussian process in mapping the hotspot field . lastly , we provide sufficient conditions that , when met , guarantee adaptivity has no benefit under an assumed environment model .", "topics": ["sampling ( signal processing )", "time complexity"]}
{"title": "fast semantic segmentation on video using motion vector-based feature interpolation", "abstract": "models optimized for accuracy on challenging , dense prediction tasks such as semantic segmentation entail significant inference costs , and are prohibitively slow to run on each frame in a video . since nearby video frames are spatially similar , however , there is substantial opportunity to reuse computation . existing work has explored basic feature reuse and feature warping based on optical flow , but has encountered limits to the speedup attainable with these techniques . in this paper , we present a new , two part approach to accelerating inference on video . firstly , we propose a fast feature propagation scheme that utilizes the block motion vector maps present in compressed video to cheaply propagate features from frame to frame . secondly , we develop a novel feature estimation scheme , termed feature interpolation , that fuses features propagated from enclosing keyframes to render accurate feature estimates , even at sparse keyframe frequencies . we evaluate our system on the cityscapes dataset , comparing to both a frame-by-frame baseline and related work . we find that we are able to substantially accelerate segmentation on video , achieving almost twice the average inference speed as prior work at any target accuracy level .", "topics": ["baseline ( configuration management )", "map"]}
{"title": "fast shadow detection from a single image using a patched convolutional neural network", "abstract": "in recent years , various shadow detection methods from a single image have been proposed and used in vision systems ; however , most of them are not appropriate for the robotic applications due to the expensive time complexity . this paper introduces a fast shadow detection method using a deep learning framework , with a time cost that is appropriate for robotic applications . in our solution , we first obtain a shadow prior map with the help of multi-class support vector machine using statistical features . then , we use a semantic- aware patch-level convolutional neural network that efficiently trains on shadow examples by combining the original image and the shadow prior map . experiments on benchmark datasets demonstrate the proposed method significantly decreases the time complexity of shadow detection , by one or two orders of magnitude compared with state-of-the-art methods , without losing accuracy .", "topics": ["support vector machine", "time complexity"]}
{"title": "image restoration using total variation with overlapping group sparsity", "abstract": "image restoration is one of the most fundamental issues in imaging science . total variation ( tv ) regularization is widely used in image restoration problems for its capability to preserve edges . in the literature , however , it is also well known for producing staircase-like artifacts . usually , the high-order total variation ( htv ) regularizer is an good option except its over-smoothing property . in this work , we study a minimization problem where the objective includes an usual $ l_2 $ data-fidelity term and an overlapping group sparsity total variation regularizer which can avoid staircase effect and allow edges preserving in the restored image . we also proposed a fast algorithm for solving the corresponding minimization problem and compare our method with the state-of-the-art tv based methods and htv based method . the numerical experiments illustrate the efficiency and effectiveness of the proposed method in terms of psnr , relative error and computing time .", "topics": ["numerical analysis", "matrix regularization"]}
{"title": "construction of neural networks for realization of localized deep learning", "abstract": "the subject of deep learning has recently attracted users of machine learning from various disciplines , including : medical diagnosis and bioinformatics , financial market analysis and online advertisement , speech and handwriting recognition , computer vision and natural language processing , time series forecasting , and search engines . however , theoretical development of deep learning is still at its infancy . the objective of this paper is to introduce a deep neural network ( also called deep-net ) approach to localized manifold learning , with each hidden layer endowed with a specific learning task . for the purpose of illustrations , we only focus on deep-nets with three hidden layers , with the first layer for dimensionality reduction , the second layer for bias reduction , and the third layer for variance reduction . a feedback component also designed to eliminate outliers . the main theoretical result in this paper is the order $ \\mathcal o\\left ( m^ { -2s/ ( 2s+d ) } \\right ) $ of approximation of the regression function with regularity $ s $ , in terms of the number $ m $ of sample points , where the ( unknown ) manifold dimension $ d $ replaces the dimension $ d $ of the sampling ( euclidean ) space for shallow nets .", "topics": ["sampling ( signal processing )", "natural language processing"]}
{"title": "self-normalizing neural networks", "abstract": "deep learning has revolutionized vision via convolutional neural networks ( cnns ) and natural language processing via recurrent neural networks ( rnns ) . however , success stories of deep learning with standard feed-forward neural networks ( fnns ) are rare . fnns that perform well are typically shallow and , therefore can not exploit many levels of abstract representations . we introduce self-normalizing neural networks ( snns ) to enable high-level abstract representations . while batch normalization requires explicit normalization , neuron activations of snns automatically converge towards zero mean and unit variance . the activation function of snns are `` scaled exponential linear units '' ( selus ) , which induce self-normalizing properties . using the banach fixed-point theorem , we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations . this convergence property of snns allows to ( 1 ) train deep networks with many layers , ( 2 ) employ strong regularization , and ( 3 ) to make learning highly robust . furthermore , for activations not close to unit variance , we prove an upper and lower bound on the variance , thus , vanishing and exploding gradients are impossible . we compared snns on ( a ) 121 tasks from the uci machine learning repository , on ( b ) drug discovery benchmarks , and on ( c ) astronomy tasks with standard fnns and other machine learning methods such as random forests and support vector machines . snns significantly outperformed all competing fnn methods at 121 uci tasks , outperformed all competing methods at the tox21 dataset , and set a new record at an astronomy data set . the winning snn architectures are often very deep . implementations are available at : github.com/bioinf-jku/snns .", "topics": ["support vector machine", "recurrent neural network"]}
{"title": "minimum weight perfect matching via blossom belief propagation", "abstract": "max-product belief propagation ( bp ) is a popular message-passing algorithm for computing a maximum-a-posteriori ( map ) assignment over a distribution represented by a graphical model ( gm ) . it has been shown that bp can solve a number of combinatorial optimization problems including minimum weight matching , shortest path , network flow and vertex cover under the following common assumption : the respective linear programming ( lp ) relaxation is tight , i.e . , no integrality gap is present . however , when lp shows an integrality gap , no model has been known which can be solved systematically via sequential applications of bp . in this paper , we develop the first such algorithm , coined blossom-bp , for solving the minimum weight matching problem over arbitrary graphs . each step of the sequential algorithm requires applying bp over a modified graph constructed by contractions and expansions of blossoms , i.e . , odd sets of vertices . our scheme guarantees termination in o ( n^2 ) of bp runs , where n is the number of vertices in the original graph . in essence , the blossom-bp offers a distributed version of the celebrated edmonds ' blossom algorithm by jumping at once over many sub-steps with a single bp . moreover , our result provides an interpretation of the edmonds ' algorithm as a sequence of lps .", "topics": ["graphical model"]}
{"title": "learning conversational systems that interleave task and non-task content", "abstract": "task-oriented dialog systems have been applied in various tasks , such as automated personal assistants , customer service providers and tutors . these systems work well when users have clear and explicit intentions that are well-aligned to the systems ' capabilities . however , they fail if users intentions are not explicit . to address this shortcoming , we propose a framework to interleave non-task content ( i.e . everyday social conversation ) into task conversations . when the task content fails , the system can still keep the user engaged with the non-task content . we trained a policy using reinforcement learning algorithms to promote long-turn conversation coherence and consistency , so that the system can have smooth transitions between task and non-task content . to test the effectiveness of the proposed framework , we developed a movie promotion dialog system . experiments with human users indicate that a system that interleaves social and task content achieves a better task success rate and is also rated as more engaging compared to a pure task-oriented system .", "topics": ["reinforcement learning"]}
{"title": "consistent individualized feature attribution for tree ensembles", "abstract": "interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is important , yet feature attribution for trees is often heuristic and not individualized for each prediction . here we show that popular feature attribution methods are inconsistent , meaning they can lower a feature 's assigned importance when the true impact of that feature actually increases . this is a fundamental problem that casts doubt on any comparison between features . to address it we turn to recent applications of game theory and develop fast exact tree solutions for shap ( shapley additive explanation ) values , which are the unique consistent and locally accurate attribution values . we then extend shap values to interaction effects and define shap interaction values . we propose a rich visualization of individualized feature attributions that improves over classic attribution summaries and partial dependence plots , and a unique `` supervised '' clustering ( clustering based on feature attributions ) . we demonstrate better agreement with human intuition through a user study , exponential improvements in run time , improved clustering performance , and better identification of influential features . an implementation of our algorithm has also been merged into xgboost and lightgbm , see http : //github.com/slundberg/shap for details .", "topics": ["cluster analysis", "time complexity"]}
{"title": "binding via reconstruction clustering", "abstract": "disentangled distributed representations of data are desirable for machine learning , since they are more expressive and can generalize from fewer examples . however , for complex data , the distributed representations of multiple objects present in the same input can interfere and lead to ambiguities , which is commonly referred to as the binding problem . we argue for the importance of the binding problem to the field of representation learning , and develop a probabilistic framework that explicitly models inputs as a composition of multiple objects . we propose an unsupervised algorithm that uses denoising autoencoders to dynamically bind features together in multi-object inputs through an expectation-maximization-like clustering process . the effectiveness of this method is demonstrated on artificially generated datasets of binary images , showing that it can even generalize to bind together new objects never seen by the autoencoder during training .", "topics": ["feature learning", "cluster analysis"]}
{"title": "learning planar ising models", "abstract": "inference and learning of graphical models are both well-studied problems in statistics and machine learning that have found many applications in science and engineering . however , exact inference is intractable in general graphical models , which suggests the problem of seeking the best approximation to a collection of random variables within some tractable family of graphical models . in this paper , we focus our attention on the class of planar ising models , for which inference is tractable using techniques of statistical physics [ kac and ward ; kasteleyn ] . based on these techniques and recent methods for planarity testing and planar embedding [ chrobak and payne ] , we propose a simple greedy algorithm for learning the best planar ising model to approximate an arbitrary collection of binary random variables ( possibly from sample data ) . given the set of all pairwise correlations among variables , we select a planar graph and optimal planar ising model defined on this graph to best approximate that set of correlations . we demonstrate our method in some simulations and for the application of modeling senate voting records .", "topics": ["graphical model", "approximation algorithm"]}
{"title": "a generative model of natural texture surrogates", "abstract": "natural images can be viewed as patchworks of different textures , where the local image statistics is roughly stationary within a small neighborhood but otherwise varies from region to region . in order to model this variability , we first applied the parametric texture algorithm of portilla and simoncelli to image patches of 64x64 pixels in a large database of natural images such that each image patch is then described by 655 texture parameters which specify certain statistics , such as variances and covariances of wavelet coefficients or coefficient magnitudes within that patch . to model the statistics of these texture parameters , we then developed suitable nonlinear transformations of the parameters that allowed us to fit their joint statistics with a multivariate gaussian distribution . we find that the first 200 principal components contain more than 99 % of the variance and are sufficient to generate textures that are perceptually extremely close to those generated with all 655 components . we demonstrate the usefulness of the model in several ways : ( 1 ) we sample ensembles of texture patches that can be directly compared to samples of patches from the natural image database and can to a high degree reproduce their perceptual appearance . ( 2 ) we further developed an image compression algorithm which generates surprisingly accurate images at bit rates as low as 0.14 bits/pixel . finally , ( 3 ) we demonstrate how our approach can be used for an efficient and objective evaluation of samples generated with probabilistic models of natural images .", "topics": ["generative model", "coefficient"]}
{"title": "learning and analyzing vector encoding of symbolic representations", "abstract": "we present a formal language with expressions denoting general symbol structures and queries which access information in those structures . a sequence-to-sequence network processing this language learns to encode symbol structures and query them . the learned representation ( approximately ) shares a simple linearity property with theoretical techniques for performing this task .", "topics": ["natural language processing", "computation"]}
{"title": "evolved policy gradients", "abstract": "we propose a meta-learning approach for learning gradient-based reinforcement learning ( rl ) algorithms . the idea is to evolve a differentiable loss function , such that an agent , which optimizes its policy to minimize this loss , will achieve high rewards . the loss is parametrized via temporal convolutions over the agent 's experience . because this loss is highly flexible in its ability to take into account the agent 's history , it enables fast task learning and eliminates the need for reward shaping at test time . empirical results show that our evolved policy gradient algorithm achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method . moreover , at test time , our learner optimizes only its learned loss function , and requires no explicit reward signal . in effect , the agent internalizes the reward structure , suggesting a direction toward agents that learn to solve new tasks simply from intrinsic motivation .", "topics": ["reinforcement learning", "loss function"]}
{"title": "probabilistic adaptive computation time", "abstract": "we present a probabilistic model with discrete latent variables that control the computation time in deep learning models such as resnets and lstms . a prior on the latent variables expresses the preference for faster computation . the amount of computation for an input is determined via amortized maximum a posteriori ( map ) inference . map inference is performed using a novel stochastic variational optimization method . the recently proposed adaptive computation time mechanism can be seen as an ad-hoc relaxation of this model . we demonstrate training using the general-purpose concrete relaxation of discrete variables . evaluation on resnet shows that our method matches the speed-accuracy trade-off of adaptive computation time , while allowing for evaluation with a simple deterministic procedure that has a lower memory footprint .", "topics": ["calculus of variations", "time complexity"]}
{"title": "hwnet v2 : an efficient word image representation for handwritten documents", "abstract": "we present a framework for learning efficient holistic representation for handwritten word images . the proposed method uses a deep convolutional neural network with traditional classification loss . the major strengths of our work lie in : ( i ) the efficient usage of synthetic data to pre-train a deep network , ( ii ) an adapted version of resnet-34 architecture with region of interest pooling ( referred as hwnet v2 ) which learns discriminative features with variable sized word images , and ( iii ) realistic augmentation of training data with multiple scales and elastic distortion which mimics the natural process of handwriting . we further investigate the process of fine-tuning at various layers to reduce the domain gap between synthetic and real domain and also analyze the in-variances learned at different layers using recent visualization techniques proposed in literature . our representation leads to state of the art word spotting performance on standard handwritten datasets and historical manuscripts in different languages with minimal representation size . on the challenging iam dataset , our method is first to report an map above 0.90 for word spotting with a representation size of just 32 dimensions . further more , we also present results on printed document datasets in english and indic scripts which validates the generic nature of the proposed framework for learning word image representation .", "topics": ["test set", "synthetic data"]}
{"title": "efficient first order methods for linear composite regularizers", "abstract": "a wide class of regularization problems in machine learning and statistics employ a regularization term which is obtained by composing a simple convex function \\omega with a linear transformation . this setting includes group lasso methods , the fused lasso and other total variation methods , multi-task learning methods and many more . in this paper , we present a general approach for computing the proximity operator of this class of regularizers , under the assumption that the proximity operator of the function \\omega is known in advance . our approach builds on a recent line of research on optimal first order optimization methods and uses fixed point iterations for numerically computing the proximity operator . it is more general than current approaches and , as we show with numerical simulations , computationally more efficient than available first order methods which do not achieve the optimal rate . in particular , our method outperforms state of the art o ( 1/t ) methods for overlapping group lasso and matches optimal o ( 1/t^2 ) methods for the fused lasso and tree structured group lasso .", "topics": ["numerical analysis", "matrix regularization"]}
{"title": "implementation of an automatic sign language lexical annotation framework based on propositional dynamic logic", "abstract": "in this paper , we present the implementation of an automatic sign language ( sl ) sign annotation framework based on a formal logic , the propositional dynamic logic ( pdl ) . our system relies heavily on the use of a specific variant of pdl , the propositional dynamic logic for sign language ( pdlsl ) , which lets us describe sl signs as formulae and corpora videos as labeled transition systems ( ltss ) . here , we intend to show how a generic annotation system can be constructed upon these underlying theoretical principles , regardless of the tracking technologies available or the input format of corpora . with this in mind , we generated a development framework that adapts the system to specific use cases . furthermore , we present some results obtained by our application when adapted to one distinct case , 2d corpora analysis with pre-processed tracking information . we also present some insights on how such a technology can be used to analyze 3d real-time data , captured with a depth device .", "topics": ["text corpus"]}
{"title": "linear contextual bandits with knapsacks", "abstract": "we consider the linear contextual bandit problem with resource consumption , in addition to reward generation . in each round , the outcome of pulling an arm is a reward as well as a vector of resource consumptions . the expected values of these outcomes depend linearly on the context of that arm . the budget/capacity constraints require that the total consumption does n't exceed the budget for each resource . the objective is once again to maximize the total reward . this problem turns out to be a common generalization of classic linear contextual bandits ( lincontextual ) , bandits with knapsacks ( bwk ) , and the online stochastic packing problem ( ospp ) . we present algorithms with near-optimal regret bounds for this problem . our bounds compare favorably to results on the unstructured version of the problem where the relation between the contexts and the outcomes could be arbitrary , but the algorithm only competes against a fixed set of policies accessible through an optimization oracle . we combine techniques from the work on lincontextual , bwk , and ospp in a nontrivial manner while also tackling new difficulties that are not present in any of these special cases .", "topics": ["regret ( decision theory )", "value ( ethics )"]}
{"title": "approximate kalman filter q-learning for continuous state-space mdps", "abstract": "we seek to learn an effective policy for a markov decision process ( mdp ) with continuous states via q-learning . given a set of basis functions over state action pairs we search for a corresponding set of linear weights that minimizes the mean bellman residual . our algorithm uses a kalman filter model to estimate those weights and we have developed a simpler approximate kalman filter model that outperforms the current state of the art projected td-learning methods on several standard benchmark problems .", "topics": ["approximation algorithm", "markov chain"]}
{"title": "projection-free online optimization with stochastic gradient : from convexity to submodularity", "abstract": "online optimization has been a successful framework for solving large-scale problems under computational constraints and partial information . current methods for online convex optimization require either a projection or exact gradient computation at each step , both of which can be prohibitively expensive for large-scale applications . at the same time , there is a growing trend of non-convex optimization in machine learning community and a need for online methods . continuous submodular functions , which exhibit a natural diminishing returns condition , have recently been proposed as a broad class of non-convex functions which may be efficiently optimized . although online methods have been introduced , they suffer from similar problems . in this work , we propose meta-frank-wolfe , the first online projectionfree algorithm that uses stochastic gradient estimates . the algorithm relies on a careful sampling of gradients in each round and achieves the optimal $ o ( \\sqrt { t } ) $ adversarial regret bounds for convex and continuous submodular optimization . we also propose one-shot frank-wolfe , a simpler algorithm which requires only a single stochastic gradient estimate in each round and achieves a $ o ( t^ { 2/3 } ) $ stochastic regret bound for convex and continuous submodular optimization . we apply our methods to develop a novel `` lifting '' framework for the online discrete submodular maximization and also see that they outperform current state of the art techniques on an extensive set of experiments .", "topics": ["sampling ( signal processing )", "regret ( decision theory )"]}
{"title": "sparse trace norm regularization", "abstract": "we study the problem of estimating multiple predictive functions from a dictionary of basis functions in the nonparametric regression setting . our estimation scheme assumes that each predictive function can be estimated in the form of a linear combination of the basis functions . by assuming that the coefficient matrix admits a sparse low-rank structure , we formulate the function estimation problem as a convex program regularized by the trace norm and the $ \\ell_1 $ -norm simultaneously . we propose to solve the convex program using the accelerated gradient ( ag ) method and the alternating direction method of multipliers ( admm ) respectively ; we also develop efficient algorithms to solve the key components in both ag and admm . in addition , we conduct theoretical analysis on the proposed function estimation scheme : we derive a key property of the optimal solution to the convex program ; based on an assumption on the basis functions , we establish a performance bound of the proposed function estimation scheme ( via the composite regularization ) . simulation studies demonstrate the effectiveness and efficiency of the proposed algorithms .", "topics": ["optimization problem", "matrix regularization"]}
{"title": "the relationship between and/or search and variable elimination", "abstract": "in this paper we compare search and inference in graphical models through the new framework of and/or search . specifically , we compare variable elimination ( ve ) and memoryintensive and/or search ( ao ) and place algorithms such as graph-based backjumping and no-good and good learning , as well as recursive conditioning [ 7 ] and value elimination [ 2 ] within the and/or search framework .", "topics": ["graphical model"]}
{"title": "fisher gan", "abstract": "generative adversarial networks ( gans ) are powerful models for learning complex distributions . stable training of gans has been addressed in many recent works which explore different metrics between distributions . in this paper we introduce fisher gan which fits within the integral probability metrics ( ipm ) framework for training gans . fisher gan defines a critic with a data dependent constraint on its second order moments . we show in this paper that fisher gan allows for stable and time efficient training that does not compromise the capacity of the critic , and does not need data independent constraints such as weight clipping . we analyze our fisher ipm theoretically and provide an algorithm based on augmented lagrangian for fisher gan . we validate our claims on both image sample generation and semi-supervised classification using fisher gan .", "topics": ["supervised learning"]}
{"title": "scilab and sip for image processing", "abstract": "this paper is an overview of image processing and analysis using scilab , a free prototyping environment for numerical calculations similar to matlab . we demonstrate the capabilities of sip -- the scilab image processing toolbox -- which extends scilab with many functions to read and write images in over 100 major file formats , including png , jpeg , bmp , and tiff . it also provides routines for image filtering , edge detection , blurring , segmentation , shape analysis , and image recognition . basic directions to install scilab and sip are given , and also a mini-tutorial on scilab . three practical examples of image analysis are presented , in increasing degrees of complexity , showing how advanced image analysis techniques seems uncomplicated in this environment .", "topics": ["image processing", "numerical analysis"]}
{"title": "are you imitating me ? unsupervised sparse modeling for group activity analysis from a single video", "abstract": "a framework for unsupervised group activity analysis from a single video is here presented . our working hypothesis is that human actions lie on a union of low-dimensional subspaces , and thus can be efficiently modeled as sparse linear combinations of atoms from a learned dictionary representing the action 's primitives . contrary to prior art , and with the primary goal of spatio-temporal action grouping , in this work only one single video segment is available for both unsupervised learning and analysis without any prior training information . after extracting simple features at a single spatio-temporal scale , we learn a dictionary for each individual in the video during each short time lapse . these dictionaries allow us to compare the individuals ' actions by producing an affinity matrix which contains sufficient discriminative information about the actions in the scene leading to grouping with simple and efficient tools . with diverse publicly available real videos , we demonstrate the effectiveness of the proposed framework and its robustness to cluttered backgrounds , changes of human appearance , and action variability .", "topics": ["unsupervised learning", "sparse matrix"]}
{"title": "a new humanlike facial attractiveness predictor with cascaded fine-tuning deep learning model", "abstract": "this paper proposes a deep leaning method to address the challenging facial attractiveness prediction problem . the method constructs a convolutional neural network of facial beauty prediction using a new deep cascaded fine-turning scheme with various face inputting channels , such as the original rgb face image , the detail layer image , and the lighting layer image . with a carefully designed cnn model of deep structure , large input size and small convolutional kernels , we have achieved a high prediction correlation of 0.88 . this result convinces us that the problem of facial attractiveness prediction can be solved by deep learning approach , and it also shows the important roles of the facial smoothness , lightness , and color information that were involved in facial beauty perception , which is consistent with the result of recent psychology studies . furthermore , we analyze the high-level features learnt by cnn through visualization of its hidden layers , and some interesting phenomena were observed . it is found that the contours and appearance of facial features , especially eyes and moth , are the most significant facial attributes for facial attractiveness prediction , which is also consistent with the visual perception intuition of human .", "topics": ["kernel ( operating system )", "computational complexity theory"]}
{"title": "human pose estimation from depth images via inference embedded multi-task learning", "abstract": "human pose estimation ( i.e . , locating the body parts / joints of a person ) is a fundamental problem in human-computer interaction and multimedia applications . significant progress has been made based on the development of depth sensors , i.e . , accessible human pose prediction from still depth images [ 32 ] . however , most of the existing approaches to this problem involve several components/models that are independently designed and optimized , leading to suboptimal performances . in this paper , we propose a novel inference-embedded multi-task learning framework for predicting human pose from still depth images , which is implemented with a deep architecture of neural networks . specifically , we handle two cascaded tasks : i ) generating the heat ( confidence ) maps of body parts via a fully convolutional network ( fcn ) ; ii ) seeking the optimal configuration of body parts based on the detected body part proposals via an inference built-in matchnet [ 10 ] , which measures the appearance and geometric kinematic compatibility of body parts and embodies the dynamic programming inference as an extra network layer . these two tasks are jointly optimized . our extensive experiments show that the proposed deep model significantly improves the accuracy of human pose estimation over other several state-of-the-art methods or sdks . we also release a large-scale dataset for comparison , which includes 100k depth images under challenging scenarios .", "topics": ["map", "sensor"]}
{"title": "selective encoding for abstractive sentence summarization", "abstract": "we propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization . it consists of a sentence encoder , a selective gate network , and an attention equipped decoder . the sentence encoder and decoder are built with recurrent neural networks . the selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder . the second level representation is tailored for sentence summarization task , which leads to better performance . we evaluate our model on the english gigaword , duc 2004 and msr abstractive sentence summarization datasets . the experimental results show that the proposed selective encoding model outperforms the state-of-the-art baseline models .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "parallel coordinate descent methods for big data optimization", "abstract": "in this work we show that randomized ( block ) coordinate descent methods can be accelerated by parallelization when applied to the problem of minimizing the sum of a partially separable smooth convex function and a simple separable convex function . the theoretical speedup , as compared to the serial method , and referring to the number of iterations needed to approximately solve the problem with high probability , is a simple expression depending on the number of parallel processors and a natural and easily computable measure of separability of the smooth component of the objective function . in the worst case , when no degree of separability is present , there may be no speedup ; in the best case , when the problem is separable , the speedup is equal to the number of processors . our analysis also works in the mode when the number of blocks being updated at each iteration is random , which allows for modeling situations with busy or unreliable processors . we show that our algorithm is able to solve a lasso problem involving a matrix with 20 billion nonzeros in 2 hours on a large memory node with 24 cores .", "topics": ["loss function", "iteration"]}
{"title": "a novel method to study bottom-up visual saliency and its neural mechanism", "abstract": "in this study , we propose a novel method to measure bottom-up saliency maps of natural images . in order to eliminate the influence of top-down signals , backward masking is used to make stimuli ( natural images ) subjectively invisible to subjects , however , the bottom-up saliency can still orient the subjects attention . to measure this orientation/attention effect , we adopt the cueing effect paradigm by deploying discrimination tasks at each location of an image , and measure the discrimination performance variation across the image as the attentional effect of the bottom-up saliency . such attentional effects are combined to construct a final bottomup saliency map . based on the proposed method , we introduce a new bottom-up saliency map dataset of natural images to benchmark computational models . we compare several state-of-the-art saliency models on the dataset . moreover , the proposed paradigm is applied to investigate the neural basis of the bottom-up visual saliency map by analyzing psychophysical and fmri experimental results . our findings suggest that the bottom-up saliency maps of natural images are constructed in v1 . it provides a strong scientific evidence to resolve the long standing dispute in neuroscience about where the bottom-up saliency map is constructed in human brain .", "topics": ["map"]}
{"title": "a dantzig selector approach to temporal difference learning", "abstract": "lstd is a popular algorithm for value function approximation . whenever the number of features is larger than the number of samples , it must be paired with some form of regularization . in particular , l1-regularization methods tend to perform feature selection by promoting sparsity , and thus , are well-suited for high-dimensional problems . however , since lstd is not a simple regression algorithm , but it solves a fixed -- point problem , its integration with l1-regularization is not straightforward and might come with some drawbacks ( e.g . , the p-matrix assumption for lasso-td ) . in this paper , we introduce a novel algorithm obtained by integrating lstd with the dantzig selector . we investigate the performance of the proposed algorithm and its relationship with the existing regularized approaches , and show how it addresses some of their drawbacks .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "learning deep matrix representations", "abstract": "we present a new distributed representation in deep neural nets wherein the information is represented in native form as a matrix . this differs from current neural architectures that rely on vector representations . we consider matrices as central to the architecture and they compose the input , hidden and output layers . the model representation is more compact and elegant -- the number of parameters grows only with the largest dimension of the incoming layer rather than the number of hidden units . we derive several new deep networks : ( i ) feed-forward nets that map an input matrix into an output matrix , ( ii ) recurrent nets which map a sequence of input matrices into a sequence of output matrices . we also reinterpret existing models for ( iii ) memory-augmented networks and ( iv ) graphs using matrix notations . for graphs we demonstrate how the new notations lead to simple but effective extensions with multiple attentions . extensive experiments on handwritten digits recognition , face reconstruction , sequence to sequence learning , eeg classification , and graph-based node classification demonstrate the efficacy and compactness of the matrix architectures .", "topics": ["neural networks"]}
{"title": "technical report - automatic contour extraction from 2d neuron images", "abstract": "this work describes a novel methodology for automatic contour extraction from 2d images of 3d neurons ( e.g . camera lucida images and other types of 2d microscopy ) . most contour-based shape analysis methods can not be used to characterize such cells because of overlaps between neuronal processes . the proposed framework is specifically aimed at the problem of contour following even in presence of multiple overlaps . first , the input image is preprocessed in order to obtain an 8-connected skeleton with one-pixel-wide branches , as well as a set of critical regions ( i.e . , bifurcations and crossings ) . next , for each subtree , the tracking stage iteratively labels all valid pixel of branches , up to a critical region , where it determines the suitable direction to proceed . finally , the labeled skeleton segments are followed in order to yield the parametric contour of the neuronal shape under analysis . the reported system was successfully tested with respect to several images and the results from a set of three neuron images are presented here , each pertaining to a different class , i.e . alpha , delta and epsilon ganglion cells , containing a total of 34 crossings . the algorithms successfully got across all these overlaps . the method has also been found to exhibit robustness even for images with close parallel segments . the proposed method is robust and may be implemented in an efficient manner . the introduction of this approach should pave the way for more systematic application of contour-based shape analysis methods in neuronal morphology .", "topics": ["pixel"]}
{"title": "fully-coupled two-stream spatiotemporal networks for extremely low resolution action recognition", "abstract": "a major emerging challenge is how to protect people 's privacy as cameras and computer vision are increasingly integrated into our daily lives , including in smart devices inside homes . a potential solution is to capture and record just the minimum amount of information needed to perform a task of interest . in this paper , we propose a fully-coupled two-stream spatiotemporal architecture for reliable human action recognition on extremely low resolution ( e.g . , 12x16 pixel ) videos . we provide an efficient method to extract spatial and temporal features and to aggregate them into a robust feature representation for an entire action video sequence . we also consider how to incorporate high resolution videos during training in order to build better low resolution action recognition models . we evaluate on two publicly-available datasets , showing significant improvements over the state-of-the-art .", "topics": ["computer vision", "pixel"]}
{"title": "analysis of the impact of negative sampling on link prediction in knowledge graphs", "abstract": "knowledge graphs are large , useful , but incomplete knowledge repositories . they encode knowledge through entities and relations which define each other through the connective structure of the graph . this has inspired methods for the joint embedding of entities and relations in continuous low-dimensional vector spaces , that can be used to induce new edges in the graph , i.e . , link prediction in knowledge graphs . learning these representations relies on contrasting positive instances with negative ones . knowledge graphs include only positive relation instances , leaving the door open for a variety of methods for selecting negative examples . in this paper we present an empirical study on the impact of negative sampling on the learned embeddings , assessed through the task of link prediction . we use state-of-the-art knowledge graph embeddings -- \\rescal , transe , distmult and complex -- and evaluate on benchmark datasets -- fb15k and wn18 . we compare well known methods for negative sampling and additionally propose embedding based sampling methods . we note a marked difference in the impact of these sampling methods on the two datasets , with the `` traditional '' corrupting positives method leading to best results on wn18 , while embedding based methods benefiting the task on fb15k .", "topics": ["sampling ( signal processing )", "entity"]}
{"title": "decision-making with complex data structures using probabilistic programming", "abstract": "existing decision-theoretic reasoning frameworks such as decision networks use simple data structures and processes . however , decisions are often made based on complex data structures , such as social networks and protein sequences , and rich processes involving those structures . we present a framework for representing decision problems with complex data structures using probabilistic programming , allowing probabilistic models to be created with programming language constructs such as data structures and control flow . we provide a way to use arbitrary data types with minimal effort from the user , and an approximate decision-making algorithm that is effective even when the information space is very large or infinite . experimental results show our algorithm working on problems with very large information spaces .", "topics": ["approximation algorithm"]}
{"title": "exploring the power of gpu 's for training polyglot language models", "abstract": "one of the major research trends currently is the evolution of heterogeneous parallel computing . gp-gpu computing is being widely used and several applications have been designed to exploit the massive parallelism that gp-gpu 's have to offer . while gpu 's have always been widely used in areas of computer vision for image processing , little has been done to investigate whether the massive parallelism provided by gp-gpu 's can be utilized effectively for natural language processing ( nlp ) tasks . in this work , we investigate and explore the power of gp-gpu 's in the task of learning language models . more specifically , we investigate the performance of training polyglot language models using deep belief neural networks . we evaluate the performance of training the model on the gpu and present optimizations that boost the performance on the gpu.one of the key optimizations , we propose increases the performance of a function involved in calculating and updating the gradient by approximately 50 times on the gpu for sufficiently large batch sizes . we show that with the above optimizations , the gp-gpu 's performance on the task increases by factor of approximately 3-4 . the optimizations we made are generic theano optimizations and hence potentially boost the performance of other models which rely on these operations.we also show that these optimizations result in the gpu 's performance at this task being now comparable to that on the cpu . we conclude by presenting a thorough evaluation of the applicability of gp-gpu 's for this task and highlight the factors limiting the performance of training a polyglot model on the gpu .", "topics": ["image processing", "computer vision"]}
{"title": "dynamic stripes : exploiting the dynamic precision requirements of activation values in neural networks", "abstract": "stripes is a deep neural network ( dnn ) accelerator that uses bit-serial computation to offer performance that is proportional to the fixed-point precision of the activation values . the fixed-point precisions are determined a priori using profiling and are selected at a per layer granularity . this paper presents dynamic stripes , an extension to stripes that detects precision variance at runtime and at a finer granularity . this extra level of precision reduction increases performance by 41 % over stripes .", "topics": ["computation"]}
{"title": "convergent tree-backup and retrace with function approximation", "abstract": "we show that the tree backup and retrace algorithms are unstable with linear function approximation , both in theory and in practice with specific examples . based on our analysis , we then derive stable and efficient gradient-based algorithms using a quadratic convex-concave saddle-point formulation . by exploiting the problem structure proper to these algorithms , we are able to provide convergence guarantees and finite-sample bounds . the applicability of our new analysis framework also goes beyond tree backup and retrace and allows us to provide new convergence rates for the gtd and gtd2 algorithms without having recourse to projections or polyak averaging .", "topics": ["reinforcement learning", "approximation"]}
{"title": "optimal convergence for distributed learning with stochastic gradient methods and spectral-regularization algorithms", "abstract": "we study generalization properties of distributed algorithms in the setting of nonparametric regression over a reproducing kernel hilbert space ( rkhs ) . we first investigate distributed stochastic gradient methods ( sgm ) , with mini-batches and multi-passes over the data . we show that optimal generalization error bounds can be retained for distributed sgm provided that the partition level is not too large . we then extend our results to spectral-regularization algorithms ( sra ) , including kernel ridge regression ( krr ) , kernel principal component analysis , and gradient methods . our results are superior to the state-of-the-art theory . particularly , our results show that distributed sgm has a smaller theoretical computational complexity , compared with distributed krr and classic sgm . moreover , even for non-distributed sra , they provide the first optimal , capacity-dependent convergence rates , considering the case that the regression function may not be in the rkhs .", "topics": ["computational complexity theory", "matrix regularization"]}
{"title": "comment on `` fastest learning in small-world neural networks ''", "abstract": "this comment reexamines simard et al . 's work in [ d. simard , l. nadeau , h . kroger , phys . lett . a 336 ( 2005 ) 8-15 ] . we found that simard et al . calculated mistakenly the local connectivity lengths dlocal of networks . the right results of dlocal are presented and the supervised learning performance of feedforward neural networks ( fnns ) with different rewirings are re-investigated in this comment . this comment discredits simard et al 's work by two conclusions : 1 ) rewiring connections of fnns can not generate networks with small-world connectivity ; 2 ) for different training sets , there do not exist networks with a certain number of rewirings generating reduced learning errors than networks with other numbers of rewiring .", "topics": ["supervised learning"]}
{"title": "an htm based cortical algorithm for detection of seismic waves", "abstract": "recognizing seismic waves immediately is very important for the realization of efficient disaster prevention . generally these systems consist of a network of seismic detectors that send real time data to a central server . the server elaborates the data and attempts to recognize the first signs of an earthquake . the current problem with this approach is that it is subject to false alarms . a critical trade-off exists between sensitivity of the system and error rate . to overcame this problems , an artificial neural network based intelligent learning systems can be used . however , conventional supervised ann systems are difficult to train , cpu intensive and prone to false alarms . to surpass these problems , here we attempt to use a next-generation unsupervised cortical algorithm htm . this novel approach does not learn particular waveforms , but adapts to continuously fed data reaching the ability to discriminate between normality ( seismic sensor background noise in no-earthquake conditions ) and anomaly ( sensor response to a jitter or an earthquake ) . main goal of this study is test the ability of the htm algorithm to be used to signal earthquakes automatically in a feasible disaster prevention system . we describe the methodology used and give the first qualitative assessments of the recognition ability of the system . our preliminary results show that the cortical algorithm used is very robust to noise and that can successfully recognize synthetic earthquake-like signals efficiently and reliably .", "topics": ["unsupervised learning", "synthetic data"]}
{"title": "an arabic-hebrew parallel corpus of ted talks", "abstract": "we describe an arabic-hebrew parallel corpus of ted talks built upon wit3 , the web inventory that repurposes the original content of the ted website in a way which is more convenient for mt researchers . the benchmark consists of about 2,000 talks , whose subtitles in arabic and hebrew have been accurately aligned and rearranged in sentences , for a total of about 3.5m tokens per language . talks have been partitioned in train , development and test sets similarly in all respects to the mt tasks of the iwslt 2016 evaluation campaign . in addition to describing the benchmark , we list the problems encountered in preparing it and the novel methods designed to solve them . baseline mt results and some measures on sentence length are provided as an extrinsic evaluation of the quality of the benchmark .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "critical remarks on single link search in learning belief networks", "abstract": "in learning belief networks , the single link lookahead search is widely adopted to reduce the search space . we show that there exists a class of probabilistic domain models which displays a special pattern of dependency . we analyze the behavior of several learning algorithms using different scoring metrics such as the entropy , conditional independence , minimal description length and bayesian metrics . we demonstrate that single link lookahead search procedures ( employed in these algorithms ) can not learn these models correctly . thus , when the underlying domain model actually belongs to this class , the use of a single link search procedure will result in learning of an incorrect model . this may lead to inference errors when the model is used . our analysis suggests that if the prior knowledge about a domain does not rule out the possible existence of these models , a multi-link lookahead search or other heuristics should be used for the learning process .", "topics": ["parsing", "bayesian network"]}
{"title": "brain eeg time series selection : a novel graph-based approach for classification", "abstract": "brain electroencephalography ( eeg ) classification is widely applied to analyze cerebral diseases in recent years . unfortunately , invalid/noisy eegs degrade the diagnosis performance and most previously developed methods ignore the necessity of eeg selection for classification . to this end , this paper proposes a novel maximum weight clique-based eeg selection approach , named mwceegs , to map eeg selection to searching maximum similarity-weighted cliques from an improved fr\\ ' { e } chet distance-weighted undirected eeg graph simultaneously considering edge weights and vertex weights . our mwceegs improves the classification performance by selecting intra-clique pairwise similar and inter-clique discriminative eegs with similarity threshold $ \\delta $ . experimental results demonstrate the algorithm effectiveness compared with the state-of-the-art time series selection algorithms on real-world eeg datasets .", "topics": ["time series"]}
{"title": "non-linear regression models for approximate bayesian computation", "abstract": "approximate bayesian inference on the basis of summary statistics is well-suited to complex problems for which the likelihood is either mathematically or computationally intractable . however the methods that use rejection suffer from the curse of dimensionality when the number of summary statistics is increased . here we propose a machine-learning approach to the estimation of the posterior density by introducing two innovations . the new method fits a nonlinear conditional heteroscedastic regression of the parameter on the summary statistics , and then adaptively improves estimation using importance sampling . the new algorithm is compared to the state-of-the-art approximate bayesian methods , and achieves considerable reduction of the computational burden in two examples of inference in statistical genetics and in a queueing model .", "topics": ["sampling ( signal processing )", "computational complexity theory"]}
{"title": "a joint model of language and perception for grounded attribute learning", "abstract": "as robots become more ubiquitous and capable , it becomes ever more important to enable untrained users to easily interact with them . recently , this has led to study of the language grounding problem , where the goal is to extract representations of the meanings of natural language tied to perception and actuation in the physical world . in this paper , we present an approach for joint learning of language and perception models for grounded attribute induction . our perception model includes attribute classifiers , for example to detect object color and shape , and the language model is based on a probabilistic categorial grammar that enables the construction of rich , compositional meaning representations . the approach is evaluated on the task of interpreting sentences that describe sets of objects in a physical workspace . we demonstrate accurate task performance and effective latent-variable concept induction in physical grounded scenes .", "topics": ["natural language", "robot"]}
{"title": "size vs . structure in training corpora for word embedding models : araneum russicum maximum and russian national corpus", "abstract": "in this paper , we present a distributional word embedding model trained on one of the largest available russian corpora : araneum russicum maximum ( over 10 billion words crawled from the web ) . we compare this model to the model trained on the russian national corpus ( rnc ) . the two corpora are much different in their size and compilation procedures . we test these differences by evaluating the trained models against the russian part of the multilingual simlex999 semantic similarity dataset . we detect and describe numerous issues in this dataset and publish a new corrected version . aside from the already known fact that the rnc is generally a better training corpus than web corpora , we enumerate and explain fine differences in how the models process semantic similarity task , what parts of the evaluation set are difficult for particular models and why . additionally , the learning curves for both models are described , showing that the rnc is generally more robust as training material for this task .", "topics": ["text corpus"]}
{"title": "z-forcing : training stochastic recurrent networks", "abstract": "many efforts have been devoted to training generative latent variable models with autoregressive decoders , such as recurrent neural networks ( rnn ) . stochastic recurrent models have been successful in capturing the variability observed in natural sequential data such as speech . we unify successful ideas from recently proposed architectures into a stochastic recurrent model : each step in the sequence is associated with a latent variable that is used to condition the recurrent dynamics for future steps . training is performed with amortized variational inference where the approximate posterior is augmented with a rnn that runs backward through the sequence . in addition to maximizing the variational lower bound , we ease training of the latent variables by adding an auxiliary cost which forces them to reconstruct the state of the backward recurrent network . this provides the latent variables with a task-independent objective that enhances the performance of the overall model . we found this strategy to perform better than alternative approaches such as kl annealing . although being conceptually simple , our model achieves state-of-the-art results on standard speech benchmarks such as timit and blizzard and competitive performance on sequential mnist . finally , we apply our model to language modeling on the imdb dataset where the auxiliary cost helps in learning interpretable latent variables . source code : \\url { https : //github.com/anirudh9119/zforcing_nips17 }", "topics": ["calculus of variations", "approximation algorithm"]}
{"title": "cgans with projection discriminator", "abstract": "we propose a novel , projection based way to incorporate the conditional information into the discriminator of gans that respects the role of the conditional information in the underlining probabilistic model . this approach is in contrast with most frameworks of conditional gans used in application today , which use the conditional information by concatenating the ( embedded ) conditional vector to the feature vectors . with this modification , we were able to significantly improve the quality of the class conditional image generation on ilsvrc2012 ( imagenet ) 1000-class image dataset from the current state-of-the-art result , and we achieved this with a single pair of a discriminator and a generator . we were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images . this new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator .", "topics": ["feature vector"]}
{"title": "automatic extraction of subcategorization from corpora", "abstract": "we describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora . each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for english . an initial experiment , on a sample of 14 verbs which exhibit multiple complementation patterns , demonstrates that the technique achieves accuracy comparable to previous approaches , which are all limited to a highly restricted set of subcategorization classes . we also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount .", "topics": ["text corpus", "dictionary"]}
{"title": "multi-task and lifelong learning of kernels", "abstract": "we consider a problem of learning kernels for use in svm classification in the multi-task and lifelong scenarios and provide generalization bounds on the error of a large margin classifier . our results show that , under mild conditions on the family of kernels used for learning , solving several related tasks simultaneously is beneficial over single task learning . in particular , as the number of observed tasks grows , assuming that in the considered family of kernels there exists one that yields low approximation error on all tasks , the overhead associated with learning such a kernel vanishes and the complexity converges to that of learning when this good kernel is given to the learner .", "topics": ["kernel ( operating system )", "reinforcement learning"]}
{"title": "argument strength is in the eye of the beholder : audience effects in persuasion", "abstract": "americans spend about a third of their time online , with many participating in online conversations on social and political issues . we hypothesize that social media arguments on such issues may be more engaging and persuasive than traditional media summaries , and that particular types of people may be more or less convinced by particular styles of argument , e.g . emotional arguments may resonate with some personalities while factual arguments resonate with others . we report a set of experiments testing at large scale how audience variables interact with argument style to affect the persuasiveness of an argument , an under-researched topic within natural language processing . we show that belief change is affected by personality factors , with conscientious , open and agreeable people being more convinced by emotional arguments .", "topics": ["natural language processing"]}
{"title": "learning deep resnet blocks sequentially using boosting theory", "abstract": "deep neural networks are known to be difficult to train due to the instability of back-propagation . a deep \\emph { residual network } ( resnet ) with identity loops remedies this by stabilizing gradient computations . we prove a boosting theory for the resnet architecture . we construct $ t $ weak module classifiers , each contains two of the $ t $ layers , such that the combined strong learner is a resnet . therefore , we introduce an alternative deep resnet training algorithm , \\emph { boostresnet } , which is particularly suitable in non-differentiable architectures . our proposed algorithm merely requires a sequential training of $ t $ `` shallow resnets '' which are inexpensive . we prove that the training error decays exponentially with the depth $ t $ if the \\emph { weak module classifiers } that we train perform slightly better than some weak baseline . in other words , we propose a weak learning condition and prove a boosting theory for resnet under the weak learning condition . our results apply to general multi-class resnets . a generalization error bound based on margin theory is proved and suggests resnet 's resistant to overfitting under network with $ l_1 $ norm bounded weights .", "topics": ["baseline ( configuration management )", "gradient"]}
{"title": "the implementation of a deep recurrent neural network language model on a xilinx fpga", "abstract": "recently , fpga has been increasingly applied to problems such as speech recognition , machine learning , and cloud computation such as the bing search engine used by microsoft . this is due to fpgas great parallel computation capacity as well as low power consumption compared to general purpose processors . however , these applications mainly focus on large scale fpga clusters which have an extreme processing power for executing massive matrix or convolution operations but are unsuitable for portable or mobile applications . this paper describes research on single-fpga platform to explore the applications of fpgas in these fields . in this project , we design a deep recurrent neural network ( drnn ) language model ( lm ) and implement a hardware accelerator with axi stream interface on a pynq board which is equipped with a xilinx zynq soc xc7z020 1clg400c . the pynq has not only abundant programmable logic resources but also a flexible embedded operation system , which makes it suitable to be applied in the natural language processing field . we design the drnn language model with python and theano , train the model on a cpu platform , and deploy the model on a pynq board to validate the model with jupyter notebook . meanwhile , we design the hardware accelerator with overlay , which is a kind of hardware library on pynq , and verify the acceleration effect on the pynq board . finally , we have found that the drnn language model can be deployed on the embedded system smoothly and the overlay accelerator with axi stream interface performs at 20 gops processing throughput , which constitutes a 70.5x and 2.75x speed up compared to the work in ref.30 and ref.31 respectively .", "topics": ["natural language processing", "recurrent neural network"]}
{"title": "a new strategy of cost-free learning in the class imbalance problem", "abstract": "in this work , we define cost-free learning ( cfl ) formally in comparison with cost-sensitive learning ( csl ) . the main difference between them is that a cfl approach seeks optimal classification results without requiring any cost information , even in the class imbalance problem . in fact , several cfl approaches exist in the related studies , such as sampling and some criteria-based pproaches . however , to our best knowledge , none of the existing cfl and csl approaches are able to process the abstaining classifications properly when no information is given about errors and rejects . based on information theory , we propose a novel cfl which seeks to maximize normalized mutual information of the targets and the decision outputs of classifiers . using the strategy , we can deal with binary/multi-class classifications with/without abstaining . significant features are observed from the new strategy . while the degree of class imbalance is changing , the proposed strategy is able to balance the errors and rejects accordingly and automatically . another advantage of the strategy is its ability of deriving optimal rejection thresholds for abstaining classifications and the `` equivalent '' costs in binary classifications . the connection between rejection thresholds and roc curve is explored . empirical investigation is made on several benchmark data sets in comparison with other existing approaches . the classification results demonstrate a promising perspective of the strategy in machine learning .", "topics": ["sampling ( signal processing )"]}
{"title": "connecting generative adversarial networks and actor-critic methods", "abstract": "both generative adversarial networks ( gan ) in unsupervised learning and actor-critic methods in reinforcement learning ( rl ) have gained a reputation for being difficult to optimize . practitioners in both fields have amassed a large number of strategies to mitigate these instabilities and improve training . here we show that gans can be viewed as actor-critic methods in an environment where the actor can not affect the reward . we review the strategies for stabilizing training for each class of models , both those that generalize between the two and those that are particular to that model . we also review a number of extensions to gans and rl algorithms with even more complicated information flow . we hope that by highlighting this formal connection we will encourage both gan and rl communities to develop general , scalable , and stable algorithms for multilevel optimization with deep networks , and to draw inspiration across communities .", "topics": ["unsupervised learning", "reinforcement learning"]}
{"title": "autoencoding topology", "abstract": "the problem of learning a manifold structure on a dataset is framed in terms of a generative model , to which we use ideas behind autoencoders ( namely adversarial/wasserstein autoencoders ) to fit deep neural networks . from a machine learning perspective , the resulting structure , an atlas of a manifold , may be viewed as a combination of dimensionality reduction and `` fuzzy '' clustering .", "topics": ["generative model", "cluster analysis"]}
{"title": "adaptive image denoising by mixture adaptation", "abstract": "we propose an adaptive learning procedure to learn patch-based image priors for image denoising . the new algorithm , called the expectation-maximization ( em ) adaptation , takes a generic prior learned from a generic external database and adapts it to the noisy image to generate a specific prior . different from existing methods that combine internal and external statistics in ad-hoc ways , the proposed algorithm is rigorously derived from a bayesian hyper-prior perspective . there are two contributions of this paper : first , we provide full derivation of the em adaptation algorithm and demonstrate methods to improve the computational complexity . second , in the absence of the latent clean image , we show how em adaptation can be modified based on pre-filtering . experimental results show that the proposed adaptation algorithm yields consistently better denoising results than the one without adaptation and is superior to several state-of-the-art algorithms .", "topics": ["computational complexity theory", "noise reduction"]}
{"title": "discriminative features via generalized eigenvectors", "abstract": "representing examples in a way that is compatible with the underlying classifier can greatly enhance the performance of a learning system . in this paper we investigate scalable techniques for inducing discriminative features by taking advantage of simple second order structure in the data . we focus on multiclass classification and show that features extracted from the generalized eigenvectors of the class conditional second moments lead to classifiers with excellent empirical performance . moreover , these features have attractive theoretical properties , such as inducing representations that are invariant to linear transformations of the input . we evaluate classifiers built from these features on three different tasks , obtaining state of the art results .", "topics": ["scalability"]}
{"title": "semi-coupled two-stream fusion convnets for action recognition at extremely low resolutions", "abstract": "deep convolutional neural networks ( convnets ) have been recently shown to attain state-of-the-art performance for action recognition on standard-resolution videos . however , less attention has been paid to recognition performance at extremely low resolutions ( elr ) ( e.g . , 16 x 12 pixels ) . reliable action recognition using elr cameras would address privacy concerns in various application environments such as private homes , hospitals , nursing/rehabilitation facilities , etc . in this paper , we propose a semi-coupled filter-sharing network that leverages high resolution ( hr ) videos during training in order to assist an elr convnet . we also study methods for fusing spatial and temporal convnets customized for elr videos in order to take advantage of appearance and motion information . our method outperforms state-of-the-art methods at extremely low resolutions on ixmas ( 93.7 % ) and hmdb ( 29.2 % ) datasets .", "topics": ["pixel"]}
{"title": "avoiding and escaping depressions in real-time heuristic search", "abstract": "heuristics used for solving hard real-time search problems have regions with depressions . such regions are bounded areas of the search space in which the heuristic function is inaccurate compared to the actual cost to reach a solution . early real-time search algorithms , like lrta* , easily become trapped in those regions since the heuristic values of their states may need to be updated multiple times , which results in costly solutions . state-of-the-art real-time search algorithms , like lss-lrta* or lrta* ( k ) , improve lrta*s mechanism to update the heuristic , resulting in improved performance . those algorithms , however , do not guide search towards avoiding depressed regions . this paper presents depression avoidance , a simple real-time search principle to guide search towards avoiding states that have been marked as part of a heuristic depression . we propose two ways in which depression avoidance can be implemented : mark-and-avoid and move-to-border . we implement these strategies on top of lss-lrta* and rtaa* , producing 4 new real-time heuristic search algorithms : alss-lrta* , dalss-lrta* , artaa* , and dartaa* . when the objective is to find a single solution by running the real-time search algorithm once , we show that dalss-lrta* and dartaa* outperform their predecessors sometimes by one order of magnitude . of the four new algorithms , dartaa* produces the best solutions given a fixed deadline on the average time allowed per planning episode . we prove all our algorithms have good theoretical properties : in finite search spaces , they find a solution if one exists , and converge to an optimal after a number of trials .", "topics": ["value ( ethics )", "heuristic"]}
{"title": "optimizing the latent space of generative networks", "abstract": "generative adversarial networks ( gans ) have been shown to be able to sample impressively realistic images . gan training consists of a saddle point optimization problem that can be thought of as an adversarial game between a generator which produces the images , and a discriminator , which judges if the images are real . both the generator and the discriminator are commonly parametrized as deep convolutional neural networks . the goal of this paper is to disentangle the contribution of the optimization procedure and the network parametrization to the success of gans . to this end we introduce and study generative latent optimization ( glo ) , a framework to train a generator without the need to learn a discriminator , thus avoiding challenging adversarial optimization problems . we show experimentally that glo enjoys many of the desirable properties of gans : learning from large data , synthesizing visually-appealing samples , interpolating meaningfully between samples , and performing linear arithmetic with noise vectors .", "topics": ["optimization problem"]}
{"title": "quicknet : maximizing efficiency and efficacy in deep architectures", "abstract": "we present quicknet , a fast and accurate network architecture that is both faster and significantly more accurate than other fast deep architectures like squeezenet . furthermore , it uses less parameters than previous networks , making it more memory efficient . we do this by making two major modifications to the reference darknet model ( redmon et al , 2015 ) : 1 ) the use of depthwise separable convolutions and 2 ) the use of parametric rectified linear units . we make the observation that parametric rectified linear units are computationally equivalent to leaky rectified linear units at test time and the observation that separable convolutions can be interpreted as a compressed inception network ( chollet , 2016 ) . using these observations , we derive a network architecture , which we call quicknet , that is both faster and more accurate than previous models . our architecture provides at least four major advantages : ( 1 ) a smaller model size , which is more tenable on memory constrained systems ; ( 2 ) a significantly faster network which is more tenable on computationally constrained systems ; ( 3 ) a high accuracy of 95.7 percent on the cifar-10 dataset which outperforms all but one result published so far , although we note that our works are orthogonal approaches and can be combined ( 4 ) orthogonality to previous model compression approaches allowing for further speed gains to be realized .", "topics": ["convolution"]}
{"title": "mesh learning for classifying cognitive processes", "abstract": "a relatively recent advance in cognitive neuroscience has been multi-voxel pattern analysis ( mvpa ) , which enables researchers to decode brain states and/or the type of information represented in the brain during a cognitive operation . mvpa methods utilize machine learning algorithms to distinguish among types of information or cognitive states represented in the brain , based on distributed patterns of neural activity . in the current investigation , we propose a new approach for representation of neural data for pattern analysis , namely a mesh learning model . in this approach , at each time instant , a star mesh is formed around each voxel , such that the voxel corresponding to the center node is surrounded by its p-nearest neighbors . the arc weights of each mesh are estimated from the voxel intensity values by least squares method . the estimated arc weights of all the meshes , called mesh arc descriptors ( mads ) , are then used to train a classifier , such as neural networks , k-nearest neighbor , na\\ '' ive bayes and support vector machines . the proposed mesh model was tested on neuroimaging data acquired via functional magnetic resonance imaging ( fmri ) during a recognition memory experiment using categorized word lists , employing a previously established experimental paradigm ( \\ '' oztekin & badre , 2011 ) . results suggest that the proposed mesh learning approach can provide an effective algorithm for pattern analysis of brain activity during cognitive processing .", "topics": ["support vector machine", "neural networks"]}
{"title": "a unified game-theoretic approach to multiagent reinforcement learning", "abstract": "to achieve general intelligence , agents must learn how to interact with others in a shared environment : this is the challenge of multiagent reinforcement learning ( marl ) . the simplest form is independent reinforcement learning ( inrl ) , where each agent treats its experience as part of its ( non-stationary ) environment . in this paper , we first observe that policies learned using inrl can overfit to the other agents ' policies during training , failing to sufficiently generalize during execution . we introduce a new metric , joint-policy correlation , to quantify this effect . we describe an algorithm for general marl , based on approximate best responses to mixtures of policies generated using deep reinforcement learning , and empirical game-theoretic analysis to compute meta-strategies for policy selection . the algorithm generalizes previous ones such as inrl , iterated best response , double oracle , and fictitious play . then , we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers . finally , we demonstrate the generality of the resulting policies in two partially observable settings : gridworld coordination games and poker .", "topics": ["approximation algorithm", "reinforcement learning"]}
{"title": "discriminative learning via semidefinite probabilistic models", "abstract": "discriminative linear models are a popular tool in machine learning . these can be generally divided into two types : the first is linear classifiers , such as support vector machines , which are well studied and provide state-of-the-art results . one shortcoming of these models is that their output ( known as the 'margin ' ) is not calibrated , and can not be translated naturally into a distribution over the labels . thus , it is difficult to incorporate such models as components of larger systems , unlike probabilistic based approaches . the second type of approach constructs class conditional distributions using a nonlinearity ( e.g . log-linear models ) , but is occasionally worse in terms of classification error . we propose a supervised learning method which combines the best of both approaches . specifically , our method provides a distribution over the labels , which is a linear function of the model parameters . as a consequence , differences between probabilities are linear functions , a property which most probabilistic models ( e.g . log-linear ) do not have . our model assumes that classes correspond to linear subspaces ( rather than to half spaces ) . using a relaxed projection operator , we construct a measure which evaluates the degree to which a given vector 'belongs ' to a subspace , resulting in a distribution over labels . interestingly , this view is closely related to similar concepts in quantum detection theory . the resulting models can be trained either to maximize the margin or to optimize average likelihood measures . the corresponding optimization problems are semidefinite programs which can be solved efficiently . we illustrate the performance of our algorithm on real world datasets , and show that it outperforms 2nd order kernel methods .", "topics": ["support vector machine"]}
{"title": "recurrent neural networks with specialized word embeddings for health-domain named-entity recognition", "abstract": "background . previous state-of-the-art systems on drug name recognition ( dnr ) and clinical concept extraction ( cce ) have focused on a combination of text `` feature engineering '' and conventional machine learning algorithms such as conditional random fields and support vector machines . however , developing good features is inherently heavily time-consuming . conversely , more modern machine learning approaches such as recurrent neural networks ( rnns ) have proved capable of automatically learning effective features from either random assignments or automated word `` embeddings '' . objectives . ( i ) to create a highly accurate dnr and cce system that avoids conventional , time-consuming feature engineering . ( ii ) to create richer , more specialized word embeddings by using health domain datasets such as mimic-iii . ( iii ) to evaluate our systems over three contemporary datasets . methods . two deep learning methods , namely the bidirectional lstm and the bidirectional lstm-crf , are evaluated . a crf model is set as the baseline to compare the deep learning systems to a traditional machine learning approach . the same features are used for all the models . results . we have obtained the best results with the bidirectional lstm-crf model , which has outperformed all previously proposed systems . the specialized embeddings have helped to cover unusual words in ddi-drugbank and ddi-medline , but not in the 2010 i2b2/va irb revision dataset . conclusion . we present a state-of-the-art system for dnr and cce . automated word embeddings has allowed us to avoid costly feature engineering and achieve higher accuracy . nevertheless , the embeddings need to be retrained over datasets that are adequate for the domain , in order to adequately cover the domain-specific vocabulary .", "topics": ["baseline ( configuration management )", "support vector machine"]}
{"title": "classification of alzheimer 's disease using fmri data and deep learning convolutional neural networks", "abstract": "over the past decade , machine learning techniques especially predictive modeling and pattern recognition in biomedical sciences from drug delivery system to medical imaging has become one of the important methods which are assisting researchers to have deeper understanding of entire issue and to solve complex medical problems . deep learning is power learning machine learning algorithm in classification while extracting high-level features . in this paper , we used convolutional neural network to classify alzheimer 's brain from normal healthy brain . the importance of classifying this kind of medical data is to potentially develop a predict model or system in order to recognize the type disease from normal subjects or to estimate the stage of the disease . classification of clinical data such as alzheimer 's disease has been always challenging and most problematic part has been always selecting the most discriminative features . using convolutional neural network ( cnn ) and the famous architecture lenet-5 , we successfully classified functional mri data of alzheimer 's subjects from normal controls where the accuracy of test data on trained data reached 96.85 % . this experiment suggests us the shift and scale invariant features extracted by cnn followed by deep learning classification is most powerful method to distinguish clinical data from healthy data in fmri . this approach also enables us to expand our methodology to predict more complicated systems .", "topics": ["high- and low-level", "neural networks"]}
{"title": "a decidable class of nested iterated schemata ( extended version )", "abstract": "many problems can be specified by patterns of propositional formulae depending on a parameter , e.g . the specification of a circuit usually depends on the number of bits of its input . we define a logic whose formulae , called `` iterated schemata '' , allow to express such patterns . schemata extend propositional logic with indexed propositions , e.g . p_i , p_i+1 , p_1 , and with generalized connectives , e.g . /\\i=1..n or i=1..n ( called `` iterations '' ) where n is an ( unbound ) integer variable called a `` parameter '' . the expressive power of iterated schemata is strictly greater than propositional logic : it is even out of the scope of first-order logic . we define a proof procedure , called dpll* , that can prove that a schema is satisfiable for at least one value of its parameter , in the spirit of the dpll procedure . however the converse problem , i.e . proving that a schema is unsatisfiable for every value of the parameter , is undecidable so dpll* does not terminate in general . still , we prove that it terminates for schemata of a syntactic subclass called `` regularly nested '' . this is the first non trivial class for which dpll* is proved to terminate . furthermore the class of regularly nested schemata is the first decidable class to allow nesting of iterations , i.e . to allow schemata of the form /\\i=1..n ( /\\j=1..n ... ) .", "topics": ["iteration"]}
{"title": "implicit temporal differences", "abstract": "in reinforcement learning , the td ( $ \\lambda $ ) algorithm is a fundamental policy evaluation method with an efficient online implementation that is suitable for large-scale problems . one practical drawback of td ( $ \\lambda $ ) is its sensitivity to the choice of the step-size . it is an empirically well-known fact that a large step-size leads to fast convergence , at the cost of higher variance and risk of instability . in this work , we introduce the implicit td ( $ \\lambda $ ) algorithm which has the same function and computational cost as td ( $ \\lambda $ ) , but is significantly more stable . we provide a theoretical explanation of this stability and an empirical evaluation of implicit td ( $ \\lambda $ ) on typical benchmark tasks . our results show that implicit td ( $ \\lambda $ ) outperforms standard td ( $ \\lambda $ ) and a state-of-the-art method that automatically tunes the step-size , and thus shows promise for wide applicability .", "topics": ["reinforcement learning"]}
{"title": "a unified rgb-t saliency detection benchmark : dataset , baselines , analysis and a novel approach", "abstract": "despite significant progress , image saliency detection still remains a challenging task in complex scenes and environments . integrating multiple different but complementary cues , like rgb and thermal ( rgb-t ) , may be an effective way for boosting saliency detection performance . the current research in this direction , however , is limited by the lack of a comprehensive benchmark . this work contributes such a rgb-t image dataset , which includes 821 spatially aligned rgb-t image pairs and their ground truth annotations for saliency detection purpose . the image pairs are with high diversity recorded under different scenes and environmental conditions , and we annotate 11 challenges on these image pairs for performing the challenge-sensitive analysis for different saliency detection algorithms . we also implement 3 kinds of baseline methods with different modality inputs to provide a comprehensive comparison platform . with this benchmark , we propose a novel approach , multi-task manifold ranking with cross-modality consistency , for rgb-t saliency detection . in particular , we introduce a weight for each modality to describe the reliability , and integrate them into the graph-based manifold ranking algorithm to achieve adaptive fusion of different source data . moreover , we incorporate the cross-modality consistent constraints to integrate different modalities collaboratively . for the optimization , we design an efficient algorithm to iteratively solve several subproblems with closed-form solutions . extensive experiments against other baseline methods on the newly created benchmark demonstrate the effectiveness of the proposed approach , and we also provide basic insights and potential future research directions for rgb-t saliency detection .", "topics": ["baseline ( configuration management )", "ground truth"]}
{"title": "prosocial learning agents solve generalized stag hunts better than selfish ones", "abstract": "deep reinforcement learning has become an important paradigm for constructing agents that can enter complex multi-agent situations and improve their policies through experience . one commonly used technique is reactive training - applying standard rl methods while treating other agents as a part of the learner 's environment . it is known that in general-sum games reactive training can lead groups of agents to converge to inefficient outcomes . we focus on one such class of environments : stag hunt games . here agents either choose a risky cooperative policy ( which leads to high payoffs if both choose it but low payoffs to an agent who attempts it alone ) or a safe one ( which leads to a safe payoff no matter what ) . we ask how we can change the learning rule of a single agent to improve its outcomes in stag hunts that include other reactive learners . we extend existing work on reward-shaping in multi-agent reinforcement learning and show that that making a single agent prosocial , that is , making them care about the rewards of their partners can increase the probability that groups converge to good outcomes . thus , even if we control a single agent in a group making that agent prosocial can increase our agent 's long-run payoff . we show experimentally that this result carries over to a variety of more complex environments with stag hunt-like dynamics including ones where agents must learn from raw input pixels .", "topics": ["reinforcement learning", "pixel"]}
{"title": "large-scale analysis of chess games with chess engines : a preliminary report", "abstract": "the strength of chess engines together with the availability of numerous chess games have attracted the attention of chess players , data scientists , and researchers during the last decades . state-of-the-art engines now provide an authoritative judgement that can be used in many applications like cheating detection , intrinsic ratings computation , skill assessment , or the study of human decision-making . a key issue for the research community is to gather a large dataset of chess games together with the judgement of chess engines . unfortunately the analysis of each move takes lots of times . in this paper , we report our effort to analyse almost 5 millions chess games with a computing grid . during summer 2015 , we processed 270 millions unique played positions using the stockfish engine with a quite high depth ( 20 ) . we populated a database of 1+ tera-octets of chess evaluations , representing an estimated time of 50 years of computation on a single machine . our effort is a first step towards the replication of research results , the supply of open data and procedures for exploring new directions , and the investigation of software engineering/scalability issues when computing billions of moves .", "topics": ["computation", "scalability"]}
{"title": "object detection in video with spatiotemporal sampling networks", "abstract": "we propose a spatiotemporal sampling network ( stsn ) that uses deformable convolutions across time for object detection in videos . our stsn performs object detection in a video frame by learning to spatially sample features from the adjacent frames . this naturally renders the approach robust to occlusion or motion blur in individual frames . our framework does not require additional supervision , as it optimizes sampling locations directly with respect to object detection performance . our stsn outperforms the state-of-the-art on the imagenet vid dataset and compared to prior video object detection methods it uses a simpler design , and does not require optical flow data for training . we also show that after training stsn on videos , we can adapt it for object detection in images , by adding and training a single deformable convolutional layer on still-image data . this leads to improvements in accuracy compared to traditional object detection in images .", "topics": ["sampling ( signal processing )", "object detection"]}
{"title": "motion estimated-compensated reconstruction with preserved-features in free-breathing cardiac mri", "abstract": "to develop an efficient motion-compensated reconstruction technique for free-breathing cardiac magnetic resonance imaging ( mri ) that allows high-quality images to be reconstructed from multiple undersampled single-shot acquisitions . the proposed method is a joint image reconstruction and motion correction method consisting of several steps , including a non-rigid motion extraction and a motion-compensated reconstruction . the reconstruction includes a denoising with the beltrami regularization , which offers an ideal compromise between feature preservation and staircasing reduction . results were assessed in simulation , phantom and volunteer experiments . the proposed joint image reconstruction and motion correction method exhibits visible quality improvement over previous methods while reconstructing sharper edges . moreover , when the acceleration factor increases , standard methods show blurry results while the proposed method preserves image quality . the method was applied to free-breathing single-shot cardiac mri , successfully achieving high image quality and higher spatial resolution than conventional segmented methods , with the potential to offer high-quality delayed enhancement scans in challenging patients .", "topics": ["noise reduction", "simulation"]}
{"title": "cbinfer : change-based inference for convolutional neural networks on video data", "abstract": "extracting per-frame features using convolutional neural networks for real-time processing of video data is currently mainly performed on powerful gpu-accelerated workstations and compute clusters . however , there are many applications such as smart surveillance cameras that require or would benefit from on-site processing . to this end , we propose and evaluate a novel algorithm for change-based evaluation of cnns for video data recorded with a static camera setting , exploiting the spatio-temporal sparsity of pixel changes . we achieve an average speed-up of 8.6x over a cudnn baseline on a realistic benchmark with a negligible accuracy loss of less than 0.1 % and no retraining of the network . the resulting energy efficiency is 10x higher than that of per-frame evaluation and reaches an equivalent of 328 gop/s/w on the tegra x1 platform .", "topics": ["baseline ( configuration management )", "sparse matrix"]}
{"title": "towards a question answering system over the semantic web", "abstract": "thanks to the development of the semantic web , a lot of new structured data has become available on the web in the form of knowledge bases ( kbs ) . making this valuable data accessible and usable for end-users is one of the main goals of question answering ( qa ) over kbs . most current qa systems query one kb , in one language ( namely english ) . the existing approaches are not designed to be easily adaptable to new kbs and languages . we first introduce a new approach for translating natural language questions to sparql queries . it is able to query several kbs simultaneously , in different languages , and can easily be ported to other kbs and languages . in our evaluation , the impact of our approach is proven using 5 different well-known and large kbs : wikidata , dbpedia , musicbrainz , dblp and freebase as well as 5 different languages namely english , german , french , italian and spanish . second , we show how we integrated our approach , to make it easily accessible by the research community and by end-users . to summarize , we provided a conceptional solution for multilingual , kb-agnostic question answering over the semantic web . the provided first approximation validates this concept .", "topics": ["natural language"]}
{"title": "recent advances in zero-shot recognition", "abstract": "with the recent renaissance of deep convolution neural networks , encouraging breakthroughs have been achieved on the supervised recognition tasks , where each class has sufficient training data and fully annotated training data . however , to scale the recognition to a large number of classes with few or now training samples for each class remains an unsolved problem . one approach to scaling up the recognition is to develop models capable of recognizing unseen categories without any training instances , or zero-shot recognition/ learning . this article provides a comprehensive review of existing zero-shot recognition techniques covering various aspects ranging from representations of models , and from datasets and evaluation settings . we also overview related recognition tasks including one-shot and open set recognition which can be used as natural extensions of zero-shot recognition when limited number of class samples become available or when zero-shot recognition is implemented in a real-world setting . importantly , we highlight the limitations of existing approaches and point out future research directions in this existing new research area .", "topics": ["test set", "convolution"]}
{"title": "learning infinite rbms with frank-wolfe", "abstract": "in this work , we propose an infinite restricted boltzmann machine~ ( rbm ) , whose maximum likelihood estimation~ ( mle ) corresponds to a constrained convex optimization . we consider the frank-wolfe algorithm to solve the program , which provides a sparse solution that can be interpreted as inserting a hidden unit at each iteration , so that the optimization process takes the form of a sequence of finite models of increasing complexity . as a side benefit , this can be used to easily and efficiently identify an appropriate number of hidden units during the optimization . the resulting model can also be used as an initialization for typical state-of-the-art rbm training algorithms such as contrastive divergence , leading to models with consistently higher test likelihood than random initialization .", "topics": ["iteration", "sparse matrix"]}
{"title": "learning complex swarm behaviors by exploiting local communication protocols with deep reinforcement learning", "abstract": "swarm systems constitute a challenging problem for reinforcement learning ( rl ) as the algorithm needs to learn decentralized control policies that can cope with limited local sensing and communication abilities of the agents . although there have been recent advances of deep rl algorithms applied to multi-agent systems , learning communication protocols while simultaneously learning the behavior of the agents is still beyond the reach of deep rl algorithms . however , while it is often difficult to directly define the behavior of the agents , simple communication protocols can be defined more easily using prior knowledge about the given task . in this paper , we propose a number of simple communication protocols that can be exploited by deep reinforcement learning to find decentralized control policies in a multi-robot swarm environment . the protocols are based on histograms that encode the local neighborhood relations of the agents and can also transmit task-specific information , such as the shortest distance and direction to a desired target . in our framework , we use an adaptation of trust region policy optimization to learn complex collaborative tasks , such as formation building , building a communication link , and pushing an intruder . we evaluate our findings in a simulated 2d-physics environment , and compare the implications of different communication protocols .", "topics": ["reinforcement learning", "simulation"]}
{"title": "cixl2 : a crossover operator for evolutionary algorithms based on population features", "abstract": "in this paper we propose a crossover operator for evolutionary algorithms with real values that is based on the statistical theory of population distributions . the operator is based on the theoretical distribution of the values of the genes of the best individuals in the population . the proposed operator takes into account the localization and dispersion features of the best individuals of the population with the objective that these features would be inherited by the offspring . our aim is the optimization of the balance between exploration and exploitation in the search process . in order to test the efficiency and robustness of this crossover , we have used a set of functions to be optimized with regard to different criteria , such as , multimodality , separability , regularity and epistasis . with this set of functions we can extract conclusions in function of the problem at hand . we analyze the results using anova and multiple comparison statistical tests . as an example of how our crossover can be used to solve artificial intelligence problems , we have applied the proposed model to the problem of obtaining the weight of each network in a ensemble of neural networks . the results obtained are above the performance of standard methods .", "topics": ["artificial intelligence"]}
{"title": "addressing cross-lingual word sense disambiguation on low-density languages : application to persian", "abstract": "we explore the use of unsupervised methods in cross-lingual word sense disambiguation ( cl-wsd ) with the application of english to persian . our proposed approach targets the languages with scarce resources ( low-density ) by exploiting word embedding and semantic similarity of the words in context . we evaluate the approach on a recent evaluation benchmark and compare it with the state-of-the-art unsupervised system ( co-graph ) . the results show that our approach outperforms both the standard baseline and the co-graph system in both of the task evaluation metrics ( out-of-five and best result ) .", "topics": ["baseline ( configuration management )", "unsupervised learning"]}
{"title": "histogram of oriented depth gradients for action recognition", "abstract": "in this paper , we report on experiments with the use of local measures for depth motion for visual action recognition from mpeg encoded rgbd video sequences . we show that such measures can be combined with local space-time video descriptors for appearance to provide a computationally efficient method for recognition of actions . fisher vectors are used for encoding and concatenating a depth descriptor with existing rgb local descriptors . we then employ a linear svm for recognizing manipulation actions using such vectors . we evaluate the effectiveness of such measures by comparison to the state-of-the-art using two recent datasets for action recognition in kitchen environments .", "topics": ["computational complexity theory"]}
{"title": "numerical coding of nominal data", "abstract": "in this paper , a novel approach for coding nominal data is proposed . for the given nominal data , a rank in a form of complex number is assigned . the proposed method does not lose any information about the attribute and brings other properties previously unknown . the approach based on these knew properties can been used for classification . the analyzed example shows that classification with the use of coded nominal data or both numerical as well as coded nominal data is more effective than the classification , which uses only numerical data .", "topics": ["numerical analysis"]}
{"title": "clevr : a diagnostic dataset for compositional language and elementary visual reasoning", "abstract": "when building artificial intelligence systems that can reason and answer questions about visual data , we need diagnostic tests to analyze our progress and discover shortcomings . existing benchmarks for visual question answering can help , but have strong biases that models can exploit to correctly answer questions without reasoning . they also conflate multiple sources of error , making it hard to pinpoint model weaknesses . we present a diagnostic dataset that tests a range of visual reasoning abilities . it contains minimal biases and has detailed annotations describing the kind of reasoning each question requires . we use this dataset to analyze a variety of modern visual reasoning systems , providing novel insights into their abilities and limitations .", "topics": ["artificial intelligence"]}
{"title": "learning hard alignments with variational inference", "abstract": "there has recently been significant interest in hard attention models for tasks such as object recognition , visual captioning and speech recognition . hard attention can offer benefits over soft attention such as decreased computational cost , but training hard attention models can be difficult because of the discrete latent variables they introduce . previous work used reinforce and q-learning to approach these issues , but those methods can provide high-variance gradient estimates and be slow to train . in this paper , we tackle the problem of learning hard attention for a sequential task using variational inference methods , specifically the recently introduced vimco and nvil . furthermore , we propose a novel baseline that adapts vimco to this setting . we demonstrate our method on a phoneme recognition task in clean and noisy environments and show that our method outperforms reinforce , with the difference being greater for a more complicated task .", "topics": ["baseline ( configuration management )", "calculus of variations"]}
{"title": "on convergent finite difference schemes for variational - pde based image processing", "abstract": "we study an adaptive anisotropic huber functional based image restoration scheme . by using a combination of l2-l1 regularization functions , an adaptive huber functional based energy minimization model provides denoising with edge preservation in noisy digital images . we study a convergent finite difference scheme based on continuous piecewise linear functions and use a variable splitting scheme , namely the split bregman , to obtain the discrete minimizer . experimental results are given in image denoising and comparison with additive operator splitting , dual fixed point , and projected gradient schemes illustrate that the best convergence rates are obtained for our algorithm .", "topics": ["image processing", "noise reduction"]}
{"title": "east : an efficient and accurate scene text detector", "abstract": "previous approaches for scene text detection have already achieved promising performances across various benchmarks . however , they usually fall short when dealing with challenging scenarios , even when equipped with deep neural network models , because the overall performance is determined by the interplay of multiple stages and components in the pipelines . in this work , we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes . the pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images , eliminating unnecessary intermediate steps ( e.g . , candidate aggregation and word partitioning ) , with a single neural network . the simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture . experiments on standard datasets including icdar 2015 , coco-text and msra-td500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency . on the icdar 2015 dataset , the proposed algorithm achieves an f-score of 0.7820 at 13.2fps at 720p resolution .", "topics": ["loss function"]}
{"title": "dual teaching : a practical semi-supervised wrapper method", "abstract": "semi-supervised wrapper methods are concerned with building effective supervised classifiers from partially labeled data . though previous works have succeeded in some fields , it is still difficult to apply semi-supervised wrapper methods to practice because the assumptions those methods rely on tend to be unrealistic in practice . for practical use , this paper proposes a novel semi-supervised wrapper method , dual teaching , whose assumptions are easy to set up . dual teaching adopts two external classifiers to estimate the false positives and false negatives of the base learner . only if the recall of every external classifier is greater than zero and the sum of the precision is greater than one , dual teaching will train a base learner from partially labeled data as effectively as the fully-labeled-data-trained classifier . the effectiveness of dual teaching is proved in both theory and practice .", "topics": ["supervised learning"]}
{"title": "resiliency of deep neural networks under quantization", "abstract": "the complexity of deep neural network algorithms for hardware implementation can be much lowered by optimizing the word-length of weights and signals . direct quantization of floating-point weights , however , does not show good performance when the number of bits assigned is small . retraining of quantized networks has been developed to relieve this problem . in this work , the effects of retraining are analyzed for a feedforward deep neural network ( ffdnn ) and a convolutional neural network ( cnn ) . the network complexity is controlled to know their effects on the resiliency of quantized networks by retraining . the complexity of the ffdnn is controlled by varying the unit size in each hidden layer and the number of layers , while that of the cnn is done by modifying the feature map configuration . we find that the performance gap between the floating-point and the retrain-based ternary ( +1 , 0 , -1 ) weight neural networks exists with a fair amount in 'complexity limited ' networks , but the discrepancy almost vanishes in fully complex networks whose capability is limited by the training data , rather than by the number of connections . this research shows that highly complex dnns have the capability of absorbing the effects of severe weight quantization through retraining , but connection limited networks are less resilient . this paper also presents the effective compression ratio to guide the trade-off between the network size and the precision when the hardware resource is limited .", "topics": ["test set", "neural networks"]}
{"title": "modelling sentence pairs with tree-structured attentive encoder", "abstract": "we describe an attentive encoder that combines tree-structured recursive neural networks and sequential recurrent neural networks for modelling sentence pairs . since existing attentive models exert attention on the sequential structure , we propose a way to incorporate attention into the tree topology . specially , given a pair of sentences , our attentive encoder uses the representation of one sentence , which generated via an rnn , to guide the structural encoding of the other sentence on the dependency parse tree . we evaluate the proposed attentive encoder on three tasks : semantic similarity , paraphrase identification and true-false question selection . experimental results show that our encoder outperforms all baselines and achieves state-of-the-art results on two tasks .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "learning word representations with hierarchical sparse coding", "abstract": "we propose a new method for learning word representations using hierarchical regularization in sparse coding inspired by the linguistic study of word meanings . we show an efficient learning algorithm based on stochastic proximal methods that is significantly faster than previous approaches , making it possible to perform hierarchical sparse coding on a corpus of billions of word tokens . experiments on various benchmark tasks -- -word similarity ranking , analogies , sentence completion , and sentiment analysis -- -demonstrate that the method outperforms or is competitive with state-of-the-art methods . our word representations are available at \\url { http : //www.ark.cs.cmu.edu/dyogatam/wordvecs/ } .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "learning discriminative features using encoder-decoder type deep neural nets", "abstract": "as machine learning is applied to an increasing variety of complex problems , which are defined by high dimensional and complex data sets , the necessity for task oriented feature learning grows in importance . with the advancement of deep learning algorithms , various successful feature learning techniques have evolved . in this paper , we present a novel way of learning discriminative features by training deep neural nets which have encoder or decoder type architecture similar to an autoencoder . we demonstrate that our approach can learn discriminative features which can perform better at pattern classification tasks when the number of training samples is relatively small in size .", "topics": ["feature learning", "statistical classification"]}
{"title": "heteroscedastic treed bayesian optimisation", "abstract": "optimising black-box functions is important in many disciplines , such as tuning machine learning models , robotics , finance and mining exploration . bayesian optimisation is a state-of-the-art technique for the global optimisation of black-box functions which are expensive to evaluate . at the core of this approach is a gaussian process prior that captures our belief about the distribution over functions . however , in many cases a single gaussian process is not flexible enough to capture non-stationarity in the objective function . consequently , heteroscedasticity negatively affects performance of traditional bayesian methods . in this paper , we propose a novel prior model with hierarchical parameter learning that tackles the problem of non-stationarity in bayesian optimisation . our results demonstrate substantial improvements in a wide range of applications , including automatic machine learning and mining exploration .", "topics": ["mathematical optimization", "bayesian network"]}
{"title": "latent tree models and approximate inference in bayesian networks", "abstract": "we propose a novel method for approximate inference in bayesian networks ( bns ) . the idea is to sample data from a bn , learn a latent tree model ( ltm ) from the data offline , and when online , make inference with the ltm instead of the original bn . because ltms are tree-structured , inference takes linear time . in the meantime , they can represent complex relationship among leaf nodes and hence the approximation accuracy is often good . empirical evidence shows that our method can achieve good approximation accuracy at low online computational cost .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "intensity video guided 4d fusion for improved highly dynamic 3d reconstruction", "abstract": "the availability of high-speed 3d video sensors has greatly facilitated 3d shape acquisition of dynamic and deformable objects , but high frame rate 3d reconstruction is always degraded by spatial noise and temporal fluctuations . this paper presents a simple yet powerful intensity video guided multi-frame 4d fusion pipeline . temporal tracking of intensity image points ( of moving and deforming objects ) allows registration of the corresponding 3d data points , whose 3d noise and fluctuations are then reduced by spatio-temporal multi-frame 4d fusion . we conducted simulated noise tests and real experiments on four 3d objects using a 1000 fps 3d video sensor . the results demonstrate that the proposed algorithm is effective at reducing 3d noise and is robust against intensity noise . it outperforms existing algorithms with good scalability on both stationary and dynamic objects .", "topics": ["simulation", "scalability"]}
{"title": "pseudo vs . true defect classification in printed circuits boards using wavelet features", "abstract": "in recent years , printed circuit boards ( pcb ) have become the backbone of a large number of consumer electronic devices leading to a surge in their production . this has made it imperative to employ automatic inspection systems to identify manufacturing defects in pcb before they are installed in the respective systems . an important task in this regard is the classification of defects as either true or pseudo defects , which decides if the pcb is to be re-manufactured or not . this work proposes a novel approach to detect most common defects in the pcbs . the problem has been approached by employing highly discriminative features based on multi-scale wavelet transform , which are further boosted by using a kernalized version of the support vector machines ( svm ) . a real world printed circuit board dataset has been used for quantitative analysis . experimental results demonstrated the efficacy of the proposed method .", "topics": ["support vector machine"]}
{"title": "on prediction using variable order markov models", "abstract": "this paper is concerned with algorithms for prediction of discrete sequences over a finite alphabet , using variable order markov models . the class of such algorithms is large and in principle includes any lossless compression algorithm . we focus on six prominent prediction algorithms , including context tree weighting ( ctw ) , prediction by partial match ( ppm ) and probabilistic suffix trees ( psts ) . we discuss the properties of these algorithms and compare their performance using real life sequences from three domains : proteins , english text and music pieces . the comparison is made with respect to prediction quality as measured by the average log-loss . we also compare classification algorithms based on these predictors with respect to a number of large protein classification tasks . our results indicate that a `` decomposed '' ctw ( a variant of the ctw algorithm ) and ppm outperform all other algorithms in sequence prediction tasks . somewhat surprisingly , a different algorithm , which is a modification of the lempel-ziv compression algorithm , significantly outperforms all algorithms on the protein classification problems .", "topics": ["statistical classification"]}
{"title": "sentence boundary detection for french with subword-level information vectors and convolutional neural networks", "abstract": "in this work we tackle the problem of sentence boundary detection applied to french as a binary classification task ( `` sentence boundary '' or `` not sentence boundary '' ) . we combine convolutional neural networks with subword-level information vectors , which are word embedding representations learned from wikipedia that take advantage of the words morphology ; so each word is represented as a bag of their character n-grams . we decide to use a big written dataset ( french gigaword ) instead of standard size transcriptions to train and evaluate the proposed architectures with the intention of using the trained models in posterior real life asr transcriptions . three different architectures are tested showing similar results ; general accuracy for all models overpasses 0.96 . all three models have good f1 scores reaching values over 0.97 regarding the `` not sentence boundary '' class . however , the `` sentence boundary '' class reflects lower scores decreasing the f1 metric to 0.778 for one of the models . using subword-level information vectors seem to be very effective leading to conclude that the morphology of words encoded in the embeddings representations behave like pixels in an image making feasible the use of convolutional neural network architectures .", "topics": ["value ( ethics )", "neural networks"]}
{"title": "a reduction for optimizing lattice submodular functions with diminishing returns", "abstract": "a function $ f : \\mathbb { z } _+^e \\rightarrow \\mathbb { r } _+ $ is dr-submodular if it satisfies $ f ( \\bx + \\chi_i ) -f ( \\bx ) \\ge f ( \\by + \\chi_i ) - f ( \\by ) $ for all $ \\bx\\le \\by , i\\in e $ . recently , the problem of maximizing a dr-submodular function $ f : \\mathbb { z } _+^e \\rightarrow \\mathbb { r } _+ $ subject to a budget constraint $ \\|\\bx\\|_1 \\leq b $ as well as additional constraints has received significant attention \\cite { skik14 , sy15 , myk15 , sy16 } . in this note , we give a generic reduction from the dr-submodular setting to the submodular setting . the running time of the reduction and the size of the resulting submodular instance depends only \\emph { logarithmically } on $ b $ . using this reduction , one can translate the results for unconstrained and constrained submodular maximization to the dr-submodular setting for many types of constraints in a unified manner .", "topics": ["time complexity"]}
{"title": "automatic tuning of interactive perception applications", "abstract": "interactive applications incorporating high-data rate sensing and computer vision are becoming possible due to novel runtime systems and the use of parallel computation resources . to allow interactive use , such applications require careful tuning of multiple application parameters to meet required fidelity and latency bounds . this is a nontrivial task , often requiring expert knowledge , which becomes intractable as resources and application load characteristics change . this paper describes a method for automatic performance tuning that learns application characteristics and effects of tunable parameters online , and constructs models that are used to maximize fidelity for a given latency constraint . the paper shows that accurate latency models can be learned online , knowledge of application structure can be used to reduce the complexity of the learning task , and operating points can be found that achieve 90 % of the optimal fidelity by exploring the parameter space only 3 % of the time .", "topics": ["computer vision", "computation"]}
{"title": "stacked deconvolutional network for semantic segmentation", "abstract": "recent progress in semantic segmentation has been driven by improving the spatial resolution under fully convolutional networks ( fcns ) . to address this problem , we propose a stacked deconvolutional network ( sdn ) for semantic segmentation . in sdn , multiple shallow deconvolutional networks , which are called as sdn units , are stacked one by one to integrate contextual information and guarantee the fine recovery of localization information . meanwhile , inter-unit and intra-unit connections are designed to assist network training and enhance feature fusion since the connections improve the flow of information and gradient propagation throughout the network . besides , hierarchical supervision is applied during the upsampling process of each sdn unit , which guarantees the discrimination of feature representations and benefits the network optimization . we carry out comprehensive experiments and achieve the new state-of-the-art results on three datasets , including pascal voc 2012 , camvid , gatech . in particular , our best model without crf post-processing achieves an intersection-over-union score of 86.6 % in the test set .", "topics": ["test set"]}
{"title": "jesc : japanese-english subtitle corpus", "abstract": "in this paper we describe the japanese-english subtitle corpus ( jesc ) . jesc is a large japanese-english parallel corpus covering the underrepresented domain of conversational dialogue . it consists of more than 3.2 million examples , making it the largest freely available dataset of its kind . the corpus was assembled by crawling and aligning subtitles found on the web . the assembly process incorporates a number of novel preprocessing elements to ensure high monolingual fluency and accurate bilingual alignments . we summarize its contents and evaluate its quality using human experts and baseline machine translation ( mt ) systems .", "topics": ["baseline ( configuration management )", "machine translation"]}
{"title": "bayesian modeling with gaussian processes using the gpstuff toolbox", "abstract": "gaussian processes ( gp ) are powerful tools for probabilistic modeling purposes . they can be used to define prior distributions over latent functions in hierarchical bayesian models . the prior over functions is defined implicitly by the mean and covariance function , which determine the smoothness and variability of the function . the inference can then be conducted directly in the function space by evaluating or approximating the posterior process . despite their attractive theoretical properties gps provide practical challenges in their implementation . gpstuff is a versatile collection of computational tools for gp models compatible with linux and windows matlab and octave . it includes , among others , various inference methods , sparse approximations and tools for model assessment . in this work , we review these tools and demonstrate the use of gpstuff in several models .", "topics": ["approximation algorithm", "approximation"]}
{"title": "time-sensitive customer churn prediction based on pu learning", "abstract": "with the fast development of internet companies throughout the world , customer churn has become a serious concern . to better help the companies retain their customers , it is important to build a customer churn prediction model to identify the customers who are most likely to churn ahead of time . in this paper , we propose a time-sensitive customer churn prediction ( tccp ) framework based on positive and unlabeled ( pu ) learning technique . specifically , we obtain the recent data by shortening the observation period , and start to train model as long as enough positive samples are collected , ignoring the absence of the negative examples . we conduct thoroughly experiments on real industry data from alipay.com . the experimental results demonstrate that tccp outperforms the rule-based models and the traditional supervised learning models .", "topics": ["supervised learning"]}
{"title": "feudal reinforcement learning for dialogue management in large domains", "abstract": "reinforcement learning ( rl ) is a promising approach to solve dialogue policy optimisation . traditional rl algorithms , however , fail to scale to large domains due to the curse of dimensionality . we propose a novel dialogue management architecture , based on feudal rl , which decomposes the decision into two steps ; a first step where a master policy selects a subset of primitive actions , and a second step where a primitive action is chosen from the selected subset . the structural information included in the domain ontology is used to abstract the dialogue state space , taking the decisions at each step using different parts of the abstracted state . this , combined with an information sharing mechanism between slots , increases the scalability to large domains . we show that an implementation of this approach , based on deep-q networks , significantly outperforms previous state of the art in several dialogue domains and environments , without the need of any additional reward signal .", "topics": ["mathematical optimization", "reinforcement learning"]}
{"title": "a generalization error bound for sparse and low-rank multivariate hawkes processes", "abstract": "we consider the problem of unveiling the implicit network structure of user interactions in a social network , based only on high-frequency timestamps . our inference is based on the minimization of the least-squares loss associated with a multivariate hawkes model , penalized by $ \\ell_1 $ and trace norms . we provide a first theoretical analysis of the generalization error for this problem , that includes sparsity and low-rank inducing priors . this result involves a new data-driven concentration inequality for matrix martingales in continuous time with observable variance , which is a result of independent interest . a consequence of our analysis is the construction of sharply tuned $ \\ell_1 $ and trace-norm penalizations , that leads to a data-driven scaling of the variability of information available for each users . numerical experiments illustrate the strong improvements achieved by the use of such data-driven penalizations .", "topics": ["interaction", "sparse matrix"]}
{"title": "safe exploration of state and action spaces in reinforcement learning", "abstract": "in this paper , we consider the important problem of safe exploration in reinforcement learning . while reinforcement learning is well-suited to domains with complex transition dynamics and high-dimensional state-action spaces , an additional challenge is posed by the need for safe and efficient exploration . traditional exploration techniques are not particularly useful for solving dangerous tasks , where the trial and error process may lead to the selection of actions whose execution in some states may result in damage to the learning system ( or any other system ) . consequently , when an agent begins an interaction with a dangerous and high-dimensional state-action space , an important question arises ; namely , that of how to avoid ( or at least minimize ) damage caused by the exploration of the state-action space . we introduce the pi-srl algorithm which safely improves suboptimal albeit robust behaviors for continuous state and action control tasks and which efficiently learns from the experience gained from the environment . we evaluate the proposed method in four complex tasks : automatic car parking , pole-balancing , helicopter hovering , and business management .", "topics": ["reinforcement learning", "causality"]}
{"title": "the surprising creativity of digital evolution : a collection of anecdotes from the evolutionary computation and artificial life research communities", "abstract": "biological evolution provides a creative fount of complex and subtle adaptations , often surprising the scientists who discover them . however , because evolution is an algorithmic process that transcends the substrate in which it occurs , evolution 's creativity is not limited to nature . indeed , many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions , exposing unrecognized bugs in their code , producing unexpected adaptations , or exhibiting outcomes uncannily convergent with ones in nature . such stories routinely reveal creativity by evolution in these digital worlds , but they rarely fit into the standard scientific narrative . instead they are often treated as mere obstacles to be overcome , rather than results that warrant study in their own right . the stories themselves are traded among researchers through oral tradition , but that mode of information transmission is inefficient and prone to error and outright loss . moreover , the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be . to our knowledge , no collection of such anecdotes has been published before . this paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases . it thus serves as a written , fact-checked collection of scientifically important and even entertaining stories . in doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world , and may indeed be a universal property of all complex evolving systems .", "topics": ["computation"]}
{"title": "re-evaluating the need for modelling term-dependence in text classification problems", "abstract": "a substantial amount of research has been carried out in developing machine learning algorithms that account for term dependence in text classification . these algorithms offer acceptable performance in most cases but they are associated with a substantial cost . they require significantly greater resources to operate . this paper argues against the justification of the higher costs of these algorithms , based on their performance in text classification problems . in order to prove the conjecture , the performance of one of the best dependence models is compared to several well established algorithms in text classification . a very specific collection of datasets have been designed , which would best reflect the disparity in the nature of text data , that are present in real world applications . the results show that even one of the best term dependence models , performs decent at best when compared to other independence models . coupled with their substantially greater requirement for hardware resources for operation , this makes them an impractical choice for being used in real world scenarios .", "topics": ["text corpus"]}
{"title": "adaptive binarization for weakly supervised affordance segmentation", "abstract": "the concept of affordance is important to understand the relevance of object parts for a certain functional interaction . affordance types generalize across object categories and are not mutually exclusive . this makes the segmentation of affordance regions of objects in images a difficult task . in this work , we build on an iterative approach that learns a convolutional neural network for affordance segmentation from sparse keypoints . during this process , the predictions of the network need to be binarized . in this work , we propose an adaptive approach for binarization and estimate the parameters for initialization by approximated cross validation . we evaluate our approach on two affordance datasets where our approach outperforms the state-of-the-art for weakly supervised affordance segmentation .", "topics": ["supervised learning", "sparse matrix"]}
{"title": "senteval : an evaluation toolkit for universal sentence representations", "abstract": "we introduce senteval , a toolkit for evaluating the quality of universal sentence representations . senteval encompasses a variety of tasks , including binary and multi-class classification , natural language inference and sentence similarity . the set of tasks was selected based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations . the toolkit comes with scripts to download and preprocess datasets , and an easy interface to evaluate sentence encoders . the aim is to provide a fairer , less cumbersome and more centralized way for evaluating sentence representations .", "topics": ["natural language", "encoder"]}
{"title": "an efficient training algorithm for kernel survival support vector machines", "abstract": "survival analysis is a fundamental tool in medical research to identify predictors of adverse events and develop systems for clinical decision support . in order to leverage large amounts of patient data , efficient optimisation routines are paramount . we propose an efficient training algorithm for the kernel survival support vector machine ( ssvm ) . we directly optimise the primal objective function and employ truncated newton optimisation and order statistic trees to significantly lower computational costs compared to previous training algorithms , which require $ o ( n^4 ) $ space and $ o ( p n^6 ) $ time for datasets with $ n $ samples and $ p $ features . our results demonstrate that our proposed optimisation scheme allows analysing data of a much larger scale with no loss in prediction performance . experiments on synthetic and 5 real-world datasets show that our technique outperforms existing kernel ssvm formulations if the amount of right censoring is high ( $ \\geq85\\ % $ ) , and performs comparably otherwise .", "topics": ["kernel ( operating system )", "support vector machine"]}
{"title": "notes on geometric measure theory applications to image processing ; de-noising , segmentation , pattern , texture , lines , gestalt and occlusion", "abstract": "regularization functionals that lower level set boundary length when used with l^1 fidelity functionals on signal de-noising on images create artifacts . these are ( i ) rounding of corners , ( ii ) shrinking of radii , ( iii ) shrinking of cusps , and ( iv ) non-smoothing of staircasing . regularity functionals based upon total curvature of level set boundaries do not create artifacts ( i ) and ( ii ) . an adjusted fidelity term based on the flat norm on the current ( a distributional graph ) representing the density of curvature of level sets boundaries can minimize ( iii ) by weighting the position of a cusp . a regularity term to eliminate staircasing can be based upon the mass of the current representing the graph of an image function or its second derivatives . densities on the grassmann bundle of the grassmann bundle of the ambient space of the graph can be used to identify patterns , textures , occlusion and lines .", "topics": ["image processing", "matrix regularization"]}
{"title": "identifying metaphoric antonyms in a corpus analysis of finance articles", "abstract": "using a corpus of 17,000+ financial news reports ( involving over 10m words ) , we perform an analysis of the argument-distributions of the up and down verbs used to describe movements of indices , stocks and shares . in study 1 participants identified antonyms of these verbs in a free-response task and a matching task from which the most commonly identified antonyms were compiled . in study 2 , we determined whether the argument-distributions for the verbs in these antonym-pairs were sufficiently similar to predict the most frequently-identified antonym . cosine similarity correlates moderately with the proportions of antonym-pairs identified by people ( r = 0.31 ) . more impressively , 87 % of the time the most frequently-identified antonym is either the first- or second-most similar pair in the set of alternatives . the implications of these results for distributional approaches to determining metaphoric knowledge are discussed .", "topics": ["text corpus"]}
{"title": "spatial graph convolutions for drug discovery", "abstract": "predicting the binding free energy , or affinity , of a small molecule for a protein target is frequently the first step along the arc of drug discovery . high throughput experimental and virtual screening both suffer from low accuracy , whereas more accurate approaches in both domains suffer from lack of scale due to either financial or temporal constraints . while machine learning ( ml ) has made immense progress in the fields of computer vision and natural language processing , it has yet to offer comparable improvements over domain-expertise driven algorithms in the molecular sciences . in this paper , we propose new deep neural network ( dnn ) architectures for affinity prediction . the new model architectures are at least competitive with , and in many cases state-of-the-art compared to previous knowledge-based and physics-based approaches . in addition to more standard evaluation metrics , we also propose the regression enrichment factor $ ef_\\chi^ { ( r ) } $ for the community to benchmark against in future affinity prediction studies . finally , we suggest the adaptation of an agglomerative clustering cross-validation strategy to more accurately reflect the generalization capacity of ml-based affinity models in future works .", "topics": ["cluster analysis", "natural language processing"]}
{"title": "feature selection through minimization of the vc dimension", "abstract": "feature selection involes identifying the most relevant subset of input features , with a view to improving generalization of predictive models by reducing overfitting . directly searching for the most relevant combination of attributes is np-hard . variable selection is of critical importance in many applications , such as micro-array data analysis , where selecting a small number of discriminative features is crucial to developing useful models of disease mechanisms , as well as for prioritizing targets for drug discovery . the recently proposed minimal complexity machine ( mcm ) provides a way to learn a hyperplane classifier by minimizing an exact ( \\boldmath { $ \\theta $ } ) bound on its vc dimension . it is well known that a lower vc dimension contributes to good generalization . for a linear hyperplane classifier in the input space , the vc dimension is upper bounded by the number of features ; hence , a linear classifier with a small vc dimension is parsimonious in the set of features it employs . in this paper , we use the linear mcm to learn a classifier in which a large number of weights are zero ; features with non-zero weights are the ones that are chosen . selected features are used to learn a kernel svm classifier . on a number of benchmark datasets , the features chosen by the linear mcm yield comparable or better test set accuracy than when methods such as relieff and fcbf are used for the task . the linear mcm typically chooses one-tenth the number of attributes chosen by the other methods ; on some very high dimensional datasets , the mcm chooses about $ 0.6\\ % $ of the features ; in comparison , relieff and fcbf choose 70 to 140 times more features , thus demonstrating that minimizing the vc dimension may provide a new , and very effective route for feature selection and for learning sparse representations .", "topics": ["kernel ( operating system )", "test set"]}
{"title": "peekaboo - where are the objects ? structure adjusting superpixels", "abstract": "this paper addresses the search for a fast and meaningful image segmentation in the context of $ k $ -means clustering . the proposed method builds on a widely-used local version of lloyd 's algorithm , called simple linear iterative clustering ( slic ) . we propose an algorithm which extends slic to dynamically adjust the local search , adopting superpixel resolution dynamically to structure existent in the image , and thus provides for more meaningful superpixels in the same linear runtime as standard slic . the proposed method is evaluated against state-of-the-art techniques and improved boundary adherence and undersegmentation error are observed , whilst still remaining among the fastest algorithms which are tested .", "topics": ["cluster analysis", "image segmentation"]}
{"title": "dependency grammar induction with neural lexicalization and big training data", "abstract": "we study the impact of big models ( in terms of the degree of lexicalization ) and big data ( in terms of the training corpus size ) on dependency grammar induction . we experimented with l-dmv , a lexicalized version of dependency model with valence and l-ndmv , our lexicalized extension of the neural dependency model with valence . we find that l-dmv only benefits from very small degrees of lexicalization and moderate sizes of training corpora . l-ndmv can benefit from big training data and lexicalization of greater degrees , especially when enhanced with good model initialization , and it achieves a result that is competitive with the current state-of-the-art .", "topics": ["test set", "text corpus"]}
{"title": "privacy for free : posterior sampling and stochastic gradient monte carlo", "abstract": "we consider the problem of bayesian learning on sensitive datasets and present two simple but somewhat surprising results that connect bayesian learning to `` differential privacy : , a cryptographic approach to protect individual-level privacy while permiting database-level utility . specifically , we show that that under standard assumptions , getting one single sample from a posterior distribution is differentially private `` for free '' . we will see that estimator is statistically consistent , near optimal and computationally tractable whenever the bayesian model of interest is consistent , optimal and tractable . similarly but separately , we show that a recent line of works that use stochastic gradient for hybrid monte carlo ( hmc ) sampling also preserve differentially privacy with minor or no modifications of the algorithmic procedure at all , these observations lead to an `` anytime '' algorithm for bayesian learning under privacy constraint . we demonstrate that it performs much better than the state-of-the-art differential private methods on synthetic and real datasets .", "topics": ["synthetic data", "bayesian network"]}
{"title": "subgraph2vec : learning distributed representations of rooted sub-graphs from large graphs", "abstract": "in this paper , we present subgraph2vec , a novel approach for learning latent representations of rooted subgraphs from large graphs inspired by recent advancements in deep learning and graph kernels . these latent representations encode semantic substructure dependencies in a continuous vector space , which is easily exploited by statistical models for tasks such as graph classification , clustering , link prediction and community detection . subgraph2vec leverages on local information obtained from neighbourhoods of nodes to learn their latent representations in an unsupervised fashion . we demonstrate that subgraph vectors learnt by our approach could be used in conjunction with classifiers such as cnns , svms and relational data clustering algorithms to achieve significantly superior accuracies . also , we show that the subgraph vectors could be used for building a deep learning variant of weisfeiler-lehman graph kernel . our experiments on several benchmark and large-scale real-world datasets reveal that subgraph2vec achieves significant improvements in accuracies over existing graph kernels on both supervised and unsupervised learning tasks . specifically , on two realworld program analysis tasks , namely , code clone and malware detection , subgraph2vec outperforms state-of-the-art kernels by more than 17 % and 4 % , respectively .", "topics": ["cluster analysis", "unsupervised learning"]}
{"title": "an alternative markov property for chain graphs", "abstract": "graphical markov models use graphs , either undirected , directed , or mixed , to represent possible dependences among statistical variables . applications of undirected graphs ( udgs ) include models for spatial dependence and image analysis , while acyclic directed graphs ( adgs ) , which are especially convenient for statistical analysis , arise in such fields as genetics and psychometrics and as models for expert systems and bayesian belief networks . lauritzen , wermuth and frydenberg ( lwf ) introduced a markov property for chain graphs , which are mixed graphs that can be used to represent simultaneously both causal and associative dependencies and which include both udgs and adgs as special cases . in this paper an alternative markov property ( amp ) for chain graphs is introduced , which in some ways is a more direct extension of the adg markov property than is the lwf property for chain graph .", "topics": ["bayesian network", "markov chain"]}
{"title": "soft constraints for quality aspects in service oriented architectures", "abstract": "we propose the use of soft constraints as a natural way to model service oriented architecture . in the framework , constraints are used to model components and connectors and constraint aggregation is used to represent their interactions . the `` quality of a service '' is measured and considered when performing queries to service providers . some examples consist in the levels of cost , performance and availability required by clients . in our framework , the qos scores are represented by the softness level of the constraint and the measure of complex ( web ) services is computed by combining the levels of the components .", "topics": ["interaction"]}
{"title": "recurrent neural networks with external memory for language understanding", "abstract": "recurrent neural networks ( rnns ) have become increasingly popular for the task of language understanding . in this task , a semantic tagger is deployed to associate a semantic label to each word in an input sequence . the success of rnn may be attributed to its ability to memorize long-term dependence that relates the current-time semantic label prediction to the observations many time instances away . however , the memory capacity of simple rnns is limited because of the gradient vanishing and exploding problem . we propose to use an external memory to improve memorization capability of rnns . we conducted experiments on the atis dataset , and observed that the proposed model was able to achieve the state-of-the-art results . we compare our proposed model with alternative models and report analysis results that may provide insights for future research .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "deepasl : enabling ubiquitous and non-intrusive word and sentence-level sign language translation", "abstract": "there is an undeniable communication barrier between deaf people and people with normal hearing ability . although innovations in sign language translation technology aim to tear down this communication barrier , the majority of existing sign language translation systems are either intrusive or constrained by resolution or ambient lighting conditions . moreover , these existing systems can only perform single-sign asl translation rather than sentence-level translation , making them much less useful in daily-life communication scenarios . in this work , we fill this critical gap by presenting deepasl , a transformative deep learning-based sign language translation technology that enables ubiquitous and non-intrusive american sign language ( asl ) translation at both word and sentence levels . deepasl uses infrared light as its sensing mechanism to non-intrusively capture the asl signs . it incorporates a novel hierarchical bidirectional deep recurrent neural network ( hb-rnn ) and a probabilistic framework based on connectionist temporal classification ( ctc ) for word-level and sentence-level asl translation respectively . to evaluate its performance , we have collected 7,306 samples from 11 participants , covering 56 commonly used asl words and 100 asl sentences . deepasl achieves an average 94.5 % word-level translation accuracy and an average 8.2 % word error rate on translating unseen asl sentences . given its promising performance , we believe deepasl represents a significant step towards breaking the communication barrier between deaf people and hearing majority , and thus has the significant potential to fundamentally change deaf people 's lives .", "topics": ["recurrent neural network"]}
{"title": "multispectral image denoising with optimized vector non-local mean filter", "abstract": "nowadays , many applications rely on images of high quality to ensure good performance in conducting their tasks . however , noise goes against this objective as it is an unavoidable issue in most applications . therefore , it is essential to develop techniques to attenuate the impact of noise , while maintaining the integrity of relevant information in images . we propose in this work to extend the application of the non-local means filter ( nlm ) to the vector case and apply it for denoising multispectral images . the objective is to benefit from the additional information brought by multispectral imaging systems . the nlm filter exploits the redundancy of information in an image to remove noise . a restored pixel is a weighted average of all pixels in the image . in our contribution , we propose an optimization framework where we dynamically fine tune the nlm filter parameters and attenuate its computational complexity by considering only pixels which are most similar to each other in computing a restored pixel . filter parameters are optimized using stein 's unbiased risk estimator ( sure ) rather than using ad hoc means . experiments have been conducted on multispectral images corrupted with additive white gaussian noise and psnr and similarity comparison with other approaches are provided to illustrate the efficiency of our approach in terms of both denoising performance and computation complexity .", "topics": ["computational complexity theory", "noise reduction"]}
{"title": "interpolating convex and non-convex tensor decompositions via the subspace norm", "abstract": "we consider the problem of recovering a low-rank tensor from its noisy observation . previous work has shown a recovery guarantee with signal to noise ratio $ o ( n^ { \\lceil k/2 \\rceil /2 } ) $ for recovering a $ k $ th order rank one tensor of size $ n\\times \\cdots \\times n $ by recursive unfolding . in this paper , we first improve this bound to $ o ( n^ { k/4 } ) $ by a much simpler approach , but with a more careful analysis . then we propose a new norm called the subspace norm , which is based on the kronecker products of factors obtained by the proposed simple estimator . the imposed kronecker structure allows us to show a nearly ideal $ o ( \\sqrt { n } +\\sqrt { h^ { k-1 } } ) $ bound , in which the parameter $ h $ controls the blend from the non-convex estimator to mode-wise nuclear norm minimization . furthermore , we empirically demonstrate that the subspace norm achieves the nearly ideal denoising performance even with $ h=o ( 1 ) $ .", "topics": ["noise reduction"]}
{"title": "an analysis of deep neural network models for practical applications", "abstract": "since the emergence of deep neural networks ( dnns ) as a prominent technique in the field of computer vision , the imagenet classification challenge has played a major role in advancing the state-of-the-art . while accuracy figures have steadily increased , the resource utilisation of winning models has not been properly taken into account . in this work , we present a comprehensive analysis of important metrics in practical applications : accuracy , memory footprint , parameters , operations count , inference time and power consumption . key findings are : ( 1 ) power consumption is independent of batch size and architecture ; ( 2 ) accuracy and inference time are in a hyperbolic relationship ; ( 3 ) energy constraint is an upper bound on the maximum achievable accuracy and model complexity ; ( 4 ) the number of operations is a reliable estimate of the inference time . we believe our analysis provides a compelling set of information that helps design and engineer efficient dnns .", "topics": ["computer vision", "neural networks"]}
{"title": "fused multiple graphical lasso", "abstract": "in this paper , we consider the problem of estimating multiple graphical models simultaneously using the fused lasso penalty , which encourages adjacent graphs to share similar structures . a motivating example is the analysis of brain networks of alzheimer 's disease using neuroimaging data . specifically , we may wish to estimate a brain network for the normal controls ( nc ) , a brain network for the patients with mild cognitive impairment ( mci ) , and a brain network for alzheimer 's patients ( ad ) . we expect the two brain networks for nc and mci to share common structures but not to be identical to each other ; similarly for the two brain networks for mci and ad . the proposed formulation can be solved using a second-order method . our key technical contribution is to establish the necessary and sufficient condition for the graphs to be decomposable . based on this key property , a simple screening rule is presented , which decomposes the large graphs into small subgraphs and allows an efficient estimation of multiple independent ( small ) subgraphs , dramatically reducing the computational cost . we perform experiments on both synthetic and real data ; our results demonstrate the effectiveness and efficiency of the proposed approach .", "topics": ["graphical model", "synthetic data"]}
{"title": "imposing hard constraints on deep networks : promises and limitations", "abstract": "imposing constraints on the output of a deep neural net is one way to improve the quality of its predictions while loosening the requirements for labeled training data . such constraints are usually imposed as soft constraints by adding new terms to the loss function that is minimized during training . an alternative is to impose them as hard constraints , which has a number of theoretical benefits but has not been explored so far due to the perceived intractability of the problem . in this paper , we show that imposing hard constraints can in fact be done in a computationally feasible way and delivers reasonable results . however , the theoretical benefits do not materialize and the resulting technique is no better than existing ones relying on soft constraints . we analyze the reasons for this and hope to spur other researchers into proposing better solutions .", "topics": ["test set", "loss function"]}
{"title": "a lexicalist approach to the translation of colloquial text", "abstract": "colloquial english ( ce ) as found in television programs or typical conversations is different than text found in technical manuals , newspapers and books . phrases tend to be shorter and less sophisticated . in this paper , we look at some of the theoretical and implementational issues involved in translating ce . we present a fully automatic large-scale multilingual natural language processing system for translation of ce input text , as found in the commercially transmitted closed-caption television signal , into simple target sentences . our approach is based on the whitelock 's shake and bake machine translation paradigm , which relies heavily on lexical resources . the system currently translates from english to spanish with the translation modules for brazilian portuguese under development .", "topics": ["natural language processing", "machine translation"]}
{"title": "automatic evaluation and uniform filter cascades for inducing n-best translation lexicons", "abstract": "this paper shows how to induce an n-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources . the knowledge sources are cast as filters , so that any subset of them can be cascaded in a uniform framework . a new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades . the best filter cascades improve lexicon quality by up to 137 % over the plain vanilla statistical method , and approach human performance . drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used . this makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable . moreover , three of the four filters prove useful even when used with large training corpora .", "topics": ["text corpus"]}
{"title": "a step forward in studying the compact genetic algorithm", "abstract": "the compact genetic algorithm ( cga ) is an estimation of distribution algorithm that generates offspring population according to the estimated probabilistic model of the parent population instead of using traditional recombination and mutation operators . the cga only needs a small amount of memory ; therefore , it may be quite useful in memory-constrained applications . this paper introduces a theoretical framework for studying the cga from the convergence point of view in which , we model the cga by a markov process and approximate its behavior using an ordinary differential equation ( ode ) . then , we prove that the corresponding ode converges to local optima and stays there . consequently , we conclude that the cga will converge to the local optima of the function to be optimized .", "topics": ["approximation algorithm"]}
{"title": "making decisions using sets of probabilities : updating , time consistency , and calibration", "abstract": "we consider how an agent should update her beliefs when her beliefs are represented by a set p of probability distributions , given that the agent makes decisions using the minimax criterion , perhaps the best-studied and most commonly-used criterion in the literature . we adopt a game-theoretic framework , where the agent plays against a bookie , who chooses some distribution from p . we consider two reasonable games that differ in what the bookie knows when he makes his choice . anomalies that have been observed before , like time inconsistency , can be understood as arising because different games are being played , against bookies with different information . we characterize the important special cases in which the optimal decision rules according to the minimax criterion amount to either conditioning or simply ignoring the information . finally , we consider the relationship between updating and calibration when uncertainty is described by sets of probabilities . our results emphasize the key role of the rectangularity condition of epstein and schneider .", "topics": ["eisenstein 's criterion"]}
{"title": "conditional generation and snapshot learning in neural dialogue systems", "abstract": "recently a variety of lstm-based conditional language models ( lm ) have been applied across a range of language generation tasks . in this work we study various model architectures and different ways to represent and aggregate the source information in an end-to-end neural dialogue system framework . a method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector . the experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the lm , and the differing architectures provide different trade-offs between the two . secondly , the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance . thirdly , snapshot learning leads to consistent performance improvements independent of which architecture is used .", "topics": ["loss function"]}
{"title": "temporal segment networks : towards good practices for deep action recognition", "abstract": "deep convolutional networks have achieved great success for visual recognition in still images . however , for action recognition in videos , the advantage over traditional methods is not so evident . this paper aims to discover the principles to design effective convnet architectures for action recognition in videos and learn these models given limited training samples . our first contribution is temporal segment network ( tsn ) , a novel framework for video-based action recognition . which is based on the idea of long-range temporal structure modeling . it combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video . the other contribution is our study on a series of good practices in learning convnets on video data with the help of temporal segment network . our approach obtains the state-the-of-art performance on the datasets of hmdb51 ( $ 69.4\\ % $ ) and ucf101 ( $ 94.2\\ % $ ) . we also visualize the learned convnet models , which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices .", "topics": ["sampling ( signal processing )", "sparse matrix"]}
{"title": "softmax q-distribution estimation for structured prediction : a theoretical interpretation for raml", "abstract": "reward augmented maximum likelihood ( raml ) , a simple and effective learning framework to directly optimize towards the reward function in structured prediction tasks , has led to a number of impressive empirical successes . raml incorporates task-specific reward by performing maximum-likelihood updates on candidate outputs sampled according to an exponentiated payoff distribution , which gives higher probabilities to candidates that are close to the reference output . while raml is notable for its simplicity , efficiency , and its impressive empirical successes , the theoretical properties of raml , especially the behavior of the exponentiated payoff distribution , has not been examined thoroughly . in this work , we introduce softmax q-distribution estimation , a novel theoretical interpretation of raml , which reveals the relation between raml and bayesian decision theory . the softmax q-distribution can be regarded as a smooth approximation of the bayes decision boundary , and the bayes decision rule is achieved by decoding with this q-distribution . we further show that raml is equivalent to approximately estimating the softmax q-distribution , with the temperature $ \\tau $ controlling approximation error . we perform two experiments , one on synthetic data of multi-class classification and one on real data of image captioning , to demonstrate the relationship between raml and the proposed softmax q-distribution estimation method , verifying our theoretical analysis . additional experiments on three structured prediction tasks with rewards defined on sequential ( named entity recognition ) , tree-based ( dependency parsing ) and irregular ( machine translation ) structures show notable improvements over maximum likelihood baselines .", "topics": ["machine translation", "reinforcement learning"]}
{"title": "design of kernels in convolutional neural networks for image classification", "abstract": "despite the effectiveness of convolutional neural networks ( cnns ) for image classification , our understanding of the relationship between shape of convolution kernels and learned representations is limited . in this work , we explore and employ the relationship between shape of kernels which define receptive fields ( rfs ) in cnns for learning of feature representations and image classification . for this purpose , we first propose a feature visualization method for visualization of pixel-wise classification score maps of learned features . motivated by our experimental results , and observations reported in the literature for modeling of visual systems , we propose a novel design of shape of kernels for learning of representations in cnns . in the experimental results , we achieved a state-of-the-art classification performance compared to a base cnn model [ 28 ] by reducing the number of parameters and computational time of the model using the ilsvrc-2012 dataset [ 24 ] . the proposed models also outperform the state-of-the-art models employed on the cifar-10/100 datasets [ 12 ] for image classification . additionally , we analyzed the robustness of the proposed method to occlusion for classification of partially occluded images compared with the state-of-the-art methods . our results indicate the effectiveness of the proposed approach . the code is available in github.com/minogame/caffe-qhconv .", "topics": ["neural networks", "computer vision"]}
{"title": "arabian horse identification benchmark dataset", "abstract": "the lack of a standard muzzle print database is a challenge for conducting researches in arabian horse identification systems . therefore , collecting a muzzle print images database is a crucial decision . the dataset presented in this paper is an option for the studies that need a dataset for testing and comparing the algorithms under development for arabian horse identification . our collected dataset consists of 300 color images that were collected from 50 arabian horse muzzle species . this dataset has been collected from 50 arabian horses with 6 muzzle print images each . a special care has been given to the quality of the collected images . the collected images cover different quality levels and degradation factors such as image rotation and image partiality for simulating real time identification operations . this dataset can be used to test the identification of arabian horse system including the extracted features and the selected classifier .", "topics": ["simulation"]}
{"title": "evaluating protein-protein interaction predictors with a novel 3-dimensional metric", "abstract": "in order for the predicted interactions to be directly adopted by biologists , the ma- chine learning predictions have to be of high precision , regardless of recall . this aspect can not be evaluated or numerically represented well by traditional metrics like accuracy , roc , or precision-recall curve . in this work , we start from the alignment in sensitivity of roc and recall of precision-recall curve , and propose an evaluation metric focusing on the ability of a model to be adopted by biologists . this metric evaluates the ability of a machine learning algorithm to predict only new interactions , meanwhile , it eliminates the influence of test dataset . in the experiment of evaluating different classifiers with a same data set and evaluating the same predictor with different datasets , our new metric fulfills the evaluation task of our interest while two widely recognized metrics , roc and precision-recall curve fail the tasks for different reasons .", "topics": ["numerical analysis", "interaction"]}
{"title": "3d reconstruction in canonical co-ordinate space from arbitrarily oriented 2d images", "abstract": "limited capture range , and the requirement to provide high quality initialization for optimization-based 2d/3d image registration methods , can significantly degrade the performance of 3d image reconstruction and motion compensation pipelines . challenging clinical imaging scenarios , which contain significant subject motion such as fetal in-utero imaging , complicate the 3d image and volume reconstruction process . in this paper we present a learning based image registration method capable of predicting 3d rigid transformations of arbitrarily oriented 2d image slices , with respect to a learned canonical atlas co-ordinate system . only image slice intensity information is used to perform registration and canonical alignment , no spatial transform initialization is required . to find image transformations we utilize a convolutional neural network ( cnn ) architecture to learn the regression function capable of mapping 2d image slices to a 3d canonical atlas space . we extensively evaluate the effectiveness of our approach quantitatively on simulated magnetic resonance imaging ( mri ) , fetal brain imagery with synthetic motion and further demonstrate qualitative results on real fetal mri data where our method is integrated into a full reconstruction and motion compensation pipeline . our learning based registration achieves an average spatial prediction error of 7 mm on simulated data and produces qualitatively improved reconstructions for heavily moving fetuses with gestational ages of approximately 20 weeks . our model provides a general and computationally efficient solution to the 2d/3d registration initialization problem and is suitable for real-time scenarios .", "topics": ["mathematical optimization", "computational complexity theory"]}
{"title": "object discovery via cohesion measurement", "abstract": "color and intensity are two important components in an image . usually , groups of image pixels , which are similar in color or intensity , are an informative representation for an object . they are therefore particularly suitable for computer vision tasks , such as saliency detection and object proposal generation . however , image pixels , which share a similar real-world color , may be quite different since colors are often distorted by intensity . in this paper , we reinvestigate the affinity matrices originally used in image segmentation methods based on spectral clustering . a new affinity matrix , which is robust to color distortions , is formulated for object discovery . moreover , a cohesion measurement ( cm ) for object regions is also derived based on the formulated affinity matrix . based on the new cohesion measurement , a novel object discovery method is proposed to discover objects latent in an image by utilizing the eigenvectors of the affinity matrix . then we apply the proposed method to both saliency detection and object proposal generation . experimental results on several evaluation benchmarks demonstrate that the proposed cm based method has achieved promising performance for these two tasks .", "topics": ["image segmentation", "cluster analysis"]}
{"title": "the bayesian structural em algorithm", "abstract": "in recent years there has been a flurry of works on learning bayesian networks from data . one of the hard problems in this area is how to effectively learn the structure of a belief network from incomplete data- that is , in the presence of missing values or hidden variables . in a recent paper , i introduced an algorithm called structural em that combines the standard expectation maximization ( em ) algorithm , which optimizes parameters , with structure search for model selection . that algorithm learns networks based on penalized likelihood scores , which include the bic/mdl score and various approximations to the bayesian score . in this paper , i extend structural em to deal directly with bayesian model selection . i prove the convergence of the resulting algorithm and show how to apply it for learning a large class of probabilistic models , including bayesian networks and some variants thereof .", "topics": ["value ( ethics )", "bayesian network"]}
{"title": "a parameterized approach to personalized variable length summarization of soccer matches", "abstract": "we present a parameterized approach to produce personalized variable length summaries of soccer matches . our approach is based on temporally segmenting the soccer video into 'plays ' , associating a user-specifiable 'utility ' for each type of play and using 'bin-packing ' to select a subset of the plays that add up to the desired length while maximizing the overall utility ( volume in bin-packing terms ) . our approach systematically allows a user to override the default weights assigned to each type of play with individual preferences and thus see a highly personalized variable length summarization of soccer matches . we demonstrate our approach based on the output of an end-to-end pipeline that we are building to produce such summaries . though aspects of the overall end-to-end pipeline are human assisted at present , the results clearly show that the proposed approach is capable of producing semantically meaningful and compelling summaries . besides the obvious use of producing summaries of superior league matches for news broadcasts , we anticipate our work to promote greater awareness of the local matches and junior leagues by producing consumable summaries of them .", "topics": ["end-to-end principle"]}
{"title": "cnn in mrf : video object segmentation via inference in a cnn-based higher-order spatio-temporal mrf", "abstract": "this paper addresses the problem of video object segmentation , where the initial object mask is given in the first frame of an input video . we propose a novel spatio-temporal markov random field ( mrf ) model defined over pixels to handle this problem . unlike conventional mrf models , the spatial dependencies among pixels in our model are encoded by a convolutional neural network ( cnn ) . specifically , for a given object , the probability of a labeling to a set of spatially neighboring pixels can be predicted by a cnn trained for this specific object . as a result , higher-order , richer dependencies among pixels in the set can be implicitly modeled by the cnn . with temporal dependencies established by optical flow , the resulting mrf model combines both spatial and temporal cues for tackling video object segmentation . however , performing inference in the mrf model is very difficult due to the very high-order dependencies . to this end , we propose a novel cnn-embedded algorithm to perform approximate inference in the mrf . this algorithm proceeds by alternating between a temporal fusion step and a feed-forward cnn step . when initialized with an appearance-based one-shot segmentation cnn , our model outperforms the winning entries of the davis 2017 challenge , without resorting to model ensembling or any dedicated detectors .", "topics": ["approximation algorithm", "pixel"]}
{"title": "decomposing overcomplete 3rd order tensors using sum-of-squares algorithms", "abstract": "tensor rank and low-rank tensor decompositions have many applications in learning and complexity theory . most known algorithms use unfoldings of tensors and can only handle rank up to $ n^ { \\lfloor p/2 \\rfloor } $ for a $ p $ -th order tensor in $ \\mathbb { r } ^ { n^p } $ . previously no efficient algorithm can decompose 3rd order tensors when the rank is super-linear in the dimension . using ideas from sum-of-squares hierarchy , we give the first quasi-polynomial time algorithm that can decompose a random 3rd order tensor decomposition when the rank is as large as $ n^ { 3/2 } /\\textrm { polylog } n $ . we also give a polynomial time algorithm for certifying the injective norm of random low rank tensors . our tensor decomposition algorithm exploits the relationship between injective norm and the tensor components . the proof relies on interesting tools for decoupling random variables to prove better matrix concentration bounds , which can be useful in other settings .", "topics": ["time complexity", "computational complexity theory"]}
{"title": "boltzmann machines for time-series", "abstract": "we review boltzmann machines extended for time-series . these models often have recurrent structure , and back propagration through time ( bptt ) is used to learn their parameters . the per-step computational complexity of bptt in online learning , however , grows linearly with respect to the length of preceding time-series ( i.e . , learning rule is not local in time ) , which limits the applicability of bptt in online learning . we then review dynamic boltzmann machines ( dybms ) , whose learning rule is local in time . dybm 's learning rule relates to spike-timing dependent plasticity ( stdp ) , which has been postulated and experimentally confirmed for biological neural networks .", "topics": ["time series", "computational complexity theory"]}
{"title": "deep learning convolutional networks for multiphoton microscopy vasculature segmentation", "abstract": "recently there has been an increasing trend to use deep learning frameworks for both 2d consumer images and for 3d medical images . however , there has been little effort to use deep frameworks for volumetric vascular segmentation . we wanted to address this by providing a freely available dataset of 12 annotated two-photon vasculature microscopy stacks . we demonstrated the use of deep learning framework consisting both 2d and 3d convolutional filters ( convnet ) . our hybrid 2d-3d architecture produced promising segmentation result . we derived the architectures from lee et al . who used the znn framework initially designed for electron microscope image segmentation . we hope that by sharing our volumetric vasculature datasets , we will inspire other researchers to experiment with vasculature dataset and improve the used network architectures .", "topics": ["image segmentation"]}
{"title": "sparse 3d convolutional neural networks", "abstract": "we have implemented a convolutional neural network designed for processing sparse three-dimensional input data . the world we live in is three dimensional so there are a large number of potential applications including 3d object recognition and analysis of space-time objects . in the quest for efficiency , we experiment with cnns on the 2d triangular-lattice and 3d tetrahedral-lattice .", "topics": ["time series", "computational complexity theory"]}
{"title": "parameter estimation based on interval-valued belief structures", "abstract": "parameter estimation based on uncertain data represented as belief structures is one of the latest problems in the dempster-shafer theory . in this paper , a novel method is proposed for the parameter estimation in the case where belief structures are uncertain and represented as interval-valued belief structures . within our proposed method , the maximization of likelihood criterion and minimization of estimated parameter 's uncertainty are taken into consideration simultaneously . as an illustration , the proposed method is employed to estimate parameters for deterministic and uncertain belief structures , which demonstrates its effectiveness and versatility .", "topics": ["eisenstein 's criterion"]}
{"title": "transfer learning for low-resource neural machine translation", "abstract": "the encoder-decoder framework for neural machine translation ( nmt ) has been shown effective in large data scenarios , but is much less effective for low-resource languages . we present a transfer learning method that significantly improves bleu scores across a range of low-resource languages . our key idea is to first train a high-resource language pair ( the parent model ) , then transfer some of the learned parameters to the low-resource pair ( the child model ) to initialize and constrain training . using our transfer learning method we improve baseline nmt models by an average of 5.6 bleu on four low-resource language pairs . ensembling and unknown word replacement add another 2 bleu which brings the nmt performance on low-resource machine translation close to a strong syntax based machine translation ( sbmt ) system , exceeding its performance on one language pair . additionally , using the transfer learning model for re-scoring , we can improve the sbmt system by an average of 1.3 bleu , improving the state-of-the-art on low-resource machine translation .", "topics": ["baseline ( configuration management )", "machine translation"]}
{"title": "deep learning for physical processes : incorporating prior scientific knowledge", "abstract": "we consider the use of deep learning methods for modeling complex phenomena like those occurring in natural physical processes . with the large amount of data gathered on these phenomena the data intensive paradigm could begin to challenge more traditional approaches elaborated over the years in fields like maths or physics . however , despite considerable successes in a variety of application domains , the machine learning field is not yet ready to handle the level of complexity required by such problems . using an example application , namely sea surface temperature prediction , we show how general background knowledge gained from physics could be used as a guideline for designing efficient deep learning models . in order to motivate the approach and to assess its generality we demonstrate a formal link between the solution of a class of differential equations underlying a large family of physical phenomena and the proposed model . experiments and comparison with series of baselines including a state of the art numerical approach is then provided .", "topics": ["baseline ( configuration management )", "numerical analysis"]}
{"title": "model based learning for accelerated , limited-view 3d photoacoustic tomography", "abstract": "recent advances in deep learning for tomographic reconstructions have shown great potential to create accurate and high quality images with a considerable speed-up . in this work we present a deep neural network that is specifically designed to provide high resolution 3d images from restricted photoacoustic measurements . the network is designed to represent an iterative scheme and incorporates gradient information of the data fit to compensate for limited view artefacts . due to the high complexity of the photoacoustic forward operator , we separate training and computation of the gradient information . a suitable prior for the desired image structures is learned as part of the training . the resulting network is trained and tested on a set of segmented vessels from lung ct scans and then applied to in-vivo photoacoustic measurement data .", "topics": ["computation", "gradient descent"]}
{"title": "generating object cluster hierarchies for benchmarking", "abstract": "the field of machine learning and the topic of clustering within it is still widely researched . recently , researchers became interested in a new variant of hierarchical clustering , where hierarchical ( partial order ) relationships exist not only between clusters but also objects . in this variant of clustering , objects can be assigned not only to leave , but other properties are also defined . although examples of this approach already exist in literature , the authors have encountered a problem with the analysis and comparison of obtained results . the problem is twofold . firstly , there is a lack of evaluation methods . secondly , there is a lack of available benchmark data , at least the authors failed to find them . the aim of this work is to fill the second gap . the main contribution of this paper is a new method of generating hierarchical structures of data . additionally , the paper includes a theoretical analysis of the generation parameters and their influence on the results . comprehensive experiments are presented and discussed . the dataset generator and visualiser tools developed are publicly available for use ( http : //kio.pwr.edu.pl/ ? page_id=396 ) .", "topics": ["cluster analysis"]}
{"title": "proposition of a theoretical model for missing data imputation using deep learning and evolutionary algorithms", "abstract": "in the last couple of decades , there has been major advancements in the domain of missing data imputation . the techniques in the domain include amongst others : expectation maximization , neural networks with evolutionary algorithms or optimization techniques and k-nearest neighbor approaches to solve the problem . the presence of missing data entries in databases render the tasks of decision-making and data analysis nontrivial . as a result this area has attracted a lot of research interest with the aim being to yield accurate and time efficient and sensitive missing data imputation techniques especially when time sensitive applications are concerned like power plants and winding processes . in this article , considering arbitrary and monotone missing data patterns , we hypothesize that the use of deep neural networks built using autoencoders and denoising autoencoders in conjunction with genetic algorithms , swarm intelligence and maximum likelihood estimator methods as novel data imputation techniques will lead to better imputed values than existing techniques . also considered are the missing at random , missing completely at random and missing not at random missing data mechanisms . we also intend to use fuzzy logic in tandem with deep neural networks to perform the missing data imputation tasks , as well as different building blocks for the deep neural networks like stacked restricted boltzmann machines and deep belief networks to test our hypothesis . the motivation behind this article is the need for missing data imputation techniques that lead to better imputed values than existing methods with higher accuracies and lower errors .", "topics": ["noise reduction", "neural networks"]}
{"title": "programs as black-box explanations", "abstract": "recent work in model-agnostic explanations of black-box machine learning has demonstrated that interpretability of complex models does not have to come at the cost of accuracy or model flexibility . however , it is not clear what kind of explanations , such as linear models , decision trees , and rule lists , are the appropriate family to consider , and different tasks and models may benefit from different kinds of explanations . instead of picking a single family of representations , in this work we propose to use `` programs '' as model-agnostic explanations . we show that small programs can be expressive yet intuitive as explanations , and generalize over a number of existing interpretable families . we propose a prototype program induction method based on simulated annealing that approximates the local behavior of black-box classifiers around a specific prediction using random perturbations . finally , we present preliminary application on small datasets and show that the generated explanations are intuitive and accurate for a number of classifiers .", "topics": ["sparse matrix"]}
{"title": "learning high-level image representation for image retrieval via multi-task dnn using clickthrough data", "abstract": "image retrieval refers to finding relevant images from an image database for a query , which is considered difficult for the gap between low-level representation of images and high-level representation of queries . recently further developed deep neural network sheds light on automatically learning high-level image representation from raw pixels . in this paper , we proposed a multi-task dnn learned for image retrieval , which contains two parts , i.e . , query-sharing layers for image representation computation and query-specific layers for relevance estimation . the weights of multi-task dnn are learned on clickthrough data by ring training . experimental results on both simulated and real dataset show the effectiveness of the proposed method .", "topics": ["high- and low-level", "simulation"]}
{"title": "inferring multilateral relations from dynamic pairwise interactions", "abstract": "correlations between anomalous activity patterns can yield pertinent information about complex social processes : a significant deviation from normal behavior , exhibited simultaneously by multiple pairs of actors , provides evidence for some underlying relationship involving those pairs -- -i.e . , a multilateral relation . we introduce a new nonparametric bayesian latent variable model that explicitly captures correlations between anomalous interaction counts and uses these shared deviations from normal activity patterns to identify and characterize multilateral relations . we showcase our model 's capabilities using the newly curated global database of events , location , and tone , a dataset that has seen considerable interest in the social sciences and the popular press , but which has is largely unexplored by the machine learning community . we provide a detailed analysis of the latent structure inferred by our model and show that the multilateral relations correspond to major international events and long-term international relationships . these findings lead us to recommend our model for any data-driven analysis of interaction networks where dynamic interactions over the edges provide evidence for latent social structure .", "topics": ["interaction"]}
{"title": "towards large-scale and ultrahigh dimensional feature selection via feature generation", "abstract": "in many real-world applications such as text mining , it is desirable to select the most relevant features or variables to improve the generalization ability , or to provide a better interpretation of the prediction models . { in this paper , a novel adaptive feature scaling ( afs ) scheme is proposed by introducing a feature scaling { vector $ \\d \\in [ 0 , 1 ] ^m $ } to alleviate the bias problem brought by the scaling bias of the diverse features . } by reformulating the resultant afs model to semi-infinite programming problem , a novel feature generating method is presented to identify the most relevant features for classification problems . in contrast to the traditional feature selection methods , the new formulation has the advantage of solving extremely high-dimensional and large-scale problems . with an exact solution to the worst-case analysis in the identification of relevant features , the proposed feature generating scheme converges globally . more importantly , the proposed scheme facilitates the group selection with or without special structures . comprehensive experiments on a wide range of synthetic and real-world datasets demonstrate that the proposed method { achieves } better or competitive performance compared with the existing methods on ( group ) feature selection in terms of generalization performance and training efficiency . the c++ and matlab implementations of our algorithm can be available at \\emph { http : //c2inet.sce.ntu.edu.sg/mingkui/robust-fgm.rar } .", "topics": ["synthetic data"]}
{"title": "ev-flownet : self-supervised optical flow estimation for event-based cameras", "abstract": "event-based cameras have shown great promise in a variety of situations where frame based cameras suffer , such as high speed motions and high dynamic range scenes . however , developing algorithms for event measurements requires a new class of hand crafted algorithms . deep learning has shown great success in providing model free solutions to many problems in the vision community , but existing networks have been developed with frame based images in mind , and there does not exist the wealth of labeled data for events as there does for images for supervised training . to these points , we present ev-flownet , a novel self-supervised deep learning pipeline for optical flow estimation for event based cameras . in particular , we introduce an image based representation of a given event stream , which is fed into a self-supervised neural network as the sole input . the corresponding grayscale images captured from the same camera at the same time as the events are then used as a supervisory signal to provide a loss function at training time , given the estimated flow from the network . we show that the resulting network is able to accurately predict optical flow from events only in a variety of different scenes , with performance competitive to image based networks . this method not only allows for accurate estimation of dense optical flow , but also provides a framework for the transfer of other self-supervised methods to the event-based domain .", "topics": ["loss function"]}
{"title": "refined lower bounds for adversarial bandits", "abstract": "we provide new lower bounds on the regret that must be suffered by adversarial bandit algorithms . the new results show that recent upper bounds that either ( a ) hold with high-probability or ( b ) depend on the total lossof the best arm or ( c ) depend on the quadratic variation of the losses , are close to tight . besides this we prove two impossibility results . first , the existence of a single arm that is optimal in every round can not improve the regret in the worst case . second , the regret can not scale with the effective range of the losses . in contrast , both results are possible in the full-information setting .", "topics": ["regret ( decision theory )"]}
{"title": "an improved epsilon constraint-handling method in moea/d for cmops with large infeasible regions", "abstract": "this paper proposes an improved epsilon constraint-handling mechanism , and combines it with a decomposition-based multi-objective evolutionary algorithm ( moea/d ) to solve constrained multi-objective optimization problems ( cmops ) . the proposed constrained multi-objective evolutionary algorithm ( cmoea ) is named moea/d-iepsilon . it adjusts the epsilon level dynamically according to the ratio of feasible to total solutions ( rfs ) in the current population . in order to evaluate the performance of moea/d-iepsilon , a new set of cmops with two and three objectives is designed , having large infeasible regions ( relative to the feasible regions ) , and they are called lir-cmops . then the fourteen benchmarks , including lir-cmop1-14 , are used to test moea/d-iepsilon and four other decomposition-based cmoeas , including moea/d-epsilon , moea/d-sr , moea/d-cdp and c-moea/d . the experimental results indicate that moea/d-iepsilon is significantly better than the other four cmoeas on all of the test instances , which shows that moea/d-iepsilon is more suitable for solving cmops with large infeasible regions . furthermore , a real-world problem , namely the robot gripper optimization problem , is used to test the five cmoeas . the experimental results demonstrate that moea/d-iepsilon also outperforms the other four cmoeas on this problem .", "topics": ["optimization problem"]}
{"title": "combining representation learning with logic for language processing", "abstract": "the current state-of-the-art in many natural language processing and automated knowledge base completion tasks is held by representation learning methods which learn distributed vector representations of symbols via gradient-based optimization . they require little or no hand-crafted features , thus avoiding the need for most preprocessing steps and task-specific assumptions . however , in many cases representation learning requires a large amount of annotated training data to generalize well to unseen data . such labeled training data is provided by human annotators who often use formal logic as the language for specifying annotations . this thesis investigates different combinations of representation learning methods with logic for reducing the need for annotated training data , and for improving generalization .", "topics": ["feature learning", "test set"]}
{"title": "a fuzzy similarity based concept mining model for text classification", "abstract": "text classification is a challenging and a red hot field in the current scenario and has great importance in text categorization applications . a lot of research work has been done in this field but there is a need to categorize a collection of text documents into mutually exclusive categories by extracting the concepts or features using supervised learning paradigm and different classification algorithms . in this paper , a new fuzzy similarity based concept mining model ( fscmm ) is proposed to classify a set of text documents into pre - defined category groups ( cg ) by providing them training and preparing on the sentence , document and integrated corpora levels along with feature reduction , ambiguity removal on each level to achieve high system performance . fuzzy feature category similarity analyzer ( ffcsa ) is used to analyze each extracted feature of integrated corpora feature vector ( icfv ) with the corresponding categories or classes . this model uses support vector machine classifier ( svmc ) to classify correctly the training data patterns into two groups ; i. e. , + 1 and - 1 , thereby producing accurate and correct results . the proposed model works efficiently and effectively with great performance and high - accuracy results .", "topics": ["test set", "feature vector"]}
{"title": "a hierarchical framework of cloud resource allocation and power management using deep reinforcement learning", "abstract": "automatic decision-making approaches , such as reinforcement learning ( rl ) , have been applied to ( partially ) solve the resource allocation problem adaptively in the cloud computing system . however , a complete cloud resource allocation framework exhibits high dimensions in state and action spaces , which prohibit the usefulness of traditional rl techniques . in addition , high power consumption has become one of the critical concerns in design and control of cloud computing systems , which degrades system reliability and increases cooling cost . an effective dynamic power management ( dpm ) policy should minimize power consumption while maintaining performance degradation within an acceptable level . thus , a joint virtual machine ( vm ) resource allocation and power management framework is critical to the overall cloud computing system . moreover , novel solution framework is necessary to address the even higher dimensions in state and action spaces . in this paper , we propose a novel hierarchical framework for solving the overall resource allocation and power management problem in cloud computing systems . the proposed hierarchical framework comprises a global tier for vm resource allocation to the servers and a local tier for distributed power management of local servers . the emerging deep reinforcement learning ( drl ) technique , which can deal with complicated control problems with large state space , is adopted to solve the global tier problem . furthermore , an autoencoder and a novel weight sharing structure are adopted to handle the high-dimensional state space and accelerate the convergence speed . on the other hand , the local tier of distributed server power managements comprises an lstm based workload predictor and a model-free rl based power manager , operating in a distributed manner .", "topics": ["reinforcement learning", "autoencoder"]}
{"title": "railway track specific traffic signal selection using deep learning", "abstract": "with the railway transportation industry moving actively towards automation , accurate location and inventory of wayside track assets like traffic signals , crossings , switches , mileposts , etc . is of extreme importance . with the new positive train control ( ptc ) regulation coming into effect , many railway safety rules will be tied directly to location of assets like mileposts and signals . newer speed regulations will be enforced based on location of the train with respect to a wayside asset . hence it is essential for the railroads to have an accurate database of the types and locations of these assets . this paper talks about a real-world use-case of detecting railway signals from a camera mounted on a moving locomotive and tracking their locations . the camera is engineered to withstand the environment factors on a moving train and provide a consistent steady image at around 30 frames per second . using advanced image analysis and deep learning techniques , signals are detected in these camera images and a database of their locations is created . railway signals differ a lot from road signals in terms of shapes and rules for placement with respect to track . due to space constraint and traffic densities in urban areas signals are not placed on the same side of the track and multiple lines can run in parallel . hence there is need to associate signal detected with the track on which the train runs . we present a method to associate the signals to the specific track they belong to using a video feed from the front facing camera mounted on the lead locomotive . a pipeline of track detection , region of interest selection , signal detection has been implemented which gives an overall accuracy of 94.7 % on a route covering 150km with 247 signals .", "topics": ["database"]}
{"title": "lifted convex quadratic programming", "abstract": "symmetry is the essential element of lifted inference that has recently demon- strated the possibility to perform very efficient inference in highly-connected , but symmetric probabilistic models models . this raises the question , whether this holds for optimisation problems in general . here we show that for a large class of optimisation methods this is actually the case . more precisely , we introduce the concept of fractional symmetries of convex quadratic programs ( qps ) , which lie at the heart of many machine learning approaches , and exploit it to lift , i.e . , to compress qps . these lifted qps can then be tackled with the usual optimization toolbox ( off-the-shelf solvers , cutting plane algorithms , stochastic gradients etc . ) . if the original qp exhibits symmetry , then the lifted one will generally be more compact , and hence their optimization is likely to be more efficient .", "topics": ["mathematical optimization"]}
{"title": "hybrid tractability of soft constraint problems", "abstract": "the constraint satisfaction problem ( csp ) is a central generic problem in computer science and artificial intelligence : it provides a common framework for many theoretical problems as well as for many real-life applications . soft constraint problems are a generalisation of the csp which allow the user to model optimisation problems . considerable effort has been made in identifying properties which ensure tractability in such problems . in this work , we initiate the study of hybrid tractability of soft constraint problems ; that is , properties which guarantee tractability of the given soft constraint problem , but which do not depend only on the underlying structure of the instance ( such as being tree-structured ) or only on the types of soft constraints in the instance ( such as submodularity ) . we present several novel hybrid classes of soft constraint problems , which include a machine scheduling problem , constraint problems of arbitrary arities with no overlapping nogoods , and the softalldiff constraint with arbitrary unary soft constraints . an important tool in our investigation will be the notion of forbidden substructures .", "topics": ["mathematical optimization", "artificial intelligence"]}
{"title": "proximal scope for distributed sparse learning : better data partition implies faster convergence rate", "abstract": "distributed sparse learning with a cluster of multiple machines has attracted much attention in machine learning , especially for large-scale applications with high-dimensional data . one popular way to implement sparse learning is to use $ l_1 $ regularization . in this paper , we propose a novel method , called proximal \\mbox { scope } ~ ( \\mbox { pscope } ) , for distributed sparse learning with $ l_1 $ regularization . pscope is based on a \\underline { c } ooperative \\underline { a } utonomous \\underline { l } ocal \\underline { l } earning~ ( \\mbox { call } ) framework . in the \\mbox { call } framework of \\mbox { pscope } , we find that the data partition affects the convergence of the learning procedure , and subsequently we define a metric to measure the goodness of a data partition . based on the defined metric , we theoretically prove that pscope is convergent with a linear convergence rate if the data partition is good enough . we also prove that better data partition implies faster convergence rate . furthermore , pscope is also communication efficient . experimental results on real data sets show that pscope can outperform other state-of-the-art distributed methods for sparse learning .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "a robust linguistic platform for efficient and domain specific web content analysis", "abstract": "web semantic access in specific domains calls for specialized search engines with enhanced semantic querying and indexing capacities , which pertain both to information retrieval ( ir ) and to information extraction ( ie ) . a rich linguistic analysis is required either to identify the relevant semantic units to index and weight them according to linguistic specific statistical distribution , or as the basis of an information extraction process . recent developments make natural language processing ( nlp ) techniques reliable enough to process large collections of documents and to enrich them with semantic annotations . this paper focuses on the design and the development of a text processing platform , ogmios , which has been developed in the alvis project . the ogmios platform exploits existing nlp modules and resources , which may be tuned to specific domains and produces linguistically annotated documents . we show how the three constraints of genericity , domain semantic awareness and performance can be handled all together .", "topics": ["natural language processing", "natural language"]}
{"title": "face2text : collecting an annotated image description corpus for the generation of rich face descriptions", "abstract": "the past few years have witnessed renewed interest in nlp tasks at the interface between vision and language . one intensively-studied problem is that of automatically generating text from images . in this paper , we extend this problem to the more specific domain of face description . unlike scene descriptions , face descriptions are more fine-grained and rely on attributes extracted from the image , rather than objects and relations . given that no data exists for this task , we present an ongoing crowdsourcing study to collect a corpus of descriptions of face images taken `in the wild ' . to gain a better understanding of the variation we find in face description and the possible issues that this may raise , we also conducted an annotation study on a subset of the corpus . primarily , we found descriptions to refer to a mixture of attributes , not only physical , but also emotional and inferential , which is bound to create further challenges for current image-to-text methods .", "topics": ["natural language processing"]}
{"title": "leveraging the exact likelihood of deep latent variable models", "abstract": "deep latent variable models combine the approximation abilities of deep neural networks and the statistical foundations of generative models . the induced data distribution is an infinite mixture model whose density is extremely delicate to compute . variational methods are consequently used for inference , following the seminal work of rezende et al . ( 2014 ) and kingma and welling ( 2014 ) . we study the well-posedness of the exact problem ( maximum likelihood ) these techniques approximatively solve . in particular , we show that most unconstrained models used for continuous data have an unbounded likelihood . this ill-posedness and the problems it causes are illustrated on real data . we also show how to insure the existence of maximum likelihood estimates , and draw useful connections with nonparametric mixture models . furthermore , we describe an algorithm that allows to perform missing data imputation using the exact conditional likelihood of a deep latent variable model . on several real data sets , our algorithm consistently and significantly outperforms the usual imputation scheme used within deep latent variable models .", "topics": ["generative model", "calculus of variations"]}
{"title": "transferring agent behaviors from videos via motion gans", "abstract": "a major bottleneck for developing general reinforcement learning agents is determining rewards that will yield desirable behaviors under various circumstances . we introduce a general mechanism for automatically specifying meaningful behaviors from raw pixels . in particular , we train a generative adversarial network to produce short sub-goals represented through motion templates . we demonstrate that this approach generates visually meaningful behaviors in unknown environments with novel agents and describe how these motions can be used to train reinforcement learning agents .", "topics": ["reinforcement learning", "pixel"]}
{"title": "zero-shot learning via category-specific visual-semantic mapping", "abstract": "zero-shot learning ( zsl ) aims to classify a test instance from an unseen category based on the training instances from seen categories , in which the gap between seen categories and unseen categories is generally bridged via visual-semantic mapping between the low-level visual feature space and the intermediate semantic space . however , the visual-semantic mapping learnt based on seen categories may not generalize well to unseen categories because the data distributions between seen categories and unseen categories are considerably different , which is known as the projection domain shift problem in zsl . to address this domain shift issue , we propose a method named adaptive embedding zsl ( aezsl ) to learn an adaptive visual-semantic mapping for each unseen category based on the similarities between each unseen category and all the seen categories . then , we further make two extensions based on our aezsl method . firstly , in order to utilize the unlabeled test instances from unseen categories , we extend our aezsl to a semi-supervised approach named aezsl with label refinement ( aezsl_lr ) , in which a progressive approach is developed to update the visual classifiers and refine the predicted test labels alternatively based on the similarities among test instances and among unseen categories . secondly , to avoid learning visual-semantic mapping for each unseen category in the large-scale classification task , we extend our aezsl to a deep adaptive embedding model named deep aezsl ( daezsl ) sharing the similar idea ( i.e . , visual-semantic mapping should be category-specific and related to the semantic space ) with aezsl , which only needs to be trained once , but can be applied to arbitrary number of unseen categories . extensive experiments demonstrate that our proposed methods achieve the state-of-the-art results for image classification on four benchmark datasets .", "topics": ["feature vector", "high- and low-level"]}
{"title": "on the robustness of nearest neighbor with noisy data", "abstract": "nearest neighbor has always been one of the most appealing non-parametric approaches in machine learning , pattern recognition , computer vision , etc . previous empirical studies partially demonstrate that nearest neighbor is resistant to noise , yet there is a lack of deep analysis . this work presents a full understanding on the robustness of nearest neighbor in the random noise setting . we provide finite-sample , distribution-dependent bounds on the consistency of nearest neighbor . the theoretical results show that , for asymmetric noises , k-nearest neighbor is robust enough to classify most data correctly , except for a handful of examples , whose labels are totally misled by random noises . for symmetric noises , however , k-nearest neighbor achieves the same consistent rate as that of noise-free setting , which verifies the robustness of $ k $ -nearest neighbor . motivated by theoretical analysis , we propose the robust k-nearest neighbor ( rnn ) approach to deal with noisy labels . the basic idea is to make unilateral corrections to examples , whose labels are totally misled by random noises , and classify the others directly by utilizing the robustness of k-nearest neighbor . extensive experiments show the effectiveness and robustness of the proposed algorithm .", "topics": ["approximation algorithm", "numerical analysis"]}
{"title": "deep gaussian mixture models", "abstract": "deep learning is a hierarchical inference method formed by subsequent multiple layers of learning able to more efficiently describe complex relationships . in this work , deep gaussian mixture models are introduced and discussed . a deep gaussian mixture model ( dgmm ) is a network of multiple layers of latent variables , where , at each layer , the variables follow a mixture of gaussian distributions . thus , the deep mixture model consists of a set of nested mixtures of linear models , which globally provide a nonlinear model able to describe the data in a very flexible way . in order to avoid overparameterized solutions , dimension reduction by factor models can be applied at each layer of the architecture thus resulting in deep mixtures of factor analysers .", "topics": ["nonlinear system"]}
{"title": "generative adversarial perturbations", "abstract": "in this paper , we propose novel generative models for creating adversarial examples , slightly perturbed images resembling natural images but maliciously crafted to fool pre-trained models . we present trainable deep neural networks for transforming images to adversarial perturbations . our proposed models can produce image-agnostic and image-dependent perturbations for both targeted and non-targeted attacks . we also demonstrate that similar architectures can achieve impressive results in fooling classification and semantic segmentation models , obviating the need for hand-crafting attack methods for each task . using extensive experiments on challenging high-resolution datasets such as imagenet and cityscapes , we show that our perturbations achieve high fooling rates with small perturbation norms . moreover , our attacks are considerably faster than current iterative methods at inference time .", "topics": ["generative model"]}
{"title": "on the limitation of convolutional neural networks in recognizing negative images", "abstract": "convolutional neural networks ( cnns ) have achieved state-of-the-art performance on a variety of computer vision tasks , particularly visual classification problems , where new algorithms reported to achieve or even surpass the human performance . in this paper , we examine whether cnns are capable of learning the semantics of training data . to this end , we evaluate cnns on negative images , since they share the same structure and semantics as regular images and humans can classify them correctly . our experimental results indicate that when training on regular images and testing on negative images , the model accuracy is significantly lower than when it is tested on regular images . this leads us to the conjecture that current training methods do not effectively train models to generalize the concepts . we then introduce the notion of semantic adversarial examples - transformed inputs that semantically represent the same objects , but the model does not classify them correctly - and present negative images as one class of such inputs .", "topics": ["test set", "neural networks"]}
{"title": "deep image mining for diabetic retinopathy screening", "abstract": "deep learning is quickly becoming the leading methodology for medical image analysis . given a large medical archive , where each image is associated with a diagnosis , efficient pathology detectors or classifiers can be trained with virtually no expert knowledge about the target pathologies . however , deep learning algorithms , including the popular convnets , are black boxes : little is known about the local patterns analyzed by convnets to make a decision at the image level . a solution is proposed in this paper to create heatmaps showing which pixels in images play a role in the image-level predictions . in other words , a convnet trained for image-level classification can be used to detect lesions as well . a generalization of the backpropagation method is proposed in order to train convnets that produce high-quality heatmaps . the proposed solution is applied to diabetic retinopathy ( dr ) screening in a dataset of almost 90,000 fundus photographs from the 2015 kaggle diabetic retinopathy competition and a private dataset of almost 110,000 photographs ( e-ophtha ) . for the task of detecting referable dr , very good detection performance was achieved : $ a_z = 0.954 $ in kaggle 's dataset and $ a_z = 0.949 $ in e-ophtha . performance was also evaluated at the image level and at the lesion level in the diaretdb1 dataset , where four types of lesions are manually segmented : microaneurysms , hemorrhages , exudates and cotton-wool spots . the proposed detector outperforms recent algorithms trained to detect those lesions specifically , as well as competing heatmap generation algorithms for convnets . this detector is part of the messidor system for mobile eye pathology screening . because it does not rely on expert knowledge or manual segmentation for detecting relevant patterns , the proposed solution is a promising image mining tool , which has the potential to discover new biomarkers in images .", "topics": ["pixel"]}
{"title": "a formalism and an algorithm for computing pragmatic inferences and detecting infelicities", "abstract": "since austin introduced the term `` infelicity '' , the linguistic literature has been flooded with its use , but no formal or computational explanation has been given for it . this thesis provides one for those infelicities that occur when a pragmatic inference is cancelled . our contribution assumes the existence of a finer grained taxonomy with respect to pragmatic inferences . it is shown that if one wants to account for the natural language expressiveness , one should distinguish between pragmatic inferences that are felicitous to defeat and pragmatic inferences that are infelicitously defeasible . thus , it is shown that one should consider at least three types of information : indefeasible , felicitously defeasible , and infelicitously defeasible . the cancellation of the last of these determines the pragmatic infelicities . a new formalism has been devised to accommodate the three levels of information , called `` stratified logic '' . within it , we are able to express formally notions such as `` utterance u presupposes p '' or `` utterance u is infelicitous '' . special attention is paid to the implications that our work has in solving some well-known existential philosophical puzzles . the formalism yields an algorithm for computing interpretations for utterances , for determining their associated presuppositions , and for signalling infelicitous utterances that has been implemented in common lisp . the algorithm applies equally to simple and complex utterances and sequences of utterances .", "topics": ["natural language"]}
{"title": "an integrated , conditional model of information extraction and coreference with applications to citation matching", "abstract": "although information extraction and coreference resolution appear together in many applications , most current systems perform them as ndependent steps . this paper describes an approach to integrated inference for extraction and coreference based on conditionally-trained undirected graphical models . we discuss the advantages of conditional probability training , and of a coreference model structure based on graph partitioning . on a data set of research paper citations , we show significant reduction in error by using extraction uncertainty to improve coreference citation matching accuracy , and using coreference to improve the accuracy of the extracted fields .", "topics": ["graphical model"]}
{"title": "evaluation of machine learning techniques for green energy prediction", "abstract": "we evaluate the following machine learning techniques for green energy ( wind , solar ) prediction : bayesian inference , neural networks , support vector machines , clustering techniques ( pca ) . our objective is to predict green energy using weather forecasts , predict deviations from forecast green energy , find correlation amongst different weather parameters and green energy availability , recover lost or missing energy ( / weather ) data . we use historical weather data and weather forecasts for the same .", "topics": ["nonlinear system"]}
{"title": "a small universal petri net", "abstract": "a universal deterministic inhibitor petri net with 14 places , 29 transitions and 138 arcs was constructed via simulation of neary and woods ' weakly universal turing machine with 2 states and 4 symbols ; the total time complexity is exponential in the running time of their weak machine . to simulate the blank words of the weakly universal turing machine , a couple of dedicated transitions insert their codes when reaching edges of the working zone . to complete a chain of a given petri net encoding to be executed by the universal petri net , a translation of a bi-tag system into a turing machine was constructed . the constructed petri net is universal in the standard sense ; a weaker form of universality for petri nets was not introduced in this work .", "topics": ["time complexity", "simulation"]}
{"title": "a study of existing ontologies in the iot-domain", "abstract": "several domains have adopted the increasing use of iot-based devices to collect sensor data for generating abstractions and perceptions of the real world . this sensor data is multi-modal and heterogeneous in nature . this heterogeneity induces interoperability issues while developing cross-domain applications , thereby restricting the possibility of reusing sensor data to develop new applications . as a solution to this , semantic approaches have been proposed in the literature to tackle problems related to interoperability of sensor data . several ontologies have been proposed to handle different aspects of iot-based sensor data collection , ranging from discovering the iot sensors for data collection to applying reasoning on the collected sensor data for drawing inferences . in this paper , we survey these existing semantic ontologies to provide an overview of the recent developments in this field . we highlight the fundamental ontological concepts ( e.g . , sensor-capabilities and context-awareness ) required for an iot-based application , and survey the existing ontologies which include these concepts . based on our study , we also identify the shortcomings of currently available ontologies , which serves as a stepping stone to state the need for a common unified ontology for the iot domain .", "topics": ["sensor"]}
{"title": "rankme : reliable human ratings for natural language generation", "abstract": "human evaluation for natural language generation ( nlg ) often suffers from inconsistent user ratings . while previous research tends to attribute this problem to individual user preferences , we show that the quality of human judgements can also be improved by experimental design . we present a novel rank-based magnitude estimation method ( rankme ) , which combines the use of continuous scales and relative assessments . we show that rankme significantly improves the reliability and consistency of human ratings compared to traditional evaluation methods . in addition , we show that it is possible to evaluate nlg systems according to multiple , distinct criteria , which is important for error analysis . finally , we demonstrate that rankme , in combination with bayesian estimation of system quality , is a cost-effective alternative for ranking multiple nlg systems .", "topics": ["natural language"]}
{"title": "why has ( reasonably accurate ) automatic speech recognition been so hard to achieve ?", "abstract": "hidden markov models ( hmms ) have been successfully applied to automatic speech recognition for more than 35 years in spite of the fact that a key hmm assumption -- the statistical independence of frames -- is obviously violated by speech data . in fact , this data/model mismatch has inspired many attempts to modify or replace hmms with alternative models that are better able to take into account the statistical dependence of frames . however it is fair to say that in 2010 the hmm is the consensus model of choice for speech recognition and that hmms are at the heart of both commercially available products and contemporary research systems . in this paper we present a preliminary exploration aimed at understanding how speech data depart from hmms and what effect this departure has on the accuracy of hmm-based speech recognition . our analysis uses standard diagnostic tools from the field of statistics -- hypothesis testing , simulation and resampling -- which are rarely used in the field of speech recognition . our main result , obtained by novel manipulations of real and resampled data , demonstrates that real data have statistical dependency and that this dependency is responsible for significant numbers of recognition errors . we also demonstrate , using simulation and resampling , that if we `remove ' the statistical dependency from data , then the resulting recognition error rates become negligible . taken together , these results suggest that a better understanding of the structure of the statistical dependency in speech data is a crucial first step towards improving hmm-based speech recognition .", "topics": ["graphical model", "natural language"]}
{"title": "perceptron like algorithms for online learning to rank", "abstract": "perceptron is a classic online algorithm for learning a classification function . in this paper , we provide a novel extension of the perceptron algorithm to the learning to rank problem in information retrieval . we consider popular listwise performance measures such as normalized discounted cumulative gain ( ndcg ) and average precision ( ap ) . a modern perspective on perceptron for classification is that it is simply an instance of online gradient descent ( ogd ) , during mistake rounds , using the hinge loss function . motivated by this interpretation , we propose a novel family of listwise , large margin ranking surrogates . members of this family can be thought of as analogs of the hinge loss . exploiting a certain self-bounding property of the proposed family , we provide a guarantee on the cumulative ndcg ( or ap ) induced loss incurred by our perceptron-like algorithm . we show that , if there exists a perfect oracle ranker which can correctly rank each instance in an online sequence of ranking data , with some margin , the cumulative loss of perceptron algorithm on that sequence is bounded by a constant , irrespective of the length of the sequence . this result is reminiscent of novikoff 's convergence theorem for the classification perceptron . moreover , we prove a lower bound on the cumulative loss achievable by any deterministic algorithm , under the assumption of existence of perfect oracle ranker . the lower bound shows that our perceptron bound is not tight , and we propose another , \\emph { purely online } , algorithm which achieves the lower bound . we provide empirical results on simulated and large commercial datasets to corroborate our theoretical results .", "topics": ["loss function", "gradient descent"]}
{"title": "knapsack constrained contextual submodular list prediction with application to multi-document summarization", "abstract": "we study the problem of predicting a set or list of options under knapsack constraint . the quality of such lists are evaluated by a submodular reward function that measures both quality and diversity . similar to dagger ( ross et al . , 2010 ) , by a reduction to online learning , we show how to adapt two sequence prediction models to imitate greedy maximization under knapsack constraint problems : conseqopt ( dey et al . , 2012 ) and scp ( ross et al . , 2013 ) . experiments on extractive multi-document summarization show that our approach outperforms existing state-of-the-art methods .", "topics": ["reinforcement learning"]}
{"title": "sparse volterra and polynomial regression models : recoverability and estimation", "abstract": "volterra and polynomial regression models play a major role in nonlinear system identification and inference tasks . exciting applications ranging from neuroscience to genome-wide association analysis build on these models with the additional requirement of parsimony . this requirement has high interpretative value , but unfortunately can not be met by least-squares based or kernel regression methods . to this end , compressed sampling ( cs ) approaches , already successful in linear regression settings , can offer a viable alternative . the viability of cs for sparse volterra and polynomial models is the core theme of this work . a common sparse regression task is initially posed for the two models . building on ( weighted ) lasso-based schemes , an adaptive rls-type algorithm is developed for sparse polynomial regressions . the identifiability of polynomial models is critically challenged by dimensionality . however , following the cs principle , when these models are sparse , they could be recovered by far fewer measurements . to quantify the sufficient number of measurements for a given level of sparsity , restricted isometry properties ( rip ) are investigated in commonly met polynomial regression settings , generalizing known results for their linear counterparts . the merits of the novel ( weighted ) adaptive cs algorithms to sparse polynomial modeling are verified through synthetic as well as real data tests for genotype-phenotype analysis .", "topics": ["sampling ( signal processing )", "nonlinear system"]}
{"title": "knife : kernel iterative feature extraction", "abstract": "selecting important features in non-linear or kernel spaces is a difficult challenge in both classification and regression problems . when many of the features are irrelevant , kernel methods such as the support vector machine and kernel ridge regression can sometimes perform poorly . we propose weighting the features within a kernel with a sparse set of weights that are estimated in conjunction with the original classification or regression problem . the iterative algorithm , knife , alternates between finding the coefficients of the original problem and finding the feature weights through kernel linearization . in addition , a slight modification of knife yields an efficient algorithm for finding feature regularization paths , or the paths of each feature 's weight . simulation results demonstrate the utility of knife for both kernel regression and support vector machines with a variety of kernels . feature path realizations also reveal important non-linear correlations among features that prove useful in determining a subset of significant variables . results on vowel recognition data , parkinson 's disease data , and microarray data are also given .", "topics": ["kernel ( operating system )", "support vector machine"]}
{"title": "how would surround vehicles move ? a unified framework for maneuver classification and motion prediction", "abstract": "reliable prediction of surround vehicle motion is a critical requirement for path planning for autonomous vehicles . in this paper we propose a unified framework for surround vehicle maneuver classification and motion prediction that exploits multiple cues , namely , the estimated motion of vehicles , an understanding of typical motion patterns of freeway traffic and inter-vehicle interaction . we report our results in terms of maneuver classification accuracy and mean and median absolute error of predicted trajectories against the ground truth for real traffic data collected using vehicle mounted sensors on freeways . an ablative analysis is performed to analyze the relative importance of each cue for trajectory prediction . additionally , an analysis of execution time for the components of the framework is presented . finally , we present multiple case studies analyzing the outputs of our model for complex traffic scenarios", "topics": ["ground truth", "sensor"]}
{"title": "hindsight experience replay", "abstract": "dealing with sparse rewards is one of the biggest challenges in reinforcement learning ( rl ) . we present a novel technique called hindsight experience replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering . it can be combined with an arbitrary off-policy rl algorithm and may be seen as a form of implicit curriculum . we demonstrate our approach on the task of manipulating objects with a robotic arm . in particular , we run experiments on three different tasks : pushing , sliding , and pick-and-place , in each case using only binary rewards indicating whether or not the task is completed . our ablation studies show that hindsight experience replay is a crucial ingredient which makes training possible in these challenging environments . we show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task .", "topics": ["reinforcement learning", "simulation"]}
{"title": "multi-label zero-shot learning via concept embedding", "abstract": "zero shot learning ( zsl ) enables a learning model to classify instances of an unseen class during training . while most research in zsl focuses on single-label classification , few studies have been done in multi-label zsl , where an instance is associated with a set of labels simultaneously , due to the difficulty in modeling complex semantics conveyed by a set of labels . in this paper , we propose a novel approach to multi-label zsl via concept embedding learned from collections of public users ' annotations of multimedia . thanks to concept embedding , multi-label zsl can be done by efficiently mapping an instance input features onto the concept embedding space in a similar manner used in single-label zsl . moreover , our semantic learning model is capable of embedding an out-of-vocabulary label by inferring its meaning from its co-occurring labels . thus , our approach allows both seen and unseen labels during the concept embedding learning to be used in the aforementioned instance mapping , which makes multi-label zsl more flexible and suitable for real applications . experimental results of multi-label zsl on images and music tracks suggest that our approach outperforms a state-of-the-art multi-label zsl model and can deal with a scenario involving out-of-vocabulary labels without re-training the semantics learning model .", "topics": ["reinforcement learning"]}
{"title": "uncertainty estimation via stochastic batch normalization", "abstract": "in this work , we investigate batch normalization technique and propose its probabilistic interpretation . we propose a probabilistic model and show that batch normalization maximazes the lower bound of its marginalized log-likelihood . then , according to the new probabilistic model , we design an algorithm which acts consistently during train and test . however , inference becomes computationally inefficient . to reduce memory and computational cost , we propose stochastic batch normalization -- an efficient approximation of proper inference procedure . this method provides us with a scalable uncertainty estimation technique . we demonstrate the performance of stochastic batch normalization on popular architectures ( including deep convolutional architectures : vgg-like and resnets ) for mnist and cifar-10 datasets .", "topics": ["scalability", "mnist database"]}
{"title": "embedding semantic relations into word representations", "abstract": "learning representations for semantic relations is important for various tasks such as analogy detection , relational search , and relation classification . although there have been several proposals for learning representations for individual words , learning word representations that explicitly capture the semantic relations between words remains under developed . we propose an unsupervised method for learning vector representations for words such that the learnt representations are sensitive to the semantic relations that exist between two words . first , we extract lexical patterns from the co-occurrence contexts of two words in a corpus to represent the semantic relations that exist between those two words . second , we represent a lexical pattern as the weighted sum of the representations of the words that co-occur with that lexical pattern . third , we train a binary classifier to detect relationally similar vs. non-similar lexical pattern pairs . the proposed method is unsupervised in the sense that the lexical pattern pairs we use as train data are automatically sampled from a corpus , without requiring any manual intervention . our proposed method statistically significantly outperforms the current state-of-the-art word representations on three benchmark datasets for proportional analogy detection , demonstrating its ability to accurately capture the semantic relations among words .", "topics": ["unsupervised learning", "text corpus"]}
{"title": "towards the application of linear programming methods for multi-camera pose estimation", "abstract": "we presented a separation based optimization algorithm which , rather than optimization the entire variables altogether , this would allow us to employ : 1 ) a class of nonlinear functions with three variables and 2 ) a convex quadratic multivariable polynomial , for minimization of reprojection error . neglecting the inversion required to minimize the nonlinear functions , in this paper we demonstrate how separation allows eradication of matrix inversion .", "topics": ["nonlinear system", "polynomial"]}
{"title": "hashing image patches for zooming", "abstract": "in this paper we present a bayesian image zooming/super-resolution algorithm based on a patch based representation . we work on a patch based model with overlap and employ a locally linear embedding ( lle ) based approach as our data fidelity term in the bayesian inference . the image prior imposes continuity constraints across the overlapping patches . we apply an error back-projection technique , with an approximate cross bilateral filter . the problem of nearest neighbor search is handled by a variant of the locality sensitive hashing ( lsh ) scheme . the novelty of our work lies in the speed up achieved by the hashing scheme and the robustness and inherent modularity and parallel structure achieved by the lle setup . the ill-posedness of the image reconstruction problem is handled by the introduction of regularization priors which encode the knowledge present in vast collections of natural images . we present comparative results for both run-time as well as visual image quality based measurements .", "topics": ["approximation algorithm", "matrix regularization"]}
{"title": "real-world font recognition using deep network and domain adaptation", "abstract": "we address a challenging fine-grain classification problem : recognizing a font style from an image of text . in this task , it is very easy to generate lots of rendered font examples but very hard to obtain real-world labeled images . this real-to-synthetic domain gap caused poor generalization to new real data in previous methods ( chen et al . ( 2014 ) ) . in this paper , we refer to convolutional neural networks , and use an adaptation technique based on a stacked convolutional auto-encoder that exploits unlabeled real-world images combined with synthetic data . the proposed method achieves an accuracy of higher than 80 % ( top-5 ) on a real-world dataset .", "topics": ["synthetic data", "neural networks"]}
{"title": "action recognition using visual attention", "abstract": "we propose a soft attention based model for the task of action recognition in videos . we use multi-layered recurrent neural networks ( rnns ) with long short-term memory ( lstm ) units which are deep both spatially and temporally . our model learns to focus selectively on parts of the video frames and classifies videos after taking a few glimpses . the model essentially learns which parts in the frames are relevant for the task at hand and attaches higher importance to them . we evaluate the model on ucf-11 ( youtube action ) , hmdb-51 and hollywood2 datasets and analyze how the model focuses its attention depending on the scene and the action being performed .", "topics": ["recurrent neural network"]}
{"title": "solving asymmetric decision problems with influence diagrams", "abstract": "while influence diagrams have many advantages as a representation framework for bayesian decision problems , they have a serious drawback in handling asymmetric decision problems . to be represented in an influence diagram , an asymmetric decision problem must be symmetrized . a considerable amount of unnecessary computation may be involved when a symmetrized influence diagram is evaluated by conventional algorithms . in this paper we present an approach for avoiding such unnecessary computation in influence diagram evaluation .", "topics": ["computation"]}
{"title": "context-dependent feature analysis with random forests", "abstract": "in many cases , feature selection is often more complicated than identifying a single subset of input variables that would together explain the output . there may be interactions that depend on contextual information , i.e . , variables that reveal to be relevant only in some specific circumstances . in this setting , the contribution of this paper is to extend the random forest variable importances framework in order ( i ) to identify variables whose relevance is context-dependent and ( ii ) to characterize as precisely as possible the effect of contextual information on these variables . the usage and the relevance of our framework for highlighting context-dependent variables is illustrated on both artificial and real datasets .", "topics": ["interaction", "relevance"]}
{"title": "unsupervised domain adaptation for semantic segmentation with gans", "abstract": "visual domain adaptation is a problem of immense importance in computer vision . previous approaches showcase the inability of even deep neural networks to learn informative representations across domain shift . this problem is more severe for tasks where acquiring hand labeled data is extremely hard and tedious . in this work , we focus on adapting the representations learned by segmentation networks across synthetic and real domains . contrary to previous approaches that use a simple adversarial objective or superpixel information to aid the process , we propose an approach based on generative adversarial networks ( gans ) that brings the embeddings closer in the learned feature space . to showcase the generality and scalability of our approach , we show that we can achieve state of the art results on two challenging scenarios of synthetic to real domain adaptation . additional exploratory experiments show that our approach : ( 1 ) generalizes to unseen domains and ( 2 ) results in improved alignment of source and target distributions .", "topics": ["feature vector", "synthetic data"]}
{"title": "nonlinear information bottleneck", "abstract": "information bottleneck [ ib ] is a technique for extracting information in some `input ' random variable that is relevant for predicting some different 'output ' random variable . ib works by encoding the input in a compressed 'bottleneck variable ' from which the output can then be accurately decoded . ib can be difficult to compute in practice , and has been mainly developed for two limited cases : ( 1 ) discrete random variables with small state spaces , and ( 2 ) continuous random variables that are jointly gaussian distributed ( in which case the encoding and decoding maps are linear ) . we propose a method to perform ib in more general domains . our approach can be applied to discrete or continuous inputs and outputs , and allows for nonlinear encoding and decoding maps . the method uses a novel upper bound on the ib objective , derived using a non-parametric estimator of mutual information and a variational approximation . we show how to implement the method using neural networks and gradient-based optimization , and demonstrate its performance on the mnist dataset .", "topics": ["calculus of variations", "nonlinear system"]}
{"title": "bayesian modeling of motion perception using dynamical stochastic textures", "abstract": "a common practice to account for psychophysical biases in vision is to frame them as consequences of a dynamic process relying on optimal inference with respect to a generative model . the present study details the complete formulation of such a generative model intended to probe visual motion perception . it is first derived in a set of axiomatic steps constrained by biological plausibility . we then extend previous contributions by detailing three equivalent formulations of the gaussian dynamic texture model . first , the composite dynamic textures are constructed by the random aggregation of warped patterns , which can be viewed as 3d gaussian fields . second , these textures are cast as solutions to a stochastic partial differential equation ( spde ) . this essential step enables real time , on-the-fly , texture synthesis using time-discretized auto- regressive processes . it also allows for the derivation of a local motion-energy model , which corresponds to the log-likelihood of the probability density . the log-likelihoods are finally essential for the construction of a bayesian inference framework . we use the model to probe speed perception in humans psychophysically using zoom-like changes in stimulus spatial frequency content . the likelihood is contained within the genera- tive model and we chose a slow speed prior consistent with previous literature . we then validated the fitting process of the model using synthesized data . the human data replicates previous findings that relative perceived speed is positively biased by spatial frequency increments . the effect can not be fully accounted for by previous models , but the current prior acting on the spatio-temporal likelihoods has proved necessary in accounting for the perceptual bias .", "topics": ["generative model"]}
{"title": "imitation learning with a value-based prior", "abstract": "the goal of imitation learning is for an apprentice to learn how to behave in a stochastic environment by observing a mentor demonstrating the correct behavior . accurate prior knowledge about the correct behavior can reduce the need for demonstrations from the mentor . we present a novel approach to encoding prior knowledge about the correct behavior , where we assume that this prior knowledge takes the form of a markov decision process ( mdp ) that is used by the apprentice as a rough and imperfect model of the mentor 's behavior . specifically , taking a bayesian approach , we treat the value of a policy in this modeling mdp as the log prior probability of the policy . in other words , we assume a priori that the mentor 's behavior is likely to be a high value policy in the modeling mdp , though quite possibly different from the optimal policy . we describe an efficient algorithm that , given a modeling mdp and a set of demonstrations by a mentor , provably converges to a stationary point of the log posterior of the mentor 's policy , where the posterior is computed with respect to the `` value based '' prior . we also present empirical evidence that this prior does in fact speed learning of the mentor 's policy , and is an improvement in our experiments over similar previous methods .", "topics": ["markov chain"]}
{"title": "photographic dataset : playing cards", "abstract": "this is a photographic dataset collected for testing image processing algorithms . the idea is to have images that can exploit the properties of total variation , therefore a set of playing cards was distributed on the scene . the dataset is made available at www.fips.fi/photographic_dataset2.php", "topics": ["image processing"]}
{"title": "modeling the sequence of brain volumes by local mesh models for brain decoding", "abstract": "we represent the sequence of fmri ( functional magnetic resonance imaging ) brain volumes recorded during a cognitive stimulus by a graph which consists of a set of local meshes . the corresponding cognitive process , encoded in the brain , is then represented by these meshes each of which is estimated assuming a linear relationship among the voxel time series in a predefined locality . first , we define the concept of locality in two neighborhood systems , namely , the spatial and functional neighborhoods . then , we construct spatially and functionally local meshes around each voxel , called seed voxel , by connecting it either to its spatial or functional p-nearest neighbors . the mesh formed around a voxel is a directed sub-graph with a star topology , where the direction of the edges is taken towards the seed voxel at the center of the mesh . we represent the time series recorded at each seed voxel in terms of linear combination of the time series of its p-nearest neighbors in the mesh . the relationships between a seed voxel and its neighbors are represented by the edge weights of each mesh , and are estimated by solving a linear regression equation . the estimated mesh edge weights lead to a better representation of information in the brain for encoding and decoding of the cognitive tasks . we test our model on a visual object recognition and emotional memory retrieval experiments using support vector machines that are trained using the mesh edge weights as features . in the experimental analysis , we observe that the edge weights of the spatial and functional meshes perform better than the state-of-the-art brain decoding models .", "topics": ["time series", "support vector machine"]}
{"title": "gemini : a natural language system for spoken-language understanding", "abstract": "gemini is a natural language understanding system developed for spoken language applications . the paper describes the architecture of gemini , paying particular attention to resolving the tension between robustness and overgeneration . gemini features a broad-coverage unification-based grammar of english , fully interleaved syntactic and semantic processing in an all-paths , bottom-up parser , and an utterance-level parser to find interpretations of sentences that might not be analyzable as complete sentences . gemini also includes novel components for recognizing and correcting grammatical disfluencies , and for doing parse preferences . this paper presents a component-by-component view of gemini , providing detailed relevant measurements of size , efficiency , and performance .", "topics": ["natural language", "parsing"]}
{"title": "fast randomized model generation for shapelet-based time series classification", "abstract": "time series classification is a field which has drawn much attention over the past decade . a new approach for classification of time series uses classification trees based on shapelets . a shapelet is a subsequence extracted from one of the time series in the dataset . a disadvantage of this approach is the time required for building the shapelet-based classification tree . the search for the best shapelet requires examining all subsequences of all lengths from all time series in the training set . a key goal of this work was to find an evaluation order of the shapelets space which enables fast convergence to an accurate model . the comparative analysis we conducted clearly indicates that a random evaluation order yields the best results . our empirical analysis of the distribution of high-quality shapelets within the shapelets space provides insights into why randomized shapelets sampling is superior to alternative evaluation orders . we present an algorithm for randomized model generation for shapelet-based classification that converges extremely quickly to a model with surprisingly high accuracy after evaluating only an exceedingly small fraction of the shapelets space .", "topics": ["sampling ( signal processing )", "time series"]}
{"title": "active mining of parallel video streams", "abstract": "the practicality of a video surveillance system is adversely limited by the amount of queries that can be placed on human resources and their vigilance in response . to transcend this limitation , a major effort under way is to include software that ( fully or at least semi ) automatically mines video footage , reducing the burden imposed to the system . herein , we propose a semi-supervised incremental learning framework for evolving visual streams in order to develop a robust and flexible track classification system . our proposed method learns from consecutive batches by updating an ensemble in each time . it tries to strike a balance between performance of the system and amount of data which needs to be labelled . as no restriction is considered , the system can address many practical problems in an evolving multi-camera scenario , such as concept drift , class evolution and various length of video streams which have not been addressed before . experiments were performed on synthetic as well as real-world visual data in non-stationary environments , showing high accuracy with fairly little human collaboration .", "topics": ["synthetic data"]}
{"title": "a sheaf model of contradictions and disagreements . preliminary report and discussion", "abstract": "we introduce a new formal model -- based on the mathematical construct of sheaves -- for representing contradictory information in textual sources . this model has the advantage of letting us ( a ) identify the causes of the inconsistency ; ( b ) measure how strong it is ; ( c ) and do something about it , e.g . suggest ways to reconcile inconsistent advice . this model naturally represents the distinction between contradictions and disagreements . it is based on the idea of representing natural language sentences as formulas with parameters sitting on lattices , creating partial orders based on predicates shared by theories , and building sheaves on these partial orders with products of lattices as stalks . degrees of disagreement are measured by the existence of global and local sections . limitations of the sheaf approach and connections to recent work in natural language processing , as well as the topics of contextuality in physics , data fusion , topological data analysis and epistemology are also discussed .", "topics": ["natural language processing", "natural language"]}
{"title": "uncovering latent style factors for expressive speech synthesis", "abstract": "prosodic modeling is a core problem in speech synthesis . the key challenge is producing desirable prosody from textual input containing only phonetic information . in this preliminary study , we introduce the concept of `` style tokens '' in tacotron , a recently proposed end-to-end neural speech synthesis model . using style tokens , we aim to extract independent prosodic styles from training data . we show that without annotation data or an explicit supervision signal , our approach can automatically learn a variety of prosodic variations in a purely data-driven way . importantly , each style token corresponds to a fixed style factor regardless of the given text sequence . as a result , we can control the prosodic style of synthetic speech in a somewhat predictable and globally consistent way .", "topics": ["test set", "synthetic data"]}
{"title": "next generation business intelligence and analytics : a survey", "abstract": "business intelligence and analytics ( bi & a ) is the process of extracting and predicting business-critical insights from data . traditional bi focused on data collection , extraction , and organization to enable efficient query processing for deriving insights from historical data . with the rise of big data and cloud computing , there are many challenges and opportunities for the bi . especially with the growing number of data sources , traditional bi\\ & a are evolving to provide intelligence at different scales and perspectives - operational bi , situational bi , self-service bi . in this survey , we review the evolution of business intelligence systems in full scale from back-end architecture to and front-end applications . we focus on the changes in the back-end architecture that deals with the collection and organization of the data . we also review the changes in the front-end applications , where analytic services and visualization are the core components . using a uses case from bi in healthcare , which is one of the most complex enterprises , we show how bi\\ & a will play an important role beyond the traditional usage . the survey provides a holistic view of business intelligence and analytics for anyone interested in getting a complete picture of the different pieces in the emerging next generation bi\\ & a solutions .", "topics": ["database"]}
{"title": "unbiased online recurrent optimization", "abstract": "the novel unbiased online recurrent optimization ( uoro ) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models . it works in a streaming fashion and avoids backtracking through past activations and inputs . uoro is computationally as costly as truncated backpropagation through time ( truncated bptt ) , a widespread algorithm for online learning of recurrent networks . uoro is a modification of nobacktrack that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks , even for complex models . like nobacktrack , uoro provides unbiased gradient estimates ; unbiasedness is the core hypothesis in stochastic gradient descent theory , without which convergence to a local optimum is not guaranteed . on the contrary , truncated bptt does not provide this property , leading to possible divergence . on synthetic tasks where truncated bptt is shown to diverge , uoro converges . for instance , when a parameter has a positive short-term but negative long-term influence , truncated bptt diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions , while uoro performs well thanks to the unbiasedness of its gradients .", "topics": ["recurrent neural network", "gradient descent"]}
{"title": "personal model training under privacy constraints", "abstract": "many current internet services rely on inferences from models trained on user data . commonly , both the training and inference tasks are carried out using cloud resources fed by personal data collected at scale from users . holding and using such large collections of personal data in the cloud creates privacy risks to the data subjects , but is currently required for users to benefit from such services . we explore how to provide for model training and inference in a system where computation is moved to the data in preference to moving data to the cloud , obviating many current privacy risks . specifically , we take an initial model learnt from a small set of users and retrain it locally using data from a single user . we evaluate on two tasks : one supervised learning task , using a neural network to recognise users ' current activity from accelerometer traces ; and one unsupervised learning task , identifying topics in a large set of documents . in both cases the accuracy is improved . we also demonstrate the robustness of our approach against adversarial attacks , as well as its feasibility by presenting a performance evaluation on a representative resource-constrained device ( a raspberry pi ) .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "actor-critic versus direct policy search : a comparison based on sample complexity", "abstract": "sample efficiency is a critical property when optimizing policy parameters for the controller of a robot . in this paper , we evaluate two state-of-the-art policy optimization algorithms . one is a recent deep reinforcement learning method based on an actor-critic algorithm , deep deterministic policy gradient ( ddpg ) , that has been shown to perform well on various control benchmarks . the other one is a direct policy search method , covariance matrix adaptation evolution strategy ( cma-es ) , a black-box optimization method that is widely used for robot learning . the algorithms are evaluated on a continuous version of the mountain car benchmark problem , so as to compare their sample complexity . from a preliminary analysis , we expect ddpg to be more sample efficient than cma-es , which is confirmed by our experimental results .", "topics": ["reinforcement learning", "gradient"]}
{"title": "non-line-of-sight tracking of people at long range", "abstract": "a remote-sensing system that can determine the position of hidden objects has applications in many critical real-life scenarios , such as search and rescue missions and safe autonomous driving . previous work has shown the ability to range and image objects hidden from the direct line of sight , employing advanced optical imaging technologies aimed at small objects at short range . in this work we demonstrate a long-range tracking system based on single laser illumination and single-pixel single-photon detection . this enables us to track one or more people hidden from view at a stand-off distance of over 50~m . these results pave the way towards next generation lidar systems that will reconstruct not only the direct-view scene but also the main elements hidden behind walls or corners .", "topics": ["autonomous car", "pixel"]}
{"title": "distributed flexible nonlinear tensor factorization", "abstract": "tensor factorization is a powerful tool to analyse multi-way data . compared with traditional multi-linear methods , nonlinear tensor factorization models are capable of capturing more complex relationships in the data . however , they are computationally expensive and may suffer severe learning bias in case of extreme data sparsity . to overcome these limitations , in this paper we propose a distributed , flexible nonlinear tensor factorization model . our model can effectively avoid the expensive computations and structural restrictions of the kronecker-product in existing tgp formulations , allowing an arbitrary subset of tensorial entries to be selected to contribute to the training . at the same time , we derive a tractable and tight variational evidence lower bound ( elbo ) that enables highly decoupled , parallel computations and high-quality inference . based on the new bound , we develop a distributed inference algorithm in the mapreduce framework , which is key-value-free and can fully exploit the memory cache mechanism in fast mapreduce systems such as spark . experimental results fully demonstrate the advantages of our method over several state-of-the-art approaches , in terms of both predictive performance and computational efficiency . moreover , our approach shows a promising potential in the application of click-through-rate ( ctr ) prediction for online advertising .", "topics": ["calculus of variations", "nonlinear system"]}
{"title": "ibsead : - a self-evolving self-obsessed learning algorithm for machine learning", "abstract": "we present ibsead or distributed autonomous entity systems based interaction - a learning algorithm for the computer to self-evolve in a self-obsessed manner . this learning algorithm will present the computer to look at the internal and external environment in series of independent entities , which will interact with each other , with and/or without knowledge of the computer 's brain . when a learning algorithm interacts , it does so by detecting and understanding the entities in the human algorithm . however , the problem with this approach is that the algorithm does not consider the interaction of the third party or unknown entities , which may be interacting with each other . these unknown entities in their interaction with the non-computer entities make an effect in the environment that influences the information and the behaviour of the computer brain . such details and the ability to process the dynamic and unsettling nature of these interactions are absent in the current learning algorithm such as the decision tree learning algorithm . ibsead is able to evaluate and consider such algorithms and thus give us a better accuracy in simulation of the highly evolved nature of the human brain . processes such as dreams , imagination and novelty , that exist in humans are not fully simulated by the existing learning algorithms . also , hidden markov models ( hmm ) are useful in finding `` hidden '' entities , which may be known or unknown . however , this model fails to consider the case of unknown entities which maybe unclear or unknown . ibsead is better because it considers three types of entities- known , unknown and invisible . we present our case with a comparison of existing algorithms in known environments and cases and present the results of the experiments using dry run of the simulated runs of the existing machine learning algorithms versus ibsead .", "topics": ["interaction", "simulation"]}
{"title": "inverse signal classification for financial instruments", "abstract": "the paper presents new machine learning methods : signal composition , which classifies time-series regardless of length , type , and quantity ; and self-labeling , a supervised-learning enhancement . the paper describes further the implementation of the methods on a financial search engine system using a collection of 7,881 financial instruments traded during 2011 to identify inverse behavior among the time-series .", "topics": ["supervised learning", "time series"]}
{"title": "a new belief markov chain model and its application in inventory prediction", "abstract": "markov chain model is widely applied in many fields , especially the field of prediction . the classical discrete-time markov chain ( dtmc ) is a widely used method for prediction . however , the classical dtmc model has some limitation when the system is complex with uncertain information or state space is not discrete . to address it , a new belief markov chain model is proposed by combining dempster-shafer evidence theory with markov chain . in our model , the uncertain data is allowed to be handle in the form of interval number and the basic probability assignment ( bpa ) is generated based on the distance between interval numbers . the new belief markov chain model overcomes the shortcomings of classical markov chain and has an efficient ability in dealing with uncertain information . moreover , an example of inventory prediction and the comparison between our model and classical dtmc model can show the effectiveness and rationality of our proposed model .", "topics": ["markov chain"]}
{"title": "geometric enclosing networks", "abstract": "training model to generate data has increasingly attracted research attention and become important in modern world applications . we propose in this paper a new geometry-based optimization approach to address this problem . orthogonal to current state-of-the-art density-based approaches , most notably vae and gan , we present a fresh new idea that borrows the principle of minimal enclosing ball to train a generator g\\left ( \\bz\\right ) in such a way that both training and generated data , after being mapped to the feature space , are enclosed in the same sphere . we develop theory to guarantee that the mapping is bijective so that its inverse from feature space to data space results in expressive nonlinear contours to describe the data manifold , hence ensuring data generated are also lying on the data manifold learned from training data . our model enjoys a nice geometric interpretation , hence termed geometric enclosing networks ( gen ) , and possesses some key advantages over its rivals , namely simple and easy-to-control optimization formulation , avoidance of mode collapsing and efficiently learn data manifold representation in a completely unsupervised manner . we conducted extensive experiments on synthesis and real-world datasets to illustrate the behaviors , strength and weakness of our proposed gen , in particular its ability to handle multi-modal data and quality of generated data .", "topics": ["test set", "feature vector"]}
{"title": "detecting cancer metastases on gigapixel pathology images", "abstract": "each year , the treatment decisions for more than 230,000 breast cancer patients in the u.s. hinge on whether the cancer has metastasized away from the breast . metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues . this process is labor intensive and error-prone . we present a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x 100,000 pixels . our method leverages a convolutional neural network ( cnn ) architecture and obtains state-of-the-art results on the camelyon16 dataset in the challenging lesion-level tumor detection task . at 8 false positives per image , we detect 92.4 % of the tumors , relative to 82.7 % by the previous best automated approach . for comparison , a human pathologist attempting exhaustive search achieved 73.2 % sensitivity . we achieve image-level auc scores above 97 % on both the camelyon16 test set and an independent set of 110 slides . in addition , we discover that two slides in the camelyon16 training set were erroneously labeled normal . our approach could considerably reduce false negative rates in metastasis detection .", "topics": ["test set", "pixel"]}
{"title": "a corpus of deep argumentative structures as an explanation to argumentative relations", "abstract": "in this paper , we compose a new task for deep argumentative structure analysis that goes beyond shallow discourse structure analysis . the idea is that argumentative relations can reasonably be represented with a small set of predefined patterns . for example , using value judgment and bipolar causality , we can explain a support relation between two argumentative segments as follows : segment 1 states that something is good , and segment 2 states that it is good because it promotes something good when it happens . we are motivated by the following questions : ( i ) how do we formulate the task ? , ( ii ) can a reasonable pattern set be created ? , and ( iii ) do the patterns work ? to examine the task feasibility , we conduct a three-stage , detailed annotation study using 357 argumentative relations from the argumentative microtext corpus , a small , but highly reliable corpus . we report the coverage of explanations captured by our patterns on a test set composed of 270 relations . our coverage result of 74.6 % indicates that argumentative relations can reasonably be explained by our small pattern set . our agreement result of 85.9 % shows that a reasonable inter-annotator agreement can be achieved . to assist with future work in computational argumentation , the annotated corpus is made publicly available .", "topics": ["test set", "causality"]}
{"title": "building a conversational agent overnight with dialogue self-play", "abstract": "we propose machines talking to machines ( m2m ) , a framework combining automation and crowdsourcing to rapidly bootstrap end-to-end dialogue agents for goal-oriented dialogues in arbitrary domains . m2m scales to new tasks with just a task schema and an api client from the dialogue system developer , but it is also customizable to cater to task-specific interactions . compared to the wizard-of-oz approach for data collection , m2m achieves greater diversity and coverage of salient dialogue flows while maintaining the naturalness of individual utterances . in the first phase , a simulated user bot and a domain-agnostic system bot converse to exhaustively generate dialogue `` outlines '' , i.e . sequences of template utterances and their semantic parses . in the second phase , crowd workers provide contextual rewrites of the dialogues to make the utterances more natural while preserving their meaning . the entire process can finish within a few hours . we propose a new corpus of 3,000 dialogues spanning 2 domains collected with m2m , and present comparisons with popular dialogue datasets on the quality and diversity of the surface forms and dialogue flows .", "topics": ["simulation"]}
{"title": "polyphonic music generation by modeling temporal dependencies using a rnn-dbn", "abstract": "in this paper , we propose a generic technique to model temporal dependencies and sequences using a combination of a recurrent neural network and a deep belief network . our technique , rnn-dbn , is an amalgamation of the memory state of the rnn that allows it to provide temporal information and a multi-layer dbn that helps in high level representation of the data . this makes rnn-dbns ideal for sequence generation . further , the use of a dbn in conjunction with the rnn makes this model capable of significantly more complex data representation than an rbm . we apply this technique to the task of polyphonic music generation .", "topics": ["recurrent neural network"]}
{"title": "a fully automated latent fingerprint matcher with embedded self-learning segmentation module", "abstract": "latent fingerprint has the practical value to identify the suspects who have unintentionally left a trace of fingerprint in the crime scenes . however , designing a fully automated latent fingerprint matcher is a very challenging task as it needs to address many challenging issues including the separation of overlapping structured patterns over the partial and poor quality latent fingerprint image , and finding a match against a large background database that would have different resolutions . currently there is no fully automated latent fingerprint matcher available to the public and most literature reports have utilized a specialized latent fingerprint matcher cots3 which is not accessible to the public . this will make it infeasible to assess and compare the relevant research work which is vital for this research community . in this study , we target to develop a fully automated latent matcher for adaptive detection of the region of interest and robust matching of latent prints . unlike the manually conducted matching procedure , the proposed latent matcher can run like a sealed black box without any manual intervention . this matcher consists of the following two modules : ( i ) the dictionary learning-based region of interest ( roi ) segmentation scheme ; and ( ii ) the genetic algorithm-based minutiae set matching unit . experimental results on nist sd27 latent fingerprint database demonstrates that the proposed matcher outperforms the currently public state-of-art latent fingerprint matcher .", "topics": ["dictionary"]}
{"title": "deep learning : a critical appraisal", "abstract": "although deep learning has historical roots going back decades , neither the term `` deep learning '' nor the approach was popular just over five years ago , when the field was reignited by papers such as krizhevsky , sutskever and hinton 's now classic ( 2012 ) deep network model of imagenet . what has the field discovered in the five subsequent years ? against a background of considerable progress in areas such as speech recognition , image recognition , and game playing , and considerable enthusiasm in the popular press , i present ten concerns for deep learning , and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence .", "topics": ["computer vision", "speech recognition"]}
{"title": "techniques for proving asynchronous convergence results for markov chain monte carlo methods", "abstract": "markov chain monte carlo ( mcmc ) methods such as gibbs sampling are finding widespread use in applied statistics and machine learning . these often lead to difficult computational problems , which are increasingly being solved on parallel and distributed systems such as compute clusters . recent work has proposed running iterative algorithms such as gradient descent and mcmc in parallel asynchronously for increased performance , with good empirical results in certain problems . unfortunately , for mcmc this parallelization technique requires new convergence theory , as it has been explicitly demonstrated to lead to divergence on some examples . recent theory on asynchronous gibbs sampling describes why these algorithms can fail , and provides a way to alter them to make them converge . in this article , we describe how to apply this theory in a generic setting , to understand the asynchronous behavior of any mcmc algorithm , including those implemented using parameter servers , and those not based on gibbs sampling .", "topics": ["sampling ( signal processing )", "gradient descent"]}
{"title": "incentivizing exploration in reinforcement learning with deep predictive models", "abstract": "achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning . while bayesian and pac-mdp approaches to the exploration problem offer strong formal guarantees , they are often impractical in higher dimensions due to their reliance on enumerating the state-action space . hence , exploration in complex domains is often performed with simple epsilon-greedy methods . in this paper , we consider the challenging atari games domain , which requires processing raw pixel inputs and delayed rewards . we evaluate several more sophisticated exploration strategies , including thompson sampling and boltzman exploration , and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics . by parameterizing our learned model with a neural network , we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex , high-dimensional state spaces . in the atari domain , our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods . in addition to raw game-scores , we also develop an auc-100 metric for the atari learning domain to evaluate the impact of exploration on this benchmark .", "topics": ["reinforcement learning", "scalability"]}
{"title": "using discretization for extending the set of predictive features", "abstract": "to date , attribute discretization is typically performed by replacing the original set of continuous features with a transposed set of discrete ones . this paper provides support for a new idea that discretized features should often be used in addition to existing features and as such , datasets should be extended , and not replaced , by discretization . we also claim that discretization algorithms should be developed with the explicit purpose of enriching a non-discretized dataset with discretized values . we present such an algorithm , d-miat , a supervised algorithm that discretizes data based on minority interesting attribute thresholds . d-miat only generates new features when strong indications exist for one of the target values needing to be learned and thus is intended to be used in addition to the original data . we present extensive empirical results demonstrating the success of using d-miat on $ 28 $ benchmark datasets . we also demonstrate that $ 10 $ other discretization algorithms can also be used to generate features that yield improved performance when used in combination with the original non-discretized data . our results show that the best predictive performance is attained using a combination of the original dataset with added features from a `` standard '' supervised discretization algorithm and d-miat .", "topics": ["value ( ethics )"]}
{"title": "visual tracking with similarity matching ratio", "abstract": "this paper presents a novel approach to visual tracking : similarity matching ratio ( smr ) . the traditional approach of tracking is minimizing some measures of the difference between the template and a patch from the frame . this approach is vulnerable to outliers and drastic appearance changes and an extensive study is focusing on making the approach more tolerant to them . however , this often results in longer , corrective algo- rithms which do not solve the original problem . this paper proposes a novel approach to the definition of the tracking problems , smr , which turns the differences into a probability measure . only pixel differences below a threshold count towards deciding the match , the rest are ignored . this approach makes the smr tracker robust to outliers and points that dramaticaly change appearance . the smr tracker is tested on challenging video sequences and achieved state-of-the-art performance .", "topics": ["pixel"]}
{"title": "emotion analysis platform on chinese microblog", "abstract": "weibo , as the largest social media service in china , has billions of messages generated every day . the huge number of messages contain rich sentimental information . in order to analyze the emotional changes in accordance with time and space , this paper presents an emotion analysis platform ( eap ) , which explores the emotional distribution of each province , so that can monitor the global pulse of each province in china . the massive data of weibo and the real-time requirements make the building of eap challenging . in order to solve the above problems , emoticons , emotion lexicon and emotion-shifting rules are adopted in eap to analyze the emotion of each tweet . in order to verify the effectiveness of the platform , case study on the sichuan earthquake is done , and the analysis result of the platform accords with the fact . in order to analyze from quantity , we manually annotate a test set and conduct experiment on it . the experimental results show that the macro-precision of eap reaches 80 % and the eap works effectively .", "topics": ["test set"]}
{"title": "time series prediction for graphs in kernel and dissimilarity spaces", "abstract": "graph models are relevant in many fields , such as distributed computing , intelligent tutoring systems or social network analysis . in many cases , such models need to take changes in the graph structure into account , i.e . a varying number of nodes or edges . predicting such changes within graphs can be expected to yield important insight with respect to the underlying dynamics , e.g . with respect to user behaviour . however , predictive techniques in the past have almost exclusively focused on single edges or nodes . in this contribution , we attempt to predict the future state of a graph as a whole . we propose to phrase time series prediction as a regression problem and apply dissimilarity- or kernel-based regression techniques , such as 1-nearest neighbor , kernel regression and gaussian process regression , which can be applied to graphs via graph kernels . the output of the regression is a point embedded in a pseudo-euclidean space , which can be analyzed using subsequent dissimilarity- or kernel-based processing methods . we discuss strategies to speed up gaussian processes regression from cubic to linear time and evaluate our approach on two well-established theoretical models of graph evolution as well as two real data sets from the domain of intelligent tutoring systems . we find that simple regression methods , such as kernel regression , are sufficient to capture the dynamics in the theoretical models , but that gaussian process regression significantly improves the prediction error for real-world data .", "topics": ["kernel ( operating system )", "time series"]}
{"title": "latent variable discovery using dependency patterns", "abstract": "the causal discovery of bayesian networks is an active and important research area , and it is based upon searching the space of causal models for those which can best explain a pattern of probabilistic dependencies shown in the data . however , some of those dependencies are generated by causal structures involving variables which have not been measured , i.e . , latent variables . some such patterns of dependency `` reveal '' themselves , in that no model based solely upon the observed variables can explain them as well as a model using a latent variable . that is what latent variable discovery is based upon . here we did a search for finding them systematically , so that they may be applied in latent variable discovery in a more rigorous fashion .", "topics": ["bayesian network", "causality"]}
{"title": "the peerrank method for peer assessment", "abstract": "we propose the peerrank method for peer assessment . this constructs a grade for an agent based on the grades proposed by the agents evaluating the agent . since the grade of an agent is a measure of their ability to grade correctly , the peerrank method weights grades by the grades of the grading agent . the peerrank method also provides an incentive for agents to grade correctly . as the grades of an agent depend on the grades of the grading agents , and as these grades themselves depend on the grades of other agents , we define the peerrank method by a fixed point equation similar to the pagerank method for ranking web-pages . we identify some formal properties of the peerrank method ( for example , it satisfies axioms of unanimity , no dummy , no discrimination and symmetry ) , discuss some examples , compare with related work and evaluate the performance on some synthetic data . our results show considerable promise , reducing the error in grade predictions by a factor of 2 or more in many cases over the natural baseline of averaging peer grades .", "topics": ["baseline ( configuration management )", "synthetic data"]}
{"title": "a physical metaphor to study semantic drift", "abstract": "in accessibility tests for digital preservation , over time we experience drifts of localized and labelled content in statistical models of evolving semantics represented as a vector field . this articulates the need to detect , measure , interpret and model outcomes of knowledge dynamics . to this end we employ a high-performance machine learning algorithm for the training of extremely large emergent self-organizing maps for exploratory data analysis . the working hypothesis we present here is that the dynamics of semantic drifts can be modeled on a relaxed version of newtonian mechanics called social mechanics . by using term distances as a measure of semantic relatedness vs . their pagerank values indicating social importance and applied as variable `term mass ' , gravitation as a metaphor to express changes in the semantic content of a vector field lends a new perspective for experimentation . from `term gravitation ' over time , one can compute its generating potential whose fluctuations manifest modifications in pairwise term similarity vs. social importance , thereby updating osgood 's semantic differential . the dataset examined is the public catalog metadata of tate galleries , london .", "topics": ["value ( ethics )", "map"]}
{"title": "bargaining for revenue shares on tree trading networks", "abstract": "we study trade networks with a tree structure , where a seller with a single indivisible good is connected to buyers , each with some value for the good , via a unique path of intermediaries . agents in the tree make multiplicative revenue share offers to their parent nodes , who choose the best offer and offer part of it to their parent , and so on ; the winning path is determined by who finally makes the highest offer to the seller . in this paper , we investigate how these revenue shares might be set via a natural bargaining process between agents on the tree , specifically , egalitarian bargaining between endpoints of each edge in the tree . we investigate the fixed point of this system of bargaining equations and prove various desirable for this solution concept , including ( i ) existence , ( ii ) uniqueness , ( iii ) efficiency , ( iv ) membership in the core , ( v ) strict monotonicity , ( vi ) polynomial-time computability to any given accuracy . finally , we present numerical evidence that asynchronous dynamics with randomly ordered updates always converges to the fixed point , indicating that the fixed point shares might arise from decentralized bargaining amongst agents on the trade network .", "topics": ["time complexity", "numerical analysis"]}
{"title": "semantic instance segmentation via deep metric learning", "abstract": "we propose a new method for semantic instance segmentation , by first computing how likely two pixels are to belong to the same object , and then by grouping similar pixels together . our similarity metric is based on a deep , fully convolutional embedding model . our grouping method is based on selecting all points that are sufficiently similar to a set of `` seed points '' , chosen from a deep , fully convolutional scoring model . we show competitive results on the pascal voc instance segmentation benchmark .", "topics": ["pixel"]}
{"title": "keyvec : key-semantics preserving document representations", "abstract": "previous studies have demonstrated the empirical success of word embeddings in various applications . in this paper , we investigate the problem of learning distributed representations for text documents which many machine learning algorithms take as input for a number of nlp tasks . we propose a neural network model , keyvec , which learns document representations with the goal of preserving key semantics of the input text . it enables the learned low-dimensional vectors to retain the topics and important information from the documents that will flow to downstream tasks . our empirical evaluations show the superior quality of keyvec representations in two different document understanding tasks .", "topics": ["natural language processing"]}
{"title": "animation and chirplet-based development of a pir sensor array for intruder classification in an outdoor environment", "abstract": "this paper presents the development of a passive infra-red sensor tower platform along with a classification algorithm to distinguish between human intrusion , animal intrusion and clutter arising from wind-blown vegetative movement in an outdoor environment . the research was aimed at exploring the potential use of wireless sensor networks as an early-warning system to help mitigate human-wildlife conflicts occurring at the edge of a forest . there are three important features to the development . firstly , the sensor platform employs multiple sensors arranged in the form of a two-dimensional array to give it a key spatial-resolution capability that aids in classification . secondly , given the challenges of collecting data involving animal intrusion , an animation-based simulation tool for passive infra-red sensor ( aspire ) was developed that simulates signals corresponding to human and animal intrusion and some limited models of vegetative clutter . this speeded up the process of algorithm development by allowing us to test different hypotheses in a time-efficient manner . finally , a chirplet-based model for intruder signal was developed that significantly helped boost classification accuracy despite drawing data from a smaller number of sensors . an svm-based classifier was used which made use of chirplet , energy and signal cross-correlation-based features . the average accuracy obtained for intruder detection and classification on real-world and simulated data sets was in excess of 97 % .", "topics": ["simulation", "sensor"]}
{"title": "reweighted low-rank tensor decomposition based on t-svd and its applications in video denoising", "abstract": "the t-svd based tensor robust principal component analysis ( trpca ) decomposes low rank multi-linear signal corrupted by gross errors into low multi-rank and sparse component by simultaneously minimizing tensor nuclear norm and l 1 norm . but if the multi-rank of the signal is considerably large and/or large amount of noise is present , the performance of trpca deteriorates . to overcome this problem , this paper proposes a new efficient iterative reweighted tensor decomposition scheme based on t-svd which significantly improves tensor multi-rank in trpca . further , the sparse component of the tensor is also recovered by reweighted l 1 norm which enhances the accuracy of decomposition . the effectiveness of the proposed method is established by applying it to the video denoising problem and the experimental results reveal that the proposed algorithm outperforms its counterparts .", "topics": ["noise reduction", "sparse matrix"]}
{"title": "q-cp : learning action values for cooperative planning", "abstract": "research on multi-robot systems has demonstrated promising results in manifold applications and domains . still , efficiently learning an effective robot behaviors is very difficult , due to unstructured scenarios , high uncertainties , and large state dimensionality ( e.g . hyper-redundant and groups of robot ) . to alleviate this problem , we present q-cp a cooperative model-based reinforcement learning algorithm , which exploits action values to both ( 1 ) guide the exploration of the state space and ( 2 ) generate effective policies . specifically , we exploit q-learning to attack the curse-of-dimensionality in the iterations of a monte-carlo tree search . we implement and evaluate q-cp on different stochastic cooperative ( general-sum ) games : ( 1 ) a simple cooperative navigation problem among 3 robots , ( 2 ) a cooperation scenario between a pair of kuka youbots performing hand-overs , and ( 3 ) a coordination task between two mobile robots entering a door . the obtained results show the effectiveness of q-cp in the chosen applications , where action values drive the exploration and reduce the computational demand of the planning process while achieving good performance .", "topics": ["reinforcement learning", "iteration"]}
{"title": "mlpnp - a real-time maximum likelihood solution to the perspective-n-point problem", "abstract": "in this paper , a statistically optimal solution to the perspective-n-point ( pnp ) problem is presented . many solutions to the pnp problem are geometrically optimal , but do not consider the uncertainties of the observations . in addition , it would be desirable to have an internal estimation of the accuracy of the estimated rotation and translation parameters of the camera pose . thus , we propose a novel maximum likelihood solution to the pnp problem , that incorporates image observation uncertainties and remains real-time capable at the same time . further , the presented method is general , as is works with 3d direction vectors instead of 2d image points and is thus able to cope with arbitrary central camera models . this is achieved by projecting ( and thus reducing ) the covariance matrices of the observations to the corresponding vector tangent space .", "topics": ["optimization problem"]}
{"title": "kernel-based just-in-time learning for passing expectation propagation messages", "abstract": "we propose an efficient nonparametric strategy for learning a message operator in expectation propagation ( ep ) , which takes as input the set of incoming messages to a factor node , and produces an outgoing message as output . this learned operator replaces the multivariate integral required in classical ep , which may not have an analytic expression . we use kernel-based regression , which is trained on a set of probability distributions representing the incoming messages , and the associated outgoing messages . the kernel approach has two main advantages : first , it is fast , as it is implemented using a novel two-layer random feature representation of the input message distributions ; second , it has principled uncertainty estimates , and can be cheaply updated online , meaning it can request and incorporate new training data when it encounters inputs on which it is uncertain . in experiments , our approach is able to solve learning problems where a single message operator is required for multiple , substantially different data sets ( logistic regression for a variety of classification problems ) , where it is essential to accurately assess uncertainty and to efficiently and robustly update the message operator .", "topics": ["test set"]}
{"title": "the expressive power of neural networks : a view from the width", "abstract": "the expressive power of neural networks is important for understanding deep learning . most existing works consider this problem from the view of the depth of a network . in this paper , we study how width affects the expressiveness of neural networks . classical results state that depth-bounded ( e.g . depth- $ 2 $ ) networks with suitable activation functions are universal approximators . we show a universal approximation theorem for width-bounded relu networks : width- $ ( n+4 ) $ relu networks , where $ n $ is the input dimension , are universal approximators . moreover , except for a measure zero set , all functions can not be approximated by width- $ n $ relu networks , which exhibits a phase transition . several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks . that is , there are classes of deep networks which can not be realized by any shallow network whose size is no more than an exponential bound . here we pose the dual question on the width-efficiency of relu networks : are there wide networks that can not be realized by narrow networks whose size is not substantially larger ? we show that there exist classes of wide networks which can not be realized by any narrow network whose depth is no more than a polynomial bound . on the other hand , we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy . our results provide more comprehensive evidence that depth is more effective than width for the expressiveness of relu networks .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "a survey of recent advances in texture representation", "abstract": "texture is a fundamental characteristic of many types of images , and texture representation is one of the essential and challenging problems in computer vision and pattern recognition which has attracted extensive research attention . since 2000 , texture representations based on bag of words ( bow ) and on convolutional neural networks ( cnns ) have been extensively studied with impressive performance . given this period of remarkable evolution , this paper aims to present a comprehensive survey of advances in texture representation over the last two decades . more than 200 major publications are cited in this survey covering different aspects of the research , which includes ( i ) problem description ; ( ii ) recent advances in the broad categories of bow-based , cnn-based and attribute-based methods ; and ( iii ) evaluation issues , specifically benchmark datasets and state of the art results . in retrospect of what has been achieved so far , the survey discusses open challenges and directions for future research .", "topics": ["computer vision"]}
{"title": "extraction of v-n-collocations from text corpora : a feasibility study for german", "abstract": "the usefulness of a statistical approach suggested by church et al . ( 1991 ) is evaluated for the extraction of verb-noun ( v-n ) collocations from german text corpora . some problematic issues of that method arising from properties of the german language are discussed and various modifications of the method are considered that might improve extraction results for german . the precision and recall of all variant methods is evaluated for v-n collocations containing support verbs , and the consequences for further work on the extraction of collocations from german corpora are discussed . with a sufficiently large corpus ( > = 6 mio . word-tokens ) , the average error rate of wrong extractions can be reduced to 2.2 % ( 97.8 % precision ) with the most restrictive method , however with a loss in data of almost 50 % compared to a less restrictive method with still 87.6 % precision . depending on the goal to be achieved , emphasis can be put on a high recall for lexicographic purposes or on high precision for automatic lexical acquisition , in each case unfortunately leading to a decrease of the corresponding other variable . low recall can still be acceptable if very large corpora ( i.e . 50 - 100 million words ) are available or if corpora for special domains are used in addition to the data found in machine readable ( collocation ) dictionaries .", "topics": ["text corpus", "dictionary"]}
{"title": "computing with logic as operator elimination : the toyelim system", "abstract": "a prototype system is described whose core functionality is , based on propositional logic , the elimination of second-order operators , such as boolean quantifiers and operators for projection , forgetting and circumscription . this approach allows to express many representational and computational tasks in knowledge representation - for example computation of abductive explanations and models with respect to logic programming semantics - in a uniform operational system , backed by a uniform classical semantic framework .", "topics": ["computation", "relevance"]}
{"title": "an efficient approach to sparse linear discriminant analysis", "abstract": "we present a novel approach to the formulation and the resolution of sparse linear discriminant analysis ( lda ) . our proposal , is based on penalized optimal scoring . it has an exact equivalence with penalized lda , contrary to the multi-class approaches based on the regression of class indicator that have been proposed so far . sparsity is obtained thanks to a group-lasso penalty that selects the same features in all discriminant directions . our experiments demonstrate that this approach generates extremely parsimonious models without compromising prediction performances . besides prediction , the resulting sparse discriminant directions are also amenable to low-dimensional representations of data . our algorithm is highly efficient for medium to large number of variables , and is thus particularly well suited to the analysis of gene expression data .", "topics": ["sparse matrix"]}
{"title": "multi-modal hybrid deep neural network for speech enhancement", "abstract": "deep neural networks ( dnn ) have been successful in en- hancing noisy speech signals . enhancement is achieved by learning a nonlinear mapping function from the features of the corrupted speech signal to that of the reference clean speech signal . the quality of predicted features can be improved by providing additional side channel information that is robust to noise , such as visual cues . in this paper we propose a novel deep learning model inspired by insights from human audio visual perception . in the proposed unified hybrid architecture , features from a convolution neural network ( cnn ) that processes the visual cues and features from a fully connected dnn that processes the audio signal are integrated using a bidirectional long short-term memory ( bilstm ) network . the parameters of the hybrid model are jointly learned using backpropagation . we compare the quality of enhanced speech from the hybrid models with those from traditional dnn and bilstm models .", "topics": ["neural networks", "nonlinear system"]}
{"title": "preservation of semantic properties during the aggregation of abstract argumentation frameworks", "abstract": "an abstract argumentation framework can be used to model the argumentative stance of an agent at a high level of abstraction , by indicating for every pair of arguments that is being considered in a debate whether the first attacks the second . when modelling a group of agents engaged in a debate , we may wish to aggregate their individual argumentation frameworks to obtain a single such framework that reflects the consensus of the group . even when agents disagree on many details , there may well be high-level agreement on important semantic properties , such as the acceptability of a given argument . using techniques from social choice theory , we analyse under what circumstances such semantic properties agreed upon by the individual agents can be preserved under aggregation .", "topics": ["high- and low-level"]}
{"title": "unsupervised learning with truncated gaussian graphical models", "abstract": "gaussian graphical models ( ggms ) are widely used for statistical modeling , because of ease of inference and the ubiquitous use of the normal distribution in practical approximations . however , they are also known for their limited modeling abilities , due to the gaussian assumption . in this paper , we introduce a novel variant of ggms , which relaxes the gaussian restriction and yet admits efficient inference . specifically , we impose a bipartite structure on the ggm and govern the hidden variables by truncated normal distributions . the nonlinearity of the model is revealed by its connection to rectified linear unit ( relu ) neural networks . meanwhile , thanks to the bipartite structure and appealing properties of truncated normals , we are able to train the models efficiently using contrastive divergence . we consider three output constructs , accounting for real-valued , binary and count data . we further extend the model to deep constructions and show that deep models can be used for unsupervised pre-training of rectifier neural networks . extensive experimental results are provided to validate the proposed models and demonstrate their superiority over competing models .", "topics": ["graphical model", "unsupervised learning"]}
{"title": "detecting suicidal ideation in chinese microblogs with psychological lexicons", "abstract": "suicide is among the leading causes of death in china . however , technical approaches toward preventing suicide are challenging and remaining under development . recently , several actual suicidal cases were preceded by users who posted microblogs with suicidal ideation to sina weibo , a chinese social media network akin to twitter . it would therefore be desirable to detect suicidal ideations from microblogs in real-time , and immediately alert appropriate support groups , which may lead to successful prevention . in this paper , we propose a real-time suicidal ideation detection system deployed over weibo , using machine learning and known psychological techniques . currently , we have identified 53 known suicidal cases who posted suicide notes on weibo prior to their deaths.we explore linguistic features of these known cases using a psychological lexicon dictionary , and train an effective suicidal weibo post detection model . 6714 tagged posts and several classifiers are used to verify the model . by combining both machine learning and psychological knowledge , svm classifier has the best performance of different classifiers , yielding an f-measure of 68:3 % , a precision of 78:9 % , and a recall of 60:3 % .", "topics": ["dictionary"]}
{"title": "dag-based long short-term memory for neural word segmentation", "abstract": "neural word segmentation has attracted more and more research interests for its ability to alleviate the effort of feature engineering and utilize the external resource by the pre-trained character or word embeddings . in this paper , we propose a new neural model to incorporate the word-level information for chinese word segmentation . unlike the previous word-based models , our model still adopts the framework of character-based sequence labeling , which has advantages on both effectiveness and efficiency at the inference stage . to utilize the word-level information , we also propose a new long short-term memory ( lstm ) architecture over directed acyclic graph ( dag ) . experimental results demonstrate that our model leads to better performances than the baseline models .", "topics": ["baseline ( configuration management )"]}
{"title": "multilevel thresholding segmentation of t2 weighted brain mri images using convergent heterogeneous particle swarm optimization", "abstract": "this paper proposes a new image thresholding segmentation approach using the heuristic method , convergent heterogeneous particle swarm optimization algorithm . the proposed algorithm incorporates a new strategy of searching the problem space by dividing the swarm into subswarms . each subswarm particles search for better solution separately lead to better exploitation while they cooperate with each other to find the best global position . the consequence of the aforementioned cooperation is better exploration , convergence and it able the algorithm to jump from local optimal solution to the better spots . a practical application of this method is demonstrated for the problem of medical image thresholding segmentation . we considered two classical thresholding techniques of otsu and kapur separately as the objective function for the optimization method and applied on a set of brain mr images . comparative experimental results reveal that the proposed method outperforms another state of the art method from the literature in terms of accuracy , computation time and stable results .", "topics": ["optimization problem", "time complexity"]}
{"title": "similarity assessment through blocking and affordance assignment in textual cbr", "abstract": "it has been conceived that children learn new objects through their affordances , that is , the actions that can be taken on them . we suggest that web pages also have affordances defined in terms of the users ' information need they meet . an assumption of the proposed approach is that different parts of a text may not be equally important / relevant to a given query . judgment on the relevance of a web document requires , therefore , a thorough look into its parts , rather than treating it as a monolithic content . we propose a method to extract and assign affordances to texts and then use these affordances to retrieve the corresponding web pages . the overall approach presented in the paper relies on case-based representations that bridge the queries to the affordances of web documents . we tested our method on the tourism domain and the results are promising .", "topics": ["relevance"]}
{"title": "memory limited , streaming pca", "abstract": "we consider streaming , one-pass principal component analysis ( pca ) , in the high-dimensional regime , with limited memory . here , $ p $ -dimensional samples are presented sequentially , and the goal is to produce the $ k $ -dimensional subspace that best approximates these points . standard algorithms require $ o ( p^2 ) $ memory ; meanwhile no algorithm can do better than $ o ( kp ) $ memory , since this is what the output itself requires . memory ( or storage ) complexity is most meaningful when understood in the context of computational and sample complexity . sample complexity for high-dimensional pca is typically studied in the setting of the { \\em spiked covariance model } , where $ p $ -dimensional points are generated from a population covariance equal to the identity ( white noise ) plus a low-dimensional perturbation ( the spike ) which is the signal to be recovered . it is now well-understood that the spike can be recovered when the number of samples , $ n $ , scales proportionally with the dimension , $ p $ . yet , all algorithms that provably achieve this , have memory complexity $ o ( p^2 ) $ . meanwhile , algorithms with memory-complexity $ o ( kp ) $ do not have provable bounds on sample complexity comparable to $ p $ . we present an algorithm that achieves both : it uses $ o ( kp ) $ memory ( meaning storage of any kind ) and is able to compute the $ k $ -dimensional spike with $ o ( p \\log p ) $ sample-complexity -- the first algorithm of its kind . while our theoretical analysis focuses on the spiked covariance model , our simulations show that our algorithm is successful on much more general models for the data .", "topics": ["simulation"]}
{"title": "learnable explicit density for continuous latent space and variational inference", "abstract": "in this paper , we study two aspects of the variational autoencoder ( vae ) : the prior distribution over the latent variables and its corresponding posterior . first , we decompose the learning of vaes into layerwise density estimation , and argue that having a flexible prior is beneficial to both sample generation and inference . second , we analyze the family of inverse autoregressive flows ( inverse af ) and show that with further improvement , inverse af could be used as universal approximation to any complicated posterior . our analysis results in a unified approach to parameterizing a vae , without the need to restrict ourselves to use factorial gaussians in the latent real space .", "topics": ["calculus of variations", "autoencoder"]}
{"title": "estimating mixture models via mixtures of polynomials", "abstract": "mixture modeling is a general technique for making any simple model more expressive through weighted combination . this generality and simplicity in part explains the success of the expectation maximization ( em ) algorithm , in which updates are easy to derive for a wide class of mixture models . however , the likelihood of a mixture model is non-convex , so em has no known global convergence guarantees . recently , method of moments approaches offer global guarantees for some mixture models , but they do not extend easily to the range of mixture models that exist . in this work , we present polymom , an unifying framework based on method of moments in which estimation procedures are easily derivable , just as in em . polymom is applicable when the moments of a single mixture component are polynomials of the parameters . our key observation is that the moments of the mixture model are a mixture of these polynomials , which allows us to cast estimation as a generalized moment problem . we solve its relaxations using semidefinite optimization , and then extract parameters using ideas from computer algebra . this framework allows us to draw insights and apply tools from convex optimization , computer algebra and the theory of moments to study problems in statistical estimation .", "topics": ["optimization problem", "simulation"]}
{"title": "automated cloud provisioning on aws using deep reinforcement learning", "abstract": "as the use of cloud computing continues to rise , controlling cost becomes increasingly important . yet there is evidence that 30\\ % - 45\\ % of cloud spend is wasted . existing tools for cloud provisioning typically rely on highly trained human experts to specify what to monitor , thresholds for triggering action , and actions . in this paper we explore the use of reinforcement learning ( rl ) to acquire policies to balance performance and spend , allowing humans to specify what they want as opposed to how to do it , minimizing the need for cloud expertise . empirical results with tabular , deep , and dueling double deep q-learning with the cloudsim simulator show the utility of rl and the relative merits of the approaches . we also demonstrate effective policy transfer learning from an extremely simple simulator to cloudsim , with the next step being transfer from cloudsim to an amazon web services physical environment .", "topics": ["reinforcement learning", "simulation"]}
{"title": "stochastic planning and lifted inference", "abstract": "lifted probabilistic inference ( poole , 2003 ) and symbolic dynamic programming for lifted stochastic planning ( boutilier et al , 2001 ) were introduced around the same time as algorithmic efforts to use abstraction in stochastic systems . over the years , these ideas evolved into two distinct lines of research , each supported by a rich literature . lifted probabilistic inference focused on efficient arithmetic operations on template-based graphical models under a finite domain assumption while symbolic dynamic programming focused on supporting sequential decision-making in rich quantified logical action models and on open domain reasoning . given their common motivation but different focal points , both lines of research have yielded highly complementary innovations . in this chapter , we aim to help close the gap between these two research areas by providing an overview of lifted stochastic planning from the perspective of probabilistic inference , showing strong connections to other chapters in this book . this also allows us to define generalized lifted inference as a paradigm that unifies these areas and elucidates open problems for future research that can benefit both lifted inference and stochastic planning .", "topics": ["graphical model"]}
{"title": "deep reinforcement learning in parameterized action space", "abstract": "recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring continuous state and action spaces . however , to the best of our knowledge no previous work has succeeded at using deep neural networks in structured ( parameterized ) continuous action spaces . to fill this gap , this paper focuses on learning within the domain of simulated robocup soccer , which features a small set of discrete action types , each of which is parameterized with continuous variables . the best learned agent can score goals more reliably than the 2012 robocup champion agent . as such , this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space mdps .", "topics": ["approximation algorithm", "reinforcement learning"]}
{"title": "causal regularization", "abstract": "in application domains such as healthcare , we want accurate predictive models that are also causally interpretable . in pursuit of such models , we propose a causal regularizer to steer predictive models towards causally-interpretable solutions and theoretically study its properties . in a large-scale analysis of electronic health records ( ehr ) , our causally-regularized model outperforms its l1-regularized counterpart in causal accuracy and is competitive in predictive performance . we perform non-linear causality analysis by causally regularizing a special neural network architecture . we also show that the proposed causal regularizer can be used together with neural representation learning algorithms to yield up to 20 % improvement over multilayer perceptron in detecting multivariate causation , a situation common in healthcare , where many causal factors should occur simultaneously to have an effect on the target variable .", "topics": ["feature learning", "nonlinear system"]}
{"title": "reward shaping with recurrent neural networks for speeding up on-line policy learning in spoken dialogue systems", "abstract": "statistical spoken dialogue systems have the attractive property of being able to be optimised from data via interactions with real users . however in the reinforcement learning paradigm the dialogue manager ( agent ) often requires significant time to explore the state-action space to learn to behave in a desirable manner . this is a critical issue when the system is trained on-line with real users where learning costs are expensive . reward shaping is one promising technique for addressing these concerns . here we examine three recurrent neural network ( rnn ) approaches for providing reward shaping information in addition to the primary ( task-orientated ) environmental feedback . these rnns are trained on returns from dialogues generated by a simulated user and attempt to diffuse the overall evaluation of the dialogue back down to the turn level to guide the agent towards good behaviour faster . in both simulated and real user scenarios these rnns are shown to increase policy learning speed . importantly , they do not require prior knowledge of the user 's goal .", "topics": ["recurrent neural network", "reinforcement learning"]}
{"title": "an analysis of the methods employed for breast cancer diagnosis", "abstract": "breast cancer research over the last decade has been tremendous . the ground breaking innovations and novel methods help in the early detection , in setting the stages of the therapy and in assessing the response of the patient to the treatment . the prediction of the recurrent cancer is also crucial for the survival of the patient . this paper studies various techniques used for the diagnosis of breast cancer . different methods are explored for their merits and de-merits for the diagnosis of breast lesion . some of the methods are yet unproven but the studies look very encouraging . it was found that the recent use of the combination of artificial neural networks in most of the instances gives accurate results for the diagnosis of breast cancer and their use can also be extended to other diseases .", "topics": ["neural networks"]}
{"title": "genetic algorithms and its use with back-propagation network", "abstract": "genetic algorithms are considered as one of the most efficient search techniques . although they do not offer an optimal solution , their ability to reach a suitable solution in considerably short time gives them their respectable role in many ai techniques . this work introduces genetic algorithms and describes their characteristics . then a novel method using genetic algorithm in best training set generation and selection for a back-propagation network is proposed . this work also offers a new extension to the original genetic algorithms", "topics": ["optimization problem"]}
{"title": "performance analysis of multiclass support vector machine classification for diagnosis of coronary heart diseases", "abstract": "automatic diagnosis of coronary heart disease helps the doctor to support in decision making a diagnosis . coronary heart disease have some types or levels . referring to the uci repository dataset , it divided into 4 types or levels that are labeled numbers 1-4 ( low , medium , high and serious ) . the diagnosis models can be analyzed with multiclass classification approach . one of multiclass classification approach used , one of which is a support vector machine ( svm ) . the svm use due to strong performance of svm in binary classification . this research study multiclass performance classification support vector machine to diagnose the type or level of coronary heart disease . coronary heart disease patient data taken from the uci repository . stages in this study is preprocessing , which consist of , to normalizing the data , divide the data into data training and testing . the next stage of multiclass classification and performance analysis . this study uses multiclass svm algorithm , namely : binary tree support vector machine ( btsvm ) , one-against-one ( oao ) , one-against-all ( oaa ) , decision direct acyclic graph ( ddag ) and exhaustive output error correction code ( ecoc ) . performance parameter used is recall , precision , f-measure and overall accuracy .", "topics": ["support vector machine"]}
{"title": "recurrent highway networks", "abstract": "many sequential processing tasks require complex nonlinear transition functions from one step to the next . however , recurrent neural networks with 'deep ' transition functions remain difficult to train , even when using long short-term memory ( lstm ) networks . we introduce a novel theoretical analysis of recurrent networks based on gersgorin 's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the lstm cell . based on this analysis we propose recurrent highway networks , which extend the lstm architecture to allow step-to-step transition depths larger than one . several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models . on the penn treebank corpus , solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters . on the larger wikipedia datasets for character prediction ( text8 and enwik8 ) , rhns outperform all previous results and achieve an entropy of 1.27 bits per character .", "topics": ["test set", "recurrent neural network"]}
{"title": "comparing two trainable grammatical relations finders", "abstract": "grammatical relationships ( grs ) form an important level of natural language processing , but different sets of grs are useful for different purposes . therefore , one may often only have time to obtain a small training corpus with the desired gr annotations . on such a small training corpus , we compare two systems . they use different learning techniques , but we find that this difference by itself only has a minor effect . a larger factor is that in english , a different gr length measure appears better suited for finding simple argument grs than for finding modifier grs . we also find that partitioning the data may help memory-based learning .", "topics": ["natural language processing"]}
{"title": "compressed inference for probabilistic sequential models", "abstract": "hidden markov models ( hmms ) and conditional random fields ( crfs ) are two popular techniques for modeling sequential data . inference algorithms designed over crfs and hmms allow estimation of the state sequence given the observations . in several applications , estimation of the state sequence is not the end goal ; instead the goal is to compute some function of it . in such scenarios , estimating the state sequence by conventional inference techniques , followed by computing the functional mapping from the estimate is not necessarily optimal . a more formal approach is to directly infer the final outcome from the observations . in particular , we consider the specific instantiation of the problem where the goal is to find the state trajectories without exact transition points and derive a novel polynomial time inference algorithm that outperforms vanilla inference techniques . we show that this particular problem arises commonly in many disparate applications and present experiments on three of them : ( 1 ) toy robot tracking ; ( 2 ) single stroke character recognition ; ( 3 ) handwritten word recognition .", "topics": ["time complexity", "markov chain"]}
{"title": "enabling medical translation for low-resource languages", "abstract": "we present research towards bridging the language gap between migrant workers in qatar and medical staff . in particular , we present the first steps towards the development of a real-world hindi-english machine translation system for doctor-patient communication . as this is a low-resource language pair , especially for speech and for the medical domain , our initial focus has been on gathering suitable training data from various sources . we applied a variety of methods ranging from fully automatic extraction from the web to manual annotation of test data . moreover , we developed a method for automatically augmenting the training data with synthetically generated variants , which yielded a very sizable improvement of more than 3 bleu points absolute .", "topics": ["test set", "machine translation"]}
{"title": "video in sentences out", "abstract": "we present a system that produces sentential descriptions of video : who did what to whom , and where and how they did it . action class is rendered as a verb , participant objects as noun phrases , properties of those objects as adjectival modifiers in those noun phrases , spatial relations between those participants as prepositional phrases , and characteristics of the event as prepositional-phrase adjuncts and adverbial modifiers . extracting the information needed to render these linguistic entities requires an approach to event recognition that recovers object tracks , the trackto-role assignments , and changing body posture .", "topics": ["entity"]}
{"title": "recurrent neural network regularization", "abstract": "we present a simple regularization technique for recurrent neural networks ( rnns ) with long short-term memory ( lstm ) units . dropout , the most successful technique for regularizing neural networks , does not work well with rnns and lstms . in this paper , we show how to correctly apply dropout to lstms , and show that it substantially reduces overfitting on a variety of tasks . these tasks include language modeling , speech recognition , image caption generation , and machine translation .", "topics": ["recurrent neural network", "machine translation"]}
{"title": "the optimization of running queries in relational databases using ant-colony algorithm", "abstract": "the issue of optimizing queries is a cost-sensitive process and with respect to the number of associated tables in a query , its number of permutations grows exponentially . on one hand , in comparison with other operators in relational database , join operator is the most difficult and complicated one in terms of optimization for reducing its runtime . accordingly , various algorithms have so far been proposed to solve this problem . on the other hand , the success of any database management system ( dbms ) means exploiting the query model . in the current paper , the heuristic ant algorithm has been proposed to solve this problem and improve the runtime of join operation . experiments and observed results reveal the efficiency of this algorithm compared to its similar algorithms .", "topics": ["mathematical optimization", "heuristic"]}
{"title": "kl-based control of the learning schedule for surrogate black-box optimization", "abstract": "this paper investigates the control of an ml component within the covariance matrix adaptation evolution strategy ( cma-es ) devoted to black-box optimization . the known cma-es weakness is its sample complexity , the number of evaluations of the objective function needed to approximate the global optimum . this weakness is commonly addressed through surrogate optimization , learning an estimate of the objective function a.k.a . surrogate model , and replacing most evaluations of the true objective function with the ( inexpensive ) evaluation of the surrogate model . this paper presents a principled control of the learning schedule ( when to relearn the surrogate model ) , based on the kullback-leibler divergence of the current search distribution and the training distribution of the former surrogate model . the experimental validation of the proposed approach shows significant performance gains on a comprehensive set of ill-conditioned benchmark problems , compared to the best state of the art including the quasi-newton high-precision bfgs method .", "topics": ["approximation algorithm", "optimization problem"]}
{"title": "a multi-objective exploratory procedure for regression model selection", "abstract": "variable selection is recognized as one of the most critical steps in statistical modeling . the problems encountered in engineering and social sciences are commonly characterized by over-abundance of explanatory variables , non-linearities and unknown interdependencies between the regressors . an added difficulty is that the analysts may have little or no prior knowledge on the relative importance of the variables . to provide a robust method for model selection , this paper introduces the multi-objective genetic algorithm for variable selection ( moga-vs ) that provides the user with an optimal set of regression models for a given data-set . the algorithm considers the regression problem as a two objective task , and explores the pareto-optimal ( best subset ) models by preferring those models over the other which have less number of regression coefficients and better goodness of fit . the model exploration can be performed based on in-sample or generalization error minimization . the model selection is proposed to be performed in two steps . first , we generate the frontier of pareto-optimal regression models by eliminating the dominated models without any user intervention . second , a decision making process is executed which allows the user to choose the most preferred model using visualisations and simple metrics . the method has been evaluated on a recently published real dataset on communities and crime within united states .", "topics": ["coefficient"]}
{"title": "implementation of an innovative bio inspired ga and pso algorithm for controller design considering steam gt dynamics", "abstract": "the application of bio inspired algorithms to complicated power system stability problems has recently attracted the researchers in the field of artificial intelligence . low frequency oscillations after a disturbance in a power system , if not sufficiently damped , can drive the system unstable . this paper provides a systematic procedure to damp the low frequency oscillations based on bio inspired genetic ( ga ) and particle swarm optimization ( pso ) algorithms . the proposed controller design is based on formulating a system damping ratio enhancement based optimization criterion to compute the optimal controller parameters for better stability . the novel and contrasting feature of this work is the mathematical modeling and simulation of the synchronous generator model including the steam governor turbine ( gt ) dynamics . to show the robustness of the proposed controller , non linear time domain simulations have been carried out under various system operating conditions . also , a detailed comparative study has been done to show the superiority of the bio inspired algorithm based controllers over the conventional lead lag controller .", "topics": ["time complexity", "simulation"]}
{"title": "improving semi-supervised support vector machines through unlabeled instances selection", "abstract": "semi-supervised support vector machines ( s3vms ) are a kind of popular approaches which try to improve learning performance by exploiting unlabeled data . though s3vms have been found helpful in many situations , they may degenerate performance and the resultant generalization ability may be even worse than using the labeled data only . in this paper , we try to reduce the chance of performance degeneration of s3vms . our basic idea is that , rather than exploiting all unlabeled data , the unlabeled instances should be selected such that only the ones which are very likely to be helpful are exploited , while some highly risky unlabeled instances are avoided . we propose the s3vm-\\emph { us } method by using hierarchical clustering to select the unlabeled instances . experiments on a broad range of data sets over eighty-eight different settings show that the chance of performance degeneration of s3vm-\\emph { us } is much smaller than that of existing s3vms .", "topics": ["cluster analysis", "support vector machine"]}
{"title": "optimal learning rates for localized svms", "abstract": "one of the limiting factors of using support vector machines ( svms ) in large scale applications are their super-linear computational requirements in terms of the number of training samples . to address this issue , several approaches that train svms on many small chunks of large data sets separately have been proposed in the literature . so far , however , almost all these approaches have only been empirically investigated . in addition , their motivation was always based on computational requirements . in this work , we consider a localized svm approach based upon a partition of the input space . for this local svm , we derive a general oracle inequality . then we apply this oracle inequality to least squares regression using gaussian kernels and deduce local learning rates that are essentially minimax optimal under some standard smoothness assumptions on the regression function . this gives the first motivation for using local svms that is not based on computational requirements but on theoretical predictions on the generalization performance . we further introduce a data-dependent parameter selection method for our local svm approach and show that this method achieves the same learning rates as before . finally , we present some larger scale experiments for our localized svm showing that it achieves essentially the same test performance as a global svm for a fraction of the computational requirements . in addition , it turns out that the computational requirements for the local svms are similar to those of a vanilla random chunk approach , while the achieved test errors are significantly better .", "topics": ["support vector machine"]}
{"title": "a neural-network technique for recognition of filaments in solar images", "abstract": "we describe a new neural-network technique developed for an automated recognition of solar filaments visible in the hydrogen h-alpha line full disk spectroheliograms . this technique allows neural networks learn from a few image fragments labelled manually to recognize the single filaments depicted on a local background . the trained network is able to recognize filaments depicted on the backgrounds with variations in brightness caused by atmospherics distortions . despite the difference in backgrounds in our experiments the neural network has properly recognized filaments in the testing image fragments . using a parabolic activation function we extend this technique to recognize multiple solar filaments which may appear in one fragment .", "topics": ["pixel"]}
{"title": "adversarial perturbations against deep neural networks for malware classification", "abstract": "deep neural networks , like many other machine learning models , have recently been shown to lack robustness against adversarially crafted inputs . these inputs are derived from regular inputs by minor yet carefully selected perturbations that deceive machine learning models into desired misclassifications . existing work in this emerging field was largely specific to the domain of image classification , since the high-entropy of images can be conveniently manipulated without changing the images ' overall visual appearance . yet , it remains unclear how such attacks translate to more security-sensitive applications such as malware detection - which may pose significant challenges in sample generation and arguably grave consequences for failure . in this paper , we show how to construct highly-effective adversarial sample crafting attacks for neural networks used as malware classifiers . the application domain of malware classification introduces additional constraints in the adversarial sample crafting problem when compared to the computer vision domain : ( i ) continuous , differentiable input domains are replaced by discrete , often binary inputs ; and ( ii ) the loose condition of leaving visual appearance unchanged is replaced by requiring equivalent functional behavior . we demonstrate the feasibility of these attacks on many different instances of malware classifiers that we trained using the drebin android malware data set . we furthermore evaluate to which extent potential defensive mechanisms against adversarial crafting can be leveraged to the setting of malware classification . while feature reduction did not prove to have a positive impact , distillation and re-training on adversarially crafted samples show promising results .", "topics": ["neural networks", "computer vision"]}
{"title": "combining fully convolutional and recurrent neural networks for 3d biomedical image segmentation", "abstract": "segmentation of 3d images is a fundamental problem in biomedical image analysis . deep learning ( dl ) approaches have achieved state-of-the-art segmentation perfor- mance . to exploit the 3d contexts using neural networks , known dl segmentation methods , including 3d convolution , 2d convolution on planes orthogonal to 2d image slices , and lstm in multiple directions , all suffer incompatibility with the highly anisotropic dimensions in common 3d biomedical images . in this paper , we propose a new dl framework for 3d image segmentation , based on a com- bination of a fully convolutional network ( fcn ) and a recurrent neural network ( rnn ) , which are responsible for exploiting the intra-slice and inter-slice contexts , respectively . to our best knowledge , this is the first dl framework for 3d image segmentation that explicitly leverages 3d image anisotropism . evaluating using a dataset from the isbi neuronal structure segmentation challenge and in-house image stacks for 3d fungus segmentation , our approach achieves promising results comparing to the known dl-based 3d segmentation approaches .", "topics": ["image segmentation", "recurrent neural network"]}
{"title": "neural network-based active learning in multivariate calibration", "abstract": "in chemometrics , data from infrared or near-infrared ( nir ) spectroscopy are often used to identify a compound or to analyze the composition of amaterial . this involves the calibration of models that predict the concentration ofmaterial constituents from the measured nir spectrum . an interesting aspect of multivariate calibration is to achieve a particular accuracy level with a minimum number of training samples , as this reduces the number of laboratory tests and thus the cost of model building . in these chemometric models , the input refers to a proper representation of the spectra and the output to the concentrations of the sample constituents . the search for a most informative new calibration sample thus has to be performed in the output space of the model , rather than in the input space as in conventionalmodeling problems . in this paper , we propose to solve the corresponding inversion problem by utilizing the disagreements of an ensemble of neural networks to represent the prediction error in the unexplored component space . the next calibration sample is then chosen at a composition where the individual models of the ensemble disagree most . the results obtained for a realistic chemometric calibration example show that the proposed active learning can achieve a given calibration accuracy with less training samples than random sampling .", "topics": ["sampling ( signal processing )"]}
{"title": "reduced space and faster convergence in imperfect-information games via regret-based pruning", "abstract": "counterfactual regret minimization ( cfr ) is the most popular iterative algorithm for solving zero-sum imperfect-information games . regret-based pruning ( rbp ) is an improvement that allows poorly-performing actions to be temporarily pruned , thus speeding up cfr . we introduce total rbp , a new form of rbp that reduces the space requirements of cfr as actions are pruned . we prove that in zero-sum games it asymptotically prunes any action that is not part of a best response to some nash equilibrium . this leads to provably faster convergence and lower space requirements . experiments show that total rbp results in an order of magnitude reduction in space , and the reduction factor increases with game size .", "topics": ["regret ( decision theory )"]}
{"title": "data mining approach for analyzing call center performance", "abstract": "the aim of our research was to apply well-known data mining techniques ( such as linear neural networks , multi-layered perceptrons , probabilistic neural networks , classification and regression trees , support vector machines and finally a hybrid decision tree neural network approach ) to the problem of predicting the quality of service in call centers ; based on the performance data actually collected in a call center of a large insurance company . our aim was two-fold . first , to compare the performance of models built using the above-mentioned techniques and , second , to analyze the characteristics of the input sensitivity in order to better understand the relationship between the perform-ance evaluation process and the actual performance and in this way help improve the performance of call centers . in this paper we summarize our findings .", "topics": ["statistical classification", "data mining"]}
{"title": "weakly supervised learning of actions from transcripts", "abstract": "we present an approach for weakly supervised learning of human actions from video transcriptions . our system is based on the idea that , given a sequence of input data and a transcript , i.e . a list of the order the actions occur in the video , it is possible to infer the actions within the video stream , and thus , learn the related action models without the need for any frame-based annotation . starting from the transcript information at hand , we split the given data sequences uniformly based on the number of expected actions . we then learn action models for each class by maximizing the probability that the training video sequences are generated by the action models given the sequence order as defined by the transcripts . the learned model can be used to temporally segment an unseen video with or without transcript . we evaluate our approach on four distinct activity datasets , namely hollywood extended , mpii cooking , breakfast and crim13 . we show that our system is able to align the scripted actions with the video data and that the learned models localize and classify actions competitively in comparison to models trained with full supervision , i.e . with frame level annotations , and that they outperform any current state-of-the-art approach for aligning transcripts with video data .", "topics": ["supervised learning"]}
{"title": "when do differences matter ? on-line feature extraction through cognitive economy", "abstract": "for an intelligent agent to be truly autonomous , it must be able to adapt its representation to the requirements of its task as it interacts with the world . most current approaches to on-line feature extraction are ad hoc ; in contrast , this paper presents an algorithm that bases judgments of state compatibility and state-space abstraction on principled criteria derived from the psychological principle of cognitive economy . the algorithm incorporates an active form of q-learning , and partitions continuous state-spaces by merging and splitting voronoi regions . the experiments illustrate a new methodology for testing and comparing representations by means of learning curves . results from the puck-on-a-hill task demonstrate the algorithm 's ability to learn effective representations , superior to those produced by some other , well-known , methods .", "topics": ["feature extraction", "autonomous car"]}
{"title": "a new tag formalism for tamil and parser analytics", "abstract": "tree adjoining grammar ( tag ) is specifically suited for morph rich and agglutinated languages like tamil due to its psycho linguistic features and parse time dependency and morph resolution . though tag and ltag formalisms have been known for about 3 decades , efforts on designing tag syntax for tamil have not been entirely successful due to the complexity of its specification and the rich morphology of tamil language . in this paper we present a minimalistic tag for tamil without much morphological considerations and also introduce a parser implementation with some obvious variations from the xtag system", "topics": ["parsing"]}
{"title": "xml matchers : approaches and challenges", "abstract": "schema matching , i.e . the process of discovering semantic correspondences between concepts adopted in different data source schemas , has been a key topic in database and artificial intelligence research areas for many years . in the past , it was largely investigated especially for classical database models ( e.g . , e/r schemas , relational databases , etc . ) . however , in the latest years , the widespread adoption of xml in the most disparate application fields pushed a growing number of researchers to design xml-specific schema matching approaches , called xml matchers , aiming at finding semantic matchings between concepts defined in dtds and xsds . xml matchers do not just take well-known techniques originally designed for other data models and apply them on dtds/xsds , but they exploit specific xml features ( e.g . , the hierarchical structure of a dtd/xsd ) to improve the performance of the schema matching process . the design of xml matchers is currently a well-established research area . the main goal of this paper is to provide a detailed description and classification of xml matchers . we first describe to what extent the specificities of dtds/xsds impact on the schema matching task . then we introduce a template , called xml matcher template , that describes the main components of an xml matcher , their role and behavior . we illustrate how each of these components has been implemented in some popular xml matchers . we consider our xml matcher template as the baseline for objectively comparing approaches that , at first glance , might appear as unrelated . the introduction of this template can be useful in the design of future xml matchers . finally , we analyze commercial tools implementing xml matchers and introduce two challenging issues strictly related to this topic , namely xml source clustering and uncertainty management in xml matchers .", "topics": ["baseline ( configuration management )", "cluster analysis"]}
{"title": "understanding probabilistic sparse gaussian process approximations", "abstract": "good sparse approximations are essential for practical inference in gaussian processes as the computational cost of exact methods is prohibitive for large datasets . the fully independent training conditional ( fitc ) and the variational free energy ( vfe ) approximations are two recent popular methods . despite superficial similarities , these approximations have surprisingly different theoretical properties and behave differently in practice . we thoroughly investigate the two methods for regression both analytically and through illustrative examples , and draw conclusions to guide practical application .", "topics": ["approximation", "sparse matrix"]}
{"title": "ezlearn : exploiting organic supervision in large-scale data annotation", "abstract": "many real-world applications require large-scale data annotation , such as identifying tissue origins based on gene expressions and classifying images into semantic categories . annotation classes are often numerous and subject to changes over time , and annotating examples has become the major bottleneck for supervised learning methods . in science and other high-value domains , large repositories of data samples are often available , together with two sources of organic supervision : a lexicon for the annotation classes , and text descriptions that accompany some data samples . distant supervision has emerged as a promising paradigm for exploiting such indirect supervision by automatically annotating examples where the text description contains a class mention in the lexicon . however , due to linguistic variations and ambiguities , such training data is inherently noisy , which limits the accuracy in this approach . in this paper , we introduce an auxiliary natural language processing system for the text modality , and incorporate co-training to reduce noise and augment signal in distant supervision . without using any manually labeled data , our ezlearn system learned to accurately annotate data samples in functional genomics and scientific figure comprehension , substantially outperforming state-of-the-art supervised methods trained on tens of thousands of annotated examples .", "topics": ["test set", "supervised learning"]}
{"title": "randomized block proximal damped newton method for composite self-concordant minimization", "abstract": "in this paper we consider the composite self-concordant ( csc ) minimization problem , which minimizes the sum of a self-concordant function $ f $ and a ( possibly nonsmooth ) proper closed convex function $ g $ . the csc minimization is the cornerstone of the path-following interior point methods for solving a broad class of convex optimization problems . it has also found numerous applications in machine learning . the proximal damped newton ( pdn ) methods have been well studied in the literature for solving this problem that enjoy a nice iteration complexity . given that at each iteration these methods typically require evaluating or accessing the hessian of $ f $ and also need to solve a proximal newton subproblem , the cost per iteration can be prohibitively high when applied to large-scale problems . inspired by the recent success of block coordinate descent methods , we propose a randomized block proximal damped newton ( rbpdn ) method for solving the csc minimization . compared to the pdn methods , the computational cost per iteration of rbpdn is usually significantly lower . the computational experiment on a class of regularized logistic regression problems demonstrate that rbpdn is indeed promising in solving large-scale csc minimization problems . the convergence of rbpdn is also analyzed in the paper . in particular , we show that rbpdn is globally convergent when $ g $ is lipschitz continuous . it is also shown that rbpdn enjoys a local linear convergence . moreover , we show that for a class of $ g $ including the case where $ g $ is lipschitz differentiable , rbpdn enjoys a global linear convergence . as a striking consequence , it shows that the classical damped newton methods [ 22,40 ] and the pdn [ 31 ] for such $ g $ are globally linearly convergent , which was previously unknown in the literature . moreover , this result can be used to sharpen the existing iteration complexity of these methods .", "topics": ["iteration"]}
{"title": "do wavenets dream of acoustic waves ?", "abstract": "various sources have reported the wavenet deep learning architecture being able to generate high-quality speech , but to our knowledge there have n't been studies on the interpretation or visualization of trained wavenets . this study investigates the possibility that wavenet understands speech by unsupervisedly learning an acoustically meaningful latent representation of the speech signals in its receptive field ; we also attempt to interpret the mechanism by which the feature extraction is performed . suggested by singular value decomposition and linear regression analysis on the activations and known acoustic features ( e.g . f0 ) , the key findings are ( 1 ) activations in the higher layers are highly correlated with spectral features ; ( 2 ) wavenet explicitly performs pitch extraction despite being trained to directly predict the next audio sample and ( 3 ) for the said feature analysis to take place , the latent signal representation is converted back and forth between baseband and wideband components .", "topics": ["feature extraction"]}
{"title": "putting a bug in ml : the moth olfactory network learns to read mnist", "abstract": "we seek to ( i ) characterize the learning architectures exploited in biological neural networks for training on very few samples , and ( ii ) port these algorithmic structures to a machine learning context . the moth olfactory network is among the simplest biological neural systems that can learn , and its architecture includes key structural elements widespread in biological neural nets , such as cascaded networks , competitive inhibition , high intrinsic noise , sparsity , reward mechanisms , and hebbian plasticity . the interactions of these structural elements play a critical enabling role in rapid learning . we assign a computational model of the moth olfactory network the task of learning to read the mnist digits . this model , mothnet , is closely aligned with the moth 's known biophysics and with in vivo electrode data , including data collected from moths learning new odors . we show that mothnet successfully learns to read given very few training samples ( 1 to 20 samples per class ) . in this few-samples regime , it substantially outperforms standard machine learning methods such as nearest-neighbors , support-vector machines , and convolutional neural networks ( cnns ) . the mothnet architecture illustrates how our proposed algorithmic structures , derived from biological brains , can be used to build alternative deep neural nets ( dnns ) that may potentially avoid some of dnns current learning rate limitations . this novel , bio-inspired neural network architecture offers a valuable complementary approach to dnn design .", "topics": ["support vector machine", "interaction"]}
{"title": "scdv : sparse composite document vectors using soft clustering over distributional representations", "abstract": "we present a feature vector formation technique for documents - sparse composite document vector ( scdv ) - which overcomes several shortcomings of the current distributional paragraph vector representations that are widely used for text representation . in scdv , word embedding 's are clustered to capture multiple semantic contexts in which words occur . they are then chained together to form document topic-vectors that can express complex , multi-topic documents . through extensive experiments on multi-class and multi-label classification tasks , we outperform the previous state-of-the-art method , ntsg ( liu et al . , 2015a ) . we also show that scdv embedding 's perform well on heterogeneous tasks like topic coherence , context-sensitive learning and information retrieval . moreover , we achieve significant reduction in training and prediction times compared to other representation methods . scdv achieves best of both worlds - better performance with lower time and space complexity .", "topics": ["feature vector", "cluster analysis"]}
{"title": "development of yes/no arabic question answering system", "abstract": "developing question answering systems has been one of the important research issues because it requires insights from a variety of disciplines , including , artificial intelligence , information retrieval , information extraction , natural language processing , and psychology.in this paper we realize a formal model for a lightweight semantic based open domain yes/no arabic question answering system based on paragraph retrieval with variable length . we propose a constrained semantic representation . using an explicit unification framework based on semantic similarities and query expansion synonyms and antonyms.this frequently improves the precision of the system . employing the passage retrieval system achieves a better precision by retrieving more paragraphs that contain relevant answers to the question ; it significantly reduces the amount of text to be processed by the system .", "topics": ["natural language processing", "artificial intelligence"]}
{"title": "breaking symmetry with different orderings", "abstract": "we can break symmetry by eliminating solutions within each symmetry class . for instance , the lex-leader method eliminates all but the smallest solution in the lexicographical ordering . unfortunately , the lex-leader method is intractable in general . we prove that , under modest assumptions , we can not reduce the worst case complexity of breaking symmetry by using other orderings on solutions . we also prove that a common type of symmetry , where rows and columns in a matrix of decision variables are interchangeable , is intractable to break when we use two promising alternatives to the lexicographical ordering : the gray code ordering ( which uses a different ordering on solutions ) , and the snake-lex ordering ( which is a variant of the lexicographical ordering that re-orders the variables ) . nevertheless , we show experimentally that using other orderings like the gray code to break symmetry can be beneficial in practice as they may better align with the objective function and branching heuristic .", "topics": ["optimization problem", "loss function"]}
{"title": "multiresolution recurrent neural networks : an application to dialogue response generation", "abstract": "we introduce the multiresolution recurrent neural network , which extends the sequence-to-sequence framework to model natural language generation as two parallel discrete stochastic processes : a sequence of high-level coarse tokens , and a sequence of natural language tokens . there are many ways to estimate or learn the high-level coarse tokens , but we argue that a simple extraction procedure is sufficient to capture a wealth of high-level discourse semantics . such procedure allows training the multiresolution recurrent neural network by maximizing the exact joint log-likelihood over both sequences . in contrast to the standard log- likelihood objective w.r.t . natural language tokens ( word perplexity ) , optimizing the joint log-likelihood biases the model towards modeling high-level abstractions . we apply the proposed model to the task of dialogue response generation in two challenging domains : the ubuntu technical support domain , and twitter conversations . on ubuntu , the model outperforms competing approaches by a substantial margin , achieving state-of-the-art results according to both automatic evaluation metrics and a human evaluation study . on twitter , the model appears to generate more relevant and on-topic responses according to automatic evaluation metrics . finally , our experiments demonstrate that the proposed model is more adept at overcoming the sparsity of natural language and is better able to capture long-term structure .", "topics": ["recurrent neural network", "high- and low-level"]}
{"title": "the road to quantum artificial intelligence", "abstract": "this paper overviews the basic principles and recent advances in the emerging field of quantum computation ( qc ) , highlighting its potential application to artificial intelligence ( ai ) . the paper provides a very brief introduction to basic qc issues like quantum registers , quantum gates and quantum algorithms and then it presents references , ideas and research guidelines on how qc can be used to deal with some basic ai problems , such as search and pattern matching , as soon as quantum computers become widely available .", "topics": ["computation", "artificial intelligence"]}
{"title": "dimensionality reduction and classification feature using mutual information applied to hyperspectral images : a wrapper strategy algorithm based on minimizing the error probability using the inequality of fano", "abstract": "in the feature classification domain , the choice of data affects widely the results . for the hyperspectral image , the bands dont all contain the information ; some bands are irrelevant like those affected by various atmospheric effects , see figure.4 , and decrease the classification accuracy . and there exist redundant bands to complicate the learning system and product incorrect prediction [ 14 ] . even the bands contain enough information about the scene they may ca n't predict the classes correctly if the dimension of space images , see figure.3 , is so large that needs many cases to detect the relationship between the bands and the scene ( hughes phenomenon ) [ 10 ] . we can reduce the dimensionality of hyperspectral images by selecting only the relevant bands ( feature selection or subset selection methodology ) , or extracting , from the original bands , new bands containing the maximal information about the classes , using any functions , logical or numerical ( feature extraction methodology ) [ 11 ] [ 9 ] . here we focus on the feature selection using mutual information . hyperspectral images have three advantages regarding the multispectral images [ 6 ] ,", "topics": ["ground truth", "pixel"]}
{"title": "new optimisation methods for machine learning", "abstract": "a thesis submitted for the degree of doctor of philosophy of the australian national university . in this work we introduce several new optimisation methods for problems in machine learning . our algorithms broadly fall into two categories : optimisation of finite sums and of graph structured objectives . the finite sum problem is simply the minimisation of objective functions that are naturally expressed as a summation over a large number of terms , where each term has a similar or identical weight . such objectives most often appear in machine learning in the empirical risk minimisation framework in the non-online learning setting . the second category , that of graph structured objectives , consists of objectives that result from applying maximum likelihood to markov random field models . unlike the finite sum case , all the non-linearity is contained within a partition function term , which does not readily decompose into a summation . for the finite sum problem , we introduce the finito and saga algorithms , as well as variants of each . for graph-structured problems , we take three complementary approaches . we look at learning the parameters for a fixed structure , learning the structure independently , and learning both simultaneously . specifically , for the combined approach , we introduce a new method for encouraging graph structures with the `` scale-free '' property . for the structure learning problem , we establish shortcut , a o ( n^ { 2.5 } ) expected time approximate structure learning method for gaussian graphical models . for problems where the structure is known but the parameters unknown , we introduce an approximate maximum likelihood learning algorithm that is capable of learning a useful subclass of gaussian graphical models .", "topics": ["graphical model", "approximation algorithm"]}
{"title": "a multiresolution deep learning framework for automated annotation of reflectance confocal microscopy images", "abstract": "morphological tissue patterns in rcm images are critical in diagnosis of melanocytic lesions . we present a multiresolution deep learning framework that can automatically annotate rcm images for these diagnostic patterns with high sensitivity and specificity", "topics": ["test set", "high- and low-level"]}
{"title": "learning latent representations in neural networks for clustering through pseudo supervision and graph-based activity regularization", "abstract": "in this paper , we propose a novel unsupervised clustering approach exploiting the hidden information that is indirectly introduced through a pseudo classification objective . specifically , we randomly assign a pseudo parent-class label to each observation which is then modified by applying the domain specific transformation associated with the assigned label . generated pseudo observation-label pairs are subsequently used to train a neural network with auto-clustering output layer ( acol ) that introduces multiple softmax nodes for each pseudo parent-class . due to the unsupervised objective based on graph-based activity regularization ( gar ) terms , softmax duplicates of each parent-class are specialized as the hidden information captured through the help of domain specific transformations is propagated during training . ultimately we obtain a k-means friendly latent representation . furthermore , we demonstrate how the chosen transformation type impacts performance and helps propagate the latent information that is useful in revealing unknown clusters . our results show state-of-the-art performance for unsupervised clustering tasks on mnist , svhn and usps datasets , with the highest accuracies reported to date in the literature .", "topics": ["cluster analysis", "neural networks"]}
{"title": "exploiting syntactic structure for natural language modeling", "abstract": "the thesis presents an attempt at using the syntactic structure in natural language for improved language models for speech recognition . the structured language model merges techniques in automatic parsing and language modeling using an original probabilistic parameterization of a shift-reduce parser . a maximum likelihood reestimation procedure belonging to the class of expectation-maximization algorithms is employed for training the model . experiments on the wall street journal , switchboard and broadcast news corpora show improvement in both perplexity and word error rate - word lattice rescoring - over the standard 3-gram language model . the significance of the thesis lies in presenting an original approach to language modeling that uses the hierarchical - syntactic - structure in natural language to improve on current 3-gram modeling techniques for large vocabulary speech recognition .", "topics": ["natural language", "parsing"]}
{"title": "a novel hopfield neural network approach for minimizing total weighted tardiness of jobs scheduled on identical machines", "abstract": "this paper explores fast , polynomial time heuristic approximate solutions to the np-hard problem of scheduling jobs on n identical machines . the jobs are independent and are allowed to be stopped and restarted on another machine at a later time . they have well-defined deadlines , and relative priorities quantified by non-negative real weights . the objective is to find schedules which minimize the total weighted tardiness ( twt ) of all jobs . we show how this problem can be mapped into quadratic form and present a polynomial time heuristic solution based on the hopfield neural network ( hnn ) approach . it is demonstrated , through the results of extensive numerical simulations , that this solution outperforms other popular heuristic methods . the proposed heuristic is both theoretically and empirically shown to be scalable to large problem sizes ( over 100 jobs to be scheduled ) , which makes it applicable to grid computing scheduling , arising in fields such as computational biology , chemistry and finance .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "peaksegjoint : fast supervised peak detection via joint segmentation of multiple count data samples", "abstract": "joint peak detection is a central problem when comparing samples in genomic data analysis , but current algorithms for this task are unsupervised and limited to at most 2 sample types . we propose peaksegjoint , a new constrained maximum likelihood segmentation model for any number of sample types . to select the number of peaks in the segmentation , we propose a supervised penalty learning model . to infer the parameters of these two models , we propose to use a discrete optimization heuristic for the segmentation , and convex optimization for the penalty learning . in comparisons with state-of-the-art peak detection algorithms , peaksegjoint achieves similar accuracy , faster speeds , and a more interpretable model with overlapping peaks that occur in exactly the same positions across all samples .", "topics": ["heuristic"]}
{"title": "brain tumor detection based on mathematical analysis and symmetry information", "abstract": "image segmentation some of the challenging issues on brain magnetic resonance image tumor segmentation caused by the weak correlation between magnetic resonance imaging intensity and anatomical meaning.with the objective of utilizing more meaningful information to improve brain tumor segmentation , an approach which employs bilateral symmetry information as an additional feature for segmentation is proposed.this is motivated by potential performance improvement in the general automatic brain tumor segmentation systems which are important for many medical and scientific applications.brain magnetic resonance imaging segmentation is a complex problem in the field of medical imaging despite various presented methods.mr image of human brain can be divided into several sub-regions especially soft tissues such as gray matter , white matter and cerebra spinal fluid.although edge information is the main clue in image segmentation , it can not get a better result in analysis the content of images without combining other information.our goal is to detect the position and boundary of tumors automatically.experiments were conducted on real pictures , and the results show that the algorithm is flexible and convenient .", "topics": ["image segmentation"]}
{"title": "soap vs rest : comparing a master-slave ga implementation", "abstract": "in this paper , a high-level comparison of both soap ( simple object access protocol ) and rest ( representational state transfer ) is made . these are the two main approaches for interfacing to the web with web services . both approaches are different and present some advantages and disadvantages for interfacing to web services : soap is conceptually more difficult ( has a steeper learning curve ) and more `` heavy-weight '' than rest , although it lacks of standards support for security . in order to test their eficiency ( in time ) , two experiments have been performed using both technologies : a client-server model implementation and a master-slave based genetic algorithm ( ga ) . the results obtained show clear differences in time between soap and rest implementations . although both techniques are suitable for developing parallel systems , soap is heavier than rest , mainly due to the verbosity of soap communications ( xml increases the time taken to parse the messages ) .", "topics": ["high- and low-level", "parsing"]}
{"title": "large-scale domain adaptation via teacher-student learning", "abstract": "high accuracy speech recognition requires a large amount of transcribed data for supervised training . in the absence of such data , domain adaptation of a well-trained acoustic model can be performed , but even here , high accuracy usually requires significant labeled data from the target domain . in this work , we propose an approach to domain adaptation that does not require transcriptions but instead uses a corpus of unlabeled parallel data , consisting of pairs of samples from the source domain of the well-trained model and the desired target domain . to perform adaptation , we employ teacher/student ( t/s ) learning , in which the posterior probabilities generated by the source-domain model can be used in lieu of labels to train the target-domain model . we evaluate the proposed approach in two scenarios , adapting a clean acoustic model to noisy speech and adapting an adults speech acoustic model to children speech . significant improvements in accuracy are obtained , with reductions in word error rate of up to 44 % over the original source model without the need for transcribed data in the target domain . moreover , we show that increasing the amount of unlabeled data results in additional model robustness , which is particularly beneficial when using simulated training data in the target-domain .", "topics": ["test set", "simulation"]}
{"title": "a divide-and-conquer approach to compressed sensing mri", "abstract": "compressed sensing ( cs ) theory assures us that we can accurately reconstruct magnetic resonance images using fewer k-space measurements than the nyquist sampling rate requires . in traditional cs-mri inversion methods , the fact that the energy within the fourier measurement domain is distributed non-uniformly is often neglected during reconstruction . as a result , more densely sampled low-frequency information tends to dominate penalization schemes for reconstructing mri at the expense of high-frequency details . in this paper , we propose a new framework for cs-mri inversion in which we decompose the observed k-space data into `` subspaces '' via sets of filters in a lossless way , and reconstruct the images in these various spaces individually using off-the-shelf algorithms . we then fuse the results to obtain the final reconstruction . in this way we are able to focus reconstruction on frequency information within the entire k-space more equally , preserving both high and low frequency details . we demonstrate that the proposed framework is competitive with state-of-the-art methods in cs-mri in terms of quantitative performance , and often improves an algorithm 's results qualitatively compared with it 's direct application to k-space .", "topics": ["sampling ( signal processing )"]}
{"title": "multiset-valued linear index grammars : imposing dominance constraints on derivations", "abstract": "this paper defines multiset-valued linear index grammar and unordered vector grammar with dominance links . the former models certain uses of multiset-valued feature structures in unification-based formalisms , while the latter is motivated by word order variation and by `` quasi-trees '' , a generalization of trees . the two formalisms are weakly equivalent , and an important subset is at most context-sensitive and polynomially parsable .", "topics": ["parsing"]}
{"title": "learning image conditioned label space for multilabel classification", "abstract": "this work addresses the task of multilabel image classification . inspired by the great success from deep convolutional neural networks ( cnns ) for single-label visual-semantic embedding , we exploit extending these models for multilabel images . specifically , we propose an image-dependent ranking model , which returns a ranked list of labels according to its relevance to the input image . in contrast to conventional cnn models that learn an image representation ( i.e . the image embedding vector ) , the developed model learns a mapping ( i.e . a transformation matrix ) from an image in an attempt to differentiate between its relevant and irrelevant labels . despite the conceptual simplicity of our approach , experimental results on a public benchmark dataset demonstrate that the proposed model achieves state-of-the-art performance while using fewer training images than other multilabel classification methods .", "topics": ["computer vision", "relevance"]}
{"title": "the vglc : the video game level corpus", "abstract": "levels are a key component of many different video games , and a large body of work has been produced on how to procedurally generate game levels . recently , machine learning techniques have been applied to video game level generation towards the purpose of automatically generating levels that have the properties of the training corpus . towards that end we have made available a corpora of video game levels in an easy to parse format ideal for different machine learning and other game ai research purposes .", "topics": ["text corpus", "parsing"]}
{"title": "bidirectional helmholtz machines", "abstract": "efficient unsupervised training and inference in deep generative models remains a challenging problem . one basic approach , called helmholtz machine , involves training a top-down directed generative model together with a bottom-up auxiliary model used for approximate inference . recent results indicate that better generative models can be obtained with better approximate inference procedures . instead of improving the inference procedure , we here propose a new model which guarantees that the top-down and bottom-up distributions can efficiently invert each other . we achieve this by interpreting both the top-down and the bottom-up directed models as approximate inference distributions and by defining the model distribution to be the geometric mean of these two . we present a lower-bound for the likelihood of this model and we show that optimizing this bound regularizes the model so that the bhattacharyya distance between the bottom-up and top-down approximate distributions is minimized . this approach results in state of the art generative models which prefer significantly deeper architectures while it allows for orders of magnitude more efficient approximate inference .", "topics": ["generative model", "approximation algorithm"]}
{"title": "recurrent mixture density network for spatiotemporal visual attention", "abstract": "in many computer vision tasks , the relevant information to solve the problem at hand is mixed to irrelevant , distracting information . this has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient , e.g . , by down-weighting irrelevant pixels . in this work , we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data . we model visual attention with a mixture of gaussians at each frame . this distribution is used to express the probability of saliency for each pixel . time consistency in videos is modeled hierarchically by : 1 ) deep 3d convolutional features to represent spatial and short-term time relations and 2 ) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds . the parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data , without knowledge of the action in each video . our experiments on hollywood2 show state-of-the-art performance on saliency prediction for video . we also show that our attentional model trained on hollywood2 generalizes well to ucf101 and it can be leveraged to improve action classification accuracy on both datasets .", "topics": ["test set", "computer vision"]}
{"title": "on consistency of optimal pricing algorithms in repeated posted-price auctions with strategic buyer", "abstract": "we study revenue optimization learning algorithms for repeated posted-price auctions where a seller interacts with a single strategic buyer that holds a fixed private valuation for a good and seeks to maximize his cumulative discounted surplus . for this setting , first , we propose a novel algorithm that never decreases offered prices and has a tight strategic regret bound in $ \\theta ( \\log\\log t ) $ under some mild assumptions on the buyer surplus discounting . this result closes the open research question on the existence of a no-regret horizon-independent weakly consistent pricing . the proposed algorithm is inspired by our observation that a double decrease of offered prices in a weakly consistent algorithm is enough to cause a linear regret . this motivates us to construct a novel transformation that maps a right-consistent algorithm to a weakly consistent one that never decreases offered prices . second , we outperform the previously known strategic regret upper bound of the algorithm prrfes , where the improvement is achieved by means of a finer constant factor $ c $ of the principal term $ c\\log\\log t $ in this upper bound . finally , we generalize results on strategic regret previously known for geometric discounting of the buyer 's surplus to discounting of other types , namely : the optimality of the pricing prrfes to the case of geometrically concave decreasing discounting ; and linear lower bound on the strategic regret of a wide range of horizon-independent weakly consistent algorithms to the case of arbitrary discounts .", "topics": ["regret ( decision theory )", "value ( ethics )"]}
{"title": "free energy and the generalized optimality equations for sequential decision making", "abstract": "the free energy functional has recently been proposed as a variational principle for bounded rational decision-making , since it instantiates a natural trade-off between utility gains and information processing costs that can be axiomatically derived . here we apply the free energy principle to general decision trees that include both adversarial and stochastic environments . we derive generalized sequential optimality equations that not only include the bellman optimality equations as a limit case , but also lead to well-known decision-rules such as expectimax , minimax and expectiminimax . we show how these decision-rules can be derived from a single free energy principle that assigns a resource parameter to each node in the decision tree . these resource parameters express a concrete computational cost that can be measured as the amount of samples that are needed from the distribution that belongs to each node . the free energy principle therefore provides the normative basis for generalized optimality equations that account for both adversarial and stochastic environments .", "topics": ["calculus of variations"]}
{"title": "using the random sprays retinex algorithm for global illumination estimation", "abstract": "in this paper the use of random sprays retinex ( rsr ) algorithm for global illumination estimation is proposed and its feasibility tested . like other algorithms based on the retinex model , rsr also provides local illumination estimation and brightness adjustment for each pixel and it is faster than other path-wise retinex algorithms . as the assumption of the uniform illumination holds in many cases , it should be possible to use the mean of local illumination estimations of rsr as a global illumination estimation for images with ( assumed ) uniform illumination allowing also the accuracy to be easily measured . therefore we propose a method for estimating global illumination estimation based on local rsr results . to our best knowledge this is the first time that rsr algorithm is used to obtain global illumination estimation . for our tests we use a publicly available color constancy image database for testing . the results are presented and discussed and it turns out that the proposed method outperforms many existing unsupervised color constancy algorithms . the source code is available at http : //www.fer.unizg.hr/ipg/resources/color_constancy/ .", "topics": ["sampling ( signal processing )", "pixel"]}
{"title": "deep clustering via joint convolutional autoencoder embedding and relative entropy minimization", "abstract": "image clustering is one of the most important computer vision applications , which has been extensively studied in literature . however , current clustering methods mostly suffer from lack of efficiency and scalability when dealing with large-scale and high-dimensional data . in this paper , we propose a new clustering model , called deep embedded regularized clustering ( depict ) , which efficiently maps data into a discriminative embedding subspace and precisely predicts cluster assignments . depict generally consists of a multinomial logistic regression function stacked on top of a multi-layer convolutional autoencoder . we define a clustering objective function using relative entropy ( kl divergence ) minimization , regularized by a prior for the frequency of cluster assignments . an alternating strategy is then derived to optimize the objective by updating parameters and estimating cluster assignments . furthermore , we employ the reconstruction loss functions in our autoencoder , as a data-dependent regularization term , to prevent the deep embedding function from overfitting . in order to benefit from end-to-end optimization and eliminate the necessity for layer-wise pretraining , we introduce a joint learning framework to minimize the unified clustering and reconstruction loss functions together and train all network layers simultaneously . experimental results indicate the superiority and faster running time of depict in real-world clustering tasks , where no labeled data is available for hyper-parameter tuning .", "topics": ["cluster analysis", "time complexity"]}
{"title": "transfer learning for improving speech emotion classification accuracy", "abstract": "the majority of existing speech emotion recognition research focuses on automatic emotion detection using training and testing data from same corpus collected under the same conditions . the performance of such systems has been shown to drop significantly in cross-corpus and cross-language scenarios . to address the problem , this paper exploits a transfer learning technique to improve the performance of speech emotion recognition systems that is novel in cross-language and cross-corpus scenarios . evaluations on five different corpora in three different languages show that deep belief networks ( dbns ) offer better accuracy than previous approaches on cross-corpus emotion recognition , relative to a sparse autoencoder and svm baseline system . results also suggest that using a large number of languages for training and using a small fraction of the target data in training can significantly boost accuracy compared with baseline also for the corpus with limited training examples .", "topics": ["text corpus", "bayesian network"]}
{"title": "dear sir or madam , may i introduce the yafc corpus : corpus , benchmarks and metrics for formality style transfer", "abstract": "style transfer is the task of automatically transforming a piece of text in one particular style into another . a major barrier to progress in this field has been a lack of training and evaluation datasets , as well as benchmarks and automatic metrics . in this work , we create the largest corpus for a particular stylistic transfer ( formality ) and show that techniques from the machine translation community can serve as strong baselines for future work . we also discuss challenges of using automatic metrics .", "topics": ["machine translation"]}
{"title": "convergence analysis of optimization algorithms", "abstract": "the regret bound of an optimization algorithms is one of the basic criteria for evaluating the performance of the given algorithm . by inspecting the differences between the regret bounds of traditional algorithms and adaptive one , we provide a guide for choosing an optimizer with respect to the given data set and the loss function . for analysis , we assume that the loss function is convex and its gradient is lipschitz continuous .", "topics": ["regret ( decision theory )", "mathematical optimization"]}
{"title": "search-space characterization for real-time heuristic search", "abstract": "recent real-time heuristic search algorithms have demonstrated outstanding performance in video-game pathfinding . however , their applications have been thus far limited to that domain . we proceed with the aim of facilitating wider applications of real-time search by fostering a greater understanding of the performance of recent algorithms . we first introduce eight algorithm-independent complexity measures for search spaces and correlate their values with algorithm performance . the complexity measures are statistically shown to be significant predictors of algorithm performance across a set of commercial video-game maps . we then extend this analysis to a wider variety of search spaces in the first application of database-driven real-time search to domains outside of video-game pathfinding . in doing so , we gain insight into algorithm performance and possible enhancement as well as into search space complexity .", "topics": ["map", "heuristic"]}
{"title": "unifying the stochastic spectral descent for restricted boltzmann machines with bernoulli or gaussian inputs", "abstract": "stochastic gradient descent based algorithms are typically used as the general optimization tools for most deep learning models . a restricted boltzmann machine ( rbm ) is a probabilistic generative model that can be stacked to construct deep architectures . for rbm with bernoulli inputs , non-euclidean algorithm such as stochastic spectral descent ( ssd ) has been specifically designed to speed up the convergence with improved use of the gradient estimation by sampling methods . however , the existing algorithm and corresponding theoretical justification depend on the assumption that the possible configurations of inputs are finite , like binary variables . the purpose of this paper is to generalize ssd for gaussian rbm being capable of mod- eling continuous data , regardless of the previous assumption . we propose the gradient descent methods in non-euclidean space of parameters , via de- riving the upper bounds of logarithmic partition function for rbms based on schatten-infinity norm . we empirically show that the advantage and improvement of ssd over stochastic gradient descent ( sgd ) .", "topics": ["sampling ( signal processing )", "generative model"]}
{"title": "neurohex : a deep q-learning hex agent", "abstract": "deepmind 's recent spectacular success in using deep convolutional neural nets and machine learning to build superhuman level agents -- - e.g . for atari games via deep q-learning and for the game of go via reinforcement learning -- - raises many questions , including to what extent these methods will succeed in other domains . in this paper we consider dql for the game of hex : after supervised initialization , we use selfplay to train neurohex , an 11-layer cnn that plays hex on the 13x13 board . hex is the classic two-player alternate-turn stone placement game played on a rhombus of hexagonal cells in which the winner is whomever connects their two opposing sides . despite the large action and state space , our system trains a q-network capable of strong play with no search . after two weeks of q-learning , neurohex achieves win-rates of 20.4 % as first player and 2.1 % as second player against a 1-second/move version of mohex , the current icga olympiad hex champion . our data suggests further improvement might be possible with more training time .", "topics": ["value ( ethics )", "reinforcement learning"]}
{"title": "indoor scene understanding in 2.5/3d : a survey", "abstract": "with the availability of low-cost and compact 2.5/3d visual sensing devices , computer vision community is experiencing a growing interest in visual scene understanding . this survey paper provides a comprehensive background to this research topic . we begin with a historical perspective , followed by popular 3d data representations and a comparative analysis of available datasets . before delving into the application specific details , this survey provides a succinct introduction to the core technologies that are the underlying methods extensively used in the literature . afterwards , we review the developed techniques according to a taxonomy based on the scene understanding tasks . this covers holistic indoor scene understanding as well as subtasks such as scene classification , object detection , pose estimation , semantic segmentation , 3d reconstruction , saliency detection , physics-based reasoning and affordance prediction . later on , we summarize the performance metrics used for evaluation in different tasks and a quantitative comparison among the recent state-of-the-art techniques . we conclude this review with the current challenges and an outlook towards the open research problems requiring further investigation .", "topics": ["object detection", "computer vision"]}
{"title": "visual7w : grounded question answering in images", "abstract": "we have seen great progress in basic perceptual tasks such as object recognition and detection . however , ai models still fail to match humans in high-level vision tasks due to the lack of capacities for deeper reasoning . recently the new task of visual question answering ( qa ) has been proposed to evaluate a model 's capacity for deep image understanding . previous works have established a loose , global association between qa sentences and images . however , many questions and answers , in practice , relate to local regions in the images . we establish a semantic link between textual descriptions and image regions by object-level grounding . it enables a new type of qa with visual answers , in addition to textual answers used in previous work . we study the visual qa tasks in a grounded setting with a large collection of 7w multiple-choice qa pairs . furthermore , we evaluate human performance and several baseline models on the qa tasks . finally , we propose a novel lstm model with spatial attention to tackle the 7w qa tasks .", "topics": ["baseline ( configuration management )", "high- and low-level"]}
{"title": "caosdb - research data management for complex , changing , and automated research workflows", "abstract": "here we present caosdb , a research data management system ( rdms ) designed to ensure seamless integration of inhomogeneous data sources and repositories of legacy data . its primary purpose is the management of data from biomedical sciences , both from simulations and experiments during the complete research data lifecycle . an rdms for this domain faces particular challenges : research data arise in huge amounts , from a wide variety of sources , and traverse a highly branched path of further processing . to be accepted by its users , an rdms must be built around workflows of the scientists and practices and thus support changes in workflow and data structure . nevertheless it should encourage and support the development and observation of standards and furthermore facilitate the automation of data acquisition and processing with specialized software . the storage data model of an rdms must reflect these complexities with appropriate semantics and ontologies while offering simple methods for finding , retrieving , and understanding relevant data . we show how caosdb responds to these challenges and give an overview of the caosdb server , its data model and its easy-to-learn caosdb query language . we briefly discuss the status of the implementation , how we currently use caosdb , and how we plan to use and extend it .", "topics": ["nonlinear system"]}
{"title": "cross-validated variable selection in tree-based methods improves predictive performance", "abstract": "recursive partitioning approaches producing tree-like models are a long standing staple of predictive modeling , in the last decade mostly as `` sub-learners '' within state of the art ensemble methods like boosting and random forest . however , a fundamental flaw in the partitioning ( or splitting ) rule of commonly used tree building methods precludes them from treating different types of variables equally . this most clearly manifests in these methods ' inability to properly utilize categorical variables with a large number of categories , which are ubiquitous in the new age of big data . such variables can often be very informative , but current tree methods essentially leave us a choice of either not using them , or exposing our models to severe overfitting . we propose a conceptual framework to splitting using leave-one-out ( loo ) cross validation for selecting the splitting variable , then performing a regular split ( in our case , following cart 's approach ) for the selected variable . the most important consequence of our approach is that categorical variables with many categories can be safely used in tree building and are only chosen if they contribute to predictive power . we demonstrate in extensive simulation and real data analysis that our novel splitting approach significantly improves the performance of both single tree models and ensemble methods that utilize trees . importantly , we design an algorithm for loo splitting variable selection which under reasonable assumptions does not increase the overall computational complexity compared to cart for two-class classification . for regression tasks , our approach carries an increased computational burden , replacing a o ( log ( n ) ) factor in cart splitting rule search with an o ( n ) term .", "topics": ["computational complexity theory", "simulation"]}
{"title": "a novel approach to fast image filtering algorithm of infrared images based on intro sort algorithm", "abstract": "in this study we investigate the fast image filtering algorithm based on intro sort algorithm and fast noise reduction of infrared images . main feature of the proposed approach is that no prior knowledge of noise required . it is developed based on stefan- boltzmann law and the fourier law . we also investigate the fast noise reduction approach that has advantage of less computation load . in addition , it can retain edges , details , text information even if the size of the window increases . intro sort algorithm begins with quick sort and switches to heap sort when the recursion depth exceeds a level based on the number of elements being sorted . this approach has the advantage of fast noise reduction by reducing the comparison time . it also significantly speed up the noise reduction process and can apply to real-time image processing . this approach will extend the infrared images applications for medicine and video conferencing .", "topics": ["image processing", "noise reduction"]}
{"title": "scalable pooled time series of big video data from the deep web", "abstract": "we contribute a scalable implementation of ryoo et al 's pooled time series algorithm from cvpr 2015 . the updated algorithm has been evaluated on a large and diverse dataset of approximately 6800 videos collected from a crawl of the deep web related to human trafficking on darpa 's memex effort . we describe the properties of pooled time series and the motivation for using it to relate videos collected from the deep web . we highlight issues that we found while running pooled time series on larger datasets and discuss solutions for those issues . our solution centers are re-imagining pooled time series as a hadoop-based algorithm in which we compute portions of the eventual solution in parallel on large commodity clusters . we demonstrate that our new hadoop-based algorithm works well on the 6800 video dataset and shares all of the properties described in the cvpr 2015 paper . we suggest avenues of future work in the project .", "topics": ["time series", "scalability"]}
{"title": "training deep convolutional neural networks with resistive cross-point devices", "abstract": "in a previous work we have detailed the requirements to obtain a maximal performance benefit by implementing fully connected deep neural networks ( dnn ) in form of arrays of resistive devices for deep learning . this concept of resistive processing unit ( rpu ) devices we extend here towards convolutional neural networks ( cnns ) . we show how to map the convolutional layers to rpu arrays such that the parallelism of the hardware can be fully utilized in all three cycles of the backpropagation algorithm . we find that the noise and bound limitations imposed due to analog nature of the computations performed on the arrays effect the training accuracy of the cnns . noise and bound management techniques are presented that mitigate these problems without introducing any additional complexity in the analog circuits and can be addressed by the digital circuits . in addition , we discuss digitally programmable update management and device variability reduction techniques that can be used selectively for some of the layers in a cnn . we show that combination of all those techniques enables a successful application of the rpu concept for training cnns . the techniques discussed here are more general and can be applied beyond cnn architectures and therefore enables applicability of rpu approach for large class of neural network architectures .", "topics": ["computation"]}
{"title": "cooperative starting movement detection of cyclists using convolutional neural networks and a boosted stacking ensemble", "abstract": "in future , vehicles and other traffic participants will be interconnected and equipped with various types of sensors , allowing for cooperation on different levels , such as situation prediction or intention detection . in this article we present a cooperative approach for starting movement detection of cyclists using a boosted stacking ensemble approach realizing feature- and decision level cooperation . we introduce a novel method based on a 3d convolutional neural network ( cnn ) to detect starting motions on image sequences by learning spatio-temporal features . the cnn is complemented by a smart device based starting movement detection originating from smart devices carried by the cyclist . both model outputs are combined in a stacking ensemble approach using an extreme gradient boosting classifier resulting in a fast and yet robust cooperative starting movement detector . we evaluate our cooperative approach on real-world data originating from experiments with 49 test subjects consisting of 84 starting motions .", "topics": ["gradient", "gradient"]}
{"title": "decision making agent searching for markov models in near-deterministic world", "abstract": "reinforcement learning has solid foundations , but becomes inefficient in partially observed ( non-markovian ) environments . thus , a learning agent -born with a representation and a policy- might wish to investigate to what extent the markov property holds . we propose a learning architecture that utilizes combinatorial policy optimization to overcome non-markovity and to develop efficient behaviors , which are easy to inherit , tests the markov property of the behavioral states , and corrects against non-markovity by running a deterministic factored finite state model , which can be learned . we illustrate the properties of architecture in the near deterministic ms. pac-man game . we analyze the architecture from the point of view of evolutionary , individual , and social learning .", "topics": ["reinforcement learning", "markov chain"]}
{"title": "gromov-hausdorff stability of linkage-based hierarchical clustering methods", "abstract": "a hierarchical clustering method is stable if small perturbations on the data set produce small perturbations in the result . these perturbations are measured using the gromov-hausdorff metric . we study the problem of stability on linkage-based hierarchical clustering methods . we obtain that , under some basic conditions , standard linkage-based methods are semi-stable . this means that they are stable if the input data is close enough to an ultrametric space . we prove that , apart from exotic examples , introducing any unchaining condition in the algorithm always produces unstable methods .", "topics": ["cluster analysis"]}
{"title": "optimization of ofdm radar waveforms using genetic algorithms", "abstract": "in this paper , we present our investigations on the use of single objective and multiobjective genetic algorithms based optimisation algorithms to improve the design of ofdm pulses for radar . we discuss these optimization procedures in the scope of a waveform design intended for two different radar processing solutions . lastly , we show how the encoding solution is suited to permit the optimizations of waveform for ofdm radar related challenges such as enhanced detection .", "topics": ["mathematical optimization"]}
{"title": "reptile : a scalable metalearning algorithm", "abstract": "this paper considers metalearning problems , where there is a distribution of tasks , and we would like to obtain an agent that performs well ( i.e . , learns quickly ) when presented with a previously unseen task sampled from this distribution . we present a remarkably simple metalearning algorithm called reptile , which learns a parameter initialization that can be fine-tuned quickly on a new task . reptile works by repeatedly sampling a task , training on it , and moving the initialization towards the trained weights on that task . unlike maml , which also learns an initialization , reptile does n't require differentiating through the optimization process , making it more suitable for optimization problems where many update steps are required . we show that reptile performs well on some well-established benchmarks for few-shot classification . we provide some theoretical analysis aimed at understanding why reptile works .", "topics": ["sampling ( signal processing )"]}
{"title": "continuous features discretization for anomaly intrusion detectors generation", "abstract": "network security is a growing issue , with the evolution of computer systems and expansion of attacks . biological systems have been inspiring scientists and designs for new adaptive solutions , such as genetic algorithms . in this paper , we present an approach that uses the genetic algorithm to generate anomaly net- work intrusion detectors . in this paper , an algorithm propose use a discretization method for the continuous features selected for the intrusion detection , to create some homogeneity between values , which have different data types . then , the intrusion detection system is tested against the nsl-kdd data set using different distance methods . a comparison is held amongst the results , and it is shown by the end that this proposed approach has good results , and recommendations is given for future experiments .", "topics": ["data mining"]}
{"title": "cur decompositions , similarity matrices , and subspace clustering", "abstract": "a general framework for solving the subspace clustering problem using the cur decomposition is presented . the cur decomposition provides a natural way to construct similarity matrices for data that come from a union of unknown subspaces $ \\mathscr { u } =\\underset { i=1 } { \\overset { m } \\bigcup } s_i $ . the similarity matrices thus constructed give the exact clustering in the noise-free case . a simple adaptation of the technique also allows clustering of noisy data . two known methods for subspace clustering can be derived from the cur technique . experiments on synthetic and real data are presented to test the method .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "learning temporal regularity in video sequences", "abstract": "perceiving meaningful activities in a long video sequence is a challenging problem due to ambiguous definition of 'meaningfulness ' as well as clutters in the scene . we approach this problem by learning a generative model for regular motion patterns , termed as regularity , using multiple sources with very limited supervision . specifically , we propose two methods that are built upon the autoencoders for their ability to work with little to no supervision . we first leverage the conventional handcrafted spatio-temporal local features and learn a fully connected autoencoder on them . second , we build a fully convolutional feed-forward autoencoder to learn both the local features and the classifiers as an end-to-end learning framework . our model can capture the regularities from multiple datasets . we evaluate our methods in both qualitative and quantitative ways - showing the learned regularity of videos in various aspects and demonstrating competitive performance on anomaly detection datasets as an application .", "topics": ["generative model", "end-to-end principle"]}
{"title": "towards distortion-predictable embedding of neural networks", "abstract": "current research in computer vision has shown that convolutional neural networks ( cnn ) give state-of-the-art performance in many classification tasks and computer vision problems . the embedding of cnn , which is the internal representation produced by the last layer , can indirectly learn topological and relational properties . moreover , by using a suitable loss function , cnn models can learn invariance to a wide range of non-linear distortions such as rotation , viewpoint angle or lighting condition . in this work , new insights are discovered about cnn embeddings and a new loss function is proposed , derived from the contrastive loss , that creates models with more predicable mappings and also quantifies distortions . in typical distortion-dependent methods , there is no simple relation between the features corresponding to one image and the features of this image distorted . therefore , these methods require to feed-forward inputs under every distortions in order to find the corresponding features representations . our contribution makes a step towards embeddings where features of distorted inputs are related and can be derived from each others by the intensity of the distortion .", "topics": ["nonlinear system", "neural networks"]}
{"title": "understanding intermediate layers using linear classifier probes", "abstract": "neural network models have a reputation for being black boxes . we propose a new method to understand better the roles and dynamics of the intermediate layers . this has direct consequences on the design of such models and it enables the expert to be able to justify certain heuristics ( such as the auxiliary heads in the inception model ) . our method uses linear classifiers , referred to as `` probes '' , where a probe can only use the hidden units of a given intermediate layer as discriminating features . moreover , these probes can not affect the training phase of a model , and they are generally added after training . they allow the user to visualize the state of the model at multiple steps of training . we demonstrate how this can be used to develop a better intuition about a known model and to diagnose potential problems .", "topics": ["heuristic"]}
{"title": "distributed preemption decisions : probabilistic graphical model , algorithm and near-optimality", "abstract": "cooperative decision making is a vision of future network management and control . distributed connection preemption is an important example where nodes can make intelligent decisions on allocating resources and controlling traffic flows for multi-class service networks . a challenge is that nodal decisions are spatially dependent as traffic flows trespass multiple nodes in a network . hence the performance-complexity trade-off becomes important , i.e . , how accurate decisions are versus how much information is exchanged among nodes . connection preemption is known to be np-complete . centralized preemption is optimal but computationally intractable . decentralized preemption is computationally efficient but may result in a poor performance . this work investigates distributed preemption where nodes decide whether and which flows to preempt using only local information exchange with neighbors . we develop , based on the probabilistic graphical models , a near-optimal distributed algorithm . the algorithm is used by each node to make collectively near-optimal preemption decisions . we study trade-offs between near-optimal performance and complexity that corresponds to the amount of information-exchange of the distributed algorithm . the algorithm is validated by both analysis and simulation .", "topics": ["graphical model", "computational complexity theory"]}
{"title": "outlier robust system identification : a bayesian kernel-based approach", "abstract": "in this paper , we propose an outlier-robust regularized kernel-based method for linear system identification . the unknown impulse response is modeled as a zero-mean gaussian process whose covariance ( kernel ) is given by the recently proposed stable spline kernel , which encodes information on regularity and exponential stability . to build robustness to outliers , we model the measurement noise as realizations of independent laplacian random variables . the identification problem is cast in a bayesian framework , and solved by a new markov chain monte carlo ( mcmc ) scheme . in particular , exploiting the representation of the laplacian random variables as scale mixtures of gaussians , we design a gibbs sampler which quickly converges to the target distribution . numerical simulations show a substantial improvement in the accuracy of the estimates over state-of-the-art kernel-based methods .", "topics": ["sampling ( signal processing )", "kernel ( operating system )"]}
{"title": "proceedings of the twentieth conference on uncertainty in artificial intelligence ( 2004 )", "abstract": "this is the proceedings of the twentieth conference on uncertainty in artificial intelligence , which was held in banff , canada , july 7 - 11 2004 .", "topics": ["artificial intelligence"]}
{"title": "deviant learning algorithm : learning sparse mismatch representations through time and space", "abstract": "predictive coding ( pdc ) has recently attracted attention in the neuroscience and computing community as a candidate unifying paradigm for neuronal studies and artificial neural network implementations particularly targeted at unsupervised learning systems . the mismatch negativity ( mmn ) has also recently been studied in relation to pc and found to be a useful ingredient in neural predictive coding systems . backed by the behavior of living organisms , such networks are particularly useful in forming spatio-temporal transitions and invariant representations of the input world . however , most neural systems still do not account for large number of synapses even though this has been shown by a few machine learning researchers as an effective and very important component of any neural system if such a system is to behave properly . our major point here is that pdc systems with the mmn effect in addition to a large number of synapses can greatly improve any neural learning system 's performance and ability to make decisions in the machine world . in this paper , we propose a novel bio-mimetic computational intelligence algorithm -- the deviant learning algorithm , inspired by these key ideas and functional properties of recent brain-cognitive discoveries and theories . we also show by numerical experiments guided by theoretical insights , how our invented bio-mimetic algorithm can achieve competitive predictions even with very small problem specific data .", "topics": ["unsupervised learning"]}
{"title": "redundant wavelets on graphs and high dimensional data clouds", "abstract": "in this paper , we propose a new redundant wavelet transform applicable to scalar functions defined on high dimensional coordinates , weighted graphs and networks . the proposed transform utilizes the distances between the given data points . we modify the filter-bank decomposition scheme of the redundant wavelet transform by adding in each decomposition level linear operators that reorder the approximation coefficients . these reordering operators are derived by organizing the tree-node features so as to shorten the path that passes through these points . we explore the use of the proposed transform to image denoising , and show that it achieves denoising results that are close to those obtained with the bm3d algorithm .", "topics": ["noise reduction", "coefficient"]}
{"title": "facial emotion detection using convolutional neural networks and representational autoencoder units", "abstract": "emotion being a subjective thing , leveraging knowledge and science behind labeled data and extracting the components that constitute it , has been a challenging problem in the industry for many years . with the evolution of deep learning in computer vision , emotion recognition has become a widely-tackled research problem . in this work , we propose two independent methods for this very task . the first method uses autoencoders to construct a unique representation of each emotion , while the second method is an 8-layer convolutional neural network ( cnn ) . these methods were trained on the posed-emotion dataset ( jaffe ) , and to test their robustness , both the models were also tested on 100 random images from the labeled faces in the wild ( lfw ) dataset , which consists of images that are candid than posed . the results show that with more fine-tuning and depth , our cnn model can outperform the state-of-the-art methods for emotion recognition . we also propose some exciting ideas for expanding the concept of representational autoencoders to improve their performance .", "topics": ["computer vision", "autoencoder"]}
{"title": "guess who rated this movie : identifying users through subspace clustering", "abstract": "it is often the case that , within an online recommender system , multiple users share a common account . can such shared accounts be identified solely on the basis of the user- provided ratings ? once a shared account is identified , can the different users sharing it be identified as well ? whenever such user identification is feasible , it opens the way to possible improvements in personalized recommendations , but also raises privacy concerns . we develop a model for composite accounts based on unions of linear subspaces , and use subspace clustering for carrying out the identification task . we show that a significant fraction of such accounts is identifiable in a reliable manner , and illustrate potential uses for personalized recommendation .", "topics": ["cluster analysis"]}
{"title": "hierarchic kernel recursive least-squares", "abstract": "we present a new hierarchic kernel based modeling technique for modeling evenly distributed multidimensional datasets that does not rely on input space sparsification . the presented method reorganizes the typical single-layer kernel based model in a hierarchical structure , such that the weights of a kernel model over each dimension are modeled over the adjacent dimension . we show that the imposition of the hierarchical structure in the kernel based model leads to significant computational speedup and improved modeling accuracy ( over an order of magnitude in many cases ) . for instance the presented method is about five times faster and more accurate than sparsified kernel recursive least- squares in modeling of a two-dimensional real-world data set .", "topics": ["kernel ( operating system )"]}
{"title": "deep residual learning for weakly-supervised relation extraction", "abstract": "deep residual learning ( resnet ) is a new method for training very deep neural networks using identity map-ping for shortcut connections . resnet has won the imagenet ilsvrc 2015 classification task , and achieved state-of-the-art performances in many computer vision tasks . however , the effect of residual learning on noisy natural language processing tasks is still not well understood . in this paper , we design a novel convolutional neural network ( cnn ) with residual learning , and investigate its impacts on the task of distantly supervised noisy relation extraction . in contradictory to popular beliefs that resnet only works well for very deep networks , we found that even with 9 layers of cnns , using identity mapping could significantly improve the performance for distantly-supervised relation extraction .", "topics": ["natural language processing", "computer vision"]}
{"title": "total variation minimization and graph cuts for moving objects segmentation", "abstract": "in this paper , we are interested in the application to video segmentation of the discrete shape optimization problem involving the shape weighted perimeter and an additional term depending on a parameter . based on recent works and in particular the one of darbon and sigelle , we justify the equivalence of the shape optimization problem and a weighted total variation regularization . for solving this problem , we adapt the projection algorithm proposed recently for solving the basic tv regularization problem . another solution to the shape optimization investigated here is the graph cut technique . both methods have the advantage to lead to a global minimum . since we can distinguish moving objects from static elements of a scene by analyzing norm of the optical flow vectors , we choose the optical flow norm as initial data . in order to have the contour as close as possible to an edge in the image , we use a classical edge detector function as the weight of the weighted total variation . this model has been used in one of our former works . we also apply the same methods to a video segmentation model used by jehan-besson , barlaud and aubert . in this case , only standard perimeter is incorporated in the shape functional . we also propose another way for finding moving objects by using an a contrario detection of objects on the image obtained by solving the rudin-osher-fatemi total variation regularization problem.we can notice the segmentation can be associated to a level set in the former methods .", "topics": ["optimization problem"]}
{"title": "causal independence for knowledge acquisition and inference", "abstract": "i introduce a temporal belief-network representation of causal independence that a knowledge engineer can use to elicit probabilistic models . like the current , atemporal belief-network representation of causal independence , the new representation makes knowledge acquisition tractable . unlike the atemproal representation , however , the temporal representation can simplify inference , and does not require the use of unobservable variables . the representation is less general than is the atemporal representation , but appears to be useful for many practical applications .", "topics": ["bayesian network", "causality"]}
{"title": "cooperative learning with visual attributes", "abstract": "learning paradigms involving varying levels of supervision have received a lot of interest within the computer vision and machine learning communities . the supervisory information is typically considered to come from a human supervisor -- a `` teacher '' figure . in this paper , we consider an alternate source of supervision -- a `` peer '' -- i.e . a different machine . we introduce cooperative learning , where two agents trying to learn the same visual concepts , but in potentially different environments using different sources of data ( sensors ) , communicate their current knowledge of these concepts to each other . given the distinct sources of data in both agents , the mode of communication between the two agents is not obvious . we propose the use of visual attributes -- semantic mid-level visual properties such as furry , wooden , etc . -- as the mode of communication between the agents . our experiments in three domains -- objects , scenes , and animals -- demonstrate that our proposed cooperative learning approach improves the performance of both agents as compared to their performance if they were to learn in isolation . our approach is particularly applicable in scenarios where privacy , security and/or bandwidth constraints restrict the amount and type of information the two agents can exchange .", "topics": ["reinforcement learning", "computer vision"]}
{"title": "continuous multimodal emotion recognition approach for avec 2017", "abstract": "this paper reports the analysis of audio and visual features in predicting the continuous emotion dimensions under the seventh audio/visual emotion challenge ( avec 2017 ) , which was done as part of a b.tech . 2nd year internship project . for visual features we used the hog ( histogram of gradients ) features , fisher encodings of sift ( scale-invariant feature transform ) features based on gaussian mixture model ( gmm ) and some pretrained convolutional neural network layers as features ; all these extracted for each video clip . for audio features we used the bag-of-audio-words ( boaw ) representation of the llds ( low-level descriptors ) generated by openxbow provided by the organisers of the event . then we trained fully connected neural network regression model on the dataset for all these different modalities . we applied multimodal fusion on the output models to get the concordance correlation coefficient on development set as well as test set .", "topics": ["test set", "high- and low-level"]}
{"title": "a heuristic search approach to planning with continuous resources in stochastic domains", "abstract": "we consider the problem of optimal planning in stochastic domains with resource constraints , where the resources are continuous and the choice of action at each step depends on resource availability . we introduce the hao* algorithm , a generalization of the ao* algorithm that performs search in a hybrid state space that is modeled using both discrete and continuous state variables , where the continuous variables represent monotonic resources . like other heuristic search algorithms , hao* leverages knowledge of the start state and an admissible heuristic to focus computational effort on those parts of the state space that could be reached from the start state by following an optimal policy . we show that this approach is especially effective when resource constraints limit how much of the state space is reachable . experimental results demonstrate its effectiveness in the domain that motivates our research : automated planning for planetary exploration rovers .", "topics": ["heuristic"]}
{"title": "cooperative inverse reinforcement learning", "abstract": "for an autonomous system to be helpful to humans and to pose no unwarranted risks , it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans . we propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning ( cirl ) . a cirl problem is a cooperative , partial-information game with two agents , human and robot ; both are rewarded according to the human 's reward function , but the robot does not initially know what this is . in contrast to classical irl , where the human is assumed to act optimally in isolation , optimal cirl solutions produce behaviors such as active teaching , active learning , and communicative actions that are more effective in achieving value alignment . we show that computing optimal joint policies in cirl games can be reduced to solving a pomdp , prove that optimality in isolation is suboptimal in cirl , and derive an approximate cirl algorithm .", "topics": ["value ( ethics )", "mathematical optimization"]}
{"title": "source-side prediction for neural headline generation", "abstract": "the encoder-decoder model is widely used in natural language generation tasks . however , the model sometimes suffers from repeated redundant generation , misses important phrases , and includes irrelevant entities . toward solving these problems we propose a novel source-side token prediction module . our method jointly estimates the probability distributions over source and target vocabularies to capture a correspondence between source and target tokens . the experiments show that the proposed model outperforms the current state-of-the-art method in the headline generation task . additionally , we show that our method has an ability to learn a reasonable token-wise correspondence without knowing any true alignments .", "topics": ["natural language", "entity"]}
{"title": "reinforcement learning with external knowledge and two-stage q-functions for predicting popular reddit threads", "abstract": "this paper addresses the problem of predicting popularity of comments in an online discussion forum using reinforcement learning , particularly addressing two challenges that arise from having natural language state and action spaces . first , the state representation , which characterizes the history of comments tracked in a discussion at a particular point , is augmented to incorporate the global context represented by discussions on world events available in an external knowledge source . second , a two-stage q-learning framework is introduced , making it feasible to search the combinatorial action space while also accounting for redundancy among sub-actions . we experiment with five reddit communities , showing that the two methods improve over previous reported results on this task .", "topics": ["reinforcement learning", "natural language"]}
{"title": "innateness , alphazero , and artificial intelligence", "abstract": "the concept of innateness is rarely discussed in the context of artificial intelligence . when it is discussed , or hinted at , it is often the context of trying to reduce the amount of innate machinery in a given system . in this paper , i consider as a test case a recent series of papers by silver et al ( silver et al . , 2017a ) on alphago and its successors that have been presented as an argument that a `` even in the most challenging of domains : it is possible to train to superhuman level , without human examples or guidance '' , `` starting tabula rasa . '' i argue that these claims are overstated , for multiple reasons . i close by arguing that artificial intelligence needs greater attention to innateness , and i point to some proposals about what that innateness might look like .", "topics": ["artificial intelligence"]}
{"title": "improving legal information retrieval by distributional composition with term order probabilities", "abstract": "legal professionals worldwide are currently trying to get up-to-pace with the explosive growth in legal document availability through digital means . this drives a need for high efficiency legal information retrieval ( ir ) and question answering ( qa ) methods . the ir task in particular has a set of unique challenges that invite the use of semantic motivated nlp techniques . in this work , a two-stage method for legal information retrieval is proposed , combining lexical statistics and distributional sentence representations in the context of competition on legal information extraction/entailment ( coliee ) . the combination is done with the use of disambiguation rules , applied over the rankings obtained through n-gram statistics . after the ranking is done , its results are evaluated for ambiguity , and disambiguation is done if a result is decided to be unreliable for a given query . competition and experimental results indicate small gains in overall retrieval performance using the proposed approach . additionally , an analysis of error and improvement cases is presented for a better understanding of the contributions .", "topics": ["natural language processing"]}
{"title": "unsupervised discovery of el nino using causal feature learning on microlevel climate data", "abstract": "we show that the climate phenomena of el nino and la nina arise naturally as states of macro-variables when our recent causal feature learning framework ( chalupka 2015 , chalupka 2016 ) is applied to micro-level measures of zonal wind ( zw ) and sea surface temperatures ( sst ) taken over the equatorial band of the pacific ocean . the method identifies these unusual climate states on the basis of the relation between zw and sst patterns without any input about past occurrences of el nino or la nina . the simpler alternatives of ( i ) clustering the sst fields while disregarding their relationship with zw patterns , or ( ii ) clustering the joint zw-sst patterns , do not discover el nino . we discuss the degree to which our method supports a causal interpretation and use a low-dimensional toy example to explain its success over other clustering approaches . finally , we propose a new robust and scalable alternative to our original algorithm ( chalupka 2016 ) , which circumvents the need for high-dimensional density learning .", "topics": ["cluster analysis", "feature learning"]}
{"title": "measurement context extraction from text : discovering opportunities and gaps in earth science", "abstract": "we propose marve , a system for extracting measurement values , units , and related words from natural language text . marve uses conditional random fields ( crf ) to identify measurement values and units , followed by a rule-based system to find related entities , descriptors and modifiers within a sentence . sentence tokens are represented by an undirected graphical model , and rules are based on part-of-speech and word dependency patterns connecting values and units to contextual words . marve is unique in its focus on measurement context and early experimentation demonstrates marve 's ability to generate high-precision extractions with strong recall . we also discuss marve 's role in refining measurement requirements for nasa 's proposed hyspiri mission , a hyperspectral infrared imaging satellite that will study the world 's ecosystems . in general , our work with hyspiri demonstrates the value of semantic measurement extractions in characterizing quantitative discussion contained in large corpuses of natural language text . these extractions accelerate broad , cross-cutting research and expose scientists new algorithmic approaches and experimental nuances . they also facilitate identification of scientific opportunities enabled by hyspiri leading to more efficient scientific investment and research .", "topics": ["graphical model", "natural language"]}
{"title": "fpnn : field probing neural networks for 3d data", "abstract": "building discriminative representations for 3d data has been an important task in computer graphics and computer vision research . convolutional neural networks ( cnns ) have shown to operate on 2d images with great success for a variety of tasks . lifting convolution operators to 3d ( 3dcnns ) seems like a plausible and promising next step . unfortunately , the computational complexity of 3d cnns grows cubically with respect to voxel resolution . moreover , since most 3d geometry representations are boundary based , occupied regions do not increase proportionately with the size of the discretization , resulting in wasted computation . in this work , we represent 3d spaces as volumetric fields , and propose a novel design that employs field probing filters to efficiently extract features from them . each field probing filter is a set of probing points -- - sensors that perceive the space . our learning algorithm optimizes not only the weights associated with the probing points , but also their locations , which deforms the shape of the probing filters and adaptively distributes them in 3d space . the optimized probing points sense the 3d space `` intelligently '' , rather than operating blindly over the entire domain . we show that field probing is significantly more efficient than 3dcnns , while providing state-of-the-art performance , on classification tasks for 3d object recognition benchmark datasets .", "topics": ["computational complexity theory", "computer vision"]}
{"title": "autonomous perceptron neural network inspired from quantum computing", "abstract": "this abstract will be modified after correcting the minor error in eq . ( 2 )", "topics": ["iteration", "computation"]}
{"title": "online but accurate inference for latent variable models with local gibbs sampling", "abstract": "we study parameter inference in large-scale latent variable models . we first propose an unified treatment of online inference for latent variable models from a non-canonical exponential family , and draw explicit links between several previously proposed frequentist or bayesian methods . we then propose a novel inference method for the frequentist estimation of parameters , that adapts mcmc methods to online inference of latent variable models with the proper use of local gibbs sampling . then , for latent dirich-let allocation , we provide an extensive set of experiments and comparisons with existing work , where our new approach outperforms all previously proposed methods . in particular , using gibbs sampling for latent variable inference is superior to variational inference in terms of test log-likelihoods . moreover , bayesian inference through variational methods perform poorly , sometimes leading to worse fits with latent variables of higher dimensionality .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "distributed object medical imaging model", "abstract": "digital medical informatics and images are commonly used in hospitals today , . because of the interrelatedness of the radiology department and other departments , especially the intensive care unit and emergency department , the transmission and sharing of medical images has become a critical issue . our research group has developed a java-based distributed object medical imaging model ( domim ) to facilitate the rapid development and deployment of medical imaging applications in a distributed environment that can be shared and used by related departments and mobile physiciansdomim is a unique suite of multimedia telemedicine applications developed for the use by medical related organizations . the applications support realtime patients ' data , image files , audio and video diagnosis annotation exchanges . the domim enables joint collaboration between radiologists and physicians while they are at distant geographical locations . the domim environment consists of heterogeneous , autonomous , and legacy resources . the common object request broker architecture ( corba ) , java database connectivity ( jdbc ) , and java language provide the capability to combine the domim resources into an integrated , interoperable , and scalable system . the underneath technology , including idl orb , event service , iiop jdbc/odbc , legacy system wrapping and java implementation are explored . this paper explores a distributed collaborative corba/jdbc based framework that will enhance medical information management requirements and development . it encompasses a new paradigm for the delivery of health services that requires process reengineering , cultural changes , as well as organizational changes", "topics": ["scalability", "autonomous car"]}
{"title": "vip-cnn : visual phrase guided convolutional neural network", "abstract": "as the intermediate level task connecting image captioning and object detection , visual relationship detection started to catch researchers ' attention because of its descriptive power and clear structure . it detects the objects and captures their pair-wise interactions with a subject-predicate-object triplet , e.g . person-ride-horse . in this paper , each visual relationship is considered as a phrase with three components . we formulate the visual relationship detection as three inter-connected recognition problems and propose a visual phrase guided convolutional neural network ( vip-cnn ) to address them simultaneously . in vip-cnn , we present a phrase-guided message passing structure ( pmps ) to establish the connection among relationship components and help the model consider the three problems jointly . corresponding non-maximum suppression method and model training strategy are also proposed . experimental results show that our vip-cnn outperforms the state-of-art method both in speed and accuracy . we further pretrain vip-cnn on our cleansed visual genome relationship dataset , which is found to perform better than the pretraining on the imagenet for this task .", "topics": ["object detection", "interaction"]}
{"title": "2^b3^c : 2 box 3 crop of facial image for gender classification with convolutional networks", "abstract": "in this paper , we tackle the classification of gender in facial images with deep learning . our convolutional neural networks ( cnn ) use the vgg-16 architecture [ 1 ] and are pretrained on imagenet for image classification . our proposed method ( 2^b3^c ) first detects the face in the facial image , increases the margin of a detected face by 50 % , cropping the face with two boxes three crop schemes ( left , middle , and right crop ) and extracts the cnn predictions on the cropped schemes . the cnns of our method is fine-tuned on the adience and lfw with gender annotations . we show the effectiveness of our method by achieving 90.8 % classification on adience and achieving competitive 95.3 % classification accuracy on lfw dataset . in addition , to check the true ability of our method , our gender classification system has a frame rate of 7-10 fps ( frames per seconds ) on a gpu considering real-time scenarios .", "topics": ["computer vision"]}
{"title": "training triplet networks with gan", "abstract": "triplet networks are widely used models that are characterized by good performance in classification and retrieval tasks . in this work we propose to train a triplet network by putting it as the discriminator in generative adversarial nets ( gans ) . we make use of the good capability of representation learning of the discriminator to increase the predictive quality of the model . we evaluated our approach on cifar10 and mnist datasets and observed significant improvement on the classification performance using the simple k-nn method .", "topics": ["feature learning", "mnist database"]}
{"title": "tracklet association by online target-specific metric learning and coherent dynamics estimation", "abstract": "in this paper , we present a novel method based on online target-specific metric learning and coherent dynamics estimation for tracklet ( track fragment ) association by network flow optimization in long-term multi-person tracking . our proposed framework aims to exploit appearance and motion cues to prevent identity switches during tracking and to recover missed detections . furthermore , target-specific metrics ( appearance cue ) and motion dynamics ( motion cue ) are proposed to be learned and estimated online , i.e . during the tracking process . our approach is effective even when such cues fail to identify or follow the target due to occlusions or object-to-object interactions . we also propose to learn the weights of these two tracking cues to handle the difficult situations , such as severe occlusions and object-to-object interactions effectively . our method has been validated on several public datasets and the experimental results show that it outperforms several state-of-the-art tracking methods .", "topics": ["interaction", "sensor"]}
{"title": "hinge-loss markov random fields and probabilistic soft logic", "abstract": "a fundamental challenge in developing high-impact machine learning technologies is balancing the need to model rich , structured domains with the ability to scale to big data . many important problem areas are both richly structured and large scale , from social and biological networks , to knowledge graphs and the web , to images , video , and natural language . in this paper , we introduce two new formalisms for modeling structured data , and show that they can both capture rich structure and scale to big data . the first , hinge-loss markov random fields ( hl-mrfs ) , is a new kind of probabilistic graphical model that generalizes different approaches to convex inference . we unite three approaches from the randomized algorithms , probabilistic graphical models , and fuzzy logic communities , showing that all three lead to the same inference objective . we then define hl-mrfs by generalizing this unified objective . the second new formalism , probabilistic soft logic ( psl ) , is a probabilistic programming language that makes hl-mrfs easy to define using a syntax based on first-order logic . we introduce an algorithm for inferring most-probable variable assignments ( map inference ) that is much more scalable than general-purpose convex optimization methods , because it uses message passing to take advantage of sparse dependency structures . we then show how to learn the parameters of hl-mrfs . the learned hl-mrfs are as accurate as analogous discrete models , but much more scalable . together , these algorithms enable hl-mrfs and psl to model rich , structured data at scales not previously possible .", "topics": ["graphical model", "scalability"]}
{"title": "multi-sensor fusion via reduction of dimensionality", "abstract": "large high-dimensional datasets are becoming more and more popular in an increasing number of research areas . processing the high dimensional data incurs a high computational cost and is inherently inefficient since many of the values that describe a data object are redundant due to noise and inner correlations . consequently , the dimensionality , i.e . the number of values that are used to describe a data object , needs to be reduced prior to any other processing of the data . the dimensionality reduction removes , in most cases , noise from the data and reduces substantially the computational cost of algorithms that are applied to the data . in this thesis , a novel coherent integrated methodology is introduced ( theory , algorithm and applications ) to reduce the dimensionality of high-dimensional datasets . the method constructs a diffusion process among the data coordinates via a random walk . the dimensionality reduction is obtained based on the eigen-decomposition of the markov matrix that is associated with the random walk . the proposed method is utilized for : ( a ) segmentation and detection of anomalies in hyper-spectral images ; ( b ) segmentation of multi-contrast mri images ; and ( c ) segmentation of video sequences . we also present algorithms for : ( a ) the characterization of materials using their spectral signatures to enable their identification ; ( b ) detection of vehicles according to their acoustic signatures ; and ( c ) classification of vascular vessels recordings to detect hyper-tension and cardio-vascular diseases . the proposed methodology and algorithms produce excellent results that successfully compete with current state-of-the-art algorithms .", "topics": ["value ( ethics )", "markov chain"]}
{"title": "combining human and machine learning for morphological analysis of galaxy images", "abstract": "the increasing importance of digital sky surveys collecting many millions of galaxy images has reinforced the need for robust methods that can perform morphological analysis of large galaxy image databases . citizen science initiatives such as galaxy zoo showed that large datasets of galaxy images can be analyzed effectively by non-scientist volunteers , but since databases generated by robotic telescopes grow much faster than the processing power of any group of citizen scientists , it is clear that computer analysis is required . here we propose to use citizen science data for training machine learning systems , and show experimental results demonstrating that machine learning systems can be trained with citizen science data . our findings show that the performance of machine learning depends on the quality of the data , which can be improved by using samples that have a high degree of agreement between the citizen scientists . the source code of the method is publicly available .", "topics": ["database", "robot"]}
{"title": "setting up a reinforcement learning task with a real-world robot", "abstract": "reinforcement learning is a promising approach to developing hard-to-engineer adaptive solutions for complex and diverse robotic tasks . however , learning with real-world robots is often unreliable and difficult , which resulted in their low adoption in reinforcement learning research . this difficulty is worsened by the lack of guidelines for setting up learning tasks with robots . in this work , we develop a learning task with a ur5 robotic arm to bring to light some key elements of a task setup and study their contributions to the challenges with robots . we find that learning performance can be highly sensitive to the setup , and thus oversights and omissions in setup details can make effective learning , reproducibility , and fair comparison hard . our study suggests some mitigating steps to help future experimenters avoid difficulties and pitfalls . we show that highly reliable and repeatable experiments can be performed in our setup , indicating the possibility of reinforcement learning research extensively based on real-world robots .", "topics": ["reinforcement learning", "robot"]}
{"title": "causal discovery from subsampled time series data by constraint optimization", "abstract": "this paper focuses on causal structure estimation from time series data in which measurements are obtained at a coarser timescale than the causal timescale of the underlying system . previous work has shown that such subsampling can lead to significant errors about the system 's causal structure if not properly taken into account . in this paper , we first consider the search for the system timescale causal structures that correspond to a given measurement timescale structure . we provide a constraint satisfaction procedure whose computational performance is several orders of magnitude better than previous approaches . we then consider finite-sample data as input , and propose the first constraint optimization approach for recovering the system timescale causal structure . this algorithm optimally recovers from possible conflicts due to statistical errors . more generally , these advances allow for a robust and non-parametric estimation of system timescale causal structures from subsampled time series data .", "topics": ["time series", "mathematical optimization"]}
{"title": "alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization", "abstract": "humans and most animals can learn new tasks without forgetting old ones . however , training artificial neural networks ( anns ) on new tasks typically cause it to forget previously learned tasks . this phenomenon is the result of `` catastrophic forgetting '' , in which training an ann disrupts connection weights that were important for solving previous tasks , degrading task performance . several recent studies have proposed methods to stabilize connection weights of anns that are deemed most important for solving a task , which helps alleviate catastrophic forgetting . here , drawing inspiration from algorithms that are believed to be implemented in vivo , we propose a complementary method : adding a context-dependent gating signal , such that only sparse , mostly non-overlapping patterns of units are active for any one task . this method is easy to implement , requires little computational overhead , and allows anns to maintain high performance across large numbers of sequentially presented tasks when combined with weight stabilization . this work provides another example of how neuroscience-inspired algorithms can benefit ann design and capability .", "topics": ["sparse matrix"]}
{"title": "cross-lingual morphological tagging for low-resource languages", "abstract": "morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language processing tools . we propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision . our approach extends existing approaches of projecting part-of-speech tags across languages , using bitext to infer constraints on the possible tags for a given word type or token . we propose a tagging model using wsabie , a discriminative embedding-based model with rank-based learning . in our evaluation on 11 languages , on average this model performs on par with a baseline weakly-supervised hmm , while being more scalable . multilingual experiments show that the method performs best when projecting between related language pairs . despite the inherently lossy projection , we show that the morphological tags predicted by our models improve the downstream performance of a parser by +0.6 las on average .", "topics": ["baseline ( configuration management )", "natural language processing"]}
{"title": "neural architecture search with reinforcement learning", "abstract": "neural networks are powerful and flexible models that work well for many difficult learning tasks in image , speech and natural language understanding . despite their success , neural networks are still hard to design . in this paper , we use a recurrent network to generate the model descriptions of neural networks and train this rnn with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set . on the cifar-10 dataset , our method , starting from scratch , can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy . our cifar-10 model achieves a test error rate of 3.65 , which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme . on the penn treebank dataset , our model can compose a novel recurrent cell that outperforms the widely-used lstm cell , and other state-of-the-art baselines . our cell achieves a test set perplexity of 62.4 on the penn treebank , which is 3.6 perplexity better than the previous state-of-the-art model . the cell can also be transferred to the character language modeling task on ptb and achieves a state-of-the-art perplexity of 1.214 .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "understanding the feedforward artificial neural network model from the perspective of network flow", "abstract": "in recent years , deep learning based on artificial neural network ( ann ) has achieved great success in pattern recognition . however , there is no clear understanding of such neural computational models . in this paper , we try to unravel `` black-box '' structure of ann model from network flow . specifically , we consider the feed forward ann as a network flow model , which consists of many directional class-pathways . each class-pathway encodes one class . the class-pathway of a class is obtained by connecting the activated neural nodes in each layer from input to output , where activation value of neural node ( node-value ) is defined by the weights of each layer in a trained ann-classifier . from the perspective of the class-pathway , training an ann-classifier can be regarded as the formulation process of class-pathways of different classes . by analyzing the the distances of each two class-pathways in a trained ann-classifiers , we try to answer the questions , why the classifier performs so ? at last , from the neural encodes view , we define the importance of each neural node through the class-pathways , which is helpful to optimize the structure of a classifier . experiments for two types of ann model including multi-layer mlp and cnn verify that the network flow based on class-pathway is a reasonable explanation for ann models .", "topics": ["statistical classification"]}
{"title": "virtual to real reinforcement learning for autonomous driving", "abstract": "reinforcement learning is considered as a promising direction for driving policy learning . however , training autonomous driving vehicle with reinforcement learning in real environment involves non-affordable trial-and-error . it is more desirable to first train in a virtual environment and then transfer to the real environment . in this paper , we propose a novel realistic translation network to make model trained in virtual environment be workable in real world . the proposed network can convert non-realistic virtual image input into a realistic one with similar scene structure . given realistic frames as input , driving policy trained by reinforcement learning can nicely adapt to real world driving . experiments show that our proposed virtual to real ( vr ) reinforcement learning ( rl ) works pretty well . to our knowledge , this is the first successful case of driving policy trained by reinforcement learning that can adapt to real world driving data .", "topics": ["test set", "supervised learning"]}
{"title": "learning-based model predictive control for safe exploration and reinforcement learning", "abstract": "learning-based methods have been successful in solving complex control tasks without significant prior knowledge about the system . however , these methods typically do not provide any safety guarantees , which prevents their use in safety-critical , real-world applications . in this paper , we present a learning-based model predictive control scheme that provides provable high-probability safety guarantees . to this end , we exploit regularity assumptions on the dynamics in terms of a gaussian process prior to construct provably accurate confidence intervals on predicted trajectories . unlike previous approaches , we do not assume that model uncertainties are independent . based on these predictions , we guarantee that trajectories satisfy safety constraints . moreover , we use a terminal set constraint to recursively guarantee the existence of safe control actions at every iteration . in our experiments , we show that the resulting algorithm can be used to safely and efficiently explore and learn about dynamic systems .", "topics": ["reinforcement learning", "iteration"]}
{"title": "dc-prophet : predicting catastrophic machine failures in datacenters", "abstract": "when will a server fail catastrophically in an industrial datacenter ? is it possible to forecast these failures so preventive actions can be taken to increase the reliability of a datacenter ? to answer these questions , we have studied what are probably the largest , publicly available datacenter traces , containing more than 104 million events from 12,500 machines . among these samples , we observe and categorize three types of machine failures , all of which are catastrophic and may lead to information loss , or even worse , reliability degradation of a datacenter . we further propose a two-stage framework-dc-prophet-based on one-class support vector machine and random forest . dc-prophet extracts surprising patterns and accurately predicts the next failure of a machine . experimental results show that dc-prophet achieves an auc of 0.93 in predicting the next machine failure , and a f3-score of 0.88 ( out of 1 ) . on average , dc-prophet outperforms other classical machine learning methods by 39.45 % in f3-score .", "topics": ["support vector machine"]}
{"title": "subspace clustering via optimal direction search", "abstract": "this letter presents a new spectral-clustering-based approach to the subspace clustering problem . underpinning the proposed method is a convex program for optimal direction search , which for each data point d finds an optimal direction in the span of the data that has minimum projection on the other data points and non-vanishing projection on d. the obtained directions are subsequently leveraged to identify a neighborhood set for each data point . an alternating direction method of multipliers framework is provided to efficiently solve for the optimal directions . the proposed method is shown to notably outperform the existing subspace clustering methods , particularly for unwieldy scenarios involving high levels of noise and close subspaces , and yields the state-of-the-art results for the problem of face clustering using subspace segmentation .", "topics": ["cluster analysis"]}
{"title": "predicting the performance of minimax and product in game-tree", "abstract": "the discovery that the minimax decision rule performs poorly in some games has sparked interest in possible alternatives to minimax . until recently , the only games in which minimax was known to perform poorly were games which were mainly of theoretical interest . however , this paper reports results showing poor performance of minimax in a more common game called kalah . for the kalah games tested , a non-minimax decision rule called the product rule performs significantly better than minimax . this paper also discusses a possible way to predict whether or not minimax will perform well in a game when compared to product . a parameter called the rate of heuristic flaw ( rhf ) has been found to correlate positively with the . performance of product against minimax . both analytical and experimental results are given that appear to support the predictive power of rhf .", "topics": ["heuristic"]}
{"title": "an emphatic approach to the problem of off-policy temporal-difference learning", "abstract": "in this paper we introduce the idea of improving the performance of parametric temporal-difference ( td ) learning algorithms by selectively emphasizing or de-emphasizing their updates on different time steps . in particular , we show that varying the emphasis of linear td ( $ \\lambda $ ) 's updates in a particular way causes its expected update to become stable under off-policy training . the only prior model-free td methods to achieve this with per-step computation linear in the number of function approximation parameters are the gradient-td family of methods including tdc , gtd ( $ \\lambda $ ) , and gq ( $ \\lambda $ ) . compared to these methods , our _emphatic td ( $ \\lambda $ ) _ is simpler and easier to use ; it has only one learned parameter vector and one step-size parameter . our treatment includes general state-dependent discounting and bootstrapping functions , and a way of specifying varying degrees of interest in accurately valuing different states .", "topics": ["computation"]}
{"title": "towards a mathematical understanding of the difficulty in learning with feedforward neural networks", "abstract": "training deep neural networks for solving machine learning problems is one great challenge in the field , mainly due to its associated optimisation problem being highly non-convex . recent developments have suggested that many training algorithms do not suffer from undesired local minima under certain scenario , and consequently led to great efforts in pursuing mathematical explanations for such observations . this work provides an alternative mathematical understanding of the challenge from a smooth optimisation perspective . by assuming exact learning of finite samples , sufficient conditions are identified via a critical point analysis to ensure any local minimum to be globally minimal as well . furthermore , a state of the art algorithm , known as the generalised gauss-newton ( ggn ) algorithm , is rigorously revisited as an approximate newton 's algorithm , which shares the property of being locally quadratically convergent to a global minimum under the condition of exact learning .", "topics": ["approximation algorithm", "mathematical optimization"]}
{"title": "robust continuous co-clustering", "abstract": "clustering consists of grouping together samples giving their similar properties . the problem of modeling simultaneously groups of samples and features is known as co-clustering . this paper introduces rocco - a robust continuous co-clustering algorithm . rocco is a scalable , hyperparameter-free , easy and ready to use algorithm to address co-clustering problems in practice over massive cross-domain datasets . it operates by learning a graph-based two-sided representation of the input matrix . the underlying proposed optimization problem is non-convex , which assures a flexible pool of solutions . moreover , we prove that it can be solved with a near linear time complexity on the input size . an exhaustive large-scale experimental testbed conducted with both synthetic and real-world datasets demonstrates rocco 's properties in practice : ( i ) state-of-the-art performance in cross-domain real-world problems including biomedicine and text mining ; ( ii ) very low sensitivity to hyperparameter settings ; ( iii ) robustness to noise and ( iv ) a linear empirical scalability in practice . these results highlight rocco as a powerful general-purpose co-clustering algorithm for cross-domain practitioners , regardless of their technical background .", "topics": ["cluster analysis", "optimization problem"]}
{"title": "illuminant estimation using ensembles of multivariate regression trees", "abstract": "white balancing is a fundamental step in the image processing pipeline . the process involves estimating the chromaticity of the illuminant or light source and using the estimate to correct the image to remove any color cast . given the importance of the problem , there has been much previous work on illuminant estimation . recently , an approach based on ensembles of univariate regression trees that are fit using the squared-error loss function has been proposed and shown to give excellent performance . in this paper , we show that a simpler and more accurate ensemble model can be learned by ( i ) using multivariate regression trees to take into account that the chromaticity components of the illuminant are correlated and constrained , and ( ii ) fitting each tree by directly minimizing a loss function of interest -- -such as recovery angular error or reproduction angular error -- -rather than indirectly using the squared-error loss function as a surrogate . we show empirically that overall our method leads to improved performance on diverse image sets .", "topics": ["image processing", "loss function"]}
{"title": "fuzzy adaptive resonance theory , diffusion maps and their applications to clustering and biclustering", "abstract": "in this paper , we describe an algorithm fardiff ( fuzzy adaptive resonance dif- fusion ) which combines diffusion maps and fuzzy adaptive resonance theory to do clustering on high dimensional data . we describe some applications of this method and some problems for future research .", "topics": ["cluster analysis"]}
{"title": "iht dies hard : provable accelerated iterative hard thresholding", "abstract": "we study -- both in theory and practice -- the use of momentum motions in classic iterative hard thresholding ( iht ) methods . by simply modifying plain iht , we investigate its convergence behavior on convex optimization criteria with non-convex constraints , under standard assumptions . in diverse scenaria , we observe that acceleration in iht leads to significant improvements , compared to state of the art projected gradient descent and frank-wolfe variants . as a byproduct of our inspection , we study the impact of selecting the momentum parameter : similar to convex settings , two modes of behavior are observed -- '' rippling '' and linear -- depending on the level of momentum .", "topics": ["gradient descent", "gradient"]}
{"title": "local higher-order statistics ( lhs ) describing images with statistics of local non-binarized pixel patterns", "abstract": "we propose a new image representation for texture categorization and facial analysis , relying on the use of higher-order local differential statistics as features . it has been recently shown that small local pixel pattern distributions can be highly discriminative while being extremely efficient to compute , which is in contrast to the models based on the global structure of images . motivated by such works , we propose to use higher-order statistics of local non-binarized pixel patterns for the image description . the proposed model does not require either ( i ) user specified quantization of the space ( of pixel patterns ) or ( ii ) any heuristics for discarding low occupancy volumes of the space . we propose to use a data driven soft quantization of the space , with parametric mixture models , combined with higher-order statistics , based on fisher scores . we demonstrate that this leads to a more expressive representation which , when combined with discriminatively learned classifiers and metrics , achieves state-of-the-art performance on challenging texture and facial analysis datasets , in low complexity setup . further , it is complementary to higher complexity features and when combined with them improves performance .", "topics": ["heuristic", "pixel"]}
{"title": "low rank representation on grassmann manifolds : an extrinsic perspective", "abstract": "many computer vision algorithms employ subspace models to represent data . the low-rank representation ( lrr ) has been successfully applied in subspace clustering for which data are clustered according to their subspace structures . the possibility of extending lrr on grassmann manifold is explored in this paper . rather than directly embedding grassmann manifold into a symmetric matrix space , an extrinsic view is taken by building the self-representation of lrr over the tangent space of each grassmannian point . a new algorithm for solving the proposed grassmannian lrr model is designed and implemented . several clustering experiments are conducted on handwritten digits dataset , dynamic texture video clips and youtube celebrity face video data . the experimental results show our method outperforms a number of existing methods .", "topics": ["cluster analysis", "computer vision"]}
{"title": "teaching machines to code : neural markup generation with visual attention", "abstract": "we present a deep recurrent neural network model with soft visual attention that learns to generate latex markup of real-world math formulas given their images . applying neural sequence generation techniques that have been very successful in the fields of machine translation and image/handwriting/speech captioning , recognition , transcription and synthesis , we construct an image-to-markup model that learns to produce syntactically and semantically correct latex markup code of over 150 words long and achieves a bleu score of 89 % ; the best reported so far for the im2latex problem . we also visually demonstrate that the model learns to scan the image left-right / up-down much as a human would read it .", "topics": ["recurrent neural network", "machine translation"]}
{"title": "design of an optical character recognition system for camera-based handheld devices", "abstract": "this paper presents a complete optical character recognition ( ocr ) system for camera captured image/graphics embedded textual documents for handheld devices . at first , text regions are extracted and skew corrected . then , these regions are binarized and segmented into lines and characters . characters are passed into the recognition module . experimenting with a set of 100 business card images , captured by cell phone camera , we have achieved a maximum recognition accuracy of 92.74 % . compared to tesseract , an open source desktop-based powerful ocr engine , present recognition accuracy is worth contributing . moreover , the developed technique is computationally efficient and consumes low memory so as to be applicable on handheld devices .", "topics": ["computational complexity theory"]}
{"title": "structure-aware and temporally coherent 3d human pose estimation", "abstract": "deep learning methods for 3d human pose estimation from rgb images require a huge amount of domain-specific labeled data for good in-the-wild performance . however , obtaining annotated 3d pose data requires a complex motion capture setup which is generally limited to controlled settings . we propose a semi-supervised learning method using a structure-aware loss function which is able to utilize abundant 2d data to learn 3d information . furthermore , we present a simple temporal network which uses additional context present in pose sequences to improve and temporally harmonize the pose estimates . our complete pipeline improves upon the state-of-the-art by 11.8 % and works at 30 fps on a commodity graphics card .", "topics": ["supervised learning", "loss function"]}
{"title": "sparse coding for multitask and transfer learning", "abstract": "we investigate the use of sparse coding and dictionary learning in the context of multitask and transfer learning . the central assumption of our learning method is that the tasks parameters are well approximated by sparse linear combinations of the atoms of a dictionary on a high or infinite dimensional space . this assumption , together with the large quantity of available data in the multitask and transfer learning settings , allows a principled choice of the dictionary . we provide bounds on the generalization error of this approach , for both settings . numerical experiments on one synthetic and two real datasets show the advantage of our method over single task learning , a previous method based on orthogonal and dense representation of the tasks and a related method learning task grouping .", "topics": ["numerical analysis", "synthetic data"]}
{"title": "dynamic texture and scene classification by transferring deep image features", "abstract": "dynamic texture and scene classification are two fundamental problems in understanding natural video content . extracting robust and effective features is a crucial step towards solving these problems . however the existing approaches suffer from the sensitivity to either varying illumination , or viewpoint changing , or even camera motion , and/or the lack of spatial information . inspired by the success of deep structures in image classification , we attempt to leverage a deep structure to extract feature for dynamic texture and scene classification . to tackle with the challenges in training a deep structure , we propose to transfer some prior knowledge from image domain to video domain . to be specific , we propose to apply a well-trained convolutional neural network ( convnet ) as a mid-level feature extractor to extract features from each frame , and then form a representation of a video by concatenating the first and the second order statistics over the mid-level features . we term this two-level feature extraction scheme as a transferred convnet feature ( tcof ) . moreover we explore two different implementations of the tcof scheme , i.e . , the \\textit { spatial } tcof and the \\textit { temporal } tcof , in which the mean-removed frames and the difference between two adjacent frames are used as the inputs of the convnet , respectively . we evaluate systematically the proposed spatial tcof and the temporal tcof schemes on three benchmark data sets , including dyntex , yupenn , and maryland , and demonstrate that the proposed approach yields superior performance .", "topics": ["feature extraction", "computer vision"]}
{"title": "texture analysis using deterministic partially self-avoiding walk with thresholds", "abstract": "in this paper , we propose a new texture analysis method using the deterministic partially self-avoiding walk performed on maps modified with thresholds . in this method , two pixels of the map are neighbors if the euclidean distance between them is less than $ \\sqrt { 2 } $ and the weight ( difference between its intensities ) is less than a given threshold . the maps obtained by using different thresholds highlight several properties of the image that are extracted by the deterministic walk . to compose the feature vector , deterministic walks are performed with different thresholds and its statistics are concatenated . thus , this approach can be considered as a multi-scale analysis . we validate our method on the brodatz database , which is very well known public image database and widely used by texture analysis methods . experimental results indicate that the proposed method presents a good texture discrimination , overcoming traditional texture methods .", "topics": ["feature vector", "map"]}
{"title": "energy-based spherical sparse coding", "abstract": "in this paper , we explore an efficient variant of convolutional sparse coding with unit norm code vectors where reconstruction quality is evaluated using an inner product ( cosine distance ) . to use these codes for discriminative classification , we describe a model we term energy-based spherical sparse coding ( eb-ssc ) in which the hypothesized class label introduces a learned linear bias into the coding step . we evaluate and visualize performance of stacking this encoder to make a deep layered model for image classification .", "topics": ["sparse matrix", "computer vision"]}
{"title": "solving # sat and bayesian inference with backtracking search", "abstract": "inference in bayes nets ( bayes ) is an important problem with numerous applications in probabilistic reasoning . counting the number of satisfying assignments of a propositional formula ( # sat ) is a closely related problem of fundamental theoretical importance . both these problems , and others , are members of the class of sum-of-products ( sumprod ) problems . in this paper we show that standard backtracking search when augmented with a simple memoization scheme ( caching ) can solve any sum-of-products problem with time complexity that is at least as good any other state-of-the-art exact algorithm , and that it can also achieve the best known time-space tradeoff . furthermore , backtracking 's ability to utilize more flexible variable orderings allows us to prove that it can achieve an exponential speedup over other standard algorithms for sumprod on some instances . the ideas presented here have been utilized in a number of solvers that have been applied to various types of sum-of-product problems . these system 's have exploited the fact that backtracking can naturally exploit more of the problem 's structure to achieve improved performance on a range of probleminstances . empirical evidence of this performance gain has appeared in published works describing these solvers , and we provide references to these works .", "topics": ["time complexity", "bayesian network"]}
{"title": "characterizing a database of sequential behaviors with latent dirichlet hidden markov models", "abstract": "this paper proposes a generative model , the latent dirichlet hidden markov models ( ldhmm ) , for characterizing a database of sequential behaviors ( sequences ) . ldhmms posit that each sequence is generated by an underlying markov chain process , which are controlled by the corresponding parameters ( i.e . , the initial state vector , transition matrix and the emission matrix ) . these sequence-level latent parameters for each sequence are modeled as latent dirichlet random variables and parameterized by a set of deterministic database-level hyper-parameters . through this way , we expect to model the sequence in two levels : the database level by deterministic hyper-parameters and the sequence-level by latent parameters . to learn the deterministic hyper-parameters and approximate posteriors of parameters in ldhmms , we propose an iterative algorithm under the variational em framework , which consists of e and m steps . we examine two different schemes , the fully-factorized and partially-factorized forms , for the framework , based on different assumptions . we present empirical results of behavior modeling and sequence classification on three real-world data sets , and compare them to other related models . the experimental results prove that the proposed ldhmms produce better generalization performance in terms of log-likelihood and deliver competitive results on the sequence classification problem .", "topics": ["generative model", "approximation algorithm"]}
{"title": "relaxations for inference in restricted boltzmann machines", "abstract": "we propose a relaxation-based approximate inference algorithm that samples near-map configurations of a binary pairwise markov random field . we experiment on map inference tasks in several restricted boltzmann machines . we also use our underlying sampler to estimate the log-partition function of restricted boltzmann machines and compare against other sampling-based methods .", "topics": ["sampling ( signal processing )"]}
{"title": "abnormality detection in mammography using deep convolutional neural networks", "abstract": "breast cancer is the most common cancer in women worldwide . the most common screening technology is mammography . to reduce the cost and workload of radiologists , we propose a computer aided detection approach for classifying and localizing calcifications and masses in mammogram images . to improve on conventional approaches , we apply deep convolutional neural networks ( cnn ) for automatic feature learning and classifier building . in computer-aided mammography , deep cnn classifiers can not be trained directly on full mammogram images because of the loss of image details from resizing at input layers . instead , our classifiers are trained on labelled image patches and then adapted to work on full mammogram images for localizing the abnormalities . state-of-the-art deep convolutional neural networks are compared on their performance of classifying the abnormalities . experimental results indicate that vggnet receives the best overall accuracy at 92.53\\ % in classifications . for localizing abnormalities , resnet is selected for computing class activation maps because it is ready to be deployed without structural change or further training . our approach demonstrates that deep convolutional neural network classifiers have remarkable localization capabilities despite no supervision on the location of abnormalities is provided .", "topics": ["feature learning", "neural networks"]}
{"title": "radiometric scene decomposition : scene reflectance , illumination , and geometry from rgb-d images", "abstract": "recovering the radiometric properties of a scene ( i.e . , the reflectance , illumination , and geometry ) is a long-sought ability of computer vision that can provide invaluable information for a wide range of applications . deciphering the radiometric ingredients from the appearance of a real-world scene , as opposed to a single isolated object , is particularly challenging as it generally consists of various objects with different material compositions exhibiting complex reflectance and light interactions that are also part of the illumination . we introduce the first method for radiometric scene decomposition that handles those intricacies . we use rgb-d images to bootstrap geometry recovery and simultaneously recover the complex reflectance and natural illumination while refining the noisy initial geometry and segmenting the scene into different material regions . most important , we handle real-world scenes consisting of multiple objects of unknown materials , which necessitates the modeling of spatially-varying complex reflectance , natural illumination , texture , interreflection and shadows . we systematically evaluate the effectiveness of our method on synthetic scenes and demonstrate its application to real-world scenes . the results show that rich radiometric information can be recovered from rgb-d images and demonstrate a new role rgb-d sensors can play for general scene understanding tasks .", "topics": ["synthetic data", "computer vision"]}
{"title": "augmenting bag-of-words : data-driven discovery of temporal and structural information for activity recognition", "abstract": "we present data-driven techniques to augment bag of words ( bow ) models , which allow for more robust modeling and recognition of complex long-term activities , especially when the structure and topology of the activities are not known a priori . our approach specifically addresses the limitations of standard bow approaches , which fail to represent the underlying temporal and causal information that is inherent in activity streams . in addition , we also propose the use of randomly sampled regular expressions to discover and encode patterns in activities . we demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in four complex datasets .", "topics": ["causality"]}
{"title": "detecting breast cancer using a compressive sensing unmixing algorithm", "abstract": "traditional breast cancer imaging methods using microwave nearfield radar imaging ( nri ) seek to recover the complex permittivity of the tissues at each voxel in the imaging region . this approach is suboptimal , in that it does not directly consider the permittivity values that healthy and cancerous breast tissues typically have . in this paper , we describe a novel unmixing algorithm for detecting breast cancer . in this approach , the breast tissue is separated into three components , low water content ( lwc ) , high water content ( hwc ) , and cancerous tissues , and the goal of the optimization procedure is to recover the mixture proportions for each component . by utilizing this approach in a hybrid dbt / nri system , the unmixing reconstruction process can be posed as a sparse recovery problem , such that compressive sensing ( cs ) techniques can be employed . a numerical analysis is performed , which demonstrates that cancerous lesions can be detected from their mixture proportion under the appropriate conditions .", "topics": ["value ( ethics )", "numerical analysis"]}
{"title": "dum : diversity-weighted utility maximization for recommendations", "abstract": "the need for diversification of recommendation lists manifests in a number of recommender systems use cases . however , an increase in diversity may undermine the utility of the recommendations , as relevant items in the list may be replaced by more diverse ones . in this work we propose a novel method for maximizing the utility of the recommended items subject to the diversity of user 's tastes , and show that an optimal solution to this problem can be found greedily . we evaluate the proposed method in two online user studies as well as in an offline analysis incorporating a number of evaluation metrics . the results of evaluations show the superiority of our method over a number of baselines .", "topics": ["baseline ( configuration management )", "optimization problem"]}
{"title": "visual dynamics : probabilistic future frame synthesis via cross convolutional networks", "abstract": "we study the problem of synthesizing a number of likely future frames from a single input image . in contrast to traditional methods , which have tackled this problem in a deterministic or non-parametric way , we propose a novel approach that models future frames in a probabilistic manner . our probabilistic model makes it possible for us to sample and synthesize many possible future frames from a single input image . future frame synthesis is challenging , as it involves low- and high-level image and motion understanding . we propose a novel network structure , namely a cross convolutional network to aid in synthesizing future frames ; this network structure encodes image and motion information as feature maps and convolutional kernels , respectively . in experiments , our model performs well on synthetic data , such as 2d shapes and animated game sprites , as well as on real-wold videos . we also show that our model can be applied to tasks such as visual analogy-making , and present an analysis of the learned network representations .", "topics": ["synthetic data", "map"]}
{"title": "latent fisher discriminant analysis", "abstract": "linear discriminant analysis ( lda ) is a well-known method for dimensionality reduction and classification . previous studies have also extended the binary-class case into multi-classes . however , many applications , such as object detection and keyframe extraction can not provide consistent instance-label pairs , while lda requires labels on instance level for training . thus it can not be directly applied for semi-supervised classification problem . in this paper , we overcome this limitation and propose a latent variable fisher discriminant analysis model . we relax the instance-level labeling into bag-level , is a kind of semi-supervised ( video-level labels of event type are required for semantic frame extraction ) and incorporates a data-driven prior over the latent variables . hence , our method combines the latent variable inference and dimension reduction in an unified bayesian framework . we test our method on musk and corel data sets and yield competitive results compared to the baseline approach . we also demonstrate its capacity on the challenging trecvid med11 dataset for semantic keyframe extraction and conduct a human-factors ranking-based experimental evaluation , which clearly demonstrates our proposed method consistently extracts more semantically meaningful keyframes than challenging baselines .", "topics": ["baseline ( configuration management )", "supervised learning"]}
{"title": "quantum computers", "abstract": "this research paper gives an overview of quantum computers - description of their operation , differences between quantum and silicon computers , major construction problems of a quantum computer and many other basic aspects . no special scientific knowledge is necessary for the reader .", "topics": ["computational complexity theory", "computation"]}
{"title": "segmentation of alzheimers disease in pet scan datasets using matlab", "abstract": "positron emission tomography ( pet ) scan images are one of the bio medical imaging techniques similar to that of mri scan images but pet scan images are helpful in finding the development of tumors.the pet scan images requires expertise in the segmentation where clustering plays an important role in the automation process.the segmentation of such images is manual to automate the process clustering is used.clustering is commonly known as unsupervised learning process of n dimensional data sets are clustered into k groups so as to maximize the inter cluster similarity and to minimize the intra cluster similarity.this paper is proposed to implement the commonly used k means and fuzzy cmeans ( fcm ) clustering algorithm.this work is implemented using matrix laboratory ( matlab ) and tested with sample pet scan image . the sample data is collected from alzheimers disease neuro imaging initiative adni . medical image processing and visualization tool ( mipav ) are used to compare the resultant images .", "topics": ["image processing", "cluster analysis"]}
{"title": "deepvesselnet : vessel segmentation , centerline prediction , and bifurcation detection in 3-d angiographic volumes", "abstract": "we present deepvesselnet , an architecture tailored to the challenges to be addressed when extracting vessel networks and corresponding features in 3-d angiography using deep learning . we discuss the problems of low execution speed and high memory requirements associated with full 3-d convolutional networks , high class imbalance arising from low percentage ( less than 3 % ) of vessel voxels , and unavailability of accurately annotated training data - and offer solutions that are the building blocks of deepvesselnet . first , we formulate 2-d orthogonal cross-hair filters which make use of 3-d context information . second , we introduce a class balancing cross-entropy score with false positive rate correction to handle the high class imbalance and high false positive rate problems associated with existing loss functions . finally , we generate synthetic dataset using a computational angiogenesis model , capable of generating vascular networks under physiological constraints on local network structure and topology , and use these data for transfer learning . deepvesselnet is optimized for segmenting vessels , predicting centerlines , and localizing bifurcations . we test the performance on a range of angiographic volumes including clinical time-of-flight mra data of the human brain , as well as synchrotron radiation x-ray tomographic microscopy scans of the rat brain . our experiments show that , by replacing 3-d filters with 2-d orthogonal cross-hair filters in our network , speed is improved by 23 % while accuracy is maintained . our class balancing metric is crucial for training the network and pre-training with synthetic data helps in early convergence of the training process .", "topics": ["test set", "synthetic data"]}
{"title": "emergence of foveal image sampling from learning to attend in visual scenes", "abstract": "we describe a neural attention model with a learnable retinal sampling lattice . the model is trained on a visual search task requiring the classification of an object embedded in a visual scene amidst background distractors using the smallest number of fixations . we explore the tiling properties that emerge in the model 's retinal sampling lattice after training . specifically , we show that this lattice resembles the eccentricity dependent sampling lattice of the primate retina , with a high resolution region in the fovea surrounded by a low resolution periphery . furthermore , we find conditions where these emergent properties are amplified or eliminated providing clues to their function .", "topics": ["sampling ( signal processing )"]}
{"title": "mirrorflow : exploiting symmetries in joint optical flow and occlusion estimation", "abstract": "optical flow estimation is one of the most studied problems in computer vision , yet recent benchmark datasets continue to reveal problem areas of today 's approaches . occlusions have remained one of the key challenges . in this paper , we propose a symmetric optical flow method to address the well-known chicken-and-egg relation between optical flow and occlusions . in contrast to many state-of-the-art methods that consider occlusions as outliers , possibly filtered out during post-processing , we highlight the importance of joint occlusion reasoning in the optimization and show how to utilize occlusion as an important cue for estimating optical flow . the key feature of our model is to fully exploit the symmetry properties that characterize optical flow and occlusions in the two consecutive images . specifically through utilizing forward-backward consistency and occlusion-disocclusion symmetry in the energy , our model jointly estimates optical flow in both forward and backward direction , as well as consistent occlusion maps in both views . we demonstrate significant performance benefits on standard benchmarks , especially from the occlusion-disocclusion symmetry . on the challenging kitti dataset we report the most accurate two-frame results to date .", "topics": ["computer vision", "map"]}
{"title": "factorial hidden markov models for learning representations of natural language", "abstract": "most representation learning algorithms for language and image processing are local , in that they identify features for a data point based on surrounding points . yet in language processing , the correct meaning of a word often depends on its global context . as a step toward incorporating global context into representation learning , we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word . we develop efficient variational methods for learning factorial hidden markov models from large texts , and use variational distributions to produce features for each word that are sensitive to the entire input sequence , not just to a local context window . experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods .", "topics": ["feature learning", "image processing"]}
{"title": "leipzig corpus miner - a text mining infrastructure for qualitative data analysis", "abstract": "this paper presents the `` leipzig corpus miner '' , a technical infrastructure for supporting qualitative and quantitative content analysis . the infrastructure aims at the integration of 'close reading ' procedures on individual documents with procedures of 'distant reading ' , e.g . lexical characteristics of large document collections . therefore information retrieval systems , lexicometric statistics and machine learning procedures are combined in a coherent framework which enables qualitative data analysts to make use of state-of-the-art natural language processing techniques on very large document collections . applicability of the framework ranges from social sciences to media studies and market research . as an example we introduce the usage of the framework in a political science study on post-democracy and neoliberalism .", "topics": ["natural language processing"]}
{"title": "additive kernels for gaussian process modeling", "abstract": "gaussian process ( gp ) models are often used as mathematical approximations of computationally expensive experiments . provided that its kernel is suitably chosen and that enough data is available to obtain a reasonable fit of the simulator , a gp model can beneficially be used for tasks such as prediction , optimization , or monte-carlo-based quantification of uncertainty . however , the former conditions become unrealistic when using classical gps as the dimension of input increases . one popular alternative is then to turn to generalized additive models ( gams ) , relying on the assumption that the simulator 's response can approximately be decomposed as a sum of univariate functions . if such an approach has been successfully applied in approximation , it is nevertheless not completely compatible with the gp framework and its versatile applications . the ambition of the present work is to give an insight into the use of gps for additive models by integrating additivity within the kernel , and proposing a parsimonious numerical method for data-driven parameter estimation . the first part of this article deals with the kernels naturally associated to additive processes and the properties of the gp models based on such kernels . the second part is dedicated to a numerical procedure based on relaxation for additive kernel parameter estimation . finally , the efficiency of the proposed method is illustrated and compared to other approaches on sobol 's g-function .", "topics": ["kernel ( operating system )", "numerical analysis"]}
{"title": "design of low noise amplifiers using particle swarm optimization", "abstract": "this short paper presents a work on the design of low noise microwave amplifiers using particle swarm optimization ( pso ) technique . particle swarm optimization is used as a method that is applied to a single stage amplifier circuit to meet two criteria : desired gain and desired low noise . the aim is to get the best optimized design using the predefined constraints for gain and low noise values . the code is written to apply the algorithm to meet the desired goals and the obtained results are verified using different simulators . the results obtained show that pso can be applied very efficiently for this kind of design problems with multiple constraints .", "topics": ["simulation"]}
{"title": "expensive optimisation : a metaheuristics perspective", "abstract": "stochastic , iterative search methods such as evolutionary algorithms ( eas ) are proven to be efficient optimizers . however , they require evaluation of the candidate solutions which may be prohibitively expensive in many real world optimization problems . use of approximate models or surrogates is being explored as a way to reduce the number of such evaluations . in this paper we investigated three such methods . the first method ( dafhea ) partially replaces an expensive function evaluation by its approximate model . the approximation is realized with support vector machine ( svm ) regression models . the second method ( dafhea ii ) is an enhancement on dafhea to accommodate for uncertain environments . the third one uses surrogate ranking with preference learning or ordinal regression . the fitness of the candidates is estimated by modeling their rank . the techniques ' performances on some of the benchmark numerical optimization problems have been reported . the comparative benefits and shortcomings of both techniques have been identified .", "topics": ["approximation algorithm", "mathematical optimization"]}
{"title": "exploring the political agenda of the european parliament using a dynamic topic modeling approach", "abstract": "this study analyzes the political agenda of the european parliament ( ep ) plenary , how it has evolved over time , and the manner in which members of the european parliament ( meps ) have reacted to external and internal stimuli when making plenary speeches . to unveil the plenary agenda and detect latent themes in legislative speeches over time , mep speech content is analyzed using a new dynamic topic modeling method based on two layers of non-negative matrix factorization ( nmf ) . this method is applied to a new corpus of all english language legislative speeches in the ep plenary from the period 1999-2014 . our findings suggest that two-layer nmf is a valuable alternative to existing dynamic topic modeling approaches found in the literature , and can unveil niche topics and associated vocabularies not captured by existing methods . substantively , our findings suggest that the political agenda of the ep evolves significantly over time and reacts to exogenous events such as eu treaty referenda and the emergence of the euro-crisis . mep contributions to the plenary agenda are also found to be impacted upon by voting behaviour and the committee structure of the parliament .", "topics": ["text corpus"]}
{"title": "evidence combination and reasoning and its application to real-world problem-solving", "abstract": "in this paper a new mathematical procedure is presented for combining different pieces of evidence which are represented in the interval form to reflect our knowledge about the truth of a hypothesis . evidences may be correlated to each other ( dependent evidences ) or conflicting in supports ( conflicting evidences ) . first , assuming independent evidences , we propose a methodology to construct combination rules which obey a set of essential properties . the method is based on a geometric model . we compare results obtained from dempster-shafer 's rule and the proposed combination rules with both conflicting and non-conflicting data and show that the values generated by proposed combining rules are in tune with our intuition in both cases . secondly , in the case that evidences are known to be dependent , we consider extensions of the rules derived for handling conflicting evidence . the performance of proposed rules are shown by different examples . the results show that the proposed rules reasonably make decision under dependent evidences", "topics": ["value ( ethics )"]}
{"title": "deep reinforcement learning for vision-based robotic grasping : a simulated comparative evaluation of off-policy methods", "abstract": "in this paper , we explore deep reinforcement learning algorithms for vision-based robotic grasping . model-free deep reinforcement learning ( rl ) has been successfully applied to a range of challenging environments , but the proliferation of algorithms makes it difficult to discern which particular approach would be best suited for a rich , diverse task like grasping . to answer this question , we propose a simulated benchmark for robotic grasping that emphasizes off-policy learning and generalization to unseen objects . off-policy learning enables utilization of grasping data over a wide variety of objects , and diversity is important to enable the method to generalize to new objects that were not seen during training . we evaluate the benchmark tasks against a variety of q-function estimation methods , a method previously proposed for robotic grasping with deep neural network models , and a novel approach based on a combination of monte carlo return estimation and an off-policy correction . our results indicate that several simple methods provide a surprisingly strong competitor to popular algorithms such as double q-learning , and our analysis of stability sheds light on the relative tradeoffs between the algorithms .", "topics": ["reinforcement learning", "simulation"]}
{"title": "time-lagged autoencoders : deep learning of slow collective variables for molecular kinetics", "abstract": "inspired by the success of deep learning techniques in the physical and chemical sciences , we apply a modification of an autoencoder type deep neural network to the task of dimension reduction of molecular dynamics data . we can show that our time-lagged autoencoder reliably finds low-dimensional embeddings for high-dimensional feature spaces which capture the slow dynamics of the underlying stochastic processes - beyond the capabilities of linear dimension reduction techniques .", "topics": ["autoencoder"]}
{"title": "community detection in networks using graph distance", "abstract": "the study of networks has received increased attention recently not only from the social sciences and statistics but also from physicists , computer scientists and mathematicians . one of the principal problem in networks is community detection . many algorithms have been proposed for community finding but most of them do not have have theoretical guarantee for sparse networks and networks close to the phase transition boundary proposed by physicists . there are some exceptions but all have some incomplete theoretical basis . here we propose an algorithm based on the graph distance of vertices in the network . we give theoretical guarantees that our method works in identifying communities for block models and can be extended for degree-corrected block models and block models with the number of communities growing with number of vertices . despite favorable simulation results , we are not yet able to conclude that our method is satisfactory for worst possible case . we illustrate on a network of political blogs , facebook networks and some other networks .", "topics": ["simulation", "sparse matrix"]}
{"title": "a dataset to evaluate the representations learned by video prediction models", "abstract": "we present a parameterized synthetic dataset called moving symbols to support the objective study of video prediction networks . using several instantiations of the dataset in which variation is explicitly controlled , we highlight issues in an existing state-of-the-art approach and propose the use of a performance metric with greater semantic meaning to improve experimental interpretability . our dataset provides canonical test cases that will help the community better understand , and eventually improve , the representations learned by such networks in the future . code is available at https : //github.com/rszeto/moving-symbols .", "topics": ["synthetic data"]}
{"title": "parallel support vector machines in practice", "abstract": "in this paper , we evaluate the performance of various parallel optimization methods for kernel support vector machines on multicore cpus and gpus . in particular , we provide the first comparison of algorithms with explicit and implicit parallelization . most existing parallel implementations for multi-core or gpu architectures are based on explicit parallelization of sequential minimal optimization ( smo ) -- -the programmers identified parallelizable components and hand-parallelized them , specifically tuned for a particular architecture . we compare these approaches with each other and with implicitly parallelized algorithms -- -where the algorithm is expressed such that most of the work is done within few iterations with large dense linear algebra operations . these can be computed with highly-optimized libraries , that are carefully parallelized for a large variety of parallel platforms . we highlight the advantages and disadvantages of both approaches and compare them on various benchmark data sets . we find an approximate implicitly parallel algorithm which is surprisingly efficient , permits a much simpler implementation , and leads to unprecedented speedups in svm training .", "topics": ["approximation algorithm", "support vector machine"]}
{"title": "statistical latent space approach for mixed data modelling and applications", "abstract": "the analysis of mixed data has been raising challenges in statistics and machine learning . one of two most prominent challenges is to develop new statistical techniques and methodologies to effectively handle mixed data by making the data less heterogeneous with minimum loss of information . the other challenge is that such methods must be able to apply in large-scale tasks when dealing with huge amount of mixed data . to tackle these challenges , we introduce parameter sharing and balancing extensions to our recent model , the mixed-variate restricted boltzmann machine ( mv.rbm ) which can transform heterogeneous data into homogeneous representation . we also integrate structured sparsity and distance metric learning into rbm-based models . our proposed methods are applied in various applications including latent patient profile modelling in medical data analysis and representation learning for image retrieval . the experimental results demonstrate the models perform better than baseline methods in medical data and outperform state-of-the-art rivals in image dataset .", "topics": ["baseline ( configuration management )", "feature learning"]}
{"title": "model-based hierarchical clustering", "abstract": "we present an approach to model-based hierarchical clustering by formulating an objective function based on a bayesian analysis . this model organizes the data into a cluster hierarchy while specifying a complex feature-set partitioning that is a key component of our model . features can have either a unique distribution in every cluster or a common distribution over some ( or even all ) of the clusters . the cluster subsets over which these features have such a common distribution correspond to the nodes ( clusters ) of the tree representing the hierarchy . we apply this general model to the problem of document clustering for which we use a multinomial likelihood function and dirichlet priors . our algorithm consists of a two-stage process wherein we first perform a flat clustering followed by a modified hierarchical agglomerative merging process that includes determining the features that will have common distributions over the merged clusters . the regularization induced by using the marginal likelihood automatically determines the optimal model structure including number of clusters , the depth of the tree and the subset of features to be modeled as having a common distribution at each node . we present experimental results on both synthetic data and a real document collection .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "principles of modal and vector theory of formal intelligence systems", "abstract": "the paper considers the class of information systems capable of solving heuristic problems on basis of formal theory that was termed modal and vector theory of formal intelligent systems ( fis ) . the paper justifies the construction of fis resolution algorithm , defines the main features of these systems and proves theorems that underlie the theory . the principle of representation diversity of fis construction is formulated . the paper deals with the main principles of constructing and functioning formal intelligent system ( fis ) on basis of fis modal and vector theory . the following phenomena are considered : modular architecture of fis presentation sub-system , algorithms of data processing at every step of the stage of creating presentations . besides the paper suggests the structure of neural elements , i.e . zone detectors and processors that are the basis for fis construction .", "topics": ["artificial intelligence", "heuristic"]}
{"title": "a deep learning model for estimating story points", "abstract": "although there has been substantial research in software analytics for effort estimation in traditional software projects , little work has been done for estimation in agile projects , especially estimating user stories or issues . story points are the most common unit of measure used for estimating the effort involved in implementing a user story or resolving an issue . in this paper , we offer for the \\emph { first } time a comprehensive dataset for story points-based estimation that contains 23,313 issues from 16 open source projects . we also propose a prediction model for estimating story points based on a novel combination of two powerful deep learning architectures : long short-term memory and recurrent highway network . our prediction system is \\emph { end-to-end } trainable from raw input data to prediction outcomes without any manual feature engineering . an empirical evaluation demonstrates that our approach consistently outperforms three common effort estimation baselines and two alternatives in both mean absolute error and the standardized accuracy .", "topics": ["baseline ( configuration management )"]}
{"title": "a lipschitz exploration-exploitation scheme for bayesian optimization", "abstract": "the problem of optimizing unknown costly-to-evaluate functions has been studied for a long time in the context of bayesian optimization . algorithms in this field aim to find the optimizer of the function by asking only a few function evaluations at locations carefully selected based on a posterior model . in this paper , we assume the unknown function is lipschitz continuous . leveraging the lipschitz property , we propose an algorithm with a distinct exploration phase followed by an exploitation phase . the exploration phase aims to select samples that shrink the search space as much as possible . the exploitation phase then focuses on the reduced search space and selects samples closest to the optimizer . considering the expected improvement ( ei ) as a baseline , we empirically show that the proposed algorithm significantly outperforms ei .", "topics": ["baseline ( configuration management )", "mathematical optimization"]}
{"title": "an approach for text steganography based on markov chains", "abstract": "a text steganography method based on markov chains is introduced , together with a reference implementation . this method allows for information hiding in texts that are automatically generated following a given markov model . other markov - based systems of this kind rely on big simplifications of the language model to work , which produces less natural looking and more easily detectable texts . the method described here is designed to generate texts within a good approximation of the original language model provided .", "topics": ["markov chain"]}
{"title": "variations of images to increase their visibility", "abstract": "the calculus of variations applied to the image processing requires some numerical models able to perform the variations of images and the extremization of appropriate actions . to produce the variations of images , there are several possibilities based on the brightness maps . before a numerical model , i propose an experimental approach , based on a tool of gimp , gnu image manipulation program , in order to visualize how the image variations can be . after the discussion of this tool , which is able to strongly increase the visibility of images , the variations and a possible functional for the visibility are proposed in the framework of a numerical model . the visibility functional is analogous to the fringe visibility of the optical interference .", "topics": ["calculus of variations", "image processing"]}
{"title": "the infochemical core", "abstract": "vocalizations and less often gestures have been the object of linguistic research over decades . however , the development of a general theory of communication with human language as a particular case requires a clear understanding of the organization of communication through other means . infochemicals are chemical compounds that carry information and are employed by small organisms that can not emit acoustic signals of optimal frequency to achieve successful communication . here the distribution of infochemicals across species is investigated when they are ranked by their degree or the number of species with which it is associated ( because they produce or they are sensitive to it ) . the quality of the fit of different functions to the dependency between degree and rank is evaluated with a penalty for the number of parameters of the function . surprisingly , a double zipf ( a zipf distribution with two regimes with a different exponent each ) is the model yielding the best fit although it is the function with the largest number of parameters . this suggests that the world wide repertoire of infochemicals contains a chemical nucleus shared by many species and reminiscent of the core vocabularies found for human language in dictionaries or large corpora .", "topics": ["text corpus", "dictionary"]}
{"title": "neural machine translation by jointly learning to align and translate", "abstract": "neural machine translation is a recently proposed approach to machine translation . unlike the traditional statistical machine translation , the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance . the models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation . in this paper , we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture , and propose to extend this by allowing a model to automatically ( soft- ) search for parts of a source sentence that are relevant to predicting a target word , without having to form these parts as a hard segment explicitly . with this new approach , we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of english-to-french translation . furthermore , qualitative analysis reveals that the ( soft- ) alignments found by the model agree well with our intuition .", "topics": ["machine translation", "encoder"]}
{"title": "size dependent word frequencies and translational invariance of books", "abstract": "it is shown that a real novel shares many characteristic features with a null model in which the words are randomly distributed throughout the text . such a common feature is a certain translational invariance of the text . another is that the functional form of the word-frequency distribution of a novel depends on the length of the text in the same way as the null model . this means that an approximate power-law tail ascribed to the data will have an exponent which changes with the size of the text-section which is analyzed . a further consequence is that a novel can not be described by text-evolution models like the simon model . the size-transformation of a novel is found to be well described by a specific random book transformation . this size transformation in addition enables a more precise determination of the functional form of the word-frequency distribution . the implications of the results are discussed .", "topics": ["approximation algorithm"]}
{"title": "towards the rapid development of a natural language understanding module", "abstract": "when developing a conversational agent , there is often an urgent need to have a prototype available in order to test the application with real users . a wizard of oz is a possibility , but sometimes the agent should be simply deployed in the environment where it will be used . here , the agent should be able to capture as many interactions as possible and to understand how people react to failure . in this paper , we focus on the rapid development of a natural language understanding module by non experts . our approach follows the learning paradigm and sees the process of understanding natural language as a classification problem . we test our module with a conversational agent that answers questions in the art domain . moreover , we show how our approach can be used by a natural language interface to a cinema database .", "topics": ["natural language", "interaction"]}
{"title": "fast non-local stereo matching based on hierarchical disparity prediction", "abstract": "stereo matching is the key step in estimating depth from two or more images . recently , some tree-based non-local stereo matching methods have been proposed , which achieved state-of-the-art performance . the algorithms employed some tree structures to aggregate cost and thus improved the performance and reduced the coputation load of the stereo matching . however , the computational complexity of these tree-based algorithms is still high because they search over the entire disparity range . in addition , the extreme greediness of the minimum spanning tree ( mst ) causes the poor performance in large areas with similar colors but varying disparities . in this paper , we propose an efficient stereo matching method using a hierarchical disparity prediction ( hdp ) framework to dramatically reduce the disparity search range so as to speed up the tree-based non-local stereo methods . our disparity prediction scheme works on a graph pyramid derived from an image whose disparity to be estimated . we utilize the disparity of a upper graph to predict a small disparity range for the lower graph . some independent disparity trees ( dt ) are generated to form a disparity prediction forest ( hdpf ) over which the cost aggregation is made . when combined with the state-of-the-art tree-based methods , our scheme not only dramatically speeds up the original methods but also improves their performance by alleviating the second drawback of the tree-based methods . this is partially because our dts overcome the extreme greediness of the mst . extensive experimental results on some benchmark datasets demonstrate the effectiveness and efficiency of our framework . for example , the segment-tree based stereo matching becomes about 25.57 times faster and 2.2 % more accurate over the middlebury 2006 full-size dataset .", "topics": ["computational complexity theory"]}
{"title": "on gaussian markov models for conditional independence", "abstract": "markov models lie at the interface between statistical independence in a probability distribution and graph separation properties . we review model selection and estimation in directed and undirected markov models with gaussian parametrization , emphasizing the main similarities and differences . these two models are similar but not equivalent , although they share a common intersection . we present the existing results from a historical perspective , taking into account the amount of literature existing from both the artificial intelligence and statistics research communities , where these models were originated . we also discuss how the gaussian assumption can be relaxed . we finally point out the main areas of application where these markov models are nowadays used .", "topics": ["artificial intelligence", "markov chain"]}
{"title": "robust clustering using outlier-sparsity regularization", "abstract": "notwithstanding the popularity of conventional clustering algorithms such as k-means and probabilistic clustering , their clustering results are sensitive to the presence of outliers in the data . even a few outliers can compromise the ability of these algorithms to identify meaningful hidden structures rendering their outcome unreliable . this paper develops robust clustering algorithms that not only aim to cluster the data , but also to identify the outliers . the novel approaches rely on the infrequent presence of outliers in the data which translates to sparsity in a judiciously chosen domain . capitalizing on the sparsity in the outlier domain , outlier-aware robust k-means and probabilistic clustering approaches are proposed . their novelty lies on identifying outliers while effecting sparsity in the outlier domain through carefully chosen regularization . a block coordinate descent approach is developed to obtain iterative algorithms with convergence guarantees and small excess computational complexity with respect to their non-robust counterparts . kernelized versions of the robust clustering algorithms are also developed to efficiently handle high-dimensional data , identify nonlinearly separable clusters , or even cluster objects that are not represented by vectors . numerical tests on both synthetic and real datasets validate the performance and applicability of the novel algorithms .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "incomplete dot products for dynamic computation scaling in neural network inference", "abstract": "we propose the use of incomplete dot products ( idp ) to dynamically adjust the number of input channels used in each layer of a convolutional neural network during feedforward inference . idp adds monotonically non-increasing coefficients , referred to as a `` profile '' , to the channels during training . the profile orders the contribution of each channel in non-increasing order . at inference time , the number of channels used can be dynamically adjusted to trade off accuracy for lowered power consumption and reduced latency by selecting only a beginning subset of channels . this approach allows for a single network to dynamically scale over a computation range , as opposed to training and deploying multiple networks to support different levels of computation scaling . additionally , we extend the notion to multiple profiles , each optimized for some specific range of computation scaling . we present experiments on the computation and accuracy trade-offs of idp for popular image classification models and datasets . we demonstrate that , for mnist and cifar-10 , idp reduces computation significantly , e.g . , by 75 % , without significantly compromising accuracy . we argue that idp provides a convenient and effective means for devices to lower computation costs dynamically to reflect the current computation budget of the system . for example , vgg-16 with 50 % idp ( using only the first 50 % of channels ) achieves 70 % in accuracy on the cifar-10 dataset compared to the standard network which achieves only 35 % accuracy when using the reduced channel set .", "topics": ["computer vision", "computation"]}
{"title": "frequency-based patrolling with heterogeneous agents and limited communication", "abstract": "this paper investigates multi-agent frequencybased patrolling of intersecting , circle graphs under conditions where graph nodes have non-uniform visitation requirements and agents have limited ability to communicate . the task is modeled as a partially observable markov decision process , and a reinforcement learning solution is developed . each agent generates its own policy from markov chains , and policies are exchanged only when agents occupy the same or adjacent nodes . this constraint on policy exchange models sparse communication conditions over large , unstructured environments . empirical results provide perspectives on convergence properties , agent cooperation , and generalization of learned patrolling policies to new instances of the task . the emergent behavior indicates learned coordination strategies between heterogeneous agents for patrolling large , unstructured regions as well as the ability to generalize to dynamic variation in node visitation requirements .", "topics": ["reinforcement learning", "sparse matrix"]}
{"title": "this just in : fake news packs a lot in title , uses simpler , repetitive content in text body , more similar to satire than real news", "abstract": "the problem of fake news has gained a lot of attention as it is claimed to have had a significant impact on 2016 us presidential elections . fake news is not a new problem and its spread in social networks is well-studied . often an underlying assumption in fake news discussion is that it is written to look like real news , fooling the reader who does not check for reliability of the sources or the arguments in its content . through a unique study of three data sets and features that capture the style and the language of articles , we show that this assumption is not true . fake news in most cases is more similar to satire than to real news , leading us to conclude that persuasion in fake news is achieved through heuristics rather than the strength of arguments . we show overall title structure and the use of proper nouns in titles are very significant in differentiating fake from real . this leads us to conclude that fake news is targeted for audiences who are not likely to read beyond titles and is aimed at creating mental associations between entities and claims .", "topics": ["entity", "heuristic"]}
{"title": "hybrid q-learning applied to ubiquitous recommender system", "abstract": "ubiquitous information access becomes more and more important nowadays and research is aimed at making it adapted to users . our work consists in applying machine learning techniques in order to bring a solution to some of the problems concerning the acceptance of the system by users . to achieve this , we propose a fundamental shift in terms of how we model the learning of recommender system : inspired by models of human reasoning developed in robotic , we combine reinforcement learning and case-base reasoning to define a recommendation process that uses these two approaches for generating recommendations on different context dimensions ( social , temporal , geographic ) . we describe an implementation of the recommender system based on this framework . we also present preliminary results from experiments with the system and show how our approach increases the recommendation quality .", "topics": ["reinforcement learning", "robot"]}
{"title": "parameterizing region covariance : an efficient way to apply sparse codes on second order statistics", "abstract": "sparse representations have been successfully applied to signal processing , computer vision and machine learning . currently there is a trend to learn sparse models directly on structure data , such as region covariance . however , such methods when combined with region covariance often require complex computation . we present an approach to transform a structured sparse model learning problem to a traditional vectorized sparse modeling problem by constructing a euclidean space representation for region covariance matrices . our new representation has multiple advantages . experiments on several vision tasks demonstrate competitive performance with the state-of-the-art methods .", "topics": ["computer vision", "sparse matrix"]}
{"title": "structured low-rank matrix factorization : global optimality , algorithms , and applications", "abstract": "recently , convex formulations of low-rank matrix factorization problems have received considerable attention in machine learning . however , such formulations often require solving for a matrix of the size of the data matrix , making it challenging to apply them to large scale datasets . moreover , in many applications the data can display structures beyond simply being low-rank , e.g . , images and videos present complex spatio-temporal structures that are largely ignored by standard low-rank methods . in this paper we study a matrix factorization technique that is suitable for large datasets and captures additional structure in the factors by using a particular form of regularization that includes well-known regularizers such as total variation and the nuclear norm as particular cases . although the resulting optimization problem is non-convex , we show that if the size of the factors is large enough , under certain conditions , any local minimizer for the factors yields a global minimizer . a few practical algorithms are also provided to solve the matrix factorization problem , and bounds on the distance from a given approximate solution of the optimization problem to the global optimum are derived . examples in neural calcium imaging video segmentation and hyperspectral compressed recovery show the advantages of our approach on high-dimensional datasets .", "topics": ["approximation algorithm", "optimization problem"]}
{"title": "on the rates of convergence from surrogate risk minimizers to the bayes optimal classifier", "abstract": "we study the rates of convergence from empirical surrogate risk minimizers to the bayes optimal classifier . specifically , we introduce the notion of \\emph { consistency intensity } to characterize a surrogate loss function and exploit this notion to obtain the rate of convergence from an empirical surrogate risk minimizer to the bayes optimal classifier , enabling fair comparisons of the excess risks of different surrogate risk minimizers . the main result of the paper has practical implications including ( 1 ) showing that hinge loss is superior to logistic and exponential loss in the sense that its empirical minimizer converges faster to the bayes optimal classifier and ( 2 ) guiding to modify surrogate loss functions to accelerate the convergence to the bayes optimal classifier .", "topics": ["loss function", "time complexity"]}
{"title": "sparse estimation from noisy observations of an overdetermined linear system", "abstract": "this note studies a method for the efficient estimation of a finite number of unknown parameters from linear equations , which are perturbed by gaussian noise . in case the unknown parameters have only few nonzero entries , the proposed estimator performs more efficiently than a traditional approach . the method consists of three steps : ( 1 ) a classical least squares estimate ( lse ) , ( 2 ) the support is recovered through a linear programming ( lp ) optimization problem which can be computed using a soft-thresholding step , ( 3 ) a de-biasing step using a lse on the estimated support set . the main contribution of this note is a formal derivation of an associated oracle property of the final estimate . that is , when the number of samples is large enough , the estimate is shown to equal the lse based on the support of the { \\em true } parameters .", "topics": ["optimization problem", "sparse matrix"]}
{"title": "deep learning as a mixed convex-combinatorial optimization problem", "abstract": "as neural networks grow deeper and wider , learning networks with hard-threshold activations is becoming increasingly important , both for network quantization , which can drastically reduce time and energy requirements , and for creating large integrated systems of deep networks , which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning . however , since gradient descent is not applicable to hard-threshold functions , it is not clear how to learn them in a principled way . we address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem , and can be solved as such . the discrete optimization goal is to find a set of targets such that each unit , including the output , has a linearly separable problem to solve . given these targets , the network decomposes into individual perceptrons , which can then be learned with standard convex approaches . based on this , we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case . empirically , we show that our algorithm improves classification accuracy in a number of settings , including for alexnet and resnet-18 on imagenet , when compared to the straight-through estimator .", "topics": ["optimization problem", "gradient descent"]}
{"title": "on-line prediction with kernels and the complexity approximation principle", "abstract": "the paper describes an application of aggregating algorithm to the problem of regression . it generalizes earlier results concerned with plain linear regression to kernel techniques and presents an on-line algorithm which performs nearly as well as any oblivious kernel predictor . the paper contains the derivation of an estimate on the performance of this algorithm . the estimate is then used to derive an application of the complexity approximation principle to kernel methods .", "topics": ["kernel ( operating system )", "approximation"]}
{"title": "binary and nonbinary description of hypointensity in human brain mr images", "abstract": "accumulating evidence has shown that iron is involved in the mechanism underlying many neurodegenerative diseases , such as alzheimer 's disease , parkinson 's disease and huntington 's disease . abnormal ( higher ) iron accumulation has been detected in the brains of most neurodegenerative patients , especially in the basal ganglia region . presence of iron leads to changes in mr signal in both magnitude and phase . accordingly , tissues with high iron concentration appear hypo-intense ( darker than usual ) in mr contrasts . in this report , we proposed an improved binary hypointensity description and a novel nonbinary hypointensity description based on principle components analysis . moreover , kendall 's rank correlation coefficient was used to compare the complementary and redundant information provided by the two methods in order to better understand the individual descriptions of iron accumulation in the brain .", "topics": ["sampling ( signal processing )", "ground truth"]}
{"title": "camera-trap images segmentation using multi-layer robust principal component analysis", "abstract": "the segmentation of animals from camera-trap images is a difficult task . to illustrate , there are various challenges due to environmental conditions and hardware limitation in these images . we proposed a multi-layer robust principal component analysis ( multi-layer rpca ) approach for background subtraction . our method computes sparse and low-rank images from a weighted sum of descriptors , using color and texture features as case of study for camera-trap images segmentation . the segmentation algorithm is composed of histogram equalization or gaussian filtering as pre-processing , and morphological filters with active contour as post-processing . the parameters of our multi-layer rpca were optimized with an exhaustive search . the database consists of camera-trap images from the colombian forest taken by the instituto de investigaci\\'on de recursos biol\\'ogicos alexander von humboldt . we analyzed the performance of our method in inherent and therefore challenging situations of camera-trap images . furthermore , we compared our method with some state-of-the-art algorithms of background subtraction , where our multi-layer rpca outperformed these other methods . our multi-layer rpca reached 76.17 and 69.97 % of average fine-grained f-measure for color and infrared sequences , respectively . to our best knowledge , this paper is the first work proposing multi-layer rpca and using it for camera-trap images segmentation .", "topics": ["sparse matrix"]}
{"title": "the convexity and design of composite multiclass losses", "abstract": "we consider composite loss functions for multiclass prediction comprising a proper ( i.e . , fisher-consistent ) loss over probability distributions and an inverse link function . we establish conditions for their ( strong ) convexity and explore the implications . we also show how the separation of concerns afforded by using this composite representation allows for the design of families of losses with the same bayes risk .", "topics": ["loss function"]}
{"title": "the watershed concept and its use in segmentation : a brief history", "abstract": "the watershed is one of the most used tools in image segmentation . we present how its concept is born and developed over time . its implementation as an algorithm or a hardwired device evolved together with the technology which allowed it . we present also how it is used in practice , first together with markers , and later introduced in a multiscale framework , in order to produce not a unique partition but a complete hierarchy .", "topics": ["image processing"]}
{"title": "gender recognition based on sift features", "abstract": "this paper proposes a robust approach for face detection and gender classification in color images . previous researches about gender recognition suppose an expensive computational and time-consuming pre-processing step in order to alignment in which face images are aligned so that facial landmarks like eyes , nose , lips , chin are placed in uniform locations in image . in this paper , a novel technique based on mathematical analysis is represented in three stages that eliminates alignment step . first , a new color based face detection method is represented with a better result and more robustness in complex backgrounds . next , the features which are invariant to affine transformations are extracted from each face using scale invariant feature transform ( sift ) method . to evaluate the performance of the proposed algorithm , experiments have been conducted by employing a svm classifier on a database of face images which contains 500 images from distinct people with equal ratio of male and female .", "topics": ["support vector machine"]}
{"title": "deep neural networks for hdr imaging", "abstract": "we propose novel methods of solving two tasks using convolutional neural networks , firstly the task of generating hdr map of a static scene using differently exposed ldr images of the scene captured using conventional cameras and secondly the task of finding an optimal tone mapping operator that would give a better score on the tmqi metric compared to the existing methods . we quantitatively show the performance of our networks and illustrate the cases where our networks performs good as well as bad .", "topics": ["neural networks"]}
{"title": "privynet : a flexible framework for privacy-preserving deep neural network training", "abstract": "massive data exist among user local platforms that usually can not support deep neural network ( dnn ) training due to computation and storage resource constraints . cloud-based training schemes provide beneficial services but suffer from potential privacy risks due to excessive user data collection . to enable cloud-based dnn training while protecting the data privacy simultaneously , we propose to leverage the intermediate representations of the data , which is achieved by splitting the dnns and deploying them separately onto local platforms and the cloud . the local neural network ( nn ) is used to generate the feature representations . to avoid local training and protect data privacy , the local nn is derived from pre-trained nns . the cloud nn is then trained based on the extracted intermediate representations for the target learning task . we validate the idea of dnn splitting by characterizing the dependency of privacy loss and classification accuracy on the local nn topology for a convolutional nn ( cnn ) based image classification task . based on the characterization , we further propose privynet to determine the local nn topology , which optimizes the accuracy of the target learning task under the constraints on privacy loss , local computation , and storage . the efficiency and effectiveness of privynet are demonstrated with the cifar-10 dataset .", "topics": ["feature extraction", "computer vision"]}
{"title": "symbolic probabilitistic inference in large bn2o networks", "abstract": "a bn2o network is a two level belief net in which the parent interactions are modeled using the noisy-or interaction model . in this paper we discuss application of the spi local expression language to efficient inference in large bn2o networks . in particular , we show that there is significant structure , which can be exploited to improve over the quickscore result . we further describe how symbolic techniques can provide information which can significantly reduce the computation required for computing all cause posterior marginals . finally , we present a novel approximation technique with preliminary experimental results .", "topics": ["interaction", "computation"]}
{"title": "distributed kernel regression : an algorithm for training collaboratively", "abstract": "this paper addresses the problem of distributed learning under communication constraints , motivated by distributed signal processing in wireless sensor networks and data mining with distributed databases . after formalizing a general model for distributed learning , an algorithm for collaboratively training regularized kernel least-squares regression estimators is derived . noting that the algorithm can be viewed as an application of successive orthogonal projection algorithms , its convergence properties are investigated and the statistical behavior of the estimator is discussed in a simplified theoretical setting .", "topics": ["kernel ( operating system )", "data mining"]}
{"title": "collecting coupons with random initial stake", "abstract": "motivated by a problem in the theory of randomized search heuristics , we give a very precise analysis for the coupon collector problem where the collector starts with a random set of coupons ( chosen uniformly from all sets ) . we show that the expected number of rounds until we have a coupon of each type is $ nh_ { n/2 } - 1/2 \\pm o ( 1 ) $ , where $ h_ { n/2 } $ denotes the $ ( n/2 ) $ th harmonic number when $ n $ is even , and $ h_ { n/2 } : = ( 1/2 ) h_ { \\lfloor n/2 \\rfloor } + ( 1/2 ) h_ { \\lceil n/2 \\rceil } $ when $ n $ is odd . consequently , the coupon collector with random initial stake is by half a round faster than the one starting with exactly $ n/2 $ coupons ( apart from additive $ o ( 1 ) $ terms ) . this result implies that classic simple heuristic called \\emph { randomized local search } needs an expected number of $ nh_ { n/2 } - 1/2 \\pm o ( 1 ) $ iterations to find the optimum of any monotonic function defined on bit-strings of length $ n $ .", "topics": ["iteration", "heuristic"]}
{"title": "approximate inference for constructing astronomical catalogs from images", "abstract": "we present a new , fully generative model for constructing astronomical catalogs from optical telescope image sets . each pixel intensity is treated as a poisson random variable with a rate parameter that depends on the latent properties of stars and galaxies . these latent properties are themselves random , with scientific prior distributions constructed from large ancillary datasets . we compare two procedures for posterior inference : markov chain monte carlo ( mcmc ) and variational inference ( vi ) . mcmc excels at quantifying uncertainty while vi is 1000x faster . both procedures outperform the current state-of-the-art method for measuring celestial bodies ' colors , shapes , and morphologies . on a supercomputer , the vi procedure efficiently uses 665,000 cpu cores ( 1.3 million hardware threads ) to construct an astronomical catalog from 50 terabytes of images .", "topics": ["generative model", "calculus of variations"]}
{"title": "gradual tuning : a better way of fine tuning the parameters of a deep neural network", "abstract": "in this paper we present an alternative strategy for fine-tuning the parameters of a network . we named the technique gradual tuning . once trained on a first task , the network is fine-tuned on a second task by modifying a progressively larger set of the network 's parameters . we test gradual tuning on different transfer learning tasks , using networks of different sizes trained with different regularization techniques . the result shows that compared to the usual fine tuning , our approach significantly reduces catastrophic forgetting of the initial task , while still retaining comparable if not better performance on the new task .", "topics": ["matrix regularization"]}
{"title": "spatio-temporal saliency networks for dynamic saliency prediction", "abstract": "computational saliency models for still images have gained significant popularity in recent years . saliency prediction from videos , on the other hand , has received relatively little interest from the community . motivated by this , in this work , we study the use of deep learning for dynamic saliency prediction and propose the so-called spatio-temporal saliency networks . the key to our models is the architecture of two-stream networks where we investigate different fusion mechanisms to integrate spatial and temporal information . we evaluate our models on the diem and ucf-sports datasets and present highly competitive results against the existing state-of-the-art models . we also carry out some experiments on a number of still images from the mit300 dataset by exploiting the optical flow maps predicted from these images . our results show that considering inherent motion information in this way can be helpful for static saliency estimation .", "topics": ["map"]}
{"title": "efficiently discovering hammock paths from induced similarity networks", "abstract": "similarity networks are important abstractions in many information management applications such as recommender systems , corpora analysis , and medical informatics . for instance , by inducing similarity networks between movies rated similarly by users , or between documents containing common terms , and or between clinical trials involving the same themes , we can aim to find the global structure of connectivities underlying the data , and use the network as a basis to make connections between seemingly disparate entities . in the above applications , composing similarities between objects of interest finds uses in serendipitous recommendation , in storytelling , and in clinical diagnosis , respectively . we present an algorithmic framework for traversing similarity paths using the notion of `hammock ' paths which are generalization of traditional paths . our framework is exploratory in nature so that , given starting and ending objects of interest , it explores candidate objects for path following , and heuristics to admissibly estimate the potential for paths to lead to a desired destination . we present three diverse applications : exploring movie similarities in the netflix dataset , exploring abstract similarities across the pubmed corpus , and exploring description similarities in a database of clinical trials . experimental results demonstrate the potential of our approach for unstructured knowledge discovery in similarity networks .", "topics": ["text corpus", "heuristic"]}
{"title": "on the global linear convergence of frank-wolfe optimization variants", "abstract": "the frank-wolfe ( fw ) optimization algorithm has lately re-gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications . however , its convergence rate is known to be slow ( sublinear ) when the solution lies at the boundary . a simple less-known fix is to add the possibility to take 'away steps ' during optimization , an operation that importantly does not require a feasibility oracle . in this paper , we highlight and clarify several variants of the frank-wolfe optimization algorithm that have been successfully applied in practice : away-steps fw , pairwise fw , fully-corrective fw and wolfe 's minimum norm point algorithm , and prove for the first time that they all enjoy global linear convergence , under a weaker condition than strong convexity of the objective . the constant in the convergence rate has an elegant interpretation as the product of the ( classical ) condition number of the function with a novel geometric quantity that plays the role of a 'condition number ' of the constraint set . we provide pointers to where these algorithms have made a difference in practice , in particular with the flow polytope , the marginal polytope and the base polytope for submodular optimization .", "topics": ["mathematical optimization", "iteration"]}
{"title": "learning the distribution with largest mean : two bandit frameworks", "abstract": "over the past few years , the multi-armed bandit model has become increasingly popular in the machine learning community , partly because of applications including online content optimization . this paper reviews two different sequential learning tasks that have been considered in the bandit literature ; they can be formulated as ( sequentially ) learning which distribution has the highest mean among a set of distributions , with some constraints on the learning process . for both of them ( regret minimization and best arm identification ) we present recent , asymptotically optimal algorithms . we compare the behaviors of the sampling rule of each algorithm as well as the complexity terms associated to each problem .", "topics": ["sampling ( signal processing )", "regret ( decision theory )"]}
{"title": "data stability in clustering : a closer look", "abstract": "we consider the model introduced by bilu and linial ( 2010 ) , who study problems for which the optimal clustering does not change when distances are perturbed . they show that even when a problem is np-hard , it is sometimes possible to obtain efficient algorithms for instances resilient to certain multiplicative perturbations , e.g . on the order of $ o ( \\sqrt { n } ) $ for max-cut clustering . awasthi et al . ( 2010 ) consider center-based objectives , and balcan and liang ( 2011 ) analyze the $ k $ -median and min-sum objectives , giving efficient algorithms for instances resilient to certain constant multiplicative perturbations . here , we are motivated by the question of to what extent these assumptions can be relaxed while allowing for efficient algorithms . we show there is little room to improve these results by giving np-hardness lower bounds for both the $ k $ -median and min-sum objectives . on the other hand , we show that constant multiplicative resilience parameters can be so strong as to make the clustering problem trivial , leaving only a narrow range of resilience parameters for which clustering is interesting . we also consider a model of additive perturbations and give a correspondence between additive and multiplicative notions of stability . our results provide a close examination of the consequences of assuming stability in data .", "topics": ["cluster analysis"]}
{"title": "a structure from motion inequality", "abstract": "we state an elementary inequality for the structure from motion problem for m cameras and n points . this structure from motion inequality relates space dimension , camera parameter dimension , the number of cameras and number points and global symmetry properties and provides a rigorous criterion for which reconstruction is not possible with probability 1 . mathematically the inequality is based on frobenius theorem which is a geometric incarnation of the fundamental theorem of linear algebra . the paper also provides a general mathematical formalism for the structure from motion problem . it includes the situation the points can move while the camera takes the pictures .", "topics": ["eisenstein 's criterion"]}
{"title": "when point process meets rnns : predicting fine-grained user interests with mutual behavioral infectivity", "abstract": "predicting fine-grained interests of users with temporal behavior is important to personalization and information filtering applications . however , existing interest prediction methods are incapable of capturing the subtle degreed user interests towards particular items , and the internal time-varying drifting attention of individuals is not studied yet . moreover , the prediction process can also be affected by inter-personal influence , known as behavioral mutual infectivity . inspired by point process in modeling temporal point process , in this paper we present a deep prediction method based on two recurrent neural networks ( rnns ) to jointly model each user 's continuous browsing history and asynchronous event sequences in the context of inter-user behavioral mutual infectivity . our model is able to predict the fine-grained interest from a user regarding a particular item and corresponding timestamps when an occurrence of event takes place . the proposed approach is more flexible to capture the dynamic characteristic of event sequences by using the temporal point process to model event data and timely update its intensity function by rnns . furthermore , to improve the interpretability of the model , the attention mechanism is introduced to emphasize both intra-personal and inter-personal behavior influence over time . experiments on real datasets demonstrate that our model outperforms the state-of-the-art methods in fine-grained user interest prediction .", "topics": ["recurrent neural network"]}
{"title": "a large dataset for improving patch matching", "abstract": "we propose a new dataset for learning local image descriptors which can be used for significantly improved patch matching . our proposed dataset consists of an order of magnitude more number of scenes , images , and positive and negative correspondences compared to the currently available multi-view stereo ( mvs ) dataset from brown et al . the new dataset also has better coverage of the overall viewpoint , scale , and lighting changes in comparison to the mvs dataset . our dataset also provides supplementary information like rgb patches with scale and rotations values , and intrinsic and extrinsic camera parameters which as shown later can be used to customize training data as per application . we train an existing state-of-the-art model on our dataset and evaluate on publicly available benchmarks such as hpatches dataset and strecha et al.\\cite { strecha } to quantify the image descriptor performance . experimental evaluations show that the descriptors trained using our proposed dataset outperform the current state-of-the-art descriptors trained on mvs by 8 % , 4 % and 10 % on matching , verification and retrieval tasks respectively on the hpatches dataset . similarly on the strecha dataset , we see an improvement of 3-5 % for the matching task in non-planar scenes .", "topics": ["test set"]}
{"title": "using kernel methods and model selection for prediction of preterm birth", "abstract": "we describe an application of machine learning to the problem of predicting preterm birth . we conduct a secondary analysis on a clinical trial dataset collected by the national in- stitute of child health and human development ( nichd ) while focusing our attention on predicting different classes of preterm birth . we compare three approaches for deriving predictive models : a support vector machine ( svm ) approach with linear and non-linear kernels , logistic regression with different model selection along with a model based on decision rules prescribed by physician experts for prediction of preterm birth . our approach highlights the pre-processing methods applied to handle the inherent dynamics , noise and gaps in the data and describe techniques used to handle skewed class distributions . empirical experiments demonstrate significant improvement in predicting preterm birth compared to past work .", "topics": ["support vector machine", "nonlinear system"]}
{"title": "faster r-cnn : towards real-time object detection with region proposal networks", "abstract": "state-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations . advances like sppnet and fast r-cnn have reduced the running time of these detection networks , exposing region proposal computation as a bottleneck . in this work , we introduce a region proposal network ( rpn ) that shares full-image convolutional features with the detection network , thus enabling nearly cost-free region proposals . an rpn is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position . the rpn is trained end-to-end to generate high-quality region proposals , which are used by fast r-cnn for detection . we further merge rpn and fast r-cnn into a single network by sharing their convolutional features -- -using the recently popular terminology of neural networks with 'attention ' mechanisms , the rpn component tells the unified network where to look . for the very deep vgg-16 model , our detection system has a frame rate of 5fps ( including all steps ) on a gpu , while achieving state-of-the-art object detection accuracy on pascal voc 2007 , 2012 , and ms coco datasets with only 300 proposals per image . in ilsvrc and coco 2015 competitions , faster r-cnn and rpn are the foundations of the 1st-place winning entries in several tracks . code has been made publicly available .", "topics": ["object detection", "time complexity"]}
{"title": "calibrated surrogate losses for classification with label-dependent costs", "abstract": "we present surrogate regret bounds for arbitrary surrogate losses in the context of binary classification with label-dependent costs . such bounds relate a classifier 's risk , assessed with respect to a surrogate loss , to its cost-sensitive classification risk . two approaches to surrogate regret bounds are developed . the first is a direct generalization of bartlett et al . [ 2006 ] , who focus on margin-based losses and cost-insensitive classification , while the second adopts the framework of steinwart [ 2007 ] based on calibration functions . nontrivial surrogate regret bounds are shown to exist precisely when the surrogate loss satisfies a `` calibration '' condition that is easily verified for many common losses . we apply this theory to the class of uneven margin losses , and characterize when these losses are properly calibrated . the uneven hinge , squared error , exponential , and sigmoid losses are then treated in detail .", "topics": ["regret ( decision theory )", "time complexity"]}
{"title": "automatic representation for lifetime value recommender systems", "abstract": "many modern commercial sites employ recommender systems to propose relevant content to users . while most systems are focused on maximizing the immediate gain ( clicks , purchases or ratings ) , a better notion of success would be the lifetime value ( ltv ) of the user-system interaction . the ltv approach considers the future implications of the item recommendation , and seeks to maximize the cumulative gain over time . the reinforcement learning ( rl ) framework is the standard formulation for optimizing cumulative successes over time . however , rl is rarely used in practice due to its associated representation , optimization and validation techniques which can be complex . in this paper we propose a new architecture for combining rl with recommendation systems which obviates the need for hand-tuned features , thus automating the state-space representation construction process . we analyze the practical difficulties in this formulation and test our solutions on batch off-line real-world recommendation data .", "topics": ["reinforcement learning"]}
{"title": "domain adaptation with randomized expectation maximization", "abstract": "domain adaptation ( da ) is the task of classifying an unlabeled dataset ( target ) using a labeled dataset ( source ) from a related domain . the majority of successful da methods try to directly match the distributions of the source and target data by transforming the feature space . despite their success , state of the art methods based on this approach are either involved or unable to directly scale to data with many features . this article shows that domain adaptation can be successfully performed by using a very simple randomized expectation maximization ( em ) method . we consider two instances of the method , which involve logistic regression and support vector machine , respectively . the underlying assumption of the proposed method is the existence of a good single linear classifier for both source and target domain . the potential limitations of this assumption are alleviated by the flexibility of the method , which can directly incorporate deep features extracted from a pre-trained deep neural network . the resulting algorithm is strikingly easy to implement and apply . we test its performance on 36 real-life adaptation tasks over text and image data with diverse characteristics . the method achieves state-of-the-art results , competitive with those of involved end-to-end deep transfer-learning methods .", "topics": ["feature vector", "support vector machine"]}
{"title": "computing functional and relational box consistency by structured propagation in atomic constraint systems", "abstract": "box consistency has been observed to yield exponentially better performance than chaotic constraint propagation in the interval constraint system obtained by decomposing the original expression into primitive constraints . the claim was made that the improvement is due to avoiding decomposition . in this paper we argue that the improvement is due to replacing chaotic iteration by a more structured alternative . to this end we distinguish the existing notion of box consistency from relational box consistency . we show that from a computational point of view it is important to maintain the functional structure in constraint systems that are associated with a system of equations . so far , it has only been considered computationally important that constraint propagation be fair . with the additional structure of functional constraint systems , one can define and implement computationally effective , structured , truncated constraint propagations . the existing algorithm for box consistency is one such . our results suggest that there are others worth investigating .", "topics": ["iteration"]}
{"title": "what does a belief function believe in ?", "abstract": "the conditioning in the dempster-shafer theory of evidence has been defined ( by shafer \\cite { shafer:90 } as combination of a belief function and of an `` event '' via dempster rule . on the other hand shafer \\cite { shafer:90 } gives a `` probabilistic '' interpretation of a belief function ( hence indirectly its derivation from a sample ) . given the fact that conditional probability distribution of a sample-derived probability distribution is a probability distribution derived from a subsample ( selected on the grounds of a conditioning event ) , the paper investigates the empirical nature of the dempster- rule of combination . it is demonstrated that the so-called `` conditional '' belief function is not a belief function given an event but rather a belief function given manipulation of original empirical data.\\\\ given this , an interpretation of belief function different from that of shafer is proposed . algorithms for construction of belief networks from data are derived for this interpretation .", "topics": ["bayesian network"]}
{"title": "human detection for night surveillance using adaptive background subtracted image", "abstract": "surveillance based on computer vision has become a major necessity in current era . most of the surveillance systems operate on visible light imaging , but performance based on visible light imaging is limited due to some factors like variation in light intensity during the daytime . the matter of concern lies in the need for processing images in low light , such as in the need of nighttime surveillance . in this paper , we have proposed a novel approach for human detection using flir ( forward looking infrared ) camera . as the principle involves sensing based on thermal radiation in the near ir region , it is possible to detect humans from an image captured using a flir camera even in low light . the proposed method for human detection involves processing of thermal images by using hog ( histogram of oriented gradients ) feature extraction technique along with some enhancements . the principle of the proposed technique lies in an adaptive background subtraction algorithm , which works in association with the hog technique . by means of this method , we are able to reduce execution time , precision and some other parameters , which result in improvement of overall accuracy of the human detection system .", "topics": ["image processing", "feature extraction"]}
{"title": "a global feature extraction model for the effective computer aided diagnosis of mild cognitive impairment using structural mri images", "abstract": "multiple modalities of biomarkers have been proved to be very sensitive in assessing the progression of alzheimer 's disease ( ad ) , and using these modalities and machine learning algorithms , several approaches have been proposed to assist in the early diagnosis of ad . among the recent investigated state-of-the-art approaches , gaussian discriminant analysis ( gda ) -based approaches have been demonstrated to be more effective and accurate in the classification of ad , especially for delineating its prodromal stage of mild cognitive impairment ( mci ) . moreover , among those binary classification investigations , the local feature extraction methods were mostly used , which made them hardly be applied to a practical computer aided diagnosis system . therefore , this study presents a novel global feature extraction model taking advantage of the recent proposed gda-based dual high-dimensional decision spaces , which can significantly improve the early diagnosis performance comparing to those local feature extraction methods . in the true test using 20 % held-out data , for discriminating the most challenging mci group from the cognitively normal control ( cn ) group , an f1 score of 91.06 % , an accuracy of 88.78 % , a sensitivity of 91.80 % , and a specificity of 83.78 % were achieved that can be considered as the best performance obtained so far .", "topics": ["feature extraction"]}
{"title": "mav stabilization using machine learning and onboard sensors", "abstract": "in many situations , miniature aerial vehicles ( mavs ) are limited to using only on-board sensors for navigation . this limits the data available to algorithms used for stabilization and localization , and current control methods are often insufficient to allow reliable hovering in place or trajectory following . in this research , we explore using machine learning to predict the drift ( flight path errors ) of an mav while executing a desired flight path . this predicted drift will allow the mav to adjust it 's flightpath to maintain a desired course .", "topics": ["sensor"]}
{"title": "leveraging sparsity for efficient submodular data summarization", "abstract": "the facility location problem is widely used for summarizing large datasets and has additional applications in sensor placement , image retrieval , and clustering . one difficulty of this problem is that submodular optimization algorithms require the calculation of pairwise benefits for all items in the dataset . this is infeasible for large problems , so recent work proposed to only calculate nearest neighbor benefits . one limitation is that several strong assumptions were invoked to obtain provable approximation guarantees . in this paper we establish that these extra assumptions are not necessary -- -solving the sparsified problem will be almost optimal under the standard assumptions of the problem . we then analyze a different method of sparsification that is a better model for methods such as locality sensitive hashing to accelerate the nearest neighbor computations and extend the use of the problem to a broader family of similarities . we validate our approach by demonstrating that it rapidly generates interpretable summaries .", "topics": ["cluster analysis", "approximation"]}
{"title": "a recommender system to restore images with impulse noise", "abstract": "we build a collaborative filtering recommender system to restore images with impulse noise for which the noisy pixels have been previously identified . we define this recommender system in terms of a new color image representation using three matrices that depend on the noise-free pixels of the image to restore , and two parameters : $ k $ , the number of features ; and $ \\lambda $ , the regularization factor . we perform experiments on a well known image database to test our algorithm and we provide image quality statistics for the results obtained . we discuss the roles of bias and variance in the performance of our algorithm as determined by the values of $ k $ and $ \\lambda $ , and provide guidance on how to choose the values of these parameters . finally , we discuss the possibility of using our collaborative filtering recommender system to perform image inpainting and super-resolution .", "topics": ["value ( ethics )", "matrix regularization"]}
{"title": "learning the latent `` look '' : unsupervised discovery of a style-coherent embedding from fashion images", "abstract": "what defines a visual style ? fashion styles emerge organically from how people assemble outfits of clothing , making them difficult to pin down with a computational model . low-level visual similarity can be too specific to detect stylistically similar images , while manually crafted style categories can be too abstract to capture subtle style differences . we propose an unsupervised approach to learn a style-coherent representation . our method leverages probabilistic polylingual topic models based on visual attributes to discover a set of latent style factors . given a collection of unlabeled fashion images , our approach mines for the latent styles , then summarizes outfits by how they mix those styles . our approach can organize galleries of outfits by style without requiring any style labels . experiments on over 100k images demonstrate its promise for retrieving , mixing , and summarizing fashion images by their style .", "topics": ["unsupervised learning"]}
{"title": "untrimmednets for weakly supervised action recognition and detection", "abstract": "current action recognition methods heavily rely on trimmed videos for model training . however , it is expensive and time-consuming to acquire a large-scale trimmed video dataset . this paper presents a new weakly supervised architecture , called untrimmednet , which is able to directly learn action recognition models from untrimmed videos without the requirement of temporal annotations of action instances . our untrimmednet couples two important components , the classification module and the selection module , to learn the action models and reason about the temporal duration of action instances , respectively . these two components are implemented with feed-forward networks , and untrimmednet is therefore an end-to-end trainable architecture . we exploit the learned models for action recognition ( wsr ) and detection ( wsd ) on the untrimmed video datasets of thumos14 and activitynet . although our untrimmednet only employs weak supervision , our method achieves performance superior or comparable to that of those strongly supervised approaches on these two datasets .", "topics": ["end-to-end principle"]}
{"title": "an optimal algorithm for bandit and zero-order convex optimization with two-point feedback", "abstract": "we consider the closely related problems of bandit convex optimization with two-point feedback , and zero-order stochastic convex optimization with two function evaluations per round . we provide a simple algorithm and analysis which is optimal for convex lipschitz functions . this improves on \\cite { dujww13 } , which only provides an optimal result for smooth functions ; moreover , the algorithm and analysis are simpler , and readily extend to non-euclidean problems . the algorithm is based on a small but surprisingly powerful modification of the gradient estimator .", "topics": ["gradient"]}
{"title": "multi-modal face pose estimation with multi-task manifold deep learning", "abstract": "human face pose estimation aims at estimating the gazing direction or head postures with 2d images . it gives some very important information such as communicative gestures , saliency detection and so on , which attracts plenty of attention recently . however , it is challenging because of complex background , various orientations and face appearance visibility . therefore , a descriptive representation of face images and mapping it to poses are critical . in this paper , we make use of multi-modal data and propose a novel face pose estimation method that uses a novel deep learning framework named multi-task manifold deep learning $ m^2dl $ . it is based on feature extraction with improved deep neural networks and multi-modal mapping relationship with multi-task learning . in the proposed deep learning based framework , manifold regularized convolutional layers ( mrcl ) improve traditional convolutional layers by learning the relationship among outputs of neurons . besides , in the proposed mapping relationship learning method , different modals of face representations are naturally combined to learn the mapping function from face images to poses . in this way , the computed mapping model with multiple tasks is improved . experimental results on three challenging benchmark datasets dpose , hpid and bkhpd demonstrate the outstanding performance of $ m^2dl $ .", "topics": ["feature extraction"]}
{"title": "optimistic initialization and greediness lead to polynomial time learning in factored mdps - extended version", "abstract": "in this paper we propose an algorithm for polynomial-time reinforcement learning in factored markov decision processes ( fmdps ) . the factored optimistic initial model ( foim ) algorithm , maintains an empirical model of the fmdp in a conventional way , and always follows a greedy policy with respect to its model . the only trick of the algorithm is that the model is initialized optimistically . we prove that with suitable initialization ( i ) foim converges to the fixed point of approximate value iteration ( avi ) ; ( ii ) the number of steps when the agent makes non-near-optimal decisions ( with respect to the solution of avi ) is polynomial in all relevant quantities ; ( iii ) the per-step costs of the algorithm are also polynomial . to our best knowledge , foim is the first algorithm with these properties . this extended version contains the rigorous proofs of the main theorem . a version of this paper appeared in icml'09 .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "faster asynchronous sgd", "abstract": "asynchronous distributed stochastic gradient descent methods have trouble converging because of stale gradients . a gradient update sent to a parameter server by a client is stale if the parameters used to calculate that gradient have since been updated on the server . approaches have been proposed to circumvent this problem that quantify staleness in terms of the number of elapsed updates . in this work , we propose a novel method that quantifies staleness in terms of moving averages of gradient statistics . we show that this method outperforms previous methods with respect to convergence speed and scalability to many clients . we also discuss how an extension to this method can be used to dramatically reduce bandwidth costs in a distributed training context . in particular , our method allows reduction of total bandwidth usage by a factor of 5 with little impact on cost convergence . we also describe ( and link to ) a software library that we have used to simulate these algorithms deterministically on a single machine .", "topics": ["gradient descent", "gradient"]}
{"title": "two-sample statistics based on anisotropic kernels", "abstract": "the paper introduces a new kernel-based maximum mean discrepancy ( mmd ) statistic for measuring the distance between two distributions given finitely-many multivariate samples . when the distributions are locally low-dimensional , the proposed test can be made more powerful to distinguish certain alternatives by incorporating local covariance matrices and constructing an anisotropic kernel . the kernel matrix is asymmetric ; it computes the affinity between $ n $ data points and a set of $ n_r $ reference points , where $ n_r $ can be drastically smaller than $ n $ . while the proposed statistic can be viewed as a special class of reproducing kernel hilbert space mmd , the consistency of the test is proved , under mild assumptions of the kernel , as long as $ \\|p-q\\| \\sim o ( n^ { -1/2+\\delta } ) $ for any $ \\delta > 0 $ , based on a result of convergence in distribution of the test statistic . applications to flow cytometry and diffusion mri datasets are demonstrated , which motivate the proposed approach to compare distributions .", "topics": ["kernel ( operating system )"]}
{"title": "towards lifelong self-supervision : a deep learning direction for robotics", "abstract": "despite outstanding success in vision amongst other domains , many of the recent deep learning approaches have evident drawbacks for robots . this manuscript surveys recent work in the literature that pertain to applying deep learning systems to the robotics domain , either as means of estimation or as a tool to resolve motor commands directly from raw percepts . these recent advances are only a piece to the puzzle . we suggest that deep learning as a tool alone is insufficient in building a unified framework to acquire general intelligence . for this reason , we complement our survey with insights from cognitive development and refer to ideas from classical control theory , producing an integrated direction for a lifelong learning architecture .", "topics": ["robot"]}
{"title": "fuzzy modeling and natural language processing for panini 's sanskrit grammar", "abstract": "indian languages have long history in world natural languages . panini was the first to define grammar for sanskrit language with about 4000 rules in fifth century . these rules contain uncertainty information . it is not possible to computer processing of sanskrit language with uncertain information . in this paper , fuzzy logic and fuzzy reasoning are proposed to deal to eliminate uncertain information for reasoning with sanskrit grammar . the sanskrit language processing is also discussed in this paper .", "topics": ["natural language processing", "natural language"]}
{"title": "cumulative distribution networks and the derivative-sum-product algorithm", "abstract": "we introduce a new type of graphical model called a `` cumulative distribution network '' ( cdn ) , which expresses a joint cumulative distribution as a product of local functions . each local function can be viewed as providing evidence about possible orderings , or rankings , of variables . interestingly , we find that the conditional independence properties of cdns are quite different from other graphical models . we also describe a messagepassing algorithm that efficiently computes conditional cumulative distributions . due to the unique independence properties of the cdn , these messages do not in general have a one-to-one correspondence with messages exchanged in standard algorithms , such as belief propagation . we demonstrate the application of cdns for structured ranking learning using a previously-studied multi-player gaming dataset .", "topics": ["graphical model"]}
{"title": "learning contextualized music semantics from tags via a siamese network", "abstract": "music information retrieval faces a challenge in modeling contextualized musical concepts formulated by a set of co-occurring tags . in this paper , we investigate the suitability of our recently proposed approach based on a siamese neural network in fighting off this challenge . by means of tag features and probabilistic topic models , the network captures contextualized semantics from tags via unsupervised learning . this leads to a distributed semantics space and a potential solution to the out of vocabulary problem which has yet to be sufficiently addressed . we explore the nature of the resultant music-based semantics and address computational needs . we conduct experiments on three public music tag collections -namely , cal500 , magtag5k and million song dataset- and compare our approach to a number of state-of-the-art semantics learning approaches . comparative results suggest that this approach outperforms previous approaches in terms of semantic priming and music tag completion .", "topics": ["unsupervised learning"]}
{"title": "a unifying framework for structural properties of csps : definitions , complexity , tractability", "abstract": "literature on constraint satisfaction exhibits the definition of several structural properties that can be possessed by csps , like ( in ) consistency , substitutability or interchangeability . current tools for constraint solving typically detect such properties efficiently by means of incomplete yet effective algorithms , and use them to reduce the search space and boost search . in this paper , we provide a unifying framework encompassing most of the properties known so far , both in csp and other fields literature , and shed light on the semantical relationships among them . this gives a unified and comprehensive view of the topic , allows new , unknown , properties to emerge , and clarifies the computational complexity of the various detection problems . in particular , among the others , two new concepts , fixability and removability emerge , that come out to be the ideal characterisations of values that may be safely assigned or removed from a variables domain , while preserving problem satisfiability . these two notions subsume a large number of known properties , including inconsistency , substitutability and others . because of the computational intractability of all the property-detection problems , by following the csp approach we then determine a number of relaxations which provide sufficient conditions for their tractability . in particular , we exploit forms of language restrictions and local reasoning .", "topics": ["computational complexity theory", "value ( ethics )"]}
{"title": "artificial neural network based optical character recognition", "abstract": "optical character recognition deals in recognition and classification of characters from an image . for the recognition to be accurate , certain topological and geometrical properties are calculated , based on which a character is classified and recognized . also , the human psychology perceives characters by its overall shape and features such as strokes , curves , protrusions , enclosures etc . these properties , also called features are extracted from the image by means of spatial pixel-based calculation . a collection of such features , called vectors , help in defining a character uniquely , by means of an artificial neural network that uses these feature vectors .", "topics": ["pixel"]}
{"title": "towards a benchmark of natural language arguments", "abstract": "the connections among natural language processing and argumentation theory are becoming stronger in the latest years , with a growing amount of works going in this direction , in different scenarios and applying heterogeneous techniques . in this paper , we present two datasets we built to cope with the combination of the textual entailment framework and bipolar abstract argumentation . in our approach , such datasets are used to automatically identify through a textual entailment system the relations among the arguments ( i.e . , attack , support ) , and then the resulting bipolar argumentation graphs are analyzed to compute the accepted arguments .", "topics": ["natural language processing", "natural language"]}
{"title": "out-of-sample extension for dimensionality reduction of noisy time series", "abstract": "this paper proposes an out-of-sample extension framework for a global manifold learning algorithm ( isomap ) that uses temporal information in out-of-sample points in order to make the embedding more robust to noise and artifacts . given a set of noise-free training data and its embedding , the proposed framework extends the embedding for a noisy time series . this is achieved by adding a spatio-temporal compactness term to the optimization objective of the embedding . to the best of our knowledge , this is the first method for out-of-sample extension of manifold embeddings that leverages timing information available for the extension set . experimental results demonstrate that our out-of-sample extension algorithm renders a more robust and accurate embedding of sequentially ordered image data in the presence of various noise and artifacts when compared to other timing-aware embeddings . additionally , we show that an out-of-sample extension framework based on the proposed algorithm outperforms the state of the art in eye-gaze estimation .", "topics": ["test set", "time series"]}
{"title": "a randomized algorithm for cca", "abstract": "we present randomizedcca , a randomized algorithm for computing canonical analysis , suitable for large datasets stored either out of core or on a distributed file system . accurate results can be obtained in as few as two data passes , which is relevant for distributed processing frameworks in which iteration is expensive ( e.g . , hadoop ) . the strategy also provides an excellent initializer for standard iterative solutions .", "topics": ["iteration"]}
{"title": "comparative study for inference of hidden classes in stochastic block models", "abstract": "inference of hidden classes in stochastic block model is a classical problem with important applications . most commonly used methods for this problem involve na\\ '' { \\i } ve mean field approaches or heuristic spectral methods . recently , belief propagation was proposed for this problem . in this contribution we perform a comparative study between the three methods on synthetically created networks . we show that belief propagation shows much better performance when compared to na\\ '' { \\i } ve mean field and spectral approaches . this applies to accuracy , computational efficiency and the tendency to overfit the data .", "topics": ["heuristic"]}
{"title": "a large dataset of object scans", "abstract": "we have created a dataset of more than ten thousand 3d scans of real objects . to create the dataset , we recruited 70 operators , equipped them with consumer-grade mobile 3d scanning setups , and paid them to scan objects in their environments . the operators scanned objects of their choosing , outside the laboratory and without direct supervision by computer vision professionals . the result is a large and diverse collection of object scans : from shoes , mugs , and toys to grand pianos , construction vehicles , and large outdoor sculptures . we worked with an attorney to ensure that data acquisition did not violate privacy constraints . the acquired data was irrevocably placed in the public domain and is available freely at http : //redwood-data.org/3dscan .", "topics": ["computer vision"]}
{"title": "a distributed ai aided 3d domino game", "abstract": "in the article a turn-based game played on four computers connected via network is investigated . there are three computers with natural intelligence and one with artificial intelligence . game table is seen by each player 's own view point in all players ' monitors . domino pieces are three dimensional . for distributed systems tcp/ip protocol is used . in order to get 3d image , microsoft xna technology is applied . domino 101 game is nondeterministic game that is result of the game depends on the initial random distribution of the pieces . number of the distributions is equal to the multiplication of following combinations : . moreover , in this game that is played by four people , players are divided into 2 pairs . accordingly , we can not predict how the player uses the dominoes that is according to the dominoes of his/her partner or according to his/her own dominoes . the fact that the natural intelligence can be a player in any level affects the outcome . these reasons make it difficult to develop an ai . in the article four levels of ai are developed . the ai in the first level is equivalent to the intelligence of a child who knows the rules of the game and recognizes the numbers . the ai in this level plays if it has any domino , suitable to play or says pass . in most of the games which can be played on the internet , the ai does the same . but the ai in the last level is a master player , and it can develop itself according to its competitors ' levels .", "topics": ["artificial intelligence"]}
{"title": "towards an author-topic-term-model visualization of 100 years of german sociological society proceedings", "abstract": "author co-citation studies employ factor analysis to reduce high-dimensional co-citation matrices to low-dimensional and possibly interpretable factors , but these studies do not use any information from the text bodies of publications . we hypothesise that term frequencies may yield useful information for scientometric analysis . in our work we ask if word features in combination with bayesian analysis allow well-founded science mapping studies . this work goes back to the roots of mosteller and wallace 's ( 1964 ) statistical text analysis using word frequency features and a bayesian inference approach , tough with different goals . to answer our research question we ( i ) introduce a new data set on which the experiments are carried out , ( ii ) describe the bayesian model employed for inference and ( iii ) present first results of the analysis .", "topics": ["bayesian network"]}
{"title": "outlier detection by consistent data selection method", "abstract": "often the challenge associated with tasks like fraud and spam detection [ 1 ] is the lack of all likely patterns needed to train suitable supervised learning models . in order to overcome this limitation , such tasks are attempted as outlier or anomaly detection tasks . we also hypothesize that out- liers have behavioral patterns that change over time . limited data and continuously changing patterns makes learning significantly difficult . in this work we are proposing an approach that detects outliers in large data sets by relying on data points that are consistent . the primary contribution of this work is that it will quickly help retrieve samples for both consistent and non-outlier data sets and is also mindful of new outlier patterns . no prior knowledge of each set is required to extract the samples . the method consists of two phases , in the first phase , consistent data points ( non- outliers ) are retrieved by an ensemble method of unsupervised clustering techniques and in the second phase a one class classifier trained on the consistent data point set is ap- plied on the remaining sample set to identify the outliers . the approach is tested on three publicly available data sets and the performance scores are competitive .", "topics": ["cluster analysis", "supervised learning"]}
{"title": "interactive deep colorization with simultaneous global and local inputs", "abstract": "colorization methods using deep neural networks have become a recent trend . however , most of them do not allow user inputs , or only allow limited user inputs ( only global inputs or only local inputs ) , to control the output colorful images . the possible reason is that it 's difficult to differentiate the influence of different kind of user inputs in network training . to solve this problem , we present a novel deep colorization method , which allows simultaneous global and local inputs to better control the output colorized images . the key step is to design an appropriate loss function that can differentiate the influence of input data , global inputs and local inputs . with this design , our method accepts no inputs , or global inputs , or local inputs , or both global and local inputs , which is not supported in previous deep colorization methods . in addition , we propose a global color theme recommendation system to help users determine global inputs . experimental results shows that our methods can better control the colorized images and generate state-of-art results .", "topics": ["loss function"]}
{"title": "the mixing method : coordinate descent for low-rank semidefinite programming", "abstract": "in this paper , we propose a coordinate descent approach to low-rank structured semidefinite programming . the approach , which we call the mixing method , is extremely simple to implement , has no free parameters , and typically attains an order of magnitude or better improvement in optimization performance over the current state of the art . we show that for certain problems , the method is strictly decreasing and guaranteed to converge to a critical point . we then apply the algorithm to three separate domains : solving the maximum cut semidefinite relaxation , solving a ( novel ) maximum satisfiability relaxation , and solving the glove word embedding optimization problem . in all settings , we demonstrate improvement over the existing state of the art along various dimensions . in total , this work substantially expands the scope and scale of problems that can be solved using semidefinite programming methods .", "topics": ["optimization problem"]}
{"title": "a spectral algorithm with additive clustering for the recovery of overlapping communities in networks", "abstract": "this paper presents a novel spectral algorithm with additive clustering designed to identify overlapping communities in networks . the algorithm is based on geometric properties of the spectrum of the expected adjacency matrix in a random graph model that we call stochastic blockmodel with overlap ( sbmo ) . an adaptive version of the algorithm , that does not require the knowledge of the number of hidden communities , is proved to be consistent under the sbmo when the degrees in the graph are ( slightly more than ) logarithmic . the algorithm is shown to perform well on simulated data and on real-world graphs with known overlapping communities .", "topics": ["cluster analysis", "simulation"]}
{"title": "learning to diagnose with lstm recurrent neural networks", "abstract": "clinical medical data , especially in the intensive care unit ( icu ) , consist of multivariate time series of observations . for each patient visit ( or episode ) , sensor data and lab test results are recorded in the patient 's electronic health record ( ehr ) . while potentially containing a wealth of insights , the data is difficult to mine effectively , owing to varying length , irregular sampling and missing data . recurrent neural networks ( rnns ) , particularly those using long short-term memory ( lstm ) hidden units , are powerful and increasingly popular models for learning from sequence data . they effectively model varying length sequences and capture long range dependencies . we present the first study to empirically evaluate the ability of lstms to recognize patterns in multivariate time series of clinical measurements . specifically , we consider multilabel classification of diagnoses , training a model to classify 128 diagnoses given 13 frequently but irregularly sampled clinical measurements . first , we establish the effectiveness of a simple lstm network for modeling clinical data . then we demonstrate a straightforward and effective training strategy in which we replicate targets at each sequence step . trained only on raw time series , our models outperform several strong baselines , including a multilayer perceptron trained on hand-engineered features .", "topics": ["sampling ( signal processing )", "time series"]}
{"title": "greedy compositional clustering for unsupervised learning of hierarchical compositional models", "abstract": "this paper proposes to integrate a feature pursuit learning process into a greedy bottom-up learning scheme . the algorithm combines the benefits of bottom-up and top-down approaches for learning hierarchical models : it allows to induce the hierarchical structure of objects in an unsupervised manner , while avoiding a hard decision on the activation of parts . we follow the principle of compositionality by assembling higher-order parts from elements of lower layers in the hierarchy . the parts are learned greedily with an em-type process that iterates between image encoding and part re-learning . the process stops when a candidate part is not able to find a free niche in the image . the algorithm proceeds layer by layer in a bottom-up manner until no further compositions are found . a subsequent top-down process composes the learned hierarchical shape vocabulary into a holistic object model . experimental evaluation of the approach shows state-of-the-art performance on a domain adaptation task . moreover , we demonstrate the capability of learning complex , semantically meaningful hierarchical compositional models without supervision .", "topics": ["unsupervised learning"]}
{"title": "using spatially varying pixels exposures and bayer-covered photosensors for high dynamic range imaging", "abstract": "the method of a linear high dynamic range imaging using solid-state photosensors with bayer colour filters array is provided in this paper . using information from neighbour pixels , it is possible to reconstruct linear images with wide dynamic range from the oversaturated images . bayer colour filters array is considered as an array of neutral filters in a quasimonochromatic light . if the camera 's response function to the desirable light source is known then one can calculate correction coefficients to reconstruct oversaturated images . reconstructed images are linearized in order to provide a linear high dynamic range images for optical-digital imaging systems . the calibration procedure for obtaining the camera 's response function to the desired light source is described . experimental results of the reconstruction of the images from the oversaturated images are presented for red , green , and blue quasimonochromatic light sources . quantitative analysis of the accuracy of the reconstructed images is provided .", "topics": ["pixel", "coefficient"]}
{"title": "learning a collaborative multiscale dictionary based on robust empirical mode decomposition", "abstract": "dictionary learning is a challenge topic in many image processing areas . the basic goal is to learn a sparse representation from an overcomplete basis set . due to combining the advantages of generic multiscale representations with learning based adaptivity , multiscale dictionary representation approaches have the power in capturing structural characteristics of natural images . however , existing multiscale learning approaches still suffer from three main weaknesses : inadaptability to diverse scales of image data , sensitivity to noise and outliers , difficulty to determine optimal dictionary structure . in this paper , we present a novel multiscale dictionary learning paradigm for sparse image representations based on an improved empirical mode decomposition . this powerful data-driven analysis tool for multi-dimensional signal can fully adaptively decompose the image into multiscale oscillating components according to intrinsic modes of data self . this treatment can obtain a robust and effective sparse representation , and meanwhile generates a raw base dictionary at multiple geometric scales and spatial frequency bands . this dictionary is refined by selecting optimal oscillating atoms based on frequency clustering . in order to further enhance sparsity and generalization , a tolerance dictionary is learned using a coherence regularized model . a fast proximal scheme is developed to optimize this model . the multiscale dictionary is considered as the product of oscillating dictionary and tolerance dictionary . experimental results demonstrate that the proposed learning approach has the superior performance in sparse image representations as compared with several competing methods . we also show the promising results in image denoising application .", "topics": ["image processing", "cluster analysis"]}
{"title": "learning large-scale bayesian networks with the sparsebn package", "abstract": "learning graphical models from data is an important problem with wide applications , ranging from genomics to the social sciences . nowadays datasets often have upwards of thousands -- -sometimes tens or hundreds of thousands -- -of variables and far fewer samples . to meet this challenge , we have developed a new r package called sparsebn for learning the structure of large , sparse graphical models with a focus on bayesian networks . while there are many existing software packages for this task , this package focuses on the unique setting of learning large networks from high-dimensional data , possibly with interventions . as such , the methods provided place a premium on scalability and consistency in a high-dimensional setting . furthermore , in the presence of interventions , the methods implemented here achieve the goal of learning a causal network from data . additionally , the sparsebn package is fully compatible with existing software packages for network analysis .", "topics": ["graphical model", "bayesian network"]}
{"title": "towards improving validation , verification , crash investigations , and event reconstruction of flight-critical systems with self-forensics", "abstract": "this paper introduces a novel concept of self-forensics to complement the standard autonomic self-chop properties of the self-managed systems , to be specified in the forensic lucid language . we argue that self-forensics , with the forensics taken out of the cybercrime domain , is applicable to `` self-dissection '' for the purpose of verification of autonomous software and hardware systems of flight-critical systems for automated incident and anomaly analysis and event reconstruction by the engineering teams in a variety of incident scenarios during design and testing as well as actual flight data .", "topics": ["autonomous car"]}
{"title": "learning neural representations of human cognition across many fmri studies", "abstract": "cognitive neuroscience is enjoying rapid increase in extensive public brain-imaging datasets . it opens the door to large-scale statistical models . finding a unified perspective for all available data calls for scalable and automated solutions to an old challenge : how to aggregate heterogeneous information on brain function into a universal cognitive system that relates mental operations/cognitive processes/psychological tasks to brain networks ? we cast this challenge in a machine-learning approach to predict conditions from statistical brain maps across different studies . for this , we leverage multi-task learning and multi-scale dimension reduction to learn low-dimensional representations of brain images that carry cognitive information and can be robustly associated with psychological stimuli . our multi-dataset classification model achieves the best prediction performance on several large reference datasets , compared to models without cognitive-aware low-dimension representations , it brings a substantial performance boost to the analysis of small datasets , and can be introspected to identify universal template cognitive concepts .", "topics": ["feature learning", "computer vision"]}
{"title": "a trans-disciplinary review of deep learning research for water resources scientists", "abstract": "deep learning ( dl ) , a new-generation artificial neural network research , has made profound strides in recent years . this review paper is intended to provide water resources scientists with a simple technical overview , trans-disciplinary progress update , and potentially inspirations about dl . effective architectures , more accessible data , advances in regularization , and new computing power enabled the success of dl . a trans-disciplinary review reveals that dl is rapidly transforming myriad scientific disciplines including high-energy physics , astronomy , chemistry , genomics and remote sensing , where systematic dl toolkits , innovative customizations , and sub-disciplines have emerged . however , with a few exceptions , its adoption in hydrology has so far been gradual . the literature suggests that novel regularization techniques can effectively prevent high-capacity deep networks from overfitting . as a result , in most scientific disciplines , dl models demonstrated superior predictive and generalization performance to conventional methods . meanwhile , less noticed is that dl may also serve as a scientific exploratory tool . a new area termed `` ai neuroscience '' , has been born . this budding sub-discipline is accumulating a significant body of work , e.g . , distilling knowledge obtained in dl networks to interpretable models , attributing decisions to inputs via back-propagation of relevance , or visualization of activations . these methods are designed to interpret the decision process of deep networks and derive insights . while scientists so far have mostly been using customized , ad-hoc methods for interpretation , vast opportunities await for dl to propel advancement in water science .", "topics": ["matrix regularization", "relevance"]}
{"title": "reinforcement learning based sensing policy optimization for energy efficient cognitive radio networks", "abstract": "this paper introduces a machine learning based collaborative multi-band spectrum sensing policy for cognitive radios . the proposed sensing policy guides secondary users to focus the search of unused radio spectrum to those frequencies that persistently provide them high data rate . the proposed policy is based on machine learning , which makes it adaptive with the temporally and spatially varying radio spectrum . furthermore , there is no need for dynamic modeling of the primary activity since it is implicitly learned over time . energy efficiency is achieved by minimizing the number of assigned sensors per each subband under a constraint on miss detection probability . it is important to control the missed detections because they cause collisions with primary transmissions and lead to retransmissions at both the primary and secondary user . simulations show that the proposed machine learning based sensing policy improves the overall throughput of the secondary network and improves the energy efficiency while controlling the miss detection probability .", "topics": ["reinforcement learning", "simulation"]}
{"title": "reducing the computational cost in multi-objective evolutionary algorithms by filtering worthless individuals", "abstract": "the large number of exact fitness function evaluations makes evolutionary algorithms to have computational cost . in some real-world problems , reducing number of these evaluations is much more valuable even by increasing computational complexity and spending more time . to fulfill this target , we introduce an effective factor , in spite of applied factor in adaptive fuzzy fitness granulation with non-dominated sorting genetic algorithm-ii , to filter out worthless individuals more precisely . our proposed approach is compared with respect to adaptive fuzzy fitness granulation with non-dominated sorting genetic algorithm-ii , using the hyper volume and the inverted generational distance performance measures . the proposed method is applied to 1 traditional and 1 state-of-the-art benchmarks with considering 3 different dimensions . from an average performance view , the results indicate that although decreasing the number of fitness evaluations leads to have performance reduction but it is not tangible compared to what we gain .", "topics": ["computational complexity theory"]}
{"title": "steady state resource allocation analysis of the stochastic diffusion search", "abstract": "this article presents the long-term behaviour analysis of stochastic diffusion search ( sds ) , a distributed agent-based system for best-fit pattern matching . sds operates by allocating simple agents into different regions of the search space . agents independently pose hypotheses about the presence of the pattern in the search space and its potential distortion . assuming a compositional structure of hypotheses about pattern matching agents perform an inference on the basis of partial evidence from the hypothesised solution . agents posing mutually consistent hypotheses about the pattern support each other and inhibit agents with inconsistent hypotheses . this results in the emergence of a stable agent population identifying the desired solution . positive feedback via diffusion of information between the agents significantly contributes to the speed with which the solution population is formed . the formulation of the sds model in terms of interacting markov chains enables its characterisation in terms of the allocation of agents , or computational resources . the analysis characterises the stationary probability distribution of the activity of agents , which leads to the characterisation of the solution population in terms of its similarity to the target pattern .", "topics": ["markov chain"]}
{"title": "end-to-end deep hdr imaging with large foreground motions", "abstract": "this paper proposes the first end-to-end deep framework for high dynamic range ( hdr ) imaging of dynamic scenes with large-scale foreground motions . in state-of-the-art deep hdr imaging such as [ 13 ] , the problem is formulated as an image composition problem , by first aligning input images using optical flows which are still error-prone due to occlusion and large motions . in our end-to-end approach , hdr imaging is formulated as an image translation problem and no optical flows are used . moreover , our simple translation network can automatically hallucinate plausible hdr details in the presence of total occlusion , saturation and under-exposure , which are otherwise almost impossible to recover by conventional optimization approaches . we perform extensive qualitative and quantitative comparisons to show that our end-to-end hdr approach produces excellent results where color artifacts and geometry distortion are significantly reduced compared with existing state-ofthe-art methods .", "topics": ["end-to-end principle"]}
{"title": "lsalsa : efficient sparse coding in single and multiple dictionary settings", "abstract": "we propose an efficient sparse coding ( sc ) framework for obtaining sparse representation of data . the proposed framework is very general and applies to both the single dictionary setting , where each data point is represented as a sparse combination of the columns of one dictionary matrix , as well as the multiple dictionary setting as given in morphological component analysis ( mca ) , where the goal is to separate the data into additive parts such that each part has distinct sparse representation within an appropriately chosen corresponding dictionary . both tasks have been cast as $ \\ell_1 $ -regularized optimization problems of minimizing quadratic reconstruction error . in an effort to accelerate traditional acquisition of sparse codes , we propose a deep learning architecture that constitutes a trainable time-unfolded version of the split augmented lagrangian shrinkage algorithm ( salsa ) , a special case of the alternating direction method of multipliers ( admm ) . we empirically validate both variants of the algorithm on image vision tasks and demonstrate that at inference our networks achieve improvements in terms of the running time and the quality of estimated sparse codes on both classic sc and mca problems over more common baselines . we finally demonstrate the visual advantage of our technique on the task of source separation .", "topics": ["baseline ( configuration management )", "time complexity"]}
{"title": "on a formal model of safe and scalable self-driving cars", "abstract": "in recent years , car makers and tech companies have been racing towards self driving cars . it seems that the main parameter in this race is who will have the first car on the road . the goal of this paper is to add to the equation two additional crucial parameters . the first is standardization of safety assurance -- - what are the minimal requirements that every self-driving car must satisfy , and how can we verify these requirements . the second parameter is scalability -- - engineering solutions that lead to unleashed costs will not scale to millions of cars , which will push interest in this field into a niche academic corner , and drive the entire field into a `` winter of autonomous driving '' . in the first part of the paper we propose a white-box , interpretable , mathematical model for safety assurance , which we call responsibility-sensitive safety ( rss ) . in the second part we describe a design of a system that adheres to our safety assurance requirements and is scalable to millions of cars .", "topics": ["scalability", "autonomous car"]}
{"title": "3dmatch : learning local geometric descriptors from rgb-d reconstructions", "abstract": "matching local geometric features on real-world depth images is a challenging task due to the noisy , low-resolution , and incomplete nature of 3d scan data . these difficulties limit the performance of current state-of-art methods , which are typically based on histograms over geometric properties . in this paper , we present 3dmatch , a data-driven model that learns a local volumetric patch descriptor for establishing correspondences between partial 3d data . to amass training data for our model , we propose a self-supervised feature learning method that leverages the millions of correspondence labels found in existing rgb-d reconstructions . experiments show that our descriptor is not only able to match local geometry in new scenes for reconstruction , but also generalize to different tasks and spatial scales ( e.g . instance-level object model alignment for the amazon picking challenge , and mesh surface correspondence ) . results show that 3dmatch consistently outperforms other state-of-the-art approaches by a significant margin . code , data , benchmarks , and pre-trained models are available online at http : //3dmatch.cs.princeton.edu", "topics": ["test set", "feature learning"]}
{"title": "convergence of a recombination-based elitist evolutionary algorithm on the royal roads test function", "abstract": "we present an analysis of the performance of an elitist evolutionary algorithm using a recombination operator known as 1-bit-swap on the royal roads test function based on a population . we derive complete , approximate and asymptotic convergence rates for the algorithm . the complete model shows the benefit of the size of the population and re- combination pool .", "topics": ["approximation algorithm"]}
{"title": "unsupervised joint alignment and clustering using bayesian nonparametrics", "abstract": "joint alignment of a collection of functions is the process of independently transforming the functions so that they appear more similar to each other . typically , such unsupervised alignment algorithms fail when presented with complex data sets arising from multiple modalities or make restrictive assumptions about the form of the functions or transformations , limiting their generality . we present a transformed bayesian infinite mixture model that can simultaneously align and cluster a data set . our model and associated learning scheme offer two key advantages : the optimal number of clusters is determined in a data-driven fashion through the use of a dirichlet process prior , and it can accommodate any transformation function parameterized by a continuous parameter vector . as a result , it is applicable to a wide range of data types , and transformation functions . we present positive results on synthetic two-dimensional data , on a set of one-dimensional curves , and on various image data sets , showing large improvements over previous work . we discuss several variations of the model and conclude with directions for future work .", "topics": ["unsupervised learning", "synthetic data"]}
{"title": "reprogramming matter , life , and purpose", "abstract": "reprogramming matter may sound far-fetched , but we have been doing it with increasing power and staggering efficiency for at least 60 years , and for centuries we have been paving the way toward the ultimate reprogrammed fate of the universe , the vessel of all programs . how will we be doing it in 60 years ' time and how will it impact life and the purpose both of machines and of humans ?", "topics": ["computation"]}
{"title": "spontaneous vs . posed smiles - can we tell the difference ?", "abstract": "smile is an irrefutable expression that shows the physical state of the mind in both true and deceptive ways . generally , it shows happy state of the mind , however , `smiles ' can be deceptive , for example people can give a smile when they feel happy and sometimes they might also give a smile ( in a different way ) when they feel pity for others . this work aims to distinguish spontaneous ( felt ) smile expressions from posed ( deliberate ) smiles by extracting and analyzing both global ( macro ) motion of the face and subtle ( micro ) changes in the facial expression features through both tracking a series of facial fiducial markers as well as using dense optical flow . specifically the eyes and lips features are captured and used for analysis . it aims to automatically classify all smiles into either `spontaneous ' or `posed ' categories , by using support vector machines ( svm ) . experimental results on large database show promising results as compared to other relevant methods .", "topics": ["support vector machine", "database"]}
{"title": "a dual approach to scalable verification of deep networks", "abstract": "this paper addresses the problem of formally verifying desirable properties of neural networks , i.e . , obtaining provable guarantees that the outputs of the neural network will always behave in a certain way for a given class of inputs . most previous work on this topic was limited in its applicability by the size of the network , network architecture and the complexity of properties to be verified . in contrast , our framework applies to much more general class of activation functions and specifications on neural network inputs and outputs . we formulate verification as an optimization problem and solve a lagrangian relaxation of the optimization problem to obtain an upper bound on the verification objective . our approach is anytime , i.e . it can be stopped at any time and a valid bound on the objective can be obtained . we develop specialized verification algorithms with provable tightness guarantees under special assumptions and demonstrate the practical significance of our general verification approach on a variety of verification tasks .", "topics": ["optimization problem"]}
{"title": "generating retinal flow maps from structural optical coherence tomography with artificial intelligence", "abstract": "despite significant advances in artificial intelligence ( ai ) for computer vision , its application in medical imaging has been limited by the burden and limits of expert-generated labels . we used images from optical coherence tomography angiography ( octa ) , a relatively new imaging modality that measures perfusion of the retinal vasculature , to train an ai algorithm to generate vasculature maps from standard structural optical coherence tomography ( oct ) images of the same retinae , both exceeding the ability and bypassing the need for expert labeling . deep learning was able to infer perfusion of microvasculature from structural oct images with similar fidelity to octa and significantly better than expert clinicians ( p < 0.00001 ) . octa suffers from need of specialized hardware , laborious acquisition protocols , and motion artifacts ; whereas our model works directly from standard oct which are ubiquitous and quick to obtain , and allows unlocking of large volumes of previously collected standard oct data both in existing clinical trials and clinical practice . this finding demonstrates a novel application of ai to medical imaging , whereby subtle regularities between different modalities are used to image the same body part and ai is used to generate detailed and accurate inferences of tissue function from structure imaging .", "topics": ["recurrent neural network", "map"]}
{"title": "concise comparative summaries ( ccs ) of large text corpora with a human experiment", "abstract": "in this paper we propose a general framework for topic-specific summarization of large text corpora and illustrate how it can be used for the analysis of news databases . our framework , concise comparative summarization ( ccs ) , is built on sparse classification methods . ccs is a lightweight and flexible tool that offers a compromise between simple word frequency based methods currently in wide use and more heavyweight , model-intensive methods such as latent dirichlet allocation ( lda ) . we argue that sparse methods have much to offer for text analysis and hope ccs opens the door for a new branch of research in this important field . for a particular topic of interest ( e.g . , china or energy ) , css automatically labels documents as being either on- or off-topic ( usually via keyword search ) , and then uses sparse classification methods to predict these labels with the high-dimensional counts of all the other words and phrases in the documents . the resulting small set of phrases found as predictive are then harvested as the summary . to validate our tool , we , using news articles from the new york times international section , designed and conducted a human survey to compare the different summarizers with human understanding . we demonstrate our approach with two case studies , a media analysis of the framing of `` egypt '' in the new york times throughout the arab spring and an informal comparison of the new york times ' and wall street journal 's coverage of `` energy . '' overall , we find that the lasso with $ l^2 $ normalization can be effectively and usefully used to summarize large corpora , regardless of document size .", "topics": ["text corpus", "sparse matrix"]}
{"title": "hidden markov model based part of speech tagger for sinhala language", "abstract": "in this paper we present a fundamental lexical semantics of sinhala language and a hidden markov model ( hmm ) based part of speech ( pos ) tagger for sinhala language . in any natural language processing task , part of speech is a very vital topic , which involves analysing of the construction , behaviour and the dynamics of the language , which the knowledge could utilized in computational linguistics analysis and automation applications . though sinhala is a morphologically rich and agglutinative language , in which words are inflected with various grammatical features , tagging is very essential for further analysis of the language . our research is based on statistical based approach , in which the tagging process is done by computing the tag sequence probability and the word-likelihood probability from the given corpus , where the linguistic knowledge is automatically extracted from the annotated corpus . the current tagger could reach more than 90 % of accuracy for known words .", "topics": ["natural language processing"]}
{"title": "automatic color image segmentation using a square elemental region-based seeded region growing and merging method", "abstract": "this paper presents an efficient automatic color image segmentation method using a seeded region growing and merging method based on square elemental regions . our segmentation method consists of the three steps : generating seed regions , merging the regions , and applying a pixel-wise boundary determination algorithm to the resultant polygonal regions . the major features of our method are as follows : the use of square elemental regions instead of pixels as the processing unit , a seed generation method based on enhanced gradient values , a seed region growing method exploiting local gradient values , a region merging method using a similarity measure including a homogeneity distance based on tsallis entropy , and a termination condition of region merging using an estimated desired number of regions . using square regions as the processing unit substantially reduces the time complexity of the algorithm and makes the performance stable . the experimental results show that our method exhibits stable performance for a variety of natural images , including heavily textured areas , and produces good segmentation results using the same parameter values . the results of our method are fairly comparable to , and in some respects better than , those of existing algorithms .", "topics": ["image segmentation", "time complexity"]}
{"title": "a progressive batching l-bfgs method for machine learning", "abstract": "the standard l-bfgs method relies on gradient approximations that are not dominated by noise , so that search directions are descent directions , the line search is reliable , and quasi-newton updating yields useful quadratic models of the objective function . all of this appears to call for a full batch approach , but since small batch sizes give rise to faster algorithms with better generalization properties , l-bfgs is currently not considered an algorithm of choice for large-scale machine learning applications . one need not , however , choose between the two extremes represented by the full batch or highly stochastic regimes , and may instead follow a progressive batching approach in which the sample size increases during the course of the optimization . in this paper , we present a new version of the l-bfgs algorithm that combines three basic components - progressive batching , a stochastic line search , and stable quasi-newton updating - and that performs well on training logistic regression and deep neural networks . we provide supporting convergence theory for the method .", "topics": ["optimization problem", "loss function"]}
{"title": "low-shot learning for the semantic segmentation of remote sensing imagery", "abstract": "recent advances in computer vision using deep learning with rgb imagery ( e.g . , object recognition and detection ) have been made possible thanks to the development of large annotated rgb image datasets . in contrast , multispectral image ( msi ) and hyperspectral image ( hsi ) datasets contain far fewer labeled images , in part due to the wide variety of sensors used . these annotations are especially limited for semantic segmentation , or pixel-wise classification , of remote sensing imagery because it is labor intensive to generate image annotations . low-shot learning algorithms can make effective inferences despite smaller amounts of annotated data . in this paper , we study low-shot learning using self-taught feature learning for semantic segmentation . we introduce 1 ) an improved self-taught feature learning framework for hsi and msi data and 2 ) a semi-supervised classification algorithm . when these are combined , they achieve state-of-the-art performance on remote sensing datasets that have little annotated training data available . these low-shot learning frameworks will reduce the manual image annotation burden and improve semantic segmentation performance for remote sensing imagery .", "topics": ["feature learning", "test set"]}
{"title": "ftvd is beyond fast total variation regularized deconvolution", "abstract": "in this paper , we revisit the `` ftvd '' algorithm for fast total variation regularized deconvolution , which has been widely used in the past few years . both its original version implemented in the matlab software ftvd 3.0 and its related variant implemented in the latter version ftvd 4.0 are considered \\cite { wang08ftvdsoftware } . we propose that the intermediate results during the iterations are the solutions of a series of combined tikhonov and total variation regularized image deconvolution models and therefore some of them often have even better image quality than the final solution , which is corresponding to the pure total variation regularized model .", "topics": ["iteration"]}
{"title": "fine-grained discriminative localization via saliency-guided faster r-cnn", "abstract": "discriminative localization is essential for fine-grained image classification task , which devotes to recognizing hundreds of subcategories in the same basic-level category . reflecting on discriminative regions of objects , key differences among different subcategories are subtle and local . existing methods generally adopt a two-stage learning framework : the first stage is to localize the discriminative regions of objects , and the second is to encode the discriminative features for training classifiers . however , these methods generally have two limitations : ( 1 ) separation of the two-stage learning is time-consuming . ( 2 ) dependence on object and parts annotations for discriminative localization learning leads to heavily labor-consuming labeling . it is highly challenging to address these two important limitations simultaneously . existing methods only focus on one of them . therefore , this paper proposes the discriminative localization approach via saliency-guided faster r-cnn to address the above two limitations at the same time , and our main novelties and advantages are : ( 1 ) end-to-end network based on faster r-cnn is designed to simultaneously localize discriminative regions and encode discriminative features , which accelerates classification speed . ( 2 ) saliency-guided localization learning is proposed to localize the discriminative region automatically , avoiding labor-consuming labeling . both are jointly employed to simultaneously accelerate classification speed and eliminate dependence on object and parts annotations . comparing with the state-of-the-art methods on the widely-used cub-200-2011 dataset , our approach achieves both the best classification accuracy and efficiency .", "topics": ["computer vision"]}
{"title": "probabilistic automata for computing with words", "abstract": "usually , probabilistic automata and probabilistic grammars have crisp symbols as inputs , which can be viewed as the formal models of computing with values . in this paper , we first introduce probabilistic automata and probabilistic grammars for computing with ( some special ) words in a probabilistic framework , where the words are interpreted as probabilistic distributions or possibility distributions over a set of crisp symbols . by probabilistic conditioning , we then establish a retraction principle from computing with words to computing with values for handling crisp inputs and a generalized extension principle from computing with words to computing with all words for handling arbitrary inputs . these principles show that computing with values and computing with all words can be respectively implemented by computing with some special words . to compare the transition probabilities of two near inputs , we also examine some analytical properties of the transition probability functions of generalized extensions . moreover , the retractions and the generalized extensions are shown to be equivalence-preserving . finally , we clarify some relationships among the retractions , the generalized extensions , and the extensions studied recently by qiu and wang .", "topics": ["markov chain"]}
{"title": "interpretation of neural networks is fragile", "abstract": "in order for machine learning to be deployed and trusted in many applications , it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions . for example , if an algorithm classifies a given pathology image to be a malignant tumor , then the doctor may need to know which parts of the image led the algorithm to this classification . how to interpret black-box predictors is thus an important and active area of research . a fundamental question is : how much can we trust the interpretation itself ? in this paper , we show that interpretation of deep learning predictions is extremely fragile in the following sense : two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations . we systematically characterize the fragility of several widely-used feature-importance interpretation methods ( saliency maps , relevance propagation , and deeplift ) on imagenet and cifar-10 . our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label . we extend these results to show that interpretations based on exemplars ( e.g . influence functions ) are similarly fragile . our analysis of the geometry of the hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches .", "topics": ["neural networks", "map"]}
{"title": "lofs : library of online streaming feature selection", "abstract": "as an emerging research direction , online streaming feature selection deals with sequentially added dimensions in a feature space while the number of data instances is fixed . online streaming feature selection provides a new , complementary algorithmic methodology to enrich online feature selection , especially targets to high dimensionality in big data analytics . this paper introduces the first comprehensive open-source library for use in matlab that implements the state-of-the-art algorithms of online streaming feature selection . the library is designed to facilitate the development of new algorithms in this exciting research direction and make comparisons between the new methods and existing ones available .", "topics": ["feature vector"]}
{"title": "tests of machine intelligence", "abstract": "although the definition and measurement of intelligence is clearly of fundamental importance to the field of artificial intelligence , no general survey of definitions and tests of machine intelligence exists . indeed few researchers are even aware of alternatives to the turing test and its many derivatives . in this paper we fill this gap by providing a short survey of the many tests of machine intelligence that have been proposed .", "topics": ["artificial intelligence"]}
{"title": "connectionist recommendation in the wild", "abstract": "the aggregate behaviors of users can collectively encode deep semantic information about the objects with which they interact . in this paper , we demonstrate novel ways in which the synthesis of these data can illuminate the terrain of users ' environment and support them in their decision making and wayfinding . a novel application of recurrent neural networks and skip-gram models , approaches popularized by their application to modeling language , are brought to bear on student university enrollment sequences to create vector representations of courses and map out traversals across them . we present demonstrations of how scrutability from these neural networks can be gained and how the combination of these techniques can be seen as an evolution of content tagging and a means for a recommender to balance user preferences inferred from data with those explicitly specified . from validation of the models to the development of a ui , we discuss additional requisite functionality informed by the results of a field study leading to the ultimate deployment of the system at a university .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "improving regret bounds for combinatorial semi-bandits with probabilistically triggered arms and its applications", "abstract": "we study combinatorial multi-armed bandit with probabilistically triggered arms ( cmab-t ) and semi-bandit feedback . we resolve a serious issue in the prior cmab-t studies where the regret bounds contain a possibly exponentially large factor of $ 1/p^* $ , where $ p^* $ is the minimum positive probability that an arm is triggered by any action . we address this issue by introducing a triggering probability modulated ( tpm ) bounded smoothness condition into the general cmab-t framework , and show that many applications such as influence maximization bandit and combinatorial cascading bandit satisfy this tpm condition . as a result , we completely remove the factor of $ 1/p^* $ from the regret bounds , achieving significantly better regret bounds for influence maximization and cascading bandits than before . finally , we provide lower bound results showing that the factor $ 1/p^* $ is unavoidable for general cmab-t problems , suggesting that the tpm condition is crucial in removing this factor .", "topics": ["regret ( decision theory )"]}
{"title": "a pointillism approach for natural language processing of social media", "abstract": "the chinese language poses challenges for natural language processing based on the unit of a word even for formal uses of the chinese language , social media only makes word segmentation in chinese even more difficult . in this document we propose a pointillism approach to natural language processing . rather than words that have individual meanings , the basic unit of a pointillism approach is trigrams of characters . these grams take on meaning in aggregate when they appear together in a way that is correlated over time . our results from three kinds of experiments show that when words and topics do have a meme-like trend , they can be reconstructed from only trigrams . for example , for 4-character idioms that appear at least 99 times in one day in our data , the unconstrained precision ( that is , precision that allows for deviation from a lexicon when the result is just as correct as the lexicon version of the word or phrase ) is 0.93 . for longer words and phrases collected from wiktionary , including neologisms , the unconstrained precision is 0.87 . we consider these results to be very promising , because they suggest that it is feasible for a machine to reconstruct complex idioms , phrases , and neologisms with good precision without any notion of words . thus the colorful and baroque uses of language that typify social media in challenging languages such as chinese may in fact be accessible to machines .", "topics": ["natural language processing", "natural language"]}
{"title": "conditional independence and markov properties in possibility theory", "abstract": "conditional independence and markov properties are powerful tools allowing expression of multidimensional probability distributions by means of low-dimensional ones . as multidimensional possibilistic models have been studied for several years , the demand for analogous tools in possibility theory seems to be quite natural . this paper is intended to be a promotion of de cooman 's measure-theoretic approcah to possibility theory , as this approach allows us to find analogies to many important results obtained in probabilistic framework . first , we recall semi-graphoid properties of conditional possibilistic independence , parameterized by a continuous t-norm , and find sufficient conditions for a class of archimedean t-norms to have the graphoid property . then we introduce markov properties and factorization of possibility distrubtions ( again parameterized by a continuous t-norm ) and find the relationships between them . these results are accompanied by a number of conterexamples , which show that the assumptions of specific theorems are substantial .", "topics": ["markov chain"]}
{"title": "improving bi-directional generation between different modalities with variational autoencoders", "abstract": "we investigate deep generative models that can exchange multiple modalities bi-directionally , e.g . , generating images from corresponding texts and vice versa . a major approach to achieve this objective is to train a model that integrates all the information of different modalities into a joint representation and then to generate one modality from the corresponding other modality via this joint representation . we simply applied this approach to variational autoencoders ( vaes ) , which we call a joint multimodal variational autoencoder ( jmvae ) . however , we found that when this model attempts to generate a large dimensional modality missing at the input , the joint representation collapses and this modality can not be generated successfully . furthermore , we confirmed that this difficulty can not be resolved even using a known solution . therefore , in this study , we propose two models to prevent this difficulty : jmvae-kl and jmvae-h . results of our experiments demonstrate that these methods can prevent the difficulty above and that they generate modalities bi-directionally with equal or higher likelihood than conventional vae methods , which generate in only one direction . moreover , we confirm that these methods can obtain the joint representation appropriately , so that they can generate various variations of modality by moving over the joint representation or changing the value of another modality .", "topics": ["calculus of variations", "autoencoder"]}
{"title": "learning knowledge graph embeddings with type regularizer", "abstract": "learning relations based on evidence from knowledge bases relies on processing the available relation instances . many relations , however , have clear domain and range , which we hypothesize could help learn a better , more generalizing , model . we include such information in the rescal model in the form of a regularization factor added to the loss function that takes into account the types ( categories ) of the entities that appear as arguments to relations in the knowledge base . we note increased performance compared to the baseline model in terms of mean reciprocal rank and hits @ n , n = 1 , 3 , 10 . furthermore , we discover scenarios that significantly impact the effectiveness of the type regularizer .", "topics": ["baseline ( configuration management )", "loss function"]}
{"title": "a survey : time travel in deep learning space : an introduction to deep learning models and how deep learning models evolved from the initial ideas", "abstract": "this report will show the history of deep learning evolves . it will trace back as far as the initial belief of connectionism modelling of brain , and come back to look at its early stage realization : neural networks . with the background of neural network , we will gradually introduce how convolutional neural network , as a representative of deep discriminative models , is developed from neural networks , together with many practical techniques that can help in optimization of neural networks . on the other hand , we will also trace back to see the evolution history of deep generative models , to see how researchers balance the representation power and computation complexity to reach restricted boltzmann machine and eventually reach deep belief nets . further , we will also look into the development history of modelling time series data with neural networks . we start with time delay neural networks and move further to currently famous model named recurrent neural network and its extension long short term memory . we will also briefly look into how to construct deep recurrent neural networks . finally , we will conclude this report with some interesting open-ended questions of deep neural networks .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "per-instance differential privacy and the adaptivity of posterior sampling in linear and ridge regression", "abstract": "differential privacy ( dp ) , ever since its advent , has been a controversial object . on the one hand , it provides strong provable protection of individuals in a data set , on the other hand , it has been heavily criticized for being not practical , partially due to its complete independence to the actual data set it tries to protect . in this paper , we address this issue by a new and more fine-grained notion of differential privacy -- - per instance differential privacy ( pdp ) , which captures the privacy of a specific individual with respect to a fixed data set . we show that this is a strict generalization of the standard dp and inherits all its desirable properties , e.g . , composition , invariance to side information and closedness to postprocessing , except that they all hold for every instance separately . when the data is drawn from a distribution , we show that per-instance dp implies generalization . moreover , we provide explicit calculations of the per-instance dp for the output perturbation on a class of smooth learning problems . the result reveals an interesting and intuitive fact that an individual has stronger privacy if he/she has small `` leverage score '' with respect to the data set and if he/she can be predicted more accurately using the leave-one-out data set . using the developed techniques , we provide a novel analysis of the one-posterior-sample ( ops ) estimator and show that when the data set is well-conditioned it provides $ ( \\epsilon , \\delta ) $ -pdp for any target individuals and matches the exact lower bound up to a $ 1+\\tilde { o } ( n^ { -1 } \\epsilon^ { -2 } ) $ multiplicative factor . we also propose adaops which uses adaptive regularization to achieve the same results with $ ( \\epsilon , \\delta ) $ -dp . simulation shows several orders-of-magnitude more favorable privacy and utility trade-off when we consider the privacy of only the users in the data set .", "topics": ["simulation", "coefficient"]}
{"title": "curve registered coupled low rank factorization", "abstract": "we propose an extension of the canonical polyadic ( cp ) tensor model where one of the latent factors is allowed to vary through data slices in a constrained way . the components of the latent factors , which we want to retrieve from data , can vary from one slice to another up to a diffeomorphism . we suppose that the diffeomorphisms are also unknown , thus merging curve registration and tensor decomposition in one model , which we call registered cp . we present an algorithm to retrieve both the latent factors and the diffeomorphism , which is assumed to be in a parametrized form . at the end of the paper , we show simulation results comparing registered cp with other models from the literature .", "topics": ["simulation"]}
{"title": "ensemble uct needs high exploitation", "abstract": "recent results have shown that the mcts algorithm ( a new , adaptive , randomized optimization algorithm ) is effective in a remarkably diverse set of applications in artificial intelligence , operations research , and high energy physics . mcts can find good solutions without domain dependent heuristics , using the uct formula to balance exploitation and exploration . it has been suggested that the optimum in the exploitation- exploration balance differs for different search tree sizes : small search trees needs more exploitation ; large search trees need more exploration . small search trees occur in variations of mcts , such as parallel and ensemble approaches . this paper investigates the possibility of improving the performance of ensemble uct by increasing the level of exploitation . as the search trees becomes smaller we achieve an improved performance . the results are important for improving the performance of large scale parallelism of mcts .", "topics": ["heuristic", "artificial intelligence"]}
{"title": "evolutionary multimodal optimization : a short survey", "abstract": "real world problems always have different multiple solutions . for instance , optical engineers need to tune the recording parameters to get as many optimal solutions as possible for multiple trials in the varied-line-spacing holographic grating design problem . unfortunately , most traditional optimization techniques focus on solving for a single optimal solution . they need to be applied several times ; yet all solutions are not guaranteed to be found . thus the multimodal optimization problem was proposed . in that problem , we are interested in not only a single optimal point , but also the others . with strong parallel search capability , evolutionary algorithms are shown to be particularly effective in solving this type of problem . in particular , the evolutionary algorithms for multimodal optimization usually not only locate multiple optima in a single run , but also preserve their population diversity throughout a run , resulting in their global optimization ability on multimodal functions . in addition , the techniques for multimodal optimization are borrowed as diversity maintenance techniques to other problems . in this chapter , we describe and review the state-of-the-arts evolutionary algorithms for multimodal optimization in terms of methodology , benchmarking , and application .", "topics": ["optimization problem"]}
{"title": "diversity regularization in deep ensembles", "abstract": "calibrating the confidence of supervised learning models is important for a variety of contexts where the certainty over predictions should be reliable . however , it has been reported that deep neural network models are often too poorly calibrated for achieving complex tasks requiring reliable uncertainty estimates in their prediction . in this work , we are proposing a strategy for training deep ensembles with a diversity function regularization , which improves the calibration property while maintaining a similar prediction accuracy .", "topics": ["supervised learning", "matrix regularization"]}
{"title": "learning vague concepts for the semantic web", "abstract": "ontologies can be a powerful tool for structuring knowledge , and they are currently the subject of extensive research . updating the contents of an ontology or improving its interoperability with other ontologies is an important but difficult process . in this paper , we focus on the presence of vague concepts , which are pervasive in natural language , within the framework of formal ontologies . we will adopt a framework in which vagueness is captured via numerical restrictions that can be automatically adjusted . since updating vague concepts , either through ontology alignment or ontology evolution , can lead to inconsistent sets of axioms , we define and implement a method to detecting and repairing such inconsistencies in a local fashion .", "topics": ["numerical analysis", "natural language"]}
{"title": "gated graph sequence neural networks", "abstract": "graph-structured data appears frequently in domains including chemistry , natural language semantics , social networks , and knowledge bases . in this work , we study feature learning techniques for graph-structured inputs . our starting point is previous work on graph neural networks ( scarselli et al . , 2009 ) , which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences . the result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models ( e.g . , lstms ) when the problem is graph-structured . we demonstrate the capabilities on some simple ai ( babi ) and graph algorithm learning tasks . we then show it achieves state-of-the-art performance on a problem from program verification , in which subgraphs need to be matched to abstract data structures .", "topics": ["feature learning", "neural networks"]}
{"title": "orthogonalized als : a theoretically principled tensor decomposition algorithm for practical use", "abstract": "the popular alternating least squares ( als ) algorithm for tensor decomposition is efficient and easy to implement , but often converges to poor local optima -- -particularly when the weights of the factors are non-uniform . we propose a modification of the als approach that is as efficient as standard als , but provably recovers the true factors with random initialization under standard incoherence assumptions on the factors of the tensor . we demonstrate the significant practical superiority of our approach over traditional als for a variety of tasks on synthetic data -- -including tensor factorization on exact , noisy and over-complete tensors , as well as tensor completion -- -and for computing word embeddings from a third-order word tri-occurrence tensor .", "topics": ["synthetic data"]}
{"title": "unassisted quantitative evaluation of despeckling filters", "abstract": "sar ( synthetic aperture radar ) imaging plays a central role in remote sensing due to , among other important features , its ability to provide high-resolution , day-and-night and almost weather-independent images . sar images are affected from a granular contamination , speckle , that can be described by a multiplicative model . many despeckling techniques have been proposed in the literature , as well as measures of the quality of the results they provide . assuming the multiplicative model , the observed image $ z $ is the product of two independent fields : the backscatter $ x $ and the speckle $ y $ . the result of any speckle filter is $ \\widehat x $ , an estimator of the backscatter $ x $ , based solely on the observed data $ z $ . an ideal estimator would be the one for which the ratio of the observed image to the filtered one $ i=z/\\widehat x $ is only speckle : a collection of independent identically distributed samples from gamma variates . we , then , assess the quality of a filter by the closeness of $ i $ to the hypothesis that it is adherent to the statistical properties of pure speckle . we analyze filters through the ratio image they produce with regards to first- and second-order statistics : the former check marginal properties , while the latter verifies lack of structure . a new quantitative image-quality index is then defined , and applied to state-of-the-art despeckling filters . this new measure provides consistent results with commonly used quality measures ( equivalent number of looks , psnr , mssim , $ \\beta $ edge correlation , and preservation of the mean ) , and ranks the filters results also in agreement with their visual analysis . we conclude our study showing that the proposed measure can be successfully used to optimize the ( often many ) parameters that define a speckle filter .", "topics": ["noise reduction"]}
{"title": "a predictive model using the markov property", "abstract": "given a data set of numerical values which are sampled from some unknown probability distribution , we will show how to check if the data set exhibits the markov property and we will show how to use the markov property to predict future values from the same distribution , with probability 1 .", "topics": ["numerical analysis"]}
{"title": "a model of virtual carrier immigration in digital images for region segmentation", "abstract": "a novel model for image segmentation is proposed , which is inspired by the carrier immigration mechanism in physical p-n junction . the carrier diffusing and drifting are simulated in the proposed model , which imitates the physical self-balancing mechanism in p-n junction . the effect of virtual carrier immigration in digital images is analyzed and studied by experiments on test images and real world images . the sign distribution of net carrier at the model 's balance state is exploited for region segmentation . the experimental results for both test images and real-world images demonstrate self-adaptive and meaningful gathering of pixels to suitable regions , which prove the effectiveness of the proposed method for image region segmentation .", "topics": ["image segmentation", "simulation"]}
{"title": "disturbance grassmann kernels for subspace-based learning", "abstract": "in this paper , we focus on subspace-based learning problems , where data elements are linear subspaces instead of vectors . to handle this kind of data , grassmann kernels were proposed to measure the space structure and used with classifiers , e.g . , support vector machines ( svms ) . however , the existing discriminative algorithms mostly ignore the instability of subspaces , which would cause the classifiers misled by disturbed instances . thus we propose considering all potential disturbance of subspaces in learning processes to obtain more robust classifiers . firstly , we derive the dual optimization of linear classifiers with disturbance subject to a known distribution , resulting in a new kernel , disturbance grassmann ( dg ) kernel . secondly , we research into two kinds of disturbance , relevant to the subspace matrix and singular values of bases , with which we extend the projection kernel on grassmann manifolds to two new kernels . experiments on action data indicate that the proposed kernels perform better compared to state-of-the-art subspace-based methods , even in a worse environment .", "topics": ["kernel ( operating system )", "support vector machine"]}
{"title": "graph 3-coloring with a hybrid self-adaptive evolutionary algorithm", "abstract": "this paper proposes a hybrid self-adaptive evolutionary algorithm for graph coloring that is hybridized with the following novel elements : heuristic genotype-phenotype mapping , a swap local search heuristic , and a neutral survivor selection operator . this algorithm was compared with the evolutionary algorithm with the saw method of eiben et al . , the tabucol algorithm of hertz and de werra , and the hybrid evolutionary algorithm of galinier and hao . the performance of these algorithms were tested on a test suite consisting of randomly generated 3-colorable graphs of various structural features , such as graph size , type , edge density , and variability in sizes of color classes . furthermore , the test graphs were generated including the phase transition where the graphs are hard to color . the purpose of the extensive experimental work was threefold : to investigate the behavior of the tested algorithms in the phase transition , to identify what impact hybridization with the dsatur traditional heuristic has on the evolutionary algorithm , and to show how graph structural features influence the performance of the graph-coloring algorithms . the results indicate that the performance of the hybrid self-adaptive evolutionary algorithm is comparable with , or better than , the performance of the hybrid evolutionary algorithm which is one of the best graph-coloring algorithms today . moreover , the fact that all the considered algorithms performed poorly on flat graphs confirms that this type of graphs is really the hardest to color .", "topics": ["heuristic"]}
{"title": "dense 3d face correspondence", "abstract": "we present an algorithm that automatically establishes dense correspondences between a large number of 3d faces . starting from automatically detected sparse correspondences on the convex hull of 3d faces , the algorithm triangulates existing correspondences and expands them iteratively along the triangle edges . new correspondences are established by matching keypoints on the geodesic patches along the triangle edges and the process is repeated . after exhausting keypoint matches , further correspondences are established by evolving level set geodesic curves from the centroids of large triangles . a deformable model ( k3dm ) is constructed from the dense corresponded faces and an algorithm is proposed for morphing the k3dm to fit unseen faces . this algorithm iterates between rigid alignment of an unseen face followed by regularized morphing of the deformable model . we have extensively evaluated the proposed algorithms on synthetic data and real 3d faces from the frgcv2 and bu3dfe databases using quantitative and qualitative benchmarks . our algorithm achieved dense correspondences with a mean localization error of 1.28mm on synthetic faces and detected 18 anthropometric landmarks on unseen real faces from the frgcv2 database with 3mm precision . furthermore , our deformable model fitting algorithm achieved 99.8 % gender classification and 98.3 % face recognition accuracy on the frgcv2 database .", "topics": ["synthetic data", "sparse matrix"]}
{"title": "intrusion detection in mobile ad hoc networks using ga based feature selection", "abstract": "mobile ad hoc networking ( manet ) has become an exciting and important technology in recent years because of the rapid proliferation of wireless devices . manets are highly vulnerable to attacks due to the open medium , dynamically changing network topology and lack of centralized monitoring point . it is important to search new architecture and mechanisms to protect the wireless networks and mobile computing application . ids analyze the network activities by means of audit data and use patterns of well-known attacks or normal profile to detect potential attacks . there are two methods to analyze : misuse detection and anomaly detection . misuse detection is not effective against unknown attacks and therefore , anomaly detection method is used . in this approach , the audit data is collected from each mobile node after simulating the attack and compared with the normal behavior of the system . if there is any deviation from normal behavior then the event is considered as an attack . some of the features of collected audit data may be redundant or contribute little to the detection process . so it is essential to select the important features to increase the detection rate . this paper focuses on implementing two feature selection methods namely , markov blanket discovery and genetic algorithm . in genetic algorithm , bayesian network is constructed over the collected features and fitness function is calculated . based on the fitness value the features are selected . markov blanket discovery also uses bayesian network and the features are selected depending on the minimum description length . during the evaluation phase , the performances of both approaches are compared based on detection rate and false alarm rate .", "topics": ["bayesian network"]}
{"title": "generating news headlines with recurrent neural networks", "abstract": "we describe an application of an encoder-decoder recurrent neural network with lstm units and attention to generating headlines from the text of news articles . we find that the model is quite effective at concisely paraphrasing news articles . furthermore , we study how the neural network decides which input words to pay attention to , and specifically we identify the function of the different neurons in a simplified attention mechanism . interestingly , our simplified attention mechanism performs better that the more complex attention mechanism on a held out set of articles .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "forecasting day-ahead electricity prices in europe : the importance of considering market integration", "abstract": "motivated by the increasing integration among electricity markets , in this paper we propose two different methods to incorporate market integration in electricity price forecasting and to improve the predictive performance . first , we propose a deep neural network that considers features from connected markets to improve the predictive accuracy in a local market . to measure the importance of these features , we propose a novel feature selection algorithm that , by using bayesian optimization and functional analysis of variance , evaluates the effect of the features on the algorithm performance . in addition , using market integration , we propose a second model that , by simultaneously predicting prices from two markets , improves the forecasting accuracy even further . as a case study , we consider the electricity market in belgium and the improvements in forecasting accuracy when using various french electricity features . we show that the two proposed models lead to improvements that are statistically significant . particularly , due to market integration , the predictive accuracy is improved from 15.7 % to 12.5 % smape ( symmetric mean absolute percentage error ) . in addition , we show that the proposed feature selection algorithm is able to perform a correct assessment , i.e . to discard the irrelevant features .", "topics": ["relevance"]}
{"title": "minimax policies for combinatorial prediction games", "abstract": "we address the online linear optimization problem when the actions of the forecaster are represented by binary vectors . our goal is to understand the magnitude of the minimax regret for the worst possible set of actions . we study the problem under three different assumptions for the feedback : full information , and the partial information models of the so-called `` semi-bandit '' , and `` bandit '' problems . we consider both $ l_\\infty $ - , and $ l_2 $ -type of restrictions for the losses assigned by the adversary . we formulate a general strategy using bregman projections on top of a potential-based gradient descent , which generalizes the ones studied in the series of papers gyorgy et al . ( 2007 ) , dani et al . ( 2008 ) , abernethy et al . ( 2008 ) , cesa-bianchi and lugosi ( 2009 ) , helmbold and warmuth ( 2009 ) , koolen et al . ( 2010 ) , uchiya et al . ( 2010 ) , kale et al . ( 2010 ) and audibert and bubeck ( 2010 ) . we provide simple proofs that recover most of the previous results . we propose new upper bounds for the semi-bandit game . moreover we derive lower bounds for all three feedback assumptions . with the only exception of the bandit game , the upper and lower bounds are tight , up to a constant factor . finally , we answer a question asked by koolen et al . ( 2010 ) by showing that the exponentially weighted average forecaster is suboptimal against $ l_ { \\infty } $ adversaries .", "topics": ["regret ( decision theory )", "optimization problem"]}
{"title": "generative adversarial active learning", "abstract": "we propose a new active learning by query synthesis approach using generative adversarial networks ( gan ) . different from regular active learning , the resulting algorithm adaptively synthesizes training instances for querying to increase learning speed . we generate queries according to the uncertainty principle , but our idea can work with other active learning principles . we report results from various numerical experiments to demonstrate the effectiveness the proposed approach . in some settings , the proposed algorithm outperforms traditional pool-based approaches . to the best our knowledge , this is the first active learning work using gan .", "topics": ["numerical analysis"]}
{"title": "low-rank matrix recovery from row-and-column affine measurements", "abstract": "we propose and study a row-and-column affine measurement scheme for low-rank matrix recovery . each measurement is a linear combination of elements in one row or one column of a matrix $ x $ . this setting arises naturally in applications from different domains . however , current algorithms developed for standard matrix recovery problems do not perform well in our case , hence the need for developing new algorithms and theory for our problem . we propose a simple algorithm for the problem based on singular value decomposition ( $ svd $ ) and least-squares ( $ ls $ ) , which we term \\alg . we prove that ( a simplified version of ) our algorithm can recover $ x $ exactly with the minimum possible number of measurements in the noiseless case . in the general noisy case , we prove performance guarantees on the reconstruction accuracy under the frobenius norm . in simulations , our row-and-column design and \\alg algorithm show improved speed , and comparable and in some cases better accuracy compared to standard measurements designs and algorithms . our theoretical and experimental results suggest that the proposed row-and-column affine measurements scheme , together with our recovery algorithm , may provide a powerful framework for affine matrix reconstruction .", "topics": ["simulation"]}
{"title": "confounder detection in high dimensional linear models using first moments of spectral measures", "abstract": "in this paper , we study the confounder detection problem in the linear model , where the target variable $ y $ is predicted using its $ n $ potential causes $ x_n= ( x_1 , ... , x_n ) ^t $ . based on an assumption of rotation invariant generating process of the model , recent study shows that the spectral measure induced by the regression coefficient vector with respect to the covariance matrix of $ x_n $ is close to a uniform measure in purely causal cases , but it differs from a uniform measure characteristically in the presence of a scalar confounder . then , analyzing spectral measure pattern could help to detect confounding . in this paper , we propose to use the first moment of the spectral measure for confounder detection . we calculate the first moment of the regression vector induced spectral measure , and compare it with the first moment of a uniform spectral measure , both defined with respect to the covariance matrix of $ x_n $ . the two moments coincide in non-confounding cases , and differ from each other in the presence of confounding . this statistical causal-confounding asymmetry can be used for confounder detection . without the need of analyzing the spectral measure pattern , our method does avoid the difficulty of metric choice and multiple parameter optimization . experiments on synthetic and real data show the performance of this method .", "topics": ["synthetic data", "causality"]}
{"title": "an improved k-means clustering based approach to detect a dna structure in h & e image of mouse tissue reacted with cd4-green antigen", "abstract": "in this manuscript we present the technique to detect and analyze the dna rich structure in haemotoxylin & eosin ( h & e ) image of a tissue treated with anti cd4 green antigen . the detection of dna rich structure can be considered as a detection of blue nuclei present through the biomedical signal/image processing technique performed on the image of the tissue obtained by the scanning electron microscope ( sem ) . earlier the tissue treated with the anti cd4 green antigen , is stained with the h & e staining solution .", "topics": ["image processing"]}
{"title": "clustering-driven deep embedding with pairwise constraints", "abstract": "recently , there has been increasing interest to leverage the competence of neural networks to analyze data . in particular , new clustering methods that employ deep embeddings have been presented . in this paper , we depart from centroid-based models and suggest a new framework , called clustering-driven deep embedding with pairwise constraints ( cpac ) , for non-parametric clustering using a neural network . we present a clustering-driven embedding based on a siamese network that encourages pairs of data points to output similar representations in the latent space . our pair-based model allows augmenting the information with labeled pairs to constitute a semi-supervised framework . our approach is based on analyzing the losses associated with each pair to refine the set of constraints . we show that clustering performance increases when using this scheme , even with a limited amount of user queries . we present state-of-the-art results on different types of datasets and compare our performance to parametric and non-parametric techniques .", "topics": ["cluster analysis"]}
{"title": "bag-of-vector embeddings of dependency graphs for semantic induction", "abstract": "vector-space models , from word embeddings to neural network parsers , have many advantages for nlp . but how to generalise from fixed-length word vectors to a vector space for arbitrary linguistic structures is still unclear . in this paper we propose bag-of-vector embeddings of arbitrary linguistic graphs . a bag-of-vector space is the minimal nonparametric extension of a vector space , allowing the representation to grow with the size of the graph , but not tying the representation to any specific tree or graph structure . we propose efficient training and inference algorithms based on tensor factorisation for embedding arbitrary graphs in a bag-of-vector space . we demonstrate the usefulness of this representation by training bag-of-vector embeddings of dependency graphs and evaluating them on unsupervised semantic induction for the semantic textual similarity and natural language inference tasks .", "topics": ["natural language processing", "natural language"]}
{"title": "fast learning and prediction for object detection using whitened cnn features", "abstract": "we combine features extracted from pre-trained convolutional neural networks ( cnns ) with the fast , linear exemplar-lda classifier to get the advantages of both : the high detection performance of cnns , automatic feature engineering , fast model learning from few training samples and efficient sliding-window detection . the adaptive real-time object detection system ( artos ) has been refactored broadly to be used in combination with caffe for the experimental studies reported in this work .", "topics": ["object detection"]}
{"title": "product kernel interpolation for scalable gaussian processes", "abstract": "recent work shows that inference for gaussian processes can be performed efficiently using iterative methods that rely only on matrix-vector multiplications ( mvms ) . structured kernel interpolation ( ski ) exploits these techniques by deriving approximate kernels with very fast mvms . unfortunately , such strategies suffer badly from the curse of dimensionality . we develop a new technique for mvm based learning that exploits product kernel structure . we demonstrate that this technique is broadly applicable , resulting in linear rather than exponential runtime with dimension for ski , as well as state-of-the-art asymptotic complexity for multi-task gps .", "topics": ["kernel ( operating system )", "approximation algorithm"]}
{"title": "multi-modal aggregation for video classification", "abstract": "in this paper , we present a solution to large-scale video classification challenge ( lsvc2017 ) [ 1 ] that ranked the 1st place . we focused on a variety of modalities that cover visual , motion and audio . also , we visualized the aggregation process to better understand how each modality takes effect . among the extracted modalities , we found temporal-spatial features calculated by 3d convolution quite promising that greatly improved the performance . we attained the official metric map 0.8741 on the testing set with the ensemble model .", "topics": ["convolution"]}
{"title": "safe policy improvement with baseline bootstrapping", "abstract": "a common goal in reinforcement learning is to derive a good strategy given a limited batch of data . in this paper , we adopt the safe policy improvement ( spi ) approach : we compute a target policy guaranteed to perform at least as well as a given baseline policy . our spi strategy , inspired by the knows-what-it-knows paradigms , consists in bootstrapping the target policy with the baseline policy when it does not know . we develop two computationally efficient bootstrapping algorithms , a value-based and a policy-based , both accompanied with theoretical spi bounds . three algorithm variants are proposed . we empirically show the literature algorithms limits on a small stochastic gridworld problem , and then demonstrate that our five algorithms not only improve the worst case scenarios , but also the mean performance .", "topics": ["baseline ( configuration management )", "computational complexity theory"]}
{"title": "developpement de methodes automatiques pour la reutilisation des composants logiciels", "abstract": "the large amount of information and the increasing complexity of applications constrain developers to have stand-alone and reusable components from libraries and component markets.our approach consists in developing methods to evaluate the quality of the software component of these libraries , on the one hand and moreover to optimize the financial cost and the adaptation 's time of these selected components . our objective function defines a metric that maximizes the value of the software component quality by minimizing the financial cost and maintenance time . this model should make it possible to classify the components and order them in order to choose the most optimized . mots-cles : d { \\'e } veloppement de m { \\'e } thode , r { \\'e } utilisation , composants logiciels , qualit { \\'e } de composant keywords : method development , reuse , software components , component quality .", "topics": ["loss function"]}
{"title": "a unified example-based and lexicalist approach to machine translation", "abstract": "we present an approach to machine translation that combines the ideas and methodologies of the example-based and lexicalist theoretical frameworks . the approach has been implemented in a multilingual machine translation system .", "topics": ["machine translation"]}
{"title": "multi-person brain activity recognition via comprehensive eeg signal analysis", "abstract": "an electroencephalography ( eeg ) based brain activity recognition is a fundamental field of study for a number of significant applications such as intention prediction , appliance control , and neurological disease diagnosis in smart home and smart healthcare domains . existing techniques mostly focus on binary brain activity recognition for a single person , which limits their deployment in wider and complex practical scenarios . therefore , multi-person and multi-class brain activity recognition has obtained popularity recently . another challenge faced by brain activity recognition is the low recognition accuracy due to the massive noises and the low signal-to-noise ratio in eeg signals . moreover , the feature engineering in eeg processing is time-consuming and highly re- lies on the expert experience . in this paper , we attempt to solve the above challenges by proposing an approach which has better eeg interpretation ability via raw electroencephalography ( eeg ) signal analysis for multi-person and multi-class brain activity recognition . specifically , we analyze inter-class and inter-person eeg signal characteristics , based on which to capture the discrepancy of inter-class eeg data . then , we adopt an autoencoder layer to automatically refine the raw eeg signals by eliminating various artifacts . we evaluate our approach on both a public and a local eeg datasets and conduct extensive experiments to explore the effect of several factors ( such as normalization methods , training data size , and autoencoder hidden neuron size ) on the recognition results . the experimental results show that our approach achieves a high accuracy comparing to competitive state-of-the-art methods , indicating its potential in promoting future research on multi-person eeg recognition .", "topics": ["test set", "autoencoder"]}
{"title": "enhanced ezw technique for compression of image by setting detail retaining pass number", "abstract": "this submission has been withdrawn by arxiv administrators because it contains excessive and unattributed reuse of content from other authors .", "topics": ["coefficient"]}
{"title": "fast global convergence via landscape of empirical loss", "abstract": "while optimizing convex objective ( loss ) functions has been a powerhouse for machine learning for at least two decades , non-convex loss functions have attracted fast growing interests recently , due to many desirable properties such as superior robustness and classification accuracy , compared with their convex counterparts . the main obstacle for non-convex estimators is that it is in general intractable to find the optimal solution . in this paper , we study the computational issues for some non-convex m-estimators . in particular , we show that the stochastic variance reduction methods converge to the global optimal with linear rate , by exploiting the statistical property of the population loss . en route , we improve the convergence analysis for the batch gradient method in \\cite { mei2016landscape } .", "topics": ["optimization problem", "loss function"]}
{"title": "online clustering of bandits", "abstract": "we introduce a novel algorithmic approach to content recommendation based on adaptive clustering of exploration-exploitation ( `` bandit '' ) strategies . we provide a sharp regret analysis of this algorithm in a standard stochastic noise setting , demonstrate its scalability properties , and prove its effectiveness on a number of artificial and real-world datasets . our experiments show a significant increase in prediction performance over state-of-the-art methods for bandit problems .", "topics": ["cluster analysis", "scalability"]}
{"title": "count-ception : counting by fully convolutional redundant counting", "abstract": "counting objects in digital images is a process that should be replaced by machines . this tedious task is time consuming and prone to errors due to fatigue of human annotators . the goal is to have a system that takes as input an image and returns a count of the objects inside and justification for the prediction in the form of object localization . we repose a problem , originally posed by lempitsky and zisserman , to instead predict a count map which contains redundant counts based on the receptive field of a smaller regression network . the regression network predicts a count of the objects that exist inside this frame . by processing the image in a fully convolutional way each pixel is going to be accounted for some number of times , the number of windows which include it , which is the size of each window , ( i.e . , 32x32 = 1024 ) . to recover the true count we take the average over the redundant predictions . our contribution is redundant counting instead of predicting a density map in order to average over errors . we also propose a novel deep neural network architecture adapted from the inception family of networks called the count-ception network . together our approach results in a 20 % relative improvement ( 2.9 to 2.3 mae ) over the state of the art method by xie , noble , and zisserman in 2016 .", "topics": ["pixel"]}
{"title": "bridging the gap between stochastic gradient mcmc and stochastic optimization", "abstract": "stochastic gradient markov chain monte carlo ( sg-mcmc ) methods are bayesian analogs to popular stochastic optimization methods ; however , this connection is not well studied . we explore this relationship by applying simulated annealing to an sgmcmc algorithm . furthermore , we extend recent sg-mcmc methods with two key components : i ) adaptive preconditioners ( as in adagrad or rmsprop ) , and ii ) adaptive element-wise momentum weights . the zero-temperature limit gives a novel stochastic optimization method with adaptive element-wise momentum weights , while conventional optimization methods only have a shared , static momentum weight . under certain assumptions , our theoretical analysis suggests the proposed simulated annealing approach converges close to the global optima . experiments on several deep neural network models show state-of-the-art results compared to related stochastic optimization algorithms .", "topics": ["simulation", "gradient"]}
{"title": "narrow artificial intelligence with machine learning for real-time estimation of a mobile agents location using hidden markov models", "abstract": "we propose to use a supervised machine learning technique to track the location of a mobile agent in real time . hidden markov models are used to build artificial intelligence that estimates the unknown position of a mobile target moving in a defined environment . this narrow artificial intelligence performs two distinct tasks . first , it provides real-time estimation of the mobile agent 's position using the forward algorithm . second , it uses the baum-welch algorithm as a statistical learning tool to gain knowledge of the mobile target . finally , an experimental environment is proposed , namely a video game that we use to test our artificial intelligence . we present statistical and graphical results to illustrate the efficiency of our method .", "topics": ["supervised learning", "artificial intelligence"]}
{"title": "deep episodic memory : encoding , recalling , and predicting episodic experiences for robot action execution", "abstract": "we present a novel deep neural network architecture for representing robot experiences in an episodic-like memory which facilitates encoding , recalling , and predicting action experiences . our proposed unsupervised deep episodic memory model 1 ) encodes observed actions in a latent vector space and , based on this latent encoding , 2 ) infers most similar episodes previously experienced , 3 ) reconstructs original episodes , and 4 ) predicts future frames in an end-to-end fashion . results show that conceptually similar actions are mapped into the same region of the latent vector space . based on these results , we introduce an action matching and retrieval mechanism , benchmark its performance on two large-scale action datasets , 20bn-something-something and activitynet and evaluate its generalization capability in a real-world scenario on a humanoid robot .", "topics": ["end-to-end principle", "robot"]}
{"title": "agnostic active learning without constraints", "abstract": "we present and analyze an agnostic active learning algorithm that works without keeping a version space . this is unlike all previous approaches where a restricted set of candidate hypotheses is maintained throughout learning , and only hypotheses from this set are ever returned . by avoiding this version space approach , our algorithm sheds the computational burden and brittleness associated with maintaining version spaces , yet still allows for substantial improvements over supervised learning for classification .", "topics": ["supervised learning"]}
{"title": "deep multi-task representation learning : a tensor factorisation approach", "abstract": "most contemporary multi-task learning methods assume linear models . this setting is considered shallow in the era of deep learning . in this paper , we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network . our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional mtl algorithms to tensor factorisation , to realise automatic learning of end-to-end knowledge sharing in deep networks . this is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy . our approach applies to both homogeneous and heterogeneous mtl . experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices .", "topics": ["feature learning", "end-to-end principle"]}
{"title": "mot16 : a benchmark for multi-object tracking", "abstract": "standardized benchmarks are crucial for the majority of computer vision applications . although leaderboards and ranking tables should not be over-claimed , benchmarks often provide the most objective measure of performance and are therefore important guides for reseach . recently , a new benchmark for multiple object tracking , motchallenge , was launched with the goal of collecting existing and new data and creating a framework for the standardized evaluation of multiple object tracking methods . the first release of the benchmark focuses on multiple people tracking , since pedestrians are by far the most studied object in the tracking community . this paper accompanies a new release of the motchallenge benchmark . unlike the initial release , all videos of mot16 have been carefully annotated following a consistent protocol . moreover , it not only offers a significant increase in the number of labeled boxes , but also provides multiple object classes beside pedestrians and the level of visibility for every single object of interest .", "topics": ["computer vision"]}
{"title": "joint recognition of handwritten text and named entities with a neural end-to-end model", "abstract": "when extracting information from handwritten documents , text transcription and named entity recognition are usually faced as separate subsequent tasks . this has the disadvantage that errors in the first module affect heavily the performance of the second module . in this work we propose to do both tasks jointly , using a single neural network with a common architecture used for plain text recognition . experimentally , the work has been tested on a collection of historical marriage records . results of experiments are presented to show the effect on the performance for different configurations : different ways of encoding the information , doing or not transfer learning and processing at text line or multi-line region level . the results are comparable to state of the art reported in the icdar 2017 information extraction competition , even though the proposed technique does not use any dictionaries , language modeling or post processing .", "topics": ["dictionary"]}
{"title": "image retrieval with fisher vectors of binary features", "abstract": "recently , the fisher vector representation of local features has attracted much attention because of its effectiveness in both image classification and image retrieval . another trend in the area of image retrieval is the use of binary features such as orb , freak , and brisk . considering the significant performance improvement for accuracy in both image classification and retrieval by the fisher vector of continuous feature descriptors , if the fisher vector were also to be applied to binary features , we would receive similar benefits in binary feature based image retrieval and classification . in this paper , we derive the closed-form approximation of the fisher vector of binary features modeled by the bernoulli mixture model . we also propose accelerating the fisher vector by using the approximate value of posterior probability . experiments show that the fisher vector representation significantly improves the accuracy of image retrieval compared with a bag of binary words approach .", "topics": ["computer vision"]}
{"title": "automatic quality assessment for speech translation using joint asr and mt features", "abstract": "this paper addresses automatic quality assessment of spoken language translation ( slt ) . this relatively new task is defined and formalized as a sequence labeling problem where each word in the slt hypothesis is tagged as good or bad according to a large feature set . we propose several word confidence estimators ( wce ) based on our automatic evaluation of transcription ( asr ) quality , translation ( mt ) quality , or both ( combined asr+mt ) . this research work is possible because we built a specific corpus which contains 6.7k utterances for which a quintuplet containing : asr output , verbatim transcript , text translation , speech translation and post-edition of translation is built . the conclusion of our multiple experiments using joint asr and mt features for wce is that mt features remain the most influent while asr feature can bring interesting complementary information . our robust quality estimators for slt can be used for re-scoring speech translation graphs or for providing feedback to the user in interactive speech translation or computer-assisted speech-to-text scenarios .", "topics": ["machine translation", "speech recognition"]}
{"title": "an algorithm to co-ordinate anaphora resolution and pps disambiguation process", "abstract": "this paper concerns both anaphora resolution and prepositional phrase ( pp ) attachment that are the most frequent ambiguities in natural language processing . several methods have been proposed to deal with each phenomenon separately , however none of proposed systems has considered the way of dealing both phenomena . we tackle this issue , proposing an algorithm to co-ordinate the treatment of these two problems efficiently , i.e . , the aim is also to exploit at each step all the results that each component can provide .", "topics": ["natural language processing"]}
{"title": "accelerometer based activity classification with variational inference on sticky hdp-slds", "abstract": "as part of daily monitoring of human activities , wearable sensors and devices are becoming increasingly popular sources of data . with the advent of smartphones equipped with acceloremeter , gyroscope and camera ; it is now possible to develop activity classification platforms everyone can use conveniently . in this paper , we propose a fast inference method for an unsupervised non-parametric time series model namely variational inference for sticky hdp-slds ( hierarchical dirichlet process switching linear dynamical system ) . we show that the proposed algorithm can differentiate various indoor activities such as sitting , walking , turning , going up/down the stairs and taking the elevator using only the acceloremeter of an android smartphone samsung galaxy s4 . we used the front camera of the smartphone to annotate activity types precisely . we compared the proposed method with hidden markov models with gaussian emission probabilities on a dataset of 10 subjects . we showed that the efficacy of the stickiness property . we further compared the variational inference to the gibbs sampler on the same model and show that variational inference is faster in one order of magnitude .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "random algorithms for the loop cutset problem", "abstract": "we show how to find a minimum loop cutset in a bayesian network with high probability . finding such a loop cutset is the first step in pearl 's method of conditioning for inference . our random algorithm for finding a loop cutset , called `` repeated wguessi '' , outputs a minimum loop cutset , after o ( c 6^k k n ) steps , with probability at least 1- ( 1 over { 6^k } ) ^ { c 6^k } ) , where c > 1 is a constant specified by the user , k is the size of a minimum weight loop cutset , and n is the number of vertices . we also show empirically that a variant of this algorithm , called wra , often finds a loop cutset that is closer to the minimum loop cutset than the ones found by the best deterministic algorithms known .", "topics": ["bayesian network"]}
{"title": "an incremental parser for abstract meaning representation", "abstract": "meaning representation ( amr ) is a semantic representation for natural language that embeds annotations related to traditional tasks such as named entity recognition , semantic role labeling , word sense disambiguation and co-reference resolution . we describe a transition-based parser for amr that parses sentences left-to-right , in linear time . we further propose a test-suite that assesses specific subtasks that are helpful in comparing amr parsers , and show that our parser is competitive with the state of the art on the ldc2015e86 dataset and that it outperforms state-of-the-art parsers for recovering named entities and handling polarity .", "topics": ["parsing", "time complexity"]}
{"title": "masked conditional neural networks for automatic sound events recognition", "abstract": "deep neural network architectures designed for application domains other than sound , especially image recognition , may not optimally harness the time-frequency representation when adapted to the sound recognition problem . in this work , we explore the conditional neural network ( clnn ) and the masked conditional neural network ( mclnn ) for multi-dimensional temporal signal recognition . the clnn considers the inter-frame relationship , and the mclnn enforces a systematic sparseness over the network 's links to enable learning in frequency bands rather than bins allowing the network to be frequency shift invariant mimicking a filterbank . the mask also allows considering several combinations of features concurrently , which is usually handcrafted through exhaustive manual search . we applied the mclnn to the environmental sound recognition problem using the esc-10 and esc-50 datasets . mclnn achieved competitive performance , using 12 % of the parameters and without augmentation , compared to state-of-the-art convolutional neural networks .", "topics": ["neural networks", "computer vision"]}
{"title": "pseudo-recursal : solving the catastrophic forgetting problem in deep neural networks", "abstract": "in general , neural networks are not currently capable of learning tasks in a sequential fashion . when a novel , unrelated task is learnt by a neural network , it substantially forgets how to solve previously learnt tasks . one of the original solutions to this problem is pseudo-rehearsal , which involves learning the new task while rehearsing generated items representative of the previous task/s . this is very effective for simple tasks . however , pseudo-rehearsal has not yet been successfully applied to very complex tasks because in these tasks it is difficult to generate representative items . we accomplish pseudo-rehearsal by using a generative adversarial network to generate items so that our deep network can learn to sequentially classify the cifar-10 , svhn and mnist datasets . after training on all tasks , our network loses only 1.67 % absolute accuracy on cifar-10 and gains 0.24 % absolute accuracy on svhn . our model 's performance is a substantial improvement compared to the current state of the art solution .", "topics": ["neural networks", "mnist database"]}
{"title": "scalable sparse subspace clustering by orthogonal matching pursuit", "abstract": "subspace clustering methods based on $ \\ell_1 $ , $ \\ell_2 $ or nuclear norm regularization have become very popular due to their simplicity , theoretical guarantees and empirical success . however , the choice of the regularizer can greatly impact both theory and practice . for instance , $ \\ell_1 $ regularization is guaranteed to give a subspace-preserving affinity ( i.e . , there are no connections between points from different subspaces ) under broad conditions ( e.g . , arbitrary subspaces and corrupted data ) . however , it requires solving a large scale convex optimization problem . on the other hand , $ \\ell_2 $ and nuclear norm regularization provide efficient closed form solutions , but require very strong assumptions to guarantee a subspace-preserving affinity , e.g . , independent subspaces and uncorrupted data . in this paper we study a subspace clustering method based on orthogonal matching pursuit . we show that the method is both computationally efficient and guaranteed to give a subspace-preserving affinity under broad conditions . experiments on synthetic data verify our theoretical analysis , and applications in handwritten digit and face clustering show that our approach achieves the best trade off between accuracy and efficiency .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "learning end-to-end goal-oriented dialog", "abstract": "traditional dialog systems used in goal-oriented applications require a lot of domain-specific handcrafting , which hinders scaling up to new domains . end-to-end dialog systems , in which all components are trained from the dialogs themselves , escape this limitation . but the encouraging success recently obtained in chit-chat dialog may not carry over to goal-oriented settings . this paper proposes a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications . set in the context of restaurant reservation , our tasks require manipulating sentences and symbols , so as to properly conduct conversations , issue api calls and use the outputs of such calls . we show that an end-to-end dialog system based on memory networks can reach promising , yet imperfect , performance and learn to perform non-trivial operations . we confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second dialog state tracking challenge ( henderson et al . , 2014a ) . we show similar result patterns on data extracted from an online concierge service .", "topics": ["baseline ( configuration management )", "end-to-end principle"]}
{"title": "the earth ai n't flat : monocular reconstruction of vehicles on steep and graded roads from a moving camera", "abstract": "accurate localization of other traffic participants is a vital task in autonomous driving systems . state-of-the-art systems employ a combination of sensing modalities such as rgb cameras and lidars for localizing traffic participants , but most such demonstrations have been confined to plain roads . we demonstrate , to the best of our knowledge , the first results for monocular object localization and shape estimation on surfaces that do not share the same plane with the moving monocular camera . we approximate road surfaces by local planar patches and use semantic cues from vehicles in the scene to initialize a local bundle-adjustment like procedure that simultaneously estimates the pose and shape of the vehicles , and the orientation of the local ground plane on which the vehicle stands as well . we evaluate the proposed approach on the kitti and synthia-sf benchmarks , for a variety of road plane configurations . the proposed approach significantly improves the state-of-the-art for monocular object localization on arbitrarily-shaped roads .", "topics": ["autonomous car"]}
{"title": "object recognition from short videos for robotic perception", "abstract": "deep neural networks have become the primary learning technique for object recognition . videos , unlike still images , are temporally coherent which makes the application of deep networks non-trivial . here , we investigate how motion can aid object recognition in short videos . our approach is based on long short-term memory ( lstm ) deep networks . unlike previous applications of lstms , we implement each gate as a convolution . we show that convolutional-based lstm models are capable of learning motion dependencies and are able to improve the recognition accuracy when more frames in a sequence are available . we evaluate our approach on the washington rgbd object dataset and on the washington rgbd scenes dataset . our approach outperforms deep nets applied to still images and sets a new state-of-the-art in this domain .", "topics": ["convolution"]}
{"title": "temporal-difference learning to assist human decision making during the control of an artificial limb", "abstract": "in this work we explore the use of reinforcement learning ( rl ) to help with human decision making , combining state-of-the-art rl algorithms with an application to prosthetics . managing human-machine interaction is a problem of considerable scope , and the simplification of human-robot interfaces is especially important in the domains of biomedical technology and rehabilitation medicine . for example , amputees who control artificial limbs are often required to quickly switch between a number of control actions or modes of operation in order to operate their devices . we suggest that by learning to anticipate ( predict ) a user 's behaviour , artificial limbs could take on an active role in a human 's control decisions so as to reduce the burden on their users . recently , we showed that rl in the form of general value functions ( gvfs ) could be used to accurately detect a user 's control intent prior to their explicit control choices . in the present work , we explore the use of temporal-difference learning and gvfs to predict when users will switch their control influence between the different motor functions of a robot arm . experiments were performed using a multi-function robot arm that was controlled by muscle signals from a user 's body ( similar to conventional artificial limb control ) . our approach was able to acquire and maintain forecasts about a user 's switching decisions in real time . it also provides an intuitive and reward-free way for users to correct or reinforce the decisions made by the machine learning system . we expect that when a system is certain enough about its predictions , it can begin to take over switching decisions from the user to streamline control and potentially decrease the time and effort needed to complete tasks . this preliminary study therefore suggests a way to naturally integrate human- and machine-based decision making systems .", "topics": ["reinforcement learning"]}
{"title": "spikes as regularizers", "abstract": "we present a confidence-based single-layer feed-forward learning algorithm spiral ( spike regularized adaptive learning ) relying on an encoding of activation spikes . we adaptively update a weight vector relying on confidence estimates and activation offsets relative to previous activity . we regularize updates proportionally to item-level confidence and weight-specific support , loosely inspired by the observation from neurophysiology that high spike rates are sometimes accompanied by low temporal precision . our experiments suggest that the new learning algorithm spiral is more robust and less prone to overfitting than both the averaged perceptron and arow .", "topics": ["sampling ( signal processing )", "value ( ethics )"]}
{"title": "case base mining for adaptation knowledge acquisition", "abstract": "in case-based reasoning , the adaptation of a source case in order to solve the target problem is at the same time crucial and difficult to implement . the reason for this difficulty is that , in general , adaptation strongly depends on domain-dependent knowledge . this fact motivates research on adaptation knowledge acquisition ( aka ) . this paper presents an approach to aka based on the principles and techniques of knowledge discovery from databases and data-mining . it is implemented in cabamaka , a system that explores the variations within the case base to elicit adaptation knowledge . this system has been successfully tested in an application of case-based reasoning to decision support in the domain of breast cancer treatment .", "topics": ["data mining", "database"]}
{"title": "fantastic 4 system for nist 2015 language recognition evaluation", "abstract": "this article describes the systems jointly submitted by institute for infocomm ( i $ ^2 $ r ) , the laboratoire d'informatique de l'universit\\'e du maine ( lium ) , nanyang technology university ( ntu ) and the university of eastern finland ( uef ) for 2015 nist language recognition evaluation ( lre ) . the submitted system is a fusion of nine sub-systems based on i-vectors extracted from different types of features . given the i-vectors , several classifiers are adopted for the language detection task including support vector machines ( svm ) , multi-class logistic regression ( mclr ) , probabilistic linear discriminant analysis ( plda ) and deep neural networks ( dnn ) .", "topics": ["support vector machine"]}
{"title": "top-down and bottom-up feature combination for multi-sensor attentive robots", "abstract": "the information available to robots in real tasks is widely distributed both in time and space , requiring the agent to search for relevant data . in humans , that face the same problem when sounds , images and smells are presented to their sensors in a daily scene , a natural system is applied : attention . as vision plays an important role in our routine , most research regarding attention has involved this sensorial system and the same has been replicated to the robotics field . however , most of the robotics tasks nowadays do not rely only in visual data , that are still costly . to allow the use of attentive concepts with other robotics sensors that are usually used in tasks such as navigation , self-localization , searching and mapping , a generic attentional model has been previously proposed . in this work , feature mapping functions were designed to build feature maps to this attentive model from data from range scanner and sonar sensors . experiments were performed in a high fidelity simulated robotics environment and results have demonstrated the capability of the model on dealing with both salient stimuli and goal-driven attention over multiple features extracted from multiple sensors .", "topics": ["simulation", "map"]}
{"title": "generating videos with scene dynamics", "abstract": "we capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks ( e.g . action classification ) and video generation tasks ( e.g . future prediction ) . we propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene 's foreground from the background . experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines , and we show its utility at predicting plausible futures of static images . moreover , experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision , suggesting scene dynamics are a promising signal for representation learning . we believe generative video models can impact many applications in video understanding and simulation .", "topics": ["baseline ( configuration management )", "feature learning"]}
{"title": "bayesian sparsification of recurrent neural networks", "abstract": "recurrent neural networks show state-of-the-art results in many text analysis tasks but often require a lot of memory to store their weights . recently proposed sparse variational dropout eliminates the majority of the weights in a feed-forward neural network without significant loss of quality . we apply this technique to sparsify recurrent neural networks . to account for recurrent specifics we also rely on binary variational dropout for rnn . we report 99.5 % sparsity level on sentiment analysis task without a quality drop and up to 87 % sparsity level on language modeling task with slight loss of accuracy .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "a genetic algorithm for structure-activity relationships : software implementation", "abstract": "the design and the implementation of a genetic algorithm are described . the applicability domain is on structure-activity relationships expressed as multiple linear regressions and predictor variables are from families of structure-based molecular descriptors . an experiment to compare different selection and survival strategies was designed and realized . the genetic algorithm was run using the designed experiment on a set of 206 polychlorinated biphenyls searching on structure-activity relationships having known the measured octanol-water partition coefficients and a family of molecular descriptors . the experiment shows that different selection and survival strategies create different partitions on the entire population of all possible genotypes .", "topics": ["coefficient"]}
{"title": "multimodal memory modelling for video captioning", "abstract": "video captioning which automatically translates video clips into natural language sentences is a very important task in computer vision . by virtue of recent deep learning technologies , e.g . , convolutional neural networks ( cnns ) and recurrent neural networks ( rnns ) , video captioning has made great progress . however , learning an effective mapping from visual sequence space to language space is still a challenging problem . in this paper , we propose a multimodal memory model ( m3 ) to describe videos , which builds a visual and textual shared memory to model the long-term visual-textual dependency and further guide global visual attention on described targets . specifically , the proposed m3 attaches an external memory to store and retrieve both visual and textual contents by interacting with video and sentence with multiple read and write operations . first , text representation in the long short-term memory ( lstm ) based text decoder is written into the memory , and the memory contents will be read out to guide an attention to select related visual targets . then , the selected visual information is written into the memory , which will be further read out to the text decoder . to evaluate the proposed model , we perform experiments on two publicly benchmark datasets : msvd and msr-vtt . the experimental results demonstrate that our method outperforms the state-of-theart methods in terms of bleu and meteor .", "topics": ["recurrent neural network", "computer vision"]}
{"title": "real-time top-k predictive query processing over event streams", "abstract": "this paper addresses the problem of predicting the k events that are most likely to occur next , over historical real-time event streams . existing approaches to causal prediction queries have a number of limitations . first , they exhaustively search over an acyclic causal network to find the most likely k effect events ; however , data from real event streams frequently reflect cyclic causality . second , they contain conservative assumptions intended to exclude all possible non-causal links in the causal network ; it leads to the omission of many less-frequent but important causal links . we overcome these limitations by proposing a novel event precedence model and a run-time causal inference mechanism . the event precedence model constructs a first order absorbing markov chain incrementally over event streams , where an edge between two events signifies a temporal precedence relationship between them , which is a necessary condition for causality . then , the run-time causal inference mechanism learns causal relationships dynamically during query processing . this is done by removing some of the temporal precedence relationships that do not exhibit causality in the presence of other events in the event precedence model . this paper presents two query processing algorithms -- one performs exhaustive search on the model and the other performs a more efficient reduced search with early termination . experiments using two real datasets ( cascading blackouts in power systems and web page views ) verify the effectiveness of the probabilistic top-k prediction queries and the efficiency of the algorithms . specifically , the reduced search algorithm reduced runtime , relative to exhaustive search , by 25-80 % ( depending on the application ) with only a small reduction in accuracy .", "topics": ["bayesian network", "causality"]}
{"title": "cloud no longer a silver bullet , edge to the rescue", "abstract": "this paper takes the position that , while cognitive computing today relies heavily on the cloud , we will soon see a paradigm shift where cognitive computing primarily happens on network edges . the shift toward edge devices is fundamentally propelled both by technological constraints in data centers and wireless network infrastructures , as well as practical considerations such as privacy and safety . the remainder of this paper lays out our view of how these constraints will impact future cognitive computing . bringing cognitive computing to edge devices opens up several new opportunities and challenges , some of which demand new solutions and some of which require us to revisit entrenched techniques in light of new technologies . we close the paper with a call to action for future research .", "topics": ["computation"]}
{"title": "linear convergence of proximal gradient algorithm with extrapolation for a class of nonconvex nonsmooth minimization problems", "abstract": "in this paper , we study the proximal gradient algorithm with extrapolation for minimizing the sum of a lipschitz differentiable function and a proper closed convex function . under the error bound condition used in [ 19 ] for analyzing the convergence of the proximal gradient algorithm , we show that there exists a threshold such that if the extrapolation coefficients are chosen below this threshold , then the sequence generated converges $ r $ -linearly to a stationary point of the problem . moreover , the corresponding sequence of objective values is also $ r $ -linearly convergent . in addition , the threshold reduces to $ 1 $ for convex problems and , as a consequence , we obtain the $ r $ -linear convergence of the sequence generated by fista with fixed restart . finally , we present some numerical experiments to illustrate our results .", "topics": ["numerical analysis", "gradient descent"]}
{"title": "light field super-resolution using a low-rank prior and deep convolutional neural networks", "abstract": "light field imaging has recently known a regain of interest due to the availability of practical light field capturing systems that offer a wide range of applications in the field of computer vision . however , capturing high-resolution light fields remains technologically challenging since the increase in angular resolution is often accompanied by a significant reduction in spatial resolution . this paper describes a learning-based spatial light field super-resolution method that allows the restoration of the entire light field with consistency across all sub-aperture images . the algorithm first uses optical flow to align the light field and then reduces its angular dimension using low-rank approximation . we then consider the linearly independent columns of the resulting low-rank model as an embedding , which is restored using a deep convolutional neural network ( dcnn ) . the super-resolved embedding is then used to reconstruct the remaining sub-aperture images . the original disparities are restored using inverse warping where missing pixels are approximated using a novel light field inpainting algorithm . experimental results show that the proposed method outperforms existing light field super-resolution algorithms , achieving psnr gains of 0.23 db over the second best performing method . this performance can be further improved using iterative back-projection as a post-processing step .", "topics": ["computer vision", "approximation"]}
{"title": "image based camera localization : an overview", "abstract": "recently , virtual reality , augmented reality , robotics , self-driving cars et al attract much attention of industrial community , in which image based camera localization is a key task . it is urgent to give an overview of image based camera localization . in this paper , an overview of image based camera localization is presented . it will be useful to not only researchers but also engineers and other people interested .", "topics": ["autonomous car"]}
{"title": "explorations in engagement for humans and robots", "abstract": "this paper explores the concept of engagement , the process by which individuals in an interaction start , maintain and end their perceived connection to one another . the paper reports on one aspect of engagement among human interactors -- the effect of tracking faces during an interaction . it also describes the architecture of a robot that can participate in conversational , collaborative interactions with engagement gestures . finally , the paper reports on findings of experiments with human participants who interacted with a robot when it either performed or did not perform engagement gestures . results of the human-robot studies indicate that people become engaged with robots : they direct their attention to the robot more often in interactions where engagement gestures are present , and they find interactions more appropriate when engagement gestures are present than when they are not .", "topics": ["interaction", "robot"]}
{"title": "learning sparse , distributed representations using the hebbian principle", "abstract": "the `` fire together , wire together '' hebbian model is a central principle for learning in neuroscience , but surprisingly , it has found limited applicability in modern machine learning . in this paper , we take a first step towards bridging this gap , by developing flavors of competitive hebbian learning which produce sparse , distributed neural codes using online adaptation with minimal tuning . we propose an unsupervised algorithm , termed adaptive hebbian learning ( ahl ) . we illustrate the distributed nature of the learned representations via output entropy computations for synthetic data , and demonstrate superior performance , compared to standard alternatives such as autoencoders , in training a deep convolutional net on standard image datasets .", "topics": ["synthetic data", "sparse matrix"]}
{"title": "spatially coherent random forests", "abstract": "spatially coherent random forest ( scrf ) extends random forest to create spatially coherent labeling . each split function in scrf is evaluated based on a traditional information gain measure that is regularized by a spatial coherency term . this way , scrf is encouraged to choose split functions that cluster pixels both in appearance space and in image space . in particular , we use scrf to detect contours in images , where contours are taken to be the boundaries between different regions . each tree in the forest produces a segmentation of the image plane and the boundaries of the segmentations of all trees are aggregated to produce a final hierarchical contour map . we show that this modification improves the performance of regular random forest by about 10 % on the standard berkeley segmentation datasets . we believe that scrf can be used in other settings as well .", "topics": ["pixel"]}
{"title": "sentiment classification using images and label embeddings", "abstract": "in this project we analysed how much semantic information images carry , and how much value image data can add to sentiment analysis of the text associated with the images . to better understand the contribution from images , we compared models which only made use of image data , models which only made use of text data , and models which combined both data types . we also analysed if this approach could help sentiment classifiers generalize to unknown sentiments .", "topics": ["text corpus"]}
{"title": "image labeling by assignment", "abstract": "we introduce a novel geometric approach to the image labeling problem . abstracting from specific labeling applications , a general objective function is defined on a manifold of stochastic matrices , whose elements assign prior data that are given in any metric space , to observed image measurements . the corresponding riemannian gradient flow entails a set of replicator equations , one for each data point , that are spatially coupled by geometric averaging on the manifold . starting from uniform assignments at the barycenter as natural initialization , the flow terminates at some global maximum , each of which corresponds to an image labeling that uniquely assigns the prior data . our geometric variational approach constitutes a smooth non-convex inner approximation of the general image labeling problem , implemented with sparse interior-point numerics in terms of parallel multiplicative updates that converge efficiently .", "topics": ["calculus of variations", "optimization problem"]}
{"title": "a graph laplacian regularization for hyperspectral data unmixing", "abstract": "this paper introduces a graph laplacian regularization in the hyperspectral unmixing formulation . the proposed regularization relies upon the construction of a graph representation of the hyperspectral image . each node in the graph represents a pixel 's spectrum , and edges connect spectrally and spatially similar pixels . the proposed graph framework promotes smoothness in the estimated abundance maps and collaborative estimation between homogeneous areas of the image . the resulting convex optimization problem is solved using the alternating direction method of multipliers ( admm ) . a special attention is given to the computational complexity of the algorithm , and graph-cut methods are proposed in order to reduce the computational burden . finally , simulations conducted on synthetic data illustrate the effectiveness of the graph laplacian regularization with respect to other classical regularizations for hyperspectral unmixing .", "topics": ["optimization problem", "computational complexity theory"]}
{"title": "a parallel corpus of python functions and documentation strings for automated code documentation and code generation", "abstract": "automated documentation of programming source code and automated code generation from natural language are challenging tasks of both practical and scientific interest . progress in these areas has been limited by the low availability of parallel corpora of code and natural language descriptions , which tend to be small and constrained to specific domains . in this work we introduce a large and diverse parallel corpus of a hundred thousands python functions with their documentation strings ( `` docstrings '' ) generated by scraping open source repositories on github . we describe baseline results for the code documentation and code generation tasks obtained by neural machine translation . we also experiment with data augmentation techniques to further increase the amount of training data . we release our datasets and processing scripts in order to stimulate research in these areas .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "on sat models enumeration in itemset mining", "abstract": "frequent itemset mining is an essential part of data analysis and data mining . recent works propose interesting sat-based encodings for the problem of discovering frequent itemsets . our aim in this work is to define strategies for adapting sat solvers to such encodings in order to improve models enumeration . in this context , we deeply study the effects of restart , branching heuristics and clauses learning . we then conduct an experimental evaluation on sat-based itemset mining instances to show how sat solvers can be adapted to obtain an efficient sat model enumerator .", "topics": ["data mining", "heuristic"]}
{"title": "neural adaptive sequential monte carlo", "abstract": "sequential monte carlo ( smc ) , or particle filtering , is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions . like other importance sampling-based methods , performance is critically dependent on the proposal distribution : a bad proposal can lead to arbitrarily inaccurate estimates of the target distribution . this paper presents a new method for automatically adapting the proposal using an approximation of the kullback-leibler divergence between the true posterior and the proposal distribution . the method is very flexible , applicable to any parameterized proposal distribution and it supports online and batch variants . we use the new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to neural adaptive sequential monte carlo ( nasmc ) . experiments indicate that nasmc significantly improves inference in a non-linear state space model outperforming adaptive proposal methods including the extended kalman and unscented particle filters . experiments also indicate that improved inference translates into improved parameter learning when nasmc is used as a subroutine of particle marginal metropolis hastings . finally we show that nasmc is able to train a latent variable recurrent neural network ( lv-rnn ) achieving results that compete with the state-of-the-art for polymorphic music modelling . nasmc can be seen as bridging the gap between adaptive smc methods and the recent work in scalable , black-box variational inference .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "evaluation method of word embedding by roots and affixes", "abstract": "word embedding has been shown to be remarkably effective in a lot of natural language processing tasks . however , existing models still have a couple of limitations in interpreting the dimensions of word vector . in this paper , we provide a new approach -- -roots and affixes model ( raam ) -- -to interpret it from the intrinsic structures of natural language . also it can be used as an evaluation measure of the quality of word embedding . we introduce the information entropy into our model and divide the dimensions into two categories , just like roots and affixes in lexical semantics . then considering each category as a whole rather than individually . we experimented with english wikipedia corpus . our result show that there is a negative linear relation between the two attributes and a high positive correlation between our model and downstream semantic evaluation tasks .", "topics": ["natural language processing", "natural language"]}
{"title": "detecting off-topic responses to visual prompts", "abstract": "automated methods for essay scoring have made great progress in recent years , achieving accuracies very close to human annotators . however , a known weakness of such automated scorers is not taking into account the semantic relevance of the submitted text . while there is existing work on detecting answer relevance given a textual prompt , very little previous research has been done to incorporate visual writing prompts . we propose a neural architecture and several extensions for detecting off-topic responses to visual prompts and evaluate it on a dataset of texts written by language learners .", "topics": ["relevance"]}
{"title": "translation-invariant shrinkage/thresholding of group sparse signals", "abstract": "this paper addresses signal denoising when large-amplitude coefficients form clusters ( groups ) . the l1-norm and other separable sparsity models do not capture the tendency of coefficients to cluster ( group sparsity ) . this work develops an algorithm , called 'overlapping group shrinkage ' ( ogs ) , based on the minimization of a convex cost function involving a group-sparsity promoting penalty function . the groups are fully overlapping so the denoising method is translation-invariant and blocking artifacts are avoided . based on the principle of majorization-minimization ( mm ) , we derive a simple iterative minimization algorithm that reduces the cost function monotonically . a procedure for setting the regularization parameter , based on attenuating the noise to a specified level , is also described . the proposed approach is illustrated on speech enhancement , wherein the ogs approach is applied in the short-time fourier transform ( stft ) domain . the denoised speech produced by ogs does not suffer from musical noise .", "topics": ["noise reduction", "matrix regularization"]}
{"title": "dictionary learning for robotic grasp recognition and detection", "abstract": "the ability to grasp ordinary and potentially never-seen objects is an important feature in both domestic and industrial robotics . for a system to accomplish this , it must autonomously identify grasping locations by using information from various sensors , such as microsoft kinect 3d camera . despite numerous progress , significant work still remains to be done in this field . to this effect , we propose a dictionary learning and sparse representation ( dlsr ) framework for representing rgbd images from 3d sensors in the context of determining such good grasping locations . in contrast to previously proposed approaches that relied on sophisticated regularization or very large datasets , the derived perception system has a fast training phase and can work with small datasets . it is also theoretically founded for dealing with masked-out entries , which are common with 3d sensors . we contribute by presenting a comparative study of several dlsr approach combinations for recognizing and detecting grasp candidates on the standard cornell dataset . importantly , experimental results show a performance improvement of 1.69 % in detection and 3.16 % in recognition over current state-of-the-art convolutional neural network ( cnn ) . even though nowadays most popular vision-based approach is cnn , this suggests that dlsr is also a viable alternative with interesting advantages that cnn has not .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "application of backpropagation neural networks to both stages of fingerprinting based wips", "abstract": "we propose a scheme to employ backpropagation neural networks ( bpnns ) for both stages of fingerprinting-based indoor positioning using wlan/wifi signal strengths ( fwips ) : radio map construction during the offline stage , and localization during the online stage . given a training radio map ( trm ) , i.e . , a set of coordinate vectors and associated wlan/wifi signal strengths of the available access points , a bpnn can be trained to output the expected signal strengths for any input position within the region of interest ( bpnn-rm ) . this can be used to provide a continuous representation of the radio map and to filter , densify or decimate a discrete radio map . correspondingly , the trm can also be used to train another bpnn to output the expected position within the region of interest for any input vector of recorded signal strengths and thus carry out localization ( bpnn-la ) .key aspects of the design of such artificial neural networks for a specific application are the selection of design parameters like the number of hidden layers and nodes within the network , and the training procedure . summarizing extensive numerical simulations , based on real measurements in a testbed , we analyze the impact of these design choices on the performance of the bpnn and compare the results in particular to those obtained using the $ k $ nearest neighbors ( $ k $ nn ) and weighted $ k $ nearest neighbors approaches to fwips .", "topics": ["numerical analysis", "simulation"]}
{"title": "a decentralized proximal-gradient method with network independent step-sizes and separated convergence rates", "abstract": "this paper considers the problem of decentralized optimization with a composite objective containing smooth and non-smooth terms . to solve the problem , a proximal-gradient scheme is studied . specifically , the smooth and nonsmooth terms are dealt with by gradient update and proximal update , respectively . the studied algorithm is closely related to a previous decentralized optimization algorithm , pg-extra [ 37 ] , but has a few advantages . first of all , in our new scheme , agents use uncoordinated step-sizes and the stable upper bounds on step-sizes are independent from network topology . the step-sizes depend on local objective functions , and they can be as large as that of the gradient descent . secondly , for the special case without non-smooth terms , linear convergence can be achieved under the strong convexity assumption . the dependence of the convergence rate on the objective functions and the network are separated , and the convergence rate of our new scheme is as good as one of the two convergence rates that match the typical rates for the general gradient descent and the consensus averaging . we also provide some numerical experiments to demonstrate the efficacy of the introduced algorithms and validate our theoretical discoveries .", "topics": ["numerical analysis", "gradient descent"]}
{"title": "photo-realistic facial texture transfer", "abstract": "style transfer methods have achieved significant success in recent years with the use of convolutional neural networks . however , many of these methods concentrate on artistic style transfer with few constraints on the output image appearance . we address the challenging problem of transferring face texture from a style face image to a content face image in a photorealistic manner without changing the identity of the original content image . our framework for face texture transfer ( facetex ) augments the prior work of mrf-cnn with a novel facial semantic regularization that incorporates a face prior regularization smoothly suppressing the changes around facial meso-structures ( e.g eyes , nose and mouth ) and a facial structure loss function which implicitly preserves the facial structure so that face texture can be transferred without changing the original identity . we demonstrate results on face images and compare our approach with recent state-of-the-art methods . our results demonstrate superior texture transfer because of the ability to maintain the identity of the original face image .", "topics": ["loss function", "matrix regularization"]}
{"title": "simulating human grandmasters : evolution and coevolution of evaluation functions", "abstract": "this paper demonstrates the use of genetic algorithms for evolving a grandmaster-level evaluation function for a chess program . this is achieved by combining supervised and unsupervised learning . in the supervised learning phase the organisms are evolved to mimic the behavior of human grandmasters , and in the unsupervised learning phase these evolved organisms are further improved upon by means of coevolution . while past attempts succeeded in creating a grandmaster-level program by mimicking the behavior of existing computer chess programs , this paper presents the first successful attempt at evolving a state-of-the-art evaluation function by learning only from databases of games played by humans . our results demonstrate that the evolved program outperforms a two-time world computer chess champion .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "near-optimal entrywise sampling for data matrices", "abstract": "we consider the problem of selecting non-zero entries of a matrix $ a $ in order to produce a sparse sketch of it , $ b $ , that minimizes $ \\|a-b\\|_2 $ . for large $ m \\times n $ matrices , such that $ n \\gg m $ ( for example , representing $ n $ observations over $ m $ attributes ) we give sampling distributions that exhibit four important properties . first , they have closed forms computable from minimal information regarding $ a $ . second , they allow sketching of matrices whose non-zeros are presented to the algorithm in arbitrary order as a stream , with $ o ( 1 ) $ computation per non-zero . third , the resulting sketch matrices are not only sparse , but their non-zero entries are highly compressible . lastly , and most importantly , under mild assumptions , our distributions are provably competitive with the optimal offline distribution . note that the probabilities in the optimal offline distribution may be complex functions of all the entries in the matrix . therefore , regardless of computational complexity , the optimal distribution might be impossible to compute in the streaming model .", "topics": ["sampling ( signal processing )", "computational complexity theory"]}
{"title": "3d planar patch extraction from stereo using probabilistic region growing", "abstract": "this article presents a novel 3d planar patch extraction method using a probabilistic region growing algorithm . our method works by simultaneously initiating multiple planar patches from seed points , the latter determined by an intensity-based 2d segmentation algorithm in the stereo-pair images . the patches are grown incrementally and in parallel as 3d scene points are considered for membership , using a probabilistic distance likelihood measure . in addition , we have incorporated prior information based on the noise model in the 2d images and the scene configuration but also include the intensity information resulting from the initial segmentation . this method works well across many different data-sets , involving real and synthetic examples of both regularly and non-regularly sampled data , and is fast enough that may be used for robot navigation tasks of path detection and obstacle avoidance .", "topics": ["synthetic data"]}
{"title": "translation of `` zur ermittlung eines objektes aus zwei perspektiven mit innerer orientierung '' by erwin kruppa ( 1913 )", "abstract": "erwin kruppa 's 1913 paper , erwin kruppa , `` zur ermittlung eines objektes aus zwei perspektiven mit innerer orientierung '' , sitzungsberichte der mathematisch-naturwissenschaftlichen kaiserlichen akademie der wissenschaften , vol . 122 ( 1913 ) , pp . 1939-1948 , which may be translated as `` to determine a 3d object from two perspective views with known inner orientation '' , is a landmark paper in computer vision because it provides the first five-point algorithm for relative pose estimation . kruppa showed that ( a finite number of solutions for ) the relative pose between two calibrated images of a rigid object can be computed from five point matches between the images . kruppa 's work also gained attention in the topic of camera self-calibration , as presented in ( maybank and faugeras , 1992 ) . since the paper is still relevant today ( more than a hundred citations within the last ten years ) and the paper is not available online , we ordered a copy from the german national library in frankfurt and provide an english translation along with the german original . we also adapt the terminology to a modern jargon and provide some clarifications ( highlighted in sans-serif font ) . for a historical review of geometric computer vision , the reader is referred to the recent survey paper ( sturm , 2011 ) .", "topics": ["baseline ( configuration management )", "computer vision"]}
{"title": "beat-event detection in action movie franchises", "abstract": "while important advances were recently made towards temporally localizing and recognizing specific human actions or activities in videos , efficient detection and classification of long video chunks belonging to semantically defined categories such as `` pursuit '' or `` romance '' remains challenging.we introduce a new dataset , action movie franchises , consisting of a collection of hollywood action movie franchises . we define 11 non-exclusive semantic categories - called beat-categories - that are broad enough to cover most of the movie footage . the corresponding beat-events are annotated as groups of video shots , possibly overlapping.we propose an approach for localizing beat-events based on classifying shots into beat-categories and learning the temporal constraints between shots . we show that temporal constraints significantly improve the classification performance . we set up an evaluation protocol for beat-event localization as well as for shot classification , depending on whether movies from the same franchise are present or not in the training data .", "topics": ["test set"]}
{"title": "gaze embeddings for zero-shot image classification", "abstract": "zero-shot image classification using auxiliary information , such as attributes describing discriminative object properties , requires time-consuming annotation by domain experts . we instead propose a method that relies on human gaze as auxiliary information , exploiting that even non-expert users have a natural ability to judge class membership . we present a data collection paradigm that involves a discrimination task to increase the information content obtained from gaze data . our method extracts discriminative descriptors from the data and learns a compatibility function between image and gaze using three novel gaze embeddings : gaze histograms ( gh ) , gaze features with grid ( gfg ) and gaze features with sequence ( gfs ) . we introduce two new gaze-annotated datasets for fine-grained image classification and show that human gaze data is indeed class discriminative , provides a competitive alternative to expert-annotated attributes , and outperforms other baselines for zero-shot image classification .", "topics": ["baseline ( configuration management )", "computer vision"]}
{"title": "creation of an annotated corpus of spanish radiology reports", "abstract": "this paper presents a new annotated corpus of 513 anonymized radiology reports written in spanish . reports were manually annotated with entities , negation and uncertainty terms and relations . the corpus was conceived as an evaluation resource for named entity recognition and relation extraction algorithms , and as input for the use of supervised methods . biomedical annotated resources are scarce due to confidentiality issues and associated costs . this work provides some guidelines that could help other researchers to undertake similar tasks .", "topics": ["supervised learning", "entity"]}
{"title": "nonextensive information theoretical machine", "abstract": "in this paper , we propose a new discriminative model named \\emph { nonextensive information theoretical machine ( nitm ) } based on nonextensive generalization of shannon information theory . in nitm , weight parameters are treated as random variables . tsallis divergence is used to regularize the distribution of weight parameters and maximum unnormalized tsallis entropy distribution is used to evaluate fitting effect . on the one hand , it is showed that some well-known margin-based loss functions such as $ \\ell_ { 0/1 } $ loss , hinge loss , squared hinge loss and exponential loss can be unified by unnormalized tsallis entropy . on the other hand , gaussian prior regularization is generalized to student-t prior regularization with similar computational complexity . the model can be solved efficiently by gradient-based convex optimization and its performance is illustrated on standard datasets .", "topics": ["computational complexity theory", "loss function"]}
{"title": "a constraint satisfaction framework for executing perceptions and actions in diagrammatic reasoning", "abstract": "diagrammatic reasoning ( dr ) is pervasive in human problem solving as a powerful adjunct to symbolic reasoning based on language-like representations . the research reported in this paper is a contribution to building a general purpose dr system as an extension to a soar-like problem solving architecture . the work is in a framework in which dr is modeled as a process where subtasks are solved , as appropriate , either by inference from symbolic representations or by interaction with a diagram , i.e . , perceiving specified information from a diagram or modifying/creating objects in a diagram in specified ways according to problem solving needs . the perceptions and actions in most dr systems built so far are hand-coded for the specific application , even when the rest of the system is built using the general architecture . the absence of a general framework for executing perceptions/actions poses as a major hindrance to using them opportunistically -- the essence of open-ended search in problem solving . our goal is to develop a framework for executing a wide variety of specified perceptions and actions across tasks/domains without human intervention . we observe that the domain/task-specific visual perceptions/actions can be transformed into domain/task-independent spatial problems . we specify a spatial problem as a quantified constraint satisfaction problem in the real domain using an open-ended vocabulary of properties , relations and actions involving three kinds of diagrammatic objects -- points , curves , regions . solving a spatial problem from this specification requires computing the equivalent simplified quantifier-free expression , the complexity of which is inherently doubly exponential . we represent objects as configuration of simple elements to facilitate decomposition of complex problems into simpler and similar subproblems . we show that , if the symbolic solution to a subproblem can be expressed concisely , quantifiers can be eliminated from spatial problems in low-order polynomial time using similar previously solved subproblems . this requires determining the similarity of two problems , the existence of a mapping between them computable in polynomial time , and designing a memory for storing previously solved problems so as to facilitate search . the efficacy of the idea is shown by time complexity analysis . we demonstrate the proposed approach by executing perceptions and actions involved in dr tasks in two army applications .", "topics": ["time complexity", "polynomial"]}
{"title": "training feedforward neural networks with standard logistic activations is feasible", "abstract": "training feedforward neural networks with standard logistic activations is considered difficult because of the intrinsic properties of these sigmoidal functions . this work aims at showing that these networks can be trained to achieve generalization performance comparable to those based on hyperbolic tangent activations . the solution consists on applying a set of conditions in parameter initialization , which have been derived from the study of the properties of a single neuron from an information-theoretic perspective . the proposed initialization is validated through an extensive experimental analysis .", "topics": ["neural networks"]}
{"title": "relation extraction from clinical texts using domain invariant convolutional neural network", "abstract": "in recent years extracting relevant information from biomedical and clinical texts such as research articles , discharge summaries , or electronic health records have been a subject of many research efforts and shared challenges . relation extraction is the process of detecting and classifying the semantic relation among entities in a given piece of texts . existing models for this task in biomedical domain use either manually engineered features or kernel methods to create feature vector . these features are then fed to classifier for the prediction of the correct class . it turns out that the results of these methods are highly dependent on quality of user designed features and also suffer from curse of dimensionality . in this work we focus on extracting relations from clinical discharge summaries . our main objective is to exploit the power of convolution neural network ( cnn ) to learn features automatically and thus reduce the dependency on manual feature engineering . we evaluate performance of the proposed model on i2b2-2010 clinical relation extraction challenge dataset . our results indicate that convolution neural network can be a good model for relation exaction in clinical text without being dependent on expert 's knowledge on defining quality features .", "topics": ["feature vector", "entity"]}
{"title": "a new greedy algorithm for multiple sparse regression", "abstract": "this paper proposes a new algorithm for multiple sparse regression in high dimensions , where the task is to estimate the support and values of several ( typically related ) sparse vectors from a few noisy linear measurements . our algorithm is a `` forward-backward '' greedy procedure that -- uniquely -- operates on two distinct classes of objects . in particular , we organize our target sparse vectors as a matrix ; our algorithm involves iterative addition and removal of both ( a ) individual elements , and ( b ) entire rows ( corresponding to shared features ) , of the matrix . analytically , we establish that our algorithm manages to recover the supports ( exactly ) and values ( approximately ) of the sparse vectors , under assumptions similar to existing approaches based on convex optimization . however , our algorithm has a much smaller computational complexity . perhaps most interestingly , it is seen empirically to require visibly fewer samples . ours represents the first attempt to extend greedy algorithms to the class of models that can only/best be represented by a combination of component structural assumptions ( sparse and group-sparse , in our case ) .", "topics": ["computational complexity theory", "sparse matrix"]}
{"title": "collaborative representation based classification for face recognition", "abstract": "by coding a query sample as a sparse linear combination of all training samples and then classifying it by evaluating which class leads to the minimal coding residual , sparse representation based classification ( src ) leads to interesting results for robust face recognition . it is widely believed that the l1- norm sparsity constraint on coding coefficients plays a key role in the success of src , while its use of all training samples to collaboratively represent the query sample is rather ignored . in this paper we discuss how src works , and show that the collaborative representation mechanism used in src is much more crucial to its success of face classification . the src is a special case of collaborative representation based classification ( crc ) , which has various instantiations by applying different norms to the coding residual and coding coefficient . more specifically , the l1 or l2 norm characterization of coding residual is related to the robustness of crc to outlier facial pixels , while the l1 or l2 norm characterization of coding coefficient is related to the degree of discrimination of facial features . extensive experiments were conducted to verify the face recognition accuracy and efficiency of crc with different instantiations .", "topics": ["sparse matrix", "coefficient"]}
{"title": "reinforced decision trees", "abstract": "in order to speed-up classification models when facing a large number of categories , one usual approach consists in organizing the categories in a particular structure , this structure being then used as a way to speed-up the prediction computation . this is for example the case when using error-correcting codes or even hierarchies of categories . but in the majority of approaches , this structure is chosen \\textit { by hand } , or during a preliminary step , and not integrated in the learning process . we propose a new model called reinforced decision tree which simultaneously learns how to organize categories in a tree structure and how to classify any input based on this structure . this approach keeps the advantages of existing techniques ( low inference complexity ) but allows one to build efficient classifiers in one learning step . the learning algorithm is inspired by reinforcement learning and policy-gradient techniques which allows us to integrate the two steps ( building the tree , and learning the classifier ) in one single algorithm .", "topics": ["reinforcement learning", "computation"]}
{"title": "joint training of deep boltzmann machines", "abstract": "we introduce a new method for training deep boltzmann machines jointly . prior methods require an initial learning pass that trains the deep boltzmann machine greedily , one layer at a time , or do not perform well on classifi- cation tasks .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "smooth sparse coding via marginal regression for learning sparse representations", "abstract": "we propose and analyze a novel framework for learning sparse representations , based on two statistical techniques : kernel smoothing and marginal regression . the proposed approach provides a flexible framework for incorporating feature similarity or temporal information present in data sets , via non-parametric kernel smoothing . we provide generalization bounds for dictionary learning using smooth sparse coding and show how the sample complexity depends on the l1 norm of kernel function used . furthermore , we propose using marginal regression for obtaining sparse codes , which significantly improves the speed and allows one to scale to large dictionary sizes easily . we demonstrate the advantages of the proposed approach , both in terms of accuracy and speed by extensive experimentation on several real data sets . in addition , we demonstrate how the proposed approach could be used for improving semi-supervised sparse coding .", "topics": ["sparse matrix", "dictionary"]}
{"title": "homotopy analysis for tensor pca", "abstract": "developing efficient and guaranteed nonconvex algorithms has been an important challenge in modern machine learning . algorithms with good empirical performance such as stochastic gradient descent often lack theoretical guarantees . in this paper , we analyze the class of homotopy or continuation methods for global optimization of nonconvex functions . these methods start from an objective function that is efficient to optimize ( e.g . convex ) , and progressively modify it to obtain the required objective , and the solutions are passed along the homotopy path . for the challenging problem of tensor pca , we prove global convergence of the homotopy method in the `` high noise '' regime . the signal-to-noise requirement for our algorithm is tight in the sense that it matches the recovery guarantee for the best degree-4 sum-of-squares algorithm . in addition , we prove a phase transition along the homotopy path for tensor pca . this allows to simplify the homotopy method to a local search algorithm , viz . , tensor power iterations , with a specific initialization and a noise injection procedure , while retaining the theoretical guarantees .", "topics": ["optimization problem", "loss function"]}
{"title": "multiscale fields of patterns", "abstract": "we describe a framework for defining high-order image models that can be used in a variety of applications . the approach involves modeling local patterns in a multiscale representation of an image . local properties of a coarsened image reflect non-local properties of the original image . in the case of binary images local properties are defined by the binary patterns observed over small neighborhoods around each pixel . with the multiscale representation we capture the frequency of patterns observed at different scales of resolution . this framework leads to expressive priors that depend on a relatively small number of parameters . for inference and learning we use an mcmc method for block sampling with very large blocks . we evaluate the approach with two example applications . one involves contour detection . the other involves binary segmentation .", "topics": ["pixel"]}
{"title": "online learning of portfolio ensembles with sector exposure regularization", "abstract": "we consider online learning of ensembles of portfolio selection algorithms and aim to regularize risk by encouraging diversification with respect to a predefined risk-driven grouping of stocks . our procedure uses online convex optimization to control capital allocation to underlying investment algorithms while encouraging non-sparsity over the given grouping . we prove a logarithmic regret for this procedure with respect to the best-in-hindsight ensemble . we applied the procedure with known mean-reversion portfolio selection algorithms using the standard gics industry sector grouping . empirical experimental results showed an impressive percentage increase of risk-adjusted return ( sharpe ratio ) .", "topics": ["regret ( decision theory )", "matrix regularization"]}
{"title": "improving the efficiency of approximate inference for probabilistic logical models by means of program specialization", "abstract": "we consider the task of performing probabilistic inference with probabilistic logical models . many algorithms for approximate inference with such models are based on sampling . from a logic programming perspective , sampling boils down to repeatedly calling the same queries on a knowledge base composed of a static part and a dynamic part . the larger the static part , the more redundancy there is in these repeated calls . this is problematic since inefficient sampling yields poor approximations . we show how to apply logic program specialization to make sampling-based inference more efficient . we develop an algorithm that specializes the definitions of the query predicates with respect to the static part of the knowledge base . in experiments on real-world data we obtain speedups of up to an order of magnitude , and these speedups grow with the data-size .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "object detection with pixel intensity comparisons organized in decision trees", "abstract": "we describe a method for visual object detection based on an ensemble of optimized decision trees organized in a cascade of rejectors . the trees use pixel intensity comparisons in their internal nodes and this makes them able to process image regions very fast . experimental analysis is provided through a face detection problem . the obtained results are encouraging and demonstrate that the method has practical value . additionally , we analyse its sensitivity to noise and show how to perform fast rotation invariant object detection . complete source code is provided at https : //github.com/nenadmarkus/pico .", "topics": ["object detection", "pixel"]}
{"title": "learning equivalence classes of bayesian networks structures", "abstract": "approaches to learning bayesian networks from data typically combine a scoring function with a heuristic search procedure . given a bayesian network structure , many of the scoring functions derived in the literature return a score for the entire equivalence class to which the structure belongs . when using such a scoring function , it is appropriate for the heuristic search algorithm to search over equivalence classes of bayesian networks as opposed to individual structures . we present the general formulation of a search space for which the states of the search correspond to equivalence classes of structures . using this space , any one of a number of heuristic search algorithms can easily be applied . we compare greedy search performance in the proposed search space to greedy search performance in a search space for which the states correspond to individual bayesian network structures .", "topics": ["mathematical optimization", "bayesian network"]}
{"title": "problem evolution : a new approach to problem solving systems", "abstract": "in this paper we present a novel tool to evaluate problem solving systems . instead of using a system to solve a problem , we suggest using the problem to evaluate the system . by finding a numerical representation of a problem 's complexity , one can implement genetic algorithm to search for the most complex problem the given system can solve . this allows a comparison between different systems that solve the same set of problems . in this paper we implement this approach on pattern recognition neural networks to try and find the most complex pattern a given configuration can solve . the complexity of the pattern is calculated using linguistic complexity . the results demonstrate the power of the problem evolution approach in ranking different neural network configurations according to their pattern recognition abilities . future research and implementations of this technique are also discussed .", "topics": ["numerical analysis"]}
{"title": "maximal sparsity with deep networks ?", "abstract": "the iterations of many sparse estimation algorithms are comprised of a fixed linear filter cascaded with a thresholding nonlinearity , which collectively resemble a typical neural network layer . consequently , a lengthy sequence of algorithm iterations can be viewed as a deep network with shared , hand-crafted layer weights . it is therefore quite natural to examine the degree to which a learned network model might act as a viable surrogate for traditional sparse estimation in domains where ample training data is available . while the possibility of a reduced computational budget is readily apparent when a ceiling is imposed on the number of layers , our work primarily focuses on estimation accuracy . in particular , it is well-known that when a signal dictionary has coherent columns , as quantified by a large rip constant , then most tractable iterative algorithms are unable to find maximally sparse representations . in contrast , we demonstrate both theoretically and empirically the potential for a trained deep network to recover minimal $ \\ell_0 $ -norm representations in regimes where existing methods fail . the resulting system is deployed on a practical photometric stereo estimation problem , where the goal is to remove sparse outliers that can disrupt the estimation of surface normals from a 3d scene .", "topics": ["test set", "nonlinear system"]}
{"title": "learning multi-target tracking with quadratic object interactions", "abstract": "we describe a model for multi-target tracking based on associating collections of candidate detections across frames of a video . in order to model pairwise interactions between different tracks , such as suppression of overlapping tracks and contextual cues about co-occurence of different objects , we augment a standard min-cost flow objective with quadratic terms between detection variables . we learn the parameters of this model using structured prediction and a loss function which approximates the multi-target tracking accuracy . we evaluate two different approaches to finding an optimal set of tracks under model objective based on an lp relaxation and a novel greedy extension to dynamic programming that handles pairwise interactions . we find the greedy algorithm achieves equivalent performance to the lp relaxation while being 2-7x faster than a commercial solver . the resulting model with learned parameters outperforms existing methods across several categories on the kitti tracking benchmark .", "topics": ["loss function", "sensor"]}
{"title": "logic tensor networks : deep learning and logical reasoning from data and knowledge", "abstract": "we propose logic tensor networks : a uniform framework for integrating automatic learning and reasoning . a logic formalism called real logic is defined on a first-order language whereby formulas have truth-value in the interval [ 0,1 ] and semantics defined concretely on the domain of real numbers . logical constants are interpreted as feature vectors of real numbers . real logic promotes a well-founded integration of deductive reasoning on a knowledge-base and efficient data-driven relational machine learning . we show how real logic can be implemented in deep tensor neural networks with the use of google 's tensorflow primitives . the paper concludes with experiments applying logic tensor networks on a simple but representative example of knowledge completion .", "topics": ["feature vector"]}
{"title": "concrete sentence spaces for compositional distributional models of meaning", "abstract": "coecke , sadrzadeh , and clark ( arxiv:1003.4394v1 [ cs.cl ] ) developed a compositional model of meaning for distributional semantics , in which each word in a sentence has a meaning vector and the distributional meaning of the sentence is a function of the tensor products of the word vectors . abstractly speaking , this function is the morphism corresponding to the grammatical structure of the sentence in the category of finite dimensional vector spaces . in this paper , we provide a concrete method for implementing this linear meaning map , by constructing a corpus-based vector space for the type of sentence . our construction method is based on structured vector spaces whereby meaning vectors of all sentences , regardless of their grammatical structure , live in the same vector space . our proposed sentence space is the tensor product of two noun spaces , in which the basis vectors are pairs of words each augmented with a grammatical role . this enables us to compare meanings of sentences by simply taking the inner product of their vectors .", "topics": ["calculus of variations", "parsing"]}
{"title": "structural bias in population-based algorithms", "abstract": "challenging optimisation problems are abundant in all areas of science . since the 1950s , scientists have developed ever-diversifying families of black box optimisation algorithms designed to address any optimisation problem , requiring only that quality of a candidate solution is calculated via a fitness function specific to the problem . for such algorithms to be successful , at least three properties are required : an effective informed sampling strategy , that guides generation of new candidates on the basis of fitnesses and locations of previously visited candidates ; mechanisms to ensure efficiency , so that same candidates are not repeatedly visited ; absence of structural bias , which , if present , would predispose the algorithm towards limiting its search to some regions of solution space . the first two of these properties have been extensively investigated , however the third is little understood . in this article we provide theoretical and empirical analyses that contribute to the understanding of structural bias . we prove a theorem concerning dynamics of population variance in the case of real-valued search spaces . this reveals how structural bias can manifest as non-uniform clustering of population over time . theory predicts that structural bias is exacerbated with increasing population size and problem difficulty . these predictions reveal two previously unrecognised aspects of structural bias . respectively , increasing population size , though ostensibly promoting diversity , will magnify any inherent structural bias , and effects of structural bias are more apparent when faced with difficult problems . our theoretical result also suggests that two commonly used approaches to enhancing exploration , increasing population size and increasing disruptiveness of search operators , have quite distinct implications in terms of structural bias .", "topics": ["sampling ( signal processing )", "cluster analysis"]}
{"title": "retinet : automatic amd identification in oct volumetric data", "abstract": "optical coherence tomography ( oct ) provides a unique ability to image the eye retina in 3d at micrometer resolution and gives ophthalmologist the ability to visualize retinal diseases such as age-related macular degeneration ( amd ) . while visual inspection of oct volumes remains the main method for amd identification , doing so is time consuming as each cross-section within the volume must be inspected individually by the clinician . in much the same way , acquiring ground truth information for each cross-section is expensive and time consuming . this fact heavily limits the ability to acquire large amounts of ground truth , which subsequently impacts the performance of learning-based methods geared at automatic pathology identification . to avoid this burden , we propose a novel strategy for automatic analysis of oct volumes where only volume labels are needed . that is , we train a classifier in a semi-supervised manner to conduct this task . our approach uses a novel convolutional neural network ( cnn ) architecture , that only needs volume-level labels to be trained to automatically asses whether an oct volume is healthy or contains amd . our architecture involves first learning a cross-section pathology classifier using pseudo-labels that could be corrupted and then leverage these towards a more accurate volume-level classification . we then show that our approach provides excellent performances on a publicly available dataset and outperforms a number of existing automatic techniques .", "topics": ["statistical classification", "ground truth"]}
{"title": "on the quality of the initial basin in overspecified neural networks", "abstract": "deep learning , in the form of artificial neural networks , has achieved remarkable practical success in recent years , for a variety of difficult machine learning applications . however , a theoretical explanation for this remains a major open problem , since training neural networks involves optimizing a highly non-convex objective function , and is known to be computationally hard in the worst case . in this work , we study the \\emph { geometric } structure of the associated non-convex objective function , in the context of relu networks and starting from a random initialization of the network parameters . we identify some conditions under which it becomes more favorable to optimization , in the sense of ( i ) high probability of initializing at a point from which there is a monotonically decreasing path to a global minimum ; and ( ii ) high probability of initializing at a basin ( suitably defined ) with a small minimal objective value . a common theme in our results is that such properties are more likely to hold for larger ( `` overspecified '' ) networks , which accords with some recent empirical and theoretical observations .", "topics": ["optimization problem", "neural networks"]}
{"title": "learning discriminative multilevel structured dictionaries for supervised image classification", "abstract": "sparse representations using overcomplete dictionaries have proved to be a powerful tool in many signal processing applications such as denoising , super-resolution , inpainting , compression or classification . the sparsity of the representation very much depends on how well the dictionary is adapted to the data at hand . in this paper , we propose a method for learning structured multilevel dictionaries with discriminative constraints to make them well suited for the supervised pixelwise classification of images . a multilevel tree-structured discriminative dictionary is learnt for each class , with a learning objective concerning the reconstruction errors of the image patches around the pixels over each class-representative dictionary . after the initial assignment of the class labels to image pixels based on their sparse representations over the learnt dictionaries , the final classification is achieved by smoothing the label image with a graph cut method and an erosion method . applied to a common set of texture images , our supervised classification method shows competitive results with the state of the art .", "topics": ["supervised learning", "noise reduction"]}
{"title": "learning cooperative visual dialog agents with deep reinforcement learning", "abstract": "we introduce the first goal-driven training for visual question answering and dialog agents . specifically , we pose a cooperative 'image guessing ' game between two agents -- qbot and abot -- who communicate in natural language dialog so that qbot can select an unseen image from a lineup of images . we use deep reinforcement learning ( rl ) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward . we demonstrate two experimental results . first , as a 'sanity check ' demonstration of pure rl ( from scratch ) , we show results on a synthetic world , where the agents communicate in ungrounded vocabulary , i.e . , symbols with no pre-specified meanings ( x , y , z ) . we find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes ( shape/color/style ) . thus , we demonstrate the emergence of grounded language and communication among 'visual ' dialog agents with no human supervision . second , we conduct large-scale real-image experiments on the visdial dataset , where we pretrain with supervised dialog data and show that the rl 'fine-tuned ' agents significantly outperform sl agents . interestingly , the rl qbot learns to ask questions that abot is good at , ultimately resulting in more informative dialog and a better team .", "topics": ["supervised learning", "reinforcement learning"]}
{"title": "the impact of random models on clustering similarity", "abstract": "clustering is a central approach for unsupervised learning . after clustering is applied , the most fundamental analysis is to quantitatively compare clusterings . such comparisons are crucial for the evaluation of clustering methods as well as other tasks such as consensus clustering . it is often argued that , in order to establish a baseline , clustering similarity should be assessed in the context of a random ensemble of clusterings . the prevailing assumption for the random clustering ensemble is the permutation model in which the number and sizes of clusters are fixed . however , this assumption does not necessarily hold in practice ; for example , multiple runs of k-means clustering returns clusterings with a fixed number of clusters , while the cluster size distribution varies greatly . here , we derive corrected variants of two clustering similarity measures ( the rand index and mutual information ) in the context of two random clustering ensembles in which the number and sizes of clusters vary . in addition , we study the impact of one-sided comparisons in the scenario with a reference clustering . the consequences of different random models are illustrated using synthetic examples , handwriting recognition , and gene expression data . we demonstrate that the choice of random model can have a drastic impact on the ranking of similar clustering pairs , and the evaluation of a clustering method with respect to a random baseline ; thus , the choice of random clustering model should be carefully justified .", "topics": ["baseline ( configuration management )", "cluster analysis"]}
{"title": "learning to recognize touch gestures : recurrent vs. convolutional features and dynamic sampling", "abstract": "we propose a fully automatic method for learning gestures on big touch devices in a potentially multi-user context . the goal is to learn general models capable of adapting to different gestures , user styles and hardware variations ( e.g . device sizes , sampling frequencies and regularities ) . based on deep neural networks , our method features a novel dynamic sampling and temporal normalization component , transforming variable length gestures into fixed length representations while preserving finger/surface contact transitions , that is , the topology of the signal . this sequential representation is then processed with a convolutional model capable , unlike recurrent networks , of learning hierarchical representations with different levels of abstraction . to demonstrate the interest of the proposed method , we introduce a new touch gestures dataset with 6591 gestures performed by 27 people , which is , up to our knowledge , the first of its kind : a publicly available multi-touch gesture dataset for interaction . we also tested our method on a standard dataset of symbolic touch gesture recognition , the mmg dataset , outperforming the state of the art and reporting close to perfect performance .", "topics": ["sampling ( signal processing )", "feature learning"]}
{"title": "secure surf with fully homomorphic encryption", "abstract": "cloud computing is an important part of today 's world because offloading computations is a method to reduce costs . in this paper , we investigate computing the speeded up robust features ( surf ) using fully homomorphic encryption ( fhe ) . performing surf in fhe enables a method to offload the computations while maintaining security and privacy of the original data . in support of this research , we developed a framework to compute surf via a rational number based compatible with fhe . although floating point ( r ) to rational numbers ( q ) conversion introduces error , our research provides tight bounds on the magnitude of error in terms of parameters of fhe . we empirically verified the proposed method against a set of images at different sizes and showed that our framework accurately computes most of the surf keypoints in fhe .", "topics": ["computation"]}
{"title": "arbitrary facial attribute editing : only change what you want", "abstract": "facial attribute editing aims to modify either single or multiple attributes on a face image . since it is practically infeasible to collect images with arbitrarily specified attributes for each person , the generative adversarial net ( gan ) and the encoder-decoder architecture are usually incorporated to handle this task . with the encoder-decoder architecture , arbitrary attribute editing can then be conducted by decoding the latent representation of the face image conditioned on the specified attributes . a few existing methods attempt to establish attribute-independent latent representation for arbitrarily changing the attributes . however , since the attributes portray the characteristics of the face image , the attribute-independent constraint on the latent representation is excessive . such constraint may result in information loss and unexpected distortion on the generated images ( e.g . over-smoothing ) , especially for those identifiable attributes such as gender , race etc . instead of imposing the attribute-independent constraint on the latent representation , we introduce an attribute classification constraint on the generated image , just requiring the correct change of the attributes . meanwhile , reconstruction learning is introduced in order to guarantee the preservation of all other attribute-excluding details on the generated image , and adversarial learning is employed for visually realistic generation . moreover , our method can be naturally extended to attribute intensity manipulation . experiments on the celeba dataset show that our method outperforms the state-of-the-arts on generating realistic attribute editing results with facial details well preserved .", "topics": ["encoder"]}
{"title": "text summarization techniques : a brief survey", "abstract": "in recent years , there has been a explosion in the amount of text data from a variety of sources . this volume of text is an invaluable source of information and knowledge which needs to be effectively summarized to be useful . in this review , the main approaches to automatic text summarization are described . we review the different processes for summarization and describe the effectiveness and shortcomings of the different methods .", "topics": ["text corpus"]}
{"title": "image denoising with multi-layer perceptrons , part 2 : training trade-offs and analysis of their mechanisms", "abstract": "image denoising can be described as the problem of mapping from a noisy image to a noise-free image . in another paper , we show that multi-layer perceptrons can achieve outstanding image denoising performance for various types of noise ( additive white gaussian noise , mixed poisson-gaussian noise , jpeg artifacts , salt-and-pepper noise and noise resembling stripes ) . in this work we discuss in detail which trade-offs have to be considered during the training procedure . we will show how to achieve good results and which pitfalls to avoid . by analysing the activation patterns of the hidden units we are able to make observations regarding the functioning principle of multi-layer perceptrons trained for image denoising .", "topics": ["noise reduction"]}
{"title": "a comparison of public causal search packages on linear , gaussian data with no latent variables", "abstract": "we compare tetrad ( java ) algorithms to the other public software packages bnt ( bayes net toolbox , matlab ) , pcalg ( r ) , bnlearn ( r ) on the \\vanilla '' task of recovering dag structure to the extent possible from data generated recursively from linear , gaussian structure equation models ( sems ) with no latent variables , for random graphs , with no additional knowledge of variable order or adjacency structure , and without additional specification of intervention information . each one of the above packages offers at least one implementation suitable to this purpose . we compare them on adjacency and orientation accuracy as well as time performance , for fixed datasets . we vary the number of variables , the number of samples , and the density of graph , for a total of 27 combinations , averaging all statistics over 10 runs , for a total of 270 datasets . all runs are carried out on the same machine and on their native platforms . an interactive visualization tool is provided for the reader who wishes to know more than can be documented explicitly in this report .", "topics": ["bayesian network", "causality"]}
{"title": "convolutional network for attribute-driven and identity-preserving human face generation", "abstract": "this paper focuses on the problem of generating human face pictures from specific attributes . the existing cnn-based face generation models , however , either ignore the identity of the generated face or fail to preserve the identity of the reference face image . here we address this problem from the view of optimization , and suggest an optimization model to generate human face with the given attributes while keeping the identity of the reference image . the attributes can be obtained from the attribute-guided image or by tuning the attribute features of the reference image . with the deep convolutional network `` vgg-face '' , the loss is defined on the convolutional feature maps . we then apply the gradient decent algorithm to solve this optimization problem . the results validate the effectiveness of our method for attribute driven and identity-preserving face generation .", "topics": ["optimization problem", "map"]}
{"title": "learning recurrent dynamics in spiking networks", "abstract": "spiking activity of neurons engaged in learning and performing a task show complex spatiotemporal dynamics . while the output of recurrent network models can learn to perform various tasks , the possible range of recurrent dynamics that emerge after learning remains unknown . here we show that modifying the recurrent connectivity with a recursive least squares algorithm provides sufficient flexibility for synaptic and spiking rate dynamics of spiking networks to produce a wide range of spatiotemporal activity . we apply the training method to learn arbitrary firing patterns , stabilize irregular spiking activity of a balanced network , and reproduce the heterogeneous spiking rate patterns of cortical neurons engaged in motor planning and movement . we identify sufficient conditions for successful learning , characterize two types of learning errors , and assess the network capacity . our findings show that synaptically-coupled recurrent spiking networks possess a vast computational capability that can support the diverse activity patterns in the brain .", "topics": ["recurrent neural network"]}
{"title": "zero-shot visual question answering", "abstract": "part of the appeal of visual question answering ( vqa ) is its promise to answer new questions about previously unseen images . most current methods demand training questions that illustrate every possible concept , and will therefore never achieve this capability , since the volume of required training data would be prohibitive . answering general questions about images requires methods capable of zero-shot vqa , that is , methods able to answer questions beyond the scope of the training questions . we propose a new evaluation protocol for vqa methods which measures their ability to perform zero-shot vqa , and in doing so highlights significant practical deficiencies of current approaches , some of which are masked by the biases in current datasets . we propose and evaluate several strategies for achieving zero-shot vqa , including methods based on pretrained word embeddings , object classifiers with semantic embeddings , and test-time retrieval of example images . our extensive experiments are intended to serve as baselines for zero-shot vqa , and they also achieve state-of-the-art performance in the standard vqa evaluation setting .", "topics": ["test set"]}
{"title": "collaborative personalized web recommender system using entropy based similarity measure", "abstract": "on the internet , web surfers , in the search of information , always strive for recommendations . the solutions for generating recommendations become more difficult because of exponential increase in information domain day by day . in this paper , we have calculated entropy based similarity between users to achieve solution for scalability problem . using this concept , we have implemented an online user based collaborative web recommender system . in this model based collaborative system , the user session is divided into two levels . entropy is calculated at both the levels . it is shown that from the set of valuable recommenders obtained at level i ; only those recommenders having lower entropy at level ii than entropy at level i , served as trustworthy recommenders . finally , top n recommendations are generated from such trustworthy recommenders for an online user .", "topics": ["time complexity", "scalability"]}
{"title": "actionness estimation using hybrid fully convolutional networks", "abstract": "actionness was introduced to quantify the likelihood of containing a generic action instance at a specific location . accurate and efficient estimation of actionness is important in video analysis and may benefit other relevant tasks such as action recognition and action detection . this paper presents a new deep architecture for actionness estimation , called hybrid fully convolutional network ( h-fcn ) , which is composed of appearance fcn ( a-fcn ) and motion fcn ( m-fcn ) . these two fcns leverage the strong capacity of deep models to estimate actionness maps from the perspectives of static appearance and dynamic motion , respectively . in addition , the fully convolutional nature of h-fcn allows it to efficiently process videos with arbitrary sizes . experiments are conducted on the challenging datasets of stanford40 , ucf sports , and jhmdb to verify the effectiveness of h-fcn on actionness estimation , which demonstrate that our method achieves superior performance to previous ones . moreover , we apply the estimated actionness maps on action proposal generation and action detection . our actionness maps advance the current state-of-the-art performance of these tasks substantially .", "topics": ["map"]}
{"title": "automated prediction of temporal relations", "abstract": "background : there has been growing research interest in automated answering of questions or generation of summary of free form text such as news article . in order to implement this task , the computer should be able to identify the sequence of events , duration of events , time at which event occurred and the relationship type between event pairs , time pairs or event-time pairs . specific problem : it is important to accurately identify the relationship type between combinations of event and time before the temporal ordering of events can be defined . the machine learning approach taken in mani et . al ( 2006 ) provides an accuracy of only 62.5 on the baseline data from timebank . the researchers used maximum entropy classifier in their methodology . timeml uses the tlink annotation to tag a relationship type between events and time . the time complexity is quadratic when it comes to tagging documents with tlink using human annotation . this research proposes using decision tree and parsing to improve the relationship type tagging . this research attempts to solve the gaps in human annotation by automating the task of relationship type tagging in an attempt to improve the accuracy of event and time relationship in annotated documents . scope information : the documents from the domain of news will be used . the tagging will be performed within the same document and not across documents . the relationship types will be identified only for a pair of event and time and not a chain of events . the research focuses on documents tagged using the timeml specification which contains tags such as event , tlink , and timex . each tag has attributes such as identifier , relation , pos , time etc .", "topics": ["baseline ( configuration management )", "time complexity"]}
{"title": "numeric input relations for relational learning with applications to community structure analysis", "abstract": "most work in the area of statistical relational learning ( srl ) is focussed on discrete data , even though a few approaches for hybrid srl models have been proposed that combine numerical and discrete variables . in this paper we distinguish numerical random variables for which a probability distribution is defined by the model from numerical input variables that are only used for conditioning the distribution of discrete response variables . we show how numerical input relations can very easily be used in the relational bayesian network framework , and that existing inference and learning methods need only minor adjustments to be applied in this generalized setting . the resulting framework provides natural relational extensions of classical probabilistic models for categorical data . we demonstrate the usefulness of rbn models with numeric input relations by several examples . in particular , we use the augmented rbn framework to define probabilistic models for multi-relational ( social ) networks in which the probability of a link between two nodes depends on numeric latent feature vectors associated with the nodes . a generic learning procedure can be used to obtain a maximum-likelihood fit of model parameters and latent feature values for a variety of models that can be expressed in the high-level rbn representation . specifically , we propose a model that allows us to interpret learned latent feature values as community centrality degrees by which we can identify nodes that are central for one community , that are hubs between communities , or that are isolated nodes . in a multi-relational setting , the model also provides a characterization of how different relations are associated with each community .", "topics": ["value ( ethics )", "high- and low-level"]}
{"title": "componentwise least squares support vector machines", "abstract": "this chapter describes componentwise least squares support vector machines ( ls-svms ) for the estimation of additive models consisting of a sum of nonlinear components . the primal-dual derivations characterizing ls-svms for the estimation of the additive model result in a single set of linear equations with size growing in the number of data-points . the derivation is elaborated for the classification as well as the regression case . furthermore , different techniques are proposed to discover structure in the data by looking for sparse components in the model based on dedicated regularization schemes on the one hand and fusion of the componentwise ls-svms training with a validation criterion on the other hand . ( keywords : ls-svms , additive models , regularization , structure detection )", "topics": ["support vector machine", "nonlinear system"]}
{"title": "extended breadth-first search algorithm", "abstract": "the task of artificial intelligence is to provide representation techniques for describing problems , as well as search algorithms that can be used to answer our questions . a widespread and elaborated model is state-space representation , which , however , has some shortcomings . classical search algorithms are not applicable in practice when the state space contains even only a few tens of thousands of states . we can give remedy to this problem by defining some kind of heuristic knowledge . in case of classical state-space representation , heuristic must be defined so that it qualifies an arbitrary state based on its `` goodness , '' which is obviously not trivial . in our paper , we introduce an algorithm that gives us the ability to handle huge state spaces and to use a heuristic concept which is easier to embed into search algorithms .", "topics": ["heuristic", "artificial intelligence"]}
{"title": "2d face recognition system based on selected gabor filters and linear discriminant analysis lda", "abstract": "we present a new approach for face recognition system . the method is based on 2d face image features using subset of non-correlated and orthogonal gabor filters instead of using the whole gabor filter bank , then compressing the output feature vector using linear discriminant analysis ( lda ) . the face image has been enhanced using multi stage image processing technique to normalize it and compensate for illumination variation . experimental results show that the proposed system is effective for both dimension reduction and good recognition performance when compared to the complete gabor filter bank . the system has been tested using casia , orl and cropped yaleb 2d face images databases and achieved average recognition rate of 98.9 % .", "topics": ["image processing", "feature vector"]}
{"title": "heuristics for vehicle routing problems : sequence or set optimization ?", "abstract": "we investigate a structural decomposition for the capacitated vehicle routing problem ( cvrp ) based on vehicle-to-customer `` assignment '' and visits `` sequencing '' decision variables . we show that an heuristic search focused on assignment decisions with a systematic optimal choice of sequences ( using concorde tsp solver ) during each move evaluation is promising but requires a prohibitive computational effort . we therefore introduce an intermediate search space , based on the dynamic programming procedure of balas & simonetti , which finds a good compromise between intensification and computational efficiency . a variety of speed-up techniques are proposed for a fast exploration : neighborhood reductions , dynamic move filters , memory structures , and concatenation techniques . finally , a tunneling strategy is designed to reshape the search space as the algorithm progresses . the combination of these techniques within a classical local search , as well as in the unified hybrid genetic search ( uhgs ) leads to significant improvements of solution accuracy . new best solutions are found for surprisingly small instances with as few as 256 customers . these solutions had not been attained up to now with classic neighborhoods . overall , this research permits to better evaluate the respective impact of sequence and assignment optimization , proposes new ways of combining the optimization of these two decision sets , and opens promising research perspectives for the cvrp and its variants .", "topics": ["mathematical optimization", "heuristic"]}
{"title": "deep convolutional acoustic word embeddings using word-pair side information", "abstract": "recent studies have been revisiting whole words as the basic modelling unit in speech recognition and query applications , instead of phonetic units . such whole-word segmental systems rely on a function that maps a variable-length speech segment to a vector in a fixed-dimensional space ; the resulting acoustic word embeddings need to allow for accurate discrimination between different word types , directly in the embedding space . we compare several old and new approaches in a word discrimination task . our best approach uses side information in the form of known word pairs to train a siamese convolutional neural network ( cnn ) : a pair of tied networks that take two speech segments as input and produce their embeddings , trained with a hinge loss that separates same-word pairs and different-word pairs by some margin . a word classifier cnn performs similarly , but requires much stronger supervision . both types of cnns yield large improvements over the best previously published results on the word discrimination task .", "topics": ["speech recognition", "map"]}
{"title": "a differential semantics of lazy ar propagation", "abstract": "in this paper we present a differential semantics of lazy ar propagation ( larp ) in discrete bayesian networks . we describe how both single and multi dimensional partial derivatives of the evidence may easily be calculated from a junction tree in larp equilibrium . we show that the simplicity of the calculations stems from the nature of larp . based on the differential semantics we describe how variable propagation in the larp architecture may give access to additional partial derivatives . the cautious larp ( clarp ) scheme is derived to produce a flexible clarp equilibrium that offers additional opportunities for calculating single and multidimensional partial derivatives of the evidence and subsets of the evidence from a single propagation . the results of an empirical evaluation illustrates how the access to a largely increased number of partial derivatives comes at a low computational cost .", "topics": ["bayesian network"]}
{"title": "methods of hierarchical clustering", "abstract": "we survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in r and other software environments . we look at hierarchical self-organizing maps , and mixture models . we review grid-based clustering , focusing on hierarchical density-based approaches . finally we describe a recently developed very efficient ( linear time ) hierarchical clustering algorithm , which can also be viewed as a hierarchical grid-based algorithm .", "topics": ["cluster analysis", "time complexity"]}
{"title": "obtaining depth maps from color images by region based stereo matching algorithms", "abstract": "in the paper , region based stereo matching algorithms are developed for extraction depth information from two color stereo image pair . a filter eliminating unreliable disparity estimation was used for increasing reliability of the disparity map . obtained results by algorithms were represented and compared .", "topics": ["map", "sensor"]}
{"title": "layer-wise training of deep networks using kernel similarity", "abstract": "deep learning has shown promising results in many machine learning applications . the hierarchical feature representation built by deep networks enable compact and precise encoding of the data . a kernel analysis of the trained deep networks demonstrated that with deeper layers , more simple and more accurate data representations are obtained . in this paper , we propose an approach for layer-wise training of a deep network for the supervised classification task . a transformation matrix of each layer is obtained by solving an optimization aimed at a better representation where a subsequent layer builds its representation on the top of the features produced by a previous layer . we compared the performance of our approach with a dnn trained using back-propagation which has same architecture as ours . experimental results on the real image datasets demonstrate efficacy of our approach . we also performed kernel analysis of layer representations to validate the claim of better feature encoding .", "topics": ["kernel ( operating system )", "supervised learning"]}
{"title": "breaking the softmax bottleneck : a high-rank rnn language model", "abstract": "we formulate language modeling as a matrix factorization problem , and show that the expressiveness of softmax-based models ( including the majority of neural language models ) is limited by a softmax bottleneck . given that natural language is highly context-dependent , this further implies that in practice softmax with distributed word embeddings does not have enough capacity to model natural language . we propose a simple and effective method to address this issue , and improve the state-of-the-art perplexities on penn treebank and wikitext-2 to 47.69 and 40.68 respectively . the proposed method also excels on the large-scale 1b word dataset , outperforming the baseline by over 5.6 points in perplexity .", "topics": ["baseline ( configuration management )", "natural language"]}
{"title": "learning a cnn-based end-to-end controller for a formula sae racecar", "abstract": "we present a set of cnn-based end-to-end models for controls of a formula sae racecar , along with various benchmarking and visualization tools to understand model performance . we tackled three main problems in the context of cone-delineated racetrack driving : ( 1 ) discretized steering , which translates a first-person frame along to the track to a predicted steering direction . ( 2 ) real-value steering , which translates a frame view to a real-value steering angle , and ( 3 ) a network design for predicting brake and throttle . we demonstrate high accuracy on our discretization task , low theoretical testing errors with our model for real-value steering , and a starting point for future work regarding a controller for our vehicle 's brake and throttle . timing benchmarks suggests that the networks we propose have the latency and throughput required for real-time controllers , when run on gpu-enabled hardware .", "topics": ["image segmentation", "map"]}
{"title": "block-coordinate frank-wolfe optimization for structural svms", "abstract": "we propose a randomized block-coordinate variant of the classic frank-wolfe algorithm for convex optimization with block-separable constraints . despite its lower iteration cost , we show that it achieves a similar convergence rate in duality gap as the full frank-wolfe algorithm . we also show that , when applied to the dual structural support vector machine ( svm ) objective , this yields an online algorithm that has the same low iteration complexity as primal stochastic subgradient methods . however , unlike stochastic subgradient methods , the block-coordinate frank-wolfe algorithm allows us to compute the optimal step-size and yields a computable duality gap guarantee . our experiments indicate that this simple algorithm outperforms competing structural svm solvers .", "topics": ["support vector machine", "iteration"]}
{"title": "scale up event extraction learning via automatic training data generation", "abstract": "the task of event extraction has long been investigated in a supervised learning paradigm , which is bound by the number and the quality of the training instances . existing training data must be manually generated through a combination of expert domain knowledge and extensive human involvement . however , due to drastic efforts required in annotating text , the resultant datasets are usually small , which severally affects the quality of the learned model , making it hard to generalize . our work develops an automatic approach for generating training data for event extraction . our approach allows us to scale up event extraction training instances from thousands to hundreds of thousands , and it does this at a much lower cost than a manual approach . we achieve this by employing distant supervision to automatically create event annotations from unlabelled text using existing structured knowledge bases or tables.we then develop a neural network model with post inference to transfer the knowledge extracted from structured knowledge bases to automatically annotate typed events with corresponding arguments in text.we evaluate our approach by using the knowledge extracted from freebase to label texts from wikipedia articles . experimental results show that our approach can generate a large number of high quality training instances . we show that this large volume of training data not only leads to a better event extractor , but also allows us to detect multiple typed events .", "topics": ["test set", "supervised learning"]}
{"title": "deep spatio-temporal manifold network for action recognition", "abstract": "visual data such as videos are often sampled from complex manifold . we propose leveraging the manifold structure to constrain the deep action feature learning , thereby minimizing the intra-class variations in the feature space and alleviating the over-fitting problem . considering that manifold can be transferred , layer by layer , from the data domain to the deep features , the manifold priori is posed from the top layer into the back propagation learning procedure of convolutional neural network ( cnn ) . the resulting algorithm -- spatio-temporal manifold network -- is solved with the efficient alternating direction method of multipliers and backward propagation ( admm-bp ) . we theoretically show that stmn recasts the problem as projection over the manifold via an embedding method . the proposed approach is evaluated on two benchmark datasets , showing significant improvements to the baselines .", "topics": ["baseline ( configuration management )", "feature learning"]}
{"title": "convexifying the bethe free energy", "abstract": "the introduction of loopy belief propagation ( lbp ) revitalized the application of graphical models in many domains . many recent works present improvements on the basic lbp algorithm in an attempt to overcome convergence and local optima problems . notable among these are convexified free energy approximations that lead to inference procedures with provable convergence and quality properties . however , empirically lbp still outperforms most of its convex variants in a variety of settings , as we also demonstrate here . motivated by this fact we seek convexified free energies that directly approximate the bethe free energy . we show that the proposed approximations compare favorably with state-of-the art convex free energy approximations .", "topics": ["approximation algorithm", "graphical model"]}
{"title": "data-dependent path normalization in neural networks", "abstract": "we propose a unified framework for neural net normalization , regularization and optimization , which includes path-sgd and batch-normalization and interpolates between them across two different dimensions . through this framework we investigate issue of invariance of the optimization , data dependence and the connection with natural gradients .", "topics": ["matrix regularization", "neural networks"]}
{"title": "zeroth-order asynchronous doubly stochastic algorithm with variance reduction", "abstract": "zeroth-order ( derivative-free ) optimization attracts a lot of attention in machine learning , because explicit gradient calculations may be computationally expensive or infeasible . to handle large scale problems both in volume and dimension , recently asynchronous doubly stochastic zeroth-order algorithms were proposed . the convergence rate of existing asynchronous doubly stochastic zeroth order algorithms is $ o ( \\frac { 1 } { \\sqrt { t } } ) $ ( also for the sequential stochastic zeroth-order optimization algorithms ) . in this paper , we focus on the finite sums of smooth but not necessarily convex functions , and propose an asynchronous doubly stochastic zeroth-order optimization algorithm using the accelerated technology of variance reduction ( asydszovr ) . rigorous theoretical analysis show that the convergence rate can be improved from $ o ( \\frac { 1 } { \\sqrt { t } } ) $ the best result of existing algorithms to $ o ( \\frac { 1 } { t } ) $ . also our theoretical results is an improvement to the ones of the sequential stochastic zeroth-order optimization algorithms .", "topics": ["gradient"]}
{"title": "consistency analysis of an empirical minimum error entropy algorithm", "abstract": "in this paper we study the consistency of an empirical minimum error entropy ( mee ) algorithm in a regression setting . we introduce two types of consistency . the error entropy consistency , which requires the error entropy of the learned function to approximate the minimum error entropy , is shown to be always true if the bandwidth parameter tends to 0 at an appropriate rate . the regression consistency , which requires the learned function to approximate the regression function , however , is a complicated issue . we prove that the error entropy consistency implies the regression consistency for homoskedastic models where the noise is independent of the input variable . but for heteroskedastic models , a counterexample is used to show that the two types of consistency do not coincide . a surprising result is that the regression consistency is always true , provided that the bandwidth parameter tends to infinity at an appropriate rate . regression consistency of two classes of special models is shown to hold with fixed bandwidth parameter , which further illustrates the complexity of regression consistency of mee . fourier transform plays crucial roles in our analysis .", "topics": ["approximation algorithm"]}
{"title": "see : towards semi-supervised end-to-end scene text recognition", "abstract": "detecting and recognizing text in natural scene images is a challenging , yet not completely solved task . in recent years several new systems that try to solve at least one of the two sub-tasks ( text detection and text recognition ) have been proposed . in this paper we present see , a step towards semi-supervised neural networks for scene text detection and recognition , that can be optimized end-to-end . most existing works consist of multiple deep neural networks and several pre-processing steps . in contrast to this , we propose to use a single deep neural network , that learns to detect and recognize text from natural images , in a semi-supervised way . see is a network that integrates and jointly learns a spatial transformer network , which can learn to detect text regions in an image , and a text recognition network that takes the identified text regions and recognizes their textual content . we introduce the idea behind our novel approach and show its feasibility , by performing a range of experiments on standard benchmark datasets , where we achieve competitive results .", "topics": ["end-to-end principle"]}
{"title": "lidioms : a multilingual linked idioms data set", "abstract": "in this paper , we describe the lidioms data set , a multilingual rdf representation of idioms currently containing five languages : english , german , italian , portuguese , and russian . the data set is intended to support natural language processing applications by providing links between idioms across languages . the underlying data was crawled and integrated from various sources . to ensure the quality of the crawled data , all idioms were evaluated by at least two native speakers . herein , we present the model devised for structuring the data . we also provide the details of linking lidioms to well-known multilingual data sets such as babelnet . the resulting data set complies with best practices according to linguistic linked open data community .", "topics": ["natural language processing"]}
{"title": "diversity is all you need : learning skills without a reward function", "abstract": "intelligent creatures can explore their environments and learn useful skills without supervision . in this paper , we propose diayn ( `` diversity is all you need '' ) , a method for learning useful skills without a reward function . our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy . on a variety of simulated robotic tasks , we show that this simple objective results in the unsupervised emergence of diverse skills , such as walking and jumping . in a number of reinforcement learning benchmark environments , our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward . in these environments , some of the learned skills correspond to solving the task , and each skill that solves the task does so in a distinct manner . our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning", "topics": ["unsupervised learning", "reinforcement learning"]}
{"title": "the rise and fall of the church-turing thesis", "abstract": "the essay consists of three parts . in the first part , it is explained how theory of algorithms and computations evaluates the contemporary situation with computers and global networks . in the second part , it is demonstrated what new perspectives this theory opens through its new direction that is called theory of super-recursive algorithms . these algorithms have much higher computing power than conventional algorithmic schemes . in the third part , we explicate how realization of what this theory suggests might influence life of people in future . it is demonstrated that now the theory is far ahead computing practice and practice has to catch up with the theory . we conclude with a comparison of different approaches to the development of information technology .", "topics": ["computation"]}
{"title": "dyadic prediction using a latent feature log-linear model", "abstract": "in dyadic prediction , labels must be predicted for pairs ( dyads ) whose members possess unique identifiers and , sometimes , additional features called side-information . special cases of this problem include collaborative filtering and link prediction . we present the first model for dyadic prediction that satisfies several important desiderata : ( i ) labels may be ordinal or nominal , ( ii ) side-information can be easily exploited if present , ( iii ) with or without side-information , latent features are inferred for dyad members , ( iv ) it is resistant to sample-selection bias , ( v ) it can learn well-calibrated probabilities , and ( vi ) it can scale to very large datasets . to our knowledge , no existing method satisfies all the above criteria . in particular , many methods assume that the labels are ordinal and ignore side-information when it is present . experimental results show that the new method is competitive with state-of-the-art methods for the special cases of collaborative filtering and link prediction , and that it makes accurate predictions on nominal data .", "topics": ["test set", "loss function"]}
{"title": "combining textual content and structure to improve dialog similarity", "abstract": "chatbots , taking advantage of the success of the messaging apps and recent advances in artificial intelligence , have become very popular , from helping business to improve customer services to chatting to users for the sake of conversation and engagement ( celebrity or personal bots ) . however , developing and improving a chatbot requires understanding their data generated by its users . dialog data has a different nature of a simple question and answering interaction , in which context and temporal properties ( turn order ) creates a different understanding of such data . in this paper , we propose a novelty metric to compute dialogs ' similarity based not only on the text content but also on the information related to the dialog structure . our experimental results performed over the switchboard dataset show that using evidence from both textual content and the dialog structure leads to more accurate results than using each measure in isolation .", "topics": ["artificial intelligence"]}
{"title": "sequence-based sleep stage classification using conditional neural fields", "abstract": "sleep signals from a polysomnographic database are sequences in nature . commonly employed analysis and classification methods , however , ignored this fact and treated the sleep signals as non-sequence data . treating the sleep signals as sequences , this paper compared two powerful unsupervised feature extractors and three sequence-based classifiers regarding accuracy and computational ( training and testing ) time after 10-folds cross-validation . the compared feature extractors are deep belief networks ( dbn ) and fuzzy c-means ( fcm ) clustering . whereas the compared sequence-based classifiers are hidden markov models ( hmm ) , conditional random fields ( crf ) and its variants , i.e . , hidden-state crf ( hcrf ) and latent-dynamic crf ( ldcrf ) ; and conditional neural fields ( cnf ) and its variant ( ldcnf ) . in this study , we use two datasets . the first dataset is an open ( public ) polysomnographic dataset downloadable from the internet , while the second dataset is our polysomnographic dataset ( also available for download ) . for the first dataset , the combination of fcm and cnf gives the highest accuracy ( 96.75\\ % ) with relatively short training time ( 0.33 hours ) . for the second dataset , the combination of dbn and crf gives the accuracy of 99.96\\ % but with 1.02 hours training time , whereas the combination of dbn and cnf gives slightly less accuracy ( 99.69\\ % ) but also less computation time ( 0.89 hours ) .", "topics": ["cluster analysis", "time complexity"]}
{"title": "cost-effective implementation of order-statistics based vector filters using minimax approximations", "abstract": "vector operators based on robust order statistics have proved successful in digital multichannel imaging applications , particularly color image filtering and enhancement , in dealing with impulsive noise while preserving edges and fine image details . these operators often have very high computational requirements which limits their use in time-critical applications . this paper introduces techniques to speed up vector filters using the minimax approximation theory . extensive experiments on a large and diverse set of color images show that proposed approximations achieve an excellent balance among ease of implementation , accuracy , and computational speed .", "topics": ["approximation"]}
{"title": "sa-iga : a multiagent reinforcement learning method towards socially optimal outcomes", "abstract": "in multiagent environments , the capability of learning is important for an agent to behave appropriately in face of unknown opponents and dynamic environment . from the system designer 's perspective , it is desirable if the agents can learn to coordinate towards socially optimal outcomes , while also avoiding being exploited by selfish opponents . to this end , we propose a novel gradient ascent based algorithm ( sa-iga ) which augments the basic gradient-ascent algorithm by incorporating social awareness into the policy update process . we theoretically analyze the learning dynamics of sa-iga using dynamical system theory and sa-iga is shown to have linear dynamics for a wide range of games including symmetric games . the learning dynamics of two representative games ( the prisoner 's dilemma game and the coordination game ) are analyzed in details . based on the idea of sa-iga , we further propose a practical multiagent learning algorithm , called sa-pga , based on q-learning update rule . simulation results show that sa-pga agent can achieve higher social welfare than previous social-optimality oriented conditional joint action learner ( cjal ) and also is robust against individually rational opponents by reaching nash equilibrium solutions .", "topics": ["reinforcement learning", "simulation"]}
{"title": "deeptransport : learning spatial-temporal dependency for traffic condition forecasting", "abstract": "predicting traffic conditions has been recently explored as a way to relieve traffic congestion . several pioneering approaches have been proposed based on traffic observations of the target location as well as its adjacent regions , but they obtain somewhat limited accuracy due to lack of mining road topology . to address the effect attenuation problem , we propose to take account of the traffic of surrounding locations ( wider than adjacent range ) . we propose an end-to-end framework called deeptransport , in which convolutional neural networks ( cnn ) and recurrent neural networks ( rnn ) are utilized to obtain spatial-temporal traffic information within a transport network topology . in addition , attention mechanism is introduced to align spatial and temporal information . moreover , we constructed and released a real-world large traffic condition dataset with 5-minute resolution . our experiments on this dataset demonstrate our method captures the complex relationship in temporal and spatial domain . it significantly outperforms traditional statistical methods and a state-of-the-art deep learning method .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "large-scale electron microscopy image segmentation in spark", "abstract": "the emerging field of connectomics aims to unlock the mysteries of the brain by understanding the connectivity between neurons . to map this connectivity , we acquire thousands of electron microscopy ( em ) images with nanometer-scale resolution . after aligning these images , the resulting dataset has the potential to reveal the shapes of neurons and the synaptic connections between them . however , imaging the brain of even a tiny organism like the fruit fly yields terabytes of data . it can take years of manual effort to examine such image volumes and trace their neuronal connections . one solution is to apply image segmentation algorithms to help automate the tracing tasks . in this paper , we propose a novel strategy to apply such segmentation on very large datasets that exceed the capacity of a single machine . our solution is robust to potential segmentation errors which could otherwise severely compromise the quality of the overall segmentation , for example those due to poor classifier generalizability or anomalies in the image dataset . we implement our algorithms in a spark application which minimizes disk i/o , and apply them to a few large em datasets , revealing both their effectiveness and scalability . we hope this work will encourage external contributions to em segmentation by providing 1 ) a flexible plugin architecture that deploys easily on different cluster environments and 2 ) an in-memory representation of segmentation that could be conducive to new advances .", "topics": ["image segmentation", "scalability"]}
{"title": "inference in probabilistic graphical models by graph neural networks", "abstract": "a useful computation when acting in a complex environment is to infer the marginal probabilities or most probable states of task-relevant variables . probabilistic graphical models can efficiently represent the structure of such complex data , but performing these inferences is generally difficult . message-passing algorithms , such as belief propagation , are a natural way to disseminate evidence amongst correlated variables while exploiting the graph structure , but these algorithms can struggle when the conditional dependency graphs contain loops . here we use graph neural networks ( gnns ) to learn a message-passing algorithm that solves these inference tasks . we first show that the architecture of gnns is well-matched to inference tasks . we then demonstrate the efficacy of this inference approach by training gnns on an ensemble of graphical models and showing that they substantially outperform belief propagation on loopy graphs . our message-passing algorithms generalize out of the training set to larger graphs and graphs with different structure .", "topics": ["graphical model", "neural networks"]}
{"title": "a baseline for detecting misclassified and out-of-distribution examples in neural networks", "abstract": "we consider the two related problems of detecting if an example is misclassified or out-of-distribution . we present a simple baseline that utilizes probabilities from softmax distributions . correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples , allowing for their detection . we assess performance by defining several tasks in computer vision , natural language processing , and automatic speech recognition , showing the effectiveness of this baseline across all . we then show the baseline can sometimes be surpassed , demonstrating the room for future research on these underexplored detection tasks .", "topics": ["baseline ( configuration management )", "natural language processing"]}
{"title": "strategic planning in air traffic control as a multi-objective stochastic optimization problem", "abstract": "with the objective of handling the airspace sector congestion subject to continuously growing air traffic , we suggest to create a collaborative working plan during the strategic phase of air traffic control . the plan obtained via a new decision support tool presented in this article consists in a schedule for controllers , which specifies time of overflight on the different waypoints of the flight plans . in order to do it , we believe that the decision-support tool shall model directly the uncertainty at a trajectory level in order to propagate the uncertainty to the sector level . then , the probability of congestion for any sector in the airspace can be computed . since air traffic regulations and sector congestion are antagonist , we designed and implemented a multi-objective optimization algorithm for determining the best trade-off between these two criteria . the solution comes up as a set of alternatives for the multi-sector planner where the severity of the congestion cost is adjustable . in this paper , the non-dominated sorting genetic algorithm ( nsga-ii ) was used to solve an artificial benchmark problem involving 24 aircraft and 11 sectors , and is able to provide a good approximation of the pareto front .", "topics": ["optimization problem", "approximation"]}
{"title": "neuralnetwork based 3d surface reconstruction", "abstract": "this paper proposes a novel neural-network-based adaptive hybrid-reflectance three-dimensional ( 3-d ) surface reconstruction model . the neural network combines the diffuse and specular components into a hybrid model . the proposed model considers the characteristics of each point and the variant albedo to prevent the reconstructed surface from being distorted . the neural network inputs are the pixel values of the two-dimensional images to be reconstructed . the normal vectors of the surface can then be obtained from the output of the neural network after supervised learning , where the illuminant direction does not have to be known in advance . finally , the obtained normal vectors can be applied to integration method when reconstructing 3-d objects . facial images were used for training in the proposed approach", "topics": ["supervised learning", "pixel"]}
{"title": "plug and play ! a simple , universal model for energy disaggregation", "abstract": "energy disaggregation is to discover the energy consumption of individual appliances from their aggregated energy values . to solve the problem , most existing approaches rely on either appliances ' signatures or their state transition patterns , both hard to obtain in practice . aiming at developing a simple , universal model that works without depending on sophisticated machine learning techniques or auxiliary equipments , we make use of easily accessible knowledge of appliances and the sparsity of the switching events to design a sparse switching event recovering ( sser ) method . by minimizing the total variation ( tv ) of the ( sparse ) event matrix , sser can effectively recover the individual energy consumption values from the aggregated ones . to speed up the process , a parallel local optimization algorithm ( ploa ) is proposed to solve the problem in active epochs of appliance activities in parallel . using real-world trace data , we compare the performance of our method with that of the state-of-the-art solutions , including least square estimation ( lse ) and iterative hidden markov model ( hmm ) . the results show that our approach has an overall higher detection accuracy and a smaller overhead .", "topics": ["sparse matrix"]}
{"title": "learning adversarially fair and transferable representations", "abstract": "in this work , we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream . we envision a scenario where learned representations may be handed off to other entities with unknown objectives . we propose and explore adversarial representation learning as a natural method of ensuring those entities will act fairly , and connect group fairness ( demographic parity , equalized odds , and equal opportunity ) to different adversarial objectives . through worst-case theoretical guarantees and experimental validation , we show that the choice of this objective is crucial to fair prediction . furthermore , we present the first in-depth experimental demonstration of fair transfer learning , by showing that our learned representations admit fair predictions on new tasks while maintaining utility , an essential goal of fair representation learning .", "topics": ["feature learning", "entity"]}
{"title": "augmenting supervised neural networks with unsupervised objectives for large-scale image classification", "abstract": "unsupervised learning and supervised learning are key research topics in deep learning . however , as high-capacity supervised neural networks trained with a large amount of labels have achieved remarkable success in many computer vision tasks , the availability of large-scale labeled images reduced the significance of unsupervised learning . inspired by the recent trend toward revisiting the importance of unsupervised learning , we investigate joint supervised and unsupervised learning in a large-scale setting by augmenting existing neural networks with decoding pathways for reconstruction . first , we demonstrate that the intermediate activations of pretrained large-scale classification networks preserve almost all the information of input images except a portion of local spatial details . then , by end-to-end training of the entire augmented architecture with the reconstructive objective , we show improvement of the network performance for supervised tasks . we evaluate several variants of autoencoders , including the recently proposed `` what-where '' autoencoder that uses the encoder pooling switches , to study the importance of the architecture design . taking the 16-layer vggnet trained under the imagenet ilsvrc 2012 protocol as a strong baseline for image classification , our methods improve the validation-set accuracy by a noticeable margin .", "topics": ["baseline ( configuration management )", "supervised learning"]}
{"title": "image search reranking", "abstract": "the existing methods for image search reranking suffer from the unfaithfulness of the assumptions under which the text-based images search result . the resulting images contain more irrelevant images . hence the re ranking concept arises to re rank the retrieved images based on the text around the image and data of data of image and visual feature of image . a number of methods are differentiated for this re-ranking . the high ranked images are used as noisy data and a k means algorithm for classification is learned to rectify the ranking further . we are study the affect ability of the cross validation method to this training data . the pre eminent originality of the overall method is in collecting text/metadata of image and visual features in order to achieve an automatic ranking of the images . supervision is initiated to learn the model weights offline , previous to reranking process . while model learning needs manual labeling of the results for a some limited queries , the resulting model is query autonomous and therefore applicable to any other query .examples are given for a selection of other classes like vehicles , animals and other classes .", "topics": ["test set", "relevance"]}
{"title": "a workflow for visual diagnostics of binary classifiers using instance-level explanations", "abstract": "human-in-the-loop data analysis applications necessitate greater transparency in machine learning models for experts to understand and trust their decisions . to this end , we propose a visual analytics workflow to help data scientists and domain experts explore , diagnose , and understand the decisions made by a binary classifier . the approach leverages `` instance-level explanations '' , measures of local feature relevance that explain single instances , and uses them to build a set of visual representations that guide the users in their investigation . the workflow is based on three main visual representations and steps : one based on aggregate statistics to see how data distributes across correct / incorrect decisions ; one based on explanations to understand which features are used to make these decisions ; and one based on raw data , to derive insights on potential root causes for the observed patterns . the workflow is derived from a long-term collaboration with a group of machine learning and healthcare professionals who used our method to make sense of machine learning models they developed . the case study from this collaboration demonstrates that the proposed workflow helps experts derive useful knowledge about the model and the phenomena it describes , thus experts can generate useful hypotheses on how a model can be improved .", "topics": ["statistical classification", "relevance"]}
{"title": "question answering system using syntactic information", "abstract": "question answering task is now being done in trec8 using english documents . we examined question answering task in japanese sentences . our method selects the answer by matching the question sentence with knowledge-based data written in natural language . we use syntactic information to obtain highly accurate answers .", "topics": ["natural language"]}
{"title": "logical inference algorithms and matrix representations for probabilistic conditional independence", "abstract": "logical inference algorithms for conditional independence ( ci ) statements have important applications from testing consistency during knowledge elicitation to constraintbased structure learning of graphical models . we prove that the implication problem for ci statements is decidable , given that the size of the domains of the random variables is known and fixed . we will present an approximate logical inference algorithm which combines a falsification and a novel validation algorithm . the validation algorithm represents each set of ci statements as a sparse 0-1 matrix a and validates instances of the implication problem by solving specific linear programs with constraint matrix a . we will show experimentally that the algorithm is both effective and efficient in validating and falsifying instances of the probabilistic ci implication problem .", "topics": ["graphical model", "approximation algorithm"]}
{"title": "filter design and performance evaluation for fingerprint image segmentation", "abstract": "fingerprint recognition plays an important role in many commercial applications and is used by millions of people every day , e.g . for unlocking mobile phones . fingerprint image segmentation is typically the first processing step of most fingerprint algorithms and it divides an image into foreground , the region of interest , and background . two types of error can occur during this step which both have a negative impact on the recognition performance : 'true ' foreground can be labeled as background and features like minutiae can be lost , or conversely 'true ' background can be misclassified as foreground and spurious features can be introduced . the contribution of this paper is threefold : firstly , we propose a novel factorized directional bandpass ( fdb ) segmentation method for texture extraction based on the directional hilbert transform of a butterworth bandpass ( dhbb ) filter interwoven with soft-thresholding . secondly , we provide a manually marked ground truth segmentation for 10560 images as an evaluation benchmark . thirdly , we conduct a systematic performance comparison between the fdb method and four of the most often cited fingerprint segmentation algorithms showing that the fdb segmentation method clearly outperforms these four widely used methods . the benchmark and the implementation of the fdb method are made publicly available .", "topics": ["image segmentation", "ground truth"]}
{"title": "label optimal regret bounds for online local learning", "abstract": "we resolve an open question from ( christiano , 2014b ) posed in colt'14 regarding the optimal dependency of the regret achievable for online local learning on the size of the label set . in this framework the algorithm is shown a pair of items at each step , chosen from a set of $ n $ items . the learner then predicts a label for each item , from a label set of size $ l $ and receives a real valued payoff . this is a natural framework which captures many interesting scenarios such as collaborative filtering , online gambling , and online max cut among others . ( christiano , 2014a ) designed an efficient online learning algorithm for this problem achieving a regret of $ o ( \\sqrt { nl^3t } ) $ , where $ t $ is the number of rounds . information theoretically , one can achieve a regret of $ o ( \\sqrt { n \\log l t } ) $ . one of the main open questions left in this framework concerns closing the above gap . in this work , we provide a complete answer to the question above via two main results . we show , via a tighter analysis , that the semi-definite programming based algorithm of ( christiano , 2014a ) , in fact achieves a regret of $ o ( \\sqrt { nlt } ) $ . second , we show a matching computational lower bound . namely , we show that a polynomial time algorithm for online local learning with lower regret would imply a polynomial time algorithm for the planted clique problem which is widely believed to be hard . we prove a similar hardness result under a related conjecture concerning planted dense subgraphs that we put forth . unlike planted clique , the planted dense subgraph problem does not have any known quasi-polynomial time algorithms . computational lower bounds for online learning are relatively rare , and we hope that the ideas developed in this work will lead to lower bounds for other online learning scenarios as well .", "topics": ["regret ( decision theory )", "time complexity"]}
{"title": "prnn : recurrent neural network with persistent memory", "abstract": "although recurrent neural network ( rnn ) has been a powerful tool for modeling sequential data , its performance is inadequate when processing sequences with multiple patterns . in this paper , we address this challenge by introducing an external memory and constructing a novel persistent memory augmented rnn ( termed as prnn ) . the prnn captures the principle patterns in training sequences and stores them in an external memory . by leveraging the persistent memory , the proposed method can adaptively update states according to the similarities between encoded inputs and memory slots , leading to a stronger capacity in assimilating sequences with multiple patterns . content-based addressing is suggested in memory accessing , and gradient descent is utilized for implicitly updating the memory . our approach can be further extended by combining the prior knowledge of data . experiments on several datasets demonstrate the effectiveness of the proposed method .", "topics": ["recurrent neural network", "gradient descent"]}
{"title": "what is the plausibility of probability ? ( revised 2003 , 2015 )", "abstract": "we present and examine a result related to uncertainty reasoning , namely that a certain plausibility space of cox 's type can be uniquely embedded in a minimal ordered field . this , although a purely mathematical result , can be claimed to imply that every rational method to reason with uncertainty must be based on sets of extended probability distributions , where extended probability is standard probability extended with infinitesimals . this claim must be supported by some argumentation of non-mathematical type , however , since pure mathematics does not tell us anything about the world . we propose one such argumentation , and relate it to results from the literature of uncertainty and statistics . in an added retrospective section we discuss some developments in the area regarding countable additivity , partially ordered domains and robustness , and philosophical stances on the cox/jaynes approach since 2003 . we also show that the most general partially ordered plausibility calculus embeddable in a ring can be represented as a set of extended probability distributions or , in algebraic terms , is a subdirect sum of ordered fields . in other words , the robust bayesian approach is universal . this result is exemplified by relating dempster-shafer 's evidence theory to robust bayesian analysis .", "topics": ["bayesian network"]}
{"title": "synthesis of parametric programs using genetic programming and model checking", "abstract": "formal methods apply algorithms based on mathematical principles to enhance the reliability of systems . it would only be natural to try to progress from verification , model checking or testing a system against its formal specification into constructing it automatically . classical algorithmic synthesis theory provides interesting algorithms but also alarming high complexity and undecidability results . the use of genetic programming , in combination with model checking and testing , provides a powerful heuristic to synthesize programs . the method is not completely automatic , as it is fine tuned by a user that sets up the specification and parameters . it also does not guarantee to always succeed and converge towards a solution that satisfies all the required properties . however , we applied it successfully on quite nontrivial examples and managed to find solutions to hard programming challenges , as well as to improve and to correct code . we describe here several versions of our method for synthesizing sequential and concurrent systems .", "topics": ["heuristic"]}
{"title": "improved speech reconstruction from silent video", "abstract": "speechreading is the task of inferring phonetic information from visually observed articulatory facial movements , and is a notoriously difficult task for humans to perform . in this paper we present an end-to-end model based on a convolutional neural network ( cnn ) for generating an intelligible and natural-sounding acoustic speech signal from silent video frames of a speaking person . we train our model on speakers from the grid and tcd-timit datasets , and evaluate the quality and intelligibility of reconstructed speech using common objective measurements . we show that speech predictions from the proposed model attain scores which indicate significantly improved quality over existing models . in addition , we show promising results towards reconstructing speech from an unconstrained dictionary .", "topics": ["end-to-end principle", "dictionary"]}
{"title": "opinion polarity identification through adjectives", "abstract": "`` what other people think '' has always been an important piece of information during various decision-making processes . today people frequently make their opinions available via the internet , and as a result , the web has become an excellent source for gathering consumer opinions . there are now numerous web resources containing such opinions , e.g . , product reviews forums , discussion groups , and blogs . but , due to the large amount of information and the wide range of sources , it is essentially impossible for a customer to read all of the reviews and make an informed decision on whether to purchase the product . it is also difficult for the manufacturer or seller of a product to accurately monitor customer opinions . for this reason , mining customer reviews , or opinion mining , has become an important issue for research in web information extraction . one of the important topics in this research area is the identification of opinion polarity . the opinion polarity of a review is usually expressed with values 'positive ' , 'negative ' or 'neutral ' . we propose a technique for identifying polarity of reviews by identifying the polarity of the adjectives that appear in them . our evaluation shows the technique can provide accuracy in the area of 73 % , which is well above the 58 % -64 % provided by naive bayesian classifiers .", "topics": ["value ( ethics )"]}
{"title": "scaling inference for markov logic with a task-decomposition approach", "abstract": "motivated by applications in large-scale knowledge base construction , we study the problem of scaling up a sophisticated statistical inference framework called markov logic networks ( mlns ) . our approach , felix , uses the idea of lagrangian relaxation from mathematical programming to decompose a program into smaller tasks while preserving the joint-inference property of the original mln . the advantage is that we can use highly scalable specialized algorithms for common tasks such as classification and coreference . we propose an architecture to support lagrangian relaxation in an rdbms which we show enables scalable joint inference for mlns . we empirically validate that felix is significantly more scalable and efficient than prior approaches to mln inference by constructing a knowledge base from 1.8m documents as part of the tac challenge . we show that felix scales and achieves state-of-the-art quality numbers . in contrast , prior approaches do not scale even to a subset of the corpus that is three orders of magnitude smaller .", "topics": ["statistical classification", "mathematical optimization"]}
{"title": "barzilai-borwein step size for stochastic gradient descent", "abstract": "one of the major issues in stochastic gradient descent ( sgd ) methods is how to choose an appropriate step size while running the algorithm . since the traditional line search technique does not apply for stochastic optimization algorithms , the common practice in sgd is either to use a diminishing step size , or to tune a fixed step size by hand , which can be time consuming in practice . in this paper , we propose to use the barzilai-borwein ( bb ) method to automatically compute step sizes for sgd and its variant : stochastic variance reduced gradient ( svrg ) method , which leads to two algorithms : sgd-bb and svrg-bb . we prove that svrg-bb converges linearly for strongly convex objective functions . as a by-product , we prove the linear convergence result of svrg with option i proposed in [ 10 ] , whose convergence result is missing in the literature . numerical experiments on standard data sets show that the performance of sgd-bb and svrg-bb is comparable to and sometimes even better than sgd and svrg with best-tuned step sizes , and is superior to some advanced sgd variants .", "topics": ["optimization problem", "numerical analysis"]}
{"title": "discriminative k-shot learning using probabilistic models", "abstract": "this paper introduces a probabilistic framework for k-shot image classification . the goal is to generalise from an initial large-scale classification task to a separate task comprising new classes and small numbers of examples . the new approach not only leverages the feature-based representation learned by a neural network from the initial task ( representational transfer ) , but also information about the classes ( concept transfer ) . the concept information is encapsulated in a probabilistic model for the final layer weights of the neural network which acts as a prior for probabilistic k-shot learning . we show that even a simple probabilistic model achieves state-of-the-art on a standard k-shot learning dataset by a large margin . moreover , it is able to accurately model uncertainty , leading to well calibrated classifiers , and is easily extensible and flexible , unlike many recent approaches to k-shot learning .", "topics": ["simulation"]}
{"title": "distributed solving through model splitting", "abstract": "constraint problems can be trivially solved in parallel by exploring different branches of the search tree concurrently . previous approaches have focused on implementing this functionality in the solver , more or less transparently to the user . we propose a new approach , which modifies the constraint model of the problem . an existing model is split into new models with added constraints that partition the search space . optionally , additional constraints are imposed that rule out the search already done . the advantages of our approach are that it can be implemented easily , computations can be stopped and restarted , moved to different machines and indeed solved on machines which are not able to communicate with each other at all .", "topics": ["computation"]}
{"title": "global pose estimation with an attention-based recurrent network", "abstract": "the ability for an agent to localize itself within an environment is crucial for many real-world applications . for unknown environments , simultaneous localization and mapping ( slam ) enables incremental and concurrent building of and localizing within a map . we present a new , differentiable architecture , neural graph optimizer , progressing towards a complete neural network solution for slam by designing a system composed of a local pose estimation model , a novel pose selection module , and a novel graph optimization process . the entire architecture is trained in an end-to-end fashion , enabling the network to automatically learn domain-specific features relevant to the visual odometry and avoid the involved process of feature engineering . we demonstrate the effectiveness of our system on a simulated 2d maze and the 3d viz-doom environment .", "topics": ["recurrent neural network", "simulation"]}
{"title": "adaptive forgetting factor fictitious play", "abstract": "it is now well known that decentralised optimisation can be formulated as a potential game , and game-theoretical learning algorithms can be used to find an optimum . one of the most common learning techniques in game theory is fictitious play . however fictitious play is founded on an implicit assumption that opponents ' strategies are stationary . we present a novel variation of fictitious play that allows the use of a more realistic model of opponent strategy . it uses a heuristic approach , from the online streaming data literature , to adaptively update the weights assigned to recently observed actions . we compare the results of the proposed algorithm with those of stochastic and geometric fictitious play in a simple strategic form game , a vehicle target assignment game and a disaster management problem . in all the tests the rate of convergence of the proposed algorithm was similar or better than the variations of fictitious play we compared it with . the new algorithm therefore improves the performance of game-theoretical learning in decentralised optimisation .", "topics": ["mathematical optimization", "heuristic"]}
{"title": "multi-task learning with group-specific feature space sharing", "abstract": "when faced with learning a set of inter-related tasks from a limited amount of usable data , learning each task independently may lead to poor generalization performance . multi-task learning ( mtl ) exploits the latent relations between tasks and overcomes data scarcity limitations by co-learning all these tasks simultaneously to offer improved performance . we propose a novel multi-task multiple kernel learning framework based on support vector machines for binary classification tasks . by considering pair-wise task affinity in terms of similarity between a pair 's respective feature spaces , the new framework , compared to other similar mtl approaches , offers a high degree of flexibility in determining how similar feature spaces should be , as well as which pairs of tasks should share a common feature space in order to benefit overall performance . the associated optimization problem is solved via a block coordinate descent , which employs a consensus-form alternating direction method of multipliers algorithm to optimize the multiple kernel learning weights and , hence , to determine task affinities . empirical evaluation on seven data sets exhibits a statistically significant improvement of our framework 's results compared to the ones of several other clustered multi-task learning methods .", "topics": ["kernel ( operating system )", "feature vector"]}
{"title": "resting state brain networks from eeg : hidden markov states vs. classical microstates", "abstract": "functional brain networks exhibit dynamics on the sub-second temporal scale and are often assumed to embody the physiological substrate of cognitive processes . here we analyse the temporal and spatial dynamics of these states , as measured by eeg , with a hidden markov model and compare this approach to classical eeg microstate analysis . we find dominating state lifetimes of 100 -- 150\\ , ms for both approaches . the state topographies show obvious similarities . however , they also feature distinct spatial and especially temporal properties . these differences may carry physiological meaningful information originating from patterns in the data that the hmm is able to integrate while the microstate analysis is not . this hypothesis is supported by a consistently high pairwise correlation of the temporal evolution of eeg microstates which is not observed for the hmm states and which seems unlikely to be a good description of the underlying physiology . however , further investigation is required to determine the robustness and the functional and clinical relevance of eeg hmm states in comparison to eeg microstates .", "topics": ["relevance"]}
{"title": "development and application of a machine learning supported methodology for measurement and verification ( m & v ) 2.0", "abstract": "the foundations of all methodologies for the measurement and verification ( m & v ) of energy savings are based on the same five key principles : accuracy , completeness , conservatism , consistency and transparency . the most widely accepted methodologies tend to generalise m & v so as to ensure applicability across the spectrum of energy conservation measures ( ecm 's ) . these do not provide a rigid calculation procedure to follow . this paper aims to bridge the gap between high-level methodologies and the practical application of modelling algorithms , with a focus on the industrial buildings sector . this is achieved with the development of a novel , machine learning supported methodology for m & v 2.0 which enables accurate quantification of savings . a novel and computationally efficient feature selection algorithm and powerful machine learning regression algorithms are employed to maximise the effectiveness of available data . the baseline period energy consumption is modelled using artificial neural networks , support vector machines , k-nearest neighbours and multiple ordinary least squares regression . improved knowledge discovery and an expanded boundary of analysis allow more complex energy systems be analysed , thus increasing the applicability of m & v . a case study in a large biomedical manufacturing facility is used to demonstrate the methodology 's ability to accurately quantify the savings under real-world conditions . the ecm was found to result in 604,527 kwh of energy savings with 57 % uncertainty at a confidence interval of 68 % . 20 baseline energy models are developed using an exhaustive approach with the optimal model being used to quantify savings . the range of savings estimated with each model are presented and the acceptability of uncertainty is reviewed . the case study demonstrates the ability of the methodology to perform m & v to an acceptable standard in challenging circumstances .", "topics": ["baseline ( configuration management )", "computational complexity theory"]}
{"title": "filtering tweets for social unrest", "abstract": "since the events of the arab spring , there has been increased interest in using social media to anticipate social unrest . while efforts have been made toward automated unrest prediction , we focus on filtering the vast volume of tweets to identify tweets relevant to unrest , which can be provided to downstream users for further analysis . we train a supervised classifier that is able to label arabic language tweets as relevant to unrest with high reliability . we examine the relationship between training data size and performance and investigate ways to optimize the model building process while minimizing cost . we also explore how confidence thresholds can be set to achieve desired levels of performance .", "topics": ["test set"]}
{"title": "sliding line point regression for shape robust scene text detection", "abstract": "traditional text detection methods mostly focus on quadrangle text . in this study we propose a novel method named sliding line point regression ( slpr ) in order to detect arbitrary-shape text in natural scene . slpr regresses multiple points on the edge of text line and then utilizes these points to sketch the outlines of the text . the proposed slpr can be adapted to many object detection architectures such as faster r-cnn and r-fcn . specifically , we first generate the smallest rectangular box including the text with region proposal network ( rpn ) , then isometrically regress the points on the edge of text by using the vertically and horizontally sliding lines . to make full use of information and reduce redundancy , we calculate x-coordinate or y-coordinate of target point by the rectangular box position , and just regress the remaining y-coordinate or x-coordinate . accordingly we can not only reduce the parameters of system , but also restrain the points which will generate more regular polygon . our approach achieved competitive results on traditional icdar2015 incidental scene text benchmark and curve text detection dataset ctw1500 .", "topics": ["object detection"]}
{"title": "maximizing top-down constraints for unification-based systems", "abstract": "a left-corner parsing algorithm with top-down filtering has been reported to show very efficient performance for unification-based systems . however , due to the nontermination of parsing with left-recursive grammars , top-down constraints must be weakened . in this paper , a general method of maximizing top-down constraints is proposed . the method provides a procedure to dynamically compute *restrictor* , a minimum set of features involved in an infinite loop for every propagation path ; thus top-down constraints are maximally propagated .", "topics": ["parsing"]}
{"title": "modelling data dispersion degree in automatic robust estimation for multivariate gaussian mixture models with an application to noisy speech processing", "abstract": "the trimming scheme with a prefixed cutoff portion is known as a method of improving the robustness of statistical models such as multivariate gaussian mixture models ( mg- mms ) in small scale tests by alleviating the impacts of outliers . however , when this method is applied to real- world data , such as noisy speech processing , it is hard to know the optimal cut-off portion to remove the outliers and sometimes removes useful data samples as well . in this paper , we propose a new method based on measuring the dispersion degree ( dd ) of the training data to avoid this problem , so as to realise automatic robust estimation for mgmms . the dd model is studied by using two different measures . for each one , we theoretically prove that the dd of the data samples in a context of mgmms approximately obeys a specific ( chi or chi-square ) distribution . the proposed method is evaluated on a real-world application with a moderately-sized speaker recognition task . experiments show that the proposed method can significantly improve the robustness of the conventional training method of gmms for speaker recognition .", "topics": ["test set"]}
{"title": "min-max kernels", "abstract": "the min-max kernel is a generalization of the popular resemblance kernel ( which is designed for binary data ) . in this paper , we demonstrate , through an extensive classification study using kernel machines , that the min-max kernel often provides an effective measure of similarity for nonnegative data . as the min-max kernel is nonlinear and might be difficult to be used for industrial applications with massive data , we show that the min-max kernel can be linearized via hashing techniques . this allows practitioners to apply min-max kernel to large-scale applications using well matured linear algorithms such as linear svm or logistic regression . the previous remarkable work on consistent weighted sampling ( cws ) produces samples in the form of ( $ i^* , t^* $ ) where the $ i^* $ records the location ( and in fact also the weights ) information analogous to the samples produced by classical minwise hashing on binary data . because the $ t^* $ is theoretically unbounded , it was not immediately clear how to effectively implement cws for building large-scale linear classifiers . in this paper , we provide a simple solution by discarding $ t^* $ ( which we refer to as the `` 0-bit '' scheme ) . via an extensive empirical study , we show that this 0-bit scheme does not lose essential information . we then apply the `` 0-bit '' cws for building linear classifiers to approximate min-max kernel classifiers , as extensively validated on a wide range of publicly available classification datasets . we expect this work will generate interests among data mining practitioners who would like to efficiently utilize the nonlinear information of non-binary and nonnegative data .", "topics": ["kernel ( operating system )", "nonlinear system"]}
{"title": "estimating a causal order among groups of variables in linear models", "abstract": "the machine learning community has recently devoted much attention to the problem of inferring causal relationships from statistical data . most of this work has focused on uncovering connections among scalar random variables . we generalize existing methods to apply to collections of multi-dimensional random vectors , focusing on techniques applicable to linear models . the performance of the resulting algorithms is evaluated and compared in simulations , which show that our methods can , in many cases , provide useful information on causal relationships even for relatively small sample sizes .", "topics": ["simulation", "causality"]}
{"title": "godseed : benevolent or malevolent ?", "abstract": "it is hypothesized by some thinkers that benign looking ai objectives may result in powerful ai drives that may pose an existential risk to human society . we analyze this scenario and find the underlying assumptions to be unlikely . we examine the alternative scenario of what happens when universal goals that are not human-centric are used for designing ai agents . we follow a design approach that tries to exclude malevolent motivations from ai agents , however , we see that objectives that seem benevolent may pose significant risk . we consider the following meta-rules : preserve and pervade life and culture , maximize the number of free minds , maximize intelligence , maximize wisdom , maximize energy production , behave like human , seek pleasure , accelerate evolution , survive , maximize control , and maximize capital . we also discuss various solution approaches for benevolent behavior including selfless goals , hybrid designs , darwinism , universal constraints , semi-autonomy , and generalization of robot laws . a `` prime directive '' for ai may help in formulating an encompassing constraint for avoiding malicious behavior . we hypothesize that social instincts for autonomous robots may be effective such as attachment learning . we mention multiple beneficial scenarios for an advanced semi-autonomous agi agent in the near future including space exploration , automation of industries , state functions , and cities . we conclude that a beneficial ai agent with intelligence beyond human-level is possible and has many practical use cases .", "topics": ["artificial intelligence"]}
{"title": "using t-norm based uncertainty calculi in a naval situation assessment application", "abstract": "rum ( reasoning with uncertainty module ) , is an integrated software tool based on a kee , a frame system implemented in an object oriented language . rum 's architecture is composed of three layers : representation , inference , and control . the representation layer is based on frame-like data structures that capture the uncertainty information used in the inference layer and the uncertainty meta-information used in the control layer . the inference layer provides a selection of five t-norm based uncertainty calculi with which to perform the intersection , detachment , union , and pooling of information . the control layer uses the meta-information to select the appropriate calculus for each context and to resolve eventual ignorance or conflict in the information . this layer also provides a context mechanism that allows the system to focus on the relevant portion of the knowledge base , and an uncertain-belief revision system that incrementally updates the certainty values of well-formed formulae ( wffs ) in an acyclic directed deduction graph . rum has been tested and validated in a sequence of experiments in both naval and aerial situation assessment ( sa ) , consisting of correlating reports and tracks , locating and classifying platforms , and identifying intents and threats . an example of naval situation assessment is illustrated . the testbed environment for developing these experiments has been provided by lotta , a symbolic simulator implemented in flavors . this simulator maintains time-varying situations in a multi-player antagonistic game where players must make decisions in light of uncertain and incomplete data . rum has been used to assist one of the lotta players to perform the sa task .", "topics": ["simulation"]}
{"title": "practical reasoning with norms for autonomous software agents ( full edition )", "abstract": "autonomous software agents operating in dynamic environments need to constantly reason about actions in pursuit of their goals , while taking into consideration norms which might be imposed on those actions . normative practical reasoning supports agents making decisions about what is best for them to ( not ) do in a given situation . what makes practical reasoning challenging is the interplay between goals that agents are pursuing and the norms that the agents are trying to uphold . we offer a formalisation to allow agents to plan for multiple goals and norms in the presence of durative actions that can be executed concurrently . we compare plans based on decision-theoretic notions ( i.e . utility ) such that the utility gain of goals and utility loss of norm violations are the basis for this comparison . the set of optimal plans consists of plans that maximise the overall utility , each of which can be chosen by the agent to execute . we provide an implementation of our proposal in answer set programming , thus allowing us to state the original problem in terms of a logic program that can be queried for solutions with specific properties . the implementation is proven to be sound and complete .", "topics": ["autonomous car"]}
{"title": "onion-peeling outlier detection in 2-d data sets", "abstract": "outlier detection is a critical and cardinal research task due its array of applications in variety of domains ranging from data mining , clustering , statistical analysis , fraud detection , network intrusion detection and diagnosis of diseases etc . over the last few decades , distance-based outlier detection algorithms have gained significant reputation as a viable alternative to the more traditional statistical approaches due to their scalable , non-parametric and simple implementation . in this paper , we present a modified onion peeling ( convex hull ) genetic algorithm to detect outliers in a gaussian 2-d point data set . we present three different scenarios of outlier detection using a ) euclidean distance metric b ) standardized euclidean distance metric and c ) mahalanobis distance metric . finally , we analyze the performance and evaluate the results .", "topics": ["data mining", "cluster analysis"]}
{"title": "mining and exploiting domain-specific corpora in the panacea platform", "abstract": "the objective of the panacea ict-2007.2.2 eu project is to build a platform that automates the stages involved in the acquisition , production , updating and maintenance of the large language resources required by , among others , mt systems . the development of a corpus acquisition component ( cac ) for extracting monolingual and bilingual data from the web is one of the most innovative building blocks of panacea . the cac , which is the first stage in the panacea pipeline for building language resources , adopts an efficient and distributed methodology to crawl for web documents with rich textual content in specific languages and predefined domains . the cac includes modules that can acquire parallel data from sites with in-domain content available in more than one language . in order to extrinsically evaluate the cac methodology , we have conducted several experiments that used crawled parallel corpora for the identification and extraction of parallel sentences using sentence alignment . the corpora were then successfully used for domain adaptation of machine translation systems .", "topics": ["machine translation", "text corpus"]}
{"title": "memory and information processing in recurrent neural networks", "abstract": "recurrent neural networks ( rnn ) are simple dynamical systems whose computational power has been attributed to their short-term memory . short-term memory of rnns has been previously studied analytically only for the case of orthogonal networks , and only under annealed approximation , and uncorrelated input . here for the first time , we present an exact solution to the memory capacity and the task-solving performance as a function of the structure of a given network instance , enabling direct determination of the function -- structure relation in rnns . we calculate the memory capacity for arbitrary networks with exponentially correlated input and further related it to the performance of the system on signal processing tasks in a supervised learning setup . we compute the expected error and the worst-case error bound as a function of the spectra of the network and the correlation structure of its inputs and outputs . our results give an explanation for learning and generalization of task solving using short-term memory , which is crucial for building alternative computer architectures using physical phenomena based on the short-term memory principle .", "topics": ["supervised learning", "recurrent neural network"]}
{"title": "deepmasterprint : fingerprint spoofing via latent variable evolution", "abstract": "biometric authentication is important for a large range of systems , including but not limited to consumer electronic devices such as phones . understanding the limits of and attacks on such systems is therefore crucial . this paper presents an attack on fingerprint recognition system using masterprints , synthetic fingerprints that are capable of spoofing multiple people 's fingerprints . the method described is the first to generate complete image-level masterprints , and further exceeds the attack accuracy of previous methods that could not produce complete images . the method , latent variable evolution , is based on training a generative adversarial network on a set of real fingerprint images . stochastic search in the form of the covariance matrix adaptation evolution strategy is then used to search for latent variable ( inputs ) to the generator network that optimize the number of matches from a fingerprint recognizer . we find masterprints that a commercial fingerprint system matches to 23 % of all users in a strict security setting , and 77 % of all users at a looser security setting . the underlying method is likely to have broad usefulness for security research as well as in aesthetic domains .", "topics": ["synthetic data"]}
{"title": "users prefer guetzli jpeg over same-sized libjpeg", "abstract": "we report on pairwise comparisons by human raters of jpeg images from libjpeg and our new guetzli encoder . although both files are size-matched , 75 % of ratings are in favor of guetzli . this implies the butteraugli psychovisual image similarity metric which guides guetzli is reasonably close to human perception at high quality levels . we provide access to the raw ratings and source images for further analysis and study .", "topics": ["encoder"]}
{"title": "policy improvement for pomdps using normalized importance sampling", "abstract": "we present a new method for estimating the expected return of a pomdp from experience . the method does not assume any knowledge of the pomdp and allows the experience to be gathered from an arbitrary sequence of policies . the return is estimated for any new policy of the pomdp . we motivate the estimator from function-approximation and importance sampling points-of-view and derive its theoretical properties . although the estimator is biased , it has low variance and the bias is often irrelevant when the estimator is used for pair-wise comparisons . we conclude by extending the estimator to policies with memory and compare its performance in a greedy search algorithm to reinforce algorithms showing an order of magnitude reduction in the number of trials required .", "topics": ["sampling ( signal processing )", "relevance"]}
{"title": "treatment of epsilon-moves in subset construction", "abstract": "the paper discusses the problem of determinising finite-state automata containing large numbers of epsilon-moves . experiments with finite-state approximations of natural language grammars often give rise to very large automata with a very large number of epsilon-moves . the paper identifies three subset construction algorithms which treat epsilon-moves . a number of experiments has been performed which indicate that the algorithms differ considerably in practice . furthermore , the experiments suggest that the average number of epsilon-moves per state can be used to predict which algorithm is likely to perform best for a given input automaton .", "topics": ["natural language", "approximation"]}
{"title": "automated visual fin identification of individual great white sharks", "abstract": "this paper discusses the automated visual identification of individual great white sharks from dorsal fin imagery . we propose a computer vision photo id system and report recognition results over a database of thousands of unconstrained fin images . to the best of our knowledge this line of work establishes the first fully automated contour-based visual id system in the field of animal biometrics . the approach put forward appreciates shark fins as textureless , flexible and partially occluded objects with an individually characteristic shape . in order to recover animal identities from an image we first introduce an open contour stroke model , which extends multi-scale region segmentation to achieve robust fin detection . secondly , we show that combinatorial , scale-space selective fingerprinting can successfully encode fin individuality . we then measure the species-specific distribution of visual individuality along the fin contour via an embedding into a global `fin space ' . exploiting this domain , we finally propose a non-linear model for individual animal recognition and combine all approaches into a fine-grained multi-instance framework . we provide a system evaluation , compare results to prior work , and report performance and properties in detail .", "topics": ["nonlinear system", "computer vision"]}
{"title": "random feature maps for dot product kernels", "abstract": "approximating non-linear kernels using feature maps has gained a lot of interest in recent years due to applications in reducing training and testing times of svm classifiers and other kernel based learning algorithms . we extend this line of work and present low distortion embeddings for dot product kernels into linear euclidean spaces . we base our results on a classical result in harmonic analysis characterizing all dot product kernels and use it to define randomized feature maps into explicit low dimensional euclidean spaces in which the native dot product provides an approximation to the dot product kernel with high confidence .", "topics": ["kernel ( operating system )", "approximation algorithm"]}
{"title": "learning neural trans-dimensional random field language models with noise-contrastive estimation", "abstract": "trans-dimensional random field language models ( trf lms ) where sentences are modeled as a collection of random fields , have shown close performance with lstm lms in speech recognition and are computationally more efficient in inference . however , the training efficiency of neural trf lms is not satisfactory , which limits the scalability of trf lms on large training corpus . in this paper , several techniques on both model formulation and parameter estimation are proposed to improve the training efficiency and the performance of neural trf lms . first , trfs are reformulated in the form of exponential tilting of a reference distribution . second , noise-contrastive estimation ( nce ) is introduced to jointly estimate the model parameters and normalization constants . third , we extend the neural trf lms by marrying the deep convolutional neural network ( cnn ) and the bidirectional lstm into the potential function to extract the deep hierarchical features and bidirectionally sequential features . utilizing all the above techniques enables the successful and efficient training of neural trf lms on a 40x larger training set with only 1/3 training time and further reduces the wer with relative reduction of 4.7 % on top of a strong lstm lm baseline .", "topics": ["baseline ( configuration management )", "speech recognition"]}
{"title": "modified alpha-rooting color image enhancement method on the two-side 2-d quaternion discrete fourier transform and the 2-d discrete fourier transform", "abstract": "color in an image is resolved into 3 or 4 color components and 2-dimages of these components are stored in separate channels . most of the color image enhancement algorithms are applied channel-by-channel on each image . but such a system of color image processing is not processing the original color . when a color image is represented as a quaternion image , processing is done in original colors . this paper proposes an implementation of the quaternion approach of enhancement algorithm for enhancing color images and is referred as the modified alpha-rooting by the two-dimensional quaternion discrete fourier transform ( 2-d qdft ) . enhancement results of this proposed method are compared with the channel-by-channel image enhancement by the 2-d dft . enhancements in color images are quantitatively measured by the color enhancement measure estimation ( ceme ) , which allows for selecting optimum parameters for processing by the genetic algorithm . enhancement of color images by the quaternion based method allows for obtaining images which are closer to the genuine representation of the real original color .", "topics": ["image processing"]}
{"title": "a novel approach to develop a new hybrid technique for trademark image retrieval", "abstract": "trademark image retrieval is playing a vital role as a part of cbir system . trademark is of great significance because it carries the status value of any company . to retrieve such a fake or copied trademark we design a retrieval system which is based on hybrid techniques . it contains a mixture of two different feature vector which combined together to give a suitable retrieval system . in the proposed system we extract the corner feature which is applied on an edge pixel image . this feature is used to extract the relevant image and to more purify the result we apply other feature which is the invariant moment feature . from the experimental result we conclude that the system is 85 percent efficient .", "topics": ["feature vector", "pixel"]}
{"title": "neural networks for joint sentence classification in medical paper abstracts", "abstract": "existing models based on artificial neural networks ( anns ) for sentence classification often do not incorporate the context in which sentences appear , and classify sentences individually . however , traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences , such as with conditional random fields . in this work , we present an ann architecture that combines the effectiveness of typical ann models to classify sentences in isolation , with the strength of structured prediction . our model achieves state-of-the-art results on two different datasets for sequential sentence classification in medical abstracts .", "topics": ["neural networks"]}
{"title": "rethinking the inception architecture for computer vision", "abstract": "convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks . since 2014 very deep convolutional networks started to become mainstream , yielding substantial gains in various benchmarks . although increased model size and computational cost tend to translate to immediate quality gains for most tasks ( as long as enough labeled data is provided for training ) , computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios . here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization . we benchmark our methods on the ilsvrc 2012 classification challenge validation set demonstrate substantial gains over the state of the art : 21.2 % top-1 and 5.6 % top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters . with an ensemble of 4 models and multi-crop evaluation , we report 3.5 % top-5 error on the validation set ( 3.6 % error on the test set ) and 17.3 % top-1 error on the validation set .", "topics": ["test set", "computer vision"]}
{"title": "detecting spammers via aggregated historical data set", "abstract": "the battle between email service providers and senders of mass unsolicited emails ( spam ) continues to gain traction . vast numbers of spam emails are sent mainly from automatic botnets distributed over the world . one method for mitigating spam in a computationally efficient manner is fast and accurate blacklisting of the senders . in this work we propose a new sender reputation mechanism that is based on an aggregated historical data-set which encodes the behavior of mail transfer agents over time . a historical data-set is created from labeled logs of received emails . we use machine learning algorithms to build a model that predicts the \\emph { spammingness } of mail transfer agents in the near future . the proposed mechanism is targeted mainly at large enterprises and email service providers and can be used for updating both the black and the white lists . we evaluate the proposed mechanism using 9.5m anonymized log entries obtained from the biggest internet service provider in europe . experiments show that proposed method detects more than 94 % of the spam emails that escaped the blacklist ( i.e . , tpr ) , while having less than 0.5 % false-alarms . therefore , the effectiveness of the proposed method is much higher than of previously reported reputation mechanisms , which rely on emails logs . in addition , the proposed method , when used for updating both the black and white lists , eliminated the need in automatic content inspection of 4 out of 5 incoming emails , which resulted in dramatic reduction in the filtering computational load .", "topics": ["computational complexity theory"]}
{"title": "loss-aware weight quantization of deep networks", "abstract": "the huge size of deep networks hinders their use in small computing devices . in this paper , we consider compressing the network by weight quantization . we extend a recently proposed loss-aware weight binarization scheme to ternarization , with possibly different scaling parameters for the positive and negative weights , and m-bit ( where m > 2 ) quantization . experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms , and is as accurate ( or even more accurate ) than the full-precision network .", "topics": ["recurrent neural network"]}
{"title": "learning a hyperplane classifier by minimizing an exact bound on the vc dimension", "abstract": "the vc dimension measures the capacity of a learning machine , and a low vc dimension leads to good generalization . while svms produce state-of-the-art learning performance , it is well known that the vc dimension of a svm can be unbounded ; despite good results in practice , there is no guarantee of good generalization . in this paper , we show how to learn a hyperplane classifier by minimizing an exact , or \\boldmath { $ \\theta $ } bound on its vc dimension . the proposed approach , termed as the minimal complexity machine ( mcm ) , involves solving a simple linear programming problem . experimental results show , that on a number of benchmark datasets , the proposed approach learns classifiers with error rates much less than conventional svms , while often using fewer support vectors . on many benchmark datasets , the number of support vectors is less than one-tenth the number used by svms , indicating that the mcm does indeed learn simpler representations .", "topics": ["support vector machine", "sparse matrix"]}
{"title": "higher-order linear logic programming of categorial deduction", "abstract": "we show how categorial deduction can be implemented in higher-order ( linear ) logic programming , thereby realising parsing as deduction for the associative and non-associative lambek calculi . this provides a method of solution to the parsing problem of lambek categorial grammar applicable to a variety of its extensions .", "topics": ["natural language processing", "parsing"]}
{"title": "sparse modeling for image and vision processing", "abstract": "in recent years , a large amount of multi-disciplinary research has been conducted on sparse models and their applications . in statistics and machine learning , the sparsity principle is used to perform model selection -- -that is , automatically selecting a simple model among a large collection of them . in signal processing , sparse coding consists of representing data with linear combinations of a few dictionary elements . subsequently , the corresponding tools have been widely adopted by several scientific communities such as neuroscience , bioinformatics , or computer vision . the goal of this monograph is to offer a self-contained view of sparse modeling for visual recognition and image processing . more specifically , we focus on applications where the dictionary is learned and adapted to data , yielding a compact representation that has been successful in various contexts .", "topics": ["image processing", "computer vision"]}
{"title": "an identification system using eye detection based on wavelets and neural networks", "abstract": "the randomness and uniqueness of human eye patterns is a major breakthrough in the search for quicker , easier and highly reliable forms of automatic human identification . it is being used extensively in security solutions . this includes access control to physical facilities , security systems and information databases , suspect tracking , surveillance and intrusion detection and by various intelligence agencies through out the world . we use the advantage of human eye uniqueness to identify people and approve its validity as a biometric . . eye detection involves first extracting the eye from a digital face image , and then encoding the unique patterns of the eye in such a way that they can be compared with pre-registered eye patterns . the eye detection system consists of an automatic segmentation system that is based on the wavelet transform , and then the wavelet analysis is used as a pre-processor for a back propagation neural network with conjugate gradient learning . the inputs to the neural network are the wavelet maxima neighborhood coefficients of face images at a particular scale . the output of the neural network is the classification of the input into an eye or non-eye region . an accuracy of 90 % is observed for identifying test images under different conditions included in training stage .", "topics": ["neural networks", "database"]}
{"title": "clicr : a dataset of clinical case reports for machine reading comprehension", "abstract": "we present a new dataset for machine comprehension in the medical domain . our dataset uses clinical case reports with around 100,000 gap-filling queries about these cases . we apply several baselines and state-of-the-art neural readers to the dataset , and observe a considerable gap in performance ( 20 % f1 ) between the best human and machine readers . we analyze the skills required for successful answering and show how reader performance varies depending on the applicable skills . we find that inferences using domain knowledge and object tracking are the most frequently required skills , and that recognizing omitted information and spatio-temporal reasoning are the most difficult for the machines .", "topics": ["baseline ( configuration management )"]}
{"title": "large margin nearest neighbor embedding for knowledge representation", "abstract": "traditional way of storing facts in triplets ( { \\it head\\_entity , relation , tail\\_entity } ) , abbreviated as ( { \\it h , r , t } ) , makes the knowledge intuitively displayed and easily acquired by mankind , but hardly computed or even reasoned by ai machines . inspired by the success in applying { \\it distributed representations } to ai-related fields , recent studies expect to represent each entity and relation with a unique low-dimensional embedding , which is different from the symbolic and atomic framework of displaying knowledge in triplets . in this way , the knowledge computing and reasoning can be essentially facilitated by means of a simple { \\it vector calculation } , i.e . $ { \\bf h } + { \\bf r } \\approx { \\bf t } $ . we thus contribute an effective model to learn better embeddings satisfying the formula by pulling the positive tail entities $ { \\bf t^ { + } } $ to get together and close to { \\bf h } + { \\bf r } ( { \\it nearest neighbor } ) , and simultaneously pushing the negatives $ { \\bf t^ { - } } $ away from the positives $ { \\bf t^ { + } } $ via keeping a { \\it large margin } . we also design a corresponding learning algorithm to efficiently find the optimal solution based on { \\it stochastic gradient descent } in iterative fashion . quantitative experiments illustrate that our approach can achieve the state-of-the-art performance , compared with several latest methods on some benchmark datasets for two classical applications , i.e . { \\it link prediction } and { \\it triplet classification } . moreover , we analyze the parameter complexities among all the evaluated models , and analytical results indicate that our model needs fewer computational resources on outperforming the other methods .", "topics": ["optimization problem", "gradient descent"]}
{"title": "a new approach to probabilistic programming inference", "abstract": "we introduce and demonstrate a new approach to inference in expressive probabilistic programming languages based on particle markov chain monte carlo . our approach is simple to implement and easy to parallelize . it applies to turing-complete probabilistic programming languages and supports accurate inference in models that make use of complex control flow , including stochastic recursion . it also includes primitives from bayesian nonparametric statistics . our experiments show that this approach can be more efficient than previously introduced single-site metropolis-hastings methods .", "topics": ["bayesian network", "markov chain"]}
{"title": "learning from simulated and unsupervised images through adversarial training", "abstract": "with recent progress in graphics , it has become more tractable to train models on synthetic images , potentially avoiding the need for expensive annotations . however , learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions . to reduce this gap , we propose simulated+unsupervised ( s+u ) learning , where the task is to learn a model to improve the realism of a simulator 's output using unlabeled real data , while preserving the annotation information from the simulator . we develop a method for s+u learning that uses an adversarial network similar to generative adversarial networks ( gans ) , but with synthetic images as inputs instead of random vectors . we make several key modifications to the standard gan algorithm to preserve annotations , avoid artifacts , and stabilize training : ( i ) a 'self-regularization ' term , ( ii ) a local adversarial loss , and ( iii ) updating the discriminator using a history of refined images . we show that this enables generation of highly realistic images , which we demonstrate both qualitatively and with a user study . we quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation . we show a significant improvement over using synthetic images , and achieve state-of-the-art results on the mpiigaze dataset without any labeled real data .", "topics": ["synthetic data", "simulation"]}
{"title": "how many faces can be recognized ? performance extrapolation for multi-class classification", "abstract": "the difficulty of multi-class classification generally increases with the number of classes . using data from a subset of the classes , can we predict how well a classifier will scale with an increased number of classes ? under the assumption that the classes are sampled exchangeably , and under the assumption that the classifier is generative ( e.g . qda or naive bayes ) , we show that the expected accuracy when the classifier is trained on $ k $ classes is the $ k-1 $ st moment of a \\emph { conditional accuracy distribution } , which can be estimated from data . this provides the theoretical foundation for performance extrapolation based on pseudolikelihood , unbiased estimation , and high-dimensional asymptotics . we investigate the robustness of our methods to non-generative classifiers in simulations and one optical character recognition example .", "topics": ["sampling ( signal processing )", "statistical classification"]}
{"title": "learning to diagnose from scratch by exploiting dependencies among labels", "abstract": "the field of medical diagnostics contains a wealth of challenges which closely resemble classical machine learning problems ; practical constraints , however , complicate the translation of these endpoints naively into classical architectures . many tasks in radiology , for example , are largely problems of multi-label classification wherein medical images are interpreted to indicate multiple present or suspected pathologies . clinical settings drive the necessity for high accuracy simultaneously across a multitude of pathological outcomes and greatly limit the utility of tools which consider only a subset . this issue is exacerbated by a general scarcity of training data and maximizes the need to extract clinically relevant features from available samples -- ideally without the use of pre-trained models which may carry forward undesirable biases from tangentially related tasks . we present and evaluate a partial solution to these constraints in using lstms to leverage interdependencies among target labels in predicting 14 pathologic patterns from chest x-rays and establish state of the art results on the largest publicly available chest x-ray dataset from the nih without pre-training . furthermore , we propose and discuss alternative evaluation metrics and their relevance in clinical practice .", "topics": ["test set", "relevance"]}
{"title": "the jump set under geometric regularisation . part 2 : higher-order approaches", "abstract": "in part 1 , we developed a new technique based on lipschitz pushforwards for proving the jump set containment property $ \\mathcal { h } ^ { m-1 } ( j_u \\setminus j_f ) =0 $ of solutions $ u $ to total variation denoising . we demonstrated that the technique also applies to huber-regularised tv . now , in this part 2 , we extend the technique to higher-order regularisers . we are not quite able to prove the property for total generalised variation ( tgv ) based on the symmetrised gradient for the second-order term . we show that the property holds under three conditions : first , the solution $ u $ is locally bounded . second , the second-order variable is of locally bounded variation , $ w \\in \\mbox { bv } _\\mbox { loc } ( \\omega ; \\mathbb { r } ^m ) $ , instead of just bounded deformation , $ w \\in \\mbox { bd } ( \\omega ) $ . third , $ w $ does not jump on $ j_u $ parallel to it . the second condition can be achieved for non-symmetric tgv . both the second and third condition can be achieved if we change the radon ( or $ l^1 $ ) norm of the symmetrised gradient $ ew $ into an $ l^p $ norm , $ p > 1 $ , in which case korn 's inequality holds . we also consider the application of the technique to infimal convolution tv , and study the limiting behaviour of the singular part of $ d u $ , as the second parameter of $ \\mbox { tgv } ^2 $ goes to zero . unsurprisingly , it vanishes , but in numerical discretisations the situation looks quite different . finally , our work additionally includes a result on tgv-strict approximation in $ \\mbox { bv } ( \\omega ) $ .", "topics": ["noise reduction", "numerical analysis"]}
{"title": "learning multi-level region consistency with dense multi-label networks for semantic segmentation", "abstract": "semantic image segmentation is a fundamental task in image understanding . per-pixel semantic labelling of an image benefits greatly from the ability to consider region consistency both locally and globally . however , many fully convolutional network based methods do not impose such consistency , which may give rise to noisy and implausible predictions . we address this issue by proposing a dense multi-label network module that is able to encourage the region consistency at different levels . this simple but effective module can be easily integrated into any semantic segmentation systems . with comprehensive experiments , we show that the dense multi-label can successfully remove the implausible labels and clear the confusion so as to boost the performance of semantic segmentation systems .", "topics": ["image segmentation", "computer vision"]}
{"title": "using gaussian processes for rumour stance classification in social media", "abstract": "social media tend to be rife with rumours while new reports are released piecemeal during breaking news . interestingly , one can mine multiple reactions expressed by social media users in those situations , exploring their stance towards rumours , ultimately enabling the flagging of highly disputed rumours as being potentially false . in this work , we set out to develop an automated , supervised classifier that uses multi-task learning to classify the stance expressed in each individual tweet in a rumourous conversation as either supporting , denying or questioning the rumour . using a classifier based on gaussian processes , and exploring its effectiveness on two datasets with very different characteristics and varying distributions of stances , we show that our approach consistently outperforms competitive baseline classifiers . our classifier is especially effective in estimating the distribution of different types of stance associated with a given rumour , which we set forth as a desired characteristic for a rumour-tracking system that will warn both ordinary users of twitter and professional news practitioners when a rumour is being rebutted .", "topics": ["baseline ( configuration management )", "supervised learning"]}
{"title": "revisiting deep intrinsic image decompositions", "abstract": "while invaluable for many computer vision applications , decomposing a natural image into intrinsic reflectance and shading layers represents a challenging , underdetermined inverse problem . as opposed to strict reliance on conventional optimization or filtering solutions with strong prior assumptions , deep learning-based approaches have also been proposed to compute intrinsic image decompositions when granted access to sufficient labeled training data . the downside is that current data sources are quite limited , and broadly speaking fall into one of two categories : either dense fully-labeled images in synthetic/narrow settings , or weakly-labeled data from relatively diverse natural scenes . in contrast to many previous learning-based approaches , which are often tailored to the structure of a particular dataset ( and may not work well on others ) , we adopt core network structures that universally reflect loose prior knowledge regarding the intrinsic image formation process and can be largely shared across datasets . we then apply flexibly supervised loss layers that are customized for each source of ground truth labels . the resulting deep architecture achieves state-of-the-art results on all of the major intrinsic image benchmarks , and runs considerably faster than most at test time .", "topics": ["numerical analysis", "end-to-end principle"]}
{"title": "interpreting deep visual representations via network dissection", "abstract": "the success of recent deep convolutional neural networks ( cnns ) depends on learning hidden representations that can summarize the important factors of variation behind the data . however , cnns often criticized as being black boxes that lack interpretability , since they have millions of unexplained model parameters . in this work , we describe network dissection , a method that interprets networks by providing labels for the units of their deep visual representations . the proposed method quantifies the interpretability of cnn representations by evaluating the alignment between individual hidden units and a set of visual semantic concepts . by identifying the best alignments , units are given human interpretable labels across a range of objects , parts , scenes , textures , materials , and colors . the method reveals that deep representations are more transparent and interpretable than expected : we find that representations are significantly more interpretable than they would be under a random equivalently powerful basis . we apply the method to interpret and compare the latent representations of various network architectures trained to solve different supervised and self-supervised training tasks . we then examine factors affecting the network interpretability such as the number of the training iterations , regularizations , different initializations , and the network depth and width . finally we show that the interpreted units can be used to provide explicit explanations of a prediction given by a cnn for an image . our results highlight that interpretability is an important property of deep neural networks that provides new insights into their hierarchical structure .", "topics": ["iteration"]}
{"title": "boosting-like deep learning for pedestrian detection", "abstract": "this paper proposes boosting-like deep learning ( bdl ) framework for pedestrian detection . due to overtraining on the limited training samples , overfitting is a major problem of deep learning . we incorporate a boosting-like technique into deep learning to weigh the training samples , and thus prevent overtraining in the iterative process . we theoretically give the details of derivation of our algorithm , and report the experimental results on open data sets showing that bdl achieves a better stable performance than the state-of-the-arts . our approach achieves 15.85 % and 3.81 % reduction in the average miss rate compared with acf and jointdeep on the largest caltech benchmark dataset , respectively .", "topics": ["iteration"]}
{"title": "proper complex gaussian processes for regression", "abstract": "complex-valued signals are used in the modeling of many systems in engineering and science , hence being of fundamental interest . often , random complex-valued signals are considered to be proper . a proper complex random variable or process is uncorrelated with its complex conjugate . this assumption is a good model of the underlying physics in many problems , and simplifies the computations . while linear processing and neural networks have been widely studied for these signals , the development of complex-valued nonlinear kernel approaches remains an open problem . in this paper we propose gaussian processes for regression as a framework to develop 1 ) a solution for proper complex-valued kernel regression and 2 ) the design of the reproducing kernel for complex-valued inputs , using the convolutional approach for cross-covariances . in this design we pay attention to preserve , in the complex domain , the measure of similarity between near inputs . the hyperparameters of the kernel are learned maximizing the marginal likelihood using wirtinger derivatives . besides , the approach is connected to the multiple output learning scenario . in the experiments included , we first solve a proper complex gaussian process where the cross-covariance does not cancel , a challenging scenario when dealing with proper complex signals . then we successfully use these novel results to solve some problems previously proposed in the literature as benchmarks , reporting a remarkable improvement in the estimation error .", "topics": ["kernel ( operating system )", "nonlinear system"]}
{"title": "contextual symmetries in probabilistic graphical models", "abstract": "an important approach for efficient inference in probabilistic graphical models exploits symmetries among objects in the domain . symmetric variables ( states ) are collapsed into meta-variables ( meta-states ) and inference algorithms are run over the lifted graphical model instead of the flat one . our paper extends existing definitions of symmetry by introducing the novel notion of contextual symmetry . two states that are not globally symmetric , can be contextually symmetric under some specific assignment to a subset of variables , referred to as the context variables . contextual symmetry subsumes previous symmetry definitions and can rep resent a large class of symmetries not representable earlier . we show how to compute contextual symmetries by reducing it to the problem of graph isomorphism . we extend previous work on exploiting symmetries in the mcmc framework to the case of contextual symmetries . our experiments on several domains of interest demonstrate that exploiting contextual symmetries can result in significant computational gains .", "topics": ["graphical model"]}
{"title": "gradient descent converges to minimizers", "abstract": "we show that gradient descent converges to a local minimizer , almost surely with random initialization . this is proved by applying the stable manifold theorem from dynamical systems theory .", "topics": ["gradient descent", "gradient"]}
{"title": "the role of cognitive modeling in achieving communicative intentions", "abstract": "a discourse planner for ( task-oriented ) dialogue must be able to make choices about whether relevant , but optional information ( for example , the `` satellites '' in an rst-based planner ) should be communicated . we claim that effective text planners must explicitly model aspects of the hearer 's cognitive state , such as what the hearer is attending to and what inferences the hearer can draw , in order to make these choices . we argue that a mere representation of the hearer 's knowledge is inadequate . we support this claim by ( 1 ) an analysis of naturally occurring dialogue , and ( 2 ) by simulating the generation of discourses in a situation in which we can vary the cognitive parameters of the hearer . our results show that modeling cognitive state can lead to more effective discourses ( measured with respect to a simple task ) .", "topics": ["simulation"]}
{"title": "on the necessity of mixed models : dynamical frustrations in the mind", "abstract": "in the present work we will present and analyze some basic processes at the local and global level in linguistic derivations that seem to go beyond the limits of markovian or turing-like computation , and require , in our opinion , a quantum processor . we will first present briefly the working hypothesis and then focus on the empirical domain . at the same time , we will argue that a model appealing to only one kind of computation ( be it quantum or not ) is necessarily insufficient , and thus both linear and non-linear formal models are to be invoked in order to pursue a fuller understanding of mental computations within a unified framework .", "topics": ["nonlinear system", "computation"]}
{"title": "calculating the similarity between words and sentences using a lexical database and corpus statistics", "abstract": "calculating the semantic similarity between sentences is a long dealt problem in the area of natural language processing . the semantic analysis field has a crucial role to play in the research related to the text analytics . the semantic similarity differs as the domain of operation differs . in this paper , we present a methodology which deals with this issue by incorporating semantic similarity and corpus statistics . to calculate the semantic similarity between words and sentences , the proposed method follows an edge-based approach using a lexical database . the methodology can be applied in a variety of domains . the methodology has been tested on both benchmark standards and mean human similarity dataset . when tested on these two datasets , it gives highest correlation value for both word and sentence similarity outperforming other similar models . for word similarity , we obtained pearson correlation coefficient of 0.8753 and for sentence similarity , the correlation obtained is 0.8794 .", "topics": ["natural language processing", "coefficient"]}
{"title": "latent sequence decompositions", "abstract": "we present the latent sequence decompositions ( lsd ) framework . lsd decomposes sequences with variable lengthed output units as a function of both the input sequence and the output sequence . we present a training algorithm which samples valid extensions and an approximate decoding algorithm . we experiment with the wall street journal speech recognition task . our lsd model achieves 12.9 % wer compared to a character baseline of 14.8 % wer . when combined with a convolutional network on the encoder , we achieve 9.6 % wer .", "topics": ["baseline ( configuration management )", "speech recognition"]}
{"title": "responsible autonomy", "abstract": "as intelligent systems are increasingly making decisions that directly affect society , perhaps the most important upcoming research direction in ai is to rethink the ethical implications of their actions . means are needed to integrate moral , societal and legal values with technological developments in ai , both during the design process as well as part of the deliberation algorithms employed by these systems . in this paper , we describe leading ethics theories and propose alternative ways to ensure ethical behavior by artificial systems . given that ethics are dependent on the socio-cultural context and are often only implicit in deliberation processes , methodologies are needed to elicit the values held by designers and stakeholders , and to make these explicit leading to better understanding and trust on artificial autonomous systems .", "topics": ["value ( ethics )", "artificial intelligence"]}
{"title": "efficient phase retrieval based on dark fringe recognition with an ability of bypassing invalid fringes", "abstract": "this paper discusses the noisy phase retrieval problem : recovering a complex image signal with independent noise from quadratic measurements . inspired by the dark fringes shown in the measured images of the array detector , a novel phase retrieval approach is proposed and demonstrated both theoretically and experimentally to recognize the dark fringes and bypass the invalid fringes . a more accurate relative phase ratio between arbitrary two pixels is achieved by calculating the multiplicative ratios ( or the sum of phase difference ) on the path between them . then the object phase image can be reconstructed precisely . our approach is a good choice for retrieving high-quality phase images from noisy signals and has many potential applications in the fields such as x-ray crystallography , diffractive imaging , and so on .", "topics": ["pixel"]}
{"title": "proceedings of the twenty-seventh conference on uncertainty in artificial intelligence ( 2011 )", "abstract": "this is the proceedings of the twenty-seventh conference on uncertainty in artificial intelligence , which was held in barcelona , spain , july 14 - 17 2011 .", "topics": ["artificial intelligence"]}
{"title": "further results on dissimilarity spaces for hyperspectral images rf-cbir", "abstract": "content-based image retrieval ( cbir ) systems are powerful search tools in image databases that have been little applied to hyperspectral images . relevance feedback ( rf ) is an iterative process that uses machine learning techniques and user 's feedback to improve the cbir systems performance . we pursued to expand previous research in hyperspectral cbir systems built on dissimilarity functions defined either on spectral and spatial features extracted by spectral unmixing techniques , or on dictionaries extracted by dictionary-based compressors . these dissimilarity functions were not suitable for direct application in common machine learning techniques . we propose to use a rf general approach based on dissimilarity spaces which is more appropriate for the application of machine learning algorithms to the hyperspectral rf-cbir . we validate the proposed rf method for hyperspectral cbir systems over a real hyperspectral dataset .", "topics": ["relevance", "database"]}
{"title": "locally adaptive factor processes for multivariate time series", "abstract": "in modeling multivariate time series , it is important to allow time-varying smoothness in the mean and covariance process . in particular , there may be certain time intervals exhibiting rapid changes and others in which changes are slow . if such time-varying smoothness is not accounted for , one can obtain misleading inferences and predictions , with over-smoothing across erratic time intervals and under-smoothing across times exhibiting slow variation . this can lead to mis-calibration of predictive intervals , which can be substantially too narrow or wide depending on the time . we propose a locally adaptive factor process for characterizing multivariate mean-covariance changes in continuous time , allowing locally varying smoothness in both the mean and covariance matrix . this process is constructed utilizing latent dictionary functions evolving in time through nested gaussian processes and linearly related to the observed data with a sparse mapping . using a differential equation representation , we bypass usual computational bottlenecks in obtaining mcmc and online algorithms for approximate bayesian inference . the performance is assessed in simulations and illustrated in a financial application .", "topics": ["time series", "approximation algorithm"]}
{"title": "visual question answering as a meta learning task", "abstract": "the predominant approach to visual question answering ( vqa ) demands that the model represents within its weights all of the information required to answer any question about any image . learning this information from any real training set seems unlikely , and representing it in a reasonable number of weights doubly so . we propose instead to approach vqa as a meta learning task , thus separating the question answering method from the information required . at test time , the method is provided with a support set of example questions/answers , over which it reasons to resolve the given question . the support set is not fixed and can be extended without retraining , thereby expanding the capabilities of the model . to exploit this dynamically provided information , we adapt a state-of-the-art vqa model with two techniques from the recent meta learning literature , namely prototypical networks and meta networks . experiments demonstrate the capability of the system to learn to produce completely novel answers ( i.e . never seen during training ) from examples provided at test time . in comparison to the existing state of the art , the proposed method produces qualitatively distinct results with higher recall of rare answers , and a better sample efficiency that allows training with little initial data . more importantly , it represents an important step towards vision-and-language methods that can learn and reason on-the-fly .", "topics": ["test set"]}
{"title": "combining knowledge- and corpus-based word-sense-disambiguation methods", "abstract": "in this paper we concentrate on the resolution of the lexical ambiguity that arises when a given word has several different meanings . this specific task is commonly referred to as word sense disambiguation ( wsd ) . the task of wsd consists of assigning the correct sense to words using an electronic dictionary as the source of word definitions . we present two wsd methods based on two main methodological approaches in this research area : a knowledge-based method and a corpus-based method . our hypothesis is that word-sense disambiguation requires several knowledge sources in order to solve the semantic ambiguity of the words . these sources can be of different kinds -- - for example , syntagmatic , paradigmatic or statistical information . our approach combines various sources of knowledge , through combinations of the two wsd methods mentioned above . mainly , the paper concentrates on how to combine these methods and sources of information in order to achieve good results in the disambiguation . finally , this paper presents a comprehensive study and experimental work on evaluation of the methods and their combinations .", "topics": ["dictionary"]}
{"title": "on backtracking in real-time heuristic search", "abstract": "real-time heuristic search algorithms are suitable for situated agents that need to make their decisions in constant time . since the original work by korf nearly two decades ago , numerous extensions have been suggested . one of the most intriguing extensions is the idea of backtracking wherein the agent decides to return to a previously visited state as opposed to moving forward greedily . this idea has been empirically shown to have a significant impact on various performance measures . the studies have been carried out in particular empirical testbeds with specific real-time search algorithms that use backtracking . consequently , the extent to which the trends observed are characteristic of backtracking in general is unclear . in this paper , we present the first entirely theoretical study of backtracking in real-time heuristic search . in particular , we present upper bounds on the solution cost exponential and linear in a parameter regulating the amount of backtracking . the results hold for a wide class of real-time heuristic search algorithms that includes many existing algorithms as a small subclass .", "topics": ["time complexity", "heuristic"]}
{"title": "improving sentiment analysis in arabic using word representation", "abstract": "the complexities of arabic language in morphology , orthography and dialects makes sentiment analysis for arabic more challenging . also , text feature extraction from short messages like tweets , in order to gauge the sentiment , makes this task even more difficult . in recent years , deep neural networks were often employed and showed very good results in sentiment classification and natural language processing applications . word embedding , or word distributing approach , is a current and powerful tool to capture together the closest words from a contextual text . in this paper , we describe how we construct word2vec models from a large arabic corpus obtained from ten newspapers in different arab countries . by applying different machine learning algorithms and convolutional neural networks with different text feature selections , we report improved accuracy of sentiment classification ( 91 % -95 % ) on our publicly available arabic language health sentiment dataset [ 1 ] . keywords - arabic sentiment analysis , machine learning , convolutional neural networks , word embedding , word2vec for arabic , lexicon .", "topics": ["natural language processing", "feature extraction"]}
{"title": "structure learning in bayesian networks of moderate size by efficient sampling", "abstract": "we study the bayesian model averaging approach to learning bayesian network structures ( dags ) from data . we develop new algorithms including the first algorithm that is able to efficiently sample dags according to the exact structure posterior . the dag samples can then be used to construct estimators for the posterior of any feature . we theoretically prove good properties of our estimators and empirically show that our estimators considerably outperform the estimators from the previous state-of-the-art methods .", "topics": ["bayesian network"]}
{"title": "learning deep context-network architectures for image annotation", "abstract": "context plays an important role in visual pattern recognition as it provides complementary clues for different learning tasks including image classification and annotation . in the particular scenario of kernel learning , the general recipe of context-based kernel design consists in learning positive semi-definite similarity functions that return high values not only when data share similar content but also similar context . however , in spite of having a positive impact on performance , the use of context in these kernel design methods has not been fully explored ; indeed , context has been handcrafted instead of being learned . in this paper , we introduce a novel context-aware kernel design framework based on deep learning . our method discriminatively learns spatial geometric context as the weights of a deep network ( dn ) . the architecture of this network is fully determined by the solution of an objective function that mixes content , context and regularization , while the parameters of this network determine the most relevant ( discriminant ) parts of the learned context . we apply this context and kernel learning framework to image classification using the challenging imageclef photo annotation benchmark ; the latter shows that our deep context learning provides highly effective kernels for image classification as corroborated through extensive experiments .", "topics": ["kernel ( operating system )", "computer vision"]}
{"title": "determining the characteristic vocabulary for a specialized dictionary using word2vec and a directed crawler", "abstract": "specialized dictionaries are used to understand concepts in specific domains , especially where those concepts are not part of the general vocabulary , or having meanings that differ from ordinary languages . the first step in creating a specialized dictionary involves detecting the characteristic vocabulary of the domain in question . classical methods for detecting this vocabulary involve gathering a domain corpus , calculating statistics on the terms found there , and then comparing these statistics to a background or general language corpus . terms which are found significantly more often in the specialized corpus than in the background corpus are candidates for the characteristic vocabulary of the domain . here we present two tools , a directed crawler , and a distributional semantics package , that can be used together , circumventing the need of a background corpus . both tools are available on the web .", "topics": ["text corpus", "dictionary"]}
{"title": "visual servoing from deep neural networks", "abstract": "we present a deep neural network-based method to perform high-precision , robust and real-time 6 dof visual servoing . the paper describes how to create a dataset simulating various perturbations ( occlusions and lighting conditions ) from a single real-world image of the scene . a convolutional neural network is fine-tuned using this dataset to estimate the relative pose between two images of the same scene . the output of the network is then employed in a visual servoing control scheme . the method converges robustly even in difficult real-world settings with strong lighting variations and occlusions.a positioning error of less than one millimeter is obtained in experiments with a 6 dof robot .", "topics": ["neural networks", "simulation"]}
{"title": "shaping proto-value functions via rewards", "abstract": "in this paper , we combine task-dependent reward shaping and task-independent proto-value functions to obtain reward dependent proto-value functions ( rpvfs ) . in constructing the rpvfs we are making use of the immediate rewards which are available during the sampling phase but are not used in the pvf construction . we show via experiments that learning with an rpvf based representation is better than learning with just reward shaping or pvfs . in particular , when the state space is symmetrical and the rewards are asymmetrical , the rpvf capture the asymmetry better than the pvfs .", "topics": ["sampling ( signal processing )", "supervised learning"]}
{"title": "collaborative discriminant locality preserving projections with its application to face recognition", "abstract": "we present a novel discriminant locality preserving projections ( dlpp ) algorithm named collaborative discriminant locality preserving projection ( cdlpp ) . in our algorithm , the discriminating power of dlpp are further exploited from two aspects . on the one hand , the global optimum of class scattering is guaranteed via using the between-class scatter matrix to replace the original denominator of dlpp . on the other hand , motivated by collaborative representation , an $ l_2 $ -norm constraint is imposed to the projections to discover the collaborations of dimensions in the sample space . we apply our algorithm to face recognition . three popular face databases , namely ar , orl and lfw-a , are employed for evaluating the performance of cdlpp . extensive experimental results demonstrate that cdlpp significantly improves the discriminating power of dlpp and outperforms the state-of-the-arts .", "topics": ["mathematical optimization", "database"]}
{"title": "vectorial dimension reduction for tensors based on bayesian inference", "abstract": "dimensionality reduction for high-order tensors is a challenging problem . in conventional approaches , higher order tensors are `vectorized` via tucker decomposition to obtain lower order tensors . this will destroy the inherent high-order structures or resulting in undesired tensors , respectively . this paper introduces a probabilistic vectorial dimensionality reduction model for tensorial data . the model represents a tensor by employing a linear combination of same order basis tensors , thus it offers a mechanism to directly reduce a tensor to a vector . under this expression , the projection base of the model is based on the tensor candecomp/parafac ( cp ) decomposition and the number of free parameters in the model only grows linearly with the number of modes rather than exponentially . a bayesian inference has been established via the variational em approach . a criterion to set the parameters ( factor number of cp decomposition and the number of extracted features ) is empirically given . the model outperforms several existing pca-based methods and cp decomposition on several publicly available databases in terms of classification and clustering accuracy .", "topics": ["calculus of variations", "cluster analysis"]}
{"title": "on the equivalence between herding and conditional gradient algorithms", "abstract": "we show that the herding procedure of welling ( 2009 ) takes exactly the form of a standard convex optimization algorithm -- namely a conditional gradient algorithm minimizing a quadratic moment discrepancy . this link enables us to invoke convergence results from convex optimization and to consider faster alternatives for the task of approximating integrals in a reproducing kernel hilbert space . we study the behavior of the different variants through numerical simulations . the experiments indicate that while we can improve over herding on the task of approximating integrals , the original herding algorithm tends to approach more often the maximum entropy distribution , shedding more light on the learning bias behind herding .", "topics": ["approximation algorithm", "numerical analysis"]}
{"title": "new edge detection technique based on the shannon entropy in gray level images", "abstract": "edge detection is an important field in image processing . edges characterize object boundaries and are therefore useful for segmentation , registration , feature extraction , and identification of objects in a scene . in this paper , an approach utilizing an improvement of baljit and amar method which uses shannon entropy other than the evaluation of derivatives of the image in detecting edges in gray level images has been proposed . the proposed method can reduce the cpu time required for the edge detection process and the quality of the edge detector of the output images is robust . a standard test images , the real-world and synthetic images are used to compare the results of the proposed edge detector with the baljit and amar edge detector method . in order to validate the results , the run time of the proposed method and the pervious method are presented . it has been observed that the proposed edge detector works effectively for different gray scale digital images . the performance evaluation of the proposed technique in terms of the measured cpu time and the quality of edge detector method are presented . experimental results demonstrate that the proposed method achieve better result than the relevant classic method .", "topics": ["image processing", "image segmentation"]}
{"title": "data mining for actionable knowledge : a survey", "abstract": "the data mining process consists of a series of steps ranging from data cleaning , data selection and transformation , to pattern evaluation and visualization . one of the central problems in data mining is to make the mined patterns or knowledge actionable . here , the term actionable refers to the mined patterns suggest concrete and profitable actions to the decision-maker . that is , the user can do something to bring direct benefits ( increase in profits , reduction in cost , improvement in efficiency , etc . ) to the organization 's advantage . however , there has been written no comprehensive survey available on this topic . the goal of this paper is to fill the void . in this paper , we first present two frameworks for mining actionable knowledge that are inexplicitly adopted by existing research methods . then we try to situate some of the research on this topic from two different viewpoints : 1 ) data mining tasks and 2 ) adopted framework . finally , we specify issues that are either not addressed or insufficiently studied yet and conclude the paper .", "topics": ["data mining"]}
{"title": "blending lstms into cnns", "abstract": "we consider whether deep convolutional networks ( cnns ) can represent decision functions with similar accuracy as recurrent networks such as lstms . first , we show that a deep cnn with an architecture inspired by the models recently introduced in image recognition can yield better accuracy than previous convolutional and lstm networks on the standard 309h switchboard automatic speech recognition task . then we show that even more accurate cnns can be trained under the guidance of lstms using a variant of model compression , which we call model blending because the teacher and student models are similar in complexity but different in inductive bias . blending further improves the accuracy of our cnn , yielding a computationally efficient model of accuracy higher than any of the other individual models . examining the effect of `` dark knowledge '' in this model compression task , we find that less than 1 % of the highest probability labels are needed for accurate model compression .", "topics": ["computer vision", "speech recognition"]}
{"title": "hierarchical imitation and reinforcement learning", "abstract": "we study the problem of learning policies over long time horizons . we present a framework that leverages and integrates two key concepts . first , we utilize hierarchical policy classes that enable planning over different time scales , i.e . , the high level planner proposes a sequence of subgoals for the low level planner to achieve . second , we utilize expert demonstrations within the hierarchical action space to dramatically reduce cost of exploration . our framework is flexible and can incorporate different combinations of imitation learning ( il ) and reinforcement learning ( rl ) at different levels of the hierarchy . using long-horizon benchmarks , including montezuma 's revenge , we empirically demonstrate that our approach can learn significantly faster compared to hierarchical rl , and can be significantly more label- and sample-efficient compared to flat il . we also provide theoretical analysis of the labeling cost for certain instantiations of our framework .", "topics": ["high- and low-level", "reinforcement learning"]}
{"title": "quantifying and reducing stereotypes in word embeddings", "abstract": "machine learning algorithms are optimized to model statistical properties of the training data . if the input data reflects stereotypes and biases of the broader society , then the output of the learning algorithm also captures these stereotypes . in this paper , we initiate the study of gender stereotypes in { \\em word embedding } , a popular framework to represent text data . as their use becomes increasingly common , applications can inadvertently amplify unwanted stereotypes . we show across multiple datasets that the embeddings contain significant gender stereotypes , especially with regard to professions . we created a novel gender analogy task and combined it with crowdsourcing to systematically quantify the gender bias in a given embedding . we developed an efficient algorithm that reduces gender stereotype using just a handful of training examples while preserving the useful geometric properties of the embedding . we evaluated our algorithm on several metrics . while we focus on male/female stereotypes , our framework may be applicable to other types of embedding biases .", "topics": ["test set", "text corpus"]}
{"title": "efficient pairwise learning using kernel ridge regression : an exact two-step method", "abstract": "pairwise learning or dyadic prediction concerns the prediction of properties for pairs of objects . it can be seen as an umbrella covering various machine learning problems such as matrix completion , collaborative filtering , multi-task learning , transfer learning , network prediction and zero-shot learning . in this work we analyze kernel-based methods for pairwise learning , with a particular focus on a recently-suggested two-step method . we show that this method offers an appealing alternative for commonly-applied kronecker-based methods that model dyads by means of pairwise feature representations and pairwise kernels . in a series of theoretical results , we establish correspondences between the two types of methods in terms of linear algebra and spectral filtering , and we analyze their statistical consistency . in addition , the two-step method allows us to establish novel algorithmic shortcuts for efficient training and validation on very large datasets . putting those properties together , we believe that this simple , yet powerful method can become a standard tool for many problems . extensive experimental results for a range of practical settings are reported .", "topics": ["kernel ( operating system )"]}
{"title": "partial optimality by pruning for map-inference with general graphical models", "abstract": "we consider the energy minimization problem for undirected graphical models , also known as map-inference problem for markov random fields which is np-hard in general . we propose a novel polynomial time algorithm to obtain a part of its optimal non-relaxed integral solution . our algorithm is initialized with variables taking integral values in the solution of a convex relaxation of the map-inference problem and iteratively prunes those , which do not satisfy our criterion for partial optimality . we show that our pruning strategy is in a certain sense theoretically optimal . also empirically our method outperforms previous approaches in terms of the number of persistently labelled variables . the method is very general , as it is applicable to models with arbitrary factors of an arbitrary order and can employ any solver for the considered relaxed problem . our method 's runtime is determined by the runtime of the convex relaxation solver for the map-inference problem .", "topics": ["graphical model", "time complexity"]}
{"title": "a statistical perspective on randomized sketching for ordinary least-squares", "abstract": "we consider statistical as well as algorithmic aspects of solving large-scale least-squares ( ls ) problems using randomized sketching algorithms . for a ls problem with input data $ ( x , y ) \\in \\mathbb { r } ^ { n \\times p } \\times \\mathbb { r } ^n $ , sketching algorithms use a sketching matrix , $ s\\in\\mathbb { r } ^ { r \\times n } $ with $ r \\ll n $ . then , rather than solving the ls problem using the full data $ ( x , y ) $ , sketching algorithms solve the ls problem using only the sketched data $ ( sx , sy ) $ . prior work has typically adopted an algorithmic perspective , in that it has made no statistical assumptions on the input $ x $ and $ y $ , and instead it has been assumed that the data $ ( x , y ) $ are fixed and worst-case ( wc ) . prior results show that , when using sketching matrices such as random projections and leverage-score sampling algorithms , with $ p < r \\ll n $ , the wc error is the same as solving the original problem , up to a small constant . from a statistical perspective , we typically consider the mean-squared error performance of randomized sketching algorithms , when data $ ( x , y ) $ are generated according to a statistical model $ y = x \\beta + \\epsilon $ , where $ \\epsilon $ is a noise process . we provide a rigorous comparison of both perspectives leading to insights on how they differ . to do this , we first develop a framework for assessing algorithmic and statistical aspects of randomized sketching methods . we then consider the statistical prediction efficiency ( pe ) and the statistical residual efficiency ( re ) of the sketched ls estimator ; and we use our framework to provide upper bounds for several types of random projection and random sampling sketching algorithms . among other results , we show that the re can be upper bounded when $ p < r \\ll n $ while the pe typically requires the sample size $ r $ to be substantially larger . lower bounds developed in subsequent results show that our upper bounds on pe can not be improved .", "topics": ["sampling ( signal processing )"]}
{"title": "ultimate intelligence part ii : physical measure and complexity of intelligence", "abstract": "we continue our analysis of volume and energy measures that are appropriate for quantifying inductive inference systems . we extend logical depth and conceptual jump size measures in ait to stochastic problems , and physical measures that involve volume and energy . we introduce a graphical model of computational complexity that we believe to be appropriate for intelligent machines . we show several asymptotic relations between energy , logical depth and volume of computation for inductive inference . in particular , we arrive at a `` black-hole equation '' of inductive inference , which relates energy , volume , space , and algorithmic information for an optimal inductive inference solution . we introduce energy-bounded algorithmic entropy . we briefly apply our ideas to the physical limits of intelligent computation in our universe .", "topics": ["computational complexity theory", "graphical model"]}
{"title": "balanced k-means and min-cut clustering", "abstract": "clustering is an effective technique in data mining to generate groups that are the matter of interest . among various clustering approaches , the family of k-means algorithms and min-cut algorithms gain most popularity due to their simplicity and efficacy . the classical k-means algorithm partitions a number of data points into several subsets by iteratively updating the clustering centers and the associated data points . by contrast , a weighted undirected graph is constructed in min-cut algorithms which partition the vertices of the graph into two sets . however , existing clustering algorithms tend to cluster minority of data points into a subset , which shall be avoided when the target dataset is balanced . to achieve more accurate clustering for balanced dataset , we propose to leverage exclusive lasso on k-means and min-cut to regulate the balance degree of the clustering results . by optimizing our objective functions that build atop the exclusive lasso , we can make the clustering result as much balanced as possible . extensive experiments on several large-scale datasets validate the advantage of the proposed algorithms compared to the state-of-the-art clustering algorithms .", "topics": ["cluster analysis", "data mining"]}
{"title": "measuring thematic fit with distributional feature overlap", "abstract": "in this paper , we introduce a new distributional method for modeling predicate-argument thematic fit judgments . we use a syntax-based dsm to build a prototypical representation of verb-specific roles : for every verb , we extract the most salient second order contexts for each of its roles ( i.e . the most salient dimensions of typical role fillers ) , and then we compute thematic fit as a weighted overlap between the top features of candidate fillers and role prototypes . our experiments show that our method consistently outperforms a baseline re-implementing a state-of-the-art system , and achieves better or comparable results to those reported in the literature for the other unsupervised systems . moreover , it provides an explicit representation of the features characterizing verb-specific semantic roles .", "topics": ["baseline ( configuration management )"]}
{"title": "active transfer learning with zero-shot priors : reusing past datasets for future tasks", "abstract": "how can we reuse existing knowledge , in the form of available datasets , when solving a new and apparently unrelated target task from a set of unlabeled data ? in this work we make a first contribution to answer this question in the context of image classification . we frame this quest as an active learning problem and use zero-shot classifiers to guide the learning process by linking the new task to the existing classifiers . by revisiting the dual formulation of adaptive svm , we reveal two basic conditions to choose greedily only the most relevant samples to be annotated . on this basis we propose an effective active learning algorithm which learns the best possible target classification model with minimum human labeling effort . extensive experiments on two challenging datasets show the value of our approach compared to the state-of-the-art active learning methodologies , as well as its potential to reuse past datasets with minimal effort for future tasks .", "topics": ["computer vision"]}
{"title": "joint dictionary learning for example-based image super-resolution", "abstract": "in this paper , we propose a new joint dictionary learning method for example-based image super-resolution ( sr ) , using sparse representation . the low-resolution ( lr ) dictionary is trained from a set of lr sample image patches . using the sparse representation coefficients of these lr patches over the lr dictionary , the high-resolution ( hr ) dictionary is trained by minimizing the reconstruction error of hr sample patches . the error criterion used here is the mean square error . in this way we guarantee that the hr patches have the same sparse representation over hr dictionary as the lr patches over the lr dictionary , and at the same time , these sparse representations can well reconstruct the hr patches . simulation results show the effectiveness of our method compared to the state-of-art sr algorithms .", "topics": ["sparse matrix", "simulation"]}
{"title": "real-time background subtraction using adaptive sampling and cascade of gaussians", "abstract": "background-foreground classification is a fundamental well-studied problem in computer vision . due to the pixel-wise nature of modeling and processing in the algorithm , it is usually difficult to satisfy real-time constraints . there is a trade-off between the speed ( because of model complexity ) and accuracy . inspired by the rejection cascade of viola-jones classifier , we decompose the gaussian mixture model ( gmm ) into an adaptive cascade of classifiers . this way we achieve a good improvement in speed without compensating for accuracy . in the training phase , we learn multiple kdes for different durations to be used as strong prior distribution and detect probable oscillating pixels which usually results in misclassifications . we propose a confidence measure for the classifier based on temporal consistency and the prior distribution . the confidence measure thus derived is used to adapt the learning rate and the thresholds of the model , to improve accuracy . the confidence measure is also employed to perform temporal and spatial sampling in a principled way . we demonstrate a speed-up factor of 5x to 10x and 17 percent average improvement in accuracy over several standard videos .", "topics": ["sampling ( signal processing )", "computer vision"]}
{"title": "improved tdnns using deep kernels and frequency dependent grid-rnns", "abstract": "time delay neural networks ( tdnns ) are an effective acoustic model for large vocabulary speech recognition . the strength of the model can be attributed to its ability to effectively model long temporal contexts . however , current tdnn models are relatively shallow , which limits the modelling capability . this paper proposes a method of increasing the network depth by deepening the kernel used in the tdnn temporal convolutions . the best performing kernel consists of three fully connected layers with a residual ( resnet ) connection from the output of the first to the output of the third . the addition of spectro-temporal processing as the input to the tdnn in the form of a convolutional neural network ( cnn ) and a newly designed grid-rnn was investigated . the grid-rnn strongly outperforms a cnn if different sets of parameters for different frequency bands are used and can be further enhanced by using a bi-directional grid-rnn . experiments using the multi-genre broadcast ( mgb3 ) english data ( 275h ) show that deep kernel tdnns reduces the word error rate ( wer ) by 6 % relative and when combined with the frequency dependent grid-rnn gives a relative wer reduction of 9 % .", "topics": ["kernel ( operating system )", "speech recognition"]}
{"title": "learning to draw samples with amortized stein variational gradient descent", "abstract": "we propose a simple algorithm to train stochastic neural networks to draw samples from given target distributions for probabilistic inference . our method is based on iteratively adjusting the neural network parameters so that the output changes along a stein variational gradient direction ( liu & wang , 2016 ) that maximally decreases the kl divergence with the target distribution . our method works for any target distribution specified by their unnormalized density function , and can train any black-box architectures that are differentiable in terms of the parameters we want to adapt . we demonstrate our method with a number of applications , including variational autoencoder ( vae ) with expressive encoders to model complex latent space structures , and hyper-parameter learning of mcmc samplers that allows bayesian inference to adaptively improve itself when seeing more data .", "topics": ["calculus of variations", "gradient descent"]}
{"title": "soft goals can be compiled away", "abstract": "soft goals extend the classical model of planning with a simple model of preferences . the best plans are then not the ones with least cost but the ones with maximum utility , where the utility of a plan is the sum of the utilities of the soft goals achieved minus the plan cost . finding plans with high utility appears to involve two linked problems : choosing a subset of soft goals to achieve and finding a low-cost plan to achieve them . new search algorithms and heuristics have been developed for planning with soft goals , and a new track has been introduced in the international planning competition ( ipc ) to test their performance . in this note , we show however that these extensions are not needed : soft goals do not increase the expressive power of the basic model of planning with action costs , as they can easily be compiled away . we apply this compilation to the problems of the net-benefit track of the most recent ipc , and show that optimal and satisficing cost-based planners do better on the compiled problems than optimal and satisficing net-benefit planners on the original problems with explicit soft goals . furthermore , we show that penalties , or negative preferences expressing conditions to avoid , can also be compiled away using a similar idea .", "topics": ["heuristic"]}
{"title": "sparse recovery via partial regularization : models , theory and algorithms", "abstract": "in the context of sparse recovery , it is known that most of existing regularizers such as $ \\ell_1 $ suffer from some bias incurred by some leading entries ( in magnitude ) of the associated vector . to neutralize this bias , we propose a class of models with partial regularizers for recovering a sparse solution of a linear system . we show that every local minimizer of these models is sufficiently sparse or the magnitude of all its nonzero entries is above a uniform constant depending only on the data of the linear system . moreover , for a class of partial regularizers , any global minimizer of these models is a sparsest solution to the linear system . we also establish some sufficient conditions for local or global recovery of the sparsest solution to the linear system , among which one of the conditions is weaker than the best known restricted isometry property ( rip ) condition for sparse recovery by $ \\ell_1 $ . in addition , a first-order feasible augmented lagrangian ( fal ) method is proposed for solving these models , in which each subproblem is solved by a nonmonotone proximal gradient ( npg ) method . despite the complication of the partial regularizers , we show that each proximal subproblem in npg can be solved as a certain number of one-dimensional optimization problems , which usually have a closed-form solution . we also show that any accumulation point of the sequence generated by fal is a first-order stationary point of the models . numerical results on compressed sensing and sparse logistic regression demonstrate that the proposed models substantially outperform the widely used ones in the literature in terms of solution quality .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "semantic web search based on ontology modeling using protege reasoner", "abstract": "the semantic web works on the existing web which presents the meaning of information as well-defined vocabularies understood by the people . semantic search , at the same time , works on improving the accuracy if a search by understanding the intent of the search and providing contextually relevant results . this paper describes a semantic approach toward web search through a php application . the goal was to parse through a user 's browsing history and return semantically relevant web pages for the search query provided .", "topics": ["parsing"]}
{"title": "neural taylor approximations : convergence and exploration in rectifier networks", "abstract": "modern convolutional networks , incorporating rectifiers and max-pooling , are neither smooth nor convex . standard guarantees therefore do not apply . nevertheless , methods from convex optimization such as gradient descent and adam are widely used as building blocks for deep learning algorithms . this paper provides the first convergence guarantee applicable to modern convnets . the guarantee matches a lower bound for convex nonsmooth functions . the key technical tool is the neural taylor approximation -- a straightforward application of taylor expansions to neural networks -- and the associated taylor loss . experiments on a range of optimizers , layers , and tasks provide evidence that the analysis accurately captures the dynamics of neural optimization . the second half of the paper applies the taylor approximation to isolate the main difficulty in training rectifier nets : that gradients are shattered . we investigate the hypothesis that , by exploring the space of activation configurations more thoroughly , adaptive optimizers such as rmsprop and adam are able to converge to better solutions .", "topics": ["gradient descent", "approximation"]}
{"title": "sandwiching the marginal likelihood using bidirectional monte carlo", "abstract": "computing the marginal likelihood ( ml ) of a model requires marginalizing out all of the parameters and latent variables , a difficult high-dimensional summation or integration problem . to make matters worse , it is often hard to measure the accuracy of one 's ml estimates . we present bidirectional monte carlo , a technique for obtaining accurate log-ml estimates on data simulated from a model . this method obtains stochastic lower bounds on the log-ml using annealed importance sampling or sequential monte carlo , and obtains stochastic upper bounds by running these same algorithms in reverse starting from an exact posterior sample . the true value can be sandwiched between these two stochastic bounds with high probability . using the ground truth log-ml estimates obtained from our method , we quantitatively evaluate a wide variety of existing ml estimators on several latent variable models : clustering , a low rank approximation , and a binary attributes model . these experiments yield insights into how to accurately estimate marginal likelihoods .", "topics": ["sampling ( signal processing )", "cluster analysis"]}
{"title": "hierarchical convolutional-deconvolutional neural networks for automatic liver and tumor segmentation", "abstract": "automatic segmentation of liver and its tumors is an essential step for extracting quantitative imaging biomarkers for accurate tumor detection , diagnosis , prognosis and assessment of tumor response to treatment . miccai 2017 liver tumor segmentation challenge ( lits ) provides a common platform for comparing different automatic algorithms on contrast-enhanced abdominal ct images in tasks including 1 ) liver segmentation , 2 ) liver tumor segmentation , and 3 ) tumor burden estimation . we participate this challenge by developing a hierarchical framework based on deep fully convolutional-deconvolutional neural networks ( cdnn ) . a simple cdnn model is firstly trained to provide a quick but coarse segmentation of the liver on the entire ct volume , then another cdnn is applied to the liver region for fine liver segmentation . at last , the segmented liver region , which is enhanced by histogram equalization , is employed as an additional input to the third cdnn for tumor segmentation . jaccard distance is used as loss function when training cdnn models to eliminate the need of sample re-weighting . our framework is trained using the 130 challenge training cases provided by lits . the evaluation on the 70 challenge testing cases resulted in a mean dice similarity coefficient ( dsc ) of 0.963 for liver segmentation , a mean dsc of 0.657 for tumor segmentation , and a root mean square error ( rmse ) of 0.017 for tumor burden estimation , which ranked our method in the first , fifth and third place , respectively", "topics": ["loss function", "coefficient"]}
{"title": "regularized spherical polar fourier diffusion mri with optimal dictionary learning", "abstract": "compressed sensing ( cs ) takes advantage of signal sparsity or compressibility and allows superb signal reconstruction from relatively few measurements . based on cs theory , a suitable dictionary for sparse representation of the signal is required . in diffusion mri ( dmri ) , cs methods were proposed to reconstruct diffusion-weighted signal and the ensemble average propagator ( eap ) , and there are two kinds of dictionary learning ( dl ) methods : 1 ) discrete representation dl ( dr-dl ) , and 2 ) continuous representation dl ( cr-dl ) . dr-dl is susceptible to numerical inaccuracy owing to interpolation and regridding errors in a discretized q-space . in this paper , we propose a novel cr-dl approach , called dictionary learning - spherical polar fourier imaging ( dl-spfi ) for effective compressed-sensing reconstruction of the q-space diffusion-weighted signal and the eap . in dl-spfi , an dictionary that sparsifies the signal is learned from the space of continuous gaussian diffusion signals . the learned dictionary is then adaptively applied to different voxels using a weighted lasso framework for robust signal reconstruction . the adaptive dictionary is proved to be optimal . compared with the start-of-the-art cr-dl and dr-dl methods proposed by merlet et al . and bilgic et al . , espectively , our work offers the following advantages . first , the learned dictionary is proved to be optimal for gaussian diffusion signals . second , to our knowledge , this is the first work to learn a voxel-adaptive dictionary . the importance of the adaptive dictionary in eap reconstruction will be demonstrated theoretically and empirically . third , optimization in dl-spfi is only performed in a small subspace resided by the spf coefficients , as opposed to the q-space approach utilized by merlet et al . the experiment results demonstrate the advantages of dl-spfi over the original spf basis and bilgic et al . 's method .", "topics": ["numerical analysis", "synthetic data"]}
{"title": "structured set matching networks for one-shot part labeling", "abstract": "diagrams often depict complex phenomena and serve as a good test bed for visual and textual reasoning . however , understanding diagrams using natural image understanding approaches requires large training datasets of diagrams , which are very hard to obtain . instead , this can be addressed as a matching problem either between labeled diagrams , images or both . this problem is very challenging since the absence of significant color and texture renders local cues ambiguous and requires global reasoning . we consider the problem of one-shot part labeling : labeling multiple parts of an object in a target image given only a single source image of that category . for this set-to-set matching problem , we introduce the structured set matching network ( ssmn ) , a structured prediction model that incorporates convolutional neural networks . the ssmn is trained using global normalization to maximize local match scores between corresponding elements and a global consistency score among all matched elements , while also enforcing a matching constraint between the two sets . the ssmn significantly outperforms several strong baselines on three label transfer scenarios : diagram-to-diagram , evaluated on a new diagram dataset of over 200 categories ; image-to-image , evaluated on a dataset built on top of the pascal part dataset ; and image-to-diagram , evaluated on transferring labels across these datasets .", "topics": ["computer vision"]}
{"title": "characterizing driving styles with deep learning", "abstract": "characterizing driving styles of human drivers using vehicle sensor data , e.g . , gps , is an interesting research problem and an important real-world requirement from automotive industries . a good representation of driving features can be highly valuable for autonomous driving , auto insurance , and many other application scenarios . however , traditional methods mainly rely on handcrafted features , which limit machine learning algorithms to achieve a better performance . in this paper , we propose a novel deep learning solution to this problem , which could be the first attempt of extending deep learning to driving behavior analysis based on gps data . the proposed approach can effectively extract high level and interpretable features describing complex driving patterns . it also requires significantly less human experience and work . the power of the learned driving style representations are validated through the driver identification problem using a large real dataset .", "topics": ["autonomous car"]}
{"title": "indoor 3d video monitoring using multiple kinect depth-cameras", "abstract": "this article describes the design and development of a system for remote indoor 3d monitoring using an undetermined number of microsoft ( r ) kinect sensors . in the proposed client-server system , the kinect cameras can be connected to different computers , addressing this way the hardware limitation of one sensor per usb controller . the reason behind this limitation is the high bandwidth needed by the sensor , which becomes also an issue for the distributed system tcp/ip communications . since traffic volume is too high , 3d data has to be compressed before it can be sent over the network . the solution consists in selfcoding the kinect data into rgb images and then using a standard multimedia codec to compress color maps . information from different sources is collected into a central client computer , where point clouds are transformed to reconstruct the scene in 3d . an algorithm is proposed to merge the skeletons detected locally by each kinect conveniently , so that monitoring of people is robust to self and inter-user occlusions . final skeletons are labeled and trajectories of every joint can be saved for event reconstruction or further analysis .", "topics": ["map", "sensor"]}
{"title": "sampling matters in deep embedding learning", "abstract": "deep embeddings answer one simple question : how similar are two images ? learning these embeddings is the bedrock of verification , zero-shot learning , and visual search . the most prominent approaches optimize a deep convolutional network with a suitable loss function , such as contrastive loss or triplet loss . while a rich line of work focuses solely on the loss functions , we show in this paper that selecting training examples plays an equally important role . we propose distance weighted sampling , which selects more informative and stable examples than traditional approaches . in addition , we show that a simple margin based loss is sufficient to outperform all other loss functions . we evaluate our approach on the stanford online products , car196 , and the cub200-2011 datasets for image retrieval and clustering , and on the lfw dataset for face verification . our method achieves state-of-the-art performance on all of them .", "topics": ["sampling ( signal processing )", "cluster analysis"]}
{"title": "similar handwritten chinese character discrimination by weakly supervised learning", "abstract": "traditional approaches for handwritten chinese character recognition suffer in classifying similar characters . in this paper , we propose to discriminate similar handwritten chinese characters by using weakly supervised learning . our approach learns a discriminative svm for each similar pair which simultaneously localizes the discriminative region of similar character and makes the classification . for the first time , similar handwritten chinese character recognition ( shccr ) is formulated as an optimization problem extended from svm . we also propose a novel feature descriptor , gradient context , and apply bag-of-words model to represent regions with different scales . in our method , we do not need to select a sized-fixed sub-window to differentiate similar characters . the unconstrained property makes our method well adapted to high variance in the size and position of discriminative regions in similar handwritten chinese characters . we evaluate our proposed approach over the casia chinese character data set and the results show that our method outperforms the state of the art .", "topics": ["supervised learning", "support vector machine"]}
{"title": "a hardware-friendly algorithm for scalable training and deployment of dimensionality reduction models on fpga", "abstract": "with ever-increasing application of machine learning models in various domains such as image classification , speech recognition and synthesis , and health care , designing efficient hardware for these models has gained a lot of popularity . while the majority of researches in this area focus on efficient deployment of machine learning models ( a.k.a inference ) , this work concentrates on challenges of training these models in hardware . in particular , this paper presents a high-performance , scalable , reconfigurable solution for both training and deployment of different dimensionality reduction models in hardware by introducing a hardware-friendly algorithm . compared to state-of-the-art implementations , our proposed algorithm and its hardware realization decrease resource consumption by 50\\ % without any degradation in accuracy .", "topics": ["computer vision", "speech recognition"]}
{"title": "boosting optical character recognition : a super-resolution approach", "abstract": "text image super-resolution is a challenging yet open research problem in the computer vision community . in particular , low-resolution images hamper the performance of typical optical character recognition ( ocr ) systems . in this article , we summarize our entry to the icdar2015 competition on text image super-resolution . experiments are based on the provided icdar2015 textsr dataset and the released tesseract-ocr 3.02 system . we report that our winning entry of text image super-resolution framework has largely improved the ocr performance with low-resolution images used as input , reaching an ocr accuracy score of 77.19 % , which is comparable with that of using the original high-resolution images 78.80 % .", "topics": ["computer vision"]}
{"title": "character composition model with convolutional neural networks for dependency parsing on morphologically rich languages", "abstract": "we present a transition-based dependency parser that uses a convolutional neural network to compose word representations from characters . the character composition model shows great improvement over the word-lookup model , especially for parsing agglutinative languages . these improvements are even better than using pre-trained word embeddings from extra data . on the spmrl data sets , our system outperforms the previous best greedy parser ( ballesteros et al . , 2015 ) by a margin of 3 % on average .", "topics": ["parsing"]}
{"title": "a low-cost ethics shaping approach for designing reinforcement learning agents", "abstract": "this paper proposes a low-cost , easily realizable strategy to equip a reinforcement learning ( rl ) agent the capability of behaving ethically . our model allows the designers of rl agents to solely focus on the task to achieve , without having to worry about the implementation of multiple trivial ethical patterns to follow . based on the assumption that the majority of human behavior , regardless which goals they are achieving , is ethical , our design integrates human policy with the rl policy to achieve the target objective with less chance of violating the ethical code that human beings normally obey .", "topics": ["reinforcement learning"]}
{"title": "deep learning for environmentally robust speech recognition : an overview of recent developments", "abstract": "eliminating the negative effect of non-stationary environmental noise is a long-standing research topic for automatic speech recognition but still remains an important challenge . data-driven supervised approaches , especially the ones based on deep neural networks , have recently emerged as potential alternatives to traditional unsupervised approaches and with sufficient training , can alleviate the shortcomings of the unsupervised methods in various real-life acoustic environments . in this light , we review recently developed , representative deep learning approaches for tackling non-stationary additive and convolutional degradation of speech with the aim of providing guidelines for those involved in the development of environmentally robust speech recognition systems . we separately discuss single- and multi-channel techniques developed for the front-end and back-end of speech recognition systems , as well as joint front-end and back-end training frameworks .", "topics": ["speech recognition"]}
{"title": "dialogue system : a brief review", "abstract": "a dialogue system is a system which interacts with human in natural language . at present many universities are developing the dialogue system in their regional language . this paper will discuss about dialogue system , its components , challenges and its evaluation . this paper helps the researchers for getting info regarding dialogues system .", "topics": ["natural language"]}
{"title": "deeppose : human pose estimation via deep neural networks", "abstract": "we propose a method for human pose estimation based on deep neural networks ( dnns ) . the pose estimation is formulated as a dnn-based regression problem towards body joints . we present a cascade of such dnn regressors which results in high precision pose estimates . the approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formulation which capitalizes on recent advances in deep learning . we present a detailed empirical analysis with state-of-art or better performance on four academic benchmarks of diverse real-world images .", "topics": ["neural networks"]}
{"title": "learning bayesian nets that perform well", "abstract": "a bayesian net ( bn ) is more than a succinct way to encode a probabilistic distribution ; it also corresponds to a function used to answer queries . a bn can therefore be evaluated by the accuracy of the answers it returns . many algorithms for learning bns , however , attempt to optimize another criterion ( usually likelihood , possibly augmented with a regularizing term ) , which is independent of the distribution of queries that are posed . this paper takes the `` performance criteria '' seriously , and considers the challenge of computing the bn whose performance - read `` accuracy over the distribution of queries '' - is optimal . we show that many aspects of this learning task are more difficult than the corresponding subtasks in the standard model .", "topics": ["bayesian network", "eisenstein 's criterion"]}
{"title": "triplespin - a generic compact paradigm for fast machine learning computations", "abstract": "we present a generic compact computational framework relying on structured random matrices that can be applied to speed up several machine learning algorithms with almost no loss of accuracy . the applications include new fast lsh-based algorithms , efficient kernel computations via random feature maps , convex optimization algorithms , quantization techniques and many more . certain models of the presented paradigm are even more compressible since they apply only bit matrices . this makes them suitable for deploying on mobile devices . all our findings come with strong theoretical guarantees . in particular , as a byproduct of the presented techniques and by using relatively new berry-esseen-type clt for random vectors , we give the first theoretical guarantees for one of the most efficient existing lsh algorithms based on the $ \\textbf { hd } _ { 3 } \\textbf { hd } _ { 2 } \\textbf { hd } _ { 1 } $ structured matrix ( `` practical and optimal lsh for angular distance '' ) . these guarantees as well as theoretical results for other aforementioned applications follow from the same general theoretical principle that we present in the paper . our structured family contains as special cases all previously considered structured schemes , including the recently introduced $ p $ -model . experimental evaluation confirms the accuracy and efficiency of triplespin matrices .", "topics": ["computation"]}
{"title": "thumbs up ? sentiment classification using machine learning techniques", "abstract": "we consider the problem of classifying documents not by topic , but by overall sentiment , e.g . , determining whether a review is positive or negative . using movie reviews as data , we find that standard machine learning techniques definitively outperform human-produced baselines . however , the three machine learning methods we employed ( naive bayes , maximum entropy classification , and support vector machines ) do not perform as well on sentiment classification as on traditional topic-based categorization . we conclude by examining factors that make the sentiment classification problem more challenging .", "topics": ["baseline ( configuration management )", "statistical classification"]}
{"title": "fast , flexible models for discovering topic correlation across weakly-related collections", "abstract": "weak topic correlation across document collections with different numbers of topics in individual collections presents challenges for existing cross-collection topic models . this paper introduces two probabilistic topic models , correlated lda ( c-lda ) and correlated hdp ( c-hdp ) . these address problems that can arise when analyzing large , asymmetric , and potentially weakly-related collections . topic correlations in weakly-related collections typically lie in the tail of the topic distribution , where they would be overlooked by models unable to fit large numbers of topics . to efficiently model this long tail for large-scale analysis , our models implement a parallel sampling algorithm based on the metropolis-hastings and alias methods ( yuan et al . , 2015 ) . the models are first evaluated on synthetic data , generated to simulate various collection-level asymmetries . we then present a case study of modeling over 300k documents in collections of sciences and humanities research from jstor .", "topics": ["synthetic data", "simulation"]}
{"title": "solving dense image matching in real-time using discrete-continuous optimization", "abstract": "dense image matching is a fundamental low-level problem in computer vision , which has received tremendous attention from both discrete and continuous optimization communities . the goal of this paper is to combine the advantages of discrete and continuous optimization in a coherent framework . we devise a model based on energy minimization , to be optimized by both discrete and continuous algorithms in a consistent way . in the discrete setting , we propose a novel optimization algorithm that can be massively parallelized . in the continuous setting we tackle the problem of non-convex regularizers by a formulation based on differences of convex functions . the resulting hybrid discrete-continuous algorithm can be efficiently accelerated by modern gpus and we demonstrate its real-time performance for the applications of dense stereo matching and optical flow .", "topics": ["computer vision"]}
{"title": "automatic diagnosis of retinal diseases from color retinal images", "abstract": "teleophthalmology holds a great potential to improve the quality , access , and affordability in health care . for patients , it can reduce the need for travel and provide the access to a superspecialist . ophthalmology lends itself easily to telemedicine as it is a largely image based diagnosis . the main goal of the proposed system is to diagnose the type of disease in the retina and to automatically detect and segment retinal diseases without human supervision or interaction . the proposed system will diagnose the disease present in the retina using a neural network based classifier.the extent of the disease spread in the retina can be identified by extracting the textural features of the retina . this system will diagnose the following type of diseases : diabetic retinopathy and drusen .", "topics": ["statistical classification"]}
{"title": "data-efficient off-policy policy evaluation for reinforcement learning", "abstract": "in this paper we present a new way of predicting the performance of a reinforcement learning policy given historical data that may have been generated by a different policy . the ability to evaluate a policy from historical data is important for applications where the deployment of a bad policy can be dangerous or costly . we show empirically that our algorithm produces estimates that often have orders of magnitude lower mean squared error than existing methods -- -it makes more efficient use of the available data . our new estimator is based on two advances : an extension of the doubly robust estimator ( jiang and li , 2015 ) , and a new way to mix between model based estimates and importance sampling based estimates .", "topics": ["sampling ( signal processing )", "reinforcement learning"]}
{"title": "mitigating evasion attacks to deep neural networks via region-based classification", "abstract": "deep neural networks ( dnns ) have transformed several artificial intelligence research areas including computer vision , speech recognition , and natural language processing . however , recent studies demonstrated that dnns are vulnerable to adversarial manipulations at testing time . specifically , suppose we have a testing example , whose label can be correctly predicted by a dnn classifier . an attacker can add a small carefully crafted noise to the testing example such that the dnn classifier predicts an incorrect label , where the crafted testing example is called adversarial example . such attacks are called evasion attacks . evasion attacks are one of the biggest challenges for deploying dnns in safety and security critical applications such as self-driving cars . in this work , we develop new methods to defend against evasion attacks . our key observation is that adversarial examples are close to the classification boundary . therefore , we propose region-based classification to be robust to adversarial examples . for a benign/adversarial testing example , we ensemble information in a hypercube centered at the example to predict its label . in contrast , traditional classifiers are point-based classification , i.e . , given a testing example , the classifier predicts its label based on the testing example alone . our evaluation results on mnist and cifar-10 datasets demonstrate that our region-based classification can significantly mitigate evasion attacks without sacrificing classification accuracy on benign examples . specifically , our region-based classification achieves the same classification accuracy on testing benign examples as point-based classification , but our region-based classification is significantly more robust than point-based classification to various evasion attacks .", "topics": ["statistical classification", "natural language processing"]}
{"title": "improving back-propagation by adding an adversarial gradient", "abstract": "the back-propagation algorithm is widely used for learning in artificial neural networks . a challenge in machine learning is to create models that generalize to new data samples not seen in the training data . recently , a common flaw in several machine learning algorithms was discovered : small perturbations added to the input data lead to consistent misclassification of data samples . samples that easily mislead the model are called adversarial examples . training a `` maxout '' network on adversarial examples has shown to decrease this vulnerability , but also increase classification performance . this paper shows that adversarial training has a regularizing effect also in networks with logistic , hyperbolic tangent and rectified linear units . a simple extension to the back-propagation method is proposed , that adds an adversarial gradient to the training . the extension requires an additional forward and backward pass to calculate a modified input sample , or mini batch , used as input for standard back-propagation learning . the first experimental results on mnist show that the `` adversarial back-propagation '' method increases the resistance to adversarial examples and boosts the classification performance . the extension reduces the classification error on the permutation invariant mnist from 1.60 % to 0.95 % in a logistic network , and from 1.40 % to 0.78 % in a network with rectified linear units . results on cifar-10 indicate that the method has a regularizing effect similar to dropout in fully connected networks . based on these promising results , adversarial back-propagation is proposed as a stand-alone regularizing method that should be further investigated .", "topics": ["test set", "gradient"]}
{"title": "a fuzzy topsis multiple-attribute decision making for scholarship selection", "abstract": "as the education fees are becoming more expensive , more students apply for scholarships . consequently , hundreds and even thousands of applications need to be handled by the sponsor . to solve the problems , some alternatives based on several attributes ( criteria ) need to be selected . in order to make a decision on such fuzzy problems , fuzzy multiple attribute decision making ( fmdam ) can be applied . in this study , unified modeling language ( uml ) in fmadm with topsis and weighted product ( wp ) methods is applied to select the candidates for academic and non-academic scholarships at universitas islam negeri sunan kalijaga . data used were a crisp and fuzzy data . the results show that topsis and weighted product fmadm methods can be used to select the most suitable candidates to receive the scholarships since the preference values applied in this method can show applicants with the highest eligibility", "topics": ["value ( ethics )"]}
{"title": "neural networks for dynamic shortest path routing problems - a survey", "abstract": "this paper reviews the overview of the dynamic shortest path routing problem and the various neural networks to solve it . different shortest path optimization problems can be solved by using various neural networks algorithms . the routing in packet switched multi-hop networks can be described as a classical combinatorial optimization problem i.e . a shortest path routing problem in graphs . the survey shows that the neural networks are the best candidates for the optimization of dynamic shortest path routing problems due to their fastness in computation comparing to other softcomputing and metaheuristics algorithms", "topics": ["optimization problem", "neural networks"]}
{"title": "binary classifier calibration : bayesian non-parametric approach", "abstract": "a set of probabilistic predictions is well calibrated if the events that are predicted to occur with probability p do in fact occur about p fraction of the time . well calibrated predictions are particularly important when machine learning models are used in decision analysis . this paper presents two new non-parametric methods for calibrating outputs of binary classification models : a method based on the bayes optimal selection and a method based on the bayesian model averaging . the advantage of these methods is that they are independent of the algorithm used to learn a predictive model , and they can be applied in a post-processing step , after the model is learned . this makes them applicable to a wide variety of machine learning models and methods . these calibration methods , as well as other methods , are tested on a variety of datasets in terms of both discrimination and calibration performance . the results show the methods either outperform or are comparable in performance to the state-of-the-art calibration methods .", "topics": ["bayesian network"]}
{"title": "training deep networks to be spatially sensitive", "abstract": "in many computer vision tasks , for example saliency prediction or semantic segmentation , the desired output is a foreground map that predicts pixels where some criteria is satisfied . despite the inherently spatial nature of this task commonly used learning objectives do not incorporate the spatial relationships between misclassified pixels and the underlying ground truth . the weighted f-measure , a recently proposed evaluation metric , does reweight errors spatially , and has been shown to closely correlate with human evaluation of quality , and stably rank predictions with respect to noisy ground truths ( such as a sloppy human annotator might generate ) . however it suffers from computational complexity which makes it intractable as an optimization objective for gradient descent , which must be evaluated thousands or millions of times while learning a model 's parameters . we propose a differentiable and efficient approximation of this metric . by incorporating spatial information into the objective we can use a simpler model than competing methods without sacrificing accuracy , resulting in faster inference speeds and alleviating the need for pre/post-processing . we match ( or improve ) performance on several tasks compared to prior state of the art by traditional metrics , and in many cases significantly improve performance by the weighted f-measure .", "topics": ["computational complexity theory", "mathematical optimization"]}
{"title": "using svm to pre-classify government purchases", "abstract": "the brazilian government often misclassifies the goods it buys . that makes it hard to audit government expenditures . we can not know whether the price paid for a ballpoint pen ( code # 7510 ) was reasonable if the pen was misclassified as a technical drawing pen ( code # 6675 ) or as any other good . this paper shows how we can use machine learning to reduce misclassification . i trained a support vector machine ( svm ) classifier that takes a product description as input and returns the most likely category codes as output . i trained the classifier using 20 million goods purchased by the brazilian government between 1999-04-01 and 2015-04-02 . in 83.3 % of the cases the correct category code was one of the three most likely category codes identified by the classifier . i used the trained classifier to develop a web app that might help the government reduce misclassification . i open sourced the code on github ; anyone can use and modify it .", "topics": ["support vector machine", "statistical classification"]}
{"title": "a deep spatial contextual long-term recurrent convolutional network for saliency detection", "abstract": "traditional saliency models usually adopt hand-crafted image features and human-designed mechanisms to calculate local or global contrast . in this paper , we propose a novel computational saliency model , i.e . , deep spatial contextual long-term recurrent convolutional network ( dsclrcn ) to predict where people looks in natural scenes . dsclrcn first automatically learns saliency related local features on each image location in parallel . then , in contrast with most other deep network based saliency models which infer saliency in local contexts , dsclrcn can mimic the cortical lateral inhibition mechanisms in human visual system to incorporate global contexts to assess the saliency of each image location by leveraging the deep spatial long short-term memory ( dslstm ) model . moreover , we also integrate scene context modulation in dslstm for saliency inference , leading to a novel deep spatial contextual lstm ( dsclstm ) model . the whole network can be trained end-to-end and works efficiently when testing . experimental results on two benchmark datasets show that dsclrcn can achieve state-of-the-art performance on saliency detection . furthermore , the proposed dsclstm model can significantly boost the saliency detection performance by incorporating both global spatial interconnections and scene context modulation , which may uncover novel inspirations for studies on them in computational saliency models .", "topics": ["end-to-end principle"]}
{"title": "a neural network approach to ordinal regression", "abstract": "ordinal regression is an important type of learning , which has properties of both classification and regression . here we describe a simple and effective approach to adapt a traditional neural network to learn ordinal categories . our approach is a generalization of the perceptron method for ordinal regression . on several benchmark datasets , our method ( nnrank ) outperforms a neural network classification method . compared with the ordinal regression methods using gaussian processes and support vector machines , nnrank achieves comparable performance . moreover , nnrank has the advantages of traditional neural networks : learning in both online and batch modes , handling very large training datasets , and making rapid predictions . these features make nnrank a useful and complementary tool for large-scale data processing tasks such as information retrieval , web page ranking , collaborative filtering , and protein ranking in bioinformatics .", "topics": ["data mining", "support vector machine"]}
{"title": "generic sketch-based retrieval learned without drawing a single sketch", "abstract": "we cast the sketch-based retrieval as edge-map matching . a shared convolutional network is trained to extract descriptors from edge maps and sketches , which are treated as a special case of edge maps . the network is fine-tuned solely from edge maps of landmark images . the training images are acquired in a fully unsupervised manner from 3d landmark models obtained by an automated structure-from-motion pipeline . the proposed method achieves the state-of-the-art results on a standard benchmark . on two other fine-grained sketch-based retrieval benchmarks , it performs on par with or comes just after the method specifically designed for the dataset .", "topics": ["unsupervised learning", "map"]}
{"title": "verbal chunk extraction in french using limited resources", "abstract": "a way of extracting french verbal chunks , inflected and infinitive , is explored and tested on effective corpus . declarative morphological and local grammar rules specifying chunks and some simple contextual structures are used , relying on limited lexical information and some simple heuristic/statistic properties obtained from restricted corpora . the specific goals , the architecture and the formalism of the system , the linguistic information on which it relies and the obtained results on effective corpus are presented .", "topics": ["text corpus", "heuristic"]}
{"title": "discriminative unsupervised feature learning with exemplar convolutional neural networks", "abstract": "deep convolutional networks have proven to be very successful in learning task specific features that allow for unprecedented performance on various computer vision tasks . training of such networks follows mostly the supervised learning paradigm , where sufficiently many input-output pairs are required for training . acquisition of large training sets is one of the key challenges , when approaching a new task . in this paper , we aim for generic feature learning and present an approach for training a convolutional network using only unlabeled data . to this end , we train the network to discriminate between a set of surrogate classes . each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed ' image patch . in contrast to supervised network training , the resulting feature representation is not class specific . it rather provides robustness to the transformations that have been applied during training . this generic feature representation allows for classification results that outperform the state of the art for unsupervised learning on several popular datasets ( stl-10 , cifar-10 , caltech-101 , caltech-256 ) . while such generic features can not compete with class specific features from supervised training on a classification task , we show that they are advantageous on geometric matching problems , where they also outperform the sift descriptor .", "topics": ["feature learning", "supervised learning"]}
{"title": "letter-based speech recognition with gated convnets", "abstract": "in this paper we introduce a new speech recognition system , leveraging a simple letter-based convnet acoustic model . the acoustic model requires -- only audio transcription for training -- no alignment annotations , nor any forced alignment step is needed . at inference , our decoder takes only a word list and a language model , and is fed with letter scores from the -- acoustic model -- no phonetic word lexicon is needed . key ingredients for the acoustic model are gated linear units and high dropout . we show near state-of-the-art results in word error rate on the librispeech corpus using log-mel filterbanks , both on the `` clean '' and `` other '' configurations .", "topics": ["speech recognition"]}
{"title": "consistency constraints for overlapping data clustering", "abstract": "we examine overlapping clustering schemes with functorial constraints , in the spirit of carlsson -- memoli . this avoids issues arising from the chaining required by partition-based methods . our principal result shows that any clustering functor is naturally constrained to refine single-linkage clusters and be refined by maximal-linkage clusters . we work in the context of metric spaces with non-expansive maps , which is appropriate for modeling data processing which does not increase information content .", "topics": ["cluster analysis", "map"]}
{"title": "rademacher complexity of stationary sequences", "abstract": "we show how to control the generalization error of time series models wherein past values of the outcome are used to predict future values . the results are based on a generalization of standard i.i.d . concentration inequalities to dependent data without the mixing assumptions common in the time series setting . our proof and the result are simpler than previous analyses with dependent data or stochastic adversaries which use sequential rademacher complexities rather than the expected rademacher complexity for i.i.d . processes . we also derive empirical rademacher results without mixing assumptions resulting in fully calculable upper bounds .", "topics": ["time series", "value ( ethics )"]}
{"title": "efficient stochastic inference of bitwise deep neural networks", "abstract": "recently published methods enable training of bitwise neural networks which allow reduced representation of down to a single bit per weight . we present a method that exploits ensemble decisions based on multiple stochastically sampled network models to increase performance figures of bitwise neural networks in terms of classification accuracy at inference . our experiments with the cifar-10 and gtsrb datasets show that the performance of such network ensembles surpasses the performance of the high-precision base model . with this technique we achieve 5.81 % best classification error on cifar-10 test set using bitwise networks . concerning inference on embedded systems we evaluate these bitwise networks using a hardware efficient stochastic rounding procedure . our work contributes to efficient embedded bitwise neural networks .", "topics": ["test set", "neural networks"]}
{"title": "sparse linear regression with missing data", "abstract": "this paper proposes a fast and accurate method for sparse regression in the presence of missing data . the underlying statistical model encapsulates the low-dimensional structure of the incomplete data matrix and the sparsity of the regression coefficients , and the proposed algorithm jointly learns the low-dimensional structure of the data and a linear regressor with sparse coefficients . the proposed stochastic optimization method , sparse linear regression with missing data ( slrm ) , performs an alternating minimization procedure and scales well with the problem size . large deviation inequalities shed light on the impact of the various problem-dependent parameters on the expected squared loss of the learned regressor . extensive simulations on both synthetic and real datasets show that slrm performs better than competing algorithms in a variety of contexts .", "topics": ["synthetic data", "simulation"]}
{"title": "an effective image feature classiffication using an improved som", "abstract": "image feature classification is a challenging problem in many computer vision applications , specifically , in the fields of remote sensing , image analysis and pattern recognition . in this paper , a novel self organizing map , termed improved som ( isom ) , is proposed with the aim of effectively classifying mammographic images based on their texture feature representation . the main contribution of the isom is to introduce a new node structure for the map representation and adopting a learning technique based on kohonen som accordingly . the main idea is to control , in an unsupervised fashion , the weight updating procedure depending on the class reliability of the node , during the weight update time . experiments held on a real mammographic images . results showed high accuracy compared to classical som and other state-of-art classifiers .", "topics": ["statistical classification", "unsupervised learning"]}
{"title": "edge detection based shape identification", "abstract": "image recognition is the need of the hour . in order to be able to recognize an image , it is of immense importance that the image should be distinguishable from the background . in the present work , an approach is presented for automatic detection and recognition of regular 2d shapes in low noise environments . the work has a large number of direct applications in the real world . the algorithm proposed is based on locating the edges and thus , in turn calculating the area of the object helps in identification of a specified shape . the results were simulated using matlab tool are encouraging and validate the proposed algorithm . index terms : edge detection , area calculation , shape detection , object recognition", "topics": ["computer vision", "simulation"]}
{"title": "a maximum matching algorithm for basis selection in spectral learning", "abstract": "we present a solution to scale spectral algorithms for learning sequence functions . we are interested in the case where these functions are sparse ( that is , for most sequences they return 0 ) . spectral algorithms reduce the learning problem to the task of computing an svd decomposition over a special type of matrix called the hankel matrix . this matrix is designed to capture the relevant statistics of the training sequences . what is crucial is that to capture long range dependencies we must consider very large hankel matrices . thus the computation of the svd becomes a critical bottleneck . our solution finds a subset of rows and columns of the hankel that realizes a compact and informative hankel submatrix . the novelty lies in the way that this subset is selected : we exploit a maximal bipartite matching combinatorial algorithm to look for a sub-block with full structural rank , and show how computation of this sub-block can be further improved by exploiting the specific structure of hankel matrices .", "topics": ["sparse matrix", "computation"]}
{"title": "generalised elastic nets", "abstract": "the elastic net was introduced as a heuristic algorithm for combinatorial optimisation and has been applied , among other problems , to biological modelling . it has an energy function which trades off a fitness term against a tension term . in the original formulation of the algorithm the tension term was implicitly based on a first-order derivative . in this paper we generalise the elastic net model to an arbitrary quadratic tension term , e.g . derived from a discretised differential operator , and give an efficient learning algorithm . we refer to these as generalised elastic nets ( gens ) . we give a theoretical analysis of the tension term for 1d nets with periodic boundary conditions , and show that the model is sensitive to the choice of finite difference scheme that represents the discretised derivative . we illustrate some of these issues in the context of cortical map models , by relating the choice of tension term to a cortical interaction function . in particular , we prove that this interaction takes the form of a mexican hat for the original elastic net , and of progressively more oscillatory mexican hats for higher-order derivatives . the results apply not only to generalised elastic nets but also to other methods using discrete differential penalties , and are expected to be useful in other areas , such as data analysis , computer graphics and optimisation problems .", "topics": ["mathematical optimization", "image processing"]}
{"title": "deep contextual recurrent residual networks for scene labeling", "abstract": "designed as extremely deep architectures , deep residual networks which provide a rich visual representation and offer robust convergence behaviors have recently achieved exceptional performance in numerous computer vision problems . being directly applied to a scene labeling problem , however , they were limited to capture long-range contextual dependence , which is a critical aspect . to address this issue , we propose a novel approach , contextual recurrent residual networks ( crrn ) which is able to simultaneously handle rich visual representation learning and long-range context modeling within a fully end-to-end deep network . furthermore , our proposed end-to-end crrn is completely trained from scratch , without using any pre-trained models in contrast to most existing methods usually fine-tuned from the state-of-the-art pre-trained models , e.g . vgg-16 , resnet , etc . the experiments are conducted on four challenging scene labeling datasets , i.e . siftflow , camvid , stanford background and sun datasets , and compared against various state-of-the-art scene labeling methods .", "topics": ["feature learning", "computer vision"]}
{"title": "derivative based focal plane array nonuniformity correction", "abstract": "this paper presents a fast and robust method for fixed pattern noise nonuniformity correction of infrared focal plane arrays . the proposed method requires neither shutter nor elaborate calibrations and therefore enables a real time correction with no interruptions . based on derivative estimation of the fixed pattern noise from pixel sized translations of the focal plane array , the proposed method has the advantages of being invariant to the noise magnitude and robust to unknown camera and inter-scene movements while being virtually transparent to the end-user .", "topics": ["pixel"]}
{"title": "predictive state representations : a new theory for modeling dynamical systems", "abstract": "modeling dynamical systems , both for control purposes and to make predictions about their behavior , is ubiquitous in science and engineering . predictive state representations ( psrs ) are a recently introduced class of models for discrete-time dynamical systems . the key idea behind psrs and the closely related ooms ( jaeger 's observable operator models ) is to represent the state of the system as a set of predictions of observable outcomes of experiments one can do in the system . this makes psrs rather different from history-based models such as nth-order markov models and hidden-state-based models such as hmms and pomdps . we introduce an interesting construct , the systemdynamics matrix , and show how psrs can be derived simply from it . we also use this construct to show formally that psrs are more general than both nth-order markov models and hmms/pomdps . finally , we discuss the main difference between psrs and ooms and conclude with directions for future work .", "topics": ["markov chain"]}
{"title": "estimating heterogeneous consumer preferences for restaurants and travel time using mobile location data", "abstract": "this paper analyzes consumer choices over lunchtime restaurants using data from a sample of several thousand anonymous mobile phone users in the san francisco bay area . the data is used to identify users ' approximate typical morning location , as well as their choices of lunchtime restaurants . we build a model where restaurants have latent characteristics ( whose distribution may depend on restaurant observables , such as star ratings , food category , and price range ) , each user has preferences for these latent characteristics , and these preferences are heterogeneous across users . similarly , each item has latent characteristics that describe users ' willingness to travel to the restaurant , and each user has individual-specific preferences for those latent characteristics . thus , both users ' willingness to travel and their base utility for each restaurant vary across user-restaurant pairs . we use a bayesian approach to estimation . to make the estimation computationally feasible , we rely on variational inference to approximate the posterior distribution , as well as stochastic gradient descent as a computational approach . our model performs better than more standard competing models such as multinomial logit and nested logit models , in part due to the personalization of the estimates . we analyze how consumers re-allocate their demand after a restaurant closes to nearby restaurants versus more distant restaurants with similar characteristics , and we compare our predictions to actual outcomes . finally , we show how the model can be used to analyze counterfactual questions such as what type of restaurant would attract the most consumers in a given location .", "topics": ["calculus of variations", "approximation algorithm"]}
{"title": "lenient multi-agent deep reinforcement learning", "abstract": "much of the success of single agent deep reinforcement learning ( drl ) in recent years can be attributed to the use of experience replay memories ( erm ) , which allow deep q-networks ( dqns ) to be trained efficiently through sampling stored state transitions . however , care is required when using erms for multi-agent deep reinforcement learning ( ma-drl ) , as stored transitions can become outdated because agents update their policies in parallel [ 11 ] . in this work we apply leniency [ 23 ] to ma-drl . lenient agents map state-action pairs to decaying temperature values that control the amount of leniency applied towards negative policy updates that are sampled from the erm . this introduces optimism in the value-function update , and has been shown to facilitate cooperation in tabular fully-cooperative multi-agent reinforcement learning problems . we evaluate our lenient-dqn ( ldqn ) empirically against the related hysteretic-dqn ( hdqn ) algorithm [ 22 ] as well as a modified version we call scheduled-hdqn , that uses average reward learning near terminal states . evaluations take place in extended variations of the coordinated multi-agent object transportation problem ( cmotp ) [ 8 ] which include fully-cooperative sub-tasks and stochastic rewards . we find that ldqn agents are more likely to converge to the optimal policy in a stochastic reward cmotp compared to standard and scheduled-hdqn agents .", "topics": ["sampling ( signal processing )", "reinforcement learning"]}
{"title": "prepositional attachment disambiguation using bilingual parsing and alignments", "abstract": "in this paper , we attempt to solve the problem of prepositional phrase ( pp ) attachments in english . the motivation for the work comes from nlp applications like machine translation , for which , getting the correct attachment of prepositions is very crucial . the idea is to correct the pp-attachments for a sentence with the help of alignments from parallel data in another language . the novelty of our work lies in the formulation of the problem into a dual decomposition based algorithm that enforces agreement between the parse trees from two languages as a constraint . experiments were performed on the english-hindi language pair and the performance improved by 10 % over the baseline , where the baseline is the attachment predicted by the mstparser model trained for english .", "topics": ["baseline ( configuration management )", "natural language processing"]}
{"title": "learning feedforward and recurrent deterministic spiking neuron network feedback controllers", "abstract": "we consider the problem of feedback control when the controller is constructed solely of deterministic spiking neurons . although spiking neurons and networks have been the subject of several previous studies , analysis has primarily been restricted to a firing rate model . in contrast , we construct a spike timing based deterministic spiking neuron controller whose control output is one or multiple sparse spike trains . we model the problem formally as a hybrid dynamical system comprised of a closed loop between a plant and a spiking neuron network controller . the construction differs from classical controllers owing to the fact that the control feedback to the plant is generated by convolving the spike trains with fixed kernels , resulting in a highly constrained and stereotyped control signal . we derive a novel synaptic weight update rule via which the spiking neuron network controller to hold process variables at desired set points . we demonstrate the efficacy of the rule by applying it to the classical control problem of the cart-pole ( inverted pendulum ) . experiments demonstrate that the proposed controller has a larger region of stability as compared to the traditional pid controller , and its trajectories differ qualitatively from those of the pid controller . in addition , the proposed controller with a recurrent network generates sparse spike trains with rates as low as 1.99hz .", "topics": ["recurrent neural network", "sparse matrix"]}
{"title": "adaptative combination rule and proportional conflict redistribution rule for information fusion", "abstract": "this paper presents two new promising rules of combination for the fusion of uncertain and potentially highly conflicting sources of evidences in the framework of the theory of belief functions in order to palliate the well-know limitations of dempster 's rule and to work beyond the limits of applicability of the dempster-shafer theory . we present both a new class of adaptive combination rules ( acr ) and a new efficient proportional conflict redistribution ( pcr ) rule allowing to deal with highly conflicting sources for static and dynamic fusion applications .", "topics": ["simulation"]}
{"title": "amr parsing using stack-lstms", "abstract": "we present a transition-based amr parser that directly generates amr parses from plain text . we use stack-lstms to represent our parser state and make decisions greedily . in our experiments , we show that our parser achieves very competitive scores on english using only amr training data . adding additional information , such as pos tags and dependency trees , improves the results further .", "topics": ["test set", "parsing"]}
{"title": "implementation of a vision system for a landmine detecting robot using artificial neural network", "abstract": "landmines , specifically anti-tank mines , cluster bombs , and unexploded ordnance form a serious problem in many countries . several landmine sweeping techniques are used for minesweeping . this paper presents the design and the implementation of the vision system of an autonomous robot for landmines localization . the proposed work develops state-of-the-art techniques in digital image processing for pre-processing captured images of the contaminated area . after enhancement , artificial neural network ( ann ) is used in order to identify , recognize and classify the landmines ' make and model . the back-propagation algorithm is used for training the network . the proposed work proved to be able to identify and classify different types of landmines under various conditions ( rotated landmine , partially covered landmine ) with a success rate of up to 90 % .", "topics": ["image processing", "robot"]}
{"title": "beyond grand theft auto v for training , testing and enhancing deep learning in self driving cars", "abstract": "as an initial assessment , over 480,000 labeled virtual images of normal highway driving were readily generated in grand theft auto v 's virtual environment . using these images , a cnn was trained to detect following distance to cars/objects ahead , lane markings , and driving angle ( angular heading relative to lane centerline ) : all variables necessary for basic autonomous driving . encouraging results were obtained when tested on over 50,000 labeled virtual images from substantially different gta-v driving environments . this initial assessment begins to define both the range and scope of the labeled images needed for training as well as the range and scope of labeled images needed for testing the definition of boundaries and limitations of trained networks . it is the efficacy and flexibility of a `` gta-v '' -like virtual environment that is expected to provide an efficient well-defined foundation for the training and testing of convolutional neural networks for safe driving . additionally , described is the princeton virtual environment ( pve ) for the training , testing and enhancement of safe driving ai , which is being developed using the video-game engine unity . pve is being developed to recreate rare but critical corner cases that can be used in re-training and enhancing machine learning models and understanding the limitations of current self driving models . the florida tesla crash is being used as an initial reference .", "topics": ["neural networks", "autonomous car"]}
{"title": "competition of self-organized rotating spiral autowaves in a nonequilibrium dissipative system of three-level phaser", "abstract": "we present results of cellular automata based investigations of rotating spiral autowaves in a nonequilibrium excitable medium which models three-level paramagnetic microwave phonon laser ( phaser ) . the computational model is described in arxiv : cond-mat/0410460v2 and arxiv : cond-mat/0602345v1 . we have observed several new scenarios of self-organization , competition and dynamical stabilization of rotating spiral autowaves under conditions of cross-relaxation between three-level active centers . in particular , phenomena of inversion of topological charge , as well as processes of regeneration and replication of rotating spiral autowaves in various excitable media were revealed and visualized for mesoscopic-scale areas of phaser-type active systems , which model real phaser devices .", "topics": ["numerical analysis", "causality"]}
{"title": "supervised fine tuning for word embedding with integrated knowledge", "abstract": "learning vector representation for words is an important research field which may benefit many natural language processing tasks . two limitations exist in nearly all available models , which are the bias caused by the context definition and the lack of knowledge utilization . they are difficult to tackle because these algorithms are essentially unsupervised learning approaches . inspired by deep learning , the authors propose a supervised framework for learning vector representation of words to provide additional supervised fine tuning after unsupervised learning . the framework is knowledge rich approacher and compatible with any numerical vectors word representation . the authors perform both intrinsic evaluation like attributional and relational similarity prediction and extrinsic evaluations like the sentence completion and sentiment analysis . experiments results on 6 embeddings and 4 tasks with 10 datasets show that the proposed fine tuning framework may significantly improve the quality of the vector representation of words .", "topics": ["feature learning", "supervised learning"]}
{"title": "unimib shar : a new dataset for human activity recognition using acceleration data from smartphones", "abstract": "smartphones , smartwatches , fitness trackers , and ad-hoc wearable devices are being increasingly used to monitor human activities . data acquired by the hosted sensors are usually processed by machine-learning-based algorithms to classify human activities . the success of those algorithms mostly depends on the availability of training ( labeled ) data that , if made publicly available , would allow researchers to make objective comparisons between techniques . nowadays , publicly available data sets are few , often contain samples from subjects with too similar characteristics , and very often lack of specific information so that is not possible to select subsets of samples according to specific criteria . in this article , we present a new dataset of acceleration samples acquired with an android smartphone designed for human activity recognition and fall detection . the dataset includes 11,771 samples of both human activities and falls performed by 30 subjects of ages ranging from 18 to 60 years . samples are divided in 17 fine grained classes grouped in two coarse grained classes : one containing samples of 9 types of activities of daily living ( adl ) and the other containing samples of 8 types of falls . the dataset has been stored to include all the information useful to select samples according to different criteria , such as the type of adl , the age , the gender , and so on . finally , the dataset has been benchmarked with four different classifiers and with two different feature vectors . we evaluated four different classification tasks : fall vs no fall , 9 activities , 8 falls , 17 activities and falls . for each classification task we performed a subject-dependent and independent evaluation . the major findings of the evaluation are the following : i ) it is more difficult to distinguish between types of falls than types of activities ; ii ) subject-dependent evaluation outperforms the subject-independent one", "topics": ["sensor"]}
{"title": "fwlbp : a scale invariant descriptor for texture classification", "abstract": "in this paper we propose a novel texture recognition feature called fractal weighted local binary pattern ( fwlbp ) . it has been observed that fractal dimension ( fd ) measure is relatively invariant to scale-changes , and presents a good correlation with human perception of surface roughness . we have utilized this property to construct a scale-invariant descriptor . we have sampled the input image using an augmented form of the local binary pattern ( lbp ) , and then used an indexing operation to assign fd weights to the collected samples . the final histogram of the descriptor has its features calculated using lbp , and its weights computed from the fd image . the proposed descriptor is scale , rotation and reflection invariant , and is also partially tolerant to noise and illumination changes . in addition , it is also shown that the local fractal dimension is relatively insensitive to the bi-lipschitz transformations , whereas its extension is able to correctly discriminate between fundamental texture primitives . experimental results show the proposed descriptor has better classification rates compared to the state-of-the-art descriptors on standard texture databases .", "topics": ["computer vision", "database"]}
{"title": "an optimized union-find algorithm for connected components labeling using gpus", "abstract": "in this paper , we report an optimized union-find ( uf ) algorithm that can label the connected components on a 2d image efficiently by employing the gpu architecture . the proposed method contains three phases : uf-based local merge , boundary analysis , and link . the coarse labeling in local merge reduces the number atomic operations , while the boundary analysis only manages the pixels on the boundary of each block . evaluation results showed that the proposed algorithm speed up the average running time by more than 1.3x .", "topics": ["time complexity", "computation"]}
{"title": "a new backpropagation algorithm without gradient descent", "abstract": "the backpropagation algorithm , which had been originally introduced in the 1970s , is the workhorse of learning in neural networks . this backpropagation algorithm makes use of the famous machine learning algorithm known as gradient descent , which is a first-order iterative optimization algorithm for finding the minimum of a function . to find a local minimum of a function using gradient descent , one takes steps proportional to the negative of the gradient ( or of the approximate gradient ) of the function at the current point . in this paper , we develop an alternative to the backpropagation without the use of the gradient descent algorithm , but instead we are going to devise a new algorithm to find the error in the weights and biases of an artificial neuron using moore-penrose pseudo inverse . the numerical studies and the experiments performed on various datasets are used to verify the working of this alternative algorithm .", "topics": ["approximation algorithm", "numerical analysis"]}
{"title": "learning spatial-aware regressions for visual tracking", "abstract": "in this paper , we analyze the spatial information of deep features , and propose two complementary regressions for robust visual tracking . first , we propose a kernelized ridge regression model wherein the kernel value is defined as the weighted sum of similarity scores of all pairs of patches between two samples . we show that this model can be formulated as a neural network and thus can be efficiently solved . second , we propose a fully convolutional neural network with spatially regularized kernels , through which the filter kernel corresponding to each output channel is forced to focus on a specific region of the target . distance transform pooling is further exploited to determine the effectiveness of each output channel of the convolution layer . the outputs from the kernelized ridge regression model and the fully convolutional neural network are combined to obtain the ultimate response . experimental results on three benchmark datasets validate the effectiveness of the proposed method .", "topics": ["kernel ( operating system )", "convolution"]}
{"title": "the effects of age , gender and region on non-standard linguistic variation in online social networks", "abstract": "we present a corpus-based analysis of the effects of age , gender and region of origin on the production of both `` netspeak '' or `` chatspeak '' features and regional speech features in flemish dutch posts that were collected from a belgian online social network platform . the present study shows that combining quantitative and qualitative approaches is essential for understanding non-standard linguistic variation in a cmc corpus . it also presents a methodology that enables the systematic study of this variation by including all non-standard words in the corpus . the analyses resulted in a convincing illustration of the adolescent peak principle . in addition , our approach revealed an intriguing correlation between the use of regional speech features and chatspeak features .", "topics": ["text corpus"]}
{"title": "a fast integrated planning and control framework for autonomous driving via imitation learning", "abstract": "for safe and efficient planning and control in autonomous driving , we need a driving policy which can achieve desirable driving quality in long-term horizon with guaranteed safety and feasibility . optimization-based approaches , such as model predictive control ( mpc ) , can provide such optimal policies , but their computational complexity is generally unacceptable for real-time implementation . to address this problem , we propose a fast integrated planning and control framework that combines learning- and optimization-based approaches in a two-layer hierarchical structure . the first layer , defined as the `` policy layer '' , is established by a neural network which learns the long-term optimal driving policy generated by mpc . the second layer , called the `` execution layer '' , is a short-term optimization-based controller that tracks the reference trajecotries given by the `` policy layer '' with guaranteed short-term safety and feasibility . moreover , with efficient and highly-representative features , a small-size neural network is sufficient in the `` policy layer '' to handle many complicated driving scenarios . this renders online imitation learning with dataset aggregation ( dagger ) so that the performance of the `` policy layer '' can be improved rapidly and continuously online . several exampled driving scenarios are demonstrated to verify the effectiveness and efficiency of the proposed framework .", "topics": ["computational complexity theory", "autonomous car"]}
{"title": "coevolutionary genetic algorithms for establishing nash equilibrium in symmetric cournot games", "abstract": "we use co-evolutionary genetic algorithms to model the players ' learning process in several cournot models , and evaluate them in terms of their convergence to the nash equilibrium . the `` social-learning '' versions of the two co-evolutionary algorithms we introduce , establish nash equilibrium in those models , in contrast to the `` individual learning '' versions which , as we see here , do not imply the convergence of the players ' strategies to the nash outcome . when players use `` canonical co-evolutionary genetic algorithms '' as learning algorithms , the process of the game is an ergodic markov chain , and therefore we analyze simulation results using both the relevant methodology and more general statistical tests , to find that in the `` social '' case , states leading to ne play are highly frequent at the stationary distribution of the chain , in contrast to the `` individual learning '' case , when ne is not reached at all in our simulations ; to find that the expected hamming distance of the states at the limiting distribution from the `` ne state '' is significantly smaller in the `` social '' than in the `` individual learning case '' ; to estimate the expected time that the `` social '' algorithms need to get to the `` ne state '' and verify their robustness and finally to show that a large fraction of the games played are indeed at the nash equilibrium .", "topics": ["reinforcement learning", "simulation"]}
{"title": "asymmetric feature maps with application to sketch based retrieval", "abstract": "we propose a novel concept of asymmetric feature maps ( afm ) , which allows to evaluate multiple kernels between a query and database entries without increasing the memory requirements . to demonstrate the advantages of the afm method , we derive a short vector image representation that , due to asymmetric feature maps , supports efficient scale and translation invariant sketch-based image retrieval . unlike most of the short-code based retrieval systems , the proposed method provides the query localization in the retrieved image . the efficiency of the search is boosted by approximating a 2d translation search via trigonometric polynomial of scores by 1d projections . the projections are a special case of afm . an order of magnitude speed-up is achieved compared to traditional trigonometric polynomials . the results are boosted by an image-based average query expansion , exceeding significantly the state of the art on standard benchmarks .", "topics": ["approximation algorithm", "map"]}
{"title": "a walk with sgd", "abstract": "exploring why stochastic gradient descent ( sgd ) based optimization methods train deep neural networks ( dnns ) that generalize well has become an active area of research . towards this end , we empirically study the dynamics of sgd when training over-parametrized dnns . specifically we study the dnn loss surface along the trajectory of sgd by interpolating the loss surface between parameters from consecutive \\textit { iterations } and tracking various metrics during training . we find that the loss interpolation between parameters before and after a training update is roughly convex with a minimum ( \\textit { valley floor } ) in between for most of the training . based on this and other metrics , we deduce that during most of the training , sgd explores regions in a valley by bouncing off valley walls at a height above the valley floor . this 'bouncing off walls at a height ' mechanism helps sgd traverse larger distance for small batch sizes and large learning rates which we find play qualitatively different roles in the dynamics . while a large learning rate maintains a large height from the valley floor , a small batch size injects noise facilitating exploration . we find this mechanism is crucial for generalization because the valley floor has barriers and this exploration above the valley floor allows sgd to quickly travel far away from the initialization point ( without being affected by barriers ) and find flatter regions , corresponding to better generalization .", "topics": ["gradient descent", "iteration"]}
{"title": "low precision rnns : quantizing rnns without losing accuracy", "abstract": "similar to convolution neural networks , recurrent neural networks ( rnns ) typically suffer from over-parameterization . quantizing bit-widths of weights and activations results in runtime efficiency on hardware , yet it often comes at the cost of reduced accuracy . this paper proposes a quantization approach that increases model size with bit-width reduction . this approach will allow networks to perform at their baseline accuracy while still maintaining the benefits of reduced precision and overall model size reduction .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "from modal to multimodal ambiguities : a classification approach", "abstract": "this paper deals with classifying ambiguities for multimodal languages . it evolves the classifications and the methods of the literature on ambiguities for natural language and visual language , empirically defining an original classification of ambiguities for multimodal interaction using a linguistic perspective . this classification distinguishes between semantic and syntactic multimodal ambiguities and their subclasses , which are intercepted using a rule-based method implemented in a software module . the experimental results have achieved an accuracy of the obtained classification compared to the expected one , which are defined by the human judgment , of 94.6 % for the semantic ambiguities classes , and 92.1 % for the syntactic ambiguities classes .", "topics": ["natural language"]}
{"title": "solarisnet : a deep regression network for solar radiation prediction", "abstract": "effective utilization of photovoltaic ( pv ) plants requires weather variability robust global solar radiation ( gsr ) forecasting models . random weather turbulence phenomena coupled with assumptions of clear sky model as suggested by hottel pose significant challenges to parametric & non-parametric models in gsr conversion rate estimation . also , a decent gsr estimate requires costly high-tech radiometer and expert dependent instrument handling and measurements , which are subjective . as such , a computer aided monitoring ( cam ) system to evaluate pv plant operation feasibility by employing smart grid past data analytics and deep learning is developed . our algorithm , solarisnet is a 6-layer deep neural network trained on data collected at two weather stations located near kalyani metrological site , west bengal , india . the daily gsr prediction performance using solarisnet outperforms the existing state of art and its efficacy in inferring past gsr data insights to comprehend daily and seasonal gsr variability along with its competence for short term forecasting is discussed .", "topics": ["time series"]}
{"title": "lock-free parallel perceptron for graph-based dependency parsing", "abstract": "dependency parsing is an important nlp task . a popular approach for dependency parsing is structured perceptron . still , graph-based dependency parsing has the time complexity of $ o ( n^3 ) $ , and it suffers from slow training . to deal with this problem , we propose a parallel algorithm called parallel perceptron . the parallel algorithm can make full use of a multi-core computer which saves a lot of training time . based on experiments we observe that dependency parsing with parallel perceptron can achieve 8-fold faster training speed than traditional structured perceptron methods when using 10 threads , and with no loss at all in accuracy .", "topics": ["time complexity", "natural language processing"]}
{"title": "neural activation constellations : unsupervised part model discovery with convolutional networks", "abstract": "part models of object categories are essential for challenging recognition tasks , where differences in categories are subtle and only reflected in appearances of small parts of the object . we present an approach that is able to learn part models in a completely unsupervised manner , without part annotations and even without given bounding boxes during learning . the key idea is to find constellations of neural activation patterns computed using convolutional neural networks . in our experiments , we outperform existing approaches for fine-grained recognition on the cub200-2011 , na birds , oxford pets , and oxford flowers dataset in case no part or bounding box annotations are available and achieve state-of-the-art performance for the stanford dog dataset . we also show the benefits of neural constellation models as a data augmentation technique for fine-tuning . furthermore , our paper unites the areas of generic and fine-grained classification , since our approach is suitable for both scenarios . the source code of our method is available online at http : //www.inf-cv.uni-jena.de/part_discovery", "topics": ["unsupervised learning"]}
{"title": "thresholding bandits with augmented ucb", "abstract": "in this paper we propose the augmented-ucb ( augucb ) algorithm for a fixed-budget version of the thresholding bandit problem ( tbp ) , where the objective is to identify a set of arms whose quality is above a threshold . a key feature of augucb is that it uses both mean and variance estimates to eliminate arms that have been sufficiently explored ; to the best of our knowledge this is the first algorithm to employ such an approach for the considered tbp . theoretically , we obtain an upper bound on the loss ( probability of mis-classification ) incurred by augucb . although ucbev in literature provides a better guarantee , it is important to emphasize that ucbev has access to problem complexity ( whose computation requires arms ' mean and variances ) , and hence is not realistic in practice ; this is in contrast to augucb whose implementation does not require any such complexity inputs . we conduct extensive simulation experiments to validate the performance of augucb . through our simulation work , we establish that augucb , owing to its utilization of variance estimates , performs significantly better than the state-of-the-art apt , csar and other non variance-based algorithms .", "topics": ["simulation", "computation"]}
{"title": "on the generalization error bounds of neural networks under diversity-inducing mutual angular regularization", "abstract": "recently diversity-inducing regularization methods for latent variable models ( lvms ) , which encourage the components in lvms to be diverse , have been studied to address several issues involved in latent variable modeling : ( 1 ) how to capture long-tail patterns underlying data ; ( 2 ) how to reduce model complexity without sacrificing expressivity ; ( 3 ) how to improve the interpretability of learned patterns . while the effectiveness of diversity-inducing regularizers such as the mutual angular regularizer has been demonstrated empirically , a rigorous theoretical analysis of them is still missing . in this paper , we aim to bridge this gap and analyze how the mutual angular regularizer ( mar ) affects the generalization performance of supervised lvms . we use neural network ( nn ) as a model instance to carry out the study and the analysis shows that increasing the diversity of hidden units in nn would reduce estimation error and increase approximation error . in addition to theoretical analysis , we also present empirical study which demonstrates that the mar can greatly improve the performance of nn and the empirical observations are in accordance with the theoretical analysis .", "topics": ["matrix regularization", "neural networks"]}
{"title": "online ranking : discrete choice , spearman correlation and other feedback", "abstract": "given a set $ v $ of $ n $ objects , an online ranking system outputs at each time step a full ranking of the set , observes a feedback of some form and suffers a loss . we study the setting in which the ( adversarial ) feedback is an element in $ v $ , and the loss is the position ( 0th , 1st , 2nd ... ) of the item in the outputted ranking . more generally , we study a setting in which the feedback is a subset $ u $ of at most $ k $ elements in $ v $ , and the loss is the sum of the positions of those elements . we present an algorithm of expected regret $ o ( n^ { 3/2 } \\sqrt { tk } ) $ over a time horizon of $ t $ steps with respect to the best single ranking in hindsight . this improves previous algorithms and analyses either by a factor of either $ \\omega ( \\sqrt { k } ) $ , a factor of $ \\omega ( \\sqrt { \\log n } ) $ or by improving running time from quadratic to $ o ( n\\log n ) $ per round . we also prove a matching lower bound . our techniques also imply an improved regret bound for online rank aggregation over the spearman correlation measure , and to other more complex ranking loss functions .", "topics": ["regret ( decision theory )", "time complexity"]}
{"title": "combining convolution and recursive neural networks for sentiment analysis", "abstract": "this paper addresses the problem of sentence-level sentiment analysis . in recent years , convolution and recursive neural networks have been proven to be effective network architecture for sentence-level sentiment analysis . nevertheless , each of them has their own potential drawbacks . for alleviating their weaknesses , we combined convolution and recursive neural networks into a new network architecture . in addition , we employed transfer learning from a large document-level labeled sentiment dataset to improve the word embedding in our models . the resulting models outperform all recent convolution and recursive neural networks . beyond that , our models achieve comparable performance with state-of-the-art systems on stanford sentiment treebank .", "topics": ["neural networks", "convolution"]}
{"title": "cross-domain transfer in reinforcement learning using target apprentice", "abstract": "in this paper , we present a new approach to transfer learning ( tl ) in reinforcement learning ( rl ) for cross-domain tasks . many of the available techniques approach the transfer architecture as a method of speeding up the target task learning . we propose to adapt and reuse the mapped source task optimal-policy directly in related domains . we show the optimal policy from a related source task can be near optimal in target domain provided an adaptive policy accounts for the model error between target and source . the main benefit of this policy augmentation is generalizing policies across multiple related domains without having to re-learn the new tasks . our results show that this architecture leads to better sample efficiency in the transfer , reducing sample complexity of target task learning to target apprentice learning .", "topics": ["reinforcement learning"]}
{"title": "conditional plausibility measures and bayesian networks", "abstract": "a general notion of algebraic conditional plausibility measures is defined . probability measures , ranking functions , possibility measures , and ( under the appropriate definitions ) sets of probability measures can all be viewed as defining algebraic conditional plausibility measures . it is shown that algebraic conditional plausibility measures can be represented using bayesian networks .", "topics": ["bayesian network"]}
{"title": "the complex-valued encoding for dicision-making based on aliasing data", "abstract": "it is proposed a complex valued channel encoding for multidimensional data . the basic approach contains overlapping of complex nonlinear mappings . its development leads to sparse representation of multi-channel data , increasing their dimensions and the distance between the images .", "topics": ["computer vision", "sparse matrix"]}
{"title": "ai safety and reproducibility : establishing robust foundations for the neuroscience of human values", "abstract": "we propose the creation of a systematic effort to identify and replicate key findings in neuroscience and allied fields related to understanding human values . our aim is to ensure that research underpinning the value alignment problem of artificial intelligence has been sufficiently validated to play a role in the design of ai systems .", "topics": ["value ( ethics )", "artificial intelligence"]}
{"title": "myths and legends of the baldwin effect", "abstract": "this position paper argues that the baldwin effect is widely misunderstood by the evolutionary computation community . the misunderstandings appear to fall into two general categories . firstly , it is commonly believed that the baldwin effect is concerned with the synergy that results when there is an evolving population of learning individuals . this is only half of the story . the full story is more complicated and more interesting . the baldwin effect is concerned with the costs and benefits of lifetime learning by individuals in an evolving population . several researchers have focussed exclusively on the benefits , but there is much to be gained from attention to the costs . this paper explains the two sides of the story and enumerates ten of the costs and benefits of lifetime learning by individuals in an evolving population . secondly , there is a cluster of misunderstandings about the relationship between the baldwin effect and lamarckian inheritance of acquired characteristics . the baldwin effect is not lamarckian . a lamarckian algorithm is not better for most evolutionary computing problems than a baldwinian algorithm . finally , lamarckian inheritance is not a better model of memetic ( cultural ) evolution than the baldwin effect .", "topics": ["computation"]}
{"title": "a neural network model with bidirectional whitening", "abstract": "we present here a new model and algorithm which performs an efficient natural gradient descent for multilayer perceptrons . natural gradient descent was originally proposed from a point of view of information geometry , and it performs the steepest descent updates on manifolds in a riemannian space . in particular , we extend an approach taken by the `` whitened neural networks '' model . we make the whitening process not only in feed-forward direction as in the original model , but also in the back-propagation phase . its efficacy is shown by an application of this `` bidirectional whitened neural networks '' model to a handwritten character recognition data ( mnist data ) .", "topics": ["gradient descent", "gradient"]}
{"title": "learning generic sentence representations using convolutional neural networks", "abstract": "we propose a new encoder-decoder approach to learn distributed sentence representations that are applicable to multiple purposes . the model is learned by using a convolutional neural network as an encoder to map an input sentence into a continuous vector , and using a long short-term memory recurrent neural network as a decoder . several tasks are considered , including sentence reconstruction and future sentence prediction . further , a hierarchical encoder-decoder model is proposed to encode a sentence to predict multiple future sentences . by training our models on a large collection of novels , we obtain a highly generic convolutional sentence encoder that performs well in practice . experimental results on several benchmark datasets , and across a broad range of applications , demonstrate the superiority of the proposed model over competing methods .", "topics": ["recurrent neural network", "unsupervised learning"]}
{"title": "an investigation of transformation-based learning in discourse", "abstract": "this paper presents results from the first attempt to apply transformation-based learning to a discourse-level natural language processing task . to address two limitations of the standard algorithm , we developed a monte carlo version of transformation-based learning to make the method tractable for a wider range of problems without degradation in accuracy , and we devised a committee method for assigning confidence measures to tags produced by transformation-based learning . the paper describes these advances , presents experimental evidence that transformation-based learning is as effective as alternative approaches ( such as decision trees and n-grams ) for a discourse task called dialogue act tagging , and argues that transformation-based learning has desirable features that make it particularly appealing for the dialogue act tagging task .", "topics": ["natural language processing"]}
{"title": "hierarchy of scales in language dynamics", "abstract": "methods and insights from statistical physics are finding an increasing variety of applications where one seeks to understand the emergent properties of a complex interacting system . one such area concerns the dynamics of language at a variety of levels of description , from the behaviour of individual agents learning simple artificial languages from each other , up to changes in the structure of languages shared by large groups of speakers over historical timescales . in this colloquium , we survey a hierarchy of scales at which language and linguistic behaviour can be described , along with the main progress in understanding that has been made at each of them -- -much of which has come from the statistical physics community . we argue that future developments may arise by linking the different levels of the hierarchy together in a more coherent fashion , in particular where this allows more effective use of rich empirical data sets .", "topics": ["artificial intelligence"]}
{"title": "3d zigzag for multislicing , multiband and video processing", "abstract": "we present a 3d zigzag rafter ( first in literature ) which allows us to obtain the exact sequence of spectral components after application of discrete cosine transform 3d ( dct-2d ) over a cube . such cube represents part of a video or eventually a group of images such as multislicing ( e.g . , magnetic resonance or computed tomography imaging ) and multi or hyperspectral imagery ( optical satellites ) . besides , we present a new version of the traditional 2d zigzag , including the case of rectangular blocks . finally , all the attached code is done in matlab , and that code serves both blocks of pixels or blocks of blocks .", "topics": ["pixel"]}
{"title": "learning probabilistic systems from tree samples", "abstract": "we consider the problem of learning a non-deterministic probabilistic system consistent with a given finite set of positive and negative tree samples . consistency is defined with respect to strong simulation conformance . we propose learning algorithms that use traditional and a new `` stochastic '' state-space partitioning , the latter resulting in the minimum number of states . we then use them to solve the problem of `` active learning '' , that uses a knowledgeable teacher to generate samples as counterexamples to simulation equivalence queries . we show that the problem is undecidable in general , but that it becomes decidable under a suitable condition on the teacher which comes naturally from the way samples are generated from failed simulation checks . the latter problem is shown to be undecidable if we impose an additional condition on the learner to always conjecture a `` minimum state '' hypothesis . we therefore propose a semi-algorithm using stochastic partitions . finally , we apply the proposed ( semi- ) algorithms to infer intermediate assumptions in an automated assume-guarantee verification framework for probabilistic systems .", "topics": ["simulation"]}
{"title": "designing effective inter-pixel information flow for natural image matting", "abstract": "we present a novel , purely affinity-based natural image matting algorithm . our method relies on carefully defined pixel-to-pixel connections that enable effective use of information available in the image . we control the information flow from the known-opacity regions into the unknown region , as well as within the unknown region itself , by utilizing multiple definitions of pixel affinities . among other forms of information flow , we introduce color-mixture flow , which builds upon local linear embedding and effectively encapsulates the relation between different pixel opacities . our resulting novel linear system formulation can be solved in closed-form and is robust against several fundamental challenges of natural matting such as holes and remote intricate structures . our evaluation using the alpha matting benchmark suggests a significant performance improvement over the current methods . while our method is primarily designed as a standalone matting tool , we show that it can also be used for regularizing mattes obtained by sampling-based methods . we extend our formulation to layer color estimation and show that the use of multiple channels of flow increases the layer color quality . we also demonstrate our performance in green-screen keying and further analyze the characteristics of the affinities used in our method .", "topics": ["sampling ( signal processing )", "pixel"]}
{"title": "genetic algorithms in time-dependent environments", "abstract": "the influence of time-dependent fitnesses on the infinite population dynamics of simple genetic algorithms ( without crossover ) is analyzed . based on general arguments , a schematic phase diagram is constructed that allows one to characterize the asymptotic states in dependence on the mutation rate and the time scale of changes . furthermore , the notion of regular changes is raised for which the population can be shown to converge towards a generalized quasispecies . based on this , error thresholds and an optimal mutation rate are approximately calculated for a generational genetic algorithm with a moving needle-in-the-haystack landscape . the so found phase diagram is fully consistent with our general considerations .", "topics": ["interaction"]}
{"title": "bounds on the number of measurements for reliable compressive classification", "abstract": "this paper studies the classification of high-dimensional gaussian signals from low-dimensional noisy , linear measurements . in particular , it provides upper bounds ( sufficient conditions ) on the number of measurements required to drive the probability of misclassification to zero in the low-noise regime , both for random measurements and designed ones . such bounds reveal two important operational regimes that are a function of the characteristics of the source : i ) when the number of classes is less than or equal to the dimension of the space spanned by signals in each class , reliable classification is possible in the low-noise regime by using a one-vs-all measurement design ; ii ) when the dimension of the spaces spanned by signals in each class is lower than the number of classes , reliable classification is guaranteed in the low-noise regime by using a simple random measurement design . simulation results both with synthetic and real data show that our analysis is sharp , in the sense that it is able to gauge the number of measurements required to drive the misclassification probability to zero in the low-noise regime .", "topics": ["synthetic data", "simulation"]}
{"title": "structured output learning with abstention : application to accurate opinion prediction", "abstract": "motivated by supervised opinion analysis , we propose a novel framework devoted to structured output learning with abstention ( sola ) . the structure prediction model is able to abstain from predicting some labels in the structured output at a cost chosen by the user in a flexible way . for that purpose , we decompose the problem into the learning of a pair of predictors , one devoted to structured abstention and the other , to structured output prediction . to compare fully labeled training data with predictions potentially containing abstentions , we define a wide class of asymmetric abstention-aware losses . learning is achieved by surrogate regression in an appropriate feature space while prediction with abstention is performed by solving a new pre-image problem . thus , sola extends recent ideas about structured output prediction via surrogate problems and calibration theory and enjoys statistical guarantees on the resulting excess risk . instantiated on a hierarchical abstention-aware loss , sola is shown to be relevant for fine-grained opinion mining and gives state-of-the-art results on this task . moreover , the abstention-aware representations can be used to competitively predict user-review ratings based on a sentence-level opinion predictor .", "topics": ["test set", "feature vector"]}
{"title": "the computational complexity of structure-based causality", "abstract": "halpern and pearl introduced a definition of actual causality ; eiter and lukasiewicz showed that computing whether x=x is a cause of y=y is np-complete in binary models ( where all variables can take on only two values ) and\\ sigma_2^p-complete in general models . in the final version of their paper , halpern and pearl slightly modified the definition of actual cause , in order to deal with problems pointed by hopkins and pearl . as we show , this modification has a nontrivial impact on the complexity of computing actual cause . to characterize the complexity , a new family d_k^p , k= 1 , 2 , 3 , ... , of complexity classes is introduced , which generalizes the class dp introduced by papadimitriou and yannakakis ( dp is just d_1^p ) . % joe2 % we show that the complexity of computing causality is $ \\d_2 $ -complete % under the new definition . chockler and halpern \\citeyear { ch04 } extended the we show that the complexity of computing causality under the updated definition is $ d_2^p $ -complete . chockler and halpern extended the definition of causality by introducing notions of responsibility and blame . the complexity of determining the degree of responsibility and blame using the original definition of causality was completely characterized . again , we show that changing the definition of causality affects the complexity , and completely characterize it using the updated definition .", "topics": ["value ( ethics )", "computational complexity theory"]}
{"title": "behaviour-based knowledge systems : an epigenetic path from behaviour to knowledge", "abstract": "in this paper we expose the theoretical background underlying our current research . this consists in the development of behaviour-based knowledge systems , for closing the gaps between behaviour-based and knowledge-based systems , and also between the understandings of the phenomena they model . we expose the requirements and stages for developing behaviour-based knowledge systems and discuss their limits . we believe that these are necessary conditions for the development of higher order cognitive capacities , in artificial and natural cognitive systems .", "topics": ["artificial intelligence"]}
{"title": "fast single image super-resolution", "abstract": "this paper addresses the problem of single image super-resolution ( sr ) , which consists of recovering a high resolution image from its blurred , decimated and noisy version . the existing algorithms for single image sr use different strategies to handle the decimation and blurring operators . in addition to the traditional first-order gradient methods , recent techniques investigate splitting-based methods dividing the sr problem into up-sampling and deconvolution steps that can be easily solved . instead of following this splitting strategy , we propose to deal with the decimation and blurring operators simultaneously by taking advantage of their particular properties in the frequency domain , leading to a new fast sr approach . specifically , an analytical solution can be obtained and implemented efficiently for the gaussian prior or any other regularization that can be formulated into an $ \\ell_2 $ -regularized quadratic model , i.e . , an $ \\ell_2 $ - $ \\ell_2 $ optimization problem . furthermore , the flexibility of the proposed sr scheme is shown through the use of various priors/regularizations , ranging from generic image priors to learning-based approaches . in the case of non-gaussian priors , we show how the analytical solution derived from the gaussian case can be embedded intotraditional splitting frameworks , allowing the computation cost of existing algorithms to be decreased significantly . simulation results conducted on several images with different priors illustrate the effectiveness of our fast sr approach compared with the existing techniques .", "topics": ["sampling ( signal processing )", "matrix regularization"]}
{"title": "planogram compliance checking based on detection of recurring patterns", "abstract": "in this paper , a novel method for automatic planogram compliance checking in retail chains is proposed without requiring product template images for training . product layout is extracted from an input image by means of unsupervised recurring pattern detection and matched via graph matching with the expected product layout specified by a planogram to measure the level of compliance . a divide and conquer strategy is employed to improve the speed . specifically , the input image is divided into several regions based on the planogram . recurring patterns are detected in each region respectively and then merged together to estimate the product layout . experimental results on real data have verified the efficacy of the proposed method . compared with a template-based method , higher accuracies are achieved by the proposed method over a wide range of products .", "topics": ["unsupervised learning"]}
{"title": "deep neural networks for bot detection", "abstract": "the problem of detecting bots , automated social media accounts governed by software but disguising as human users , has strong implications . for example , bots have been used to sway political elections by distorting online discourse , to manipulate the stock market , or to push anti-vaccine conspiracy theories that caused health epidemics . most techniques proposed to date detect bots at the account level , by processing large amount of social media posts , and leveraging information from network structure , temporal dynamics , sentiment analysis , etc . in this paper , we propose a deep neural network based on contextual long short-term memory ( lstm ) architecture that exploits both content and metadata to detect bots at the tweet level : contextual features are extracted from user metadata and fed as auxiliary input to lstm deep nets processing the tweet text . another contribution that we make is proposing a technique based on synthetic minority oversampling to generate a large labeled dataset , suitable for deep nets training , from a minimal amount of labeled data ( roughly 3,000 examples of sophisticated twitter bots ) . we demonstrate that , from just one single tweet , our architecture can achieve high classification accuracy ( auc > 96 % ) in separating bots from humans . we apply the same architecture to account-level bot detection , achieving nearly perfect classification accuracy ( auc > 99 % ) . our system outperforms previous state of the art while leveraging a small and interpretable set of features yet requiring minimal training data .", "topics": ["test set", "synthetic data"]}
{"title": "exploiting compositionality to explore a large space of model structures", "abstract": "the recent proliferation of richly structured probabilistic models raises the question of how to automatically determine an appropriate model for a dataset . we investigate this question for a space of matrix decomposition models which can express a variety of widely used models from unsupervised learning . to enable model selection , we organize these models into a context-free grammar which generates a wide variety of structures through the compositional application of a few simple rules . we use our grammar to generically and efficiently infer latent components and estimate predictive likelihood for nearly 2500 structures using a small toolbox of reusable algorithms . using a greedy search over our grammar , we automatically choose the decomposition structure from raw data by evaluating only a small fraction of all models . the proposed method typically finds the correct structure for synthetic data and backs off gracefully to simpler models under heavy noise . it learns sensible structures for datasets as diverse as image patches , motion capture , 20 questions , and u.s. senate votes , all using exactly the same code .", "topics": ["unsupervised learning", "synthetic data"]}
{"title": "optimization with parity constraints : from binary codes to discrete integration", "abstract": "many probabilistic inference tasks involve summations over exponentially large sets . recently , it has been shown that these problems can be reduced to solving a polynomial number of map inference queries for a model augmented with randomly generated parity constraints . by exploiting a connection with max-likelihood decoding of binary codes , we show that these optimizations are computationally hard . inspired by iterative message passing decoding algorithms , we propose an integer linear programming ( ilp ) formulation for the problem , enhanced with new sparsification techniques to improve decoding performance . by solving the ilp through a sequence of lp relaxations , we get both lower and upper bounds on the partition function , which hold with high probability and are much tighter than those obtained with variational methods .", "topics": ["calculus of variations", "polynomial"]}
{"title": "a radically new theory of how the brain represents and computes with probabilities", "abstract": "the brain is believed to implement probabilistic reasoning and to represent information via population , or distributed , coding . most previous population-based probabilistic ( ppc ) theories share several basic properties : 1 ) continuous-valued neurons ; 2 ) fully ( densely ) -distributed codes , i.e . , all ( most ) units participate in every code ; 3 ) graded synapses ; 4 ) rate coding ; 5 ) units have innate unimodal tuning functions ( tfs ) ; 6 ) intrinsically noisy units ; and 7 ) noise/correlation is considered harmful . we present a radically different theory that assumes : 1 ) binary units ; 2 ) only a small subset of units , i.e . , a sparse distributed representation ( sdr ) ( cell assembly ) , comprises any individual code ; 3 ) binary synapses ; 4 ) signaling formally requires only single ( i.e . , first ) spikes ; 5 ) units initially have completely flat tfs ( all weights zero ) ; 6 ) units are far less intrinsically noisy than traditionally thought ; rather 7 ) noise is a resource generated/used to cause similar inputs to map to similar codes , controlling a tradeoff between storage capacity and embedding the input space statistics in the pattern of intersections over stored codes , epiphenomenally determining correlation patterns across neurons . the theory , sparsey , was introduced 20+ years ago as a canonical cortical circuit/algorithm model achieving efficient sequence learning/recognition , but not elaborated as an alternative to ppc theories . here , we show that : a ) the active sdr simultaneously represents both the most similar/likely input and the entire ( coarsely-ranked ) similarity likelihood/distribution over all stored inputs ( hypotheses ) ; and b ) given an input , the sdr code selection algorithm , which underlies both learning and inference , updates both the most likely hypothesis and the entire likelihood distribution ( cf . belief update ) with a number of steps that remains constant as the number of stored items increases .", "topics": ["sparse matrix"]}
{"title": "template-based matching using weight maps", "abstract": "template matching is one of the most prevalent pattern recognition methods worldwide . it has found uses in most visual concept detection fields . in this work , we investigate methods for improving template matching by adjusting the weights of different regions of the template . we compare several weight maps and test the methods using the feret face test set in the context of human eye detection .", "topics": ["test set", "map"]}
{"title": "from community detection to community profiling", "abstract": "most existing community-related studies focus on detection , which aim to find the community membership for each user from user friendship links . however , membership alone , without a complete profile of what a community is and how it interacts with other communities , has limited applications . this motivates us to consider systematically profiling the communities and thereby developing useful community-level applications . in this paper , we for the first time formalize the concept of community profiling . with rich user information on the network , such as user published content and user diffusion links , we characterize a community in terms of both its internal content profile and external diffusion profile . the difficulty of community profiling is often underestimated . we novelly identify three unique challenges and propose a joint community profiling and detection ( cpd ) model to address them accordingly . we also contribute a scalable inference algorithm , which scales linearly with the data size and it is easily parallelizable . we evaluate cpd on large-scale real-world data sets , and show that it is significantly better than the state-of-the-art baselines in various tasks .", "topics": ["baseline ( configuration management )", "scalability"]}
{"title": "learning the roots of visual domain shift", "abstract": "in this paper we focus on the spatial nature of visual domain shift , attempting to learn where domain adaptation originates in each given image of the source and target set . we borrow concepts and techniques from the cnn visualization literature , and learn domainnes maps able to localize the degree of domain specificity in images . we derive from these maps features related to different domainnes levels , and we show that by considering them as a preprocessing step for a domain adaptation algorithm , the final classification performance is strongly improved . combined with the whole image representation , these features provide state of the art results on the office dataset .", "topics": ["map"]}
{"title": "variations of the turing test in the age of internet and virtual reality", "abstract": "inspired by hofstadter 's coffee-house conversation ( 1982 ) and by the science fiction short story sam by schattschneider ( 1988 ) , we propose and discuss criteria for non-mechanical intelligence . firstly , we emphasize the practical need for such tests in view of massively multiuser online role-playing games ( mmorpgs ) and virtual reality systems like second life . secondly , we demonstrate second life as a useful framework for implementing ( some iterations of ) that test .", "topics": ["simulation", "iteration"]}
{"title": "toward a deep neural approach for knowledge-based ir", "abstract": "this paper tackles the problem of the semantic gap between a document and a query within an ad-hoc information retrieval task . in this context , knowledge bases ( kbs ) have already been acknowledged as valuable means since they allow the representation of explicit relations between entities . however , they do not necessarily represent implicit relations that could be hidden in a corpora . this latter issue is tackled by recent works dealing with deep representation learn ing of texts . with this in mind , we argue that embedding kbs within deep neural architectures supporting documentquery matching would give rise to fine-grained latent representations of both words and their semantic relations . in this paper , we review the main approaches of neural-based document ranking as well as those approaches for latent representation of entities and relations via kbs . we then propose some avenues to incorporate kbs in deep neural approaches for document ranking . more particularly , this paper advocates that kbs can be used either to support enhanced latent representations of queries and documents based on both distributional and relational semantics or to serve as a semantic translator between their latent distributional representations .", "topics": ["feature learning", "entity"]}
{"title": "clustering with deep learning : taxonomy and new methods", "abstract": "clustering is a fundamental machine learning method . the quality of its results is dependent on the data distribution . for this reason , deep neural networks can be used for learning better representations of the data . in this paper , we propose a systematic taxonomy for clustering with deep learning , in addition to a review of methods from the field . based on our taxonomy , creating new methods is more straightforward . we also propose a new approach which is built on the taxonomy and surpasses some of the limitations of some previous work . our experimental evaluation on image datasets shows that the method approaches state-of-the-art clustering quality , and performs better in some cases .", "topics": ["cluster analysis"]}
{"title": "toward deeper understanding of neural networks : the power of initialization and a dual view on expressivity", "abstract": "we develop a general duality between neural networks and compositional kernels , striving towards a better understanding of deep learning . we show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space . hence , though the training objective is hard to optimize in the worst case , the initial weights form a good starting point for optimization . our dual view also reveals a pragmatic and aesthetic perspective of neural networks and underscores their expressive power .", "topics": ["approximation algorithm", "nonlinear system"]}
{"title": "sliced wasserstein distance for learning gaussian mixture models", "abstract": "gaussian mixture models ( gmm ) are powerful parametric tools with many applications in machine learning and computer vision . expectation maximization ( em ) is the most popular algorithm for estimating the gmm parameters . however , em guarantees only convergence to a stationary point of the log-likelihood function , which could be arbitrarily worse than the optimal solution . inspired by the relationship between the negative log-likelihood function and the kullback-leibler ( kl ) divergence , we propose an alternative formulation for estimating the gmm parameters using the sliced wasserstein distance , which gives rise to a new algorithm . specifically , we propose minimizing the sliced-wasserstein distance between the mixture model and the data distribution with respect to the gmm parameters . in contrast to the kl-divergence , the energy landscape for the sliced-wasserstein distance is more well-behaved and therefore more suitable for a stochastic gradient descent scheme to obtain the optimal gmm parameters . we show that our formulation results in parameter estimates that are more robust to random initializations and demonstrate that it can estimate high-dimensional data distributions more faithfully than the em algorithm .", "topics": ["optimization problem", "gradient descent"]}
{"title": "trainable and dynamic computing : error backpropagation through physical media", "abstract": "machine learning algorithms , and more in particular neural networks , arguably experience a revolution in terms of performance . currently , the best systems we have for speech recognition , computer vision and similar problems are based on neural networks , trained using the half-century old backpropagation algorithm . despite the fact that neural networks are a form of analog computers , they are still implemented digitally for reasons of convenience and availability . in this paper we demonstrate how we can design physical linear dynamic systems with non-linear feedback as a generic platform for dynamic , neuro-inspired analog computing . we show that a crucial advantage of this setup is that the error backpropagation can be performed physically as well , which greatly speeds up the optimisation process . as we show in this paper , using one experimentally validated and one conceptual example , such systems may be the key to providing a relatively straightforward mechanism for constructing highly scalable , fully dynamic analog computers .", "topics": ["mathematical optimization", "nonlinear system"]}
{"title": "auto-pooling : learning to improve invariance of image features from image sequences", "abstract": "learning invariant representations from images is one of the hardest challenges facing computer vision . spatial pooling is widely used to create invariance to spatial shifting , but it is restricted to convolutional models . in this paper , we propose a novel pooling method that can learn soft clustering of features from image sequences . it is trained to improve the temporal coherence of features , while keeping the information loss at minimum . our method does not use spatial information , so it can be used with non-convolutional models too . experiments on images extracted from natural videos showed that our method can cluster similar features together . when trained by convolutional features , auto-pooling outperformed traditional spatial pooling on an image classification task , even though it does not use the spatial topology of features .", "topics": ["cluster analysis", "computer vision"]}
{"title": "sparse bayesian methods for low-rank matrix estimation", "abstract": "recovery of low-rank matrices has recently seen significant activity in many areas of science and engineering , motivated by recent theoretical results for exact reconstruction guarantees and interesting practical applications . a number of methods have been developed for this recovery problem . however , a principled method for choosing the unknown target rank is generally not provided . in this paper , we present novel recovery algorithms for estimating low-rank matrices in matrix completion and robust principal component analysis based on sparse bayesian learning ( sbl ) principles . starting from a matrix factorization formulation and enforcing the low-rank constraint in the estimates as a sparsity constraint , we develop an approach that is very effective in determining the correct rank while providing high recovery performance . we provide connections with existing methods in other similar problems and empirical results and comparisons with current state-of-the-art methods that illustrate the effectiveness of this approach .", "topics": ["sparse matrix"]}
{"title": "learning adversary behavior in security games : a pac model perspective", "abstract": "recent applications of stackelberg security games ( ssg ) , from wildlife crime to urban crime , have employed machine learning tools to learn and predict adversary behavior using available data about defender-adversary interactions . given these recent developments , this paper commits to an approach of directly learning the response function of the adversary . using the pac model , this paper lays a firm theoretical foundation for learning in ssgs ( e.g . , theoretically answer questions about the numbers of samples required to learn adversary behavior ) and provides utility guarantees when the learned adversary model is used to plan the defender 's strategy . the paper also aims to answer practical questions such as how much more data is needed to improve an adversary model 's accuracy . additionally , we explain a recently observed phenomenon that prediction accuracy of learned adversary behavior is not enough to discover the utility maximizing defender strategy . we provide four main contributions : ( 1 ) a pac model of learning adversary response functions in ssgs ; ( 2 ) pac-model analysis of the learning of key , existing bounded rationality models in ssgs ; ( 3 ) an entirely new approach to adversary modeling based on a non-parametric class of response functions with pac-model analysis and ( 4 ) identification of conditions under which computing the best defender strategy against the learned adversary behavior is indeed the optimal strategy . finally , we conduct experiments with real-world data from a national park in uganda , showing the benefit of our new adversary modeling approach and verification of our pac model predictions .", "topics": ["interaction"]}
{"title": "mathematical foundations for designing and development of intelligent systems of information analysis", "abstract": "this article is an attempt to combine different ways of working with sets of objects and their classes for designing and development of artificial intelligent systems ( ais ) of analysis information , using object-oriented programming ( oop ) . this paper contains analysis of basic concepts of oop and their relation with set theory and artificial intelligence ( ai ) . process of sets and multisets creation from different sides , in particular mathematical set theory , oop and ai is considered . definition of object and its properties , homogeneous and inhomogeneous classes of objects , set of objects , multiset of objects and constructive methods of their creation and classification are proposed . in addition , necessity of some extension of existing oop tools for the purpose of practical implementation ais of analysis information , using proposed approach , is shown .", "topics": ["artificial intelligence"]}
{"title": "natural language inference over interaction space", "abstract": "natural language inference ( nli ) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis . we introduce interactive inference network ( iin ) , a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space . we show that an interaction tensor ( attention weight ) contains semantic information to solve natural language inference , and a denser interaction tensor contains richer semantic information . one instance of such architecture , densely interactive inference network ( diin ) , demonstrates the state-of-the-art performance on large scale nli copora and large-scale nli alike corpus . it 's noteworthy that diin achieve a greater than 20 % error reduction on the challenging multi-genre nli ( multinli ) dataset with respect to the strongest published system .", "topics": ["high- and low-level", "natural language"]}
{"title": "an evaluation of two alternatives to minimax", "abstract": "in the field of artificial intelligence , traditional approaches to choosing moves in games involve the we of the minimax algorithm . however , recent research results indicate that minimizing may not always be the best approach . in this paper we summarize the results of some measurements on several model games with several different evaluation functions . these measurements , which are presented in detail in [ npt ] , show that there are some new algorithms that can make significantly better use of evaluation function values than the minimax algorithm does .", "topics": ["artificial intelligence"]}
{"title": "embedding label structures for fine-grained feature representation", "abstract": "recent algorithms in convolutional neural networks ( cnn ) considerably advance the fine-grained image classification , which aims to differentiate subtle differences among subordinate classes . however , previous studies have rarely focused on learning a fined-grained and structured feature representation that is able to locate similar images at different levels of relevance , e.g . , discovering cars from the same make or the same model , both of which require high precision . in this paper , we propose two main contributions to tackle this problem . 1 ) a multi-task learning framework is designed to effectively learn fine-grained feature representations by jointly optimizing both classification and similarity constraints . 2 ) to model the multi-level relevance , label structures such as hierarchy or shared attributes are seamlessly embedded into the framework by generalizing the triplet loss . extensive and thorough experiments have been conducted on three fine-grained datasets , i.e . , the stanford car , the car-333 , and the food datasets , which contain either hierarchical labels or shared attributes . our proposed method has achieved very competitive performance , i.e . , among state-of-the-art classification accuracy . more importantly , it significantly outperforms previous fine-grained feature representations for image retrieval at different levels of relevance .", "topics": ["computer vision", "relevance"]}
{"title": "analysis of nuclear norm regularization for full-rank matrix completion", "abstract": "in this paper , we provide a theoretical analysis of the nuclear-norm regularized least squares for full-rank matrix completion . although similar formulations have been examined by previous studies , their results are unsatisfactory because only additive upper bounds are provided . under the assumption that the top eigenspaces of the target matrix are incoherent , we derive a relative upper bound for recovering the best low-rank approximation of the unknown matrix . our relative upper bound is tighter than previous additive bounds of other methods if the mass of the target matrix is concentrated on its top eigenspaces , and also implies perfect recovery if it is low-rank . the analysis is built upon the optimality condition of the regularized formulation and existing guarantees for low-rank matrix completion . to the best of our knowledge , this is first time such a relative bound is proved for the regularized formulation of matrix completion .", "topics": ["matrix regularization"]}
{"title": "external evaluation of event extraction classifiers for automatic pathway curation : an extended study of the mtor pathway", "abstract": "this paper evaluates the impact of various event extraction systems on automatic pathway curation using the popular mtor pathway . we quantify the impact of training data sets as well as different machine learning classifiers and show that some improve the quality of automatically extracted pathways .", "topics": ["test set"]}
{"title": "applying reliability metrics to co-reference annotation", "abstract": "studies of the contextual and linguistic factors that constrain discourse phenomena such as reference are coming to depend increasingly on annotated language corpora . in preparing the corpora , it is important to evaluate the reliability of the annotation , but methods for doing so have not been readily available . in this report , i present a method for computing reliability of coreference annotation . first i review a method for applying the information retrieval metrics of recall and precision to coreference annotation proposed by marc vilain and his collaborators . i show how this method makes it possible to construct contingency tables for computing cohen 's kappa , a familiar reliability metric . by comparing recall and precision to reliability on the same data sets , i also show that recall and precision can be misleadingly high . because kappa factors out chance agreement among coders , it is a preferable measure for developing annotated corpora where no pre-existing target annotation exists .", "topics": ["text corpus", "coefficient"]}
{"title": "fast classification using sparse decision dags", "abstract": "in this paper we propose an algorithm that builds sparse decision dags ( directed acyclic graphs ) from a list of base classifiers provided by an external learning method such as adaboost . the basic idea is to cast the dag design task as a markov decision process . each instance can decide to use or to skip each base classifier , based on the current state of the classifier being built . the result is a sparse decision dag where the base classifiers are selected in a data-dependent way . the method has a single hyperparameter with a clear semantics of controlling the accuracy/speed trade-off . the algorithm is competitive with state-of-the-art cascade detectors on three object-detection benchmarks , and it clearly outperforms them when there is a small number of base classifiers . unlike cascades , it is also readily applicable for multi-class classification . using the multi-class setup , we show on a benchmark web page ranking data set that we can significantly improve the decision speed without harming the performance of the ranker .", "topics": ["sparse matrix"]}
{"title": "theory of the superposition principle for randomized connectionist representations in neural networks", "abstract": "to understand cognitive reasoning in the brain , it has been proposed that symbols and compositions of symbols are represented by activity patterns ( vectors ) in a large population of neurons . formal models implementing this idea [ plate 2003 ] , [ kanerva 2009 ] , [ gayler 2003 ] , [ eliasmith 2012 ] include a reversible superposition operation for representing with a single vector an entire set of symbols or an ordered sequence of symbols . if the representation space is high-dimensional , large sets of symbols can be superposed and individually retrieved . however , crosstalk noise limits the accuracy of retrieval and information capacity . to understand information processing in the brain and to design artificial neural systems for cognitive reasoning , a theory of this superposition operation is essential . here , such a theory is presented . the superposition operations in different existing models are mapped to linear neural networks with unitary recurrent matrices , in which retrieval accuracy can be analyzed by a single equation . we show that networks representing information in superposition can achieve a channel capacity of about half a bit per neuron , a significant fraction of the total available entropy . going beyond existing models , superposition operations with recency effects are proposed that avoid catastrophic forgetting when representing the history of infinite data streams . these novel models correspond to recurrent networks with non-unitary matrices or with nonlinear neurons , and can be analyzed and optimized with an extension of our theory .", "topics": ["nonlinear system"]}
{"title": "an efficient character-level neural machine translation", "abstract": "neural machine translation aims at building a single large neural network that can be trained to maximize translation performance . the encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems on the task of english-to-french translation . however , the use of large vocabulary becomes the bottleneck in both training and improving the performance . in this paper , we propose an efficient architecture to train a deep character-level neural machine translation by introducing a decimator and an interpolator . the decimator is used to sample the source sequence before encoding while the interpolator is used to resample after decoding . such a deep model has two major advantages . it avoids the large vocabulary issue radically ; at the same time , it is much faster and more memory-efficient in training than conventional character-based models . more interestingly , our model is able to translate the misspelled word like human beings .", "topics": ["machine translation", "encoder"]}
{"title": "artwork creation by a cognitive architecture integrating computational creativity and dual process approaches", "abstract": "the paper proposes a novel cognitive architecture ( ca ) for computational creativity based on the psi model and on the mechanisms inspired by dual process theories of reasoning and rationality . in recent years , many cognitive models have focused on dual process theories to better describe and implement complex cognitive skills in artificial agents , but creativity has been approached only at a descriptive level . in previous works we have described various modules of the cognitive architecture that allows a robot to execute creative paintings . by means of dual process theories we refine some relevant mechanisms to obtain artworks , and in particular we explain details about the resolution level of the ca dealing with different strategies of access to the long term memory ( ltm ) and managing the interaction between s1 and s2 processes of the dual process theory . the creative process involves both divergent and convergent processes in either implicit or explicit manner . this leads to four activities ( exploratory , reflective , tacit , and analytic ) that , triggered by urges and motivations , generate creative acts . these creative acts exploit both the ltm and the wm in order to make novel substitutions to a perceived image by properly mixing parts of pictures coming from different domains . the paper highlights the role of the interaction between s1 and s2 processes , modulated by the resolution level , which focuses the attention of the creative agent by broadening or narrowing the exploration of novel solutions , or even drawing the solution from a set of already made associations . an example of artificial painter is described in some experimentations by using a robotic platform .", "topics": ["artificial intelligence", "robot"]}
{"title": "semi-supervised zero-shot learning by a clustering-based approach", "abstract": "in some of object recognition problems , labeled data may not be available for all categories . zero-shot learning utilizes auxiliary information ( also called signatures ) describing each category in order to find a classifier that can recognize samples from categories with no labeled instance . in this paper , we propose a novel semi-supervised zero-shot learning method that works on an embedding space corresponding to abstract deep visual features . we seek a linear transformation on signatures to map them onto the visual features , such that the mapped signatures of the seen classes are close to labeled samples of the corresponding classes and unlabeled data are also close to the mapped signatures of one of the unseen classes . we use the idea that the rich deep visual features provide a representation space in which samples of each class are usually condensed in a cluster . the effectiveness of the proposed method is demonstrated through extensive experiments on four public benchmarks improving the state-of-the-art prediction accuracy on three of them .", "topics": ["cluster analysis"]}
{"title": "reservoir of diverse adaptive learners and stacking fast hoeffding drift detection methods for evolving data streams", "abstract": "the last decade has seen a surge of interest in adaptive learning algorithms for data stream classification , with applications ranging from predicting ozone level peaks , learning stock market indicators , to detecting computer security violations . in addition , a number of methods have been developed to detect concept drifts in these streams . consider a scenario where we have a number of classifiers with diverse learning styles and different drift detectors . intuitively , the current 'best ' ( classifier , detector ) pair is application dependent and may change as a result of the stream evolution . our research builds on this observation . we introduce the $ \\mbox { tornado } $ framework that implements a reservoir of diverse classifiers , together with a variety of drift detection algorithms . in our framework , all ( classifier , detector ) pairs proceed , in parallel , to construct models against the evolving data streams . at any point in time , we select the pair which currently yields the best performance . we further incorporate two novel stacking-based drift detection methods , namely the $ \\mbox { fhddms } $ and $ \\mbox { fhddms } _ { add } $ approaches . the experimental evaluation confirms that the current 'best ' ( classifier , detector ) pair is not only heavily dependent on the characteristics of the stream , but also that this selection evolves as the stream flows . further , our $ \\mbox { fhddms } $ variants detect concept drifts accurately in a timely fashion while outperforming the state-of-the-art .", "topics": ["statistical classification"]}
{"title": "predicting gaze in egocentric video by learning task-dependent attention transition", "abstract": "we present a new computational model for gaze prediction in egocentric videos by exploring patterns in temporal shift of gaze fixations ( attention transition ) that are dependent on egocentric manipulation tasks . our assumption is that the high-level context of how a task is completed in a certain way has a strong influence on attention transition and should be modeled for gaze prediction in natural dynamic scenes . specifically , we propose a hybrid model based on deep neural networks which integrates task-dependent attention transition with bottom-up saliency prediction . in particular , the task-dependent attention transition is learned with a recurrent neural network to exploit the temporal context of gaze fixations , e.g . looking at a cup after moving gaze away from a grasped bottle . experiments on public egocentric activity datasets show that our model significantly outperforms state-of-the-art gaze prediction methods and is able to learn meaningful transition of human attention .", "topics": ["recurrent neural network", "high- and low-level"]}
{"title": "reinforcement learning with parameterized actions", "abstract": "we introduce a model-free algorithm for learning in markov decision processes with parameterized actions-discrete actions with continuous parameters . at each step the agent must select both which action to use and which parameters to use with that action . we introduce the q-pamdp algorithm for learning in these domains , show that it converges to a local optimum , and compare it to direct policy search in the goal-scoring and platform domains .", "topics": ["mathematical optimization", "reinforcement learning"]}
{"title": "efficient preconditioning for noisy separable nmfs by successive projection based low-rank approximations", "abstract": "the successive projection algorithm ( spa ) can quickly solve a nonnegative matrix factorization problem under a separability assumption . even if noise is added to the problem , spa is robust as long as the perturbations caused by the noise are small . in particular , robustness against noise should be high when handling the problems arising from real applications . the preconditioner proposed by gillis and vavasis ( 2015 ) makes it possible to enhance the noise robustness of spa . meanwhile , an additional computational cost is required . the construction of the preconditioner contains a step to compute the top- $ k $ truncated singular value decomposition of an input matrix . it is known that the decomposition provides the best rank- $ k $ approximation to the input matrix ; in other words , a matrix with the smallest approximation error among all matrices of rank less than $ k $ . this step is an obstacle to an efficient implementation of the preconditioned spa . to address the cost issue , we propose a modification of the algorithm for constructing the preconditioner . although the original algorithm uses the best rank- $ k $ approximation , instead of it , our modification uses an alternative . ideally , this alternative should have high approximation accuracy and low computational cost . to ensure this , our modification employs a rank- $ k $ approximation produced by an spa based algorithm . we analyze the accuracy of the approximation and evaluate the computational cost of the algorithm . we then present an empirical study revealing the actual performance of the spa based rank- $ k $ approximation algorithm and the modified preconditioned spa .", "topics": ["approximation algorithm", "approximation"]}
{"title": "recurrent pixel embedding for instance grouping", "abstract": "we introduce a differentiable , end-to-end trainable framework for solving pixel-level grouping problems such as instance segmentation consisting of two novel components . first , we regress pixels into a hyper-spherical embedding space so that pixels from the same group have high cosine similarity while those from different groups have similarity below a specified margin . we analyze the choice of embedding dimension and margin , relating them to theoretical results on the problem of distributing points uniformly on the sphere . second , to group instances , we utilize a variant of mean-shift clustering , implemented as a recurrent neural network parameterized by kernel bandwidth . this recurrent grouping module is differentiable , enjoys convergent dynamics and probabilistic interpretability . backpropagating the group-weighted loss through this module allows learning to focus on only correcting embedding errors that wo n't be resolved during subsequent clustering . our framework , while conceptually simple and theoretically abundant , is also practically effective and computationally efficient . we demonstrate substantial improvements over state-of-the-art instance segmentation for object proposal generation , as well as demonstrating the benefits of grouping loss for classification tasks such as boundary detection and semantic segmentation .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "image-embodied knowledge representation learning", "abstract": "entity images could provide significant visual information for knowledge representation learning . most conventional methods learn knowledge representations merely from structured triples , ignoring rich visual information extracted from entity images . in this paper , we propose a novel image-embodied knowledge representation learning model ( ikrl ) , where knowledge representations are learned with both triple facts and images . more specifically , we first construct representations for all images of an entity with a neural image encoder . these image representations are then integrated into an aggregated image-based representation via an attention-based method . we evaluate our ikrl models on knowledge graph completion and triple classification . experimental results demonstrate that our models outperform all baselines on both tasks , which indicates the significance of visual information for knowledge representations and the capability of our models in learning knowledge representations with images .", "topics": ["baseline ( configuration management )", "encoder"]}
{"title": "understanding intra-class knowledge inside cnn", "abstract": "convolutional neural network ( cnn ) has been successful in image recognition tasks , and recent works shed lights on how cnn separates different classes with the learned inter-class knowledge through visualization . in this work , we instead visualize the intra-class knowledge inside cnn to better understand how an object class is represented in the fully-connected layers . to invert the intra-class knowledge into more interpretable images , we propose a non-parametric patch prior upon previous cnn visualization models . with it , we show how different `` styles '' of templates for an object class are organized by cnn in terms of location and content , and represented in a hierarchical and ensemble way . moreover , such intra-class knowledge can be used in many interesting applications , e.g . style-based image retrieval and style-based object completion .", "topics": ["computer vision"]}
{"title": "improving multiple object tracking with optical flow and edge preprocessing", "abstract": "in this paper , we present a new method for detecting road users in an urban environment which leads to an improvement in multiple object tracking . our method takes as an input a foreground image and improves the object detection and segmentation . this new image can be used as an input to trackers that use foreground blobs from background subtraction . the first step is to create foreground images for all the frames in an urban video . then , starting from the original blobs of the foreground image , we merge the blobs that are close to one another and that have similar optical flow . the next step is extracting the edges of the different objects to detect multiple objects that might be very close ( and be merged in the same blob ) and to adjust the size of the original blobs . at the same time , we use the optical flow to detect occlusion of objects that are moving in opposite directions . finally , we make a decision on which information we keep in order to construct a new foreground image with blobs that can be used for tracking . the system is validated on four videos of an urban traffic dataset . our method improves the recall and precision metrics for the object detection task compared to the vanilla background subtraction method and improves the clear mot metrics in the tracking tasks for most videos .", "topics": ["image segmentation", "object detection"]}
{"title": "graph-based learning with unbalanced clusters", "abstract": "graph construction is a crucial step in spectral clustering ( sc ) and graph-based semi-supervised learning ( ssl ) . spectral methods applied on standard graphs such as full-rbf , $ \\epsilon $ -graphs and $ k $ -nn graphs can lead to poor performance in the presence of proximal and unbalanced data . this is because spectral methods based on minimizing ratiocut or normalized cut on these graphs tend to put more importance on balancing cluster sizes over reducing cut values . we propose a novel graph construction technique and show that the ratiocut solution on this new graph is able to handle proximal and unbalanced data . our method is based on adaptively modulating the neighborhood degrees in a $ k $ -nn graph , which tends to sparsify neighborhoods in low density regions . our method adapts to data with varying levels of unbalancedness and can be naturally used for small cluster detection . we justify our ideas through limit cut analysis . unsupervised and semi-supervised experiments on synthetic and real data sets demonstrate the superiority of our method .", "topics": ["cluster analysis", "supervised learning"]}
{"title": "provable methods for training neural networks with sparse connectivity", "abstract": "we provide novel guaranteed approaches for training feedforward neural networks with sparse connectivity . we leverage on the techniques developed previously for learning linear networks and show that they can also be effectively adopted to learn non-linear networks . we operate on the moments involving label and the score function of the input , and show that their factorization provably yields the weight matrix of the first layer of a deep network under mild conditions . in practice , the output of our method can be employed as effective initializers for gradient descent .", "topics": ["neural networks", "nonlinear system"]}
{"title": "drawing and analyzing causal dags with dagitty", "abstract": "dagitty is a software for drawing and analyzing causal diagrams , also known as directed acyclic graphs ( dags ) . functions include identification of minimal sufficient adjustment sets for estimating causal effects , diagnosis of insufficient or invalid adjustment via the identification of biasing paths , identification of instrumental variables , and derivation of testable implications . dagitty is provided in the hope that it is useful for researchers and students in epidemiology , sociology , psychology , and other empirical disciplines . the software should run in any web browser that supports modern javascript , html , and svg . this is the user manual for dagitty version 2.3 . the manual is updated with every release of a new stable version . dagitty is available at dagitty.net .", "topics": ["causality"]}
{"title": "the artists who forged themselves : detecting creativity in art", "abstract": "creativity and the understanding of cognitive processes involved in the creative process are relevant to all of human activities . comprehension of creativity in the arts is of special interest due to the involvement of many scientific and non scientific disciplines . using digital representation of paintings , we show that creative process in painting art may be objectively recognized within the mathematical framework of self organization , a process characteristic of nonlinear dynamic systems and occurring in natural and social sciences . unlike the artist identification process or the recognition of forgery , which presupposes the knowledge of the original work , our method requires no prior knowledge on the originality of the work of art . the original paintings are recognized as realizations of the creative process which , in general , is shown to correspond to self-organization of texture features which determine the aesthetic complexity of the painting . the method consists of the wavelet based statistical digital image processing and the measure of statistical complexity which represents the minimal ( average ) information necessary for optimal prediction . the statistical complexity is based on the properly defined causal states with optimal predictive properties . two different time concepts related to the works of art are introduced : the internal time and the artistic time . the internal time of the artwork is determined by the span of causal dependencies between wavelet coefficients while the artistic time refers to the internal time during which complexity increases where complexity refers to compositional , aesthetic and structural arrangement of texture features . the method is illustrated by recognizing the original paintings from the copies made by the artists themselves , including the works of the famous surrealist painter ren\\ ' { e } magritte .", "topics": ["image processing", "nonlinear system"]}
{"title": "fighting with the sparsity of synonymy dictionaries", "abstract": "graph-based synset induction methods , such as maxmax and watset , induce synsets by performing a global clustering of a synonymy graph . however , such methods are sensitive to the structure of the input synonymy graph : sparseness of the input dictionary can substantially reduce the quality of the extracted synsets . in this paper , we propose two different approaches designed to alleviate the incompleteness of the input dictionaries . the first one performs a pre-processing of the graph by adding missing edges , while the second one performs a post-processing by merging similar synset clusters . we evaluate these approaches on two datasets for the russian language and discuss their impact on the performance of synset induction methods . finally , we perform an extensive error analysis of each approach and discuss prominent alternative methods for coping with the problem of the sparsity of the synonymy dictionaries .", "topics": ["cluster analysis", "sparse matrix"]}
{"title": "diff-dac : distributed actor-critic for multitask deep reinforcement learning", "abstract": "we propose a multiagent distributed actor-critic algorithm for multitask reinforcement learning ( mrl ) , named diff-dac . the agents are connected , forming a ( possibly sparse ) network . each agent is assigned a task and has access to data from this local task only . during the learning process , the agents are able to communicate some parameters to their neighbors . since the agents incorporate their neighbors ' parameters into their own learning rules , the information is diffused across the network , and they can learn a common policy that generalizes well across all tasks . diff-dac is scalable since the computational complexity and communication overhead per agent grow with the number of neighbors , rather than with the total number of agents . moreover , the algorithm is fully distributed in the sense that agents self-organize , with no need for coordinator node . diff-dac follows an actor-critic scheme where the value function and the policy are approximated with deep neural networks , being able to learn expressive policies from raw data . as a by-product of diff-dac 's derivation from duality theory , we provide novel insights into the standard actor-critic framework , showing that it is actually an instance of the dual ascent method to approximate the solution of a linear program . experiments illustrate the performance of the algorithm in the cart-pole , inverted pendulum , and swing-up cart-pole environments .", "topics": ["approximation algorithm", "computational complexity theory"]}
{"title": "bayesian optimisation for safe navigation under localisation uncertainty", "abstract": "in outdoor environments , mobile robots are required to navigate through terrain with varying characteristics , some of which might significantly affect the integrity of the platform . ideally , the robot should be able to identify areas that are safe for navigation based on its own percepts about the environment while avoiding damage to itself . bayesian optimisation ( bo ) has been successfully applied to the task of learning a model of terrain traversability while guiding the robot through more traversable areas . an issue , however , is that localisation uncertainty can end up guiding the robot to unsafe areas and distort the model being learnt . in this paper , we address this problem and present a novel method that allows bo to consider localisation uncertainty by applying a gaussian process model for uncertain inputs as a prior . we evaluate the proposed method in simulation and in experiments with a real robot navigating over rough terrain and compare it against standard bo methods .", "topics": ["mathematical optimization", "simulation"]}
{"title": "greedy deep dictionary learning", "abstract": "in this work we propose a new deep learning tool called deep dictionary learning . multi-level dictionaries are learnt in a greedy fashion , one layer at a time . this requires solving a simple ( shallow ) dictionary learning problem , the solution to this is well known . we apply the proposed technique on some benchmark deep learning datasets . we compare our results with other deep learning tools like stacked autoencoder and deep belief network ; and state of the art supervised dictionary learning tools like discriminative ksvd and label consistent ksvd . our method yields better results than all .", "topics": ["bayesian network", "dictionary"]}
{"title": "a deep recurrent framework for cleaning motion capture data", "abstract": "we present a deep , bidirectional , recurrent framework for cleaning noisy and incomplete motion capture data . it exploits temporal coherence and joint correlations to infer adaptive filters for each joint in each frame . a single model can be trained to denoise a heterogeneous mix of action types , under substantial amounts of noise . a signal that has both noise and gaps is preprocessed with a second bidirectional network that synthesizes missing frames from surrounding context . the approach handles a wide variety of noise types and long gaps , does not rely on knowledge of the noise distribution , and operates in a streaming setting . we validate our approach through extensive evaluations on noise both in joint angles and in joint positions , and show that it improves upon various alternatives .", "topics": ["noise reduction"]}
{"title": "guided optical flow learning", "abstract": "we study the unsupervised learning of cnns for optical flow estimation using proxy ground truth data . supervised cnns , due to their immense learning capacity , have shown superior performance on a range of computer vision problems including optical flow prediction . they however require the ground truth flow which is usually not accessible except on limited synthetic data . without the guidance of ground truth optical flow , unsupervised cnns often perform worse as they are naturally ill-conditioned . we therefore propose a novel framework in which proxy ground truth data generated from classical approaches is used to guide the cnn learning . the models are further refined in an unsupervised fashion using an image reconstruction loss . our guided learning approach is competitive with or superior to state-of-the-art approaches on three standard benchmark datasets yet is completely unsupervised and can run in real time .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "vibnn : hardware acceleration of bayesian neural networks", "abstract": "bayesian neural networks ( bnns ) have been proposed to address the problem of model uncertainty in training and inference . by introducing weights associated with conditioned probability distributions , bnns are capable of resolving the overfitting issue commonly seen in conventional neural networks and allow for small-data training , through the variational inference process . frequent usage of gaussian random variables in this process requires a properly optimized gaussian random number generator ( grng ) . the high hardware cost of conventional grng makes the hardware implementation of bnns challenging . in this paper , we propose vibnn , an fpga-based hardware accelerator design for variational inference on bnns . we explore the design space for massive amount of gaussian variable sampling tasks in bnns . specifically , we introduce two high performance gaussian ( pseudo ) random number generators : the ram-based linear feedback gaussian random number generator ( rlf-grng ) , which is inspired by the properties of binomial distribution and linear feedback logics ; and the bayesian neural network-oriented wallace gaussian random number generator . to achieve high scalability and efficient memory access , we propose a deep pipelined accelerator architecture with fast execution and good hardware utilization . experimental results demonstrate that the proposed vibnn implementations on an fpga can achieve throughput of 321,543.4 images/s and energy efficiency upto 52,694.8 images/j while maintaining similar accuracy as its software counterpart .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "argument ranking with categoriser function", "abstract": "recently , ranking-based semantics is proposed to rank-order arguments from the most acceptable to the weakest one ( s ) , which provides a graded assessment to arguments . in general , the ranking on arguments is derived from the strength values of the arguments . categoriser function is a common approach that assigns a strength value to a tree of arguments . when it encounters an argument system with cycles , then the categoriser strength is the solution of the non-linear equations . however , there is no detail about the existence and uniqueness of the solution , and how to find the solution ( if exists ) . in this paper , we will cope with these issues via fixed point technique . in addition , we define the categoriser-based ranking semantics in light of categoriser strength , and investigate some general properties of it . finally , the semantics is shown to satisfy some of the axioms that a ranking-based semantics should satisfy .", "topics": ["value ( ethics )", "nonlinear system"]}
{"title": "spudd : stochastic planning using decision diagrams", "abstract": "markov decisions processes ( mdps ) are becoming increasing popular as models of decision theoretic planning . while traditional dynamic programming methods perform well for problems with small state spaces , structured methods are needed for large problems . we propose and examine a value iteration algorithm for mdps that uses algebraic decision diagrams ( adds ) to represent value functions and policies . an mdp is represented using bayesian networks and adds and dynamic programming is applied directly to these adds . we demonstrate our method on large mdps ( up to 63 million states ) and show that significant gains can be had when compared to tree-structured representations ( with up to a thirty-fold reduction in the number of nodes required to represent optimal value functions ) .", "topics": ["iteration"]}
{"title": "graph-based approaches to clustering network-constrained trajectory data", "abstract": "clustering trajectory data attracted considerable attention in the last few years . most of prior work assumed that moving objects can move freely in an euclidean space and did not consider the eventual presence of an underlying road network and its influence on evaluating the similarity between trajectories . in this paper , we present an approach to clustering such network-constrained trajectory data . more precisely we aim at discovering groups of road segments that are often travelled by the same trajectories . to achieve this end , we model the interactions between segments w.r.t . their similarity as a weighted graph to which we apply a community detection algorithm to discover meaningful clusters . we showcase our proposition through experimental results obtained on synthetic datasets .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "improving dpll solver performance with domain-specific heuristics : the asp case", "abstract": "in spite of the recent improvements in the performance of the solvers based on the dpll procedure , it is still possible for the search algorithm to focus on the wrong areas of the search space , preventing the solver from returning a solution in an acceptable amount of time . this prospect is a real concern e.g . in an industrial setting , where users typically expect consistent performance . to overcome this problem , we propose a framework that allows learning and using domain-specific heuristics in solvers based on the dpll procedure . the learning is done off-line , on representative instances from the target domain , and the learned heuristics are then used for choice-point selection . in this paper we focus on answer set programming ( asp ) solvers . in our experiments , the introduction of domain-specific heuristics improved performance on hard instances by up to 3 orders of magnitude ( and 2 on average ) , nearly completely eliminating the cases in which the solver had to be terminated because the wait for an answer had become unacceptable .", "topics": ["heuristic"]}
{"title": "optimized algorithms to sample determinantal point processes", "abstract": "in this technical report , we discuss several sampling algorithms for determinantal point processes ( dpp ) . dpps have recently gained a broad interest in the machine learning and statistics literature as random point processes with negative correlation , i.e . , ones that can generate a `` diverse '' sample from a set of items . they are parametrized by a matrix $ \\mathbf { l } $ , called $ l $ -ensemble , that encodes the correlations between items . the standard sampling algorithm is separated in three phases : 1/~eigendecomposition of $ \\mathbf { l } $ , 2/~an eigenvector sampling phase where $ \\mathbf { l } $ 's eigenvectors are sampled independently via a bernoulli variable parametrized by their associated eigenvalue , 3/~a gram-schmidt-type orthogonalisation procedure of the sampled eigenvectors . in a naive implementation , the computational cost of the third step is on average $ \\mathcal { o } ( n\\mu^3 ) $ where $ \\mu $ is the average number of samples of the dpp . we give an algorithm which runs in $ \\mathcal { o } ( n\\mu^2 ) $ and is extremely simple to implement . if memory is a constraint , we also describe a dual variant with reduced memory costs . in addition , we discuss implementation details often missing in the literature .", "topics": ["sampling ( signal processing )"]}
{"title": "regret analysis of stochastic and nonstochastic multi-armed bandit problems", "abstract": "multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration-exploitation trade-off . this is the balance between staying with the option that gave highest payoffs in the past and exploring new options that might give higher payoffs in the future . although the study of bandit problems dates back to the thirties , exploration-exploitation trade-offs arise in several modern applications , such as ad placement , website optimization , and packet routing . mathematically , a multi-armed bandit is defined by the payoff process associated with each option . in this survey , we focus on two extreme cases in which the analysis of regret is particularly simple and elegant : i.i.d . payoffs and adversarial payoffs . besides the basic setting of finitely many actions , we also analyze some of the most important variants and extensions , such as the contextual bandit model .", "topics": ["regret ( decision theory )"]}
{"title": "sequential dynamic decision making with deep neural nets on a test-time budget", "abstract": "deep neural network ( dnn ) based approaches hold significant potential for reinforcement learning ( rl ) and have already shown remarkable gains over state-of-art methods in a number of applications . the effectiveness of dnn methods can be attributed to leveraging the abundance of supervised data to learn value functions , q-functions , and policy function approximations without the need for feature engineering . nevertheless , the deployment of dnn-based predictors with very deep architectures can pose an issue due to computational and other resource constraints at test-time in a number of applications . we propose a novel approach for reducing the average latency by learning a computationally efficient gating function that is capable of recognizing states in a sequential decision process for which policy prescriptions of a shallow network suffices and deeper layers of the dnn have little marginal utility . the overall system is adaptive in that it dynamically switches control actions based on state-estimates in order to reduce average latency without sacrificing terminal performance . we experiment with a number of alternative loss-functions to train gating functions and shallow policies and show that in a number of applications a speed-up of up to almost 5x can be obtained with little loss in performance .", "topics": ["computational complexity theory", "reinforcement learning"]}
{"title": "lime : a method for low-light image enhancement", "abstract": "when one captures images in low-light conditions , the images often suffer from low visibility . this poor quality may significantly degrade the performance of many computer vision and multimedia algorithms that are primarily designed for high-quality inputs . in this paper , we propose a very simple and effective method , named as lime , to enhance low-light images . more concretely , the illumination of each pixel is first estimated individually by finding the maximum value in r , g and b channels . further , we refine the initial illumination map by imposing a structure prior on it , as the final illumination map . having the well-constructed illumination map , the enhancement can be achieved accordingly . experiments on a number of challenging real-world low-light images are present to reveal the efficacy of our lime and show its superiority over several state-of-the-arts .", "topics": ["computer vision", "pixel"]}
{"title": "event management for large scale event-driven digital hardware spiking neural networks", "abstract": "the interest in brain-like computation has led to the design of a plethora of innovative neuromorphic systems . individually , spiking neural networks ( snns ) , event-driven simulation and digital hardware neuromorphic systems get a lot of attention . despite the popularity of event-driven snns in software , very few digital hardware architectures are found . this is because existing hardware solutions for event management scale badly with the number of events . this paper introduces the structured heap queue , a pipelined digital hardware data structure , and demonstrates its suitability for event management . the structured heap queue scales gracefully with the number of events , allowing the efficient implementation of large scale digital hardware event-driven snns . the scaling is linear for memory , logarithmic for logic resources and constant for processing time . the use of the structured heap queue is demonstrated on field-programmable gate array ( fpga ) with an image segmentation experiment and a snn of 65~536 neurons and 513~184 synapses . events can be processed at the rate of 1 every 7 clock cycles and a 406 $ \\times $ 158 pixel image is segmented in 200 ms .", "topics": ["image segmentation", "simulation"]}
{"title": "black-box generation of adversarial text sequences to evade deep learning classifiers", "abstract": "although various techniques have been proposed to generate adversarial samples for white-box attacks on text , little attention has been paid to a black-box attack , which is a more realistic scenario . in this paper , we present a novel algorithm , deepwordbug , to effectively generate small text perturbations in a black-box setting that forces a deep-learning classifier to misclassify a text input . we develop novel scoring strategies to find the most important words to modify such that the deep classifier makes a wrong prediction . simple character-level transformations are applied to the highest-ranked words in order to minimize the edit distance of the perturbation . we evaluated deepwordbug on two real-world text datasets : enron spam emails and imdb movie reviews . our experimental results indicate that deepwordbug can reduce the classification accuracy from $ 99\\ % $ to around $ 40\\ % $ on enron data and from $ 87\\ % $ to about $ 26\\ % $ on imdb . also , our experimental results strongly demonstrate that the generated adversarial sequences from a deep-learning model can similarly evade other deep models .", "topics": ["gradient"]}
{"title": "directed time series regression for control", "abstract": "we propose directed time series regression , a new approach to estimating parameters of time-series models for use in certainty equivalent model predictive control . the approach combines merits of least squares regression and empirical optimization . through a computational study involving a stochastic version of a well known inverted pendulum balancing problem , we demonstrate that directed time series regression can generate significant improvements in controller performance over either of the aforementioned alternatives .", "topics": ["time series"]}
{"title": "bootstrap for neural model selection", "abstract": "bootstrap techniques ( also called resampling computation techniques ) have introduced new advances in modeling and model evaluation . using resampling methods to construct a series of new samples which are based on the original data set , allows to estimate the stability of the parameters . properties such as convergence and asymptotic normality can be checked for any particular observed data set . in most cases , the statistics computed on the generated data sets give a good idea of the confidence regions of the estimates . in this paper , we debate on the contribution of such methods for model selection , in the case of feedforward neural networks . the method is described and compared with the leave-one-out resampling method . the effectiveness of the bootstrap method , versus the leave-one-out methode , is checked through a number of examples .", "topics": ["computation"]}
{"title": "cross device matching for online advertising with neural feature ensembles : first place solution at cikm cup 2016", "abstract": "we describe the 1st place winning approach for the cikm cup 2016 challenge . in this paper , we provide an approach to reasonably identify same users across multiple devices based on browsing logs . our approach regards a candidate ranking problem as pairwise classification and utilizes an unsupervised neural feature ensemble approach to learn latent features of users . combined with traditional hand crafted features , each user pair feature is fed into a supervised classifier in order to perform pairwise classification . lastly , we propose supervised and unsupervised inference techniques .", "topics": ["unsupervised learning"]}
{"title": "can you fool ai with adversarial examples on a visual turing test ?", "abstract": "deep learning has achieved impressive results in many areas of computer vision and natural language pro- cessing . among others , visual question answering ( vqa ) , also referred to a visual turing test , is considered one of the most compelling problems , and recent deep learning models have reported significant progress in vision and language modeling . although artificial intelligence ( ai ) is getting closer to passing the visual turing test , at the same time the existence of adversarial examples to deep learning systems may hinder the practical application of such systems . in this work , we conduct the first extensive study on adversarial examples for vqa systems . in particular , we focus on generating targeted adversarial examples for a vqa system while the target is considered to be a question-answer pair . our evaluation shows that the success rate of whether a targeted adversarial example can be generated is mostly dependent on the choice of the target question-answer pair , and less on the choice of images to which the question refers . we also report the language prior phenomenon of a vqa model , which can explain why targeted adversarial examples are hard to generate for some question-answer targets . we also demonstrate that a compositional vqa architecture is slightly more resilient to adversarial attacks than a non-compositional one . our study sheds new light on how to build deep vision and language resilient models robust against adversarial examples .", "topics": ["natural language processing", "computer vision"]}
{"title": "practical bayesian optimization for variable cost objectives", "abstract": "we propose a novel bayesian optimization approach for black-box functions with an environmental variable whose value determines the tradeoff between evaluation cost and the fidelity of the evaluations . further , we use a novel approach to sampling support points , allowing faster construction of the acquisition function . this allows us to achieve optimization with lower overheads than previous approaches and is implemented for a more general class of problem . we show this approach to be effective on synthetic and real world benchmark problems .", "topics": ["sampling ( signal processing )", "synthetic data"]}
{"title": "parametric learning and monte carlo optimization", "abstract": "this paper uncovers and explores the close relationship between monte carlo optimization of a parametrized integral ( mco ) , parametric machine-learning ( pl ) , and `blackbox ' or `oracle'-based optimization ( bo ) . we make four contributions . first , we prove that mco is mathematically identical to a broad class of pl problems . this identity potentially provides a new application domain for all broadly applicable pl techniques : mco . second , we introduce immediate sampling , a new version of the probability collectives ( pc ) algorithm for blackbox optimization . immediate sampling transforms the original bo problem into an mco problem . accordingly , by combining these first two contributions , we can apply all pl techniques to bo . in our third contribution we validate this way of improving bo by demonstrating that cross-validation and bagging improve immediate sampling . finally , conventional mc and mco procedures ignore the relationship between the sample point locations and the associated values of the integrand ; only the values of the integrand at those locations are considered . we demonstrate that one can exploit the sample location information using pl techniques , for example by forming a fit of the sample locations to the associated values of the integrand . this provides an additional way to apply pl techniques to improve mco .", "topics": ["sampling ( signal processing )"]}
{"title": "phonetic temporal neural model for language identification", "abstract": "deep neural models , particularly the lstm-rnn model , have shown great potential for language identification ( lid ) . however , the use of phonetic information has been largely overlooked by most existing neural lid methods , although this information has been used very successfully in conventional phonetic lid systems . we present a phonetic temporal neural model for lid , which is an lstm-rnn lid system that accepts phonetic features produced by a phone-discriminative dnn as the input , rather than raw acoustic features . this new model is similar to traditional phonetic lid methods , but the phonetic knowledge here is much richer : it is at the frame level and involves compacted information of all phones . our experiments conducted on the babel database and the ap16-olr database demonstrate that the temporal phonetic neural approach is very effective , and significantly outperforms existing acoustic neural models . it also outperforms the conventional i-vector approach on short utterances and in noisy conditions .", "topics": ["recurrent neural network"]}
{"title": "scene labeling using gated recurrent units with explicit long range conditioning", "abstract": "recurrent neural network ( rnn ) , as a powerful contextual dependency modeling framework , has been widely applied to scene labeling problems . however , this work shows that directly applying traditional rnn architectures , which unfolds a 2d lattice grid into a sequence , is not sufficient to model structure dependencies in images due to the `` impact vanishing '' problem . first , we give an empirical analysis about the `` impact vanishing '' problem . then , a new rnn unit named recurrent neural network with explicit long range conditioning ( rnn-elc ) is designed to alleviate this problem . a novel neural network architecture is built for scene labeling tasks where one of the variants of the new rnn unit , gated recurrent unit with explicit long-range conditioning ( gru-elc ) , is used to model multi scale contextual dependencies in images . we validate the use of gru-elc units with state-of-the-art performance on three standard scene labeling datasets . comprehensive experiments demonstrate that the new gru-elc unit benefits scene labeling problem a lot as it can encode longer contextual dependencies in images more effectively than traditional rnn units .", "topics": ["recurrent neural network"]}
{"title": "robust online multi-task learning with correlative and personalized structures", "abstract": "multi-task learning ( mtl ) can enhance a classifier 's generalization performance by learning multiple related tasks simultaneously . conventional mtl works under the offline or batch setting , and suffers from expensive training cost and poor scalability . to address such inefficiency issues , online learning techniques have been applied to solve mtl problems . however , most existing algorithms of online mtl constrain task relatedness into a presumed structure via a single weight matrix , which is a strict restriction that does not always hold in practice . in this paper , we propose a robust online mtl framework that overcomes this restriction by decomposing the weight matrix into two components : the first one captures the low-rank common structure among tasks via a nuclear norm and the second one identifies the personalized patterns of outlier tasks via a group lasso . theoretical analysis shows the proposed algorithm can achieve a sub-linear regret with respect to the best linear model in hindsight . even though the above framework achieves good performance , the nuclear norm that simply adds all nonzero singular values together may not be a good low-rank approximation . to improve the results , we use a log-determinant function as a non-convex rank approximation . the gradient scheme is applied to optimize log-determinant function and can obtain a closed-form solution for this refined problem . experimental results on a number of real-world applications verify the efficacy of our method .", "topics": ["regret ( decision theory )", "value ( ethics )"]}
{"title": "inductive coherence", "abstract": "while probability theory is normally applied to external environments , there has been some recent interest in probabilistic modeling of the outputs of computations that are too expensive to run . since mathematical logic is a powerful tool for reasoning about computer programs , we consider this problem from the perspective of integrating probability and logic . recent work on assigning probabilities to mathematical statements has used the concept of coherent distributions , which satisfy logical constraints such as the probability of a sentence and its negation summing to one . although there are algorithms which converge to a coherent probability distribution in the limit , this yields only weak guarantees about finite approximations of these distributions . in our setting , this is a significant limitation : coherent distributions assign probability one to all statements provable in a specific logical theory , such as peano arithmetic , which can prove what the output of any terminating computation is ; thus , a coherent distribution must assign probability one to the output of any terminating computation . to model uncertainty about computations , we propose to work with approximations to coherent distributions . we introduce inductive coherence , a strengthening of coherence that provides appropriate constraints on finite approximations , and propose an algorithm which satisfies this criterion .", "topics": ["approximation", "computation"]}
{"title": "on the connection between learning two-layers neural networks and tensor decomposition", "abstract": "we establish connections between the problem of learning a two-layers neural network with good generalization error and tensor decomposition . we consider a model with input $ \\boldsymbol x \\in \\mathbb r^d $ , $ r $ hidden units with weights $ \\ { \\boldsymbol w_i\\ } _ { 1\\le i \\le r } $ and output $ y\\in \\mathbb r $ , i.e . , $ y=\\sum_ { i=1 } ^r \\sigma ( \\left \\langle \\boldsymbol x , \\boldsymbol w_i\\right \\rangle ) $ , where $ \\sigma $ denotes the activation function . first , we show that , if we can not learn the weights $ \\ { \\boldsymbol w_i\\ } _ { 1\\le i \\le r } $ accurately , then the neural network does not generalize well . more specifically , the generalization error is close to that of a trivial predictor with access only to the norm of the input . this result holds for any activation function , and it requires that the weights are roughly isotropic and the input distribution is gaussian , which is a typical assumption in the theoretical literature . then , we show that the problem of learning the weights $ \\ { \\boldsymbol w_i\\ } _ { 1\\le i \\le r } $ is at least as hard as the problem of tensor decomposition . this result holds for any input distribution and assumes that the activation function is a polynomial whose degree is related to the order of the tensor to be decomposed . by putting everything together , we prove that learning a two-layers neural network that generalizes well is at least as hard as tensor decomposition . it has been observed that neural network models with more parameters than training samples often generalize well , even if the problem is highly underdetermined . this means that the learning algorithm does not estimate the weights accurately and yet is able to yield a good generalization error . this paper shows that such a phenomenon can not occur when the input distribution is gaussian and the weights are roughly isotropic . we also provide numerical evidence supporting our theoretical findings .", "topics": ["numerical analysis", "neural networks"]}
{"title": "the lexicographic closure as a revision process", "abstract": "the connections between nonmonotonic reasoning and belief revision are well-known . a central problem in the area of nonmonotonic reasoning is the problem of default entailment , i.e . , when should an item of default information representing `` if a is true then , normally , b is true '' be said to follow from a given set of items of such information . many answers to this question have been proposed but , surprisingly , virtually none have attempted any explicit connection to belief revision . the aim of this paper is to give an example of how such a connection can be made by showing how the lexicographic closure of a set of defaults may be conceptualised as a process of iterated revision by sets of sentences . specifically we use the revision process of nayak .", "topics": ["iteration"]}
{"title": "digital genesis : computers , evolution and artificial life", "abstract": "the application of evolution in the digital realm , with the goal of creating artificial intelligence and artificial life , has a history as long as that of the digital computer itself . we illustrate the intertwined history of these ideas , starting with the early theoretical work of john von neumann and the pioneering experimental work of nils aall barricelli . we argue that evolutionary thinking and artificial life will continue to play an integral role in the future development of the digital world .", "topics": ["artificial intelligence"]}
{"title": "generating discriminative object proposals via submodular ranking", "abstract": "a multi-scale greedy-based object proposal generation approach is presented . based on the multi-scale nature of objects in images , our approach is built on top of a hierarchical segmentation . we first identify the representative and diverse exemplar clusters within each scale by using a diversity ranking algorithm . object proposals are obtained by selecting a subset from the multi-scale segment pool via maximizing a submodular objective function , which consists of a weighted coverage term , a single-scale diversity term and a multi-scale reward term . the weighted coverage term forces the selected set of object proposals to be representative and compact ; the single-scale diversity term encourages choosing segments from different exemplar clusters so that they will cover as many object patterns as possible ; the multi-scale reward term encourages the selected proposals to be discriminative and selected from multiple layers generated by the hierarchical image segmentation . the experimental results on the berkeley segmentation dataset and pascal voc2012 segmentation dataset demonstrate the accuracy and efficiency of our object proposal model . additionally , we validate our object proposals in simultaneous segmentation and detection and outperform the state-of-art performance .", "topics": ["image segmentation", "loss function"]}
{"title": "a probabilistic model of machine translation", "abstract": "a probabilistic model for computer-based generation of a machine translation system on the basis of english-russian parallel text corpora is suggested . the model is trained using parallel text corpora with pre-aligned source and target sentences . the training of the model results in a bilingual dictionary of words and `` word blocks '' with relevant translation probability .", "topics": ["machine translation", "text corpus"]}
{"title": "learning feature pyramids for human pose estimation", "abstract": "articulated human pose estimation is a fundamental yet challenging task in computer vision . the difficulty is particularly pronounced in scale variations of human body parts when camera view changes or severe foreshortening happens . although pyramid methods are widely used to handle scale changes at inference time , learning feature pyramids in deep convolutional neural networks ( dcnns ) is still not well explored . in this work , we design a pyramid residual module ( prms ) to enhance the invariance in scales of dcnns . given input features , the prms learn convolutional filters on various scales of input features , which are obtained with different subsampling ratios in a multi-branch network . moreover , we observe that it is inappropriate to adopt existing methods to initialize the weights of multi-branch networks , which achieve superior performance than plain networks in many tasks recently . therefore , we provide theoretic derivation to extend the current weight initialization scheme to multi-branch network structures . we investigate our method on two standard benchmarks for human pose estimation . our approach obtains state-of-the-art results on both benchmarks . code is available at https : //github.com/bearpaw/pyranet .", "topics": ["computer vision"]}
{"title": "bayesian joint matrix decomposition for data integration with heterogeneous noise", "abstract": "matrix decomposition is a popular and fundamental approach in machine learning and data mining . it has been successfully applied into various fields . most matrix decomposition methods focus on decomposing a data matrix from one single source . however , it is common that data are from different sources with heterogeneous noise . a few of matrix decomposition methods have been extended for such multi-view data integration and pattern discovery . while only few methods were designed to consider the heterogeneity of noise in such multi-view data for data integration explicitly . to this end , we propose a joint matrix decomposition framework ( bjmd ) , which models the heterogeneity of noise by gaussian distribution in a bayesian framework . we develop two algorithms to solve this model : one is a variational bayesian inference algorithm , which makes full use of the posterior distribution ; and another is a maximum a posterior algorithm , which is more scalable and can be easily paralleled . extensive experiments on synthetic and real-world datasets demonstrate that bjmd considering the heterogeneity of noise is superior or competitive to the state-of-the-art methods .", "topics": ["data mining", "calculus of variations"]}
{"title": "target-side context for discriminative models in statistical machine translation", "abstract": "discriminative translation models utilizing source context have been shown to help statistical machine translation performance . we propose a novel extension of this work using target context information . surprisingly , we show that this model can be efficiently integrated directly in the decoding process . our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs . we also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target-context model and we show that our extension allows us to better capture morphological coherence . our work is freely available as part of moses .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "collective intelligence for control of distributed dynamical systems", "abstract": "we consider the el farol bar problem , also known as the minority game ( w. b . arthur , `` the american economic review '' , 84 ( 2 ) : 406 -- 411 ( 1994 ) , d. challet and y.c . zhang , `` physica a '' , 256:514 ( 1998 ) ) . we view it as an instance of the general problem of how to configure the nodal elements of a distributed dynamical system so that they do not `` work at cross purposes '' , in that their collective dynamics avoids frustration and thereby achieves a provided global goal . we summarize a mathematical theory for such configuration applicable when ( as in the bar problem ) the global goal can be expressed as minimizing a global energy function and the nodes can be expressed as minimizers of local free energy functions . we show that a system designed with that theory performs nearly optimally for the bar problem .", "topics": ["mathematical optimization"]}
{"title": "optimisation of a crossdocking distribution centre simulation model", "abstract": "this paper reports on continuing research into the modelling of an order picking process within a crossdocking distribution centre using simulation optimisation . the aim of this project is to optimise a discrete event simulation model and to understand factors that affect finding its optimal performance . our initial investigation revealed that the precision of the selected simulation output performance measure and the number of replications required for the evaluation of the optimisation objective function through simulation influences the ability of the optimisation technique . we experimented with common random numbers , in order to improve the precision of our simulation output performance measure , and intended to use the number of replications utilised for this purpose as the initial number of replications for the optimisation of our crossdocking distribution centre simulation model . our results demonstrate that we can improve the precision of our selected simulation output performance measure value using common random numbers at various levels of replications . furthermore , after optimising our crossdocking distribution centre simulation model , we are able to achieve optimal performance using fewer simulations runs for the simulation model which uses common random numbers as compared to the simulation model which does not use common random numbers .", "topics": ["mathematical optimization", "optimization problem"]}
{"title": "linear disentangled representation learning for facial actions", "abstract": "limited annotated data available for the recognition of facial expression and action units embarrasses the training of deep networks , which can learn disentangled invariant features . however , a linear model with just several parameters normally is not demanding in terms of training data . in this paper , we propose an elegant linear model to untangle confounding factors in challenging realistic multichannel signals such as 2d face videos . the simple yet powerful model does not rely on huge training data and is natural for recognizing facial actions without explicitly disentangling the identity . base on well-understood intuitive linear models such as sparse representation based classification ( src ) , previous attempts require a prepossessing of explicit decoupling which is practically inexact . instead , we exploit the low-rank property across frames to subtract the underlying neutral faces which are modeled jointly with sparse representation on the action components with group sparsity enforced . on the extended cohn-kanade dataset ( ck+ ) , our one-shot automatic method on raw face videos performs as competitive as src applied on manually prepared action components and performs even better than src in terms of true positive rate . we apply the model to the even more challenging task of facial action unit recognition , verified on the mpi face video database ( mpi-vdb ) achieving a decent performance . all the programs and data have been made publicly available .", "topics": ["test set", "sparse matrix"]}
{"title": "stretching domain adaptation : how far is too far ?", "abstract": "while deep learning has led to significant advances in visual recognition over the past few years , such advances often require a lot of annotated data . while unsupervised domain adaptation has emerged as an alternative approach that does n't require as much annotated data , prior evaluations of domain adaptation have been limited to relatively simple datasets . this work pushes the state of the art in unsupervised domain adaptation through an in depth evaluation of alexnet , densenet and residual transfer networks ( rtn ) on multimodal benchmark datasets that shows and identifies which layers more effectively transfer features across different domains . we also modify the existing rtn architecture and propose a novel domain adaptation architecture called `` deep magnet '' that combines deep convolutional blocks with multiple maximum mean discrepancy losses . our experiments show quantitative and qualitative improvements in performance of our method on benchmarking datasets for complex data domains .", "topics": ["unsupervised learning", "computer vision"]}
{"title": "elastic regularization in restricted boltzmann machines : dealing with $ p\\gg n $", "abstract": "restricted boltzmann machines ( rbms ) are endowed with the universal power of modeling ( binary ) joint distributions . meanwhile , as a result of their confining network structure , training rbms confronts less difficulties ( compared with more complicated models , e.g . , boltzmann machines ) when dealing with approximation and inference issues . however , in certain computational biology scenarios , such as the cancer data analysis , employing rbms to model data features may lose its efficacy due to the `` $ p\\gg n $ '' problem , in which the number of features/predictors is much larger than the sample size . the `` $ p\\gg n $ '' problem puts the bias-variance trade-off in a more crucial place when designing statistical learning methods . in this manuscript , we try to address this problem by proposing a novel rbm model , called elastic restricted boltzmann machine ( erbm ) , which incorporates the elastic regularization term into the likelihood/cost function . we provide several theoretical analysis on the superiority of our model . furthermore , attributed to the classic contrastive divergence ( cd ) algorithm , erbms can be trained efficiently . our novel model is a promising method for future cancer data analysis .", "topics": ["matrix regularization"]}
{"title": "citlab argus for historical handwritten documents", "abstract": "we describe citlab 's recognition system for the htrts competition attached to the 14. international conference on frontiers in handwriting recognition , icfhr 2014 . the task comprises the recognition of historical handwritten documents . the core algorithms of our system are based on multi-dimensional recurrent neural networks ( mdrnn ) and connectionist temporal classification ( ctc ) . the software modules behind that as well as the basic utility technologies are essentially powered by planet 's argus framework for intelligent text recognition and image processing .", "topics": ["image processing", "recurrent neural network"]}
{"title": "reinforcement-based simultaneous algorithm and its hyperparameters selection", "abstract": "many algorithms for data analysis exist , especially for classification problems . to solve a data analysis problem , a proper algorithm should be chosen , and also its hyperparameters should be selected . in this paper , we present a new method for the simultaneous selection of an algorithm and its hyperparameters . in order to do so , we reduced this problem to the multi-armed bandit problem . we consider an algorithm as an arm and algorithm hyperparameters search during a fixed time as the corresponding arm play . we also suggest a problem-specific reward function . we performed the experiments on 10 real datasets and compare the suggested method with the existing one implemented in auto-weka . the results show that our method is significantly better in most of the cases and never worse than the auto-weka .", "topics": ["reinforcement learning"]}
{"title": "dkn : deep knowledge-aware network for news recommendation", "abstract": "online news recommender systems aim to address the information explosion of news and make personalized recommendation for users . in general , news language is highly condensed , full of knowledge entities and common sense . however , existing methods are unaware of such external knowledge and can not fully discover latent knowledge-level connections among news . the recommended results for a user are consequently limited to simple patterns and can not be extended reasonably . moreover , news recommendation also faces the challenges of high time-sensitivity of news and dynamic diversity of users ' interests . to solve the above problems , in this paper , we propose a deep knowledge-aware network ( dkn ) that incorporates knowledge graph representation into news recommendation . dkn is a content-based deep recommendation framework for click-through rate prediction . the key component of dkn is a multi-channel and word-entity-aligned knowledge-aware convolutional neural network ( kcnn ) that fuses semantic-level and knowledge-level representations of news . kcnn treats words and entities as multiple channels , and explicitly keeps their alignment relationship during convolution . in addition , to address users ' diverse interests , we also design an attention module in dkn to dynamically aggregate a user 's history with respect to current candidate news . through extensive experiments on a real online news platform , we demonstrate that dkn achieves substantial gains over state-of-the-art deep recommendation models . we also validate the efficacy of the usage of knowledge in dkn .", "topics": ["entity", "convolution"]}
{"title": "active classification : theory and application to underwater inspection", "abstract": "we discuss the problem in which an autonomous vehicle must classify an object based on multiple views . we focus on the active classification setting , where the vehicle controls which views to select to best perform the classification . the problem is formulated as an extension to bayesian active learning , and we show connections to recent theoretical guarantees in this area . we formally analyze the benefit of acting adaptively as new information becomes available . the analysis leads to a probabilistic algorithm for determining the best views to observe based on information theoretic costs . we validate our approach in two ways , both related to underwater inspection : 3d polyhedra recognition in synthetic depth maps and ship hull inspection with imaging sonar . these tasks encompass both the planning and recognition aspects of the active classification problem . the results demonstrate that actively planning for informative views can reduce the number of necessary views by up to 80 % when compared to passive methods .", "topics": ["synthetic data", "map"]}
{"title": "deflecting adversarial attacks with pixel deflection", "abstract": "cnns are poised to become integral parts of many critical systems . despite their robustness to natural variations , image pixel values can be manipulated , via small , carefully crafted , imperceptible perturbations , to cause a model to misclassify images . we present an algorithm to process an image so that classification accuracy is significantly preserved in the presence of such adversarial manipulations . image classifiers tend to be robust to natural noise , and adversarial attacks tend to be agnostic to object location . these observations motivate our strategy , which leverages model robustness to defend against adversarial perturbations by forcing the image to match natural image statistics . our algorithm locally corrupts the image by redistributing pixel values via a process we term pixel deflection . a subsequent wavelet-based denoising operation softens this corruption , as well as some of the adversarial changes . we demonstrate experimentally that the combination of these techniques enables the effective recovery of the true class , against a variety of robust attacks . our results compare favorably with current state-of-the-art defenses , without requiring retraining or modifying the cnn .", "topics": ["value ( ethics )", "noise reduction"]}
{"title": "3d face morphable models `` in-the-wild ''", "abstract": "3d morphable models ( 3dmms ) are powerful statistical models of 3d facial shape and texture , and among the state-of-the-art methods for reconstructing facial shape from single images . with the advent of new 3d sensors , many 3d facial datasets have been collected containing both neutral as well as expressive faces . however , all datasets are captured under controlled conditions . thus , even though powerful 3d facial shape models can be learnt from such data , it is difficult to build statistical texture models that are sufficient to reconstruct faces captured in unconstrained conditions ( `` in-the-wild '' ) . in this paper , we propose the first , to the best of our knowledge , `` in-the-wild '' 3dmm by combining a powerful statistical model of facial shape , which describes both identity and expression , with an `` in-the-wild '' texture model . we show that the employment of such an `` in-the-wild '' texture model greatly simplifies the fitting procedure , because there is no need to optimize with regards to the illumination parameters . furthermore , we propose a new fast algorithm for fitting the 3dmm in arbitrary images . finally , we have captured the first 3d facial database with relatively unconstrained conditions and report quantitative evaluations with state-of-the-art performance . complementary qualitative reconstruction results are demonstrated on standard `` in-the-wild '' facial databases . an open source implementation of our technique is released as part of the menpo project .", "topics": ["database", "sensor"]}
{"title": "bayesian kernel and mutual $ k $ -nearest neighbor regression", "abstract": "we propose bayesian extensions of two nonparametric regression methods which are kernel and mutual $ k $ -nearest neighbor regression methods . derived based on gaussian process models for regression , the extensions provide distributions for target value estimates and the framework to select the hyperparameters . it is shown that both the proposed methods asymptotically converge to kernel and mutual $ k $ -nearest neighbor regression methods , respectively . the simulation results show that the proposed methods can select proper hyperparameters and are better than or comparable to the former methods for an artificial data set and a real world data set .", "topics": ["kernel ( operating system )", "simulation"]}
{"title": "robust principal component analysis using statistical estimators", "abstract": "principal component analysis ( pca ) finds a linear mapping and maximizes the variance of the data which makes pca sensitive to outliers and may cause wrong eigendirection . in this paper , we propose techniques to solve this problem ; we use the data-centering method and reestimate the covariance matrix using robust statistic techniques such as median , robust scaling which is a booster to data-centering and huber m-estimator which measures the presentation of outliers and reweight them with small values . the results on several real world data sets show that our proposed method handles outliers and gains better results than the original pca and provides the same accuracy with lower computation cost than the kernel pca using the polynomial kernel in classification tasks .", "topics": ["computation", "polynomial"]}
{"title": "simplified long short-term memory recurrent neural networks : part i", "abstract": "we present five variants of the standard long short-term memory ( lstm ) recurrent neural networks by uniformly reducing blocks of adaptive parameters in the gating mechanisms . for simplicity , we refer to these models as lstm1 , lstm2 , lstm3 , lstm4 , and lstm5 , respectively . such parameter-reduced variants enable speeding up data training computations and would be more suitable for implementations onto constrained embedded platforms . we comparatively evaluate and verify our five variant models on the classical mnist dataset and demonstrate that these variant models are comparable to a standard implementation of the lstm model while using less number of parameters . moreover , we observe that in some cases the standard lstm 's accuracy performance will drop after a number of epochs when using the relu nonlinearity ; in contrast , however , lstm3 , lstm4 and lstm5 will retain their performance .", "topics": ["recurrent neural network", "nonlinear system"]}
{"title": "solving stable matching problems using answer set programming", "abstract": "since the introduction of the stable marriage problem ( smp ) by gale and shapley ( 1962 ) , several variants and extensions have been investigated . while this variety is useful to widen the application potential , each variant requires a new algorithm for finding the stable matchings . to address this issue , we propose an encoding of the smp using answer set programming ( asp ) , which can straightforwardly be adapted and extended to suit the needs of specific applications . the use of asp also means that we can take advantage of highly efficient off-the-shelf solvers . to illustrate the flexibility of our approach , we show how our asp encoding naturally allows us to select optimal stable matchings , i.e . matchings that are optimal according to some user-specified criterion . to the best of our knowledge , our encoding offers the first exact implementation to find sex-equal , minimum regret , egalitarian or maximum cardinality stable matchings for smp instances in which individuals may designate unacceptable partners and ties between preferences are allowed . this paper is under consideration in theory and practice of logic programming ( tplp ) .", "topics": ["regret ( decision theory )", "eisenstein 's criterion"]}
{"title": "word vector enrichment of low frequency words in the bag-of-words model for short text multi-class classification problems", "abstract": "the bag-of-words model is a standard representation of text for many linear classifier learners . in many problem domains , linear classifiers are preferred over more complex models due to their efficiency , robustness and interpretability , and the bag-of-words text representation can capture sufficient information for linear classifiers to make highly accurate predictions . however in settings where there is a large vocabulary , large variance in the frequency of terms in the training corpus , many classes and very short text ( e.g . , single sentences or document titles ) the bag-of-words representation becomes extremely sparse , and this can reduce the accuracy of classifiers . a particular issue in such settings is that short texts tend to contain infrequently occurring or rare terms which lack class-conditional evidence . in this work we introduce a method for enriching the bag-of-words model by complementing such rare term information with related terms from both general and domain-specific word vector models . by reducing sparseness in the bag-of-words models , our enrichment approach achieves improved classification over several baseline classifiers in a variety of text classification problems . our approach is also efficient because it requires no change to the linear classifier before or during training , since bag-of-words enrichment applies only to text being classified .", "topics": ["baseline ( configuration management )", "sparse matrix"]}
{"title": "automatic generation of language-independent features for cross-lingual classification", "abstract": "many applications require categorization of text documents using predefined categories . the main approach to performing text categorization is learning from labeled examples . for many tasks , it may be difficult to find examples in one language but easy in others . the problem of learning from examples in one or more languages and classifying ( categorizing ) in another is called cross-lingual learning . in this work , we present a novel approach that solves the general cross-lingual text categorization problem . our method generates , for each training document , a set of language-independent features . using these features for training yields a language-independent classifier . at the classification stage , we generate language-independent features for the unlabeled document , and apply the classifier on the new representation . to build the feature generator , we utilize a hierarchical language-independent ontology , where each concept has a set of support documents for each language involved . in the preprocessing stage , we use the support documents to build a set of language-independent feature generators , one for each language . the collection of these generators is used to map any document into the language-independent feature space . our methodology works on the most general cross-lingual text categorization problems , being able to learn from any mix of languages and classify documents in any other language . we also present a method for exploiting the hierarchical structure of the ontology to create virtual supporting documents for languages that do not have them . we tested our method , using wikipedia as our ontology , on the most commonly used test collections in cross-lingual text categorization , and found that it outperforms existing methods .", "topics": ["feature vector"]}
{"title": "learning functions of few arbitrary linear parameters in high dimensions", "abstract": "let us assume that $ f $ is a continuous function defined on the unit ball of $ \\mathbb r^d $ , of the form $ f ( x ) = g ( a x ) $ , where $ a $ is a $ k \\times d $ matrix and $ g $ is a function of $ k $ variables for $ k \\ll d $ . we are given a budget $ m \\in \\mathbb n $ of possible point evaluations $ f ( x_i ) $ , $ i=1 , ... , m $ , of $ f $ , which we are allowed to query in order to construct a uniform approximating function . under certain smoothness and variation assumptions on the function $ g $ , and an { \\it arbitrary } choice of the matrix $ a $ , we present in this paper 1. a sampling choice of the points $ \\ { x_i\\ } $ drawn at random for each function approximation ; 2. algorithms ( algorithm 1 and algorithm 2 ) for computing the approximating function , whose complexity is at most polynomial in the dimension $ d $ and in the number $ m $ of points . due to the arbitrariness of $ a $ , the choice of the sampling points will be according to suitable random distributions and our results hold with overwhelming probability . our approach uses tools taken from the { \\it compressed sensing } framework , recent chernoff bounds for sums of positive-semidefinite matrices , and classical stability bounds for invariant subspaces of singular value decompositions .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "single-shot bidirectional pyramid networks for high-quality object detection", "abstract": "recent years have witnessed many exciting achievements for object detection using deep learning techniques . despite achieving significant progresses , most existing detectors are designed to detect objects with relatively low-quality prediction of locations , i.e . , often trained with the threshold of intersection over union ( iou ) set to 0.5 by default , which can yield low-quality or even noisy detections . it remains an open challenge for how to devise and train a high-quality detector that can achieve more precise localization ( i.e . , iou $ > $ 0.5 ) without sacrificing the detection performance . in this paper , we propose a novel single-shot detection framework of bidirectional pyramid networks ( bpn ) towards high-quality object detection , which consists of two novel components : ( i ) a bidirectional feature pyramid structure for more effective and robust feature representations ; and ( ii ) a cascade anchor refinement to gradually refine the quality of predesigned anchors for more effective training . our experiments showed that the proposed bpn achieves the best performances among all the single-stage object detectors on both pascal voc and ms coco datasets , especially for high-quality detections .", "topics": ["object detection", "sensor"]}
{"title": "a collaborative kalman filter for time-evolving dyadic processes", "abstract": "we present the collaborative kalman filter ( ckf ) , a dynamic model for collaborative filtering and related factorization models . using the matrix factorization approach to collaborative filtering , the ckf accounts for time evolution by modeling each low-dimensional latent embedding as a multidimensional brownian motion . each observation is a random variable whose distribution is parameterized by the dot product of the relevant brownian motions at that moment in time . this is naturally interpreted as a kalman filter with multiple interacting state space vectors . we also present a method for learning a dynamically evolving drift parameter for each location by modeling it as a geometric brownian motion . we handle posterior intractability via a mean-field variational approximation , which also preserves tractability for downstream calculations in a manner similar to the kalman filter . we evaluate the model on several large datasets , providing quantitative evaluation on the 10 million movielens and 100 million netflix datasets and qualitative evaluation on a set of 39 million stock returns divided across roughly 6,500 companies from the years 1962-2014 .", "topics": ["calculus of variations"]}
{"title": "separation of concerns in reinforcement learning", "abstract": "in this paper , we propose a framework for solving a single-agent task by using multiple agents , each focusing on different aspects of the task . this approach has two main advantages : 1 ) it allows for training specialized agents on different parts of the task , and 2 ) it provides a new way to transfer knowledge , by transferring trained agents . our framework generalizes the traditional hierarchical decomposition , in which , at any moment in time , a single agent has control until it has solved its particular subtask . we illustrate our framework with empirical experiments on two domains .", "topics": ["reinforcement learning"]}
{"title": "seq2sql : generating structured queries from natural language using reinforcement learning", "abstract": "a significant amount of the world 's knowledge is stored in relational databases . however , the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as sql . we propose seq2sql , a deep neural network for translating natural language questions to corresponding sql queries . our model leverages the structure of sql queries to significantly reduce the output space of generated queries . moreover , we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query , which we show are less suitable for optimization via cross entropy loss . in addition , we will publish wikisql , a dataset of 80654 hand-annotated examples of questions and sql queries distributed across 24241 tables from wikipedia . this dataset is required to train our model and is an order of magnitude larger than comparable datasets . by applying policy-based reinforcement learning with a query execution environment to wikisql , our model seq2sql outperforms attentional sequence to sequence models , improving execution accuracy from 35.9 % to 59.4 % and logical form accuracy from 23.4 % to 48.3 % .", "topics": ["natural language", "reinforcement learning"]}
{"title": "learning causal structures using regression invariance", "abstract": "we study causal inference in a multi-environment setting , in which the functional relations for producing the variables from their direct causes remain the same across environments , while the distribution of exogenous noises may vary . we introduce the idea of using the invariance of the functional relations of the variables to their causes across a set of environments . we define a notion of completeness for a causal inference algorithm in this setting and prove the existence of such algorithm by proposing the baseline algorithm . additionally , we present an alternate algorithm that has significantly improved computational and sample complexity compared to the baseline algorithm . the experiment results show that the proposed algorithm outperforms the other existing algorithms .", "topics": ["baseline ( configuration management )", "causality"]}
{"title": "copula variational inference", "abstract": "we develop a general variational inference method that preserves dependency among the latent variables . our method uses copulas to augment the families of distributions used in mean-field and structured approximations . copulas model the dependency that is not captured by the original variational distribution , and thus the augmented variational family guarantees better approximations to the posterior . with stochastic optimization , inference on the augmented distribution is scalable . furthermore , our strategy is generic : it can be applied to any inference procedure that currently uses the mean-field or structured approach . copula variational inference has many advantages : it reduces bias ; it is less sensitive to local optima ; it is less sensitive to hyperparameters ; and it helps characterize and interpret the dependency among the latent variables .", "topics": ["calculus of variations", "approximation"]}
{"title": "an investigation into language complexity of world-of-warcraft game-external texts", "abstract": "we present a language complexity analysis of world of warcraft ( wow ) community texts , which we compare to texts from a general corpus of web english . results from several complexity types are presented , including lexical diversity , density , readability and syntactic complexity . the language of wow texts is found to be comparable to the general corpus on some complexity measures , yet more specialized on other measures . our findings can be used by educators willing to include game-related activities into school curricula .", "topics": ["text corpus"]}
{"title": "analyzing roles of classifiers and code-mixed factors for sentiment identification", "abstract": "multilingual speakers often switch between languages to express themselves on social communication platforms . sometimes , the original script of the language is preserved , while using a common script for all the languages is quite popular as well due to convenience . on such occasions , multiple languages are being mixed with different rules of grammar , using the same script which makes it a challenging task for natural language processing even in case of accurate sentiment identification . in this paper , we report results of various experiments carried out on movie reviews dataset having this code-mixing property of two languages , english and bengali , both typed in roman script . we have tested various machine learning algorithms trained only on english features on our code-mixed data and have achieved the maximum accuracy of 59.00 % using naive bayes ( nb ) model . we have also tested various models trained on code-mixed data , as well as english features and the highest accuracy of 72.50 % was obtained by a support vector machine ( svm ) model . finally , we have analyzed the misclassified snippets and have discussed the challenges needed to be resolved for better accuracy .", "topics": ["natural language processing", "support vector machine"]}
{"title": "strategies and principles of distributed machine learning on big data", "abstract": "the rise of big data has led to new demands for machine learning ( ml ) systems to learn complex models with millions to billions of parameters , that promise adequate capacity to digest massive datasets and offer powerful predictive analytics thereupon . in order to run ml algorithms at such scales , on a distributed cluster with 10s to 1000s of machines , it is often the case that significant engineering efforts are required -- - and one might fairly ask if such engineering truly falls within the domain of ml research or not . taking the view that big ml systems can benefit greatly from ml-rooted statistical and algorithmic insights -- - and that ml researchers should therefore not shy away from such systems design -- - we discuss a series of principles and strategies distilled from our recent efforts on industrial-scale ml solutions . these principles and strategies span a continuum from application , to engineering , and to theoretical research and development of big ml systems and architectures , with the goal of understanding how to make them efficient , generally-applicable , and supported with convergence and scaling guarantees . they concern four key questions which traditionally receive little attention in ml research : how to distribute an ml program over a cluster ? how to bridge ml computation with inter-machine communication ? how to perform such communication ? what should be communicated between machines ? by exposing underlying statistical and algorithmic characteristics unique to ml programs but not typically seen in traditional computer programs , and by dissecting successful cases to reveal how we have harnessed these principles to design and develop both high-performance distributed ml software as well as general-purpose ml frameworks , we present opportunities for ml researchers and practitioners to further shape and grow the area that lies between ml and systems .", "topics": ["computation"]}
{"title": "label stability in multiple instance learning", "abstract": "we address the problem of \\emph { instance label stability } in multiple instance learning ( mil ) classifiers . these classifiers are trained only on globally annotated images ( bags ) , but often can provide fine-grained annotations for image pixels or patches ( instances ) . this is interesting for computer aided diagnosis ( cad ) and other medical image analysis tasks for which only a coarse labeling is provided . unfortunately , the instance labels may be unstable . this means that a slight change in training data could potentially lead to abnormalities being detected in different parts of the image , which is undesirable from a cad point of view . despite mil gaining popularity in the cad literature , this issue has not yet been addressed . we investigate the stability of instance labels provided by several mil classifiers on 5 different datasets , of which 3 are medical image datasets ( breast histopathology , diabetic retinopathy and computed tomography lung images ) . we propose an unsupervised measure to evaluate instance stability , and demonstrate that a performance-stability trade-off can be made when comparing mil classifiers .", "topics": ["test set", "pixel"]}
{"title": "deep complex networks", "abstract": "at present , the vast majority of building blocks , techniques , and architectures for deep learning are based on real-valued operations and representations . however , recent work on recurrent neural networks and older fundamental theoretical analysis suggests that complex numbers could have a richer representational capacity and could also facilitate noise-robust memory retrieval mechanisms . despite their attractive properties and potential for opening up entirely new neural architectures , complex-valued deep neural networks have been marginalized due to the absence of the building blocks required to design such models . in this work , we provide the key atomic components for complex-valued deep neural networks and apply them to convolutional feed-forward networks and convolutional lstms . more precisely , we rely on complex convolutions and present algorithms for complex batch-normalization , complex weight initialization strategies for complex-valued neural nets and we use them in experiments with end-to-end training schemes . we demonstrate that such complex-valued models are competitive with their real-valued counterparts . we test deep complex models on several computer vision tasks , on music transcription using the musicnet dataset and on speech spectrum prediction using the timit dataset . we achieve state-of-the-art performance on these audio-related tasks .", "topics": ["recurrent neural network", "computer vision"]}
{"title": "dcfnet : discriminant correlation filters network for visual tracking", "abstract": "discriminant correlation filters ( dcf ) based methods now become a kind of dominant approach to online object tracking . the features used in these methods , however , are either based on hand-crafted features like hogs , or convolutional features trained independently from other tasks like image classification . in this work , we present an end-to-end lightweight network architecture , namely dcfnet , to learn the convolutional features and perform the correlation tracking process simultaneously . specifically , we treat dcf as a special correlation filter layer added in a siamese network , and carefully derive the backpropagation through it by defining the network output as the probability heatmap of object location . since the derivation is still carried out in fourier frequency domain , the efficiency property of dcf is preserved . this enables our tracker to run at more than 60 fps during test time , while achieving a significant accuracy gain compared with kcf using hogs . extensive evaluations on otb-2013 , otb-2015 , and vot2015 benchmarks demonstrate that the proposed dcfnet tracker is competitive with several state-of-the-art trackers , while being more compact and much faster .", "topics": ["computer vision", "end-to-end principle"]}
{"title": "improved training for online end-to-end speech recognition systems", "abstract": "achieving high accuracy with end-to-end speech recognizers requires careful parameter initialization prior to training . otherwise , the networks may fail to find a good local optimum . this is particularly true for low-latency online networks , such as unidirectional lstms . currently , the best strategy to train such systems is to bootstrap the training from a tied-triphone system . however , this is time consuming , and more importantly , is impossible for languages without a high-quality pronunciation lexicon . in this work , we propose an initialization strategy that uses teacher-student learning to transfer knowledge from a large , well-trained , offline end-to-end speech recognition model to an online end-to-end model , eliminating the need for a lexicon or any other linguistic resources . we also explore curriculum learning and label smoothing and show how they can be combined with the proposed teacher-student learning for further improvements . we evaluate our methods on a microsoft cortana personal assistant task and show that the proposed method results in a 19 % relative improvement in word error rate compared to a randomly-initialized baseline system .", "topics": ["baseline ( configuration management )", "speech recognition"]}
{"title": "hierarchical deep reinforcement learning : integrating temporal abstraction and intrinsic motivation", "abstract": "learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms . the primary difficulty arises due to insufficient exploration , resulting in an agent being unable to learn robust value functions . intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems . such intrinsic behaviors could eventually help the agent solve tasks posed by the environment . we present hierarchical-dqn ( h-dqn ) , a framework to integrate hierarchical value functions , operating at different temporal scales , with intrinsically motivated deep reinforcement learning . a top-level value function learns a policy over intrinsic goals , and a lower-level function learns a policy over atomic actions to satisfy the given goals . h-dqn allows for flexible goal specifications , such as functions over entities and relations . this provides an efficient space for exploration in complicated environments . we demonstrate the strength of our approach on two problems with very sparse , delayed feedback : ( 1 ) a complex discrete stochastic decision process , and ( 2 ) the classic atari game `montezuma 's revenge ' .", "topics": ["reinforcement learning", "entity"]}
{"title": "generating bayesian networks from probability logic knowledge bases", "abstract": "we present a method for dynamically generating bayesian networks from knowledge bases consisting of first-order probability logic sentences . we present a subset of probability logic sufficient for representing the class of bayesian networks with discrete-valued nodes . we impose constraints on the form of the sentences that guarantee that the knowledge base contains all the probabilistic information necessary to generate a network . we define the concept of d-separation for knowledge bases and prove that a knowledge base with independence conditions defined by d-separation is a complete specification of a probability distribution . we present a network generation algorithm that , given an inference problem in the form of a query q and a set of evidence e , generates a network to compute p ( q|e ) . we prove the algorithm to be correct .", "topics": ["bayesian network"]}
{"title": "why we need new evaluation metrics for nlg", "abstract": "the majority of nlg evaluation relies on automatic metrics , such as bleu . in this paper , we motivate the need for novel , system- and data-independent automatic evaluation methods : we investigate a wide range of metrics , including state-of-the-art word-based and novel grammar-based ones , and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven , end-to-end nlg . we also show that metric performance is data- and system-specific . nevertheless , our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly .", "topics": ["end-to-end principle"]}
{"title": "early stopping is nonparametric variational inference", "abstract": "we show that unconverged stochastic gradient descent can be interpreted as a procedure that samples from a nonparametric variational approximate posterior distribution . this distribution is implicitly defined as the transformation of an initial distribution by a sequence of optimization updates . by tracking the change in entropy over this sequence of transformations during optimization , we form a scalable , unbiased estimate of the variational lower bound on the log marginal likelihood . we can use this bound to optimize hyperparameters instead of using cross-validation . this bayesian interpretation of sgd suggests improved , overfitting-resistant optimization procedures , and gives a theoretical foundation for popular tricks such as early stopping and ensembling . we investigate the properties of this marginal likelihood estimator on neural network models .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "applications of a graph theoretic based clustering framework in computer vision and pattern recognition", "abstract": "recently , several clustering algorithms have been used to solve variety of problems from different discipline . this dissertation aims to address different challenging tasks in computer vision and pattern recognition by casting the problems as a clustering problem . we proposed novel approaches to solve multi-target tracking , visual geo-localization and outlier detection problems using a unified underlining clustering framework , i.e . , dominant set clustering and its extensions , and presented a superior result over several state-of-the-art approaches .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "algorithmic bio-surveillance for precise spatio-temporal prediction of zoonotic emergence", "abstract": "viral zoonoses have emerged as the key drivers of recent pandemics . human infection by zoonotic viruses are either spillover events -- isolated infections that fail to cause a widespread contagion -- or species jumps , where successful adaptation to the new host leads to a pandemic . despite expensive bio-surveillance efforts , historically emergence response has been reactive , and post-hoc . here we use machine inference to demonstrate a high accuracy predictive bio-surveillance capability , designed to pro-actively localize an impending species jump via automated interrogation of massive sequence databases of viral proteins . our results suggest that a jump might not purely be the result of an isolated unfortunate cross-infection localized in space and time ; there are subtle yet detectable patterns of genotypic changes accumulating in the global viral population leading up to emergence . using tens of thousands of protein sequences simultaneously , we train models that track maximum achievable accuracy for disambiguating host tropism from the primary structure of surface proteins , and show that the inverse classification accuracy is a quantitative indicator of jump risk . we validate our claim in the context of the 2009 swine flu outbreak , and the 2004 emergence of h5n1 subspecies of influenza a from avian reservoirs ; illustrating that interrogation of the global viral population can unambiguously track a near monotonic risk elevation over several preceding years leading to eventual emergence .", "topics": ["database"]}
{"title": "shirtless and dangerous : quantifying linguistic signals of gender bias in an online fiction writing community", "abstract": "imagine a princess asleep in a castle , waiting for her prince to slay the dragon and rescue her . tales like the famous sleeping beauty clearly divide up gender roles . but what about more modern stories , borne of a generation increasingly aware of social constructs like sexism and racism ? do these stories tend to reinforce gender stereotypes , or counter them ? in this paper , we present a technique that combines natural language processing with a crowdsourced lexicon of stereotypes to capture gender biases in fiction . we apply this technique across 1.8 billion words of fiction from the wattpad online writing community , investigating gender representation in stories , how male and female characters behave and are described , and how authors ' use of gender stereotypes is associated with the community 's ratings . we find that male over-representation and traditional gender stereotypes ( e.g . , dominant men and submissive women ) are common throughout nearly every genre in our corpus . however , only some of these stereotypes , like sexual or violent men , are associated with highly rated stories . finally , despite women often being the target of negative stereotypes , female authors are equally likely to write such stereotypes as men .", "topics": ["natural language processing", "text corpus"]}
{"title": "heuristic search value iteration for pomdps", "abstract": "we present a novel pomdp planning algorithm called heuristic search value iteration ( hsvi ) .hsvi is an anytime algorithm that returns a policy and a provable bound on its regret with respect to the optimal policy . hsvi gets its power by combining two well-known techniques : attention-focusing search heuristics and piecewise linear convex representations of the value function . hsvi 's soundness and convergence have been proven . on some benchmark problems from the literature , hsvi displays speedups of greater than 100 with respect to other state-of-the-art pomdp value iteration algorithms . we also apply hsvi to a new rover exploration problem 10 times larger than most pomdp problems in the literature .", "topics": ["regret ( decision theory )", "iteration"]}
{"title": "from traditional to modern : domain adaptation for action classification in short social video clips", "abstract": "short internet video clips like vines present a significantly wild distribution compared to traditional video datasets . in this paper , we focus on the problem of unsupervised action classification in wild vines using traditional labeled datasets . to this end , we use a data augmentation based simple domain adaptation strategy . we utilise semantic word2vec space as a common subspace to embed video features from both , labeled source domain and unlablled target domain . our method incrementally augments the labeled source with target samples and iteratively modifies the embedding function to bring the source and target distributions together . additionally , we utilise a multi-modal representation that incorporates noisy semantic information available in form of hash-tags . we show the effectiveness of this simple adaptation technique on a test set of vines and achieve notable improvements in performance .", "topics": ["test set", "unsupervised learning"]}
{"title": "the difficulties of learning logic programs with cut", "abstract": "as real logic programmers normally use cut ( ! ) , an effective learning procedure for logic programs should be able to deal with it . because the cut predicate has only a procedural meaning , clauses containing cut can not be learned using an extensional evaluation method , as is done in most learning systems . on the other hand , searching a space of possible programs ( instead of a space of independent clauses ) is unfeasible . an alternative solution is to generate first a candidate base program which covers the positive examples , and then make it consistent by inserting cut where appropriate . the problem of learning programs with cut has not been investigated before and this seems to be a natural and reasonable approach . we generalize this scheme and investigate the difficulties that arise . some of the major shortcomings are actually caused , in general , by the need for intensional evaluation . as a conclusion , the analysis of this paper suggests , on precise and technical grounds , that learning cut is difficult , and current induction techniques should probably be restricted to purely declarative logic languages .", "topics": ["causality"]}
{"title": "research on the multiple feature fusion image retrieval algorithm based on texture feature and rough set theory", "abstract": "recently , we have witnessed the explosive growth of images with complex information and content . in order to effectively and precisely retrieve desired images from a large-scale image database with low time-consuming , we propose the multiple feature fusion image retrieval algorithm based on the texture feature and rough set theory in this paper . in contrast to the conventional approaches that only use the single feature or standard , we fuse the different features with operation of normalization . the rough set theory will assist us to enhance the robustness of retrieval system when facing with incomplete data warehouse . to enhance the texture extraction paradigm , we use the wavelet gabor function that holds better robustness . in addition , from the perspectives of the internal and external normalization , we re-organize extracted feature with the better combination . the numerical experiment has verified general feasibility of our methodology . we enhance the overall accuracy compared with the other state-of-the-art algorithms .", "topics": ["numerical analysis"]}
{"title": "learning to recommend via inverse optimal matching", "abstract": "we consider recommendation in the context of optimal matching , i.e . , we need to pair or match a user with an item in an optimal way . the framework is particularly relevant when the supply of an individual item is limited and it can only satisfy a small number of users even though it may be preferred by many . we leverage the methodology of optimal transport of discrete distributions and formulate an inverse optimal transport problem in order to learn the cost which gives rise to the observed matching . it leads to a non-convex optimization problem which is solved by alternating optimization . a key novel aspect of our formulation is the incorporation of marginal relaxation via regularized wasserstein distance , significantly improving the robustness of the method in the face of observed empirical matchings . our model has wide applicability including labor market , online dating , college application recommendation . we back up our claims with experiments on both synthetic data and real world datasets .", "topics": ["optimization problem", "synthetic data"]}
{"title": "modern hierarchical , agglomerative clustering algorithms", "abstract": "this paper presents algorithms for hierarchical , agglomerative clustering which perform most efficiently in the general-purpose setup that is given in modern standard software . requirements are : ( 1 ) the input data is given by pairwise dissimilarities between data points , but extensions to vector data are also discussed ( 2 ) the output is a `` stepwise dendrogram '' , a data structure which is shared by all implementations in current standard software . we present algorithms ( old and new ) which perform clustering in this setting efficiently , both in an asymptotic worst-case analysis and from a practical point of view . the main contributions of this paper are : ( 1 ) we present a new algorithm which is suitable for any distance update scheme and performs significantly better than the existing algorithms . ( 2 ) we prove the correctness of two algorithms by rohlf and murtagh , which is necessary in each case for different reasons . ( 3 ) we give well-founded recommendations for the best current algorithms for the various agglomerative clustering schemes .", "topics": ["cluster analysis"]}
{"title": "distributed evolutionary computation : a new technique for solving large number of equations", "abstract": "evolutionary computation techniques have mostly been used to solve various optimization and learning problems successfully . evolutionary algorithm is more effective to gain optimal solution ( s ) to solve complex problems than traditional methods . in case of problems with large set of parameters , evolutionary computation technique incurs a huge computational burden for a single processing unit . taking this limitation into account , this paper presents a new distributed evolutionary computation technique , which decomposes decision vectors into smaller components and achieves optimal solution in a short time . in this technique , a jacobi-based time variant adaptive ( jbtva ) hybrid evolutionary algorithm is distributed incorporating cluster computation . moreover , two new selection methods named best all selection ( bas ) and twin selection ( ts ) are introduced for selecting best fit solution vector . experimental results show that optimal solution is achieved for different kinds of problems having huge parameters and a considerable speedup is obtained in proposed distributed system .", "topics": ["optimization problem", "computation"]}
{"title": "instantaneously trained neural networks", "abstract": "this paper presents a review of instantaneously trained neural networks ( itnns ) . these networks trade learning time for size and , in the basic model , a new hidden node is created for each training sample . various versions of the corner-classification family of itnns , which have found applications in artificial intelligence ( ai ) , are described . implementation issues are also considered .", "topics": ["neural networks", "artificial intelligence"]}
{"title": "algorithms for batch hierarchical reinforcement learning", "abstract": "hierarchical reinforcement learning ( hrl ) exploits temporal abstraction to solve large markov decision processes ( mdp ) and provide transferable subtask policies . in this paper , we introduce an off-policy hrl algorithm : hierarchical q-value iteration ( hqi ) . we show that it is possible to effectively learn recursive optimal policies for any valid hierarchical decomposition of the original mdp , given a fixed dataset collected from a flat stochastic behavioral policy . we first formally prove the convergence of the algorithm for tabular mdp . then our experiments on the taxi domain show that hqi converges faster than a flat q-value iteration and enjoys easy state abstraction . also , we demonstrate that our algorithm is able to learn optimal policies for different hierarchical structures from the same fixed dataset , which enables model comparison without recollecting data .", "topics": ["reinforcement learning", "iteration"]}
{"title": "fast and robust least squares estimation in corrupted linear models", "abstract": "subsampling methods have been recently proposed to speed up least squares estimation in large scale settings . however , these algorithms are typically not robust to outliers or corruptions in the observed covariates . the concept of influence that was developed for regression diagnostics can be used to detect such corrupted observations as shown in this paper . this property of influence -- for which we also develop a randomized approximation -- motivates our proposed subsampling algorithm for large scale corrupted linear regression which limits the influence of data points since highly influential points contribute most to the residual error . under a general model of corrupted observations , we show theoretically and empirically on a variety of simulated and real datasets that our algorithm improves over the current state-of-the-art approximation schemes for ordinary least squares .", "topics": ["simulation"]}
{"title": "ising processing units : potential and challenges for discrete optimization", "abstract": "the recent emergence of novel computational devices , such as adiabatic quantum computers , cmos annealers , and optical parametric oscillators , presents new opportunities for hybrid-optimization algorithms that leverage these kinds of specialized hardware . in this work , we propose the idea of an ising processing unit as a computational abstraction for these emerging tools . challenges involved in using and benchmarking these devices are presented , and open-source software tools are proposed to address some of these challenges . the proposed benchmarking tools and methodology are demonstrated by conducting a baseline study of established solution methods to a d-wave 2x adiabatic quantum computer , one example of a commercially available ising processing unit .", "topics": ["baseline ( configuration management )"]}
{"title": "a two-layer local constrained sparse coding method for fine-grained visual categorization", "abstract": "fine-grained categories are more difficulty distinguished than generic categories due to the similarity of inter-class and the diversity of intra-class . therefore , the fine-grained visual categorization ( fgvc ) is considered as one of challenge problems in computer vision recently . a new feature learning framework , which is based on a two-layer local constrained sparse coding architecture , is proposed in this paper . the two-layer architecture is introduced for learning intermediate-level features , and the local constrained term is applied to guarantee the local smooth of coding coefficients . for extracting more discriminative information , local orientation histograms are the input of sparse coding instead of raw pixels . moreover , a quick dictionary updating process is derived to further improve the training speed . two experimental results show that our method achieves 85.29 % accuracy on the oxford 102 flowers dataset and 67.8 % accuracy on the cub-200-2011 bird dataset , and the performance of our framework is highly competitive with existing literatures .", "topics": ["feature learning", "computer vision"]}
{"title": "neural networks and database systems", "abstract": "object-oriented database systems proved very valuable at handling and administrating complex objects . in the following guidelines for embedding neural networks into such systems are presented . it is our goal to treat networks as normal data in the database system . from the logical point of view , a neural network is a complex data value and can be stored as a normal data object . it is generally accepted that rule-based reasoning will play an important role in future database applications . the knowledge base consists of facts and rules , which are both stored and handled by the underlying database system . neural networks can be seen as representation of intensional knowledge of intelligent database systems . so they are part of a rule based knowledge pool and can be used like conventional rules . the user has a unified view about his knowledge base regardless of the origin of the unique rules .", "topics": ["neural networks"]}
{"title": "comparing writing styles using word embedding and dynamic time warping", "abstract": "the development of plot or story in novels is reflected in the content and the words used . the flow of sentiments , which is one aspect of writing style , can be quantified by analyzing the flow of words . this study explores literary works as signals in word embedding space and tries to compare writing styles of popular classic novels using dynamic time warping .", "topics": ["natural language processing", "time series"]}
{"title": "towards cooperation in sequential prisoner 's dilemmas : a deep multiagent reinforcement learning approach", "abstract": "the iterated prisoner 's dilemma has guided research on social dilemmas for decades . however , it distinguishes between only two atomic actions : cooperate and defect . in real-world prisoner 's dilemmas , these choices are temporally extended and different strategies may correspond to sequences of actions , reflecting grades of cooperation . we introduce a sequential prisoner 's dilemma ( spd ) game to better capture the aforementioned characteristics . in this work , we propose a deep multiagent reinforcement learning approach that investigates the evolution of mutual cooperation in spd games . our approach consists of two phases . the first phase is offline : it synthesizes policies with different cooperation degrees and then trains a cooperation degree detection network . the second phase is online : an agent adaptively selects its policy based on the detected degree of opponent cooperation . the effectiveness of our approach is demonstrated in two representative spd 2d games : the apple-pear game and the fruit gathering game . experimental results show that our strategy can avoid being exploited by exploitative opponents and achieve cooperation with cooperative opponents .", "topics": ["reinforcement learning"]}
{"title": "a simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling", "abstract": "we introduce a simple and accurate neural model for dependency-based semantic role labeling . our model predicts predicate-argument dependencies relying on states of a bidirectional lstm encoder . the semantic role labeler achieves competitive performance on english , even without any kind of syntactic information and only using local inference . however , when automatically predicted part-of-speech tags are provided as input , it substantially outperforms all previous local models and approaches the best reported results on the english conll-2009 dataset . we also consider chinese , czech and spanish where our approach also achieves competitive results . syntactic parsers are unreliable on out-of-domain data , so standard ( i.e . , syntactically-informed ) srl models are hindered when tested in this setting . our syntax-agnostic model appears more robust , resulting in the best reported results on standard out-of-domain test sets .", "topics": ["parsing"]}
{"title": "foundations of the pareto iterated local search metaheuristic", "abstract": "the paper describes the proposition and application of a local search metaheuristic for multi-objective optimization problems . it is based on two main principles of heuristic search , intensification through variable neighborhoods , and diversification through perturbations and successive iterations in favorable regions of the search space . the concept is successfully tested on permutation flow shop scheduling problems under multiple objectives . while the obtained results are encouraging in terms of their quality , another positive attribute of the approach is its ' simplicity as it does require the setting of only very few parameters . the implementation of the pareto iterated local search metaheuristic is based on the moopps computer system of local search heuristics for multi-objective scheduling which has been awarded the european academic software award 2002 in ronneby , sweden ( http : //www.easa-award.net/ , http : //www.bth.se/llab/easa_2002.nsf )", "topics": ["iteration", "heuristic"]}
{"title": "nonparametric relational topic models through dependent gamma processes", "abstract": "traditional relational topic models provide a way to discover the hidden topics from a document network . many theoretical and practical tasks , such as dimensional reduction , document clustering , link prediction , benefit from this revealed knowledge . however , existing relational topic models are based on an assumption that the number of hidden topics is known in advance , and this is impractical in many real-world applications . therefore , in order to relax this assumption , we propose a nonparametric relational topic model in this paper . instead of using fixed-dimensional probability distributions in its generative model , we use stochastic processes . specifically , a gamma process is assigned to each document , which represents the topic interest of this document . although this method provides an elegant solution , it brings additional challenges when mathematically modeling the inherent network structure of typical document network , i.e . , two spatially closer documents tend to have more similar topics . furthermore , we require that the topics are shared by all the documents . in order to resolve these challenges , we use a subsampling strategy to assign each document a different gamma process from the global gamma process , and the subsampling probabilities of documents are assigned with a markov random field constraint that inherits the document network structure . through the designed posterior inference algorithm , we can discover the hidden topics and its number simultaneously . experimental results on both synthetic and real-world network datasets demonstrate the capabilities of learning the hidden topics and , more importantly , the number of topics .", "topics": ["generative model", "cluster analysis"]}
{"title": "continuous learning : engineering super features with feature algebras", "abstract": "in this paper we consider a problem of searching a space of predictive models for a given training data set . we propose an iterative procedure for deriving a sequence of improving models and a corresponding sequence of sets of non-linear features on the original input space . after a finite number of iterations n , the non-linear features become 2^n -degree polynomials on the original space . we show that in a limit of an infinite number of iterations derived non-linear features must form an associative algebra : a product of two features is equal to a linear combination of features from the same feature space for any given input point . because each iteration consists of solving a series of convex problems that contain all previous solutions , the likelihood of the models in the sequence is increasing with each iteration while the dimension of the model parameter space is set to a limited controlled value .", "topics": ["test set", "feature vector"]}
{"title": "approximate positively correlated distributions and approximation algorithms for d-optimal design", "abstract": "experimental design is a classical problem in statistics and has also found new applications in machine learning . in the experimental design problem , the aim is to estimate an unknown vector x in m-dimensions from linear measurements where a gaussian noise is introduced in each measurement . the goal is to pick k out of the given n experiments so as to make the most accurate estimate of the unknown parameter x . given a set s of chosen experiments , the most likelihood estimate x ' can be obtained by a least squares computation . one of the robust measures of error estimation is the d-optimality criterion which aims to minimize the generalized variance of the estimator . this corresponds to minimizing the volume of the standard confidence ellipsoid for the estimation error x-x ' . the problem gives rise to two natural variants depending on whether repetitions are allowed or not . the latter variant , while being more general , has also found applications in the geographical location of sensors . in this work , we first show that a 1/e-approximation for the d-optimal design problem with and without repetitions giving us the first constant factor approximation for the problem . we also consider the case when the number of experiments chosen is much larger than the dimension of the measurements and provide an asymptotically optimal approximation algorithm .", "topics": ["approximation algorithm", "approximation"]}
{"title": "stamp processing with examplar features", "abstract": "document digitization is becoming increasingly crucial . in this work , we propose a shape based approach for automatic stamp verification/detection in document images using an unsupervised feature learning . given a small set of training images , our algorithm learns an appropriate shape representation using an unsupervised clustering . experimental results demonstrate the effectiveness of our framework in challenging scenarios .", "topics": ["feature learning", "unsupervised learning"]}
{"title": "deep learning from crowds", "abstract": "over the last few years , deep learning has revolutionized the field of machine learning by dramatically improving the state-of-the-art in various domains . however , as the size of supervised artificial neural networks grows , typically so does the need for larger labeled datasets . recently , crowdsourcing has established itself as an efficient and cost-effective solution for labeling large sets of data in a scalable manner , but it often requires aggregating labels from multiple noisy contributors with different levels of expertise . in this paper , we address the problem of learning deep neural networks from crowds . we begin by describing an em algorithm for jointly learning the parameters of the network and the reliabilities of the annotators . then , a novel general-purpose crowd layer is proposed , which allows us to train deep neural networks end-to-end , directly from the noisy labels of multiple annotators , using only backpropagation . we empirically show that the proposed approach is able to internally capture the reliability and biases of different annotators and achieve new state-of-the-art results for various crowdsourced datasets across different settings , namely classification , regression and sequence labeling .", "topics": ["statistical classification", "end-to-end principle"]}
{"title": "error asymmetry in causal and anticausal regression", "abstract": "it is generally difficult to make any statements about the expected prediction error in an univariate setting without further knowledge about how the data were generated . recent work showed that knowledge about the real underlying causal structure of a data generation process has implications for various machine learning settings . assuming an additive noise and an independence between data generating mechanism and its input , we draw a novel connection between the intrinsic causal relationship of two variables and the expected prediction error . we formulate the theorem that the expected error of the true data generating function as prediction model is generally smaller when the effect is predicted from its cause and , on the contrary , greater when the cause is predicted from its effect . the theorem implies an asymmetry in the error depending on the prediction direction . this is further corroborated with empirical evaluations in artificial and real-world data sets .", "topics": ["causality"]}
{"title": "density adaptive parallel clustering", "abstract": "in this paper we are going to introduce a new nearest neighbours based approach to clustering , and compare it with previous solutions ; the resulting algorithm , which takes inspiration from both dbscan and minimum spanning tree approaches , is deterministic but proves simpler , faster and doesnt require to set in advance a value for k , the number of clusters .", "topics": ["cluster analysis"]}
{"title": "weakly submodular maximization beyond cardinality constraints : does randomization help greedy ?", "abstract": "submodular functions are a broad class of set functions , which naturally arise in diverse areas . many algorithms have been suggested for the maximization of these functions . unfortunately , once the function deviates from submodularity , the known algorithms may perform arbitrarily poorly . amending this issue , by obtaining approximation results for set functions generalizing submodular functions , has been the focus of recent works . one such class , known as weakly submodular functions , has received a lot of attention . a key result proved by das and kempe ( 2011 ) showed that the approximation ratio of the greedy algorithm for weakly submodular maximization subject to a cardinality constraint degrades smoothly with the distance from submodularity . however , no results have been obtained for maximization subject to constraints beyond cardinality . in particular , it is not known whether the greedy algorithm achieves any non-trivial approximation ratio for such constraints . in this paper , we prove that a randomized version of the greedy algorithm ( previously used by buchbinder et al . ( 2014 ) for a different problem ) achieves an approximation ratio of $ ( 1 + 1/\\gamma ) ^ { -2 } $ for the maximization of a weakly submodular function subject to a general matroid constraint , where $ \\gamma $ is a parameter measuring the distance of the function from submodularity . moreover , we also experimentally compare the performance of this version of the greedy algorithm on real world problems against natural benchmarks , and show that the algorithm we study performs well also in practice . to the best of our knowledge , this is the first algorithm with a non-trivial approximation guarantee for maximizing a weakly submodular function subject to a constraint other than the simple cardinality constraint . in particular , it is the first algorithm with such a guarantee for the important and broad class of matroid constraints .", "topics": ["approximation algorithm", "sparse matrix"]}
{"title": "the unreasonable effectiveness of deep features as a perceptual metric", "abstract": "while it is nearly effortless for humans to quickly assess the perceptual similarity between two images , the underlying processes are thought to be quite complex . despite this , the most widely used perceptual metrics today , such as psnr and ssim , are simple , shallow functions , and fail to account for many nuances of human perception . recently , the deep learning community has found that features of the vgg network trained on the imagenet classification task has been remarkably useful as a training loss for image synthesis . but how perceptual are these so-called `` perceptual losses '' ? what elements are critical for their success ? to answer these questions , we introduce a new full reference image quality assessment ( fr-iqa ) dataset of perceptual human judgments , orders of magnitude larger than previous datasets . we systematically evaluate deep features across different architectures and tasks and compare them with classic metrics . we find that deep features outperform all previous metrics by huge margins . more surprisingly , this result is not restricted to imagenet-trained vgg features , but holds across different deep architectures and levels of supervision ( supervised , self-supervised , or even unsupervised ) . our results suggest that perceptual similarity is an emergent property shared across deep visual representations .", "topics": ["computer vision", "pixel"]}
{"title": "syntax and semantics of italian poetry in the first half of the 20th century", "abstract": "in this paper we study , analyse and comment rhetorical figures present in some of most interesting poetry of the first half of the twentieth century . these figures are at first traced back to some famous poet of the past and then compared to classical latin prose . linguistic theory is then called in to show how they can be represented in syntactic structures and classified as noncanonical structures , by positioning discontinuous or displaced linguistic elements in spec xp projections at various levels of constituency . then we introduce lfg ( lexical functional grammar ) as the theory that allows us to connect syntactic noncanonical structures with informational structure and psycholinguistic theories for complexity evaluation . we end up with two computational linguistics experiments and then evaluate the results . the first one uses best online parsers of italian to parse poetic structures ; the second one uses getarun , the system created at ca foscari computational linguistics laboratory . as will be shown , the first approach is unable to cope with these structures due to the use of only statistical probabilistic information . on the contrary , the second one , being a symbolic rule based system , is by far superior and allows also to complete both semantic an pragmatic analysis .", "topics": ["parsing"]}
{"title": "deep learning and its applications to machine health monitoring : a survey", "abstract": "since 2006 , deep learning ( dl ) has become a rapidly growing research direction , redefining state-of-the-art performances in a wide range of areas such as object recognition , image segmentation , speech recognition and machine translation . in modern manufacturing systems , data-driven machine health monitoring is gaining in popularity due to the widespread deployment of low-cost sensors and their connection to the internet . meanwhile , deep learning provides useful tools for processing and analyzing these big machinery data . the main purpose of this paper is to review and summarize the emerging research work of deep learning on machine health monitoring . after the brief introduction of deep learning techniques , the applications of deep learning in machine health monitoring systems are reviewed mainly from the following aspects : auto-encoder ( ae ) and its variants , restricted boltzmann machines and its variants including deep belief network ( dbn ) and deep boltzmann machines ( dbm ) , convolutional neural networks ( cnn ) and recurrent neural networks ( rnn ) . finally , some new trends of dl-based machine health monitoring methods are discussed .", "topics": ["recurrent neural network", "machine translation"]}
{"title": "denoising of 3d magnetic resonance images with multi-channel residual learning of convolutional neural network", "abstract": "the denoising of magnetic resonance ( mr ) images is a task of great importance for improving the acquired image quality . many methods have been proposed in the literature to retrieve noise free images with good performances . howerever , the state-of-the-art denoising methods , all needs a time-consuming optimization processes and their performance strongly depend on the estimated noise level parameter . within this manuscript we propose the idea of denoising mri rician noise using a convolutional neural network . the advantage of the proposed methodology is that the learning based model can be directly used in the denosing process without optimization and even without the noise level parameter . specifically , a ten convolutional layers neural network combined with residual learning and multi-channel strategy was proposed . two training ways : training on a specific noise level and training on a general level were conducted to demonstrate the capability of our methods . experimental results over synthetic and real 3d mr data demonstrate our proposed network can achieve superior performance compared with other methods in term of both of the peak signal to noise ratio and the global of structure similarity index . without noise level parameter , our general noise-applicable model is also better than the other compared methods in two datasets . furthermore , our training model shows good general applicability .", "topics": ["noise reduction", "synthetic data"]}
{"title": "circumventing the curse of dimensionality in prediction : causal rate-distortion for infinite-order markov processes", "abstract": "predictive rate-distortion analysis suffers from the curse of dimensionality : clustering arbitrarily long pasts to retain information about arbitrarily long futures requires resources that typically grow exponentially with length . the challenge is compounded for infinite-order markov processes , since conditioning on finite sequences can not capture all of their past dependencies . spectral arguments show that algorithms which cluster finite-length sequences fail dramatically when the underlying process has long-range temporal correlations and can fail even for processes generated by finite-memory hidden markov models . we circumvent the curse of dimensionality in rate-distortion analysis of infinite-order processes by casting predictive rate-distortion objective functions in terms of the forward- and reverse-time causal states of computational mechanics . examples demonstrate that the resulting causal rate-distortion theory substantially improves current predictive rate-distortion analyses .", "topics": ["causality"]}
{"title": "auto analysis of customer feedback using cnn and gru network", "abstract": "analyzing customer feedback is the best way to channelize the data into new marketing strategies that benefit entrepreneurs as well as customers . therefore an automated system which can analyze the customer behavior is in great demand . users may write feedbacks in any language , and hence mining appropriate information often becomes intractable . especially in a traditional feature-based supervised model , it is difficult to build a generic system as one has to understand the concerned language for finding the relevant features . in order to overcome this , we propose deep convolutional neural network ( cnn ) and recurrent neural network ( rnn ) based approaches that do not require handcrafting of features . we evaluate these techniques for analyzing customer feedback sentences in four languages , namely english , french , japanese and spanish . our empirical analysis shows that our models perform well in all the four languages on the setups of ijcnlp shared task on customer feedback analysis . our model achieved the second rank in french , with an accuracy of 71.75 % and third ranks for all the other languages .", "topics": ["recurrent neural network"]}
{"title": "crf autoencoder for unsupervised dependency parsing", "abstract": "unsupervised dependency parsing , which tries to discover linguistic dependency structures from unannotated data , is a very challenging task . almost all previous work on this task focuses on learning generative models . in this paper , we develop an unsupervised dependency parsing model based on the crf autoencoder . the encoder part of our model is discriminative and globally normalized which allows us to use rich features as well as universal linguistic priors . we propose an exact algorithm for parsing as well as a tractable learning algorithm . we evaluated the performance of our model on eight multilingual treebanks and found that our model achieved comparable performance with state-of-the-art approaches .", "topics": ["generative model", "unsupervised learning"]}
{"title": "geometric scene parsing with hierarchical lstm", "abstract": "this paper addresses the problem of geometric scene parsing , i.e . simultaneously labeling geometric surfaces ( e.g . sky , ground and vertical plane ) and determining the interaction relations ( e.g . layering , supporting , siding and affinity ) between main regions . this problem is more challenging than the traditional semantic scene labeling , as recovering geometric structures necessarily requires the rich and diverse contextual information . to achieve these goals , we propose a novel recurrent neural network model , named hierarchical long short-term memory ( h-lstm ) . it contains two coupled sub-networks : the pixel lstm ( p-lstm ) and the multi-scale super-pixel lstm ( ms-lstm ) for handling the surface labeling and relation prediction , respectively . the two sub-networks provide complementary information to each other to exploit hierarchical scene contexts , and they are jointly optimized for boosting the performance . our extensive experiments show that our model is capable of parsing scene geometric structures and outperforming several state-of-the-art methods by large margins . in addition , we show promising 3d reconstruction results from the still images based on the geometric parsing .", "topics": ["recurrent neural network", "parsing"]}
{"title": "counterfactual prediction with deep instrumental variables networks", "abstract": "we are in the middle of a remarkable rise in the use and capability of artificial intelligence . much of this growth has been fueled by the success of deep learning architectures : models that map from observables to outputs via multiple layers of latent representations . these deep learning algorithms are effective tools for unstructured prediction , and they can be combined in ai systems to solve complex automated reasoning problems . this paper provides a recipe for combining ml algorithms to solve for causal effects in the presence of instrumental variables -- sources of treatment randomization that are conditionally independent from the response . we show that a flexible iv specification resolves into two prediction tasks that can be solved with deep neural nets : a first-stage network for treatment prediction and a second-stage network whose loss function involves integration over the conditional treatment distribution . this deep iv framework imposes some specific structure on the stochastic gradient descent routine used for training , but it is general enough that we can take advantage of off-the-shelf ml capabilities and avoid extensive algorithm customization . we outline how to obtain out-of-sample causal validation in order to avoid over-fit . we also introduce schemes for both bayesian and frequentist inference : the former via a novel adaptation of dropout training , and the latter via a data splitting routine .", "topics": ["loss function", "gradient descent"]}
{"title": "strongly adaptive online learning", "abstract": "strongly adaptive algorithms are algorithms whose performance on every time interval is close to optimal . we present a reduction that can transform standard low-regret algorithms to strongly adaptive . as a consequence , we derive simple , yet efficient , strongly adaptive algorithms for a handful of problems .", "topics": ["regret ( decision theory )", "approximation algorithm"]}
{"title": "a framework for the analysis of computational imaging systems with practical applications", "abstract": "over the last decade , a number of computational imaging ( ci ) systems have been proposed for tasks such as motion deblurring , defocus deblurring and multispectral imaging . these techniques increase the amount of light reaching the sensor via multiplexing and then undo the deleterious effects of multiplexing by appropriate reconstruction algorithms . given the widespread appeal and the considerable enthusiasm generated by these techniques , a detailed performance analysis of the benefits conferred by this approach is important . unfortunately , a detailed analysis of ci has proven to be a challenging problem because performance depends equally on three components : ( 1 ) the optical multiplexing , ( 2 ) the noise characteristics of the sensor , and ( 3 ) the reconstruction algorithm . a few recent papers have performed analysis taking multiplexing and noise characteristics into account . however , analysis of ci systems under state-of-the-art reconstruction algorithms , most of which exploit signal prior models , has proven to be unwieldy . in this paper , we present a comprehensive analysis framework incorporating all three components . in order to perform this analysis , we model the signal priors using a gaussian mixture model ( gmm ) . a gmm prior confers two unique characteristics . firstly , gmm satisfies the universal approximation property which says that any prior density function can be approximated to any fidelity using a gmm with appropriate number of mixtures . secondly , a gmm prior lends itself to analytical tractability allowing us to derive simple expressions for the `minimum mean square error ' ( mmse ) , which we use as a metric to characterize the performance of ci systems . we use our framework to analyze several previously proposed ci techniques , giving conclusive answer to the question : `how much performance gain is due to use of a signal prior and how much is due to multiplexing ?", "topics": ["computational complexity theory"]}
{"title": "classification of breast cancer histology using deep learning", "abstract": "breast cancer is a major cause of death worldwide among women . hematoxylin and eosin ( h & e ) stained breast tissue samples from biopsies are observed under microscopes for the primary diagnosis of breast cancer . in this paper , we propose a deep learning-based method for classification of h & e stained breast tissue images released for bach challenge 2018 by fine-tuning inception-v3 convolutional neural network ( cnn ) proposed by szegedy et al . these images are to be classified into four classes namely , i ) normal tissue , ii ) benign tumor , iii ) in-situ carcinoma and iv ) invasive carcinoma . our strategy is to extract patches based on nuclei density instead of random or grid sampling , along with rejection of patches that are not rich in nuclei ( non-epithelial ) regions for training and testing . every patch ( nuclei-dense region ) in an image is classified in one of the four above mentioned categories . the class of the entire image is determined using majority voting over the nuclear classes . we obtained an average four class accuracy of 85 % and an average two class ( non-cancer vs. carcinoma ) accuracy of 93 % , which improves upon a previous benchmark by araujo et al .", "topics": ["sampling ( signal processing )"]}
{"title": "steepest ascent hill climbing for a mathematical problem", "abstract": "the paper proposes artificial intelligence technique called hill climbing to find numerical solutions of diophantine equations . such equations are important as they have many applications in fields like public key cryptography , integer factorization , algebraic curves , projective curves and data dependency in super computers . importantly , it has been proved that there is no general method to find solutions of such equations . this paper is an attempt to find numerical solutions of diophantine equations using steepest ascent version of hill climbing . the method , which uses tree representation to depict possible solutions of diophantine equations , adopts a novel methodology to generate successors . the heuristic function used help to make the process of finding solution as a minimization process . the work illustrates the effectiveness of the proposed methodology using a class of diophantine equations given by a1 . x1 p1 + a2 . x2 p2 + ... ... + an . xn pn = n where ai and n are integers . the experimental results validate that the procedure proposed is successful in finding solutions of diophantine equations with sufficiently large powers and large number of variables .", "topics": ["numerical analysis", "gradient descent"]}
{"title": "an inequality paradigm for probabilistic knowledge", "abstract": "we propose an inequality paradigm for probabilistic reasoning based on a logic of upper and lower bounds on conditional probabilities . we investigate a family of probabilistic logics , generalizing the work of nilsson [ 14 ] . we develop a variety of logical notions for probabilistic reasoning , including soundness , completeness justification ; and convergence : reduction of a theory to a simpler logical class . we argue that a bound view is especially useful for describing the semantics of probabilistic knowledge representation and for describing intermediate states of probabilistic inference and updating . we show that the dempster-shafer theory of evidence is formally identical to a special case of our generalized probabilistic logic . our paradigm thus incorporates both bayesian `` rule-based '' approaches and avowedly non-bayesian `` evidential '' approaches such as mycin and dempstershafer . we suggest how to integrate the two `` schools '' , and explore some possibilities for novel synthesis of a variety of ideas in probabilistic reasoning .", "topics": ["value ( ethics )"]}
{"title": "large-margin classification with multiple decision rules", "abstract": "binary classification is a common statistical learning problem in which a model is estimated on a set of covariates for some outcome indicating the membership of one of two classes . in the literature , there exists a distinction between hard and soft classification . in soft classification , the conditional class probability is modeled as a function of the covariates . in contrast , hard classification methods only target the optimal prediction boundary . while hard and soft classification methods have been studied extensively , not much work has been done to compare the actual tasks of hard and soft classification . in this paper we propose a spectrum of statistical learning problems which span the hard and soft classification tasks based on fitting multiple decision rules to the data . by doing so , we reveal a novel collection of learning tasks of increasing complexity . we study the problems using the framework of large-margin classifiers and a class of piecewise linear convex surrogates , for which we derive statistical properties and a corresponding sub-gradient descent algorithm . we conclude by applying our approach to simulation settings and a magnetic resonance imaging ( mri ) dataset from the alzheimer 's disease neuroimaging initiative ( adni ) study .", "topics": ["statistical classification", "simulation"]}
{"title": "fast verification of convexity of piecewise-linear surfaces", "abstract": "we show that a realization of a closed connected pl-manifold of dimension n-1 in n-dimensional euclidean space ( n > 2 ) is the boundary of a convex polyhedron ( finite or infinite ) if and only if the interior of each ( n-3 ) -face has a point , which has a neighborhood lying on the boundary of an n-dimensional convex body . no initial assumptions about the topology or orientability of the input surface are made . the theorem is derived from a refinement and generalization of van heijenoort 's theorem on locally convex manifolds to spherical spaces . our convexity criterion for pl-manifolds implies an easy polynomial-time algorithm for checking convexity of a given pl-surface in n-dimensional euclidean or spherical space , n > 2 . the algorithm is worst case optimal with respect to both the number of operations and the algebraic degree . the algorithm works under significantly weaker assumptions and is easier to implement than convexity verification algorithms suggested by mehlhorn et al ( 1996-1999 ) , and devillers et al . ( 1998 ) . a paradigm of approximate convexity is suggested and a simplified algorithm of smaller degree and complexity is suggested for approximate floating point convexity verification .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "potential conditional mutual information : estimators , properties and applications", "abstract": "the conditional mutual information i ( x ; y|z ) measures the average information that x and y contain about each other given z . this is an important primitive in many learning problems including conditional independence testing , graphical model inference , causal strength estimation and time-series problems . in several applications , it is desirable to have a functional purely of the conditional distribution p_ { y|x , z } rather than of the joint distribution p_ { x , y , z } . we define the potential conditional mutual information as the conditional mutual information calculated with a modified joint distribution p_ { y|x , z } q_ { x , z } , where q_ { x , z } is a potential distribution , fixed airport . we develop k nearest neighbor based estimators for this functional , employing importance sampling , and a coupling trick , and prove the finite k consistency of such an estimator . we demonstrate that the estimator has excellent practical performance and show an application in dynamical system inference .", "topics": ["sampling ( signal processing )", "graphical model"]}
{"title": "exploiting local structures with the kronecker layer in convolutional networks", "abstract": "in this paper , we propose and study a technique to reduce the number of parameters and computation time in convolutional neural networks . we use kronecker product to exploit the local structures within convolution and fully-connected layers , by replacing the large weight matrices by combinations of multiple kronecker products of smaller matrices . just as the kronecker product is a generalization of the outer product from vectors to matrices , our method is a generalization of the low rank approximation method for convolution neural networks . we also introduce combinations of different shapes of kronecker product to increase modeling capacity . experiments on svhn , scene text recognition and imagenet dataset demonstrate that we can achieve $ 3.3 \\times $ speedup or $ 3.6 \\times $ parameter reduction with less than 1\\ % drop in accuracy , showing the effectiveness and efficiency of our method . moreover , the computation efficiency of kronecker layer makes using larger feature map possible , which in turn enables us to outperform the previous state-of-the-art on both svhn ( digit recognition ) and casia-hwdb ( handwritten chinese character recognition ) datasets .", "topics": ["time complexity", "computation"]}
{"title": "nonapproximability results for partially observable markov decision processes", "abstract": "we show that for several variations of partially observable markov decision processes , polynomial-time algorithms for finding control policies are unlikely to or simply do n't have guarantees of finding policies within a constant factor or a constant summand of optimal . here `` unlikely '' means `` unless some complexity classes collapse , '' where the collapses considered are p=np , p=pspace , or p=exp . until or unless these collapses are shown to hold , any control-policy designer must choose between such performance guarantees and efficient computation .", "topics": ["time complexity", "computation"]}
{"title": "finding exogenous variables in data with many more variables than observations", "abstract": "many statistical methods have been proposed to estimate causal models in classical situations with fewer variables than observations ( p < n , p : the number of variables and n : the number of observations ) . however , modern datasets including gene expression data need high-dimensional causal modeling in challenging situations with orders of magnitude more variables than observations ( p > > n ) . in this paper , we propose a method to find exogenous variables in a linear non-gaussian causal model , which requires much smaller sample sizes than conventional methods and works even when p > > n . the key idea is to identify which variables are exogenous based on non-gaussianity instead of estimating the entire structure of the model . exogenous variables work as triggers that activate a causal chain in the model , and their identification leads to more efficient experimental designs and better understanding of the causal mechanism . we present experiments with artificial data and real-world gene expression data to evaluate the method .", "topics": ["causality"]}
{"title": "invertible autoencoder for domain adaptation", "abstract": "the unsupervised image-to-image translation aims at finding a mapping between the source ( $ a $ ) and target ( $ b $ ) image domains , where in many applications aligned image pairs are not available at training . this is an ill-posed learning problem since it requires inferring the joint probability distribution from marginals . joint learning of coupled mappings $ f_ { ab } : a \\rightarrow b $ and $ f_ { ba } : b \\rightarrow a $ is commonly used by the state-of-the-art methods , like cyclegan [ zhu et al . , 2017 ] , to learn this translation by introducing cycle consistency requirement to the learning problem , i.e . $ f_ { ab } ( f_ { ba } ( b ) ) \\approx b $ and $ f_ { ba } ( f_ { ab } ( a ) ) \\approx a $ . cycle consistency enforces the preservation of the mutual information between input and translated images . however , it does not explicitly enforce $ f_ { ba } $ to be an inverse operation to $ f_ { ab } $ . we propose a new deep architecture that we call invertible autoencoder ( invauto ) to explicitly enforce this relation . this is done by forcing an encoder to be an inverted version of the decoder , where corresponding layers perform opposite mappings and share parameters . the mappings are constrained to be orthonormal . the resulting architecture leads to the reduction of the number of trainable parameters ( up to $ 2 $ times ) . we present image translation results on benchmark data sets and demonstrate state-of-the art performance of our approach . finally , we test the proposed domain adaptation method on the task of road video conversion . we demonstrate that the videos converted with invauto have high quality and show that the nvidia neural-network-based end-to-end learning system for autonomous driving , known as pilotnet , trained on real road videos performs well when tested on the converted ones .", "topics": ["unsupervised learning", "encoder"]}
{"title": "unsupervised learning of geometry with edge-aware depth-normal consistency", "abstract": "learning to reconstruct depths in a single image by watching unlabeled videos via deep convolutional network ( dcn ) is attracting significant attention in recent years . in this paper , we introduce a surface normal representation for unsupervised depth estimation framework . our estimated depths are constrained to be compatible with predicted normals , yielding more robust geometry results . specifically , we formulate an edge-aware depth-normal consistency term , and solve it by constructing a depth-to-normal layer and a normal-to-depth layer inside of the dcn . the depth-to-normal layer takes estimated depths as input , and computes normal directions using cross production based on neighboring pixels . then given the estimated normals , the normal-to-depth layer outputs a regularized depth map through local planar smoothness . both layers are computed with awareness of edges inside the image to help address the issue of depth/normal discontinuity and preserve sharp edges . finally , to train the network , we apply the photometric error and gradient smoothness for both depth and normal predictions . we conducted experiments on both outdoor ( kitti ) and indoor ( nyuv2 ) datasets , and show that our algorithm vastly outperforms state of the art , which demonstrates the benefits from our approach .", "topics": ["unsupervised learning", "gradient descent"]}
{"title": "real time object tracking based on inter-frame coding : a review", "abstract": "inter-frame coding plays significant role for video compression and computer vision . computer vision systems have been incorporated in many real life applications ( e.g . surveillance systems , medical imaging , robot navigation and identity verification systems ) . object tracking is a key computer vision topic , which aims at detecting the position of a moving object from a video sequence . the application of inter-frame coding for low frame rate video , as well as for low resolution video . various methods based on top-down approach just like kernel based or mean shift technique are used to track the object for video , so , inter-frame coding algorithms are widely adopted by video coding standards , mainly due to their simplicity and good distortion performance for object tracking .", "topics": ["computer vision"]}
{"title": "learning convolutional text representations for visual question answering", "abstract": "visual question answering is a recently proposed artificial intelligence task that requires a deep understanding of both images and texts . in deep learning , images are typically modeled through convolutional neural networks , and texts are typically modeled through recurrent neural networks . while the requirement for modeling images is similar to traditional computer vision tasks , such as object recognition and image classification , visual question answering raises a different need for textual representation as compared to other natural language processing tasks . in this work , we perform a detailed analysis on natural language questions in visual question answering . based on the analysis , we propose to rely on convolutional neural networks for learning textual representations . by exploring the various properties of convolutional neural networks specialized for text data , such as width and depth , we present our `` cnn inception + gate '' model . we show that our model improves question representations and thus the overall accuracy of visual question answering models . we also show that the text representation requirement in visual question answering is more complicated and comprehensive than that in conventional natural language processing tasks , making it a better task to evaluate textual representation methods . shallow models like fasttext , which can obtain comparable results with deep learning models in tasks like text classification , are not suitable in visual question answering .", "topics": ["natural language processing", "recurrent neural network"]}
{"title": "optimal ridge detection using coverage risk", "abstract": "we introduce the concept of coverage risk as an error measure for density ridge estimation . the coverage risk generalizes the mean integrated square error to set estimation . we propose two risk estimators for the coverage risk and we show that we can select tuning parameters by minimizing the estimated risk . we study the rate of convergence for coverage risk and prove consistency of the risk estimators . we apply our method to three simulated datasets and to cosmology data . in all the examples , the proposed method successfully recover the underlying density structure .", "topics": ["simulation"]}
{"title": "deep learning of constrained autoencoders for enhanced understanding of data", "abstract": "unsupervised feature extractors are known to perform an efficient and discriminative representation of data . insight into the mappings they perform and human ability to understand them , however , remain very limited . this is especially prominent when multilayer deep learning architectures are used . this paper demonstrates how to remove these bottlenecks within the architecture of nonnegativity constrained autoencoder ( ncsae ) . it is shown that by using both l1 and l2 regularization that induce nonnegativity of weights , most of the weights in the network become constrained to be nonnegative thereby resulting into a more understandable structure with minute deterioration in classification accuracy . also , this proposed approach extracts features that are more sparse and produces additional output layer sparsification . the method is analyzed for accuracy and feature interpretation on the mnist data , the norb normalized uniform object data , and the reuters text categorization dataset .", "topics": ["sparse matrix", "autoencoder"]}
{"title": "facenet2expnet : regularizing a deep face recognition net for expression recognition", "abstract": "relatively small data sets available for expression recognition research make the training of deep networks for expression recognition very challenging . although fine-tuning can partially alleviate the issue , the performance is still below acceptable levels as the deep features probably contain redun- dant information from the pre-trained domain . in this paper , we present facenet2expnet , a novel idea to train an expression recognition network based on static images . we first propose a new distribution function to model the high-level neurons of the expression network . based on this , a two-stage training algorithm is carefully designed . in the pre-training stage , we train the convolutional layers of the expression net , regularized by the face net ; in the refining stage , we append fully- connected layers to the pre-trained convolutional layers and train the whole network jointly . visualization shows that the model trained with our method captures improved high-level expression semantics . evaluations on four public expression databases , ck+ , oulu-casia , tfd , and sfew demonstrate that our method achieves better results than state-of-the-art .", "topics": ["high- and low-level", "database"]}
{"title": "lifelong learning for sentiment classification", "abstract": "this paper proposes a novel lifelong learning ( ll ) approach to sentiment classification . ll mimics the human continuous learning process , i.e . , retaining the knowledge learned from past tasks and use it to help future learning . in this paper , we first discuss ll in general and then ll for sentiment classification in particular . the proposed ll approach adopts a bayesian optimization framework based on stochastic gradient descent . our experimental results show that the proposed method outperforms baseline methods significantly , which demonstrates that lifelong learning is a promising research direction .", "topics": ["baseline ( configuration management )", "gradient descent"]}
{"title": "exploiting web images for weakly supervised object detection", "abstract": "in recent years , the performance of object detection has advanced significantly with the evolving deep convolutional neural networks . however , the state-of-the-art object detection methods still rely on accurate bounding box annotations that require extensive human labelling . object detection without bounding box annotations , i.e , weakly supervised detection methods , are still lagging far behind . as weakly supervised detection only uses image level labels and does not require the ground truth of bounding box location and label of each object in an image , it is generally very difficult to distill knowledge of the actual appearances of objects . inspired by curriculum learning , this paper proposes an easy-to-hard knowledge transfer scheme that incorporates easy web images to provide prior knowledge of object appearance as a good starting point . while exploiting large-scale free web imagery , we introduce a sophisticated labour free method to construct a web dataset with good diversity in object appearance . after that , semantic relevance and distribution relevance are introduced and utilized in the proposed curriculum training scheme . our end-to-end learning with the constructed web data achieves remarkable improvement across most object classes especially for the classes that are often considered hard in other works .", "topics": ["object detection", "end-to-end principle"]}
{"title": "on mixed memberships and symmetric nonnegative matrix factorizations", "abstract": "the problem of finding overlapping communities in networks has gained much attention recently . optimization-based approaches use non-negative matrix factorization ( nmf ) or variants , but the global optimum can not be provably attained in general . model-based approaches , such as the popular mixed-membership stochastic blockmodel or mmsb ( airoldi et al . , 2008 ) , use parameters for each node to specify the overlapping communities , but standard inference techniques can not guarantee consistency . we link the two approaches , by ( a ) establishing sufficient conditions for the symmetric nmf optimization to have a unique solution under mmsb , and ( b ) proposing a computationally efficient algorithm called geonmf that is provably optimal and hence consistent for a broad parameter regime . we demonstrate its accuracy on both simulated and real-world datasets .", "topics": ["computational complexity theory", "simulation"]}
{"title": "winning on the merits : the joint effects of content and style on debate outcomes", "abstract": "debate and deliberation play essential roles in politics and government , but most models presume that debates are won mainly via superior style or agenda control . ideally , however , debates would be won on the merits , as a function of which side has the stronger arguments . we propose a predictive model of debate that estimates the effects of linguistic features and the latent persuasive strengths of different topics , as well as the interactions between the two . using a dataset of 118 oxford-style debates , our model 's combination of content ( as latent topics ) and style ( as linguistic features ) allows us to predict audience-adjudicated winners with 74 % accuracy , significantly outperforming linguistic features alone ( 66 % ) . our model finds that winning sides employ stronger arguments , and allows us to identify the linguistic features associated with strong or weak arguments .", "topics": ["interaction"]}
{"title": "recurrent neural networks as weighted language recognizers", "abstract": "we investigate the computational complexity of various problems for simple recurrent neural networks ( rnns ) as formal models for recognizing weighted languages . we focus on the single-layer , relu-activation , rational-weight rnns with softmax , which are commonly used in natural language processing applications . we show that most problems for such rnns are undecidable , including consistency , equivalence , minimization , and the determination of the highest-weighted string . however , for consistent rnns the last problem becomes decidable , although the solution length can surpass all computable bounds . if additionally the string is limited to polynomial length , the problem becomes np-complete and apx-hard . in summary , this shows that approximations and heuristic algorithms are necessary in practical applications of those rnns .", "topics": ["computational complexity theory", "natural language processing"]}
{"title": "comparison of machine learning methods for classifying mediastinal lymph node metastasis of non-small cell lung cancer from 18f-fdg pet/ct images", "abstract": "the present study shows that the performance of cnn is not significantly different from the best classical methods and human doctors for classifying mediastinal lymph node metastasis of nsclc from pet/ct images . because cnn does not need tumor segmentation or feature calculation , it is more convenient and more objective than the classical methods . however , cnn does not make use of the import diagnostic features , which have been proved more discriminative than the texture features for classifying small-sized lymph nodes . therefore , incorporating the diagnostic features into cnn is a promising direction for future research .", "topics": ["support vector machine", "support vector machine"]}
{"title": "proceedings of the workshop on data mining for oil and gas", "abstract": "the process of exploring and exploiting oil and gas ( o & g ) generates a lot of data that can bring more efficiency to the industry . the opportunities for using data mining techniques in the `` digital oil-field '' remain largely unexplored or uncharted . with the high rate of data expansion , companies are scrambling to develop ways to develop near-real-time predictive analytics , data mining and machine learning capabilities , and are expanding their data storage infrastructure and resources . with these new goals , come the challenges of managing data growth , integrating intelligence tools , and analyzing the data to glean useful insights . oil and gas companies need data solutions to economically extract value from very large volumes of a wide variety of data generated from exploration , well drilling and production devices and sensors . data mining for oil and gas industry throughout the lifecycle of the reservoir includes the following roles : locating hydrocarbons , managing geological data , drilling and formation evaluation , well construction , well completion , and optimizing production through the life of the oil field . for each of these phases during the lifecycle of oil field , data mining play a significant role . based on which phase were talking about , knowledge creation through scientific models , data analytics and machine learning , a effective , productive , and on demand data insight is critical for decision making within the organization . the significant challenges posed by this complex and economically vital field justify a meeting of data scientists that are willing to share their experience and knowledge . thus , the worskhop on data mining for oil and gas ( dm4og ) aims to provide a quality forum for researchers that work on the significant challenges arising from the synergy between data science , machine learning , and the modeling and optimization problems in the o & g industry .", "topics": ["data mining", "sensor"]}
{"title": "difficulty adjustable and scalable constrained multi-objective test problem toolkit", "abstract": "multi-objective evolutionary algorithms ( moeas ) have achieved great progress in recent decades , but most of them are designed to solve unconstrained multi-objective optimization problems . in fact , many real-world multi-objective problems usually contain a number of constraints . to promote the research of constrained multi-objective optimization , we first propose three primary types of difficulty , which reflect the challenges in the real-world optimization problems , to characterize the constraint functions in cmops , including feasibility-hardness , convergence-hardness and diversity-hardness . we then develop a general toolkit to construct difficulty adjustable and scalable constrained multi-objective optimization problems ( cmops ) with three types of parameterized constraint functions according to the proposed three primary types of difficulty . in fact , combination of the three primary constraint functions with different parameters can lead to construct a large variety of cmops , whose difficulty can be uniquely defined by a triplet with each of its parameter specifying the level of each primary difficulty type respectively . furthermore , the number of objectives in this toolkit are able to scale to more than two . based on this toolkit , we suggest nine difficulty adjustable and scalable cmops named das-cmop1-9 . to evaluate the proposed test problems , two popular cmoeas - moea/d-cdp and nsga-ii-cdp are adopted to test their performances on das-cmop1-9 with different difficulty triplets . the experiment results demonstrate that none of them can solve these problems efficiently , which stimulate us to develop new constrained moeas to solve the suggested das-cmops .", "topics": ["optimization problem", "scalability"]}
{"title": "implementation and comparative quantitative assessment of different multispectral image pansharpening approches", "abstract": "in remote sensing , images acquired by various earth observation satellites tend to have either a high spatial and low spectral resolution or vice versa . pansharpening is a technique which aims to improve spatial resolution of multispectral image . the challenges involve in the pansharpening are not only to improve the spatial resolution but also to preserve spectral quality of the multispectral image . in this paper , various pansharpening algorithms are discussed and classified based on approaches they have adopted . using matlab image processing toolbox , several state-of-art pan-sharpening algorithms are implemented . quality of pansharpened images are assessed visually and quantitatively . correlation coefficient ( cc ) , root mean square error ( rmse ) , relative average spectral error ( rase ) and universal quality index ( q ) indices are used to easure spectral quality while to spatial-cc ( scc ) quantitative parameter is used for spatial quality measurement . finally , the paper is concluded with useful remarks .", "topics": ["image processing", "coefficient"]}
{"title": "rank of experts : detection network ensemble", "abstract": "the recent advances of convolutional detectors show impressive performance improvement for large scale object detection . however , in general , the detection performance usually decreases as the object classes to be detected increases , and it is a practically challenging problem to train a dominant model for all classes due to the limitations of detection models and datasets . in most cases , therefore , there are distinct performance differences of the modern convolutional detectors for each object class detection . in this paper , in order to build an ensemble detector for large scale object detection , we present a conceptually simple but very effective class-wise ensemble detection which is named as rank of experts . we first decompose an intractable problem of finding the best detections for all object classes into small subproblems of finding the best ones for each object class . we then solve the detection problem by ranking detectors in order of the average precision rate for each class , and then aggregate the responses of the top ranked detectors ( i.e . experts ) for class-wise ensemble detection . the main benefit of our method is easy to implement and does not require any joint training of experts for ensemble . based on the proposed rank of experts , we won the 2nd place in the ilsvrc 2017 object detection competition .", "topics": ["object detection", "computational complexity theory"]}
{"title": "revisiting knowledge transfer for training object class detectors", "abstract": "we propose to revisit knowledge transfer for training object detectors on target classes from weakly supervised training images , helped by a set of source classes with bounding-box annotations . we present a unified knowledge transfer framework based on training a single neural network multi-class object detector over all source classes , organized in a semantic hierarchy . this generates proposals with scores at multiple levels in the hierarchy , which we use to explore knowledge transfer over a broad range of generality , ranging from class-specific ( bicycle to motorbike ) to class-generic ( objectness to any class ) . experiments on the 200 object classes in the ilsvrc 2013 detection dataset show that our technique ( 1 ) leads to much better performance on the target classes ( 70.3 % corloc , 36.9 % map ) than a weakly supervised baseline which uses manually engineered objectness [ 10 ] ( 50.5 % corloc , 25.4 % map ) . ( 2 ) delivers target object detectors reaching 80 % of the map of their fully supervised counterparts . ( 3 ) outperforms the best reported transfer learning results [ 17 , 42 ] on this dataset ( +41 % corloc , +3 % map ) . moreover , we also carry out several across-dataset knowledge transfer experiments [ 25 , 22 , 32 ] and find that ( 4 ) our technique outperforms the weakly supervised baseline in all dataset pairs by 1.5x - 1.9x , establishing its general applicability .", "topics": ["baseline ( configuration management )"]}
{"title": "effective warm start for the online actor-critic reinforcement learning based mhealth intervention", "abstract": "online reinforcement learning ( rl ) is increasingly popular for the personalized mobile health ( mhealth ) intervention . it is able to personalize the type and dose of interventions according to user 's ongoing statuses and changing needs . however , at the beginning of online learning , there are usually too few samples to support the rl updating , which leads to poor performances . a delay in good performance of the online learning algorithms can be especially detrimental in the mhealth , where users tend to quickly disengage with the mhealth app . to address this problem , we propose a new online rl methodology that focuses on an effective warm start . the main idea is to make full use of the data accumulated and the decision rule achieved in a former study . as a result , we can greatly enrich the data size at the beginning of online learning in our method . such case accelerates the online learning process for new users to achieve good performances not only at the beginning of online learning but also through the whole online learning process . besides , we use the decision rules achieved in a previous study to initialize the parameter in our online rl model for new users . it provides a good initialization for the proposed online rl algorithm . experiment results show that promising improvements have been achieved by our method compared with the state-of-the-art method .", "topics": ["reinforcement learning"]}
{"title": "lexical disambiguation in natural language questions ( nlqs )", "abstract": "question processing is a fundamental step in a question answering ( qa ) application , and its quality impacts the performance of qa application . the major challenging issue in processing question is how to extract semantic of natural language questions ( nlqs ) . a human language is ambiguous . ambiguity may occur at two levels ; lexical and syntactic . in this paper , we propose a new approach for resolving lexical ambiguity problem by integrating context knowledge and concepts knowledge of a domain , into shallow natural language processing ( snlp ) techniques . concepts knowledge is modeled using ontology , while context knowledge is obtained from wordnet , and it is determined based on neighborhood words in a question . the approach will be applied to a university qa system .", "topics": ["natural language processing", "natural language"]}
{"title": "cluster-based specification techniques in dempster-shafer theory", "abstract": "when reasoning with uncertainty there are many situations where evidences are not only uncertain but their propositions may also be weakly specified in the sense that it may not be certain to which event a proposition is referring . it is then crucial not to combine such evidences in the mistaken belief that they are referring to the same event . this situation would become manageable if the evidences could be clustered into subsets representing events that should be handled separately . in an earlier article we established within dempster-shafer theory a criterion function called the metaconflict function . with this criterion we can partition a set of evidences into subsets . each subset representing a separate event . in this article we will not only find the most plausible subset for each piece of evidence , we will also find the plausibility for every subset that the evidence belongs to the subset . also , when the number of subsets are uncertain we aim to find a posterior probability distribution regarding the number of subsets .", "topics": ["loss function", "eisenstein 's criterion"]}
{"title": "a linear-time kernel goodness-of-fit test", "abstract": "we propose a novel adaptive test of goodness-of-fit , with computational cost linear in the number of samples . we learn the test features that best indicate the differences between observed samples and a reference model , by minimizing the false negative rate . these features are constructed via stein 's method , meaning that it is not necessary to compute the normalising constant of the model . we analyse the asymptotic bahadur efficiency of the new test , and prove that under a mean-shift alternative , our test always has greater relative efficiency than a previous linear-time kernel test , regardless of the choice of parameters for that test . in experiments , the performance of our method exceeds that of the earlier linear-time test , and matches or exceeds the power of a quadratic-time kernel test . in high dimensions and where model structure may be exploited , our goodness of fit test performs far better than a quadratic-time two-sample test based on the maximum mean discrepancy , with samples drawn from the model .", "topics": ["kernel ( operating system )", "time complexity"]}
{"title": "towards design and implementation of a language technology based information processor for pdm systems", "abstract": "product data management ( pdm ) aims to provide 'systems ' contributing in industries by electronically maintaining organizational data , improving data repository system , facilitating with easy access to cad and providing additional information engineering and management modules to access , store , integrate , secure , recover and manage information . targeting one of the unresolved issues i.e . , provision of natural language based processor for the implementation of an intelligent record search mechanism , an approach is proposed and discussed in detail in this manuscript . designing an intelligent application capable of reading and analyzing user 's structured and unstructured natural language based text requests and then extracting desired concrete and optimized results from knowledge base is still a challenging task for the designers because it is still very difficult to completely extract meta data out of raw data . residing within the limited scope of current research and development ; we present an approach capable of reading user 's natural language based input text , understanding the semantic and extracting results from repositories . to evaluate the effectiveness of implemented prototyped version of proposed approach , it is compared with some existing pdm systems , in the end the discussion is concluded with an abstract presentation of resultant comparison amongst implemented prototype and some existing pdm systems .", "topics": ["natural language"]}
{"title": "feature learning from spectrograms for assessment of personality traits", "abstract": "several methods have recently been proposed to analyze speech and automatically infer the personality of the speaker . these methods often rely on prosodic and other hand crafted speech processing features extracted with off-the-shelf toolboxes . to achieve high accuracy , numerous features are typically extracted using complex and highly parameterized algorithms . in this paper , a new method based on feature learning and spectrogram analysis is proposed to simplify the feature extraction process while maintaining a high level of accuracy . the proposed method learns a dictionary of discriminant features from patches extracted in the spectrogram representations of training speech segments . each speech segment is then encoded using the dictionary , and the resulting feature set is used to perform classification of personality traits . experiments indicate that the proposed method achieves state-of-the-art results with a significant reduction in complexity when compared to the most recent reference methods . the number of features , and difficulties linked to the feature extraction process are greatly reduced as only one type of descriptors is used , for which the 6 parameters can be tuned automatically . in contrast , the simplest reference method uses 4 types of descriptors to which 6 functionals are applied , resulting in over 20 parameters to be tuned .", "topics": ["feature learning", "feature extraction"]}
{"title": "learning topic models by neighborhood aggregation", "abstract": "topic models are one of the most frequently used models in machine learning due to its high interpretability and modular structure . however extending the model to include supervisory signal , incorporate pre-trained word embedding vectors and add nonlinear output function to the model is not an easy task because one has to resort to highly intricate approximate inference procedure . in this paper , we show that topic models could be viewed as performing a neighborhood aggregation algorithm where the messages are passed through a network defined over words . under the network view of topic models , nodes corresponds to words in a document and edges correspond to either a relationship describing co-occurring words in a document or a relationship describing same word in the corpus . the network view allows us to extend the model to include supervisory signals , incorporate pre-trained word embedding vectors and add nonlinear output function to the model in a simple manner . moreover , we describe a simple way to train the model that is well suited in a semi-supervised setting where we only have supervisory signals for some portion of the corpus and the goal is to improve prediction performance in the held-out data . through careful experiments we show that our approach outperforms state-of-the-art supervised latent dirichlet allocation implementation in both held-out document classification tasks and topic coherence .", "topics": ["approximation algorithm", "nonlinear system"]}
{"title": "multi-level feature descriptor for robust texture classification via locality-constrained collaborative strategy", "abstract": "this paper introduces a simple but highly efficient ensemble for robust texture classification , which can effectively deal with translation , scale and changes of significant viewpoint problems . the proposed method first inherits the spirit of spatial pyramid matching model ( spm ) , which is popular for encoding spatial distribution of local features , but in a flexible way , partitioning the original image into different levels and incorporating different overlapping patterns of each level . this flexible setup helps capture the informative features and produces sufficient local feature codes by some well-chosen aggregation statistics or pooling operations within each partitioned region , even when only a few sample images are available for training . then each texture image is represented by several orderless feature codes and thereby all the training data form a reliable feature pond . finally , to take full advantage of this feature pond , we develop a collaborative representation-based strategy with locality constraint ( lc-crc ) for the final classification , and experimental results on three well-known public texture datasets demonstrate the proposed approach is very competitive and even outperforms several state-of-the-art methods . particularly , when only a few samples of each category are available for training , our approach still achieves very high classification performance .", "topics": ["test set"]}
{"title": "dataiku 's solution to sphere 's activity recognition challenge", "abstract": "our team won the second prize of the safe aging with sphere challenge organized by sphere , in conjunction with ecml-pkdd and driven data . the goal of the competition was to recognize activities performed by humans , using sensor data . this paper presents our solution . it is based on a rich pre-processing and state of the art machine learning methods . from the raw train data , we generate a synthetic train set with the same statistical characteristics as the test set . we then perform feature engineering . the machine learning modeling part is based on stacking weak learners through a grid searched xgboost algorithm . finally , we use post-processing to smooth our predictions over time .", "topics": ["test set", "synthetic data"]}
{"title": "guaranteed clustering and biclustering via semidefinite programming", "abstract": "identifying clusters of similar objects in data plays a significant role in a wide range of applications . as a model problem for clustering , we consider the densest k-disjoint-clique problem , whose goal is to identify the collection of k disjoint cliques of a given weighted complete graph maximizing the sum of the densities of the complete subgraphs induced by these cliques . in this paper , we establish conditions ensuring exact recovery of the densest k cliques of a given graph from the optimal solution of a particular semidefinite program . in particular , the semidefinite relaxation is exact for input graphs corresponding to data consisting of k large , distinct clusters and a smaller number of outliers . this approach also yields a semidefinite relaxation for the biclustering problem with similar recovery guarantees . given a set of objects and a set of features exhibited by these objects , biclustering seeks to simultaneously group the objects and features according to their expression levels . this problem may be posed as partitioning the nodes of a weighted bipartite complete graph such that the sum of the densities of the resulting bipartite complete subgraphs is maximized . as in our analysis of the densest k-disjoint-clique problem , we show that the correct partition of the objects and features can be recovered from the optimal solution of a semidefinite program in the case that the given data consists of several disjoint sets of objects exhibiting similar features . empirical evidence from numerical experiments supporting these theoretical guarantees is also provided .", "topics": ["cluster analysis", "optimization problem"]}
{"title": "a classification refinement strategy for semantic segmentation", "abstract": "based on the observation that semantic segmentation errors are partially predictable , we propose a compact formulation using confusion statistics of the trained classifier to refine ( re-estimate ) the initial pixel label hypotheses . the proposed strategy is contingent upon computing the classifier confusion probabilities for a given dataset and estimating a relevant prior on the object classes present in the image to be classified . we provide a procedure to robustly estimate the confusion probabilities and explore multiple prior definitions . experiments are shown comparing performances on multiple challenging datasets using different priors to improve a state-of-the-art semantic segmentation classifier . this study demonstrates the potential to significantly improve semantic labeling and motivates future work for reliable label prior estimation from images .", "topics": ["pixel"]}
{"title": "learning quadratic variance function ( qvf ) dag models via overdispersion scoring ( ods )", "abstract": "learning dag or bayesian network models is an important problem in multi-variate causal inference . however , a number of challenges arises in learning large-scale dag models including model identifiability and computational complexity since the space of directed graphs is huge . in this paper , we address these issues in a number of steps for a broad class of dag models where the noise or variance is signal-dependent . firstly we introduce a new class of identifiable dag models , where each node has a distribution where the variance is a quadratic function of the mean ( qvf dag models ) . our qvf dag models include many interesting classes of distributions such as poisson , binomial , geometric , exponential , gamma and many other distributions in which the noise variance depends on the mean . we prove that this class of qvf dag models is identifiable , and introduce a new algorithm , the overdispersion scoring ( ods ) algorithm , for learning large-scale qvf dag models . our algorithm is based on firstly learning the moralized or undirected graphical model representation of the dag to reduce the dag search-space , and then exploiting the quadratic variance property to learn the causal ordering . we show through theoretical results and simulations that our algorithm is statistically consistent in the high-dimensional p > n setting provided that the degree of the moralized graph is bounded and performs well compared to state-of-the-art dag-learning algorithms .", "topics": ["graphical model", "computational complexity theory"]}
{"title": "belief propagation in conditional rbms for structured prediction", "abstract": "restricted boltzmann machines~ ( rbms ) and conditional rbms~ ( crbms ) are popular models for a wide range of applications . in previous work , learning on such models has been dominated by contrastive divergence~ ( cd ) and its variants . belief propagation~ ( bp ) algorithms are believed to be slow for structured prediction on conditional rbms~ ( e.g . , mnih et al . [ 2011 ] ) , and not as good as cd when applied in learning~ ( e.g . , larochelle et al . [ 2012 ] ) . in this work , we present a matrix-based implementation of belief propagation algorithms on crbms , which is easily scalable to tens of thousands of visible and hidden units . we demonstrate that , in both maximum likelihood and max-margin learning , training conditional rbms with bp as the inference routine can provide significantly better results than current state-of-the-art cd methods on structured prediction problems . we also include practical guidelines on training crbms with bp , and some insights on the interaction of learning and inference algorithms for crbms .", "topics": ["scalability"]}
{"title": "selective greedy equivalence search : finding optimal bayesian networks using a polynomial number of score evaluations", "abstract": "we introduce selective greedy equivalence search ( sges ) , a restricted version of greedy equivalence search ( ges ) . sges retains the asymptotic correctness of ges but , unlike ges , has polynomial performance guarantees . in particular , we show that when data are sampled independently from a distribution that is perfect with respect to a dag $ { \\cal g } $ defined over the observable variables then , in the limit of large data , sges will identify $ { \\cal g } $ 's equivalence class after a number of score evaluations that is ( 1 ) polynomial in the number of nodes and ( 2 ) exponential in various complexity measures including maximum-number-of-parents , maximum-clique-size , and a new measure called { \\em v-width } that is at least as small as -- -and potentially much smaller than -- -the other two . more generally , we show that for any hereditary and equivalence-invariant property $ \\pi $ known to hold in $ { \\cal g } $ , we retain the large-sample optimality guarantees of ges even if we ignore any ges deletion operator during the backward phase that results in a state for which $ \\pi $ does not hold in the common-descendants subgraph .", "topics": ["time complexity", "bayesian network"]}
{"title": "hamiltonian annealed importance sampling for partition function estimation", "abstract": "we introduce an extension to annealed importance sampling that uses hamiltonian dynamics to rapidly estimate normalization constants . we demonstrate this method by computing log likelihoods in directed and undirected probabilistic image models . we compare the performance of linear generative models with both gaussian and laplace priors , product of experts models with laplace and student 's t experts , the mc-rbm , and a bilinear generative model . we provide code to compare additional models .", "topics": ["sampling ( signal processing )", "generative model"]}
{"title": "tensor-based backpropagation in neural networks with non-sequential input", "abstract": "neural networks have been able to achieve groundbreaking accuracy at tasks conventionally considered only doable by humans . using stochastic gradient descent , optimization in many dimensions is made possible , albeit at a relatively high computational cost . by splitting training data into batches , networks can be distributed and trained vastly more efficiently and with minimal accuracy loss . we have explored the mathematics behind efficiently implementing tensor-based batch backpropagation algorithms . a common approach to batch training is iterating over batch items individually . explicitly using tensor operations to backpropagate allows training to be performed non-linearly , increasing computational efficiency .", "topics": ["test set", "neural networks"]}
{"title": "a new algorithm based entropic threshold for edge detection in images", "abstract": "edge detection is one of the most critical tasks in automatic image analysis . there exists no universal edge detection method which works well under all conditions . this paper shows the new approach based on the one of the most efficient techniques for edge detection , which is entropy-based thresholding . the main advantages of the proposed method are its robustness and its flexibility . we present experimental results for this method , and compare results of the algorithm against several leading edge detection methods , such as canny , log , and sobel . experimental results demonstrate that the proposed method achieves better result than some classic methods and the quality of the edge detector of the output images is robust and decrease the computation time .", "topics": ["time complexity", "computation"]}
{"title": "bayesian clustering of shapes of curves", "abstract": "unsupervised clustering of curves according to their shapes is an important problem with broad scientific applications . the existing model-based clustering techniques either rely on simple probability models ( e.g . , gaussian ) that are not generally valid for shape analysis or assume the number of clusters . we develop an efficient bayesian method to cluster curve data using an elastic shape metric that is based on joint registration and comparison of shapes of curves . the elastic-inner product matrix obtained from the data is modeled using a wishart distribution whose parameters are assigned carefully chosen prior distributions to allow for automatic inference on the number of clusters . posterior is sampled through an efficient markov chain monte carlo procedure based on the chinese restaurant process to infer ( 1 ) the posterior distribution on the number of clusters , and ( 2 ) clustering configuration of shapes . this method is demonstrated on a variety of synthetic data and real data examples on protein structure analysis , cell shape analysis in microscopy images , and clustering of shaped from mpeg7 database .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "arc-swift : a novel transition system for dependency parsing", "abstract": "transition-based dependency parsers often need sequences of local shift and reduce operations to produce certain attachments . correct individual decisions hence require global information about the sentence context and mistakes cause error propagation . this paper proposes a novel transition system , arc-swift , that enables direct attachments between tokens farther apart with a single transition . this allows the parser to leverage lexical information more directly in transition decisions . hence , arc-swift can achieve significantly better performance with a very small beam size . our parsers reduce error by 3.7 -- 7.6 % relative to those using existing transition systems on the penn treebank dependency parsing task and english universal dependencies .", "topics": ["parsing"]}
{"title": "probabilistic backpropagation for scalable learning of bayesian neural networks", "abstract": "large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems . however , using backprop for neural net learning still has some disadvantages , e.g . , having to tune a large number of hyperparameters to the data , lack of calibrated probabilistic predictions , and a tendency to overfit the training data . in principle , the bayesian approach to learning neural networks does not have these problems . however , existing bayesian techniques lack scalability to large dataset and network sizes . in this work we present a novel scalable method for learning bayesian neural networks , called probabilistic backpropagation ( pbp ) . similar to classical backpropagation , pbp works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients . a series of experiments on ten real-world datasets show that pbp is significantly faster than other techniques , while offering competitive predictive abilities . our experiments also show that pbp provides accurate estimates of the posterior variance on the network weights .", "topics": ["test set", "computation"]}
{"title": "metastatic liver tumour segmentation from discriminant grassmannian manifolds", "abstract": "the early detection , diagnosis and monitoring of liver cancer progression can be achieved with the precise delineation of metastatic tumours . however , accurate automated segmentation remains challenging due to the presence of noise , inhomogeneity and the high appearance variability of malignant tissue . in this paper , we propose an unsupervised metastatic liver tumour segmentation framework using a machine learning approach based on discriminant grassmannian manifolds which learns the appearance of tumours with respect to normal tissue . first , the framework learns within-class and between-class similarity distributions from a training set of images to discover the optimal manifold discrimination between normal and pathological tissue in the liver . second , a conditional optimisation scheme computes nonlocal pairwise as well as pattern-based clique potentials from the manifold subspace to recognise regions with similar labelings and to incorporate global consistency in the segmentation process . the proposed framework was validated on a clinical database of 43 ct images from patients with metastatic liver cancer . compared to state-of-the-art methods , our method achieves a better performance on two separate datasets of metastatic liver tumours from different clinical sites , yielding an overall mean dice similarity coefficient of 90.7 +/- 2.4 in over 50 tumours with an average volume of 27.3 mm3 .", "topics": ["unsupervised learning", "mathematical optimization"]}
{"title": "optimization beyond the convolution : generalizing spatial relations with end-to-end metric learning", "abstract": "to operate intelligently in domestic environments , robots require the ability to understand arbitrary spatial relations between objects and to generalize them to objects of varying sizes and shapes . in this work , we present a novel end-to-end approach to generalize spatial relations based on distance metric learning . we train a neural network to transform 3d point clouds of objects to a metric space that captures the similarity of the depicted spatial relations , using only geometric models of the objects . our approach employs gradient-based optimization to compute object poses in order to imitate an arbitrary target relation by reducing the distance to it under the learned metric . our results based on simulated and real-world experiments show that the proposed method enables robots to generalize spatial relations to unknown objects over a continuous spectrum .", "topics": ["mathematical optimization", "simulation"]}
{"title": "rao-blackwellised particle filtering for dynamic bayesian networks", "abstract": "particle filters ( pfs ) are powerful sampling-based inference/learning algorithms for dynamic bayesian networks ( dbns ) . they allow us to treat , in a principled way , any type of probability distribution , nonlinearity and non-stationarity . they have appeared in several fields under such names as `` condensation '' , `` sequential monte carlo '' and `` survival of the fittest '' . in this paper , we show how we can exploit the structure of the dbn to increase the efficiency of particle filtering , using a technique known as rao-blackwellisation . essentially , this samples some of the variables , and marginalizes out the rest exactly , using the kalman filter , hmm filter , junction tree algorithm , or any other finite dimensional optimal filter . we show that rao-blackwellised particle filters ( rbpfs ) lead to more accurate estimates than standard pfs . we demonstrate rbpfs on two problems , namely non-stationary online regression with radial basis function networks and robot localization and map building . we also discuss other potential application areas and provide references to some finite dimensional optimal filters .", "topics": ["nonlinear system", "bayesian network"]}
{"title": "contrast and visual saliency similarity induced index for image quality assessment", "abstract": "perceptual image quality assessment ( iqa ) defines/utilizes a computational model to assess the image quality in consistent with human opinions . a good iqa model should consider both the effectiveness and efficiency , while most previous iqa models are hard to reach simultaneously . so we attempt to make another effort to develop an effective and efficiency image quality assessment metric . considering that contrast is a distinctive visual attribute that indicates the quality of an image , and visual saliency ( vs ) attracts the most attention of the human visual system , the proposed model utilized these two features to characterize the image local quality . after obtaining the local contrast quality map and global visual saliency quality map , we add the weighted standard deviation of the previous two quality maps together to yield the final quality score . the experimental results on three benchmark database ( live , tid2008 , csiq ) showed that the proposed model yields the best performance in terms of the correlation with human judgments of visual quality . furthermore , it is more efficient when compared with other competing iqa models .", "topics": ["map"]}
{"title": "discrete mdl predicts in total variation", "abstract": "the minimum description length ( mdl ) principle selects the model that has the shortest code for data plus model . we show that for a countable class of models , mdl predictions are close to the true distribution in a strong sense . the result is completely general . no independence , ergodicity , stationarity , identifiability , or other assumption on the model class need to be made . more formally , we show that for any countable class of models , the distributions selected by mdl ( or map ) asymptotically predict ( merge with ) the true measure in the class in total variation distance . implications for non-i.i.d . domains like time-series forecasting , discriminative learning , and reinforcement learning are discussed .", "topics": ["time series", "reinforcement learning"]}
{"title": "beyond cca : moment matching for multi-view models", "abstract": "we introduce three novel semi-parametric extensions of probabilistic canonical correlation analysis with identifiability guarantees . we consider moment matching techniques for estimation in these models . for that , by drawing explicit links between the new models and a discrete version of independent component analysis ( dica ) , we first extend the dica cumulant tensors to the new discrete version of cca . by further using a close connection with independent component analysis , we introduce generalized covariance matrices , which can replace the cumulant tensors in the moment matching framework , and , therefore , improve sample complexity and simplify derivations and algorithms significantly . as the tensor power method or orthogonal joint diagonalization are not applicable in the new setting , we use non-orthogonal joint diagonalization techniques for matching the cumulants . we demonstrate performance of the proposed models and estimation techniques on experiments with both synthetic and real datasets .", "topics": ["synthetic data"]}
{"title": "3d-a-nets : 3d deep dense descriptor for volumetric shapes with adversarial networks", "abstract": "recently researchers have been shifting their focus towards learned 3d shape descriptors from hand-craft ones to better address challenging issues of the deformation and structural variation inherently present in 3d objects . 3d geometric data are often transformed to 3d voxel grids with regular format in order to be better fed to a deep neural net architecture . however , the computational intractability of direct application of 3d convolutional nets to 3d volumetric data severely limits the efficiency ( i.e . slow processing ) and effectiveness ( i.e . unsatisfied accuracy ) in processing 3d geometric data . in this paper , powered with a novel design of adversarial networks ( 3d-a-nets ) , we have developed a novel 3d deep dense shape descriptor ( 3d-ddsd ) to address the challenging issues of efficient and effective 3d volumetric data processing . we developed new definition of 2d multilayer dense representation ( mdr ) of 3d volumetric data to extract concise but geometrically informative shape description and a novel design of adversarial networks that jointly train a set of convolution neural network ( cnn ) , recurrent neural network ( rnn ) and an adversarial discriminator . more specifically , the generator network produces 3d shape features that encourages the clustering of samples from the same category with correct class label , whereas the discriminator network discourages the clustering by assigning them misleading adversarial class labels . by addressing the challenges posed by the computational inefficiency of direct application of cnn to 3d volumetric data , 3d-a-nets can learn high-quality 3d-dsdd which demonstrates superior performance on 3d shape classification and retrieval over other state-of-the-art techniques by a great margin .", "topics": ["recurrent neural network", "cluster analysis"]}
{"title": "malaria detection using image processing and machine learning", "abstract": "malaria is mosquito-borne blood disease caused by parasites of the genus plasmodium . conventional diagnostic tool for malaria is the examination of stained blood cell of patient in microscope . the blood to be tested is placed in a slide and is observed under a microscope to count the number of infected rbc . an expert technician is involved in the examination of the slide with intense visual and mental concentration . this is tiresome and time consuming process . in this paper , we construct a new mage processing system for detection and quantification of plasmodium parasites in blood smear slide , later we develop machine learning algorithm to learn , detect and determine the types of infected cells according to its features .", "topics": ["image processing", "support vector machine"]}
{"title": "semantic cross-view matching", "abstract": "matching cross-view images is challenging because the appearance and viewpoints are significantly different . while low-level features based on gradient orientations or filter responses can drastically vary with such changes in viewpoint , semantic information of images however shows an invariant characteristic in this respect . consequently , semantically labeled regions can be used for performing cross-view matching . in this paper , we therefore explore this idea and propose an automatic method for detecting and representing the semantic information of an rgb image with the goal of performing cross-view matching with a ( non-rgb ) geographic information system ( gis ) . a segmented image forms the input to our system with segments assigned to semantic concepts such as traffic signs , lakes , roads , foliage , etc . we design a descriptor to robustly capture both , the presence of semantic concepts and the spatial layout of those segments . pairwise distances between the descriptors extracted from the gis map and the query image are then used to generate a shortlist of the most promising locations with similar semantic concepts in a consistent spatial layout . an experimental evaluation with challenging query images and a large urban area shows promising results .", "topics": ["high- and low-level", "gradient"]}
{"title": "using a distributional semantic vector space with a knowledge base for reasoning in uncertain conditions", "abstract": "the inherent inflexibility and incompleteness of commonsense knowledge bases ( kb ) has limited their usefulness . we describe a system called displacer for performing kb queries extended with the analogical capabilities of the word2vec distributional semantic vector space ( dsvs ) . this allows the system to answer queries with information which was not contained in the original kb in any form . by performing analogous queries on semantically related terms and mapping their answers back into the context of the original query using displacement vectors , we are able to give approximate answers to many questions which , if posed to the kb alone , would return no results . we also show how the hand-curated knowledge in a kb can be used to increase the accuracy of a dsvs in solving analogy problems . in these ways , a kb and a dsvs can make up for each other 's weaknesses .", "topics": ["approximation algorithm"]}
{"title": "a compact architecture for dialogue management based on scripts and meta-outputs", "abstract": "we describe an architecture for spoken dialogue interfaces to semi-autonomous systems that transforms speech signals through successive representations of linguistic , dialogue , and domain knowledge . each step produces an output , and a meta-output describing the transformation , with an executable program in a simple scripting language as the final result . the output/meta-output distinction permits perspicuous treatment of diverse tasks such as resolving pronouns , correcting user misconceptions , and optimizing scripts .", "topics": ["natural language processing", "natural language"]}
{"title": "colour terms : a categorisation model inspired by visual cortex neurons", "abstract": "although it seems counter-intuitive , categorical colours do not exist as external physical entities but are very much the product of our brains . our cortical machinery segments the world and associate objects to specific colour terms , which is not only convenient for communication but also increases the efficiency of visual processing by reducing the dimensionality of input scenes . although the neural substrate for this phenomenon is unknown , a recent study of cortical colour processing has discovered a set of neurons that are isoresponsive to stimuli in the shape of 3d-ellipsoidal surfaces in colour-opponent space . we hypothesise that these neurons might help explain the underlying mechanisms of colour naming in the visual cortex . following this , we propose a biologically-inspired colour naming model where each colour term - e.g . red , green , blue , yellow , etc . - is represented through an ellipsoid in 3d colour-opponent space . this paradigm is also supported by previous psychophysical colour categorisation experiments whose results resemble such shapes . `` belongingness '' of each pixel to different colour categories is computed by a non-linear sigmoidal logistic function . the final colour term for a given pixel is calculated by a maximum pooling mechanism . the simplicity of our model allows its parameters to be learnt from a handful of segmented images . it also offers a straightforward extension to include further colour terms . additionally , ellipsoids of proposed model can adapt to image contents offering a dynamical solution in order to address phenomenon of colour constancy . our results on the munsell chart and two datasets of real-world images show an overall improvement comparing to state-of-the-art algorithms .", "topics": ["nonlinear system", "entity"]}
{"title": "limited attention and discourse structure", "abstract": "this squib examines the role of limited attention in a theory of discourse structure and proposes a model of attentional state that relates current hierarchical theories of discourse structure to empirical evidence about human discourse processing capabilities . first , i present examples that are not predicted by grosz and sidner 's stack model of attentional state . then i consider an alternative model of attentional state , the cache model , which accounts for the examples , and which makes particular processing predictions . finally i suggest a number of ways that future research could distinguish the predictions of the cache model and the stack model .", "topics": ["entity"]}
{"title": "compressive sensing via low-rank gaussian mixture models", "abstract": "we develop a new compressive sensing ( cs ) inversion algorithm by utilizing the gaussian mixture model ( gmm ) . while the compressive sensing is performed globally on the entire image as implemented in our lensless camera , a low-rank gmm is imposed on the local image patches . this low-rank gmm is derived via eigenvalue thresholding of the gmm trained on the projection of the measurement data , thus learned { \\em in situ } . the gmm and the projection of the measurement data are updated iteratively during the reconstruction . our gmm algorithm degrades to the piecewise linear estimator ( ple ) if each patch is represented by a single gaussian model . inspired by this , a low-rank ple algorithm is also developed for cs inversion , constituting an additional contribution of this paper . extensive results on both simulation data and real data captured by the lensless camera demonstrate the efficacy of the proposed algorithm . furthermore , we compare the cs reconstruction results using our algorithm with the jpeg compression . simulation results demonstrate that when limited bandwidth is available ( a small number of measurements ) , our algorithm can achieve comparable results as jpeg .", "topics": ["simulation"]}
{"title": "train , diagnose and fix : interpretable approach for fine-grained action recognition", "abstract": "despite the growing discriminative capabilities of modern deep learning methods for recognition tasks , the inner workings of the state-of-art models still remain mostly black-boxes . in this paper , we propose a systematic interpretation of model parameters and hidden representations of residual temporal convolutional networks ( res-tcn ) for action recognition in time-series data . we also propose a feature map decoder as part of the interpretation analysis , which outputs a representation of model 's hidden variables in the same domain as the input . such analysis empowers us to expose model 's characteristic learning patterns in an interpretable way . for example , through the diagnosis analysis , we discovered that our model has learned to achieve view-point invariance by implicitly learning to perform rotational normalization of the input to a more discriminative view . based on the findings from the model interpretation analysis , we propose a targeted refinement technique , which can generalize to various other recognition models . the proposed work introduces a three-stage paradigm for model learning : training , interpretable diagnosis and targeted refinement . we validate our approach on skeleton based 3d human action recognition benchmark of ntu rgb+d . we show that the proposed workflow is an effective model learning strategy and the resulting multi-stream residual temporal convolutional network ( ms-res-tcn ) achieves the state-of-the-art performance on ntu rgb+d .", "topics": ["time series"]}
{"title": "non-projective dependency parsing with non-local transitions", "abstract": "we present a novel transition system , based on the covington non-projective parser , introducing non-local transitions that can directly create arcs involving nodes to the left of the current focus positions . this avoids the need for long sequences of no-arc transitions to create long-distance arcs , thus alleviating error propagation . the resulting parser outperforms the original version and achieves the best accuracy on the stanford dependencies conversion of the penn treebank among greedy transition-based algorithms .", "topics": ["parsing"]}
{"title": "avoiding your teacher 's mistakes : training neural networks with controlled weak supervision", "abstract": "training deep neural networks requires massive amounts of training data , but for many tasks only limited labeled data is available . this makes weak supervision attractive , using weak or noisy signals like the output of heuristic methods or user click-through data for training . in a semi-supervised setting , we can use a large set of data with weak labels to pretrain a neural network and then fine-tune the parameters with a small amount of data with true labels . this feels intuitively sub-optimal as these two independent stages leave the model unaware about the varying label quality . what if we could somehow inform the model about the label quality ? in this paper , we propose a semi-supervised learning method where we train two neural networks in a multi-task fashion : a `` target network '' and a `` confidence network '' . the target network is optimized to perform a given task and is trained using a large set of unlabeled data that are weakly annotated . we propose to weight the gradient updates to the target network using the scores provided by the second confidence network , which is trained on a small amount of supervised data . thus we avoid that the weight updates computed from noisy labels harm the quality of the target network model . we evaluate our learning strategy on two different tasks : document ranking and sentiment classification . the results demonstrate that our approach not only enhances the performance compared to the baselines but also speeds up the learning process from weak labels .", "topics": ["baseline ( configuration management )", "supervised learning"]}
{"title": "sub-sampled cubic regularization for non-convex optimization", "abstract": "we consider the minimization of non-convex functions that typically arise in machine learning . specifically , we focus our attention on a variant of trust region methods known as cubic regularization . this approach is particularly attractive because it escapes strict saddle points and it provides stronger convergence guarantees than first- and second-order as well as classical trust region methods . however , it suffers from a high computational complexity that makes it impractical for large-scale learning . here , we propose a novel method that uses sub-sampling to lower this computational cost . by the use of concentration inequalities we provide a sampling scheme that gives sufficiently accurate gradient and hessian approximations to retain the strong global and local convergence guarantees of cubically regularized methods . to the best of our knowledge this is the first work that gives global convergence guarantees for a sub-sampled variant of cubic regularization on non-convex functions . furthermore , we provide experimental results supporting our theory .", "topics": ["matrix regularization"]}
{"title": "a selection of giant radio sources from nvss", "abstract": "results of the application of pattern recognition techniques to the problem of identifying giant radio sources ( grs ) from the data in the nvss catalog are presented and issues affecting the process are explored . decision-tree pattern recognition software was applied to training set source pairs developed from known nvss large angular size radio galaxies . the full training set consisted of 51,195 source pairs , 48 of which were known grs for which each lobe was primarily represented by a single catalog component . the source pairs had a maximum separation of 20 arc minutes and a minimum component area of 1.87 square arc minutes at the 1.4 mjy level . the importance of comparing resulting probability distributions of the training and application sets for cases of unknown class ratio is demonstrated . the probability of correctly ranking a randomly selected ( grs , non-grs ) pair from the best of the tested classifiers was determined to be 97.8 +/- 1.5 % . the best classifiers were applied to the over 870,000 candidate pairs from the entire catalog . images of higher ranked sources were visually screened and a table of over sixteen hundred candidates , including morphological annotation , is presented . these systems include doubles and triples , wide-angle tail ( wat ) and narrow-angle tail ( nat ) , s- or z-shaped systems , and core-jets and resolved cores . while some resolved lobe systems are recovered with this technique , generally it is expected that such systems would require a different approach .", "topics": ["image processing", "database"]}
{"title": "the iit bombay english-hindi parallel corpus", "abstract": "we present the iit bombay english-hindi parallel corpus . the corpus is a compilation of parallel corpora previously available in the public domain as well as new parallel corpora we collected . the corpus contains 1.49 million parallel segments , of which 694k segments were not previously available in the public domain . the corpus has been pre-processed for machine translation , and we report baseline phrase-based smt and nmt translation results on this corpus . this corpus has been used in two editions of shared tasks at the workshop on asian language transation ( 2016 and 2017 ) . the corpus is freely available for non-commercial research . to the best of our knowledge , this is the largest publicly available english-hindi parallel corpus .", "topics": ["natural language processing", "text corpus"]}
{"title": "neural tensor factorization", "abstract": "neural collaborative filtering ( ncf ) and recurrent recommender systems ( rrn ) have been successful in modeling user-item relational data . however , they are also limited in their assumption of static or sequential modeling of relational data as they do not account for evolving users ' preference over time as well as changes in the underlying factors that drive the change in user-item relationship over time . we address these limitations by proposing a neural tensor factorization ( ntf ) model for predictive tasks on dynamic relational data . the ntf model generalizes conventional tensor factorization from two perspectives : first , it leverages the long short-term memory architecture to characterize the multi-dimensional temporal interactions on relational data . second , it incorporates the multi-layer perceptron structure for learning the non-linearities between different latent factors . our extensive experiments demonstrate the significant improvement in rating prediction and link prediction on dynamic relational data by our ntf model over both neural network based factorization models and other traditional methods .", "topics": ["data mining", "interaction"]}
{"title": "using behavior objects to manage complexity in virtual worlds", "abstract": "the quality of high-level ai of non-player characters ( npcs ) in commercial open-world games ( owgs ) has been increasing during the past years . however , due to constraints specific to the game industry , this increase has been slow and it has been driven by larger budgets rather than adoption of new complex ai techniques . most of the contemporary ai is still expressed as hard-coded scripts . the complexity and manageability of the script codebase is one of the key limiting factors for further ai improvements . in this paper we address this issue . we present behavior objects - a general approach to development of npc behaviors for large owgs . behavior objects are inspired by object-oriented programming and extend the concept of smart objects . our approach promotes encapsulation of data and code for multiple related behaviors in one place , hiding internal details and embedding intelligence in the environment . behavior objects are a natural abstraction of five different techniques that we have implemented to manage ai complexity in an upcoming aaa owg . we report the details of the implementations in the context of behavior trees and the lessons learned during development . our work should serve as inspiration for ai architecture designers from both the academia and the industry .", "topics": ["high- and low-level", "artificial intelligence"]}
{"title": "deep exponential families", "abstract": "we describe \\textit { deep exponential families } ( defs ) , a class of latent variable models that are inspired by the hidden structures used in deep neural networks . defs capture a hierarchy of dependencies between latent variables , and are easily generalized to many settings through exponential families . we perform inference using recent `` black box '' variational inference techniques . we then evaluate various defs on text and combine multiple defs into a model for pairwise recommendation data . in an extensive study , we show that going beyond one layer improves predictions for defs . we demonstrate that defs find interesting exploratory structure in large data sets , and give better predictive performance than state-of-the-art models .", "topics": ["calculus of variations", "time complexity"]}
{"title": "better global polynomial approximation for image rectification", "abstract": "when using images to locate objects , there is the problem of correcting for distortion and misalignment in the images . an elegant way of solving this problem is to generate an error correcting function that maps points in an image to their corrected locations . we generate such a function by fitting a polynomial to a set of sample points . the objective is to identify a polynomial that passes `` sufficiently close '' to these points with `` good '' approximation of intermediate points . in the past , it has been difficult to achieve good global polynomial approximation using only sample points . we report on the development of a global polynomial approximation algorithm for solving this problem . key words : polynomial approximation , interpolation , image rectification .", "topics": ["approximation algorithm", "approximation"]}
{"title": "a simple algorithm for semi-supervised learning with improved generalization error bound", "abstract": "in this work , we develop a simple algorithm for semi-supervised regression . the key idea is to use the top eigenfunctions of integral operator derived from both labeled and unlabeled examples as the basis functions and learn the prediction function by a simple linear regression . we show that under appropriate assumptions about the integral operator , this approach is able to achieve an improved regression error bound better than existing bounds of supervised learning . we also verify the effectiveness of the proposed algorithm by an empirical study .", "topics": ["supervised learning"]}
{"title": "notes on elementary spectral graph theory . applications to graph clustering using normalized cuts", "abstract": "these are notes on the method of normalized graph cuts and its applications to graph clustering . i provide a fairly thorough treatment of this deeply original method due to shi and malik , including complete proofs . i include the necessary background on graphs and graph laplacians . i then explain in detail how the eigenvectors of the graph laplacian can be used to draw a graph . this is an attractive application of graph laplacians . the main thrust of this paper is the method of normalized cuts . i give a detailed account for k = 2 clusters , and also for k > 2 clusters , based on the work of yu and shi . three points that do not appear to have been clearly articulated before are elaborated : 1 . the solutions of the main optimization problem should be viewed as tuples in the k-fold cartesian product of projective space rp^ { n-1 } . 2 . when k > 2 , the solutions of the relaxed problem should be viewed as elements of the grassmannian g ( k , n ) . 3 . two possible riemannian distances are available to compare the closeness of solutions : ( a ) the distance on ( rp^ { n-1 } ) ^k . ( b ) the distance on the grassmannian . i also clarify what should be the necessary and sufficient conditions for a matrix to represent a partition of the vertices of a graph to be clustered .", "topics": ["cluster analysis", "optimization problem"]}
{"title": "wavelet-based semantic features for hyperspectral signature discrimination", "abstract": "hyperspectral signature classification is a quantitative analysis approach for hyperspectral imagery which performs detection and classification of the constituent materials at the pixel level in the scene . the classification procedure can be operated directly on hyperspectral data or performed by using some features extracted from the corresponding hyperspectral signatures containing information like the signature 's energy or shape . in this paper , we describe a technique that applies non-homogeneous hidden markov chain ( nhmc ) models to hyperspectral signature classification . the basic idea is to use statistical models ( such as nhmc ) to characterize wavelet coefficients which capture the spectrum semantics ( i.e . , structural information ) at multiple levels . experimental results show that the approach based on nhmc models can outperform existing approaches relevant in classification tasks .", "topics": ["markov chain", "pixel"]}
{"title": "a generalization of the noisy-or model", "abstract": "the noisy-or model is convenient for describing a class of uncertain relationships in bayesian networks [ pearl 1988 ] . pearl describes the noisy-or model for boolean variables . here we generalize the model to nary input and output variables and to arbitrary functions other than the boolean or function . this generalization is a useful modeling aid for construction of bayesian networks . we illustrate with some examples including digital circuit diagnosis and network reliability analysis .", "topics": ["bayesian network"]}
{"title": "beyond word-based language model in statistical machine translation", "abstract": "language model is one of the most important modules in statistical machine translation and currently the word-based language model dominants this community . however , many translation models ( e.g . phrase-based models ) generate the target language sentences by rendering and compositing the phrases rather than the words . thus , it is much more reasonable to model dependency between phrases , but few research work succeed in solving this problem . in this paper , we tackle this problem by designing a novel phrase-based language model which attempts to solve three key sub-problems : 1 , how to define a phrase in language model ; 2 , how to determine the phrase boundary in the large-scale monolingual data in order to enlarge the training set ; 3 , how to alleviate the data sparsity problem due to the huge vocabulary size of phrases . by carefully handling these issues , the extensive experiments on chinese-to-english translation show that our phrase-based language model can significantly improve the translation quality by up to +1.47 absolute bleu score .", "topics": ["test set", "machine translation"]}
{"title": "quotient based multiresolution image fusion of thermal and visual images using daubechies wavelet transform for human face recognition", "abstract": "this paper investigates the multiresolution level-1 and level-2 quotient based fusion of thermal and visual images . in the proposed system , the method-1 namely `` decompose then quotient fuse level-1 '' and the method-2 namely `` decompose-reconstruct then quotient fuse level-2 '' both work on wavelet transformations of the visual and thermal face images . the wavelet transform is well-suited to manage different image resolution and allows the image decomposition in different kinds of coefficients , while preserving the image information without any loss . this approach is based on a definition of an illumination invariant signature image which enables an analytic generation of the image space with varying illumination . the quotient fused images are passed through principal component analysis ( pca ) for dimension reduction and then those images are classified using a multi-layer perceptron ( mlp ) . the performances of both the methods have been evaluated using otcbvs and iris databases . all the different classes have been tested separately , among them the maximum recognition result is 100 % .", "topics": ["database"]}
{"title": "high-rank matrix completion and subspace clustering with missing data", "abstract": "this paper considers the problem of completing a matrix with many missing entries under the assumption that the columns of the matrix belong to a union of multiple low-rank subspaces . this generalizes the standard low-rank matrix completion problem to situations in which the matrix rank can be quite high or even full rank . since the columns belong to a union of subspaces , this problem may also be viewed as a missing-data version of the subspace clustering problem . let x be an n x n matrix whose ( complete ) columns lie in a union of at most k subspaces , each of rank < = r < n , and assume n > > kn . the main result of the paper shows that under mild assumptions each column of x can be perfectly recovered with high probability from an incomplete version so long as at least crnlog^2 ( n ) entries of x are observed uniformly at random , with c > 1 a constant depending on the usual incoherence conditions , the geometrical arrangement of subspaces , and the distribution of columns over the subspaces . the result is illustrated with numerical experiments and an application to internet distance matrix completion and topology identification .", "topics": ["cluster analysis", "numerical analysis"]}
{"title": "generative adversarial network based on resnet for conditional image restoration", "abstract": "the gans promote an adversarive game to approximate complex and jointed example probability . the networks driven by noise generate fake examples to approximate realistic data distributions . later the conditional gan merges prior-conditions as input in order to transfer attribute vectors to the corresponding data . however , the cgan is not designed to deal with the high dimension conditions since indirect guide of the learning is inefficiency . in this paper , we proposed a network resgan to generate fine images in terms of extremely degenerated images . the coarse images aligned to attributes are embedded as the generator inputs and classifier labels . in generative network , a straight path similar to the resnet is cohered to directly transfer the coarse images to the higher layers . and adversarial training is circularly implemented to prevent degeneration of the generated images . experimental results of applying the resgan to datasets mnist , cifar10/100 and celeba show its higher accuracy to the state-of-art gans .", "topics": ["statistical classification", "approximation algorithm"]}
{"title": "how to augment a small learning set for improving the performances of a cnn-based steganalyzer ?", "abstract": "deep learning and convolutional neural networks ( cnn ) have been intensively used in many image processing topics during last years . as far as steganalysis is concerned , the use of cnn allows reaching the state-of-the-art results . the performances of such networks often rely on the size of their learning database . an obvious preliminary assumption could be considering that `` the bigger a database is , the better the results are '' . however , it appears that cautions have to be taken when increasing the database size if one desire to improve the classification accuracy i.e . enhance the steganalysis efficiency . to our knowledge , no study has been performed on the enrichment impact of a learning database on the steganalysis performance . what kind of images can be added to the initial learning set ? what are the sensitive criteria : the camera models used for acquiring the images , the treatments applied to the images , the cameras proportions in the database , etc ? this article continues the work carried out in a previous paper , and explores the ways to improve the performances of cnn . it aims at studying the effects of `` base augmentation '' on the performance of steganalysis using a cnn . we present the results of this study using various experimental protocols and various databases to define the good practices in base augmentation for steganalysis .", "topics": ["image processing", "database"]}
{"title": "sentiment analysis : a survey", "abstract": "sentiment analysis ( also known as opinion mining ) refers to the use of natural language processing , text analysis and computational linguistics to identify and extract subjective information in source materials . mining opinions expressed in the user generated content is a challenging yet practically very useful problem . this survey would cover various approaches and methodology used in sentiment analysis and opinion mining in general . the focus would be on internet text like , product review , tweets and other social media .", "topics": ["natural language processing"]}
{"title": "active sensing of social networks", "abstract": "this paper develops an active sensing method to estimate the relative weight ( or trust ) agents place on their neighbors ' information in a social network . the model used for the regression is based on the steady state equation in the linear degroot model under the influence of stubborn agents , i.e . , agents whose opinions are not influenced by their neighbors . this method can be viewed as a \\emph { social radar } , where the stubborn agents excite the system and the latter can be estimated through the reverberation observed from the analysis of the agents ' opinions . the social network sensing problem can be interpreted as a blind compressed sensing problem with a sparse measurement matrix . we prove that the network structure will be revealed when a sufficient number of stubborn agents independently influence a number of ordinary ( non-stubborn ) agents . we investigate the scenario with a deterministic or randomized degroot model and propose a consistent estimator of the steady states for the latter scenario . simulation results on synthetic and real world networks support our findings .", "topics": ["simulation", "synthetic data"]}
{"title": "neural-symbolic learning and reasoning : a survey and interpretation", "abstract": "the study and understanding of human behaviour is relevant to computer science , artificial intelligence , neural computation , cognitive science , philosophy , psychology , and several other areas . presupposing cognition as basis of behaviour , among the most prominent tools in the modelling of behaviour are computational-logic systems , connectionist models of cognition , and models of uncertainty . recent studies in cognitive science , artificial intelligence , and psychology have produced a number of cognitive models of reasoning , learning , and language that are underpinned by computation . in addition , efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning . such systems have shown promise in a range of applications , including computational biology , fault diagnosis , training and assessment in simulators , and software verification . this joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning . the article is organised in three parts : firstly , we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations . we then proceed to describe the realisations of neural-symbolic computation , systems , and applications . finally we present the challenges facing the area and avenues for further research .", "topics": ["computation", "artificial intelligence"]}
{"title": "a 3d coarse-to-fine framework for automatic pancreas segmentation", "abstract": "in this paper , we adopt 3d cnns to segment the pancreas in ct images . although deep neural networks have been proven to be very effective on many 2d vision tasks , it is still challenging to apply them to 3d applications due to the limited amount of annotated 3d data and limited computational resources . we propose a novel 3d-based coarse-to-fine framework for volumetric pancreas segmentation to tackle these challenges . the proposed 3d-based framework outperforms the 2d counterpart to a large margin since it can leverage the rich spatial information along all three axes . we conduct experiments on two datasets which include healthy and pathological pancreases respectively , and achieve the state-of-the-art in terms of dice-s { \\o } rensen coefficient ( dsc ) . moreover , the worst case of dsc on the nih dataset was improved by 7 % to reach almost 70 % , which indicates the reliability of our framework in clinical applications .", "topics": ["coefficient"]}
{"title": "bioclimating modelling : a machine learning perspective", "abstract": "many machine learning ( ml ) approaches are widely used to generate bioclimatic models for prediction of geographic range of organism as a function of climate . applications such as prediction of range shift in organism , range of invasive species influenced by climate change are important parameters in understanding the impact of climate change . however , success of machine learning-based approaches depends on a number of factors . while it can be safely said that no particular ml technique can be effective in all applications and success of a technique is predominantly dependent on the application or the type of the problem , it is useful to understand their behaviour to ensure informed choice of techniques . this paper presents a comprehensive review of machine learning-based bioclimatic model generation and analyses the factors influencing success of such models . considering the wide use of statistical techniques , in our discussion we also include conventional statistical techniques used in bioclimatic modelling .", "topics": ["statistical classification"]}
{"title": "skin cancer reorganization and classification with deep neural network", "abstract": "as one kind of skin cancer , melanoma is very dangerous . dermoscopy based early detection and recarbonization strategy is critical for melanoma therapy . however , well-trained dermatologists dominant the diagnostic accuracy . in order to solve this problem , many effort focus on developing automatic image analysis systems . here we report a novel strategy based on deep learning technique , and achieve very high skin lesion segmentation and melanoma diagnosis accuracy : 1 ) we build a segmentation neural network ( skin_segnn ) , which achieved very high lesion boundary detection accuracy ; 2 ) we build another very deep neural network based on google inception v3 network ( skin_recnn ) and its well-trained weight . the novel designed transfer learning based deep neural network skin_inceptions_v3_nn helps to achieve a high prediction accuracy .", "topics": ["computer vision", "speech recognition"]}
{"title": "fast knn mode seeking clustering applied to active learning", "abstract": "a significantly faster algorithm is presented for the original knn mode seeking procedure . it has the advantages over the well-known mean shift algorithm that it is feasible in high-dimensional vector spaces and results in uniquely , well defined modes . moreover , without any additional computational effort it may yield a multi-scale hierarchy of clusterings . the time complexity is just o ( n^1.5 ) . resulting computing times range from seconds for 10^4 objects to minutes for 10^5 objects and to less than an hour for 10^6 objects . the space complexity is just o ( n ) . the procedure is well suited for finding large sets of small clusters and is thereby a candidate to analyze thousands of clusters in millions of objects . the knn mode seeking procedure can be used for active learning by assigning the clusters to the class of the modal objects of the clusters . its feasibility is shown by some examples with up to 1.5 million handwritten digits . the obtained classification results based on the clusterings are compared with those obtained by the nearest neighbor rule and the support vector classifier based on the same labeled objects for training . it can be concluded that using the clustering structure for classification can be significantly better than using the trained classifiers . a drawback of using the clustering for classification , however , is that no classifier is obtained that may be used for out-of-sample objects .", "topics": ["cluster analysis", "time complexity"]}
{"title": "adaptive parameter selection in evolutionary algorithms by reinforcement learning with dynamic discretization of parameter range", "abstract": "online parameter controllers for evolutionary algorithms adjust values of parameters during the run of an evolutionary algorithm . recently a new efficient parameter controller based on reinforcement learning was proposed by karafotias et al . in this method ranges of parameters are discretized into several intervals before the run . however , performing adaptive discretization during the run may increase efficiency of an evolutionary algorithm . aleti et al . proposed another efficient controller with adaptive discretization . in the present paper we propose a parameter controller based on reinforcement learning with adaptive discretization . the proposed controller is compared with the existing parameter adjusting methods on several test problems using different configurations of an evolutionary algorithm . for the test problems , we consider four continuous functions , namely the sphere function , the rosenbrock function , the levi function and the rastrigin function . results show that the new controller outperforms the other controllers on most of the considered test problems .", "topics": ["reinforcement learning"]}
{"title": "sequence-to-sequence generation for spoken dialogue via deep syntax trees and strings", "abstract": "we present a natural language generator based on the sequence-to-sequence approach that can be trained to produce natural language strings as well as deep syntax dependency trees from input dialogue acts , and we use it to directly compare two-step generation with separate sentence planning and surface realization stages to a joint , one-step approach . we were able to train both setups successfully using very little training data . the joint setup offers better performance , surpassing state-of-the-art with regards to n-gram-based scores while providing more relevant outputs .", "topics": ["test set", "natural language"]}
{"title": "mapping generative models onto a network of digital spiking neurons", "abstract": "stochastic neural networks such as restricted boltzmann machines ( rbms ) have been successfully used in applications ranging from speech recognition to image classification . inference and learning in these algorithms use a markov chain monte carlo procedure called gibbs sampling , where a logistic function forms the kernel of this sampler . on the other side of the spectrum , neuromorphic systems have shown great promise for low-power and parallelized cognitive computing , but lack well-suited applications and automation procedures . in this work , we propose a systematic method for bridging the rbm algorithm and digital neuromorphic systems , with a generative pattern completion task as proof of concept . for this , we first propose a method of producing the gibbs sampler using bio-inspired digital noisy integrate-and-fire neurons . next , we describe the process of mapping generative rbms trained offline onto the ibm truenorth neurosynaptic processor -- a low-power digital neuromorphic vlsi substrate . mapping these algorithms onto neuromorphic hardware presents unique challenges in network connectivity and weight and bias quantization , which , in turn , require architectural and design strategies for the physical realization . generative performance metrics are analyzed to validate the neuromorphic requirements and to best select the neuron parameters for the model . lastly , we describe a design automation procedure which achieves optimal resource usage , accounting for the novel hardware adaptations . this work represents the first implementation of generative rbm inference on a neuromorphic vlsi substrate .", "topics": ["sampling ( signal processing )", "computer vision"]}
{"title": "terngrad : ternary gradients to reduce communication in distributed deep learning", "abstract": "high network communication cost for synchronizing gradients and parameters is the well-known bottleneck of distributed training . in this work , we propose terngrad that uses ternary gradients to accelerate distributed deep learning in data parallelism . our approach requires only three numerical levels { -1,0,1 } , which can aggressively reduce the communication time . we mathematically prove the convergence of terngrad under the assumption of a bound on gradients . guided by the bound , we propose layer-wise ternarizing and gradient clipping to improve its convergence . our experiments show that applying terngrad on alexnet does not incur any accuracy loss and can even improve accuracy . the accuracy loss of googlenet induced by terngrad is less than 2 % on average . finally , a performance model is proposed to study the scalability of terngrad . experiments show significant speed gains for various deep neural networks . our source code is available .", "topics": ["numerical analysis", "gradient descent"]}
{"title": "an efficient implementation of belief function propagation", "abstract": "the local computation technique ( shafer et al . 1987 , shafer and shenoy 1988 , shenoy and shafer 1986 ) is used for propagating belief functions in so called a markov tree . in this paper , we describe an efficient implementation of belief function propagation on the basis of the local computation technique . the presented method avoids all the redundant computations in the propagation process , and so makes the computational complexity decrease with respect to other existing implementations ( hsia and shenoy 1989 , zarley et al . 1988 ) . we also give a combined algorithm for both propagation and re-propagation which makes the re-propagation process more efficient when one or more of the prior belief functions is changed .", "topics": ["computational complexity theory", "computation"]}
{"title": "a proposed infrastructure for adding online interaction to any evolutionary domain", "abstract": "to address the difficulty of creating online collaborative evolutionary systems , this paper presents a new prototype library called worldwide infrastructure for neuroevolution ( win ) and its accompanying site win online ( http : //winark.org/ ) . the win library is a collection of software packages built on top of node.js that reduce the complexity of creating fully persistent , online , and interactive ( or automated ) evolutionary platforms around any domain . win online is the public interface for win , providing an online collection of domains built with the win library that lets novice and expert users browse and meaningfully contribute to ongoing experiments . the long term goal of win is to make it trivial to connect any platform to the world , providing both a stream of online users , and archives of data and discoveries for later extension by humans or computers .", "topics": ["computation"]}
{"title": "value of information lattice : exploiting probabilistic independence for effective feature subset acquisition", "abstract": "we address the cost-sensitive feature acquisition problem , where misclassifying an instance is costly but the expected misclassification cost can be reduced by acquiring the values of the missing features . because acquiring the features is costly as well , the objective is to acquire the right set of features so that the sum of the feature acquisition cost and misclassification cost is minimized . we describe the value of information lattice ( voila ) , an optimal and efficient feature subset acquisition framework . unlike the common practice , which is to acquire features greedily , voila can reason with subsets of features . voila efficiently searches the space of possible feature subsets by discovering and exploiting conditional independence properties between the features and it reuses probabilistic inference computations to further speed up the process . through empirical evaluation on five medical datasets , we show that the greedy strategy is often reluctant to acquire features , as it can not forecast the benefit of acquiring multiple features in combination .", "topics": ["value ( ethics )", "computation"]}
{"title": "hierarchical mixture-of-experts model for large-scale gaussian process regression", "abstract": "we propose a practical and scalable gaussian process model for large-scale nonlinear probabilistic regression . our mixture-of-experts model is conceptually simple and hierarchically recombines computations for an overall approximation of a full gaussian process . closed-form and distributed computations allow for efficient and massive parallelisation while keeping the memory consumption small . given sufficient computing resources , our model can handle arbitrarily large data sets , without explicit sparse approximations . we provide strong experimental evidence that our model can be applied to large data sets of sizes far beyond millions . hence , our model has the potential to lay the foundation for general large-scale gaussian process research .", "topics": ["nonlinear system", "sparse matrix"]}
{"title": "end-to-end learning of brain tissue segmentation from imperfect labeling", "abstract": "segmenting a structural magnetic resonance imaging ( mri ) scan is an important pre-processing step for analytic procedures and subsequent inferences about longitudinal tissue changes . manual segmentation defines the current gold standard in quality but is prohibitively expensive . automatic approaches are computationally intensive , incredibly slow at scale , and error prone due to usually involving many potentially faulty intermediate steps . in order to streamline the segmentation , we introduce a deep learning model that is based on volumetric dilated convolutions , subsequently reducing both processing time and errors . compared to its competitors , the model has a reduced set of parameters and thus is easier to train and much faster to execute . the contrast in performance between the dilated network and its competitors becomes obvious when both are tested on a large dataset of unprocessed human brain volumes . the dilated network consistently outperforms not only another state-of-the-art deep learning approach , the up convolutional network , but also the ground truth on which it was trained . not only can the incredible speed of our model make large scale analyses much easier but we also believe it has great potential in a clinical setting where , with little to no substantial delay , a patient and provider can go over test results .", "topics": ["ground truth", "convolution"]}
{"title": "compressing word embeddings via deep compositional code learning", "abstract": "natural language processing ( nlp ) models often require a massive number of parameters for word embeddings , resulting in a large storage or memory footprint . deploying neural nlp models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance . for this purpose , we propose to construct the embeddings with few basis vectors . for each word , the composition of basis vectors is determined by a hash code . to maximize the compression rate , we adopt the multi-codebook quantization approach instead of binary coding scheme . each code is composed of multiple discrete numbers , such as ( 3 , 2 , 1 , 8 ) , where the value of each component is limited to a fixed range . we propose to directly learn the discrete codes in an end-to-end neural network by applying the gumbel-softmax trick . experiments show the compression rate achieves 98 % in a sentiment analysis task and 94 % ~ 99 % in machine translation tasks without performance loss . in both tasks , the proposed method can improve the model performance by slightly lowering the compression rate . compared to other approaches such as character-level segmentation , the proposed method is language-independent and does not require modifications to the network architecture .", "topics": ["natural language processing", "machine translation"]}
{"title": "on the possible computational power of the human mind", "abstract": "the aim of this paper is to address the question : can an artificial neural network ( ann ) model be used as a possible characterization of the power of the human mind ? we will discuss what might be the relationship between such a model and its natural counterpart . a possible characterization of the different power capabilities of the mind is suggested in terms of the information contained ( in its computational complexity ) or achievable by it . such characterization takes advantage of recent results based on natural neural networks ( nnn ) and the computational power of arbitrary artificial neural networks ( ann ) . the possible acceptance of neural networks as the model of the human mind 's operation makes the aforementioned quite relevant .", "topics": ["computational complexity theory"]}
{"title": "a theoretical investigation of graph degree as an unsupervised normality measure", "abstract": "for a graph representation of a dataset , a straightforward normality measure for a sample can be its graph degree . considering a weighted graph , degree of a sample is the sum of the corresponding row 's values in a similarity matrix . the measure is intuitive given the abnormal samples are usually rare and they are dissimilar to the rest of the data . in order to have an in-depth theoretical understanding , in this manuscript , we investigate the graph degree in spectral graph clustering based and kernel based point of views and draw connections to a recent kernel method for the two sample problem . we show that our analyses guide us to choose fully-connected graphs whose edge weights are calculated via universal kernels . we show that a simple graph degree based unsupervised anomaly detection method with the above properties , achieves higher accuracy compared to other unsupervised anomaly detection methods on average over 10 widely used datasets . we also provide an extensive analysis on the effect of the kernel parameter on the method 's accuracy .", "topics": ["sampling ( signal processing )", "kernel ( operating system )"]}
{"title": "developing all-skyrmion spiking neural network", "abstract": "in this work , we have proposed a revolutionary neuromorphic computing methodology to implement all-skyrmion spiking neural network ( as-snn ) . such proposed methodology is based on our finding that skyrmion is a topological stable spin texture and its spatiotemporal motion along the magnetic nano-track intuitively interprets the pulse signal transmission between two interconnected neurons . in such design , spike train in snn could be encoded as particle-like skyrmion train and further processed by the proposed skyrmion-synapse and skyrmion-neuron within the same magnetic nano-track to generate output skyrmion as post-spike . then , both pre-neuron spikes and post-neuron spikes are encoded as particle-like skyrmions without conversion between charge and spin signals , which fundamentally differentiates our proposed design from other hybrid spin-cmos designs . the system level simulation shows 87.1 % inference accuracy for handwritten digit recognition task , while the energy dissipation is ~1 fj/per spike which is 3 orders smaller in comparison with cmos based ibm truenorth system .", "topics": ["simulation"]}
{"title": "perspective alignment in spatial language", "abstract": "it is well known that perspective alignment plays a major role in the planning and interpretation of spatial language . in order to understand the role of perspective alignment and the cognitive processes involved , we have made precise complete cognitive models of situated embodied agents that self-organise a communication system for dialoging about the position and movement of real world objects in their immediate surroundings . we show in a series of robotic experiments which cognitive mechanisms are necessary and sufficient to achieve successful spatial language and why and how perspective alignment can take place , either implicitly or based on explicit marking .", "topics": ["robot"]}
{"title": "computer-aided knee joint magnetic resonance image segmentation - a survey", "abstract": "osteoarthritis ( oa ) is one of the major health issues among the elderly population . mri is the most popular technology to observe and evaluate the progress of oa course . however , the extreme labor cost of mri analysis makes the process inefficient and expensive . also , due to human error and subjective nature , the inter- and intra-observer variability is rather high . computer-aided knee mri segmentation is currently an active research field because it can alleviate doctors and radiologists from the time consuming and tedious job , and improve the diagnosis performance which has immense potential for both clinic and scientific research . in the past decades , researchers have investigated automatic/semi-automatic knee mri segmentation methods extensively . however , to the best of our knowledge , there is no comprehensive survey paper in this field yet . in this survey paper , we classify the existing methods by their principles and discuss the current research status and point out the future research trend in-depth .", "topics": ["image segmentation"]}
{"title": "strongly-typed agents are guaranteed to interact safely", "abstract": "as artificial agents proliferate , it is becoming increasingly important to ensure that their interactions with one another are well-behaved . in this paper , we formalize a common-sense notion of when algorithms are well-behaved : an algorithm is safe if it does no harm . motivated by recent progress in deep learning , we focus on the specific case where agents update their actions according to gradient descent . the first result is that gradient descent converges to a nash equilibrium in safe games . the paper provides sufficient conditions that guarantee safe interactions . the main contribution is to define strongly-typed agents and show they are guaranteed to interact safely . a series of examples show that strong-typing generalizes certain key features of convexity and is closely related to blind source separation . the analysis introduce a new perspective on classical multilinear games based on tensor decomposition .", "topics": ["interaction", "gradient descent"]}
{"title": "empirical analysis of foundational distinctions in the web of data", "abstract": "a main difference between pre-web artificial intelligence and the current one is that the web and its semantic extension ( i.e . web of data ) contain open global-scale knowledge and make it available to potentially intelligent machines that may want to benefit from it . nevertheless , most of the web of data lacks ontological distinctions and has a sparse distribution of axiomatisations . for example , foundational distinctions such as whether an entity is inherently a class or an individual , or whether it is a physical object or not , are hardly expressed in the data , although they have been largely studied and formalised by foundational ontologies ( e.g . dolce , sumo ) . there is a gap between these ontologies , that often formalise or are inspired by preexisting philosophical theories and are developed with a top-down approach , and the web of data that is mostly derived from existing databases or from crowd-based effort ( e.g . dbpedia , wikidata , freebase ) . we investigate whether the web provides an empirical foundation for characterising entities of the web of data according to foundational distinctions . we want to answer questions such as `` is the dbpedia entity for dog a class or an instance ? '' we report on a set of experiments based on machine learning and crowdsourcing that show promising results .", "topics": ["entity", "sparse matrix"]}
{"title": "effective neural solution for multi-criteria word segmentation", "abstract": "we present a simple yet elegant solution to train a single joint model on multi-criteria corpora for chinese word segmentation ( cws ) . our novel design requires no private layers in model architecture , instead , introduces two artificial tokens at the beginning and ending of input sentence to specify the required target criteria . the rest of the model including long short-term memory ( lstm ) layer and conditional random fields ( crfs ) layer remains unchanged and is shared across all datasets , keeping the size of parameter collection minimal and constant . on bakeoff 2005 and bakeoff 2008 datasets , our innovative design has surpassed both single-criterion and multi-criteria state-of-the-art learning results . to the best knowledge , our design is the first one that has achieved the latest high performance on such large scale datasets . source codes and corpora of this paper are available on github .", "topics": ["text corpus", "artificial intelligence"]}
{"title": "learning nested agent models in an information economy", "abstract": "we present our approach to the problem of how an agent , within an economic multi-agent system , can determine when it should behave strategically ( i.e . learn and use models of other agents ) , and when it should act as a simple price-taker . we provide a framework for the incremental implementation of modeling capabilities in agents , and a description of the forms of knowledge required . the agents were implemented and different populations simulated in order to learn more about their behavior and the merits of using and learning agent models . our results show , among other lessons , how savvy buyers can avoid being `` cheated '' by sellers , how price volatility can be used to quantitatively predict the benefits of deeper models , and how specific types of agent populations influence system behavior .", "topics": ["simulation"]}
{"title": "building a neural machine translation system using only synthetic parallel data", "abstract": "recent works have shown that synthetic parallel data automatically generated by translation models can be effective for various neural machine translation ( nmt ) issues . in this study , we build nmt systems using only synthetic parallel data . as an efficient alternative to real parallel data , we also present a new type of synthetic parallel corpus . the proposed pseudo parallel data are distinct from previous works in that ground truth and synthetic examples are mixed on both sides of sentence pairs . experiments on czech-german and french-german translations demonstrate the efficacy of the proposed pseudo parallel corpus , which shows not only enhanced results for bidirectional translation tasks but also substantial improvement with the aid of a ground truth real parallel corpus .", "topics": ["synthetic data", "machine translation"]}
{"title": "decision support systems using intelligent paradigms", "abstract": "decision-making is a process of choosing among alternative courses of action for solving complicated problems where multi-criteria objectives are involved . the past few years have witnessed a growing recognition of soft computing ( sc ) technologies that underlie the conception , design and utilization of intelligent systems . in this paper , we present different sc paradigms involving an artificial neural network trained using the scaled conjugate gradient algorithm , two different fuzzy inference methods optimised using neural network learning/evolutionary algorithms and regression trees for developing intelligent decision support systems . we demonstrate the efficiency of the different algorithms by developing a decision support system for a tactical air combat environment ( tace ) . some empirical comparisons between the different algorithms are also provided .", "topics": ["gradient"]}
{"title": "towards an automated requirements-driven development of smart cyber-physical systems", "abstract": "the invariant refinement method for self adaptation ( irm-sa ) is a design method targeting development of smart cyber-physical systems ( scps ) . it allows for a systematic translation of the system requirements into the system architecture expressed as an ensemble-based component system ( ebcs ) . however , since the requirements are captured using natural language , there exists the danger of their misinterpretation due to natural language requirements ' ambiguity , which could eventually lead to design errors . thus , automation and validation of the design process is desirable . in this paper , we ( i ) analyze the translation process of natural language requirements into the irm-sa model , ( ii ) identify individual steps that can be automated and/or validated using natural language processing techniques , and ( iii ) propose suitable methods .", "topics": ["natural language processing", "natural language"]}
{"title": "achieving privacy in the adversarial multi-armed bandit", "abstract": "in this paper , we improve the previously best known regret bound to achieve $ \\epsilon $ -differential privacy in oblivious adversarial bandits from $ \\mathcal { o } { ( t^ { 2/3 } /\\epsilon ) } $ to $ \\mathcal { o } { ( \\sqrt { t } \\ln t /\\epsilon ) } $ . this is achieved by combining a laplace mechanism with exp3 . we show that though exp3 is already differentially private , it leaks a linear amount of information in $ t $ . however , we can improve this privacy by relying on its intrinsic exponential mechanism for selecting actions . this allows us to reach $ \\mathcal { o } { ( \\sqrt { \\ln t } ) } $ -dp , with a regret of $ \\mathcal { o } { ( t^ { 2/3 } ) } $ that holds against an adaptive adversary , an improvement from the best known of $ \\mathcal { o } { ( t^ { 3/4 } ) } $ . this is done by using an algorithm that run exp3 in a mini-batch loop . finally , we run experiments that clearly demonstrate the validity of our theoretical analysis .", "topics": ["regret ( decision theory )", "time complexity"]}
{"title": "integrating lexical and temporal signals in neural ranking models for searching social media streams", "abstract": "time is an important relevance signal when searching streams of social media posts . the distribution of document timestamps from the results of an initial query can be leveraged to infer the distribution of relevant documents , which can then be used to rerank the initial results . previous experiments have shown that kernel density estimation is a simple yet effective implementation of this idea . this paper explores an alternative approach to mining temporal signals with recurrent neural networks . our intuition is that neural networks provide a more expressive framework to capture the temporal coherence of neighboring documents in time . to our knowledge , we are the first to integrate lexical and temporal signals in an end-to-end neural network architecture , in which existing neural ranking models are used to generate query-document similarity vectors that feed into a bidirectional lstm layer for temporal modeling . our results are mixed : existing neural models for document ranking alone yield limited improvements over simple baselines , but the integration of lexical and temporal signals yield significant improvements over competitive temporal baselines .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "the absent-minded driver problem redux", "abstract": "this paper reconsiders the problem of the absent-minded driver who must choose between alternatives with different payoff with imperfect recall and varying degrees of knowledge of the system . the classical absent-minded driver problem represents the case with limited information and it has bearing on the general area of communication and learning , social choice , mechanism design , auctions , theories of knowledge , belief , and rational agency . within the framework of extensive games , this problem has applications to many artificial intelligence scenarios . it is obvious that the performance of the agent improves as information available increases . it is shown that a non-uniform assignment strategy for successive choices does better than a fixed probability strategy . we consider both classical and quantum approaches to the problem . we argue that the superior performance of quantum decisions with access to entanglement can not be fairly compared to a classical algorithm . if the cognitive systems of agents are taken to have access to quantum resources , or have a quantum mechanical basis , then that can be leveraged into superior performance .", "topics": ["artificial intelligence"]}
{"title": "pose-invariant face alignment with a single cnn", "abstract": "face alignment has witnessed substantial progress in the last decade . one of the recent focuses has been aligning a dense 3d face shape to face images with large head poses . the dominant technology used is based on the cascade of regressors , e.g . , cnn , which has shown promising results . nonetheless , the cascade of cnns suffers from several drawbacks , e.g . , lack of end-to-end training , hand-crafted features and slow training speed . to address these issues , we propose a new layer , named visualization layer , that can be integrated into the cnn architecture and enables joint optimization with different loss functions . extensive evaluation of the proposed method on multiple datasets demonstrates state-of-the-art accuracy , while reducing the training time by more than half compared to the typical cascade of cnns . in addition , we compare multiple cnn architectures with the visualization layer to further demonstrate the advantage of its utilization .", "topics": ["loss function", "end-to-end principle"]}
{"title": "grounded lexicon acquisition - case studies in spatial language", "abstract": "this paper discusses grounded acquisition experiments of increasing complexity . humanoid robots acquire english spatial lexicons from robot tutors . we identify how various spatial language systems , such as projective , absolute and proximal can be learned . the proposed learning mechanisms do not rely on direct meaning transfer or direct access to world models of interlocutors . finally , we show how multiple systems can be acquired at the same time .", "topics": ["robot"]}
{"title": "combinatorial bandits revisited", "abstract": "this paper investigates stochastic and adversarial combinatorial multi-armed bandit problems . in the stochastic setting under semi-bandit feedback , we derive a problem-specific regret lower bound , and discuss its scaling with the dimension of the decision space . we propose escb , an algorithm that efficiently exploits the structure of the problem and provide a finite-time analysis of its regret . escb has better performance guarantees than existing algorithms , and significantly outperforms these algorithms in practice . in the adversarial setting under bandit feedback , we propose \\textsc { combexp } , an algorithm with the same regret scaling as state-of-the-art algorithms , but with lower computational complexity for some combinatorial problems .", "topics": ["regret ( decision theory )", "computational complexity theory"]}
{"title": "a novel methodology on distributed representations of proteins using their interacting ligands", "abstract": "the effective representation of proteins is a crucial task that directly affects the performance of many bioinformatics problems . related proteins usually bind to similar ligands . chemical characteristics of ligands are known to capture the functional and mechanistic properties of proteins suggesting that a ligand based approach can be utilized in protein representation . in this study , we propose smilesvec , a smiles-based method to represent ligands and a novel method to compute similarity of proteins by describing them based on their ligands . the proteins are defined utilizing the word-embeddings of the smiles strings of their ligands . the performance of the proposed protein description method is evaluated in protein clustering task using transclust and mcl algorithms . two other protein representation methods that utilize protein sequence , blast and protvec , and two compound fingerprint based protein representation methods are compared . we showed that ligand-based protein representation , which uses only smiles strings of the ligands that proteins bind to , performs as well as protein-sequence based representation methods in protein clustering . the results suggest that ligand-based protein description can be an alternative to the traditional sequence or structure based representation of proteins and this novel approach can be applied to different bioinformatics problems such as prediction of new protein-ligand interactions and protein function annotation .", "topics": ["cluster analysis"]}
{"title": "a syntactic approach to domain-specific automatic question generation", "abstract": "factoid questions are questions that require short fact-based answers . automatic generation ( aqg ) of factoid questions from a given text can contribute to educational activities , interactive question answering systems , search engines , and other applications . the goal of our research is to generate factoid source-question-answer triplets based on a specific domain . we propose a four-component pipeline , which obtains as input a training corpus of domain-specific documents , along with a set of declarative sentences from the same domain , and generates as output a set of factoid questions that refer to the source sentences but are slightly different from them , so that a question-answering system or a person can be asked a question that requires a deeper understanding and knowledge than a simple word-matching . contrary to existing domain-specific aqg systems that utilize the template-based approach to question generation , we propose to transform each source sentence into a set of questions by applying a series of domain-independent rules ( a syntactic-based approach ) . our pipeline was evaluated in the domain of cyber security using a series of experiments on each component of the pipeline separately and on the end-to-end system . the proposed approach generated a higher percentage of acceptable questions than a prior state-of-the-art aqg system .", "topics": ["end-to-end principle"]}
{"title": "matching convolutional neural networks without priors about data", "abstract": "we propose an extension of convolutional neural networks ( cnns ) to graph-structured data , including strided convolutions and data augmentation on graphs . our method matches the accuracy of state-of-the-art cnns when applied on images , without any prior about their 2d regular structure . on fmri data , we obtain a significant gain in accuracy compared with existing graph-based alternatives .", "topics": ["convolution"]}
{"title": "impression network for video object detection", "abstract": "video object detection is more challenging compared to image object detection . previous works proved that applying object detector frame by frame is not only slow but also inaccurate . visual clues get weakened by defocus and motion blur , causing failure on corresponding frames . multi-frame feature fusion methods proved effective in improving the accuracy , but they dramatically sacrifice the speed . feature propagation based methods proved effective in improving the speed , but they sacrifice the accuracy . so is it possible to improve speed and performance simultaneously ? inspired by how human utilize impression to recognize objects from blurry frames , we propose impression network that embodies a natural and efficient feature aggregation mechanism . in our framework , an impression feature is established by iteratively absorbing sparsely extracted frame features . the impression feature is propagated all the way down the video , helping enhance features of low-quality frames . this impression mechanism makes it possible to perform long-range multi-frame feature fusion among sparse keyframes with minimal overhead . it significantly improves per-frame detection baseline on imagenet vid while being 3 times faster ( 20 fps ) . we hope impression network can provide a new perspective on video feature enhancement . code will be made available .", "topics": ["baseline ( configuration management )", "object detection"]}
{"title": "embodied evolution in collective robotics : a review", "abstract": "this paper provides an overview of evolutionary robotics techniques applied to on-line distributed evolution for robot collectives -- namely , embodied evolution . it provides a definition of embodied evolution as well as a thorough description of the underlying concepts and mechanisms . the paper also presents a comprehensive summary of research published in the field since its inception ( 1999-2017 ) , providing various perspectives to identify the major trends . in particular , we identify a shift from considering embodied evolution as a parallel search method within small robot collectives ( fewer than 10 robots ) to embodied evolution as an on-line distributed learning method for designing collective behaviours in swarm-like collectives . the paper concludes with a discussion of applications and open questions , providing a milestone for past and an inspiration for future research .", "topics": ["robot"]}
{"title": "how many topics ? stability analysis for topic models", "abstract": "topic modeling refers to the task of discovering the underlying thematic structure in a text corpus , where the output is commonly presented as a report of the top terms appearing in each topic . despite the diversity of topic modeling algorithms that have been proposed , a common challenge in successfully applying these techniques is the selection of an appropriate number of topics for a given corpus . choosing too few topics will produce results that are overly broad , while choosing too many will result in the `` over-clustering '' of a corpus into many small , highly-similar topics . in this paper , we propose a term-centric stability analysis strategy to address this issue , the idea being that a model with an appropriate number of topics will be more robust to perturbations in the data . using a topic modeling approach based on matrix factorization , evaluations performed on a range of corpora show that this strategy can successfully guide the model selection process .", "topics": ["cluster analysis", "text corpus"]}
{"title": "understanding and improving convolutional neural networks via concatenated rectified linear units", "abstract": "recently , convolutional neural networks ( cnns ) have been used as a powerful tool to solve many problems of machine learning and computer vision . in this paper , we aim to provide insight on the property of convolutional neural networks , as well as a generic method to improve the performance of many cnn architectures . specifically , we first examine existing cnn models and observe an intriguing property that the filters in the lower layers form pairs ( i.e . , filters with opposite phase ) . inspired by our observation , we propose a novel , simple yet effective activation scheme called concatenated relu ( crelu ) and theoretically analyze its reconstruction property in cnns . we integrate crelu into several state-of-the-art cnn architectures and demonstrate improvement in their recognition performance on cifar-10/100 and imagenet datasets with fewer trainable parameters . our results suggest that better understanding of the properties of cnns can lead to significant performance improvement with a simple modification .", "topics": ["neural networks", "computer vision"]}
{"title": "an investigation of recurrent neural architectures for drug name recognition", "abstract": "drug name recognition ( dnr ) is an essential step in the pharmacovigilance ( pv ) pipeline . dnr aims to find drug name mentions in unstructured biomedical texts and classify them into predefined categories . state-of-the-art dnr approaches heavily rely on hand crafted features and domain specific resources which are difficult to collect and tune . for this reason , this paper investigates the effectiveness of contemporary recurrent neural architectures - the elman and jordan networks and the bidirectional lstm with crf decoding - at performing dnr straight from the text . the experimental results achieved on the authoritative semeval-2013 task 9.1 benchmarks show that the bidirectional lstm-crf ranks closely to highly-dedicated , hand-crafted systems .", "topics": ["noise reduction"]}
{"title": "temporal 3d convnets : new architecture and transfer learning for video classification", "abstract": "the work in this paper is driven by the question how to exploit the temporal cues available in videos for their accurate classification , and for human action recognition in particular ? thus far , the vision community has focused on spatio-temporal approaches with fixed temporal convolution kernel depths . we introduce a new temporal layer that models variable temporal convolution kernel depths . we embed this new temporal layer in our proposed 3d cnn . we extend the densenet architecture - which normally is 2d - with 3d filters and pooling kernels . we name our proposed video convolutional network `temporal 3d convnet'~ ( t3d ) and its new temporal layer `temporal transition layer'~ ( ttl ) . our experiments show that t3d outperforms the current state-of-the-art methods on the hmdb51 , ucf101 and kinetics datasets . the other issue in training 3d convnets is about training them from scratch with a huge labeled dataset to get a reasonable performance . so the knowledge learned in 2d convnets is completely ignored . another contribution in this work is a simple and effective technique to transfer knowledge from a pre-trained 2d cnn to a randomly initialized 3d cnn for a stable weight initialization . this allows us to significantly reduce the number of training samples for 3d cnns . thus , by finetuning this network , we beat the performance of generic and recent methods in 3d cnns , which were trained on large video datasets , e.g . sports-1m , and finetuned on the target datasets , e.g . hmdb51/ucf101 . the t3d codes will be released", "topics": ["convolution"]}
{"title": "locating 3d object proposals : a depth-based online approach", "abstract": "2d object proposals , quickly detected regions in an image that likely contain an object of interest , are an effective approach for improving the computational efficiency and accuracy of object detection in color images . in this work , we propose a novel online method that generates 3d object proposals in a rgb-d video sequence . our main observation is that depth images provide important information about the geometry of the scene . diverging from the traditional goal of 2d object proposals to provide a high recall ( lots of 2d bounding boxes near potential objects ) , we aim for precise 3d proposals . we leverage on depth information per frame and multi-view scene information to obtain accurate 3d object proposals . using efficient but robust registration enables us to combine multiple frames of a scene in near real time and generate 3d bounding boxes for potential 3d regions of interest . using standard metrics , such as precision-recall curves and f-measure , we show that the proposed approach is significantly more accurate than the current state-of-the-art techniques . our online approach can be integrated into slam based video processing for quick 3d object localization . our method takes less than a second in matlab on the uw-rgbd scene dataset on a single thread cpu and thus , has potential to be used in low-power chips in unmanned aerial vehicles ( uavs ) , quadcopters , and drones .", "topics": ["object detection"]}
{"title": "learning linear dynamical systems with high-order tensor data for skeleton based action recognition", "abstract": "in recent years , there has been renewed interest in developing methods for skeleton-based human action recognition . a skeleton sequence can be naturally represented as a high-order tensor time series . in this paper , we model and analyze tensor time series with linear dynamical system ( lds ) which is the most common for encoding spatio-temporal time-series data in various disciplines dut to its relative simplicity and efficiency . however , the traditional lds treats the latent and observation state at each frame of video as a column vector . such a vector representation fails to take into account the curse of dimensionality as well as valuable structural information with human action . considering this fact , we propose generalized linear dynamical system ( glds ) for modeling tensor observation in the time series and employ tucker decomposition to estimate the lds parameters as action descriptors . therefore , an action can be represented as a subspace corresponding to a point on a grassmann manifold . then we perform classification using dictionary learning and sparse coding over grassmann manifold . experiments on msr action3d dataset , ucf kinect dataset and northwestern-ucla multiview action3d dataset demonstrate that our proposed method achieves superior performance to the state-of-the-art algorithms .", "topics": ["time series", "sparse matrix"]}
{"title": "design of a framework to facilitate decisions using information fusion", "abstract": "information fusion is an advanced research area which can assist decision makers in enhancing their decisions . this paper aims at designing a new multi-layer framework that can support the process of performing decisions from the obtained beliefs using information fusion . since it is not an easy task to cross the gap between computed beliefs of certain hypothesis and decisions , the proposed framework consists of the following layers in order to provide a suitable architecture ( ordered bottom up ) : 1 . a layer for combination of basic belief assignments using an information fusion approach . such approach exploits dezert-smarandache theory , dsmt , and proportional conflict redistribution to provide more realistic final beliefs . 2 . a layer for computation of pignistic probability of the underlying propositions from the corresponding final beliefs . 3 . a layer for performing probabilistic reasoning using a bayesian network that can obtain the probable reason of a proposition from its pignistic probability . 4 . ranking the system decisions is ultimately used to support decision making . a case study has been accomplished at various operational conditions in order to prove the concept , in addition it pointed out that : 1 . the use of dsmt for information fusion yields not only more realistic beliefs but also reliable pignistic probabilities for the underlying propositions . 2 . exploiting the pignistic probability for the integration of the information fusion with the bayesian network provides probabilistic inference and enable decision making on the basis of both belief based probabilities for the underlying propositions and bayesian based probabilities for the corresponding reasons . a comparative study of the proposed framework with respect to other information fusion systems confirms its superiority to support decision making .", "topics": ["computation", "bayesian network"]}
{"title": "automatic local gabor features extraction for face recognition", "abstract": "we present in this paper a biometric system of face detection and recognition in color images . the face detection technique is based on skin color information and fuzzy classification . a new algorithm is proposed in order to detect automatically face features ( eyes , mouth and nose ) and extract their correspondent geometrical points . these fiducial points are described by sets of wavelet components which are used for recognition . to achieve the face recognition , we use neural networks and we study its performances for different inputs . we compare the two types of features used for recognition : geometric distances and gabor coefficients which can be used either independently or jointly . this comparison shows that gabor coefficients are more powerful than geometric distances . we show with experimental results how the importance recognition ratio makes our system an effective tool for automatic face detection and recognition .", "topics": ["feature extraction", "coefficient"]}
{"title": "frequency analysis of temporal graph signals", "abstract": "this letter extends the concept of graph-frequency to graph signals that evolve with time . our goal is to generalize and , in fact , unify the familiar concepts from time- and graph-frequency analysis . to this end , we study a joint temporal and graph fourier transform ( jft ) and demonstrate its attractive properties . we build on our results to create filters which act on the joint ( temporal and graph ) frequency domain , and show how these can be used to perform interference cancellation . the proposed algorithms are distributed , have linear complexity , and can approximate any desired joint filtering objective .", "topics": ["approximation algorithm", "numerical analysis"]}
{"title": "unsupervised , knowledge-free , and interpretable word sense disambiguation", "abstract": "interpretability of a predictive model is a powerful feature that gains the trust of users in the correctness of the predictions . in word sense disambiguation ( wsd ) , knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements representing word senses , such as hypernyms , usage examples , and images . we present a wsd system that bridges the gap between these two so far disconnected groups of methods . namely , our system , providing access to several state-of-the-art wsd models , aims to be interpretable as a knowledge-based system while it remains completely unsupervised and knowledge-free . the presented tool features a web interface for all-word disambiguation of texts that makes the sense predictions human readable by providing interpretable word sense inventories , sense representations , and disambiguation results . we provide a public api , enabling seamless integration .", "topics": ["unsupervised learning"]}
{"title": "fpgaconvnet : a toolflow for mapping diverse convolutional neural networks on embedded fpgas", "abstract": "in recent years , convolutional neural networks ( convnets ) have become an enabling technology for a wide range of novel embedded artificial intelligence systems . across the range of applications , the performance needs vary significantly , from high-throughput video surveillance to the very low-latency requirements of autonomous cars . in this context , fpgas can provide a potential platform that can be optimally configured based on the different performance needs . however , the complexity of convnet models keeps increasing making their mapping to an fpga device a challenging task . this work presents fpgaconvnet , an end-to-end framework for mapping convnets on fpgas . the proposed framework employs an automated design methodology based on the synchronous dataflow ( sdf ) paradigm and defines a set of sdf transformations in order to efficiently explore the architectural design space . by selectively optimising for throughput , latency or multiobjective criteria , the presented tool is able to efficiently explore the design space and generate hardware designs from high-level convnet specifications , explicitly optimised for the performance metric of interest . overall , our framework yields designs that improve the performance by up to 6.65x over highly optimised embedded gpu designs for the same power constraints in embedded environments .", "topics": ["high- and low-level", "neural networks"]}
{"title": "predicting out-of-view feature points for model-based camera pose estimation", "abstract": "in this work we present a novel framework that uses deep learning to predict object feature points that are out-of-view in the input image . this system was developed with the application of model-based tracking in mind , particularly in the case of autonomous inspection robots , where only partial views of the object are available . out-of-view prediction is enabled by applying scaling to the feature point labels during network training . this is combined with a recurrent neural network architecture designed to provide the final prediction layers with rich feature information from across the spatial extent of the input image . to show the versatility of these out-of-view predictions , we describe how to integrate them in both a particle filter tracker and an optimisation based tracker . to evaluate our work we compared our framework with one that predicts only points inside the image . we show that as the amount of the object in view decreases , being able to predict outside the image bounds adds robustness to the final pose estimation .", "topics": ["recurrent neural network", "mathematical optimization"]}
{"title": "improved twitter sentiment analysis using naive bayes and custom language model", "abstract": "in the last couple decades , social network services like twitter have generated large volumes of data about users and their interests , providing meaningful business intelligence so organizations can better understand and engage their customers . all businesses want to know who is promoting their products , who is complaining about them , and how are these opinions bringing or diminishing value to a company . companies want to be able to identify their high-value customers and quantify the value each user brings . many businesses use social media metrics to calculate the user contribution score , which enables them to quantify the value that influential users bring on social media , so the businesses can offer them more differentiated services . however , the score calculation can be refined to provide a better illustration of a user 's contribution . using microsoft azure as a case study , we conducted twitter sentiment analysis to develop a machine learning classification model that identifies tweet contents and sentiments most illustrative of positive-value user contribution . using data mining and ai-powered cognitive tools , we analyzed factors of social influence and specifically , promotional language in the developer community . our predictive model was a combination of a traditional supervised machine learning algorithm and a custom-developed natural language model for identifying promotional tweets , that identifies a product-specific promotion on twitter with a 90 % accuracy rate .", "topics": ["data mining", "supervised learning"]}
{"title": "learning spatial-temporal regularized correlation filters for visual tracking", "abstract": "discriminative correlation filters ( dcf ) are efficient in visual tracking but suffer from unwanted boundary effects . spatially regularized dcf ( srdcf ) has been suggested to resolve this issue by enforcing spatial penalty on dcf coefficients , which , inevitably , improves the tracking performance at the price of increasing complexity . to tackle online updating , srdcf formulates its model on multiple training images , further adding difficulties in improving efficiency . in this work , by introducing temporal regularization to srdcf with single sample , we present our spatial-temporal regularized correlation filters ( strcf ) . motivated by online passive-agressive ( pa ) algorithm , we introduce the temporal regularization to srdcf with single sample , thus resulting in our spatial-temporal regularized correlation filters ( strcf ) . the strcf formulation can not only serve as a reasonable approximation to srdcf with multiple training samples , but also provide a more robust appearance model than srdcf in the case of large appearance variations . besides , it can be efficiently solved via the alternating direction method of multipliers ( admm ) . by incorporating both temporal and spatial regularization , our strcf can handle boundary effects without much loss in efficiency and achieve superior performance over srdcf in terms of accuracy and speed . experiments are conducted on three benchmark datasets : otb-2015 , temple-color , and vot-2016 . compared with srdcf , strcf with hand-crafted features provides a 5 times speedup and achieves a gain of 5.4 % and 3.6 % auc score on otb-2015 and temple-color , respectively . moreover , strcf combined with cnn features also performs favorably against state-of-the-art cnn-based trackers and achieves an auc score of 68.3 % on otb-2015 .", "topics": ["matrix regularization", "coefficient"]}
{"title": "end-to-end text recognition with hybrid hmm maxout models", "abstract": "the problem of detecting and recognizing text in natural scenes has proved to be more challenging than its counterpart in documents , with most of the previous work focusing on a single part of the problem . in this work , we propose new solutions to the character and word recognition problems and then show how to combine these solutions in an end-to-end text-recognition system . we do so by leveraging the recently introduced maxout networks along with hybrid hmm models that have proven useful for voice recognition . using these elements , we build a tunable and highly accurate recognition system that beats state-of-the-art results on all the sub-problems for both the icdar 2003 and svt benchmark datasets .", "topics": ["end-to-end principle"]}
{"title": "a practically competitive and provably consistent algorithm for uplift modeling", "abstract": "randomized experiments have been critical tools of decision making for decades . however , subjects can show significant heterogeneity in response to treatments in many important applications . therefore it is not enough to simply know which treatment is optimal for the entire population . what we need is a model that correctly customize treatment assignment base on subject characteristics . the problem of constructing such models from randomized experiments data is known as uplift modeling in the literature . many algorithms have been proposed for uplift modeling and some have generated promising results on various data sets . yet little is known about the theoretical properties of these algorithms . in this paper , we propose a new tree-based ensemble algorithm for uplift modeling . experiments show that our algorithm can achieve competitive results on both synthetic and industry-provided data . in addition , by properly tuning the `` node size '' parameter , our algorithm is proved to be consistent under mild regularity conditions . this is the first consistent algorithm for uplift modeling that we are aware of .", "topics": ["synthetic data"]}
{"title": "mmse estimation for poisson noise removal in images", "abstract": "poisson noise suppression is an important preprocessing step in several applications , such as medical imaging , microscopy , and astronomical imaging . in this work , we propose a novel patch-wise poisson noise removal strategy , in which the mmse estimator is utilized in order to produce the denoising result for each image patch . fast and accurate computation of the mmse estimator is carried out using k-d tree search followed by search in the k-nearest neighbor graph . our experiments show that the proposed method is the preferable choice for low signal-to-noise ratios .", "topics": ["noise reduction", "computation"]}
{"title": "decomposeme : simplifying convnets for end-to-end learning", "abstract": "deep learning and convolutional neural networks ( convnets ) have been successfully applied to most relevant tasks in the computer vision community . however , these networks are computationally demanding and not suitable for embedded devices where memory and time consumption are relevant . in this paper , we propose decomposeme , a simple but effective technique to learn features using 1d convolutions . the proposed architecture enables both simplicity and filter sharing leading to increased learning capacity . a comprehensive set of large-scale experiments on imagenet and places2 demonstrates the ability of our method to improve performance while significantly reducing the number of parameters required . notably , on places2 , we obtain an improvement in relative top-1 classification accuracy of 7.7\\ % with an architecture that requires 92 % fewer parameters compared to vgg-b . the proposed network is also demonstrated to generalize to other tasks by converting existing networks .", "topics": ["computer vision", "convolution"]}
{"title": "emergent parsing and generation with generalized chart", "abstract": "a new , flexible inference method for horn logic program is proposed , which is a drastic generalization of chart parsing , partial instantiations of clauses in a program roughly corresponding to arcs in a chart . chart-like parsing and semantic-head-driven generation emerge from this method . with a parsimonious instantiation scheme for ambiguity packing , the parsing complexity reduces to that of standard chart-based algorithms .", "topics": ["parsing"]}
{"title": "learning to generate long-term future via hierarchical prediction", "abstract": "we propose a hierarchical approach for making long-term predictions of future frames . to avoid inherent compounding errors in recursive pixel-level prediction , we propose to first estimate high-level structure in the input frames , then predict how that structure evolves in the future , and finally by observing a single frame from the past and the predicted high-level structure , we construct the future frames without having to observe any of the pixel-level predictions . long-term video prediction is difficult to perform by recurrently observing the predicted frames because the small errors in pixel space exponentially amplify as predictions are made deeper into the future . our approach prevents pixel-level error propagation from happening by removing the need to observe the predicted frames . our model is built with a combination of lstm and analogy based encoder-decoder convolutional neural networks , which independently predict the video structure and generate the future frames , respectively . in experiments , our model is evaluated on the human3.6m and penn action datasets on the task of long-term pixel-level video prediction of humans performing actions and demonstrate significantly better results than the state-of-the-art .", "topics": ["map", "ground truth"]}
{"title": "memory-based parameter adaptation", "abstract": "deep neural networks have excelled on a wide range of problems , from vision to language and game playing . neural networks very gradually incorporate information into weights as they process data , requiring very low learning rates . if the training distribution shifts , the network is slow to adapt , and when it does adapt , it typically performs badly on the training distribution before the shift . our method , memory-based parameter adaptation , stores examples in memory and then uses a context-based lookup to directly modify the weights of a neural network . much higher learning rates can be used for this local adaptation , reneging the need for many iterations over similar data before good predictions can be made . as our method is memory-based , it alleviates several shortcomings of neural networks , such as catastrophic forgetting , fast , stable acquisition of new knowledge , learning with an imbalanced class labels , and fast learning during evaluation . we demonstrate this on a range of supervised tasks : large-scale image classification and language modelling .", "topics": ["supervised learning", "computer vision"]}
{"title": "stratified bayesian optimization", "abstract": "we consider derivative-free black-box global optimization of expensive noisy functions , when most of the randomness in the objective is produced by a few influential scalar random inputs . we present a new bayesian global optimization algorithm , called stratified bayesian optimization ( sbo ) , which uses this strong dependence to improve performance . our algorithm is similar in spirit to stratification , a technique from simulation , which uses strong dependence on a categorical representation of the random input to reduce variance . we demonstrate in numerical experiments that sbo outperforms state-of-the-art bayesian optimization benchmarks that do not leverage this dependence .", "topics": ["numerical analysis", "simulation"]}
{"title": "robot language learning , generation , and comprehension", "abstract": "we present a unified framework which supports grounding natural-language semantics in robotic driving . this framework supports acquisition ( learning grounded meanings of nouns and prepositions from human annotation of robotic driving paths ) , generation ( using such acquired meanings to generate sentential description of new robotic driving paths ) , and comprehension ( using such acquired meanings to support automated driving to accomplish navigational goals specified in natural language ) . we evaluate the performance of these three tasks by having independent human judges rate the semantic fidelity of the sentences associated with paths , achieving overall average correctness of 94.6 % and overall average completeness of 85.6 % .", "topics": ["natural language", "autonomous car"]}
{"title": "on inconsistency indices and inconsistency axioms in pairwise comparisons", "abstract": "pairwise comparisons are an important tool of modern ( multiple criteria ) decision making . since human judgments are often inconsistent , many studies focused on the ways how to express and measure this inconsistency , and several inconsistency indices were proposed as an alternative to saaty inconsistency index and inconsistency ratio for reciprocal pairwise comparisons matrices . this paper aims to : firstly , introduce a new measure of inconsistency of pairwise comparisons and to prove its basic properties ; secondly , to postulate an additional axiom , an upper boundary axiom , to an existing set of axioms ; and the last , but not least , the paper provides proofs of satisfaction of this additional axiom by selected inconsistency indices as well as it provides their numerical comparison .", "topics": ["numerical analysis"]}
{"title": "a compact linear programming relaxation for binary sub-modular mrf", "abstract": "we propose a novel compact linear programming ( lp ) relaxation for binary sub-modular mrf in the context of object segmentation . our model is obtained by linearizing an $ l_1^+ $ -norm derived from the quadratic programming ( qp ) form of the mrf energy . the resultant lp model contains significantly fewer variables and constraints compared to the conventional lp relaxation of the mrf energy . in addition , unlike qp which can produce ambiguous labels , our model can be viewed as a quasi-total-variation minimization problem , and it can therefore preserve the discontinuities in the labels . we further establish a relaxation bound between our lp model and the conventional lp model . in the experiments , we demonstrate our method for the task of interactive object segmentation . our lp model outperforms qp when converting the continuous labels to binary labels using different threshold values on the entire oxford interactive segmentation dataset . the computational complexity of our lp is of the same order as that of the qp , and it is significantly lower than the conventional lp relaxation .", "topics": ["approximation algorithm", "computation"]}
{"title": "hybrid srl with optimization modulo theories", "abstract": "generally speaking , the goal of constructive learning could be seen as , given an example set of structured objects , to generate novel objects with similar properties . from a statistical-relational learning ( srl ) viewpoint , the task can be interpreted as a constraint satisfaction problem , i.e . the generated objects must obey a set of soft constraints , whose weights are estimated from the data . traditional srl approaches rely on ( finite ) first-order logic ( fol ) as a description language , and on max-sat solvers to perform inference . alas , fol is unsuited for con- structive problems where the objects contain a mixture of boolean and numerical variables . it is in fact difficult to implement , e.g . linear arithmetic constraints within the language of fol . in this paper we propose a novel class of hybrid srl methods that rely on satisfiability modulo theories , an alternative class of for- mal languages that allow to describe , and reason over , mixed boolean-numerical objects and constraints . the resulting methods , which we call learning mod- ulo theories , are formulated within the structured output svm framework , and employ a weighted smt solver as an optimization oracle to perform efficient in- ference and discriminative max margin weight learning . we also present a few examples of constructive learning applications enabled by our method .", "topics": ["mathematical optimization", "numerical analysis"]}
{"title": "a variational bayesian state-space approach to online passive-aggressive regression", "abstract": "online passive-aggressive ( pa ) learning is a class of online margin-based algorithms suitable for a wide range of real-time prediction tasks , including classification and regression . pa algorithms are formulated in terms of deterministic point-estimation problems governed by a set of user-defined hyperparameters : the approach fails to capture model/prediction uncertainty and makes their performance highly sensitive to hyperparameter configurations . in this paper , we introduce a novel pa learning framework for regression that overcomes the above limitations . we contribute a bayesian state-space interpretation of pa regression , along with a novel online variational inference scheme , that not only produces probabilistic predictions , but also offers the benefit of automatic hyperparameter tuning . experiments with various real-world data sets show that our approach performs significantly better than a more standard , linear gaussian state-space model .", "topics": ["calculus of variations"]}
{"title": "a hybrid feature selection method to improve performance of a group of classification algorithms", "abstract": "in this paper a hybrid feature selection method is proposed which takes advantages of wrapper subset evaluation with a lower cost and improves the performance of a group of classifiers . the method uses combination of sample domain filtering and resampling to refine the sample domain and two feature subset evaluation methods to select reliable features . this method utilizes both feature space and sample domain in two phases . the first phase filters and resamples the sample domain and the second phase adopts a hybrid procedure by information gain , wrapper subset evaluation and genetic search to find the optimal feature space . experiments carried out on different types of datasets from uci repository of machine learning databases and the results show a rise in the average performance of five classifiers ( naive bayes , logistic , multilayer perceptron , best first decision tree and jrip ) simultaneously and the classification error for these classifiers decreases considerably . the experiments also show that this method outperforms other feature selection methods with a lower cost .", "topics": ["feature vector", "database"]}
{"title": "rule-and dictionary-based solution for variations in written arabic names in social networks , big data , accounting systems and large databases", "abstract": "this paper investigates the problem that some arabic names can be written in multiple ways . when someone searches for only one form of a name , neither exact nor approximate matching is appropriate for returning the multiple variants of the name . exact matching requires the user to enter all forms of the name for the search , and approximate matching yields names not among the variations of the one being sought . in this paper , we attempt to solve the problem with a dictionary of all arabic names mapped to their different ( alternative ) writing forms . we generated alternatives based on rules we derived from reviewing the first names of 9.9 million citizens and former citizens of jordan . this dictionary can be used for both standardizing the written form when inserting a new name into a database and for searching for the name and all its alternative written forms . creating the dictionary automatically based on rules resulted in at least 7 % erroneous acceptance errors and 7.9 % erroneous rejection errors . we addressed the errors by manually editing the dictionary . the dictionary can be of help to real world-databases , with the qualification that manual editing does not guarantee 100 % correctness .", "topics": ["approximation algorithm", "dictionary"]}
{"title": "latent variable modeling with diversity-inducing mutual angular regularization", "abstract": "latent variable models ( lvms ) are a large family of machine learning models providing a principled and effective way to extract underlying patterns , structure and knowledge from observed data . due to the dramatic growth of volume and complexity of data , several new challenges have emerged and can not be effectively addressed by existing lvms : ( 1 ) how to capture long-tail patterns that carry crucial information when the popularity of patterns is distributed in a power-law fashion ? ( 2 ) how to reduce model complexity and computational cost without compromising the modeling power of lvms ? ( 3 ) how to improve the interpretability and reduce the redundancy of discovered patterns ? to addresses the three challenges discussed above , we develop a novel regularization technique for lvms , which controls the geometry of the latent space during learning to enable the learned latent components of lvms to be diverse in the sense that they are favored to be mutually different from each other , to accomplish long-tail coverage , low redundancy , and better interpretability . we propose a mutual angular regularizer ( mar ) to encourage the components in lvms to have larger mutual angles . the mar is non-convex and non-smooth , entailing great challenges for optimization . to cope with this issue , we derive a smooth lower bound of the mar and optimize the lower bound instead . we show that the monotonicity of the lower bound is closely aligned with the mar to qualify the lower bound as a desirable surrogate of the mar . using neural network ( nn ) as an instance , we analyze how the mar affects the generalization performance of nn . on two popular latent variable models -- - restricted boltzmann machine and distance metric learning , we demonstrate that mar can effectively capture long-tail patterns , reduce model complexity without sacrificing expressivity and improve interpretability .", "topics": ["matrix regularization", "computer vision"]}
{"title": "camera identification by grouping images from database , based on shared noise patterns", "abstract": "previous research showed that camera specific noise patterns , so-called prnu-patterns , are extracted from images and related images could be found . in this particular research the focus is on grouping images from a database , based on a shared noise pattern as an identification method for cameras . using the method as described in this article , groups of images , created using the same camera , could be linked from a large database of images . using matlab programming , relevant image noise patterns are extracted from images much quicker than common methods by the use of faster noise extraction filters and improvements to reduce the calculation costs . relating noise patterns , with a correlation above a certain threshold value , can quickly be matched . hereby , from a database of images , groups of relating images could be linked and the method could be used to scan a large number of images for suspect noise patterns .", "topics": ["database"]}
{"title": "long short-term memory and learning-to-learn in networks of spiking neurons", "abstract": "networks of spiking neurons ( snns ) are frequently studied as models for networks of neurons in the brain , but also as paradigm for novel energy efficient computing hardware . in principle they are especially suitable for computations in the temporal domain , such as speech processing , because their computations are carried out via events in time and space . but so far they have been lacking the capability to preserve information for longer time spans during a computation , until it is updated or needed - like a register of a digital computer . this function is provided to artificial neural networks through long short-term memory ( lstm ) units . we show here that snns attain similar capabilities if one includes adapting neurons in the network . adaptation denotes an increase of the firing threshold of a neuron after preceding firing . a substantial fraction of neurons in the neocortex of rodents and humans has been found to be adapting . it turns out that if adapting neurons are integrated in a suitable manner into the architecture of snns , the performance of these enhanced snns , which we call lsnns , for computation in the temporal domain approaches that of artificial neural networks with lstm-units . in addition , the computing and learning capabilities of lsnns can be substantially enhanced through learning-to-learn ( l2l ) methods from machine learning , that have so far been applied primarily to lstm networks and apparently never to ssns . this preliminary report on arxiv will be replaced by a more detailed version in about a month .", "topics": ["computation"]}
{"title": "mcmc for hierarchical semi-markov conditional random fields", "abstract": "deep architecture such as hierarchical semi-markov models is an important class of models for nested sequential data . current exact inference schemes either cost cubic time in sequence length , or exponential time in model depth . these costs are prohibitive for large-scale problems with arbitrary length and depth . in this contribution , we propose a new approximation technique that may have the potential to achieve sub-cubic time complexity in length and linear time depth , at the cost of some loss of quality . the idea is based on two well-known methods : gibbs sampling and rao-blackwellisation . we provide some simulation-based evaluation of the quality of the rgbs with respect to run time and sequence length .", "topics": ["time complexity", "simulation"]}
{"title": "deep contrast learning for salient object detection", "abstract": "salient object detection has recently witnessed substantial progress due to powerful features extracted using deep convolutional neural networks ( cnns ) . however , existing cnn-based methods operate at the patch level instead of the pixel level . resulting saliency maps are typically blurry , especially near the boundary of salient objects . furthermore , image patches are treated as independent samples even when they are overlapping , giving rise to significant redundancy in computation and storage . in this cvpr 2016 paper , we propose an end-to-end deep contrast network to overcome the aforementioned limitations . our deep network consists of two complementary components , a pixel-level fully convolutional stream and a segment-wise spatial pooling stream . the first stream directly produces a saliency map with pixel-level accuracy from an input image . the second stream extracts segment-wise features very efficiently , and better models saliency discontinuities along object boundaries . finally , a fully connected crf model can be optionally incorporated to improve spatial coherence and contour localization in the fused result from these two streams . experimental results demonstrate that our deep model significantly improves the state of the art .", "topics": ["object detection", "map"]}
{"title": "high-dimensional graphical model search with graphd r package", "abstract": "this paper presents the r package graphd for efficient selection of high-dimensional undirected graphical models . the package provides tools for selecting trees , forests and decomposable models minimizing information criteria such as aic or bic , and for displaying the independence graphs of the models . it has also some useful tools for analysing graphical structures . it supports the use of discrete , continuous , or both types of variables simultaneously .", "topics": ["graphical model"]}
{"title": "statistical tools to assess the reliability of self-organizing maps", "abstract": "results of neural network learning are always subject to some variability , due to the sensitivity to initial conditions , to convergence to local minima , and , sometimes more dramatically , to sampling variability . this paper presents a set of tools designed to assess the reliability of the results of self-organizing maps ( som ) , i.e . to test on a statistical basis the confidence we can have on the result of a specific som . the tools concern the quantization error in a som , and the neighborhood relations ( both at the level of a specific pair of observations and globally on the map ) . as a by-product , these measures also allow to assess the adequacy of the number of units chosen in a map . the tools may also be used to measure objectively how the som are less sensitive to non-linear optimization problems ( local minima , convergence , etc . ) than other neural network models .", "topics": ["sampling ( signal processing )", "nonlinear system"]}
{"title": "searching for biophysically realistic parameters for dynamic neuron models by genetic algorithms from calcium imaging recording", "abstract": "individual neurons in the nervous systems exploit various dynamics . to capture these dynamics for single neurons , we tune the parameters of an electrophysiological model of nerve cells , to fit experimental data obtained by calcium imaging . a search for the biophysical parameters of this model is performed by means of a genetic algorithm , where the model neuron is exposed to a predefined input current representing overall inputs from other parts of the nervous system . the algorithm is then constrained for keeping the ion-channel currents within reasonable ranges , while producing the best fit to a calcium imaging time series of the ava interneuron , from the brain of the soil-worm , c . elegans . our settings enable us to project a set of biophysical parameters to the the neuron kinetics observed in neuronal imaging .", "topics": ["time series", "nonlinear system"]}
{"title": "vietnamese open information extraction", "abstract": "open information extraction ( oie ) is the process to extract relations and their arguments automatically from textual documents without the need to restrict the search to predefined relations . in recent years , several oie systems for the english language have been created but there is not any system for the vietnamese language . in this paper , we propose a method of oie for vietnamese using a clause-based approach . accordingly , we exploit vietnamese dependency parsing using grammar clauses that strives to consider all possible relations in a sentence . the corresponding clause types are identified by their propositions as extractable relations based on their grammatical functions of constituents . as a result , our system is the first oie system named vnoie for the vietnamese language that can generate open relations and their arguments from vietnamese text with highly scalable extraction while being domain independent . experimental results show that our oie system achieves promising results with a precision of 83.71 % .", "topics": ["parsing"]}
{"title": "flatcam : thin , bare-sensor cameras using coded aperture and computation", "abstract": "flatcam is a thin form-factor lensless camera that consists of a coded mask placed on top of a bare , conventional sensor array . unlike a traditional , lens-based camera where an image of the scene is directly recorded on the sensor pixels , each pixel in flatcam records a linear combination of light from multiple scene elements . a computational algorithm is then used to demultiplex the recorded measurements and reconstruct an image of the scene . flatcam is an instance of a coded aperture imaging system ; however , unlike the vast majority of related work , we place the coded mask extremely close to the image sensor that can enable a thin system . we employ a separable mask to ensure that both calibration and image reconstruction are scalable in terms of memory requirements and computational complexity . we demonstrate the potential of the flatcam design using two prototypes : one at visible wavelengths and one at infrared wavelengths .", "topics": ["computational complexity theory", "computation"]}
{"title": "deep successor reinforcement learning", "abstract": "learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms . there is a third alternative , called successor representations ( sr ) , which decomposes the value function into two components -- a reward predictor and a successor map . the successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards . the value function of a state can be computed as the inner product between the successor map and the reward weights . in this paper , we present dsr , which generalizes sr within an end-to-end deep reinforcement learning framework . dsr has several appealing properties including : increased sensitivity to distal reward changes due to factorization of reward and world dynamics , and the ability to extract bottleneck states ( subgoals ) given successor maps trained under a random policy . we show the efficacy of our approach on two diverse environments given raw pixel observations -- simple grid-world domains ( mazebase ) and the doom game engine .", "topics": ["reinforcement learning", "map"]}
{"title": "linguistic knowledge as memory for recurrent neural networks", "abstract": "training recurrent neural networks to model long term dependencies is difficult . hence , we propose to use external linguistic knowledge as an explicit signal to inform the model which memories it should utilize . specifically , external knowledge is used to augment a sequence with typed edges between arbitrarily distant elements , and the resulting graph is decomposed into directed acyclic subgraphs . we introduce a model that encodes such graphs as explicit memory in recurrent neural networks , and use it to model coreference relations in text . we apply our model to several text comprehension tasks and achieve new state-of-the-art results on all considered benchmarks , including cnn , babi , and lambada . on the babi qa tasks , our model solves 15 out of the 20 tasks with only 1000 training examples per task . analysis of the learned representations further demonstrates the ability of our model to encode fine-grained entity information across a document .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "quality of geographic information : ontological approach and artificial intelligence tools", "abstract": "the objective is to present one important aspect of the european ist-fet project `` rev ! gis '' 1 : the methodology which has been developed for the translation ( interpretation ) of the quality of the data into a `` fitness for use '' information , that we can confront to the user needs in its application . this methodology is based upon the notion of `` ontologies '' as a conceptual framework able to capture the explicit and implicit knowledge involved in the application . we do not address the general problem of formalizing such ontologies , instead , we rather try to illustrate this with three applications which are particular cases of the more general `` data fusion '' problem . in each application , we show how to deploy our methodology , by comparing several possible solutions , and we try to enlighten where are the quality issues , and what kind of solution to privilege , even at the expense of a highly complex computational approach . the expectation of the rev ! gis project is that computationally tractable solutions will be available among the next generation ai tools .", "topics": ["artificial intelligence"]}
{"title": "house price prediction using lstm", "abstract": "in this paper , we use the house price data ranging from january 2004 to october 2016 to predict the average house price of november and december in 2016 for each district in beijing , shanghai , guangzhou and shenzhen . we apply autoregressive integrated moving average model to generate the baseline while lstm networks to build prediction model . these algorithms are compared in terms of mean squared error . the result shows that the lstm model has excellent properties with respect to predict time series . also , stateful lstm networks and stack lstm networks are employed to further study the improvement of accuracy of the house prediction model .", "topics": ["baseline ( configuration management )", "time series"]}
{"title": "do we need binary features for 3d reconstruction ?", "abstract": "binary features have been incrementally popular in the past few years due to their low memory footprints and the efficient computation of hamming distance between binary descriptors . they have been shown with promising results on some real time applications , e.g . , slam , where the matching operations are relative few . however , in computer vision , there are many applications such as 3d reconstruction requiring lots of matching operations between local features . therefore , a natural question is that is the binary feature still a promising solution to this kind of applications ? to get the answer , this paper conducts a comparative study of binary features and their matching methods on the context of 3d reconstruction in a recently proposed large scale mutliview stereo dataset . our evaluations reveal that not all binary features are capable of this task . most of them are inferior to the classical sift based method in terms of reconstruction accuracy and completeness with a not significant better computational performance .", "topics": ["computer vision", "computation"]}
{"title": "online reinforcement learning in stochastic games", "abstract": "we study online reinforcement learning in average-reward stochastic games ( sgs ) . an sg models a two-player zero-sum game in a markov environment , where state transitions and one-step payoffs are determined simultaneously by a learner and an adversary . we propose the ucsg algorithm that achieves a sublinear regret compared to the game value when competing with an arbitrary opponent . this result improves previous ones under the same setting . the regret bound has a dependency on the diameter , which is an intrinsic value related to the mixing property of sgs . if we let the opponent play an optimistic best response to the learner , ucsg finds an $ \\varepsilon $ -maximin stationary policy with a sample complexity of $ \\tilde { \\mathcal { o } } \\left ( \\text { poly } ( 1/\\varepsilon ) \\right ) $ , where $ \\varepsilon $ is the gap to the best policy .", "topics": ["regret ( decision theory )", "reinforcement learning"]}
{"title": "co-clustering through optimal transport", "abstract": "in this paper , we present a novel method for co-clustering , an unsupervised learning approach that aims at discovering homogeneous groups of data instances and features by grouping them simultaneously . the proposed method uses the entropy regularized optimal transport between empirical measures defined on data instances and features in order to obtain an estimated joint probability density function represented by the optimal coupling matrix . this matrix is further factorized to obtain the induced row and columns partitions using multiscale representations approach . to justify our method theoretically , we show how the solution of the regularized optimal transport can be seen from the variational inference perspective thus motivating its use for co-clustering . the algorithm derived for the proposed method and its kernelized version based on the notion of gromov-wasserstein distance are fast , accurate and can determine automatically the number of both row and column clusters . these features are vividly demonstrated through extensive experimental evaluations .", "topics": ["cluster analysis", "calculus of variations"]}
{"title": "craft : cluster-specific assorted feature selection", "abstract": "we present a framework for clustering with cluster-specific feature selection . the framework , craft , is derived from asymptotic log posterior formulations of nonparametric map-based clustering models . craft handles assorted data , i.e . , both numeric and categorical data , and the underlying objective functions are intuitively appealing . the resulting algorithm is simple to implement and scales nicely , requires minimal parameter tuning , obviates the need to specify the number of clusters a priori , and compares favorably with other methods on real datasets .", "topics": ["cluster analysis", "map"]}
{"title": "boolvar/pb v1.0 , a java library for translating pseudo-boolean constraints into cnf formulae", "abstract": "boolvar/pb is an open source java library dedicated to the translation of pseudo-boolean constraints into cnf formulae . input constraints can be categorized with tags . several encoding schemes are implemented in a way that each input constraint can be translated using one or several encoders , according to the related tags . the library can be easily extended by adding new encoders and / or new output formats .", "topics": ["encoder"]}
{"title": "a spatial layout and scale invariant feature representation for indoor scene classification", "abstract": "unlike standard object classification , where the image to be classified contains one or multiple instances of the same object , indoor scene classification is quite different since the image consists of multiple distinct objects . further , these objects can be of varying sizes and are present across numerous spatial locations in different layouts . for automatic indoor scene categorization , large scale spatial layout deformations and scale variations are therefore two major challenges and the design of rich feature descriptors which are robust to these challenges is still an open problem . this paper introduces a new learnable feature descriptor called `` spatial layout and scale invariant convolutional activations '' to deal with these challenges . for this purpose , a new convolutional neural network architecture is designed which incorporates a novel 'spatially unstructured ' layer to introduce robustness against spatial layout deformations . to achieve scale invariance , we present a pyramidal image representation . for feasible training of the proposed network for images of indoor scenes , the paper proposes a new methodology which efficiently adapts a trained network model ( on a large scale data ) for our task with only a limited amount of available training data . compared with existing state of the art , the proposed approach achieves a relative performance improvement of 3.2 % , 3.8 % , 7.0 % , 11.9 % and 2.1 % on mit-67 , scene-15 , sports-8 , graz-02 and nyu datasets respectively .", "topics": ["test set"]}
{"title": "pomdps under probabilistic semantics", "abstract": "we consider partially observable markov decision processes ( pomdps ) with limit-average payoff , where a reward value in the interval [ 0,1 ] is associated to every transition , and the payoff of an infinite path is the long-run average of the rewards . we consider two types of path constraints : ( i ) quantitative constraint defines the set of paths where the payoff is at least a given threshold { \\lambda } in ( 0 , 1 ] ; and ( ii ) qualitative constraint which is a special case of quantitative constraint with { \\lambda } = 1 . we consider the computation of the almost-sure winning set , where the controller needs to ensure that the path constraint is satisfied with probability 1 . our main results for qualitative path constraint are as follows : ( i ) the problem of deciding the existence of a finite-memory controller is exptime-complete ; and ( ii ) the problem of deciding the existence of an infinite-memory controller is undecidable . for quantitative path constraint we show that the problem of deciding the existence of a finite-memory controller is undecidable .", "topics": ["computation"]}
{"title": "sparse auto-regressive : robust estimation of ar parameters", "abstract": "in this paper i present a new approach for regression of time series using their own samples . this is a celebrated problem known as auto-regression . dealing with outlier or missed samples in a time series makes the problem of estimation difficult , so it should be robust against them . moreover for coding purposes i will show that it is desired the residual of auto-regression be sparse . to these aims , i first assume a multivariate gaussian prior on the residual and then obtain the estimation . two simple simulations have been done on spectrum estimation and speech coding .", "topics": ["time series", "simulation"]}
{"title": "measuring the tendency of cnns to learn surface statistical regularities", "abstract": "deep cnns are known to exhibit the following peculiarity : on the one hand they generalize extremely well to a test set , while on the other hand they are extremely sensitive to so-called adversarial perturbations . the extreme sensitivity of high performance cnns to adversarial examples casts serious doubt that these networks are learning high level abstractions in the dataset . we are concerned with the following question : how can a deep cnn that does not learn any high level semantics of the dataset manage to generalize so well ? the goal of this article is to measure the tendency of cnns to learn surface statistical regularities of the dataset . to this end , we use fourier filtering to construct datasets which share the exact same high level abstractions but exhibit qualitatively different surface statistical regularities . for the svhn and cifar-10 datasets , we present two fourier filtered variants : a low frequency variant and a randomly filtered variant . each of the fourier filtering schemes is tuned to preserve the recognizability of the objects . our main finding is that cnns exhibit a tendency to latch onto the fourier image statistics of the training dataset , sometimes exhibiting up to a 28 % generalization gap across the various test sets . moreover , we observe that significantly increasing the depth of a network has a very marginal impact on closing the aforementioned generalization gap . thus we provide quantitative evidence supporting the hypothesis that deep cnns tend to learn surface statistical regularities in the dataset rather than higher-level abstract concepts .", "topics": ["test set"]}
{"title": "sequential person recognition in photo albums with a recurrent network", "abstract": "recognizing the identities of people in everyday photos is still a very challenging problem for machine vision , due to non-frontal faces , changes in clothing , location , lighting and similar . recent studies have shown that rich relational information between people in the same photo can help in recognizing their identities . in this work , we propose to model the relational information between people as a sequence prediction task . at the core of our work is a novel recurrent network architecture , in which relational information between instances ' labels and appearance are modeled jointly . in addition to relational cues , scene context is incorporated in our sequence prediction model with no additional cost . in this sense , our approach is a unified framework for modeling both contextual cues and visual appearance of person instances . our model is trained end-to-end with a sequence of annotated instances in a photo as inputs , and a sequence of corresponding labels as targets . we demonstrate that this simple but elegant formulation achieves state-of-the-art performance on the newly released people in photo albums ( pipa ) dataset .", "topics": ["recurrent neural network", "end-to-end principle"]}
{"title": "end-to-end learning of semantic grasping", "abstract": "we consider the task of semantic robotic grasping , in which a robot picks up an object of a user-specified class using only monocular images . inspired by the two-stream hypothesis of visual reasoning , we present a semantic grasping framework that learns object detection , classification , and grasp planning in an end-to-end fashion . a `` ventral stream '' recognizes object class while a `` dorsal stream '' simultaneously interprets the geometric relationships necessary to execute successful grasps . we leverage the autonomous data collection capabilities of robots to obtain a large self-supervised dataset for training the dorsal stream , and use semi-supervised label propagation to train the ventral stream with only a modest amount of human supervision . we experimentally show that our approach improves upon grasping systems whose components are not learned end-to-end , including a baseline method that uses bounding box detection . furthermore , we show that jointly training our model with auxiliary data consisting of non-semantic grasping data , as well as semantically labeled images without grasp actions , has the potential to substantially improve semantic grasping performance .", "topics": ["baseline ( configuration management )", "object detection"]}
{"title": "quantum neural machine learning - backpropagation and dynamics", "abstract": "the current work addresses quantum machine learning in the context of quantum artificial neural networks such that the networks ' processing is divided in two stages : the learning stage , where the network converges to a specific quantum circuit , and the backpropagation stage where the network effectively works as a self-programing quantum computing system that selects the quantum circuits to solve computing problems . the results are extended to general architectures including recurrent networks that interact with an environment , coupling with it in the neural links ' activation order , and self-organizing in a dynamical regime that intermixes patterns of dynamical stochasticity and persistent quasiperiodic dynamics , making emerge a form of noise resilient dynamical record .", "topics": ["neural networks"]}
{"title": "lcanet : end-to-end lipreading with cascaded attention-ctc", "abstract": "machine lipreading is a special type of automatic speech recognition ( asr ) which transcribes human speech by visually interpreting the movement of related face regions including lips , face , and tongue . recently , deep neural network based lipreading methods show great potential and have exceeded the accuracy of experienced human lipreaders in some benchmark datasets . however , lipreading is still far from being solved , and existing methods tend to have high error rates on the wild data . in this paper , we propose lcanet , an end-to-end deep neural network based lipreading system . lcanet encodes input video frames using a stacked 3d convolutional neural network ( cnn ) , highway network and bidirectional gru network . the encoder effectively captures both short-term and long-term spatio-temporal information . more importantly , lcanet incorporates a cascaded attention-ctc decoder to generate output texts . by cascading ctc with attention , it partially eliminates the defect of the conditional independence assumption of ctc within the hidden neural layers , and this yields notably performance improvement as well as faster convergence . the experimental results show the proposed system achieves a 1.3 % cer and 3.0 % wer on the grid corpus database , leading to a 12.3 % improvement compared to the state-of-the-art methods .", "topics": ["speech recognition", "speech recognition"]}
{"title": "do deep neural networks learn facial action units when doing expression recognition ?", "abstract": "despite being the appearance-based classifier of choice in recent years , relatively few works have examined how much convolutional neural networks ( cnns ) can improve performance on accepted expression recognition benchmarks and , more importantly , examine what it is they actually learn . in this work , not only do we show that cnns can achieve strong performance , but we also introduce an approach to decipher which portions of the face influence the cnn 's predictions . first , we train a zero-bias cnn on facial expression data and achieve , to our knowledge , state-of-the-art performance on two expression recognition benchmarks : the extended cohn-kanade ( ck+ ) dataset and the toronto face dataset ( tfd ) . we then qualitatively analyze the network by visualizing the spatial patterns that maximally excite different neurons in the convolutional layers and show how they resemble facial action units ( faus ) . finally , we use the fau labels provided in the ck+ dataset to verify that the faus observed in our filter visualizations indeed align with the subject 's facial movements .", "topics": ["neural networks"]}
{"title": "dynamic network models for forecasting", "abstract": "we have developed a probabilistic forecasting methodology through a synthesis of belief network models and classical time-series analysis . we present the dynamic network model ( dnm ) and describe methods for constructing , refining , and performing inference with this representation of temporal probabilistic knowledge . the dnm representation extends static belief-network models to more general dynamic forecasting models by integrating and iteratively refining contemporaneous and time-lagged dependencies . we discuss key concepts in terms of a model for forecasting u.s. car sales in japan .", "topics": ["time series", "bayesian network"]}
{"title": "cascaded scene flow prediction using semantic segmentation", "abstract": "given two consecutive frames from a pair of stereo cameras , 3d scene flow methods simultaneously estimate the 3d geometry and motion of the observed scene . many existing approaches use superpixels for regularization , but may predict inconsistent shapes and motions inside rigidly moving objects . we instead assume that scenes consist of foreground objects rigidly moving in front of a static background , and use semantic cues to produce pixel-accurate scene flow estimates . our cascaded classification framework accurately models 3d scenes by iteratively refining semantic segmentation masks , stereo correspondences , 3d rigid motion estimates , and optical flow fields . we evaluate our method on the challenging kitti autonomous driving benchmark , and show that accounting for the motion of segmented vehicles leads to state-of-the-art performance .", "topics": ["matrix regularization", "autonomous car"]}
{"title": "improving gravitational search algorithm performance with artificial bee colony algorithm for constrained numerical optimization", "abstract": "in this paper , we propose an improved gravitational search algorithm named gsabc . the algorithm improves gravitational search algorithm ( gsa ) results improved by using artificial bee colony algorithm ( abc ) to solve constrained numerical optimization problems . in gsa , solutions are attracted towards each other by applying gravitational forces , which depending on the masses assigned to the solutions , to each other . the heaviest mass will move slower than other masses and gravitate others . due to nature of gravitation , gsa may pass global minimum if some solutions stuck to local minimum . abc updates the positions of the best solutions that has obtained from gsa , preventing the gsa from sticking to the local minimum by its strong searching ability . the proposed algorithm improves the performance of gsa . the proposed method tested on 23 well-known unimodal , multimodal and fixed-point multimodal benchmark test functions . experimental results show that gsabc outperforms or performs similarly to five state-of-the-art optimization approaches .", "topics": ["mathematical optimization", "numerical analysis"]}
{"title": "a kernel method for canonical correlation analysis", "abstract": "canonical correlation analysis is a technique to extract common features from a pair of multivariate data . in complex situations , however , it does not extract useful features because of its linearity . on the other hand , kernel method used in support vector machine is an efficient approach to improve such a linear method . in this paper , we investigate the effectiveness of applying kernel method to canonical correlation analysis .", "topics": ["kernel ( operating system )", "support vector machine"]}
{"title": "fire detection in a still image using colour information", "abstract": "colour analysis is a crucial step in image-based fire detection algorithms . many of the proposed fire detection algorithms in a still image are prone to false alarms caused by objects with a colour similar to fire . to design a colour-based system with a better false alarm rate , a new colour-differentiating conversion matrix , efficient on images of high colour complexity , is proposed . the elements of this conversion matrix are obtained by performing k-medoids clustering and particle swarm optimisation procedures on a fire sample image with a background of high fire-colour similarity . the proposed conversion matrix is then used to construct two new fire colour detection frameworks . the first detection method is a two-stage non-linear image transformation framework , while the second is a direct transformation of an image with the proposed conversion matrix . a performance comparison of the proposed methods with alternate methods in the literature was carried out . experimental results indicate that the linear image transformation method outperforms other methods regarding false alarm rate while the non-linear two-stage image transformation method has the best performance on the f-score metric and provides a better trade-off between missed detection and false alarm rate .", "topics": ["cluster analysis", "nonlinear system"]}
{"title": "distributed dictionary learning over a sensor network", "abstract": "we consider the problem of distributed dictionary learning , where a set of nodes is required to collectively learn a common dictionary from noisy measurements . this approach may be useful in several contexts including sensor networks . diffusion cooperation schemes have been proposed to solve the distributed linear regression problem . in this work we focus on a diffusion-based adaptive dictionary learning strategy : each node records observations and cooperates with its neighbors by sharing its local dictionary . the resulting algorithm corresponds to a distributed block coordinate descent ( alternate optimization ) . beyond dictionary learning , this strategy could be adapted to many matrix factorization problems and generalized to various settings . this article presents our approach and illustrates its efficiency on some numerical examples .", "topics": ["numerical analysis", "reinforcement learning"]}
{"title": "smooth and sparse optimal transport", "abstract": "entropic regularization is quickly emerging as a new standard in optimal transport ( ot ) . it enables to cast the ot computation as a differentiable and unconstrained convex optimization problem , which can be efficiently solved using the sinkhorn algorithm . however , entropy keeps the transportation plan strictly positive and therefore completely dense , unlike unregularized ot . this lack of sparsity can be problematic in applications where the transportation plan itself is of interest . in this paper , we explore regularizing the primal and dual ot formulations with a strongly convex term , which corresponds to relaxing the dual and primal constraints with smooth approximations . we show how to incorporate squared $ 2 $ -norm and group lasso regularizations within that framework , leading to sparse and group-sparse transportation plans . on the theoretical side , we bound the approximation error introduced by regularizing the primal and dual formulations . our results suggest that , for the regularized primal , the approximation error can often be smaller with squared $ 2 $ -norm than with entropic regularization . we showcase our proposed framework on the task of color transfer .", "topics": ["approximation algorithm", "optimization problem"]}
{"title": "retrieval from captioned image databases using natural language processing", "abstract": "it might appear that natural language processing should improve the accuracy of information retrieval systems , by making available a more detailed analysis of queries and documents . although past results appear to show that this is not so , if the focus is shifted to short phrases rather than full documents , the situation becomes somewhat different . the anvil system uses a natural language technique to obtain high accuracy retrieval of images which have been annotated with a descriptive textual caption . the natural language techniques also allow additional contextual information to be derived from the relation between the query and the caption , which can help users to understand the overall collection of retrieval results . the techniques have been successfully used in a information retrieval system which forms both a testbed for research and the basis of a commercial system .", "topics": ["natural language processing", "natural language"]}
{"title": "image decomposition with anisotropic diffusion applied to leaf-texture analysis", "abstract": "texture analysis is an important field of investigation that has received a great deal of interest from computer vision community . in this paper , we propose a novel approach for texture modeling based on partial differential equation ( pde ) . each image $ f $ is decomposed into a family of derived sub-images . $ f $ is split into the $ u $ component , obtained with anisotropic diffusion , and the $ v $ component which is calculated by the difference between the original image and the $ u $ component . after enhancing the texture attribute $ v $ of the image , gabor features are computed as descriptors . we validate the proposed approach on two texture datasets with high variability . we also evaluate our approach on an important real-world application : leaf-texture analysis . experimental results indicate that our approach can be used to produce higher classification rates and can be successfully employed for different texture applications .", "topics": ["computer vision"]}
{"title": "regularizing recurrent networks - on injected noise and norm-based methods", "abstract": "advancements in parallel processing have lead to a surge in multilayer perceptrons ' ( mlp ) applications and deep learning in the past decades . recurrent neural networks ( rnns ) give additional representational power to feedforward mlps by providing a way to treat sequential data . however , rnns are hard to train using conventional error backpropagation methods because of the difficulty in relating inputs over many time-steps . regularization approaches from mlp sphere , like dropout and noisy weight training , have been insufficiently applied and tested on simple rnns . moreover , solutions have been proposed to improve convergence in rnns but not enough to improve the long term dependency remembering capabilities thereof . in this study , we aim to empirically evaluate the remembering and generalization ability of rnns on polyphonic musical datasets . the models are trained with injected noise , random dropout , norm-based regularizers and their respective performances compared to well-initialized plain rnns and advanced regularization methods like fast-dropout . we conclude with evidence that training with noise does not improve performance as conjectured by a few works in rnn optimization before ours .", "topics": ["recurrent neural network", "matrix regularization"]}
{"title": "enhanced lstm for natural language inference", "abstract": "reasoning and inference are central to human and artificial intelligence . modeling inference in human language is very challenging . with the availability of large annotated data ( bowman et al . , 2015 ) , it has recently become feasible to train neural network based inference models , which have shown to be very effective . in this paper , we present a new state-of-the-art result , achieving the accuracy of 88.6 % on the stanford natural language inference dataset . unlike the previous top models that use very complicated network architectures , we first demonstrate that carefully designing sequential inference models based on chain lstms can outperform all previous models . based on this , we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition , we achieve additional improvement . particularly , incorporating syntactic parsing information contributes to our best result -- -it further improves the performance even when added to the already very strong model .", "topics": ["natural language", "parsing"]}
{"title": "enriching frame representations with distributionally induced senses", "abstract": "we introduce a new lexical resource that enriches the framester knowledge graph , which links framnet , wordnet , verbnet and other resources , with semantic features from text corpora . these features are extracted from distributionally induced sense inventories and subsequently linked to the manually-constructed frame representations to boost the performance of frame disambiguation in context . since framester is a frame-based knowledge graph , which enables full-fledged owl querying and reasoning , our resource paves the way for the development of novel , deeper semantic-aware applications that could benefit from the combination of knowledge from text and complex symbolic representations of events and participants . together with the resource we also provide the software we developed for the evaluation in the task of word frame disambiguation ( wfd ) .", "topics": ["text corpus"]}
{"title": "performance bounds for lambda policy iteration and application to the game of tetris", "abstract": "we consider the discrete-time infinite-horizon optimal control problem formalized by markov decision processes . we revisit the work of bertsekas and ioffe , that introduced $ \\lambda $ policy iteration , a family of algorithms parameterized by $ \\lambda $ that generalizes the standard algorithms value iteration and policy iteration , and has some deep connections with the temporal differences algorithm td ( $ \\lambda $ ) described by sutton and barto . we deepen the original theory developped by the authors by providing convergence rate bounds which generalize standard bounds for value iteration described for instance by puterman . then , the main contribution of this paper is to develop the theory of this algorithm when it is used in an approximate form and show that this is sound . doing so , we extend and unify the separate analyses developped by munos for approximate value iteration and approximate policy iteration . eventually , we revisit the use of this algorithm in the training of a tetris playing controller as originally done by bertsekas and ioffe . we provide an original performance bound that can be applied to such an undiscounted control problem . our empirical results are different from those of bertsekas and ioffe ( which were originally qualified as `` paradoxical '' and `` intriguing '' ) , and much more conform to what one would expect from a learning experiment . we discuss the possible reason for such a difference .", "topics": ["approximation algorithm", "iteration"]}
{"title": "malware detection by eating a whole exe", "abstract": "in this work we introduce malware detection from raw byte sequences as a fruitful research area to the larger machine learning community . building a neural network for such a problem presents a number of interesting challenges that have not occurred in tasks such as image processing or nlp . in particular , we note that detection from raw bytes presents a sequence problem with over two million time steps and a problem where batch normalization appear to hinder the learning process . we present our initial work in building a solution to tackle this problem , which has linear complexity dependence on the sequence length , and allows for interpretable sub-regions of the binary to be identified . in doing so we will discuss the many challenges in building a neural network to process data at this scale , and the methods we used to work around them .", "topics": ["image processing", "natural language processing"]}
{"title": "construction of vietnamese sentiwordnet by using vietnamese dictionary", "abstract": "sentiwordnet is an important lexical resource supporting sentiment analysis in opinion mining applications . in this paper , we propose a novel approach to construct a vietnamese sentiwordnet ( vswn ) . sentiwordnet is typically generated from wordnet in which each synset has numerical scores to indicate its opinion polarities . many previous studies obtained these scores by applying a machine learning method to wordnet . however , vietnamese wordnet is not available unfortunately by the time of this paper . therefore , we propose a method to construct vswn from a vietnamese dictionary , not from wordnet . we show the effectiveness of the proposed method by generating a vswn with 39,561 synsets automatically . the method is experimentally tested with 266 synsets with aspect of positivity and negativity . it attains a competitive result compared with english sentiwordnet that is 0.066 and 0.052 differences for positivity and negativity sets respectively .", "topics": ["numerical analysis", "dictionary"]}
{"title": "consistent selection of tuning parameters via variable selection stability", "abstract": "penalized regression models are popularly used in high-dimensional data analysis to conduct variable selection and model fitting simultaneously . whereas success has been widely reported in literature , their performances largely depend on the tuning parameters that balance the trade-off between model fitting and model sparsity . existing tuning criteria mainly follow the route of minimizing the estimated prediction error or maximizing the posterior model probability , such as cross-validation , aic and bic . this article introduces a general tuning parameter selection criterion based on a novel concept of variable selection stability . the key idea is to select the tuning parameters so that the resultant penalized regression model is stable in variable selection . the asymptotic selection consistency is established for both fixed and diverging dimensions . the effectiveness of the proposed criterion is also demonstrated in a variety of simulated examples as well as an application to the prostate cancer data .", "topics": ["simulation", "sparse matrix"]}
{"title": "medical diagnosis as pattern recognition in a framework of information compression by multiple alignment , unification and search", "abstract": "this paper describes a novel approach to medical diagnosis based on the sp theory of computing and cognition . the main attractions of this approach are : a format for representing diseases that is simple and intuitive ; an ability to cope with errors and uncertainties in diagnostic information ; the simplicity of storing statistical information as frequencies of occurrence of diseases ; a method for evaluating alternative diagnostic hypotheses that yields true probabilities ; and a framework that should facilitate unsupervised learning of medical knowledge and the integration of medical diagnosis with other ai applications .", "topics": ["unsupervised learning"]}
{"title": "generative adversarial source separation", "abstract": "generative source separation methods such as non-negative matrix factorization ( nmf ) or auto-encoders , rely on the assumption of an output probability density . generative adversarial networks ( gans ) can learn data distributions without needing a parametric assumption on the output density . we show on a speech source separation experiment that , a multi-layer perceptron trained with a wasserstein-gan formulation outperforms nmf , auto-encoders trained with maximum likelihood , and variational auto-encoders in terms of source to distortion ratio .", "topics": ["calculus of variations", "encoder"]}
{"title": "invariants of objects and their images under surjective maps", "abstract": "we examine the relationships between the differential invariants of objects and of their images under a surjective map . we analyze both the case when the underlying transformation group is projectable and hence induces an action on the image , and the case when only a proper subgroup of the entire group acts projectably . in the former case , we establish a constructible isomorphism between the algebra of differential invariants of the images and the algebra of fiber-wise constant ( gauge ) differential invariants of the objects . in the latter case , we describe residual effects of the full transformation group on the image invariants . our motivation comes from the problem of reconstruction of an object from multiple-view images , with central and parallel projections of curves from three-dimensional space to the two-dimensional plane serving as our main examples .", "topics": ["map"]}
{"title": "krylov subspace recycling for fast iterative least-squares in machine learning", "abstract": "solving symmetric positive definite linear problems is a fundamental computational task in machine learning . the exact solution , famously , is cubicly expensive in the size of the matrix . to alleviate this problem , several linear-time approximations , such as spectral and inducing-point methods , have been suggested and are now in wide use . these are low-rank approximations that choose the low-rank space a priori and do not refine it over time . while this allows linear cost in the data-set size , it also causes a finite , uncorrected approximation error . authors from numerical linear algebra have explored ways to iteratively refine such low-rank approximations , at a cost of a small number of matrix-vector multiplications . this idea is particularly interesting in the many situations in machine learning where one has to solve a sequence of related symmetric positive definite linear problems . from the machine learning perspective , such deflation methods can be interpreted as transfer learning of a low-rank approximation across a time-series of numerical tasks . we study the use of such methods for our field . our empirical results show that , on regression and classification problems of intermediate size , this approach can interpolate between low computational cost and numerical precision .", "topics": ["time series", "time complexity"]}
{"title": "critical control of a genetic algorithm", "abstract": "based on speculations coming from statistical mechanics and the conjectured existence of critical states , i propose a simple heuristic in order to control the mutation probability and the population size of a genetic algorithm .", "topics": ["heuristic"]}
{"title": "population based training of neural networks", "abstract": "neural networks dominate the modern machine learning landscape , but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture , loss function , and optimisation algorithm . in this work we present \\emph { population based training ( pbt ) } , a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance . importantly , pbt discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training . with just a small modification to a typical distributed hyperparameter training framework , our method allows robust and reliable training of models . we demonstrate the effectiveness of pbt on deep reinforcement learning problems , showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters . in addition , we show the same method can be applied to supervised learning for machine translation , where pbt is used to maximise the bleu score directly , and also to training of generative adversarial networks to maximise the inception score of generated images . in all cases pbt results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance .", "topics": ["supervised learning", "mathematical optimization"]}
{"title": "complete end-to-end low cost solution to a 3d scanning system with integrated turntable", "abstract": "3d reconstruction is a technique used in computer vision which has a wide range of applications in areas like object recognition , city modelling , virtual reality , physical simulations , video games and special effects . previously , to perform a 3d reconstruction , specialized hardwares were required . such systems were often very expensive and was only available for industrial or research purpose . with the rise of the availability of high-quality low cost 3d sensors , it is now possible to design inexpensive complete 3d scanning systems . the objective of this work was to design an acquisition and processing system that can perform 3d scanning and reconstruction of objects seamlessly . in addition , the goal of this work also included making the 3d scanning process fully automated by building and integrating a turntable alongside the software . this means the user can perform a full 3d scan only by a press of a few buttons from our dedicated graphical user interface . three main steps were followed to go from acquisition of point clouds to the finished reconstructed 3d model . first , our system acquires point cloud data of a person/object using inexpensive camera sensor . second , align and convert the acquired point cloud data into a watertight mesh of good quality . third , export the reconstructed model to a 3d printer to obtain a proper 3d print of the model .", "topics": ["computer vision", "simulation"]}
{"title": "sparseness helps : sparsity augmented collaborative representation for classification", "abstract": "many classification approaches first represent a test sample using the training samples of all the classes . this collaborative representation is then used to label the test sample . it was a common belief that sparseness of the representation is the key to success for this classification scheme . however , more recently , it has been claimed that it is the collaboration and not the sparseness that makes the scheme effective . this claim is attractive as it allows to relinquish the computationally expensive sparsity constraint over the representation . in this paper , we first extend the analysis supporting this claim and then show that sparseness explicitly contributes to improved classification , hence it should not be completely ignored for computational gains . inspired by this result , we augment a dense collaborative representation with a sparse representation and propose an efficient classification method that capitalizes on the resulting representation . the augmented representation and the classification method work together meticulously to achieve higher accuracy and lower computational time compared to state-of-the-art collaborative representation based classification approaches . experiments on benchmark face , object and action databases show the efficacy of our approach .", "topics": ["statistical classification", "time complexity"]}
{"title": "stacked cross attention for image-text matching", "abstract": "in this paper , we study the problem of image-text matching . inferring the latent semantic alignment between objects or other salient stuffs ( e.g . snow , sky , lawn ) and the corresponding words in sentences allows to capture fine-grained interplay between vision and language , and makes image-text matching more interpretable . prior works either simply aggregate the similarity of all possible pairs of regions and words without attending differentially to more and less important words or regions , or use a multi-step attentional process to capture limited number of semantic alignments which is less interpretable . in this paper , we present stacked cross attention to discover the full latent alignments using both image regions and words in sentence as context and infer the image-text similarity . our approach achieves the state-of-the-art results on the ms-coco and flickr30k datasets . on flickr30k , our approach outperforms the current best methods by 22.1 % in text retrieval from image query , and 18.2 % in image retrieval with text query ( based on recall @ 1 ) . on ms-coco , our approach improves sentence retrieval by 17.8 % and image retrieval by 16.6 % ( based on recall @ 1 using the 5k test set ) .", "topics": ["test set"]}
{"title": "a comparative study of complexity of handwritten bharati characters with that of major indian scripts", "abstract": "we present bharati , a simple , novel script that can represent the characters of a majority of contemporary indian scripts . the shapes/motifs of bharati characters are drawn from some of the simplest characters of existing indian scripts . bharati characters are designed such that they strictly reflect the underlying phonetic organization , thereby attributing to the script qualities of simplicity , familiarity , ease of acquisition and use . thus , employing bharati script as a common script for a majority of indian languages can ameliorate several existing communication bottlenecks in india . we perform a complexity analysis of handwritten bharati script and compare its complexity with that of 9 major indian scripts . the measures of complexity are derived from a theory of handwritten characters based on catastrophe theory . bharati script is shown to be simpler than the 9 major indian scripts in most measures of complexity .", "topics": ["test set", "map"]}
{"title": "learning compatibility across categories for heterogeneous item recommendation", "abstract": "identifying relationships between items is a key task of an online recommender system , in order to help users discover items that are functionally complementary or visually compatible . in domains like clothing recommendation , this task is particularly challenging since a successful system should be capable of handling a large corpus of items , a huge amount of relationships among them , as well as the high-dimensional and semantically complicated features involved . furthermore , the human notion of `` compatibility '' to capture goes beyond mere similarity : for two items to be compatible -- -whether jeans and a t-shirt , or a laptop and a charger -- -they should be similar in some ways , but systematically different in others . in this paper we propose a novel method , monomer , to learn complicated and heterogeneous relationships between items in product recommendation settings . recently , scalable methods have been developed that address this task by learning similarity metrics on top of the content of the products involved . here our method relaxes the metricity assumption inherent in previous work and models multiple localized notions of 'relatedness , ' so as to uncover ways in which related items should be systematically similar , and systematically different . quantitatively , we show that our system achieves state-of-the-art performance on large-scale compatibility prediction tasks , especially in cases where there is substantial heterogeneity between related items . qualitatively , we demonstrate that richer notions of compatibility can be learned that go beyond similarity , and that our model can make effective recommendations of heterogeneous content .", "topics": ["text corpus", "scalability"]}
{"title": "policy distillation", "abstract": "policies for complex visual tasks have been successfully learned with deep reinforcement learning , using an approach called deep q-networks ( dqn ) , but relatively large ( task-specific ) networks and extensive training are needed to achieve good performance . in this work , we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient . furthermore , the same method can be used to consolidate multiple task-specific policies into a single policy . we demonstrate these claims using the atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained dqn agent .", "topics": ["reinforcement learning"]}
{"title": "creating scalable and interactive web applications using high performance latent variable models", "abstract": "in this project we outline a modularized , scalable system for comparing amazon products in an interactive and informative way using efficient latent variable models and dynamic visualization . we demonstrate how our system can build on the structure and rich review information of amazon products in order to provide a fast , multifaceted , and intuitive comparison . by providing a condensed per-topic comparison visualization to the user , we are able to display aggregate information from the entire set of reviews while providing an interface that is at least as compact as the `` most helpful reviews '' currently displayed by amazon , yet far more informative .", "topics": ["scalability"]}
{"title": "on the necessity of irrelevant variables", "abstract": "this work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers . the analysis uses the assumption that the variables are conditionally independent given the class , and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing . the main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant . we also show that accurate learning is possible even when there are so few examples that one can not determine with high confidence whether or not any individual variable is relevant .", "topics": ["relevance"]}
{"title": "integrating specialized classifiers based on continuous time markov chain", "abstract": "specialized classifiers , namely those dedicated to a subset of classes , are often adopted in real-world recognition systems . however , integrating such classifiers is nontrivial . existing methods , e.g . weighted average , usually implicitly assume that all constituents of an ensemble cover the same set of classes . such methods can produce misleading predictions when used to combine specialized classifiers . this work explores a novel approach . instead of combining predictions from individual classifiers directly , it first decomposes the predictions into sets of pairwise preferences , treating them as transition channels between classes , and thereon constructs a continuous-time markov chain , and use the equilibrium distribution of this chain as the final prediction . this way allows us to form a coherent picture over all specialized predictions . on large public datasets , the proposed method obtains considerable improvement compared to mainstream ensemble methods , especially when the classifier coverage is highly unbalanced .", "topics": ["statistical classification", "markov chain"]}
{"title": "tone biased mmr text summarization", "abstract": "text summarization is an interesting area for researchers to develop new techniques to provide human like summaries for vast amounts of information . summarization techniques tend to focus on providing accurate representation of content , and often the tone of the content is ignored . tone of the content sets a baseline for how a reader perceives the content . as such being able to generate summary with tone that is appropriate for the reader is important . in our work we implement maximal marginal relevance [ mmr ] based multi-document text summarization and propose a naive model to change tone of the summarization by setting a bias to specific set of words and restricting other words in the summarization output . this bias towards a specified set of words produces a summary whose tone is same as tone of specified words .", "topics": ["baseline ( configuration management )", "relevance"]}
{"title": "fast deterministic tourist walk for texture analysis", "abstract": "deterministic tourist walk ( dtw ) has attracted increasing interest in computer vision . in the last years , different methods for analysis of dynamic and static textures were proposed . so far , all works based on the dtw for texture analysis use all image pixels as initial point of a walk . however , this requires much runtime . in this paper , we conducted a study to verify the performance of the dtw method according to the number of initial points to start a walk . the proposed method assigns a unique code to each image pixel , then , the pixels whose code is not divisible by a given $ k $ value are ignored as initial points of walks . feature vectors were extracted and a classification process was performed for different percentages of initial points . experimental results on the brodatz and vistex datasets indicate that to use fewer pixels as initial points significantly improves the runtime compared to use all image pixels . in addition , the correct classification rate decreases very little .", "topics": ["computer vision", "pixel"]}
{"title": "end-to-end learning of lda by mirror-descent back propagation over a deep architecture", "abstract": "we develop a fully discriminative learning approach for supervised latent dirichlet allocation ( lda ) model using back propagation ( i.e . , bp-slda ) , which maximizes the posterior probability of the prediction variable given the input document . different from traditional variational learning or gibbs sampling approaches , the proposed learning method applies ( i ) the mirror descent algorithm for maximum a posterior inference and ( ii ) back propagation over a deep architecture together with stochastic gradient/mirror descent for model parameter estimation , leading to scalable and end-to-end discriminative learning of the model . as a byproduct , we also apply this technique to develop a new learning method for the traditional unsupervised lda model ( i.e . , bp-lda ) . experimental results on three real-world regression and classification tasks show that the proposed methods significantly outperform the previous supervised topic models , neural networks , and is on par with deep neural networks .", "topics": ["calculus of variations", "gradient"]}
{"title": "distributed learning for cooperative inference", "abstract": "we study the problem of cooperative inference where a group of agents interact over a network and seek to estimate a joint parameter that best explains a set of observations . agents do not know the network topology or the observations of other agents . we explore a variational interpretation of the bayesian posterior density , and its relation to the stochastic mirror descent algorithm , to propose a new distributed learning algorithm . we show that , under appropriate assumptions , the beliefs generated by the proposed algorithm concentrate around the true parameter exponentially fast . we provide explicit non-asymptotic bounds for the convergence rate . moreover , we develop explicit and computationally efficient algorithms for observation models belonging to exponential families .", "topics": ["calculus of variations", "computational complexity theory"]}
{"title": "modelling dependency completion in sentence comprehension as a bayesian hierarchical mixture process : a case study involving chinese relative clauses", "abstract": "we present a case-study demonstrating the usefulness of bayesian hierarchical mixture modelling for investigating cognitive processes . in sentence comprehension , it is widely assumed that the distance between linguistic co-dependents affects the latency of dependency resolution : the longer the distance , the longer the retrieval time ( the distance-based account ) . an alternative theory , direct-access , assumes that retrieval times are a mixture of two distributions : one distribution represents successful retrievals ( these are independent of dependency distance ) and the other represents an initial failure to retrieve the correct dependent , followed by a reanalysis that leads to successful retrieval . we implement both models as bayesian hierarchical models and show that the direct-access model explains chinese relative clause reading time data better than the distance account .", "topics": ["bayesian network"]}
{"title": "computational models of attention", "abstract": "this chapter reviews recent computational models of visual attention . we begin with models for the bottom-up or stimulus-driven guidance of attention to salient visual items , which we examine in seven different broad categories . we then examine more complex models which address the top-down or goal-oriented guidance of attention towards items that are more relevant to the task at hand .", "topics": ["high- and low-level", "parsing"]}
{"title": "causal bandits : learning good interventions via causal inference", "abstract": "we study the problem of using causal models to improve the rate at which good interventions can be learned online in a stochastic environment . our formalism combines multi-arm bandits and causal inference to model a novel type of bandit feedback that is not exploited by existing approaches . we propose a new algorithm that exploits the causal feedback and prove a bound on its simple regret that is strictly better ( in all quantities ) than algorithms that do not use the additional causal information .", "topics": ["regret ( decision theory )", "causality"]}
{"title": "unsupervised feature analysis with class margin optimization", "abstract": "unsupervised feature selection has been always attracting research attention in the communities of machine learning and data mining for decades . in this paper , we propose an unsupervised feature selection method seeking a feature coefficient matrix to select the most distinctive features . specifically , our proposed algorithm integrates the maximum margin criterion with a sparsity-based model into a joint framework , where the class margin and feature correlation are taken into account at the same time . to maximize the total data separability while preserving minimized within-class scatter simultaneously , we propose to embed kmeans into the framework generating pseudo class label information in a scenario of unsupervised feature selection . meanwhile , a sparsity-based model , ` 2 , p-norm , is imposed to the regularization term to effectively discover the sparse structures of the feature coefficient matrix . in this way , noisy and irrelevant features are removed by ruling out those features whose corresponding coefficients are zeros . to alleviate the local optimum problem that is caused by random initializations of k-means , a convergence guaranteed algorithm with an updating strategy for the clustering indicator matrix , is proposed to iteractively chase the optimal solution . performance evaluation is extensively conducted over six benchmark data sets . from plenty of experimental results , it is demonstrated that our method has superior performance against all other compared approaches .", "topics": ["cluster analysis", "data mining"]}
{"title": "reinforcement learning based on active learning method", "abstract": "in this paper , a new reinforcement learning approach is proposed which is based on a powerful concept named active learning method ( alm ) in modeling . alm expresses any multi-input-single-output system as a fuzzy combination of some single-input-singleoutput systems . the proposed method is an actor-critic system similar to generalized approximate reasoning based intelligent control ( garic ) structure to adapt the alm by delayed reinforcement signals . our system uses temporal difference ( td ) learning to model the behavior of useful actions of a control system . the goodness of an action is modeled on reward- penalty-plane . ids planes will be updated according to this plane . it is shown that the system can learn with a predefined fuzzy system or without it ( through random actions ) .", "topics": ["reinforcement learning"]}
{"title": "cascade one-vs-rest detection network for fine-grained recognition without part annotations", "abstract": "fine-grained recognition is a challenging task due to the small intra-category variances . most of top-performing fine-grained recognition methods leverage parts of objects for better performance . therefore , part annotations which are extremely computationally expensive are required . in this paper , we propose a novel cascaded deep cnn detection framework for fine-grained recognition which is trained to detect the whole object without considering parts . nevertheless , most of current top-performing detection networks use the n+1 class ( n object categories plus background ) softmax loss , and the background category with much more training samples dominates the feature learning progress so that the features are not good for object categories with fewer samples . to bridge this gap , we introduce a cascaded structure to eliminate background and exploit a one-vs-rest loss to capture more minute variances among different subordinate categories . experiments show that our proposed recognition framework achieves comparable performance with state-of-the-art , part-free , fine-grained recognition methods on the cub-200-2011 bird dataset . moreover , our method even outperforms most of part-based methods while does not need part annotations at the training stage and is free from any annotations at test stage .", "topics": ["feature learning"]}
{"title": "active regression with adaptive huber loss", "abstract": "this paper addresses the scalar regression problem through a novel solution to exactly optimize the huber loss in a general semi-supervised setting , which combines multi-view learning and manifold regularization . we propose a principled algorithm to 1 ) avoid computationally expensive iterative schemes while 2 ) adapting the huber loss threshold in a data-driven fashion and 3 ) actively balancing the use of labelled data to remove noisy or inconsistent annotations at the training stage . in a wide experimental evaluation , dealing with diverse applications , we assess the superiority of our paradigm which is able to combine robustness towards noise with both strong performance and low computational cost .", "topics": ["matrix regularization"]}
{"title": "detailed garment recovery from a single-view image", "abstract": "most recent garment capturing techniques rely on acquiring multiple views of clothing , which may not always be readily available , especially in the case of pre-existing photographs from the web . as an alternative , we pro- pose a method that is able to compute a rich and realistic 3d model of a human body and its outfits from a single photograph with little human in- teraction . our algorithm is not only able to capture the global shape and geometry of the clothing , it can also extract small but important details of cloth , such as occluded wrinkles and folds . unlike previous methods using full 3d information ( i.e . depth , multi-view images , or sampled 3d geom- etry ) , our approach achieves detailed garment recovery from a single-view image by using statistical , geometric , and physical priors and a combina- tion of parameter estimation , semantic parsing , shape recovery , and physics- based cloth simulation . we demonstrate the effectiveness of our algorithm by re-purposing the reconstructed garments for virtual try-on and garment transfer applications , as well as cloth animation for digital characters .", "topics": ["simulation", "parsing"]}
{"title": "non deterministic logic programs", "abstract": "non deterministic applications arise in many domains , including , stochastic optimization , multi-objectives optimization , stochastic planning , contingent stochastic planning , reinforcement learning , reinforcement learning in partially observable markov decision processes , and conditional planning . we present a logic programming framework called non deterministic logic programs , along with a declarative semantics and fixpoint semantics , to allow representing and reasoning about inherently non deterministic real-world applications . the language of non deterministic logic programs framework is extended with non-monotonic negation , and two alternative semantics are defined : the stable non deterministic model semantics and the well-founded non deterministic model semantics as well as their relationship is studied . these semantics subsume the deterministic stable model semantics and the deterministic well-founded semantics of deterministic normal logic programs , and they reduce to the semantics of deterministic definite logic programs without negation . we show the application of the non deterministic logic programs framework to a conditional planning problem .", "topics": ["reinforcement learning", "markov chain"]}
{"title": "rectified factor networks", "abstract": "we propose rectified factor networks ( rfns ) to efficiently construct very sparse , non-linear , high-dimensional representations of the input . rfn models identify rare and small events in the input , have a low interference between code units , have a small reconstruction error , and explain the data covariance structure . rfn learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means . we proof convergence and correctness of the rfn learning algorithm . on benchmarks , rfns are compared to other unsupervised methods like autoencoders , rbms , factor analysis , ica , and pca . in contrast to previous sparse coding methods , rfns yield sparser codes , capture the data 's covariance structure more precisely , and have a significantly smaller reconstruction error . we test rfns as pretraining technique for deep networks on different vision datasets , where rfns were superior to rbms and autoencoders . on gene expression data from two pharmaceutical drug discovery studies , rfns detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods .", "topics": ["calculus of variations", "noise reduction"]}
{"title": "ep-gig priors and applications in bayesian sparse learning", "abstract": "in this paper we propose a novel framework for the construction of sparsity-inducing priors . in particular , we define such priors as a mixture of exponential power distributions with a generalized inverse gaussian density ( ep-gig ) . ep-gig is a variant of generalized hyperbolic distributions , and the special cases include gaussian scale mixtures and laplace scale mixtures . furthermore , laplace scale mixtures can subserve a bayesian framework for sparse learning with nonconvex penalization . the densities of ep-gig can be explicitly expressed . moreover , the corresponding posterior distribution also follows a generalized inverse gaussian distribution . these properties lead us to em algorithms for bayesian sparse learning . we show that these algorithms bear an interesting resemblance to iteratively re-weighted $ \\ell_2 $ or $ \\ell_1 $ methods . in addition , we present two extensions for grouped variable selection and logistic regression .", "topics": ["time complexity", "sparse matrix"]}
{"title": "convolutional sketch inversion", "abstract": "in this paper , we use deep neural networks for inverting face sketches to synthesize photorealistic face images . we first construct a semi-simulated dataset containing a very large number of computer-generated face sketches with different styles and corresponding face images by expanding existing unconstrained face data sets . we then train models achieving state-of-the-art results on both computer-generated sketches and hand-drawn sketches by leveraging recent advances in deep learning such as batch normalization , deep residual learning , perceptual losses and stochastic optimization in combination with our new dataset . we finally demonstrate potential applications of our models in fine arts and forensic arts . in contrast to existing patch-based approaches , our deep-neural-network-based approach can be used for synthesizing photorealistic face images by inverting face sketches in the wild .", "topics": ["simulation"]}
{"title": "stochastic weighted function norm regularization", "abstract": "deep neural networks ( dnns ) have become increasingly important due to their excellent empirical performance on a wide range of problems . however , regularization is generally achieved by indirect means , largely due to the complex set of functions defined by a network and the difficulty in measuring function complexity . there exists no method in the literature for additive regularization based on a norm of the function , as is classically considered in statistical learning theory . in this work , we propose sampling-based approximations to weighted function norms as regularizers for deep neural networks . we provide , to the best of our knowledge , the first proof in the literature of the np-hardness of computing function norms of dnns , motivating the necessity of a stochastic optimization strategy . based on our proposed regularization scheme , stability-based bounds yield a $ \\mathcal { o } ( n^ { -\\frac { 1 } { 2 } } ) $ generalization error for our proposed regularizer when applied to convex function sets . we demonstrate broad conditions for the convergence of stochastic gradient descent on our objective , including for non-convex function sets such as those defined by dnns . finally , we empirically validate the improved performance of the proposed regularization strategy for both convex function sets as well as dnns on real-world classification and segmentation tasks .", "topics": ["sampling ( signal processing )", "matrix regularization"]}
{"title": "loopy belief propagation for approximate inference : an empirical study", "abstract": "recently , researchers have demonstrated that loopy belief propagation - the use of pearls polytree algorithm in a bayesian network with loops of error- correcting codes.the most dramatic instance of this is the near shannon - limit performance of turbo codes codes whose decoding algorithm is equivalent to loopy belief propagation in a chain - structured bayesian network . in this paper we ask : is there something special about the error - correcting code context , or does loopy propagation work as an approximate inference schemein a more general setting ? we compare the marginals computed using loopy propagation to the exact ones in four bayesian network architectures , including two real - world networks : alarm and qmr.we find that the loopy beliefs often converge and when they do , they give a good approximation to the correct marginals.however , on the qmr network , the loopy beliefs oscillated and had no obvious relationship to the correct posteriors . we present some initial investigations into the cause of these oscillations , and show that some simple methods of preventing them lead to the wrong results .", "topics": ["approximation", "bayesian network"]}
{"title": "enhancing volumetric bouligand-minkowski fractal descriptors by using functional data analysis", "abstract": "this work proposes and study the concept of functional data analysis transform , applying it to the performance improving of volumetric bouligand-minkowski fractal descriptors . the proposed transform consists essentially in changing the descriptors originally defined in the space of the calculus of fractal dimension into the space of coefficients used in the functional data representation of these descriptors . the transformed decriptors are used here in texture classification problems . the enhancement provided by the fda transform is measured by comparing the transformed to the original descriptors in terms of the correctness rate in the classification of well known datasets .", "topics": ["coefficient"]}
{"title": "toward scalable verification for safety-critical deep networks", "abstract": "the increasing use of deep neural networks for safety-critical applications , such as autonomous driving and flight control , raises concerns about their safety and reliability . formal verification can address these concerns by guaranteeing that a deep learning system operates as intended , but the state of the art is limited to small systems . in this work-in-progress report we give an overview of our work on mitigating this difficulty , by pursuing two complementary directions : devising scalable verification techniques , and identifying design choices that result in deep learning systems that are more amenable to verification .", "topics": ["scalability", "autonomous car"]}
{"title": "a grassmannian graph approach to affine invariant feature matching", "abstract": "in this work , we present a novel and practical approach to address one of the longstanding problems in computer vision : 2d and 3d affine invariant feature matching . our grassmannian graph ( grassgraph ) framework employs a two stage procedure that is capable of robustly recovering correspondences between two unorganized , affinely related feature ( point ) sets . the first stage maps the feature sets to an affine invariant grassmannian representation , where the features are mapped into the same subspace . it turns out that coordinate representations extracted from the grassmannian differ by an arbitrary orthonormal matrix . in the second stage , by approximating the laplace-beltrami operator ( lbo ) on these coordinates , this extra orthonormal factor is nullified , providing true affine-invariant coordinates which we then utilize to recover correspondences via simple nearest neighbor relations . the resulting grassgraph algorithm is empirically shown to work well in non-ideal scenarios with noise , outliers , and occlusions . our validation benchmarks use an unprecedented 440,000+ experimental trials performed on 2d and 3d datasets , with a variety of parameter settings and competing methods . state-of-the-art performance in the majority of these extensive evaluations confirm the utility of our method .", "topics": ["approximation algorithm", "computer vision"]}
{"title": "quantized neural network design under weight capacity constraint", "abstract": "the complexity of deep neural network algorithms for hardware implementation can be lowered either by scaling the number of units or reducing the word-length of weights . both approaches , however , can accompany the performance degradation although many types of research are conducted to relieve this problem . thus , it is an important question which one , between the network size scaling and the weight quantization , is more effective for hardware optimization . for this study , the performances of fully-connected deep neural networks ( fcdnns ) and convolutional neural networks ( cnns ) are evaluated while changing the network complexity and the word-length of weights . based on these experiments , we present the effective compression ratio ( ecr ) to guide the trade-off between the network size and the precision of weights when the hardware resource is limited .", "topics": ["scalability"]}
{"title": "relative upper confidence bound for the k-armed dueling bandit problem", "abstract": "this paper proposes a new method for the k-armed dueling bandit problem , a variation on the regular k-armed bandit problem that offers only relative feedback about pairs of arms . our approach extends the upper confidence bound algorithm to the relative setting by using estimates of the pairwise probabilities to select a promising arm and applying upper confidence bound with the winner as a benchmark . we prove a finite-time regret bound of order o ( log t ) . in addition , our empirical results using real data from an information retrieval application show that it greatly outperforms the state of the art .", "topics": ["regret ( decision theory )"]}
{"title": "mtim : rapid and accurate transcript reconstruction from rna-seq data", "abstract": "recent advances in high-throughput cdna sequencing ( rna-seq ) technology have revolutionized transcriptome studies . a major motivation for rna-seq is to map the structure of expressed transcripts at nucleotide resolution . with accurate computational tools for transcript reconstruction , this technology may also become useful for genome ( re- ) annotation , which has mostly relied on de novo gene finding where gene structures are primarily inferred from the genome sequence . we developed a machine-learning method , called mtim ( margin-based transcript inference method ) for transcript reconstruction from rna-seq read alignments that is based on discriminatively trained hidden markov support vector machines . in addition to features derived from read alignments , it utilizes characteristic genomic sequences , e.g . around splice sites , to improve transcript predictions . mtim inferred transcripts that were highly accurate and relatively robust to alignment errors in comparison to those from cufflinks , a widely used transcript assembly method .", "topics": ["support vector machine", "support vector machine"]}
{"title": "applying mdl to learning best model granularity", "abstract": "the minimum description length ( mdl ) principle is solidly based on a provably ideal method of inference using kolmogorov complexity . we test how the theory behaves in practice on a general problem in model selection : that of learning the best model granularity . the performance of a model depends critically on the granularity , for example the choice of precision of the parameters . too high precision generally involves modeling of accidental noise and too low precision may lead to confusion of models that should be distinguished . this precision is often determined ad hoc . in mdl the best model is the one that most compresses a two-part code of the data set : this embodies `` occam 's razor . '' in two quite different experimental settings the theoretical value determined using mdl coincides with the best value found experimentally . in the first experiment the task is to recognize isolated handwritten characters in one subject 's handwriting , irrespective of size and orientation . based on a new modification of elastic matching , using multiple prototypes per character , the optimal prediction rate is predicted for the learned parameter ( length of sampling interval ) considered most likely by mdl , which is shown to coincide with the best value found experimentally . in the second experiment the task is to model a robot arm with two degrees of freedom using a three layer feed-forward neural network where we need to determine the number of nodes in the hidden layer giving best modeling performance . the optimal model ( the one that extrapolizes best on unseen examples ) is predicted for the number of nodes in the hidden layer considered most likely by mdl , which again is found to coincide with the best value found experimentally .", "topics": ["sampling ( signal processing )"]}
{"title": "cvpaper.challenge in 2015 - a review of cvpr2015 and deepsurvey", "abstract": "the `` cvpaper.challenge '' is a group composed of members from aist , tokyo denki univ . ( tdu ) , and univ . of tsukuba that aims to systematically summarize papers on computer vision , pattern recognition , and related fields . for this particular review , we focused on reading the all 602 conference papers presented at the cvpr2015 , the premier annual computer vision event held in june 2015 , in order to grasp the trends in the field . further , we are proposing `` deepsurvey '' as a mechanism embodying the entire process from the reading through all the papers , the generation of ideas , and to the writing of paper .", "topics": ["computer vision"]}
{"title": "the perceptron with dynamic margin", "abstract": "the classical perceptron rule provides a varying upper bound on the maximum margin , namely the length of the current weight vector divided by the total number of updates up to that time . requiring that the perceptron updates its internal state whenever the normalized margin of a pattern is found not to exceed a certain fraction of this dynamic upper bound we construct a new approximate maximum margin classifier called the perceptron with dynamic margin ( pdm ) . we demonstrate that pdm converges in a finite number of steps and derive an upper bound on them . we also compare experimentally pdm with other perceptron-like algorithms and support vector machines on hard margin tasks involving linear kernels which are equivalent to 2-norm soft margin .", "topics": ["approximation algorithm", "support vector machine"]}
{"title": "magnetic hamiltonian monte carlo", "abstract": "hamiltonian monte carlo ( hmc ) exploits hamiltonian dynamics to construct efficient proposals for markov chain monte carlo ( mcmc ) . in this paper , we present a generalization of hmc which exploits \\textit { non-canonical } hamiltonian dynamics . we refer to this algorithm as magnetic hmc , since in 3 dimensions a subset of the dynamics map onto the mechanics of a charged particle coupled to a magnetic field . we establish a theoretical basis for the use of non-canonical hamiltonian dynamics in mcmc , and construct a symplectic , leapfrog-like integrator allowing for the implementation of magnetic hmc . finally , we exhibit several examples where these non-canonical dynamics can lead to improved mixing of magnetic hmc relative to ordinary hmc .", "topics": ["markov chain"]}
{"title": "programming in logic without logic programming", "abstract": "in previous work , we proposed a logic-based framework in which computation is the execution of actions in an attempt to make reactive rules of the form if antecedent then consequent true in a canonical model of a logic program determined by an initial state , sequence of events , and the resulting sequence of subsequent states . in this model-theoretic semantics , reactive rules are the driving force , and logic programs play only a supporting role . in the canonical model , states , actions and other events are represented with timestamps . but in the operational semantics , for the sake of efficiency , timestamps are omitted and only the current state is maintained . state transitions are performed reactively by executing actions to make the consequents of rules true whenever the antecedents become true . this operational semantics is sound , but incomplete . it can not make reactive rules true by preventing their antecedents from becoming true , or by proactively making their consequents true before their antecedents become true . in this paper , we characterize the notion of reactive model , and prove that the operational semantics can generate all and only such models . in order to focus on the main issues , we omit the logic programming component of the framework .", "topics": ["computation"]}
{"title": "dropout training of matrix factorization and autoencoder for link prediction in sparse graphs", "abstract": "matrix factorization ( mf ) and autoencoder ( ae ) are among the most successful approaches of unsupervised learning . while mf based models have been extensively exploited in the graph modeling and link prediction literature , the ae family has not gained much attention . in this paper we investigate both mf and ae 's application to the link prediction problem in sparse graphs . we show the connection between ae and mf from the perspective of multiview learning , and further propose mf+ae : a model training mf and ae jointly with shared parameters . we apply dropout to training both the mf and ae parts , and show that it can significantly prevent overfitting by acting as an adaptive regularization . we conduct experiments on six real world sparse graph datasets , and show that mf+ae consistently outperforms the competing methods , especially on datasets that demonstrate strong non-cohesive structures .", "topics": ["unsupervised learning", "matrix regularization"]}
{"title": "gncgcp - graduated nonconvexity and graduated concavity procedure", "abstract": "in this paper we propose the graduated nonconvexity and graduated concavity procedure ( gncgcp ) as a general optimization framework to approximately solve the combinatorial optimization problems on the set of partial permutation matrices . gncgcp comprises two sub-procedures , graduated nonconvexity ( gnc ) which realizes a convex relaxation and graduated concavity ( gc ) which realizes a concave relaxation . it is proved that gncgcp realizes exactly a type of convex-concave relaxation procedure ( ccrp ) , but with a much simpler formulation without needing convex or concave relaxation in an explicit way . actually , gncgcp involves only the gradient of the objective function and is therefore very easy to use in practical applications . two typical np-hard problems , ( sub ) graph matching and quadratic assignment problem ( qap ) , are employed to demonstrate its simplicity and state-of-the-art performance .", "topics": ["loss function", "gradient"]}
{"title": "recurrent neural network attention mechanisms for interpretable system log anomaly detection", "abstract": "deep learning has recently demonstrated state-of-the art performance on key tasks related to the maintenance of computer systems , such as intrusion detection , denial of service attack detection , hardware and software system failures , and malware detection . in these contexts , model interpretability is vital for administrator and analyst to trust and act on the automated analysis of machine learning models . deep learning methods have been criticized as black box oracles which allow limited insight into decision factors . in this work we seek to `` bridge the gap '' between the impressive performance of deep learning models and the need for interpretable model introspection . to this end we present recurrent neural network ( rnn ) language models augmented with attention for anomaly detection in system logs . our methods are generally applicable to any computer system and logging source . by incorporating attention variants into our rnn language models we create opportunities for model introspection and analysis without sacrificing state-of-the art performance . we demonstrate model performance and illustrate model interpretability on an intrusion detection task using the los alamos national laboratory ( lanl ) cyber security dataset , reporting upward of 0.99 area under the receiver operator characteristic curve despite being trained only on a single day 's worth of data .", "topics": ["unsupervised learning", "recurrent neural network"]}
{"title": "machine learning on images using a string-distance", "abstract": "we present a new method for image feature-extraction which is based on representing an image by a finite-dimensional vector of distances that measure how different the image is from a set of image prototypes . we use the recently introduced universal image distance ( uid ) \\cite { ratsabychesterieee2012 } to compare the similarity between an image and a prototype image . the advantage in using the uid is the fact that no domain knowledge nor any image analysis need to be done . each image is represented by a finite dimensional feature vector whose components are the uid values between the image and a finite set of image prototypes from each of the feature categories . the method is automatic since once the user selects the prototype images , the feature vectors are automatically calculated without the need to do any image analysis . the prototype images can be of different size , in particular , different than the image size . based on a collection of such cases any supervised or unsupervised learning algorithm can be used to train and produce an image classifier or image cluster analysis . in this paper we present the image feature-extraction method and use it on several supervised and unsupervised learning experiments for satellite image data .", "topics": ["feature vector", "cluster analysis"]}
{"title": "evaluation of the performance of the markov blanket bayesian classifier algorithm", "abstract": "the markov blanket bayesian classifier is a recently-proposed algorithm for construction of probabilistic classifiers . this paper presents an empirical comparison of the mbbc algorithm with three other bayesian classifiers : naive bayes , tree-augmented naive bayes and a general bayesian network . all of these are implemented using the k2 framework of cooper and herskovits . the classifiers are compared in terms of their performance ( using simple accuracy measures and roc curves ) and speed , on a range of standard benchmark data sets . it is concluded that mbbc is competitive in terms of speed and accuracy with the other algorithms considered .", "topics": ["bayesian network"]}
{"title": "a survey on two dimensional cellular automata and its application in image processing", "abstract": "parallel algorithms for solving any image processing task is a highly demanded approach in the modern world . cellular automata ( ca ) are the most common and simple models of parallel computation . so , ca has been successfully used in the domain of image processing for the last couple of years . this paper provides a survey of available literatures of some methodologies employed by different researchers to utilize the cellular automata for solving some important problems of image processing . the survey includes some important image processing tasks such as rotation , zooming , translation , segmentation , edge detection , compression and noise reduction of images . finally , the experimental results of some methodologies are presented .", "topics": ["image processing", "image segmentation"]}
{"title": "modular action language alm", "abstract": "the paper introduces a new modular action language , alm , and illustrates the methodology of its use . it is based on the approach of gelfond and lifschitz ( 1993 ; 1998 ) in which a high-level action language is used as a front end for a logic programming system description . the resulting logic programming representation is used to perform various computational tasks . the methodology based on existing action languages works well for small and even medium size systems , but is not meant to deal with larger systems that require structuring of knowledge . alm is meant to remedy this problem . structuring of knowledge in alm is supported by the concepts of module ( a formal description of a specific piece of knowledge packaged as a unit ) , module hierarchy , and library , and by the division of a system description of alm into two parts : theory and structure . a theory consists of one or more modules with a common theme , possibly organized into a module hierarchy based on a dependency relation . it contains declarations of sorts , attributes , and properties of the domain together with axioms describing them . structures are used to describe the domain 's objects . these features , together with the means for defining classes of a domain as special cases of previously defined ones , facilitate the stepwise development , testing , and readability of a knowledge base , as well as the creation of knowledge representation libraries . to appear in theory and practice of logic programming ( tplp ) .", "topics": ["high- and low-level"]}
{"title": "how does knowledge of the auc constrain the set of possible ground-truth labelings ?", "abstract": "recent work on privacy-preserving machine learning has considered how data-mining competitions such as kaggle could potentially be `` hacked '' , either intentionally or inadvertently , by using information from an oracle that reports a classifier 's accuracy on the test set . for binary classification tasks in particular , one of the most common accuracy metrics is the area under the roc curve ( auc ) , and in this paper we explore the mathematical structure of how the auc is computed from an n-vector of real-valued `` guesses '' with respect to the ground-truth labels . we show how knowledge of a classifier 's auc on the test set can constrain the set of possible ground-truth labelings , and we derive an algorithm both to compute the exact number of such labelings and to enumerate efficiently over them . finally , we provide empirical evidence that , surprisingly , the number of compatible labelings can actually decrease as n grows , until a test set-dependent threshold is reached .", "topics": ["test set", "data mining"]}
{"title": "neurogenesis-inspired dictionary learning : online model adaption in a changing world", "abstract": "in this paper , we focus on online representation learning in non-stationary environments which may require continuous adaptation of model architecture . we propose a novel online dictionary-learning ( sparse-coding ) framework which incorporates the addition and deletion of hidden units ( dictionary elements ) , and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus , known to be associated with improved cognitive function and adaptation to new environments . in the online learning setting , where new input instances arrive sequentially in batches , the neuronal-birth is implemented by adding new units with random initial weights ( random dictionary elements ) ; the number of new units is determined by the current performance ( representation error ) of the dictionary , higher error causing an increase in the birth rate . neuronal-death is implemented by imposing l1/l2-regularization ( group sparsity ) on the dictionary within the block-coordinate descent optimization at each iteration of our online alternating minimization scheme , which iterates between the code and dictionary updates . finally , hidden unit connectivity adaptation is facilitated by introducing sparsity in dictionary elements . our empirical evaluation on several real-life datasets ( images and language ) as well as on synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art fixed-size ( nonadaptive ) online sparse coding of mairal et al . ( 2009 ) in the presence of nonstationary data . moreover , we identify certain properties of the data ( e.g . , sparse inputs with nearly non-overlapping supports ) and of the model ( e.g . , dictionary sparsity ) associated with such improvements .", "topics": ["feature learning", "synthetic data"]}
{"title": "phase transitions , optimal errors and optimality of message-passing in generalized linear models", "abstract": "we consider generalized linear models ( glms ) where an unknown $ n $ -dimensional signal vector is observed through the application of a random matrix and a non-linear ( possibly probabilistic ) componentwise output function . we consider the models in the high-dimensional limit , where the observation consists of m points , and $ m/n\\to\\alpha $ where $ \\alpha $ stays finite in the limit $ m , n\\to\\infty $ . this situation is ubiquitous in applications ranging from supervised machine learning to signal processing . a substantial amount of theoretical work analyzed the model-case when the observation matrix has i.i.d . elements and the components of the ground-truth signal are taken independently from some known distribution . while statistical physics provided number of explicit conjectures for special cases of this model , results existing for non-linear output functions were so far non-rigorous . at the same time glms with non-linear output functions are used as a basic building block of powerful multilayer feedforward neural networks . therefore rigorously establishing the formulas conjectured for the mutual information is a key open problem that we solve in this paper . we also provide an explicit asymptotic formula for the optimal generalization error , and confirm the prediction of phase transitions in glms . analyzing the resulting formulas for several non-linear output functions , including the rectified linear unit or modulus functions , we obtain quantitative descriptions of information-theoretic limitations of high-dimensional inference . our proof technique relies on a new version of the interpolation method with an adaptive interpolation path and is of independent interest . furthermore we show that a polynomial-time algorithm referred to as generalized approximate message-passing reaches the optimal generalization error for a large set of parameters .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "correlated action effects in decision theoretic regression", "abstract": "much recent research in decision theoretic planning has adopted markov decision processes ( mdps ) as the model of choice , and has attempted to make their solution more tractable by exploiting problem structure . one particular algorithm , structured policy construction achieves this by means of a decision theoretic analog of goal regression using action descriptions based on bayesian networks with tree-structured conditional probability tables . the algorithm as presented is not able to deal with actions with correlated effects . we describe a new decision theoretic regression operator that corrects this weakness . while conceptually straightforward , this extension requires a somewhat more complicated technical approach .", "topics": ["bayesian network"]}
{"title": "composition-aided sketch-realistic portrait generation", "abstract": "sketch portrait generation is of wide applications including digital entertainment and law enforcement . despite the great progress achieved by existing face sketch generation methods , they mostly yield blurred effects and great deformation over various facial parts . in order to tackle this challenge , we propose a novel composition-aided generative adversarial network ( ca-gan ) for sketch portrait generation . first , we utilize paired inputs including a face photo and the corresponding pixel-wise face labels for generating the portrait . second , we propose an improved pixel loss , termed compositional loss , to focus training on hard-generated components and delicate facial structures . moreover , we use stacked ca-gans ( stack-ca-gan ) to further rectify defects and add compelling details . experimental results show that our method is capable of generating identity-preserving , sketch-realistic , and visually comfortable sketch portraits over a wide range of challenging data , and outperforms existing methods . besides , our methods show considerable generalization ability .", "topics": ["pixel"]}
{"title": "towards stratification learning through homology inference", "abstract": "a topological approach to stratification learning is developed for point cloud data drawn from a stratified space . given such data , our objective is to infer which points belong to the same strata . first we define a multi-scale notion of a stratified space , giving a stratification for each radius level . we then use methods derived from kernel and cokernel persistent homology to cluster the data points into different strata , and we prove a result which guarantees the correctness of our clustering , given certain topological conditions ; some geometric intuition for these topological conditions is also provided . our correctness result is then given a probabilistic flavor : we give bounds on the minimum number of sample points required to infer , with probability , which points belong to the same strata . finally , we give an explicit algorithm for the clustering , prove its correctness , and apply it to some simulated data .", "topics": ["cluster analysis", "simulation"]}
{"title": "a knowledge representation meta-model for rule-based modelling of signalling networks", "abstract": "the study of cellular signalling pathways and their deregulation in disease states , such as cancer , is a large and extremely complex task . indeed , these systems involve many parts and processes but are studied piecewise and their literatures and data are consequently fragmented , distributed and sometimes -- at least apparently -- inconsistent . this makes it extremely difficult to build significant explanatory models with the result that effects in these systems that are brought about by many interacting factors are poorly understood . the rule-based approach to modelling has shown some promise for the representation of the highly combinatorial systems typically found in signalling where many of the proteins are composed of multiple binding domains , capable of simultaneous interactions , and/or peptide motifs controlled by post-translational modifications . however , the rule-based approach requires highly detailed information about the precise conditions for each and every interaction which is rarely available from any one single source . rather , these conditions must be painstakingly inferred and curated , by hand , from information contained in many papers -- each of which contains only part of the story . in this paper , we introduce a graph-based meta-model , attuned to the representation of cellular signalling networks , which aims to ease this massive cognitive burden on the rule-based curation process . this meta-model is a generalization of that used by kappa and bngl which allows for the flexible representation of knowledge at various levels of granularity . in particular , it allows us to deal with information which has either too little , or too much , detail with respect to the strict rule-based meta-model . our approach provides a basis for the gradual aggregation of fragmented biological knowledge extracted from the literature into an instance of the meta-model from which we can define an automated translation into executable kappa programs .", "topics": ["machine translation", "interaction"]}
{"title": "markov chain modeling and simulation of breathing patterns", "abstract": "the lack of large video databases obtained from real patients with respiratory disorders makes the design and optimization of video-based monitoring systems quite critical . the purpose of this study is the development of suitable models and simulators of breathing behaviors and disorders , such as respiratory pauses and apneas , in order to allow efficient design and test of video-based monitoring systems . more precisely , a novel continuous-time markov chain ( ctmc ) statistical model of breathing patterns is presented . the respiratory rate ( rr ) pattern , estimated by measured vital signs of hospital-monitored patients , is approximated as a ctmc , whose states and parameters are selected through an appropriate statistical analysis . then , two simulators , software- and hardware-based , are proposed . after validation of the ctmc model , the proposed simulators are tested with previously developed video-based algorithms for the estimation of the rr and the detection of apnea events . examples of application to assess the performance of systems for video-based rr estimation and apnea detection are presented . the results , in terms of kullback-leibler divergence , show that realistic breathing patterns , including specific respiratory disorders , can be accurately described by the proposed model ; moreover , the simulators are able to reproduce practical breathing patterns for video analysis . the presented ctmc statistical model can be strategic to describe realistic breathing patterns and devise simulators useful to develop and test novel and effective video processing-based monitoring systems .", "topics": ["simulation", "database"]}
{"title": "learning sequence neighbourhood metrics", "abstract": "recurrent neural networks ( rnns ) in combination with a pooling operator and the neighbourhood components analysis ( nca ) objective function are able to detect the characterizing dynamics of sequences and embed them into a fixed-length vector space of arbitrary dimensionality . subsequently , the resulting features are meaningful and can be used for visualization or nearest neighbour classification in linear time . this kind of metric learning for sequential data enables the use of algorithms tailored towards fixed length vector spaces such as r^n .", "topics": ["recurrent neural network", "time complexity"]}
{"title": "challenges in bridging social semantics and formal semantics on the web", "abstract": "this paper describes several results of wimmics , a research lab which names stands for : web-instrumented man-machine interactions , communities , and semantics . the approaches introduced here rely on graph-oriented knowledge representation , reasoning and operationalization to model and support actors , actions and interactions in web-based epistemic communities . the re-search results are applied to support and foster interactions in online communities and manage their resources .", "topics": ["interaction"]}
{"title": "decentralized supply chain formation : a market protocol and competitive equilibrium analysis", "abstract": "supply chain formation is the process of determining the structure and terms of exchange relationships to enable a multilevel , multiagent production activity . we present a simple model of supply chains , highlighting two characteristic features : hierarchical subtask decomposition , and resource contention . to decentralize the formation process , we introduce a market price system over the resources produced along the chain . in a competitive equilibrium for this system , agents choose locally optimal allocations with respect to prices , and outcomes are optimal overall . to determine prices , we define a market protocol based on distributed , progressive auctions , and myopic , non-strategic agent bidding policies . in the presence of resource contention , this protocol produces better solutions than the greedy protocols common in the artificial intelligence and multiagent systems literature . the protocol often converges to high-value supply chains , and when competitive equilibria exist , typically to approximate competitive equilibria . however , complementarities in agent production technologies can cause the protocol to wastefully allocate inputs to agents that do not produce their outputs . a subsequent decommitment phase recovers a significant fraction of the lost surplus .", "topics": ["approximation algorithm", "artificial intelligence"]}
{"title": "high-throughput and language-agnostic entity disambiguation and linking on user generated data", "abstract": "the entity disambiguation and linking ( edl ) task matches entity mentions in text to a unique knowledge base ( kb ) identifier such as a wikipedia or freebase id . it plays a critical role in the construction of a high quality information network , and can be further leveraged for a variety of information retrieval and nlp tasks such as text categorization and document tagging . edl is a complex and challenging problem due to ambiguity of the mentions and real world text being multi-lingual . moreover , edl systems need to have high throughput and should be lightweight in order to scale to large datasets and run on off-the-shelf machines . more importantly , these systems need to be able to extract and disambiguate dense annotations from the data in order to enable an information retrieval or extraction task running on the data to be more efficient and accurate . in order to address all these challenges , we present the lithium edl system and algorithm - a high-throughput , lightweight , language-agnostic edl system that extracts and correctly disambiguates 75 % more entities than state-of-the-art edl systems and is significantly faster than them .", "topics": ["natural language processing"]}
{"title": "learning less is more - 6d camera localization via 3d surface regression", "abstract": "popular research areas like autonomous driving and augmented reality have renewed the interest in image-based camera localization . in this work , we address the task of predicting the 6d camera pose from a single rgb image in a given 3d environment . with the advent of neural networks , previous works have either learned the entire camera localization process , or multiple components of a camera localization pipeline . our key contribution is to demonstrate and explain that learning a single component of this pipeline is sufficient . this component is a fully convolutional neural network for densely regressing so-called scene coordinates , defining the correspondence between the input image and the 3d scene space . the neural network is prepended to a new end-to-end trainable pipeline . our system is efficient , highly accurate , robust in training , and exhibits outstanding generalization capabilities . it exceeds state-of-the-art consistently on indoor and outdoor datasets . interestingly , our approach surpasses existing techniques even without utilizing a 3d model of the scene during training , since the network is able to discover 3d scene geometry automatically , solely from single-view constraints .", "topics": ["end-to-end principle", "autonomous car"]}
{"title": "detecting image forgeries using geometric cues", "abstract": "this chapter presents a framework for detecting fake regions by using various methods including watermarking technique and blind approaches . in particular , we describe current categories on blind approaches which can be divided into five : pixel-based techniques , format-based techniques , camera-based techniques , physically-based techniques and geometric-based techniques . then we take a second look on the geometric-based techniques and further categorize them in detail . in the following section , the state-of-the-art methods involved in the geometric technique are elaborated .", "topics": ["entity", "pixel"]}
{"title": "global minimum for a finsler elastica minimal path approach", "abstract": "in this paper , we propose a novel curvature-penalized minimal path model via an orientation-lifted finsler metric and the euler elastica curve . the original minimal path model computes the globally minimal geodesic by solving an eikonal partial differential equation ( pde ) . essentially , this first-order model is unable to penalize curvature which is related to the path rigidity property in the classical active contour models . to solve this problem , we present an eikonal pde-based finsler elastica minimal path approach to address the curvature-penalized geodesic energy minimization problem . we were successful at adding the curvature penalization to the classical geodesic energy . the basic idea of this work is to interpret the euler elastica bending energy via a novel finsler elastica metric that embeds a curvature penalty . this metric is non-riemannian , anisotropic and asymmetric , and is defined over an orientation-lifted space by adding to the image domain the orientation as an extra space dimension . based on this orientation lifting , the proposed minimal path model can benefit from both the curvature and orientation of the paths . thanks to the fast marching method , the global minimum of the curvature-penalized geodesic energy can be computed efficiently . we introduce two anisotropic image data-driven speed functions that are computed by steerable filters . based on these orientation-dependent speed functions , we can apply the proposed finsler elastica minimal path model to the applications of closed contour detection , perceptual grouping and tubular structure extraction . numerical experiments on both synthetic and real images show that these applications of the proposed model indeed obtain promising results .", "topics": ["synthetic data"]}
{"title": "pvanet : lightweight deep neural networks for real-time object detection", "abstract": "in object detection , reducing computational cost is as important as improving accuracy for most practical usages . this paper proposes a novel network structure , which is an order of magnitude lighter than other state-of-the-art networks while maintaining the accuracy . based on the basic principle of more layers with less channels , this new deep neural network minimizes its redundancy by adopting recent innovations including c.relu and inception structure . we also show that this network can be trained efficiently to achieve solid results on well-known object detection benchmarks : 84.9 % and 84.2 % map on voc2007 and voc2012 while the required compute is less than 10 % of the recent resnet-101 .", "topics": ["object detection"]}
{"title": "incorporating language level information into acoustic models", "abstract": "this paper proposed a class of novel deep recurrent neural networks which can incorporate language-level information into acoustic models . for simplicity , we named these networks recurrent deep language networks ( rdlns ) . multiple variants of rdlns were considered , including two kinds of context information , two methods to process the context , and two methods to incorporate the language-level information . rdlns provided possible methods to fine-tune the whole automatic speech recognition ( asr ) system in the acoustic modeling process .", "topics": ["recurrent neural network", "speech recognition"]}
{"title": "the impact of mutation rate on the computation time of evolutionary dynamic optimization", "abstract": "mutation has traditionally been regarded as an important operator in evolutionary algorithms . in particular , there have been many experimental studies which showed the effectiveness of adapting mutation rates for various static optimization problems . given the perceived effectiveness of adaptive and self-adaptive mutation for static optimization problems , there have been speculations that adaptive and self-adaptive mutation can benefit dynamic optimization problems even more since adaptation and self-adaptation are capable of following a dynamic environment . however , few theoretical results are available in analyzing rigorously evolutionary algorithms for dynamic optimization problems . it is unclear when adaptive and self-adaptive mutation rates are likely to be useful for evolutionary algorithms in solving dynamic optimization problems . this paper provides the first rigorous analysis of adaptive mutation and its impact on the computation times of evolutionary algorithms in solving certain dynamic optimization problems . more specifically , for both individual-based and population-based eas , we have shown that any time-variable mutation rate scheme will not significantly outperform a fixed mutation rate on some dynamic optimization problem instances . the proofs also offer some insights into conditions under which any time-variable mutation scheme is unlikely to be useful and into the relationships between the problem characteristics and algorithmic features ( e.g . , different mutation schemes ) .", "topics": ["optimization problem", "computation"]}
{"title": "on training deep boltzmann machines", "abstract": "the deep boltzmann machine ( dbm ) has been an important development in the quest for powerful `` deep '' probabilistic models . to date , simultaneous or joint training of all layers of the dbm has been largely unsuccessful with existing training methods . we introduce a simple regularization scheme that encourages the weight vectors associated with each hidden unit to have similar norms . we demonstrate that this regularization can be easily combined with standard stochastic maximum likelihood to yield an effective training strategy for the simultaneous training of all layers of the deep boltzmann machine .", "topics": ["matrix regularization"]}
{"title": "in all likelihood , deep belief is not enough", "abstract": "statistical models of natural stimuli provide an important tool for researchers in the fields of machine learning and computational neuroscience . a canonical way to quantitatively assess and compare the performance of statistical models is given by the likelihood . one class of statistical models which has recently gained increasing popularity and has been applied to a variety of complex data are deep belief networks . analyses of these models , however , have been typically limited to qualitative analyses based on samples due to the computationally intractable nature of the model likelihood . motivated by these circumstances , the present article provides a consistent estimator for the likelihood that is both computationally tractable and simple to apply in practice . using this estimator , a deep belief network which has been suggested for the modeling of natural image patches is quantitatively investigated and compared to other models of natural image patches . contrary to earlier claims based on qualitative results , the results presented in this article provide evidence that the model under investigation is not a particularly good model for natural images", "topics": ["computational complexity theory", "bayesian network"]}
{"title": "provable inductive matrix completion", "abstract": "consider a movie recommendation system where apart from the ratings information , side information such as user 's age or movie 's genre is also available . unlike standard matrix completion , in this setting one should be able to predict inductively on new users/movies . in this paper , we study the problem of inductive matrix completion in the exact recovery setting . that is , we assume that the ratings matrix is generated by applying feature vectors to a low-rank matrix and the goal is to recover back the underlying matrix . furthermore , we generalize the problem to that of low-rank matrix estimation using rank-1 measurements . we study this generic problem and provide conditions that the set of measurements should satisfy so that the alternating minimization method ( which otherwise is a non-convex method with no convergence guarantees ) is able to recover back the { \\em exact } underlying low-rank matrix . in addition to inductive matrix completion , we show that two other low-rank estimation problems can be studied in our framework : a ) general low-rank matrix sensing using rank-1 measurements , and b ) multi-label regression with missing labels . for both the problems , we provide novel and interesting bounds on the number of measurements required by alternating minimization to provably converges to the { \\em exact } low-rank matrix . in particular , our analysis for the general low rank matrix sensing problem significantly improves the required storage and computational cost than that required by the rip-based matrix sensing methods \\cite { rechtfp2007 } . finally , we provide empirical validation of our approach and demonstrate that alternating minimization is able to recover the true matrix for the above mentioned problems using a small number of measurements .", "topics": ["feature vector"]}
{"title": "learning graph while training : an evolving graph convolutional neural network", "abstract": "convolution neural networks on graphs are important generalization and extension of classical cnns . while previous works generally assumed that the graph structures of samples are regular with unified dimensions , in many applications , they are highly diverse or even not well defined . under some circumstances , e.g . chemical molecular data , clustering or coarsening for simplifying the graphs is hard to be justified chemically . in this paper , we propose a more general and flexible graph convolution network ( egcn ) fed by batch of arbitrarily shaped data together with their evolving graph laplacians trained in supervised fashion . extensive experiments have been conducted to demonstrate the superior performance in terms of both the acceleration of parameter fitting and the significantly improved prediction accuracy on multiple graph-structured datasets .", "topics": ["cluster analysis", "neural networks"]}
{"title": "m-best solutions for a class of fuzzy constraint satisfaction problems", "abstract": "the article considers one of the possible generalizations of constraint satisfaction problems where relations are replaced by multivalued membership functions . in this case operations of disjunction and conjunction are replaced by maximum and minimum , and consistency of a solution becomes multivalued rather than binary . the article studies the problem of finding d most admissible solutions for a given d. a tractable subclass of these problems is defined by the concepts of invariants and polymorphisms similar to the classic constraint satisfaction approach . these concepts are adapted in two ways . firstly , the correspondence of `` invariant-polymorphism '' is generalized to ( min , max ) semirings . secondly , we consider non-uniform polymorphisms , where each variable has its own operator , in contrast to the case of one operator common for all variables . the article describes an algorithm that finds $ d $ most admissible solutions in polynomial time , provided that the problem is invariant with respect to some non-uniform majority operator . it is essential that this operator needs not to be known for the algorithm to work . moreover , even a guarantee for the existence of such an operator is not necessary . the algorithm either finds the solution or discards the problem . the latter is possible only if the problem has no majority polymorphism .", "topics": ["time complexity", "polynomial"]}
{"title": "speeding up the convergence of value iteration in partially observable markov decision processes", "abstract": "partially observable markov decision processes ( pomdps ) have recently become popular among many ai researchers because they serve as a natural model for planning under uncertainty . value iteration is a well-known algorithm for finding optimal policies for pomdps . it typically takes a large number of iterations to converge . this paper proposes a method for accelerating the convergence of value iteration . the method has been evaluated on an array of benchmark problems and was found to be very effective : it enabled value iteration to converge after only a few iterations on all the test problems .", "topics": ["iteration"]}
{"title": "semi-supervised text categorization using recursive k-means clustering", "abstract": "in this paper , we present a semi-supervised learning algorithm for classification of text documents . a method of labeling unlabeled text documents is presented . the presented method is based on the principle of divide and conquer strategy . it uses recursive k-means algorithm for partitioning both labeled and unlabeled data collection . the k-means algorithm is applied recursively on each partition till a desired level partition is achieved such that each partition contains labeled documents of a single class . once the desired clusters are obtained , the respective cluster centroids are considered as representatives of the clusters and the nearest neighbor rule is used for classifying an unknown text document . series of experiments have been conducted to bring out the superiority of the proposed model over other recent state of the art models on 20newsgroups dataset .", "topics": ["cluster analysis", "supervised learning"]}
{"title": "learning fast sparsifying transforms", "abstract": "given a dataset , the task of learning a transform that allows sparse representations of the data bears the name of dictionary learning . in many applications , these learned dictionaries represent the data much better than the static well-known transforms ( fourier , hadamard etc . ) . the main downside of learned transforms is that they lack structure and therefore they are not computationally efficient , unlike their classical counterparts . these posse several difficulties especially when using power limited hardware such as mobile devices , therefore discouraging the application of sparsity techniques in such scenarios . in this paper we construct orthogonal and non-orthogonal dictionaries that are factorized as a product of a few basic transformations . in the orthogonal case , we solve exactly the dictionary update problem for one basic transformation , which can be viewed as a generalized givens rotation , and then propose to construct orthogonal dictionaries that are a product of these transformations , guaranteeing their fast manipulation . we also propose a method to construct fast square but non-orthogonal dictionaries that are factorized as a product of few transforms that can be viewed as a further generalization of givens rotations to the non-orthogonal setting . we show how the proposed transforms can balance very well data representation performance and computational complexity . we also compare with classical fast and learned general and orthogonal transforms .", "topics": ["computational complexity theory", "sparse matrix"]}
{"title": "graphlab : a new framework for parallel machine learning", "abstract": "designing and implementing efficient , provably correct parallel machine learning ( ml ) algorithms is challenging . existing high-level parallel abstractions like mapreduce are insufficiently expressive while low-level tools like mpi and pthreads leave ml experts repeatedly solving the same design challenges . by targeting common patterns in ml , we developed graphlab , which improves upon abstractions like mapreduce by compactly expressing asynchronous iterative algorithms with sparse computational dependencies while ensuring data consistency and achieving a high degree of parallel performance . we demonstrate the expressiveness of the graphlab framework by designing and implementing parallel versions of belief propagation , gibbs sampling , co-em , lasso and compressed sensing . we show that using graphlab we can achieve excellent parallel performance on large scale real-world problems .", "topics": ["high- and low-level", "sparse matrix"]}
{"title": "kalman temporal differences", "abstract": "because reinforcement learning suffers from a lack of scalability , online value ( and q- ) function approximation has received increasing interest this last decade . this contribution introduces a novel approximation scheme , namely the kalman temporal differences ( ktd ) framework , that exhibits the following features : sample-efficiency , non-linear approximation , non-stationarity handling and uncertainty management . a first ktd-based algorithm is provided for deterministic markov decision processes ( mdp ) which produces biased estimates in the case of stochastic transitions . than the extended ktd framework ( xktd ) , solving stochastic mdp , is described . convergence is analyzed for special cases for both deterministic and stochastic transitions . related algorithms are experimented on classical benchmarks . they compare favorably to the state of the art while exhibiting the announced features .", "topics": ["nonlinear system", "reinforcement learning"]}
{"title": "clustering of scientific citations in wikipedia", "abstract": "the instances of templates in wikipedia form an interesting data set of structured information . here i focus on the cite journal template that is primarily used for citation to articles in scientific journals . these citations can be extracted and analyzed : non-negative matrix factorization is performed on a ( article x journal ) matrix resulting in a soft clustering of wikipedia articles and scientific journals , each cluster more or less representing a scientific topic .", "topics": ["cluster analysis"]}
{"title": "pushing the boundaries of boundary detection using deep learning", "abstract": "in this work we show that adapting deep convolutional neural network training to the task of boundary detection can result in substantial improvements over the current state-of-the-art in boundary detection . our contributions consist firstly in combining a careful design of the loss for boundary detection training , a multi-resolution architecture and training with external data to improve the detection accuracy of the current state of the art . when measured on the standard berkeley segmentation dataset , we improve theoptimal dataset scale f-measure from 0.780 to 0.808 - while human performance is at 0.803 . we further improve performance to 0.813 by combining deep learning with grouping , integrating the normalized cuts technique within a deep network . we also examine the potential of our boundary detector in conjunction with the task of semantic segmentation and demonstrate clear improvements over state-of-the-art systems . our detector is fully integrated in the popular caffe framework and processes a 320x420 image in less than a second .", "topics": ["neural networks"]}
{"title": "higher-order momentum distributions and locally affine lddmm registration", "abstract": "to achieve sparse parametrizations that allows intuitive analysis , we aim to represent deformation with a basis containing interpretable elements , and we wish to use elements that have the description capacity to represent the deformation compactly . to accomplish this , we introduce in this paper higher-order momentum distributions in the lddmm registration framework . while the zeroth order moments previously used in lddmm only describe local displacement , the first-order momenta that are proposed here represent a basis that allows local description of affine transformations and subsequent compact description of non-translational movement in a globally non-rigid deformation . the resulting representation contains directly interpretable information from both mathematical and modeling perspectives . we develop the mathematical construction of the registration framework with higher-order momenta , we show the implications for sparse image registration and deformation description , and we provide examples of how the parametrization enables registration with a very low number of parameters . the capacity and interpretability of the parametrization using higher-order momenta lead to natural modeling of articulated movement , and the method promises to be useful for quantifying ventricle expansion and progressing atrophy during alzheimer 's disease .", "topics": ["sparse matrix"]}
{"title": "generate to adapt : aligning domains using generative adversarial networks", "abstract": "domain adaptation is an actively researched problem in computer vision . in this work , we propose an approach that leverages unsupervised data to bring the source and target distributions closer in a learned joint feature space . we accomplish this by inducing a symbiotic relationship between the learned embedding and a generative adversarial network . this is in contrast to methods which use the adversarial framework for realistic data generation and retraining deep models with such data . we demonstrate the strength and generality of our approach by performing experiments on three different tasks with varying levels of difficulty : ( 1 ) digit classification ( mnist , svhn and usps datasets ) ( 2 ) object recognition using office dataset and ( 3 ) domain adaptation from synthetic to real data . our method achieves state-of-the art performance in most experimental settings and by far the only gan-based method that has been shown to work well across different datasets such as office and digits .", "topics": ["unsupervised learning", "feature vector"]}
{"title": "cluster trees on manifolds", "abstract": "in this paper we investigate the problem of estimating the cluster tree for a density $ f $ supported on or near a smooth $ d $ -dimensional manifold $ m $ isometrically embedded in $ \\mathbb { r } ^d $ . we analyze a modified version of a $ k $ -nearest neighbor based algorithm recently proposed by chaudhuri and dasgupta . the main results of this paper show that under mild assumptions on $ f $ and $ m $ , we obtain rates of convergence that depend on $ d $ only but not on the ambient dimension $ d $ . we also show that similar ( albeit non-algorithmic ) results can be obtained for kernel density estimators . we sketch a construction of a sample complexity lower bound instance for a natural class of manifold oblivious clustering algorithms . we further briefly consider the known manifold case and show that in this case a spatially adaptive algorithm achieves better rates .", "topics": ["cluster analysis"]}
{"title": "exploring temporal preservation networks for precise temporal action localization", "abstract": "temporal action localization is an important task of computer vision . though a variety of methods have been proposed , it still remains an open question how to predict the temporal boundaries of action segments precisely . most works use segment-level classifiers to select video segments pre-determined by action proposal or dense sliding windows . however , in order to achieve more precise action boundaries , a temporal localization system should make dense predictions at a fine granularity . a newly proposed work exploits convolutional-deconvolutional-convolutional ( cdc ) filters to upsample the predictions of 3d convnets , making it possible to perform per-frame action predictions and achieving promising performance in terms of temporal action localization . however , cdc network loses temporal information partially due to the temporal downsampling operation . in this paper , we propose an elegant and powerful temporal preservation convolutional ( tpc ) network that equips 3d convnets with tpc filters . tpc network can fully preserve temporal resolution and downsample the spatial resolution simultaneously , enabling frame-level granularity action localization . tpc network can be trained in an end-to-end manner . experiment results on public datasets show that tpc network achieves significant improvement on per-frame action prediction and competing results on segment-level temporal action localization .", "topics": ["computer vision", "end-to-end principle"]}
{"title": "a survey on application of machine learning techniques in optical networks", "abstract": "today , the amount of data that can be retrieved from communications networks is extremely high and diverse ( e.g . , data regarding users behavior , traffic traces , network alarms , signal quality indicators , etc . ) . advanced mathematical tools are required to extract useful information from this large set of network data . in particular , machine learning ( ml ) is regarded as a promising methodological area to perform network-data analysis and enable , e.g . , automatized network self-configuration and fault management . in this survey we classify and describe relevant studies dealing with the applications of ml to optical communications and networking . optical networks and system are facing an unprecedented growth in terms of complexity due to the introduction of a huge number of adjustable parameters ( such as routing configurations , modulation format , symbol rate , coding schemes , etc . ) , mainly due to the adoption of , among the others , coherent transmission/reception technology , advanced digital signal processing and to the presence of nonlinear effects in optical fiber systems . although a good number of research papers have appeared in the last years , the application of ml to optical networks is still in its early stage . in this survey we provide an introductory reference for researchers and practitioners interested in this field . to stimulate further work in this area , we conclude the paper proposing new possible research directions .", "topics": ["nonlinear system"]}
{"title": "axondeepseg : automatic axon and myelin segmentation from microscopy data using convolutional neural networks", "abstract": "segmentation of axon and myelin from microscopy images of the nervous system provides useful quantitative information about the tissue microstructure , such as axon density and myelin thickness . this could be used for instance to document cell morphometry across species , or to validate novel non-invasive quantitative magnetic resonance imaging techniques . most currently-available segmentation algorithms are based on standard image processing and usually require multiple processing steps and/or parameter tuning by the user to adapt to different modalities . moreover , only few methods are publicly available . we introduce axondeepseg , an open-source software that performs axon and myelin segmentation of microscopic images using deep learning . axondeepseg features : ( i ) a convolutional neural network architecture ; ( ii ) an easy training procedure to generate new models based on manually-labelled data and ( iii ) two ready-to-use models trained from scanning electron microscopy ( sem ) and transmission electron microscopy ( tem ) . results show high pixel-wise accuracy across various species : 85 % on rat sem , 81 % on human sem , 95 % on mice tem and 84 % on macaque tem . segmentation of a full rat spinal cord slice is computed and morphological metrics are extracted and compared against the literature . axondeepseg is freely available at https : //github.com/neuropoly/axondeepseg", "topics": ["image processing", "pixel"]}
{"title": "large-scale subspace clustering using sketching and validation", "abstract": "the nowadays massive amounts of generated and communicated data present major challenges in their processing . while capable of successfully classifying nonlinearly separable objects in various settings , subspace clustering ( sc ) methods incur prohibitively high computational complexity when processing large-scale data . inspired by the random sampling and consensus ( ransac ) approach to robust regression , the present paper introduces a randomized scheme for sc , termed sketching and validation ( skeva- ) sc , tailored for large-scale data . at the heart of skeva-sc lies a randomized scheme for approximating the underlying probability density function of the observed data by kernel smoothing arguments . sparsity in data representations is also exploited to reduce the computational burden of sc , while achieving high clustering accuracy . performance analysis as well as extensive numerical tests on synthetic and real data corroborate the potential of skeva-sc and its competitive performance relative to state-of-the-art scalable sc approaches . keywords : subspace clustering , big data , kernel smoothing , randomization , sketching , validation , sparsity .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "can you find a face in a hevc bitstream ?", "abstract": "finding faces in images is one of the most important tasks in computer vision , with applications in biometrics , surveillance , human-computer interaction , and other areas . in our earlier work , we demonstrated that it is possible to tell whether or not an image contains a face by only examining the hevc syntax , without fully reconstructing the image . in the present work we move further in this direction by showing how to localize faces in hevc-coded images , without full reconstruction . we also demonstrate the benefits that such approach can have in privacy-friendly face localization .", "topics": ["computer vision"]}
{"title": "a novel design specification distance ( dsd ) based k-mean clustering performace evluation on engineering materials database", "abstract": "organizing data into semantically more meaningful is one of the fundamental modes of understanding and learning . cluster analysis is a formal study of methods for understanding and algorithm for learning . k-mean clustering algorithm is one of the most fundamental and simple clustering algorithms . when there is no prior knowledge about the distribution of data sets , k-mean is the first choice for clustering with an initial number of clusters . in this paper a novel distance metric called design specification ( ds ) distance measure function is integrated with k-mean clustering algorithm to improve cluster accuracy . the k-means algorithm with proposed distance measure maximizes the cluster accuracy to 99.98 % at p = 1.525 , which is determined through the iterative procedure . the performance of design specification ( ds ) distance measure function with k - mean algorithm is compared with the performances of other standard distance functions such as euclidian , squared euclidean , city block , and chebshew similarity measures deployed with k-mean algorithm.the proposed method is evaluated on the engineering materials database . the experiments on cluster analysis and the outlier profiling show that these is an excellent improvement in the performance of the proposed method .", "topics": ["cluster analysis", "database"]}
{"title": "an efficient two-stage sparse representation method", "abstract": "there are a large number of methods for solving under-determined linear inverse problem . many of them have very high time complexity for large datasets . we propose a new method called two-stage sparse representation ( tssr ) to tackle this problem . we decompose the representing space of signals into two parts , the measurement dictionary and the sparsifying basis . the dictionary is designed to approximate a sub-gaussian distribution to exploit its concentration property . we apply sparse coding to the signals on the dictionary in the first stage , and obtain the training and testing coefficients respectively . then we design the basis to approach an identity matrix in the second stage , to acquire the restricted isometry property ( rip ) and universality property . the testing coefficients are encoded over the basis and the final representing coefficients are obtained . we verify that the projection of testing coefficients onto the basis is a good approximation of the signal onto the representing space . since the projection is conducted on a much sparser space , the runtime is greatly reduced . for concrete realization , we provide an instance for the proposed tssr . experiments on four biometrics databases show that tssr is effective and efficient , comparing with several classical methods for solving linear inverse problem .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "information-theoretic limits of bayesian network structure learning", "abstract": "in this paper , we study the information-theoretic limits of learning the structure of bayesian networks ( bns ) , on discrete as well as continuous random variables , from a finite number of samples . we show that the minimum number of samples required by any procedure to recover the correct structure grows as $ \\omega ( m ) $ and $ \\omega ( k \\log m + ( k^2/m ) ) $ for non-sparse and sparse bns respectively , where $ m $ is the number of variables and $ k $ is the maximum number of parents per node . we provide a simple recipe , based on an extension of the fano 's inequality , to obtain information-theoretic limits of structure recovery for any exponential family bn . we instantiate our result for specific conditional distributions in the exponential family to characterize the fundamental limits of learning various commonly used bns , such as conditional probability table based networks , gaussian bns , noisy-or networks , and logistic regression networks . en route to obtaining our main results , we obtain tight bounds on the number of sparse and non-sparse essential-dags . finally , as a byproduct , we recover the information-theoretic limits of sparse variable selection for logistic regression .", "topics": ["graphical model", "bayesian network"]}
{"title": "generative temporal models with memory", "abstract": "we consider the general problem of modeling temporal data with long-range dependencies , wherein new observations are fully or partially predictable based on temporally-distant , past observations . a sufficiently powerful temporal model should separate predictable elements of the sequence from unpredictable elements , express uncertainty about those unpredictable elements , and rapidly identify novel elements that may help to predict the future . to create such models , we introduce generative temporal models augmented with external memory systems . they are developed within the variational inference framework , which provides both a practical training methodology and methods to gain insight into the models ' operation . we show , on a range of problems with sparse , long-term temporal dependencies , that these models store information from early in a sequence , and reuse this stored information efficiently . this allows them to perform substantially better than existing models based on well-known recurrent neural networks , like lstms .", "topics": ["calculus of variations", "recurrent neural network"]}
{"title": "read , tag , and parse all at once , or fully-neural dependency parsing", "abstract": "we present a dependency parser implemented as a single deep neural network that reads orthographic representations of words and directly generates dependencies and their labels . unlike typical approaches to parsing , the model does n't require part-of-speech ( pos ) tagging of the sentences . with proper regularization and additional supervision achieved with multitask learning we reach state-of-the-art performance on slavic languages from the universal dependencies treebank : with no linguistic features other than characters , our parser is as accurate as a transition- based system trained on perfect pos tags .", "topics": ["parsing", "matrix regularization"]}
{"title": "robust high dimensional sparse regression and matching pursuit", "abstract": "we consider high dimensional sparse regression , and develop strategies able to deal with arbitrary -- possibly , severe or coordinated -- errors in the covariance matrix $ x $ . these may come from corrupted data , persistent experimental errors , or malicious respondents in surveys/recommender systems , etc . such non-stochastic error-in-variables problems are notoriously difficult to treat , and as we demonstrate , the problem is particularly pronounced in high-dimensional settings where the primary goal is { \\em support recovery } of the sparse regressor . we develop algorithms for support recovery in sparse regression , when some number $ n_1 $ out of $ n+n_1 $ total covariate/response pairs are { \\it arbitrarily ( possibly maliciously ) corrupted } . we are interested in understanding how many outliers , $ n_1 $ , we can tolerate , while identifying the correct support . to the best of our knowledge , neither standard outlier rejection techniques , nor recently developed robust regression algorithms ( that focus only on corrupted response variables ) , nor recent algorithms for dealing with stochastic noise or erasures , can provide guarantees on support recovery . perhaps surprisingly , we also show that the natural brute force algorithm that searches over all subsets of $ n $ covariate/response pairs , and all subsets of possible support coordinates in order to minimize regression error , is remarkably poor , unable to correctly identify the support with even $ n_1 = o ( n/k ) $ corrupted points , where $ k $ is the sparsity . this is true even in the basic setting we consider , where all authentic measurements and noise are independent and sub-gaussian . in this setting , we provide a simple algorithm -- no more computationally taxing than omp -- that gives stronger performance guarantees , recovering the support with up to $ n_1 = o ( n/ ( \\sqrt { k } \\log p ) ) $ corrupted points , where $ p $ is the dimension of the signal to be recovered .", "topics": ["sparse matrix"]}
{"title": "recent advances in the applications of convolutional neural networks to medical image contour detection", "abstract": "the fast growing deep learning technologies have become the main solution of many machine learning problems for medical image analysis . deep convolution neural networks ( cnns ) , as one of the most important branch of the deep learning family , have been widely investigated for various computer-aided diagnosis tasks including long-term problems and continuously emerging new problems . image contour detection is a fundamental but challenging task that has been studied for more than four decades . recently , we have witnessed the significantly improved performance of contour detection thanks to the development of cnns . beyond purusing performance in existing natural image benchmarks , contour detection plays a particularly important role in medical image analysis . segmenting various objects from radiology images or pathology images requires accurate detection of contours . however , some problems , such as discontinuity and shape constraints , are insufficiently studied in cnns . it is necessary to clarify the challenges to encourage further exploration . the performance of cnn based contour detection relies on the state-of-the-art cnn architectures . careful investigation of their design principles and motivations is critical and beneficial to contour detection . in this paper , we first review recent development of medical image contour detection and point out the current confronting challenges and problems . we discuss the development of general cnns and their applications in image contours ( or edges ) detection . we compare those methods in detail , clarify their strengthens and weaknesses . then we review their recent applications in medical image analysis and point out limitations , with the goal to light some potential directions in medical image analysis . we expect the paper to cover comprehensive technical ingredients of advanced cnns to enrich the study in the medical image domain .", "topics": ["neural networks", "convolution"]}
{"title": "optimized clothes segmentation to boost gender classification in unconstrained scenarios", "abstract": "several applications require demographic information of ordinary people in unconstrained scenarios . this is not a trivial task due to significant human appearance variations . in this work , we introduce trixels for clustering image regions , enumerating their advantages compared to superpixels . the classical grabcut algorithm is later modified to segment trixels instead of pixels in an unsupervised context . combining with face detection lead us to a clothes segmentation approach close to real time . the study uses the challenging pascal voc dataset for segmentation evaluation experiments . a final experiment analyzes the fusion of clothes features with state-of-the-art gender classifiers in clothesdb , revealing a significant performance improvement in gender classification .", "topics": ["cluster analysis", "pixel"]}
{"title": "neural machine translation with pivot languages", "abstract": "while recent neural machine translation approaches have delivered state-of-the-art performance for resource-rich language pairs , they suffer from the data scarcity problem for resource-scarce language pairs . although this problem can be alleviated by exploiting a pivot language to bridge the source and target languages , the source-to-pivot and pivot-to-target translation models are usually independently trained . in this work , we introduce a joint training algorithm for pivot-based neural machine translation . we propose three methods to connect the two models and enable them to interact with each other during training . experiments on europarl and wmt corpora show that joint training of source-to-pivot and pivot-to-target models leads to significant improvements over independent training across various languages .", "topics": ["machine translation", "text corpus"]}
{"title": "a study of deep learning robustness against computation failures", "abstract": "for many types of integrated circuits , accepting larger failure rates in computations can be used to improve energy efficiency . we study the performance of faulty implementations of certain deep neural networks based on pessimistic and optimistic models of the effect of hardware faults . after identifying the impact of hyperparameters such as the number of layers on robustness , we study the ability of the network to compensate for computational failures through an increase of the network size . we show that some networks can achieve equivalent performance under faulty implementations , and quantify the required increase in computational complexity .", "topics": ["computational complexity theory", "computation"]}
{"title": "devnagari handwritten numeral recognition using geometric features and statistical combination classifier", "abstract": "this paper presents a devnagari numerical recognition method based on statistical discriminant functions . 17 geometric features based on pixel connectivity , lines , line directions , holes , image area , perimeter , eccentricity , solidity , orientation etc . are used for representing the numerals . five discriminant functions viz . linear , quadratic , diaglinear , diagquadratic and mahalanobis distance are used for classification . 1500 handwritten numerals are used for training . another 1500 handwritten numerals are used for testing . experimental results show that linear , quadratic and mahalanobis discriminant functions provide better results . results of these three discriminants are fed to a majority voting type combination classifier . it is found that combination classifier offers better results over individual classifiers .", "topics": ["pixel"]}
{"title": "a feature-based comparison of evolutionary computing techniques for constrained continuous optimisation", "abstract": "evolutionary algorithms have been frequently applied to constrained continuous optimisation problems . we carry out feature based comparisons of different types of evolutionary algorithms such as evolution strategies , differential evolution and particle swarm optimisation for constrained continuous optimisation . in our study , we examine how sets of constraints influence the difficulty of obtaining close to optimal solutions . using a multi-objective approach , we evolve constrained continuous problems having a set of linear and/or quadratic constraints where the different evolutionary approaches show a significant difference in performance . afterwards , we discuss the features of the constraints that exhibit a difference in performance of the different evolutionary approaches under consideration .", "topics": ["mathematical optimization"]}
{"title": "field geology with a wearable computer : 1st results of the cyborg astrobiologist system", "abstract": "we present results from the first geological field tests of the `cyborg astrobiologist ' , which is a wearable computer and video camcorder system that we are using to test and train a computer-vision system towards having some of the autonomous decision-making capabilities of a field-geologist . the cyborg astrobiologist platform has thus far been used for testing and development of these algorithms and systems : robotic acquisition of quasi-mosaics of images , real-time image segmentation , and real-time determination of interesting points in the image mosaics . this work is more of a test of the whole system , rather than of any one part of the system . however , beyond the concept of the system itself , the uncommon map ( despite its simplicity ) is the main innovative part of the system . the uncommon map helps to determine interest-points in a context-free manner . overall , the hardware and software systems function reliably , and the computer-vision algorithms are adequate for the first field tests . in addition to the proof-of-concept aspect of these field tests , the main result of these field tests is the enumeration of those issues that we can improve in the future , including : dealing with structural shadow and microtexture , and also , controlling the camera 's zoom lens in an intelligent manner . nonetheless , despite these and other technical inadequacies , this cyborg astrobiologist system , consisting of a camera-equipped wearable-computer and its computer-vision algorithms , has demonstrated its ability of finding genuinely interesting points in real-time in the geological scenery , and then gathering more information about these interest points in an automated manner . we use these capabilities for autonomous guidance towards geological points-of-interest .", "topics": ["image segmentation", "computer vision"]}
{"title": "unsupervised cross-media hashing with structure preservation", "abstract": "recent years have seen the exponential growth of heterogeneous multimedia data . the need for effective and accurate data retrieval from heterogeneous data sources has attracted much research interest in cross-media retrieval . here , given a query of any media type , cross-media retrieval seeks to find relevant results of different media types from heterogeneous data sources . to facilitate large-scale cross-media retrieval , we propose a novel unsupervised cross-media hashing method . our method incorporates local affinity and distance repulsion constraints into a matrix factorization framework . correspondingly , the proposed method learns hash functions that generates unified hash codes from different media types , while ensuring intrinsic geometric structure of the data distribution is preserved . these hash codes empower the similarity between data of different media types to be evaluated directly . experimental results on two large-scale multimedia datasets demonstrate the effectiveness of the proposed method , where we outperform the state-of-the-art methods .", "topics": ["time complexity"]}
{"title": "machine learning for the geosciences : challenges and opportunities", "abstract": "geosciences is a field of great societal relevance that requires solutions to several urgent problems facing our humanity and the planet . as geosciences enters the era of big data , machine learning ( ml ) -- that has been widely successful in commercial domains -- offers immense potential to contribute to problems in geosciences . however , problems in geosciences have several unique challenges that are seldom found in traditional applications , requiring novel problem formulations and methodologies in machine learning . this article introduces researchers in the machine learning ( ml ) community to these challenges offered by geoscience problems and the opportunities that exist for advancing both machine learning and geosciences . we first highlight typical sources of geoscience data and describe their properties that make it challenging to use traditional machine learning techniques . we then describe some of the common categories of geoscience problems where machine learning can play a role , and discuss some of the existing efforts and promising directions for methodological development in machine learning . we conclude by discussing some of the emerging research themes in machine learning that are applicable across all problems in the geosciences , and the importance of a deep collaboration between machine learning and geosciences for synergistic advancements in both disciplines .", "topics": ["relevance"]}
{"title": "a target classification decision aid", "abstract": "a submarine 's sonar team is responsible for detecting , localising and classifying targets using information provided by the platform 's sensor suite . the information used to make these assessments is typically uncertain and/or incomplete and is likely to require a measure of confidence in its reliability . moreover , improvements in sensor and communication technology are resulting in increased amounts of on-platform and off-platform information available for evaluation . this proliferation of imprecise information increases the risk of overwhelming the operator . to assist the task of localisation and classification a concept demonstration decision aid ( horizon ) , based on evidential reasoning , has been developed . horizon is an information fusion software package for representing and fusing imprecise information about the state of the world , expressed across suitable frames of reference . the horizon software is currently at prototype stage .", "topics": ["sensor"]}
{"title": "deep residual learning for instrument segmentation in robotic surgery", "abstract": "detection , tracking , and pose estimation of surgical instruments are crucial tasks for computer assistance during minimally invasive robotic surgery . in the majority of cases , the first step is the automatic segmentation of surgical tools . prior work has focused on binary segmentation , where the objective is to label every pixel in an image as tool or background . we improve upon previous work in two major ways . first , we leverage recent techniques such as deep residual learning and dilated convolutions to advance binary-segmentation performance . second , we extend the approach to multi-class segmentation , which lets us segment different parts of the tool , in addition to background . we demonstrate the performance of this method on the miccai endoscopic vision challenge robotic instruments dataset .", "topics": ["image segmentation", "convolution"]}
{"title": "on-the-fly network pruning for object detection", "abstract": "object detection with deep neural networks is often performed by passing a few thousand candidate bounding boxes through a deep neural network for each image . these bounding boxes are highly correlated since they originate from the same image . in this paper we investigate how to exploit feature occurrence at the image scale to prune the neural network which is subsequently applied to all bounding boxes . we show that removing units which have near-zero activation in the image allows us to significantly reduce the number of parameters in the network . results on the pascal 2007 object detection challenge demonstrate that up to 40 % of units in some fully-connected layers can be entirely eliminated with little change in the detection result .", "topics": ["object detection"]}
{"title": "poseidon : an efficient communication architecture for distributed deep learning on gpu clusters", "abstract": "deep learning models can take weeks to train on a single gpu-equipped machine , necessitating scaling out dl training to a gpu-cluster . however , current distributed dl implementations can scale poorly due to substantial parameter synchronization over the network , because the high throughput of gpus allows more data batches to be processed per unit time than cpus , leading to more frequent network synchronization . we present poseidon , an efficient communication architecture for distributed dl on gpus . poseidon exploits the layered model structures in dl programs to overlap communication and computation , reducing bursty network communication . moreover , poseidon uses a hybrid communication scheme that optimizes the number of bytes required to synchronize each layer , according to layer properties and the number of machines . we show that poseidon is applicable to different dl frameworks by plugging poseidon into caffe and tensorflow . we show that poseidon enables caffe and tensorflow to achieve 15.5x speed-up on 16 single-gpu machines , even with limited bandwidth ( 10gbe ) and the challenging vgg19-22k network for image classification . moreover , poseidon-enabled tensorflow achieves 31.5x speed-up with 32 single-gpu machines on inception-v3 , a 50 % improvement over the open-source tensorflow ( 20x speed-up ) .", "topics": ["computer vision", "computation"]}
{"title": "towards a constructive multilayer perceptron for regression task using non-parametric clustering . a case study of photo-z redshift reconstruction", "abstract": "the choice of architecture of artificial neuron network ( ann ) is still a challenging task that users face every time . it greatly affects the accuracy of the built network . in fact there is no optimal method that is applicable to various implementations at the same time . in this paper we propose a method to construct ann based on clustering , that resolves the problems of random and ad hoc approaches for multilayer ann architecture . our method can be applied to regression problems . experimental results obtained with different datasets , reveals the efficiency of our method .", "topics": ["cluster analysis"]}
{"title": "a nonconformity approach to model selection for svms", "abstract": "we investigate the issue of model selection and the use of the nonconformity ( strangeness ) measure in batch learning . using the nonconformity measure we propose a new training algorithm that helps avoid the need for cross-validation or leave-one-out model selection strategies . we provide a new generalisation error bound using the notion of nonconformity to upper bound the loss of each test example and show that our proposed approach is comparable to standard model selection methods , but with theoretical guarantees of success and faster convergence . we demonstrate our novel model selection technique using the support vector machine .", "topics": ["support vector machine"]}
{"title": "smart pacing for effective online ad campaign optimization", "abstract": "in targeted online advertising , advertisers look for maximizing campaign performance under delivery constraint within budget schedule . most of the advertisers typically prefer to impose the delivery constraint to spend budget smoothly over the time in order to reach a wider range of audiences and have a sustainable impact . since lots of impressions are traded through public auctions for online advertising today , the liquidity makes price elasticity and bid landscape between demand and supply change quite dynamically . therefore , it is challenging to perform smooth pacing control and maximize campaign performance simultaneously . in this paper , we propose a smart pacing approach in which the delivery pace of each campaign is learned from both offline and online data to achieve smooth delivery and optimal performance goals . the implementation of the proposed approach in a real dsp system is also presented . experimental evaluations on both real online ad campaigns and offline simulations show that our approach can effectively improve campaign performance and achieve delivery goals .", "topics": ["simulation"]}
{"title": "an efficient hierarchical graph based image segmentation", "abstract": "hierarchical image segmentation provides region-oriented scalespace , i.e . , a set of image segmentations at different detail levels in which the segmentations at finer levels are nested with respect to those at coarser levels . most image segmentation algorithms , such as region merging algorithms , rely on a criterion for merging that does not lead to a hierarchy , and for which the tuning of the parameters can be difficult . in this work , we propose a hierarchical graph based image segmentation relying on a criterion popularized by felzenzwalb and huttenlocher . we illustrate with both real and synthetic images , showing efficiency , ease of use , and robustness of our method .", "topics": ["image segmentation", "synthetic data"]}
{"title": "unsupervised representation learning with deep convolutional generative adversarial networks", "abstract": "in recent years , supervised learning with convolutional networks ( cnns ) has seen huge adoption in computer vision applications . comparatively , unsupervised learning with cnns has received less attention . in this work we hope to help bridge the gap between the success of cnns for supervised learning and unsupervised learning . we introduce a class of cnns called deep convolutional generative adversarial networks ( dcgans ) , that have certain architectural constraints , and demonstrate that they are a strong candidate for unsupervised learning . training on various image datasets , we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator . additionally , we use the learned features for novel tasks - demonstrating their applicability as general image representations .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "generalized intersection kernel", "abstract": "following the very recent line of work on the `` generalized min-max '' ( gmm ) kernel , this study proposes the `` generalized intersection '' ( gint ) kernel and the related `` normalized generalized min-max '' ( ngmm ) kernel . in computer vision , the ( histogram ) intersection kernel has been popular , and the gint kernel generalizes it to data which can have both negative and positive entries . through an extensive empirical classification study on 40 datasets from the uci repository , we are able to show that this ( tuning-free ) gint kernel performs fairly well . the empirical results also demonstrate that the ngmm kernel typically outperforms the gint kernel . interestingly , the ngmm kernel has another interpretation -- - it is the `` asymmetrically transformed '' version of the gint kernel , based on the idea of `` asymmetric hashing '' . just like the gmm kernel , the ngmm kernel can be efficiently linearized through ( e.g . , ) generalized consistent weighted sampling ( gcws ) , as empirically validated in our study . owing to the discrete nature of hashed values , it also provides a scheme for approximate near neighbor search .", "topics": ["kernel ( operating system )", "sampling ( signal processing )"]}
{"title": "sub-optimum signal linear detector using wavelets and support vector machines", "abstract": "the problem of known signal detection in additive white gaussian noise is considered . in previous work , a new detection scheme was introduced by the authors , and it was demonstrated that optimum performance can not be reached in a real implementation . in this paper we analyse support vector machines ( svm ) as an alternative , evaluating the results in terms of probability of detection curves for a fixed probability of false alarm .", "topics": ["support vector machine"]}
{"title": "evolutionary acyclic graph partitioning", "abstract": "directed graphs are widely used to model data flow and execution dependencies in streaming applications . this enables the utilization of graph partitioning algorithms for the problem of parallelizing computation for multiprocessor architectures . however due to resource restrictions , an acyclicity constraint on the partition is necessary when mapping streaming applications to an embedded multiprocessor . here , we contribute a multi-level algorithm for the acyclic graph partitioning problem . based on this , we engineer an evolutionary algorithm to further reduce communication cost , as well as to improve load balancing and the scheduling makespan on embedded multiprocessor architectures .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "estimation of stochastic attribute-value grammars using an informative sample", "abstract": "we argue that some of the computational complexity associated with estimation of stochastic attribute-value grammars can be reduced by training upon an informative subset of the full training set . results using the parsed wall street journal corpus show that in some circumstances , it is possible to obtain better estimation results using an informative sample than when training upon all the available material . further experimentation demonstrates that with unlexicalised models , a gaussian prior can reduce overfitting . however , when models are lexicalised and contain overlapping features , overfitting does not seem to be a problem , and a gaussian prior makes minimal difference to performance . our approach is applicable for situations when there are an infeasibly large number of parses in the training set , or else for when recovery of these parses from a packed representation is itself computationally expensive .", "topics": ["test set", "computational complexity theory"]}
{"title": "pose-normalized image generation for person re-identification", "abstract": "person re-identification ( re-id ) faces two major challenges : the lack of cross-view paired training data and learning discriminative identity-sensitive and view-invariant features in the presence of large pose variations . in this work , we address both problems by proposing a novel deep person image generation model for synthesizing realistic person images conditional on pose . the model is based on a generative adversarial network ( gan ) and used specifically for pose normalization in re-id , thus termed pose-normalization gan ( pn-gan ) . with the synthesized images , we can learn a new type of deep re-id feature free of the influence of pose variations . we show that this feature is strong on its own and highly complementary to features learned with the original images . importantly , we now have a model that generalizes to any new re-id dataset without the need for collecting any training data for model fine-tuning , thus making a deep re-id model truly scalable . extensive experiments on five benchmarks show that our model outperforms the state-of-the-art models , often significantly . in particular , the features learned on market-1501 can achieve a rank-1 accuracy of 68.67 % on viper without any model fine-tuning , beating almost all existing models fine-tuned on the dataset .", "topics": ["test set", "scalability"]}
{"title": "semigraphoids are two-antecedental approximations of stochastic conditional independence models", "abstract": "the semigraphoid closure of every couple of ci-statements ( gi=conditional independence ) is a stochastic ci-model . as a consequence of this result it is shown that every probabilistically sound inference rule for ci-model , having at most two antecedents , is derivable from the semigraphoid inference rules . this justifies the use of semigraphoids as approximations of stochastic ci-models in probabilistic reasoning . the list of all 19 potential dominant elements of the mentioned semigraphoid closure is given as a byproduct .", "topics": ["approximation"]}
{"title": "recovery guarantees for one-hidden-layer neural networks", "abstract": "in this paper , we consider regression problems with one-hidden-layer neural networks ( 1nns ) . we distill some properties of activation functions that lead to $ \\mathit { local~strong~convexity } $ in the neighborhood of the ground-truth parameters for the 1nn squared-loss objective . most popular nonlinear activation functions satisfy the distilled properties , including rectified linear units ( relus ) , leaky relus , squared relus and sigmoids . for activation functions that are also smooth , we show $ \\mathit { local~linear~convergence } $ guarantees of gradient descent under a resampling rule . for homogeneous activations , we show tensor methods are able to initialize the parameters to fall into the local strong convexity region . as a result , tensor initialization followed by gradient descent is guaranteed to recover the ground truth with sample complexity $ d \\cdot \\log ( 1/\\epsilon ) \\cdot \\mathrm { poly } ( k , \\lambda ) $ and computational complexity $ n\\cdot d \\cdot \\mathrm { poly } ( k , \\lambda ) $ for smooth homogeneous activations with high probability , where $ d $ is the dimension of the input , $ k $ ( $ k\\leq d $ ) is the number of hidden nodes , $ \\lambda $ is a conditioning property of the ground-truth parameter matrix between the input layer and the hidden layer , $ \\epsilon $ is the targeted precision and $ n $ is the number of samples . to the best of our knowledge , this is the first work that provides recovery guarantees for 1nns with both sample complexity and computational complexity $ \\mathit { linear } $ in the input dimension and $ \\mathit { logarithmic } $ in the precision .", "topics": ["computational complexity theory", "nonlinear system"]}
{"title": "extracting formal models from normative texts", "abstract": "we are concerned with the analysis of normative texts - documents based on the deontic notions of obligation , permission , and prohibition . our goal is to make queries about these notions and verify that a text satisfies certain properties concerning causality of actions and timing constraints . this requires taking the original text and building a representation ( model ) of it in a formal language , in our case the c-o diagram formalism . we present an experimental , semi-automatic aid that helps to bridge the gap between a normative text in natural language and its c-o diagram representation . our approach consists of using dependency structures obtained from the state-of-the-art stanford parser , and applying our own rules and heuristics in order to extract the relevant components . the result is a tabular data structure where each sentence is split into suitable fields , which can then be converted into a c-o diagram . the process is not fully automatic however , and some post-editing is generally required of the user . we apply our tool and perform experiments on documents from different domains , and report an initial evaluation of the accuracy and feasibility of our approach .", "topics": ["heuristic", "causality"]}
{"title": "visualising interactive inferences with idpd3", "abstract": "a large part of the use of knowledge base systems is the interpretation of the output by the end-users and the interaction with these users . even during the development process visualisations can be a great help to the developer . we created idpd3 as a library to visualise models of logic theories . idpd3 is a new version of $ id^ { p } _ { draw } $ and adds support for visualised interactive simulations .", "topics": ["simulation"]}
{"title": "a framework for predicting phishing websites using neural networks", "abstract": "in india many people are now dependent on online banking . this raises security concerns as the banking websites are forged and fraud can be committed by identity theft . these forged websites are called as phishing websites and created by malicious people to mimic web pages of real websites and it attempts to defraud people of their personal information . detecting and identifying phishing websites is a really complex and dynamic problem involving many factors and criteria . this paper discusses about the prediction of phishing websites using neural networks . a neural network is a multilayer system which reduces the error and increases the performance . this paper describes a framework to better classify and predict the phishing sites using neural networks .", "topics": ["neural networks"]}
{"title": "automatic face recognition from video", "abstract": "the objective of this work is to automatically recognize faces from video sequences in a realistic , unconstrained setup in which illumination conditions are extreme and greatly changing , viewpoint and user motion pattern have a wide variability , and video input is of low quality . at the centre of focus are face appearance manifolds : this thesis presents a significant advance of their understanding and application in the sphere of face recognition . the two main contributions are the generic shape-illumination manifold recognition algorithm and the anisotropic manifold space clustering . the generic shape-illumination manifold is evaluated on a large data corpus acquired in real-world conditions and its performance is shown to greatly exceed that of state-of-the-art methods in the literature and the best performing commercial software . empirical evaluation of the anisotropic manifold space clustering on a popular situation comedy is also described with excellent preliminary results .", "topics": ["generative model", "cluster analysis"]}
{"title": "consensus attention-based neural networks for chinese reading comprehension", "abstract": "reading comprehension has embraced a booming in recent nlp research . several institutes have released the cloze-style reading comprehension data , and these have greatly accelerated the research of machine comprehension . in this work , we firstly present chinese reading comprehension datasets , which consist of people daily news dataset and children 's fairy tale ( cft ) dataset . also , we propose a consensus attention-based neural network architecture to tackle the cloze-style reading comprehension problem , which aims to induce a consensus attention over every words in the query . experimental results show that the proposed neural network significantly outperforms the state-of-the-art baselines in several public datasets . furthermore , we setup a baseline for chinese reading comprehension task , and hopefully this would speed up the process for future research .", "topics": ["baseline ( configuration management )", "natural language processing"]}
{"title": "data clustering via principal direction gap partitioning", "abstract": "we explore the geometrical interpretation of the pca based clustering algorithm principal direction divisive partitioning ( pddp ) . we give several examples where this algorithm breaks down , and suggest a new method , gap partitioning , which takes into account natural gaps in the data between clusters . geometric features of the pca space are derived and illustrated and experimental results are given which show our method is comparable on the datasets used in the original paper on pddp .", "topics": ["cluster analysis", "data mining"]}
{"title": "similarity-based multi-label learning", "abstract": "multi-label classification is an important learning problem with many applications . in this work , we propose a principled similarity-based approach for multi-label learning called sml . we also introduce a similarity-based approach for predicting the label set size . the experimental results demonstrate the effectiveness of sml for multi-label classification where it is shown to compare favorably with a wide variety of existing algorithms across a range of evaluation criterion .", "topics": ["eisenstein 's criterion"]}
{"title": "investigating human factors in image forgery detection", "abstract": "in today 's age of internet and social media , one can find an enormous volume of forged images on-line . these images have been used in the past to convey falsified information and achieve harmful intentions . the spread and the effect of the social media only makes this problem more severe . while creating forged images has become easier due to software advancements , there is no automated algorithm which can reliably detect forgery . image forgery detection can be seen as a subset of image understanding problem . human performance is still the gold-standard for these type of problems when compared to existing state-of-art automated algorithms . we conduct a subjective evaluation test with the aid of eye-tracker to investigate into human factors associated with this problem . we compare the performance of an automated algorithm and humans for forgery detection problem . we also develop an algorithm which uses the data from the evaluation test to predict the difficulty-level of an image ( the difficulty-level of an image here denotes how difficult it is for humans to detect forgery in an image . terms such as `` easy/difficult image '' will be used in the same context ) . the experimental results presented in this paper should facilitate development of better algorithms in the future .", "topics": ["computer vision"]}
{"title": "rio : minimizing user interaction in ontology debugging", "abstract": "efficient ontology debugging is a cornerstone for many activities in the context of the semantic web , especially when automatic tools produce ( parts of ) ontologies such as in the field of ontology matching . the best currently known interactive debugging systems rely upon some meta information in terms of fault probabilities , which can speed up the debugging procedure in the good case , but can also have negative impact on the performance in the bad case . the problem is that assessment of the meta information is only possible a-posteriori . consequently , as long as the actual fault is unknown , there is always some risk of suboptimal interactive diagnoses discrimination . as an alternative , one might prefer to rely on a tool which pursues a no-risk strategy . in this case , however , possibly well-chosen meta information can not be exploited , resulting again in inefficient debugging actions . in this work we present a reinforcement learning strategy that continuously adapts its behavior depending on the performance achieved and minimizes the risk of using low-quality meta information . therefore , this method is suitable for application scenarios where reliable a-priori fault estimates are difficult to obtain . using problematic ontologies in the field of ontology matching , we show that the proposed risk-aware query strategy outperforms both active learning approaches and no-risk strategies on average in terms of required amount of user interaction .", "topics": ["reinforcement learning", "iteration"]}
{"title": "autocompletion interfaces make crowd workers slower , but their use promotes response diversity", "abstract": "creative tasks such as ideation or question proposal are powerful applications of crowdsourcing , yet the quantity of workers available for addressing practical problems is often insufficient . to enable scalable crowdsourcing thus requires gaining all possible efficiency and information from available workers . one option for text-focused tasks is to allow assistive technology , such as an autocompletion user interface ( aui ) , to help workers input text responses . but support for the efficacy of auis is mixed . here we designed and conducted a randomized experiment where workers were asked to provide short text responses to given questions . our experimental goal was to determine if an aui helps workers respond more quickly and with improved consistency by mitigating typos and misspellings . surprisingly , we found that neither occurred : workers assigned to the aui treatment were slower than those assigned to the non-aui control and their responses were more diverse , not less , than those of the control . both the lexical and semantic diversities of responses were higher , with the latter measured using word2vec . a crowdsourcer interested in worker speed may want to avoid using an aui , but using an aui to boost response diversity may be valuable to crowdsourcers interested in receiving as much novel information from workers as possible .", "topics": ["scalability"]}
{"title": "crowd counting via scale-adaptive convolutional neural network", "abstract": "the task of crowd counting is to automatically estimate the pedestrian number in crowd images . to cope with the scale and perspective changes that commonly exist in crowd images , state-of-the-art approaches employ multi-column cnn architectures to regress density maps of crowd images . multiple columns have different receptive fields corresponding to pedestrians ( heads ) of different scales . we instead propose a scale-adaptive cnn ( sacnn ) architecture with a backbone of fixed small receptive fields . we extract feature maps from multiple layers and adapt them to have the same output size ; we combine them to produce the final density map . the number of people is computed by integrating the density map . we also introduce a relative count loss along with the density map loss to improve the network generalization on crowd scenes with few pedestrians , where most representative approaches perform poorly on . we conduct extensive experiments on the shanghaitech , ucf_cc_50 and worldexpo datasets as well as a new dataset smartcity that we collect for crowd scenes with few people . the results demonstrate significant improvements of sacnn over the state-of-the-art .", "topics": ["map"]}
{"title": "content-based table retrieval for web queries", "abstract": "understanding the connections between unstructured text and semi-structured table is an important yet neglected problem in natural language processing . in this work , we focus on content-based table retrieval . given a query , the task is to find the most relevant table from a collection of tables . further progress towards improving this area requires powerful models of semantic matching and richer training and evaluation resources . to remedy this , we present a ranking based approach , and implement both carefully designed features and neural network architectures to measure the relevance between a query and the content of a table . furthermore , we release an open-domain dataset that includes 21,113 web queries for 273,816 tables . we conduct comprehensive experiments on both real world and synthetic datasets . results verify the effectiveness of our approach and present the challenges for this task .", "topics": ["natural language processing", "synthetic data"]}
{"title": "diverse landmark sampling from determinantal point processes for scalable manifold learning", "abstract": "high computational costs of manifold learning prohibit its application for large point sets . a common strategy to overcome this problem is to perform dimensionality reduction on selected landmarks and to successively embed the entire dataset with the nystr\\ '' om method . the two main challenges that arise are : ( i ) the landmarks selected in non-euclidean geometries must result in a low reconstruction error , ( ii ) the graph constructed from sparsely sampled landmarks must approximate the manifold well . we propose the sampling of landmarks from determinantal distributions on non-euclidean spaces . since current determinantal sampling algorithms have the same complexity as those for manifold learning , we present an efficient approximation running in linear time . further , we recover the local geometry after the sparsification by assigning each landmark a local covariance matrix , estimated from the original point set . the resulting neighborhood selection based on the bhattacharyya distance improves the embedding of sparsely sampled manifolds . our experiments show a significant performance improvement compared to state-of-the-art landmark selection techniques .", "topics": ["sampling ( signal processing )", "time complexity"]}
{"title": "reinforcement learning for transition-based mention detection", "abstract": "this paper describes an application of reinforcement learning to the mention detection task . we define a novel action-based formulation for the mention detection task , in which a model can flexibly revise past labeling decisions by grouping together tokens and assigning partial mention labels . we devise a method to create mention-level episodes and we train a model by rewarding correctly labeled complete mentions , irrespective of the inner structure created . the model yields results which are on par with a competitive supervised counterpart while being more flexible in terms of achieving targeted behavior through reward modeling and generating internal mention structure , especially on longer mentions .", "topics": ["supervised learning", "reinforcement learning"]}
{"title": "syllable parsing in english and french", "abstract": "in this paper i argue that optimality theory provides for an explanatory model of syllabic parsing in english and french . the argument is based on psycholinguistic facts that have been mysterious up to now . this argument is further buttressed by the computational implementation developed here . this model is important for several reasons . first , it provides a demonstration of how ot can be used in a performance domain . second , it suggests a new relationship between phonological theory and psycholinguistics . ( code in perl is included and a www-interface is running at http : //mayo.douglass.arizona.edu . )", "topics": ["parsing"]}
{"title": "learning a dilated residual network for sar image despeckling", "abstract": "in this paper , to break the limit of the traditional linear models for synthetic aperture radar ( sar ) image despeckling , we propose a novel deep learning approach by learning a non-linear end-to-end mapping between the noisy and clean sar images with a dilated residual network ( sar-drn ) . sar-drn is based on dilated convolutions , which can both enlarge the receptive field and maintain the filter size and layer depth with a lightweight structure . in addition , skip connections and residual learning strategy are added to the despeckling model to maintain the image details and reduce the vanishing gradient problem . compared with the traditional despeckling methods , the proposed method shows superior performance over the state-of-the-art methods on both quantitative and visual assessments , especially for strong speckle noise .", "topics": ["noise reduction", "nonlinear system"]}
{"title": "logical inferences with contexts of rdf triples", "abstract": "logical inference , an integral feature of the semantic web , is the process of deriving new triples by applying entailment rules on knowledge bases . the entailment rules are determined by the model-theoretic semantics . incorporating context of an rdf triple ( e.g . , provenance , time , and location ) into the inferencing process requires the formal semantics to be capable of describing the context of rdf triples also in the form of triples , or in other words , rdf contextual triples about triples . the formal semantics should also provide the rules that could entail new contextual triples about triples . in this paper , we propose the first inferencing mechanism that allows context of rdf triples , represented in the form of rdf triples about triples , to be the first-class citizens in the model-theoretic semantics and in the logical rules . our inference mechanism is well-formalized with all new concepts being captured in the model-theoretic semantics . this formal semantics also allows us to derive a new set of entailment rules that could entail new contextual triples about triples . to demonstrate the feasibility and the scalability of the proposed mechanism , we implement a new tool in which we transform the existing knowledge bases to our representation of rdf triples about triples and provide the option for this tool to compute the inferred triples for the proposed rules . we evaluate the computation of the proposed rules on a large scale using various real-world knowledge bases such as bio2rdf ncbi genes and dbpedia . the results show that the computation of the inferred triples can be highly scalable . on average , one billion inferred triples adds 5-6 minutes to the overall transformation process . ncbi genes , with 20 billion triples in total , took only 232 minutes for the transformation of 12 billion triples and added 42 minutes for inferring 8 billion triples to the overall process .", "topics": ["computation", "scalability"]}
{"title": "a unifying framework in vector-valued reproducing kernel hilbert spaces for manifold regularization and co-regularized multi-view learning", "abstract": "this paper presents a general vector-valued reproducing kernel hilbert spaces ( rkhs ) framework for the problem of learning an unknown functional dependency between a structured input space and a structured output space . our formulation encompasses both vector-valued manifold regularization and co-regularized multi-view learning , providing in particular a unifying framework linking these two important learning approaches . in the case of the least square loss function , we provide a closed form solution , which is obtained by solving a system of linear equations . in the case of support vector machine ( svm ) classification , our formulation generalizes in particular both the binary laplacian svm to the multi-class , multi-view settings and the multi-class simplex cone svm to the semi-supervised , multi-view settings . the solution is obtained by solving a single quadratic optimization problem , as in standard svm , via the sequential minimal optimization ( smo ) approach . empirical results obtained on the task of object recognition , using several challenging datasets , demonstrate the competitiveness of our algorithms compared with other state-of-the-art methods .", "topics": ["optimization problem", "support vector machine"]}
{"title": "replication study : development and validation of deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs", "abstract": "we have replicated some experiments in 'development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs ' that was published in jama 2016 ; 316 ( 22 ) . we re-implemented the methods since the source code is not available . the original study used fundus images from eyepacs and three hospitals in india for training their detection algorithm . we used a different eyepacs data set that was made available in a kaggle competition . for evaluating the algorithm 's performance the benchmark data set messidor-2 was used . we used the similar messidor-original data set to evaluate our algorithm 's performance . in the original study licensed ophthalmologists re-graded all their obtained images for diabetic retinopathy , macular edema , and image gradability . our challenge was to re-implement the methods with publicly available data sets and one diabetic retinopathy grade per image , find the hyper-parameter settings for training and validation that were not described in the original study , and make an assessment on the impact of training with ungradable images . we were not able to reproduce the performance as reported in the original study . we believe our model did not learn to recognize lesions in fundus images , since we only had a singular grade for diabetic retinopathy per image , instead of multiple grades per images . furthermore , the original study missed details regarding hyper-parameter settings for training and validation . the original study may also have used image quality grades as input for training the network . we believe that deep learning algorithms should be easily replicated , and that ideally source code should be published so that other researchers can confirm the results of the experiments . our source code and instructions for running the replication are available at : https : //github.com/mikevoets/jama16-retina-replication .", "topics": ["test set"]}
{"title": "on the relations of correlation filter based trackers and struck", "abstract": "in recent years , two types of trackers , namely correlation filter based tracker ( cf tracker ) and structured output tracker ( struck ) , have exhibited the state-of-the-art performance . however , there seems to be lack of analytic work on their relations in the computer vision community . in this paper , we investigate two state-of-the-art cf trackers , i.e . , spatial regularization discriminative correlation filter ( srdcf ) and correlation filter with limited boundaries ( cflb ) , and struck , and reveal their relations . specifically , after extending the cflb to its multiple channel version we prove the relation between srdcf and cflb on the condition that the spatial regularization factor of srdcf is replaced by the masking matrix of cflb . we also prove the asymptotical approximate relation between srdcf and struck on the conditions that the spatial regularization factor of srdcf is replaced by an indicator function of object bounding box , the weights of srdcf in its loss item are replaced by those of struck , the linear kernel is employed by struck , and the search region tends to infinity . extensive experiments on public benchmarks otb50 and otb100 are conducted to verify our theoretical results . moreover , we explain how detailed differences among srdcf , cflb , and struck would give rise to slightly different performances on visual sequences", "topics": ["approximation algorithm", "matrix regularization"]}
{"title": "enhancing the lexvec distributed word representation model using positional contexts and external memory", "abstract": "in this paper we take a state-of-the-art model for distributed word representation that explicitly factorizes the positive pointwise mutual information ( ppmi ) matrix using window sampling and negative sampling and address two of its shortcomings . we improve syntactic performance by using positional contexts , and solve the need to store the ppmi matrix in memory by working on aggregate data in external memory . the effectiveness of both modifications is shown using word similarity and analogy tasks .", "topics": ["sampling ( signal processing )"]}
{"title": "geometry of online packing linear programs", "abstract": "we consider packing lp 's with $ m $ rows where all constraint coefficients are normalized to be in the unit interval . the n columns arrive in random order and the goal is to set the corresponding decision variables irrevocably when they arrive so as to obtain a feasible solution maximizing the expected reward . previous ( 1 - \\epsilon ) -competitive algorithms require the right-hand side of the lp to be omega ( ( m/\\epsilon^2 ) log ( n/\\epsilon ) ) , a bound that worsens with the number of columns and rows . however , the dependence on the number of columns is not required in the single-row case and known lower bounds for the general case are also independent of n . our goal is to understand whether the dependence on n is required in the multi-row case , making it fundamentally harder than the single-row version . we refute this by exhibiting an algorithm which is ( 1 - \\epsilon ) -competitive as long as the right-hand sides are omega ( ( m^2/\\epsilon^2 ) log ( m/\\epsilon ) ) . our techniques refine previous pac-learning based approaches which interpret the online decisions as linear classifications of the columns based on sampled dual prices . the key ingredient of our improvement comes from a non-standard covering argument together with the realization that only when the columns of the lp belong to few 1-d subspaces we can obtain small such covers ; bounding the size of the cover constructed also relies on the geometry of linear classifiers . general packing lp 's are handled by perturbing the input columns , which can be seen as making the learning problem more robust .", "topics": ["coefficient"]}
{"title": "emergence of grid-like representations by training recurrent neural networks to perform spatial localization", "abstract": "decades of research on the neural code underlying spatial navigation have revealed a diverse set of neural response properties . the entorhinal cortex ( ec ) of the mammalian brain contains a rich set of spatial correlates , including grid cells which encode space using tessellating patterns . however , the mechanisms and functional significance of these spatial representations remain largely mysterious . as a new way to understand these neural representations , we trained recurrent neural networks ( rnns ) to perform navigation tasks in 2d arenas based on velocity inputs . surprisingly , we find that grid-like spatial response patterns emerge in trained networks , along with units that exhibit other spatial correlates , including border cells and band-like cells . all these different functional types of neurons have been observed experimentally . the order of the emergence of grid-like and border cells is also consistent with observations from developmental studies . together , our results suggest that grid cells , border cells and others as observed in ec may be a natural solution for representing space efficiently given the predominant recurrent connections in the neural circuits .", "topics": ["recurrent neural network"]}
{"title": "multi-font multi-size kannada numeral recognition based on structural features", "abstract": "in this paper a fast and novel method is proposed for multi-font multi-size kannada numeral recognition which is thinning free and without size normalization approach . the different structural feature are used for numeral recognition namely , directional density of pixels in four directions , water reservoirs , maximum profile distances , and fill hole density are used for the recognition of kannada numerals . a euclidian minimum distance criterion is used to find minimum distances and k-nearest neighbor classifier is used to classify the kannada numerals by varying the size of numeral image from 16 to 50 font sizes for the 20 different font styles from nudi and baraha popular word processing kannada software . the total 1150 numeral images are tested and the overall accuracy of classification is found to be 100 % . the average time taken by this method is 0.1476 seconds .", "topics": ["eisenstein 's criterion", "pixel"]}
{"title": "reduced egomotion estimation drift using omnidirectional views", "abstract": "estimation of camera motion from a given image sequence becomes degraded as the length of the sequence increases . in this letter , this phenomenon is demonstrated and an approach to increase the estimation accuracy is proposed . the proposed method uses an omnidirectional camera in addition to the perspective one and takes advantage of its enlarged view by exploiting the correspondences between the omnidirectional and perspective images . simulated and real image experiments show that the proposed approach improves the estimation accuracy .", "topics": ["simulation", "computer vision"]}
{"title": "interactive illumination invariance", "abstract": "illumination effects cause problems for many computer vision algorithms . we present a user-friendly interactive system for robust illumination-invariant image generation . compared with the previous automated illumination-invariant image derivation approaches , our system enables users to specify a particular kind of illumination variation for removal . the derivation of illumination-invariant image is guided by the user input . the input is a stroke that defines an area covering a set of pixels whose intensities are influenced predominately by the illumination variation . this additional flexibility enhances the robustness for processing non-linearly rendered images and the images of the scenes where their illumination variations are difficult to estimate automatically . finally , we present some evaluation results of our method .", "topics": ["image segmentation", "mathematical optimization"]}
{"title": "watch your step : learning graph embeddings through attention", "abstract": "graph embedding methods represent nodes in a continuous vector space , preserving information from the graph ( e.g . by sampling random walks ) . there are many hyper-parameters to these methods ( such as random walk length ) which have to be manually tuned for every graph . in this paper , we replace random walk hyper-parameters with trainable parameters that we automatically learn via backpropagation . in particular , we learn a novel attention model on the power series of the transition matrix , which guides the random walk to optimize an upstream objective . unlike previous approaches to attention models , the method that we propose utilizes attention parameters exclusively on the data ( e.g . on the random walk ) , and not used by the model for inference . we experiment on link prediction tasks , as we aim to produce embeddings that best-preserve the graph structure , generalizing to unseen information . we improve state-of-the-art on a comprehensive suite of real world datasets including social , collaboration , and biological networks . adding attention to random walks can reduce the error by 20 % to 45 % on datasets we attempted . further , our learned attention parameters are different for every graph , and our automatically-found values agree with the optimal choice of hyper-parameter if we manually tune existing methods .", "topics": ["sampling ( signal processing )"]}
{"title": "alternative structures for character-level rnns", "abstract": "recurrent neural networks are convenient and efficient models for language modeling . however , when applied on the level of characters instead of words , they suffer from several problems . in order to successfully model long-term dependencies , the hidden representation needs to be large . this in turn implies higher computational costs , which can become prohibitive in practice . we propose two alternative structural modifications to the classical rnn model . the first one consists on conditioning the character level representation on the previous word representation . the other one uses the character history to condition the output probability . we evaluate the performance of the two proposed modifications on challenging , multi-lingual real world data .", "topics": ["recurrent neural network"]}
{"title": "learning hidden unit contributions for unsupervised acoustic model adaptation", "abstract": "this work presents a broad study on the adaptation of neural network acoustic models by means of learning hidden unit contributions ( lhuc ) -- a method that linearly re-combines hidden units in a speaker- or environment-dependent manner using small amounts of unsupervised adaptation data . we also extend lhuc to a speaker adaptive training ( sat ) framework that leads to a more adaptable dnn acoustic model , working both in a speaker-dependent and a speaker-independent manner , without the requirements to maintain auxiliary speaker-dependent feature extractors or to introduce significant speaker-dependent changes to the dnn structure . through a series of experiments on four different speech recognition benchmarks ( ted talks , switchboard , ami meetings , and aurora4 ) comprising 270 test speakers , we show that lhuc in both its test-only and sat variants results in consistent word error rate reductions ranging from 5 % to 23 % relative depending on the task and the degree of mismatch between training and test data . in addition , we have investigated the effect of the amount of adaptation data per speaker , the quality of unsupervised adaptation targets , the complementarity to other adaptation techniques , one-shot adaptation , and an extension to adapting dnns trained in a sequence discriminative manner .", "topics": ["unsupervised learning", "feature extraction"]}
{"title": "comparative analysis of viterbi training and maximum likelihood estimation for hmms", "abstract": "we present an asymptotic analysis of viterbi training ( vt ) and contrast it with a more conventional maximum likelihood ( ml ) approach to parameter estimation in hidden markov models . while ml estimator works by ( locally ) maximizing the likelihood of the observed data , vt seeks to maximize the probability of the most likely hidden state sequence . we develop an analytical framework based on a generating function formalism and illustrate it on an exactly solvable model of hmm with one unambiguous symbol . for this particular model the ml objective function is continuously degenerate . vt objective , in contrast , is shown to have only finite degeneracy . furthermore , vt converges faster and results in sparser ( simpler ) models , thus realizing an automatic occam 's razor for hmm learning . for more general scenario vt can be worse compared to ml but still capable of correctly recovering most of the parameters .", "topics": ["loss function"]}
{"title": "t-cnn : tubelets with convolutional neural networks for object detection from videos", "abstract": "the state-of-the-art performance for object detection has been significantly improved over the past two years . besides the introduction of powerful deep neural networks such as googlenet and vgg , novel object detection frameworks such as r-cnn and its successors , fast r-cnn and faster r-cnn , play an essential role in improving the state-of-the-art . despite their effectiveness on still images , those frameworks are not specifically designed for object detection from videos . temporal and contextual information of videos are not fully investigated and utilized . in this work , we propose a deep learning framework that incorporates temporal and contextual information from tubelets obtained in videos , which dramatically improves the baseline performance of existing still-image detection frameworks when they are applied to videos . it is called t-cnn , i.e . tubelets with convolutional neueral networks . the proposed framework won the recently introduced object-detection-from-video ( vid ) task with provided data in the imagenet large-scale visual recognition challenge 2015 ( ilsvrc2015 ) .", "topics": ["baseline ( configuration management )", "object detection"]}
{"title": "spatial contrasting for deep unsupervised learning", "abstract": "convolutional networks have marked their place over the last few years as the best performing model for various visual tasks . they are , however , most suited for supervised learning from large amounts of labeled data . previous attempts have been made to use unlabeled data to improve model performance by applying unsupervised techniques . these attempts require different architectures and training methods . in this work we present a novel approach for unsupervised training of convolutional networks that is based on contrasting between spatial regions within images . this criterion can be employed within conventional neural networks and trained using standard techniques such as sgd and back-propagation , thus complementing supervised methods .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "extremal optimization : an evolutionary local-search algorithm", "abstract": "a recently introduced general-purpose heuristic for finding high-quality solutions for many hard optimization problems is reviewed . the method is inspired by recent progress in understanding far-from-equilibrium phenomena in terms of { \\em self-organized criticality , } a concept introduced to describe emergent complexity in physical systems . this method , called { \\em extremal optimization , } successively replaces the value of extremely undesirable variables in a sub-optimal solution with new , random ones . large , avalanche-like fluctuations in the cost function self-organize from this dynamics , effectively scaling barriers to explore local optima in distant neighborhoods of the configuration space while eliminating the need to tune parameters . drawing upon models used to simulate the dynamics of granular media , evolution , or geology , extremal optimization complements approximation methods inspired by equilibrium statistical physics , such as { \\em simulated annealing } . it may be but one example of applying new insights into { \\em non-equilibrium phenomena } systematically to hard optimization problems . this method is widely applicable and so far has proved competitive with -- and even superior to -- more elaborate general-purpose heuristics on testbeds of constrained optimization problems with up to $ 10^5 $ variables , such as bipartitioning , coloring , and satisfiability . analysis of a suitable model predicts the only free parameter of the method in accordance with all experimental results .", "topics": ["optimization problem", "simulation"]}
{"title": "do deep neural networks suffer from crowding ?", "abstract": "crowding is a visual effect suffered by humans , in which an object that can be recognized in isolation can no longer be recognized when other objects , called flankers , are placed close to it . in this work , we study the effect of crowding in artificial deep neural networks for object recognition . we analyze both standard deep convolutional neural networks ( dcnns ) as well as a new version of dcnns which is 1 ) multi-scale and 2 ) with size of the convolution filters change depending on the eccentricity wrt to the center of fixation . such networks , that we call eccentricity-dependent , are a computational model of the feedforward path of the primate visual cortex . our results reveal that the eccentricity-dependent model , trained on target objects in isolation , can recognize such targets in the presence of flankers , if the targets are near the center of the image , whereas dcnns can not . also , for all tested networks , when trained on targets in isolation , we find that recognition accuracy of the networks decreases the closer the flankers are to the target and the more flankers there are . we find that visual similarity between the target and flankers also plays a role and that pooling in early layers of the network leads to more crowding . additionally , we show that incorporating the flankers into the images of the training set does not improve performance with crowding .", "topics": ["neural networks", "computer vision"]}
{"title": "optimizing the trade-off between single-stage and two-stage object detectors using image difficulty prediction", "abstract": "there are mainly two types of state-of-the-art object detectors . on one hand , we have two-stage detectors , such as faster r-cnn ( region-based convolutional neural networks ) or mask r-cnn , that ( i ) use a region proposal network to generate regions of interests in the first stage and ( ii ) send the region proposals down the pipeline for object classification and bounding-box regression . such models reach the highest accuracy rates , but are typically slower . on the other hand , we have single-stage detectors , such as yolo ( you only look once ) and ssd ( singe shot multibox detector ) , that treat object detection as a simple regression problem which takes an input image and learns the class probabilities and bounding box coordinates . such models reach lower accuracy rates , but are much faster than two-stage object detectors . in this paper , we propose to use an image difficulty predictor to achieve an optimal trade-off between accuracy and speed in object detection . the image difficulty predictor is applied on the test images to split them into easy versus hard images . once separated , the easy images are sent to the faster single-stage detector , while the hard images are sent to the more accurate two-stage detector . our experiments on pascal voc 2007 show that using image difficulty compares favorably to a random split of the images . our method is flexible , in that it allows to choose a desired threshold for splitting the images into easy versus hard .", "topics": ["object detection"]}
{"title": "encoding higher level extensions of petri nets in answer set programming", "abstract": "answering realistic questions about biological systems and pathways similar to the ones used by text books to test understanding of students about biological systems is one of our long term research goals . often these questions require simulation based reasoning . to answer such questions , we need formalisms to build pathway models , add extensions , simulate , and reason with them . we chose petri nets and answer set programming ( asp ) as suitable formalisms , since petri net models are similar to biological pathway diagrams ; and asp provides easy extension and strong reasoning abilities . we found that certain aspects of biological pathways , such as locations and substance types , can not be represented succinctly using regular petri nets . as a result , we need higher level constructs like colored tokens . in this paper , we show how petri nets with colored tokens can be encoded in asp in an intuitive manner , how additional petri net extensions can be added by making small code changes , and how this work furthers our long term research goals . our approach can be adapted to other domains with similar modeling needs .", "topics": ["simulation"]}
{"title": "evolutionary generative adversarial networks", "abstract": "generative adversarial networks ( gan ) have been effective for learning generative models for real-world data . however , existing gans ( gan and its variants ) tend to suffer from training problems such as instability and mode collapse . in this paper , we propose a novel gan framework called evolutionary generative adversarial networks ( e-gan ) for stable gan training and improved generative performance . unlike existing gans , which employ a pre-defined adversarial objective function alternately training a generator and a discriminator , we utilize different adversarial training objectives as mutation operations and evolve a population of generators to adapt to the environment ( i.e . , the discriminator ) . we also utilize an evaluation mechanism to measure the quality and diversity of generated samples , such that only well-performing generator ( s ) are preserved and used for further training . in this way , e-gan overcomes the limitations of an individual adversarial training objective and always preserves the best offspring , contributing to progress in and the success of gans . experiments on several datasets demonstrate that e-gan achieves convincing generative performance and reduces the training problems inherent in existing gans .", "topics": ["optimization problem", "loss function"]}
{"title": "case-based subgoaling in real-time heuristic search for video game pathfinding", "abstract": "real-time heuristic search algorithms satisfy a constant bound on the amount of planning per action , independent of problem size . as a result , they scale up well as problems become larger . this property would make them well suited for video games where artificial intelligence controlled agents must react quickly to user commands and to other agents actions . on the downside , real-time search algorithms employ learning methods that frequently lead to poor solution quality and cause the agent to appear irrational by re-visiting the same problem states repeatedly . the situation changed recently with a new algorithm , d lrta* , which attempted to eliminate learning by automatically selecting subgoals . d lrta* is well poised for video games , except it has a complex and memory-demanding pre-computation phase during which it builds a database of subgoals . in this paper , we propose a simpler and more memory-efficient way of pre-computing subgoals thereby eliminating the main obstacle to applying state-of-the-art real-time search methods in video games . the new algorithm solves a number of randomly chosen problems off-line , compresses the solutions into a series of subgoals and stores them in a database . when presented with a novel problem on-line , it queries the database for the most similar previously solved case and uses its subgoals to solve the problem . in the domain of pathfinding on four large video game maps , the new algorithm delivers solutions eight times better while using 57 times less memory and requiring 14 % less pre-computation time .", "topics": ["time complexity", "computation"]}
{"title": "near-infrared image dehazing via color regularization", "abstract": "near-infrared imaging can capture haze-free near-infrared gray images and visible color images , according to physical scattering models , e.g . , rayleigh or mie models . however , there exist serious discrepancies in brightness and image structures between the near-infrared gray images and the visible color images . the direct use of the near-infrared gray images brings about another color distortion problem in the dehazed images . therefore , the color distortion should also be considered for near-infrared dehazing . to reflect this point , this paper presents an approach of adding a new color regularization to conventional dehazing framework . the proposed color regularization can model the color prior for unknown haze-free images from two captured images . thus , natural-looking colors and fine details can be induced on the dehazed images . the experimental results show that the proposed color regularization model can help remove the color distortion and the haze at the same time . also , the effectiveness of the proposed color regularization is verified by comparing with other conventional regularizations . it is also shown that the proposed color regularization can remove the edge artifacts which arise from the use of the conventional dark prior model .", "topics": ["matrix regularization"]}
{"title": "a comparison of word embeddings for english and cross-lingual chinese word sense disambiguation", "abstract": "word embeddings are now ubiquitous forms of word representation in natural language processing . there have been applications of word embeddings for monolingual word sense disambiguation ( wsd ) in english , but few comparisons have been done . this paper attempts to bridge that gap by examining popular embeddings for the task of monolingual english wsd . our simplified method leads to comparable state-of-the-art performance without expensive retraining . cross-lingual wsd - where the word senses of a word in a source language e come from a separate target translation language f - can also assist in language learning ; for example , when providing translations of target vocabulary for learners . thus we have also applied word embeddings to the novel task of cross-lingual wsd for chinese and provide a public dataset for further benchmarking . we have also experimented with using word embeddings for lstm networks and found surprisingly that a basic lstm network does not work well . we discuss the ramifications of this outcome .", "topics": ["natural language processing"]}
{"title": "augmented reality meets computer vision : efficient data generation for urban driving scenes", "abstract": "the success of deep learning in computer vision is based on availability of large annotated datasets . to lower the need for hand labeled images , virtually rendered 3d worlds have recently gained popularity . creating realistic 3d content is challenging on its own and requires significant human effort . in this work , we propose an alternative paradigm which combines real and synthetic data for learning semantic instance segmentation and object detection models . exploiting the fact that not all aspects of the scene are equally important for this task , we propose to augment real-world imagery with virtual objects of the target category . capturing real-world images at large scale is easy and cheap , and directly provides real background appearances without the need for creating complex 3d models of the environment . we present an efficient procedure to augment real images with virtual objects . this allows us to create realistic composite images which exhibit both realistic background appearance and a large number of complex object arrangements . in contrast to modeling complete 3d environments , our augmentation approach requires only a few user interactions in combination with 3d shapes of the target object . through extensive experimentation , we conclude the right set of parameters to produce augmented data which can maximally enhance the performance of instance segmentation models . further , we demonstrate the utility of our approach on training standard deep models for semantic instance segmentation and object detection of cars in outdoor driving scenes . we test the models trained on our augmented data on the kitti 2015 dataset , which we have annotated with pixel-accurate ground truth , and on cityscapes dataset . our experiments demonstrate that models trained on augmented imagery generalize better than those trained on synthetic data or models trained on limited amount of annotated real data .", "topics": ["object detection", "synthetic data"]}
{"title": "deep neural networks in fully connected crf for image labeling with social network metadata", "abstract": "we propose a novel method for predicting image labels by fusing image content descriptors with the social media context of each image . an image uploaded to a social media site such as flickr often has meaningful , associated information , such as comments and other images the user has uploaded , that is complementary to pixel content and helpful in predicting labels . prediction challenges such as imagenet~\\cite { imagenet_cvpr09 } and mscoco~\\cite { linmbhprdz : eccv14 } use only pixels , while other methods make predictions purely from social media context \\cite { mcauleyeccv12 } . our method is based on a novel fully connected conditional random field ( crf ) framework , where each node is an image , and consists of two deep convolutional neural networks ( cnn ) and one recurrent neural network ( rnn ) that model both textual and visual node/image information . the edge weights of the crf graph represent textual similarity and link-based metadata such as user sets and image groups . we model the crf as an rnn for both learning and inference , and incorporate the weighted ranking loss and cross entropy loss into the crf parameter optimization to handle the training data imbalance issue . our proposed approach is evaluated on the mir-9k dataset and experimentally outperforms current state-of-the-art approaches .", "topics": ["test set", "recurrent neural network"]}
{"title": "a century of portraits : a visual historical record of american high school yearbooks", "abstract": "many details about our world are not captured in written records because they are too mundane or too abstract to describe in words . fortunately , since the invention of the camera , an ever-increasing number of photographs capture much of this otherwise lost information . this plethora of artifacts documenting our `` visual culture '' is a treasure trove of knowledge as yet untapped by historians . we present a dataset of 37,921 frontal-facing american high school yearbook photos that allow us to use computation to glimpse into the historical visual record too voluminous to be evaluated manually . the collected portraits provide a constant visual frame of reference with varying content . we can therefore use them to consider issues such as a decade 's defining style elements , or trends in fashion and social norms over time . we demonstrate that our historical image dataset may be used together with weakly-supervised data-driven techniques to perform scalable historical analysis of large image corpora with minimal human effort , much in the same way that large text corpora together with natural language processing revolutionized historians ' workflow . furthermore , we demonstrate the use of our dataset in dating grayscale portraits using deep learning methods .", "topics": ["statistical classification"]}
{"title": "adaptive learning in cartesian product of reproducing kernel hilbert spaces", "abstract": "we propose a novel adaptive learning algorithm based on iterative orthogonal projections in the cartesian product of multiple reproducing kernel hilbert spaces ( rkhss ) . the task is estimating/tracking nonlinear functions which are supposed to contain multiple components such as ( i ) linear and nonlinear components , ( ii ) high- and low- frequency components etc . in this case , the use of multiple rkhss permits a compact representation of multicomponent functions . the proposed algorithm is where two different methods of the author meet : multikernel adaptive filtering and the algorithm of hyperplane projection along affine subspace ( hypass ) . in a certain particular case , the sum space of the rkhss is isomorphic to the product space and hence the proposed algorithm can also be regarded as an iterative projection method in the sum space . the efficacy of the proposed algorithm is shown by numerical examples .", "topics": ["nonlinear system", "numerical analysis"]}
{"title": "t-conv : a convolutional neural network for multi-scale taxi trajectory prediction", "abstract": "precise destination prediction of taxi trajectories can benefit many intelligent location based services such as accurate ad for passengers . traditional prediction approaches , which treat trajectories as one-dimensional sequences and process them in single scale , fail to capture the diverse two-dimensional patterns of trajectories in different spatial scales . in this paper , we propose t-conv which models trajectories as two-dimensional images , and adopts multi-layer convolutional neural networks to combine multi-scale trajectory patterns to achieve precise prediction . furthermore , we conduct gradient analysis to visualize the multi-scale spatial patterns captured by t-conv and extract the areas with distinct influence on the ultimate prediction . finally , we integrate multiple local enhancement convolutional fields to explore these important areas deeply for better prediction . comprehensive experiments based on real trajectory data show that t-conv can achieve higher accuracy than the state-of-the-art methods .", "topics": ["gradient"]}
{"title": "agnostic system identification for model-based reinforcement learning", "abstract": "a fundamental problem in control is to learn a model of a system from observations that is useful for controller synthesis . to provide good performance guarantees , existing methods must assume that the real system is in the class of models considered during learning . we present an iterative method with strong guarantees even in the agnostic case where the system is not in the class . in particular , we show that any no-regret online learning algorithm can be used to obtain a near-optimal policy , provided some model achieves low training error and access to a good exploration distribution . our approach applies to both discrete and continuous domains . we demonstrate its efficacy and scalability on a challenging helicopter domain from the literature .", "topics": ["regret ( decision theory )", "reinforcement learning"]}
{"title": "quantifying natural and artificial intelligence in robots and natural systems with an algorithmic behavioural test", "abstract": "one of the most important aims of the fields of robotics , artificial intelligence and artificial life is the design and construction of systems and machines as versatile and as reliable as living organisms at performing high level human-like tasks . but how are we to evaluate artificial systems if we are not certain how to measure these capacities in living systems , let alone how to define life or intelligence ? here i survey a concrete metric towards measuring abstract properties of natural and artificial systems , such as the ability to react to the environment and to control one 's own behaviour .", "topics": ["artificial intelligence", "robot"]}
{"title": "sparse hierachical extrapolated parametric methods for cortical data analysis", "abstract": "many neuroimaging studies focus on the cortex , in order to benefit from better signal to noise ratios and reduced computational burden . cortical data are usually projected onto a reference mesh , where subsequent analyses are carried out . several multiscale approaches have been proposed for analyzing these surface data , such as spherical harmonics and graph wavelets . as far as we know , however , the hierarchical structure of the template icosahedral meshes used by most neuroimaging software has never been exploited for cortical data factorization . in this paper , we demonstrate how the structure of the ubiquitous icosahedral meshes can be exploited by data factorization methods such as sparse dictionary learning , and we assess the optimization speed-up offered by extrapolation methods in this context . by testing different sparsity-inducing norms , extrapolation methods , and factorization schemes , we compare the performances of eleven methods for analyzing four datasets : two structural and two functional mri datasets obtained by processing the data publicly available for the hundred unrelated subjects of the human connectome project . our results demonstrate that , depending on the level of details requested , a speedup of several orders of magnitudes can be obtained .", "topics": ["sparse matrix"]}
{"title": "similarity-driven cluster merging method for unsupervised fuzzy clustering", "abstract": "in this paper , a similarity-driven cluster merging method is proposed for unsuper-vised fuzzy clustering . the cluster merging method is used to resolve the problem of cluster validation . starting with an overspecified number of clusters in the data , pairs of similar clusters are merged based on the proposed similarity-driven cluster merging criterion . the similarity between clusters is calculated by a fuzzy cluster similarity matrix , while an adaptive threshold is used for merging . in addition , a modified generalized ob- jective function is used for prototype-based fuzzy clustering . the function includes the p-norm distance measure as well as principal components of the clusters . the number of the principal components is determined automatically from the data being clustered . the properties of this unsupervised fuzzy clustering algorithm are illustrated by several experiments .", "topics": ["cluster analysis", "loss function"]}
{"title": "fast k-nn search", "abstract": "efficient index structures for fast approximate nearest neighbor queries are required in many applications such as recommendation systems . in high-dimensional spaces , many conventional methods suffer from excessive usage of memory and slow response times . we propose a method where multiple random projection trees are combined by a novel voting scheme . the key idea is to exploit the redundancy in a large number of candidate sets obtained by independently generated random projections in order to reduce the number of expensive exact distance evaluations . the method is straightforward to implement using sparse projections which leads to a reduced memory footprint and fast index construction . furthermore , it enables grouping of the required computations into big matrix multiplications , which leads to additional savings due to cache effects and low-level parallelization . we demonstrate by extensive experiments on a wide variety of data sets that the method is faster than existing partitioning tree or hashing based approaches , making it the fastest available technique on high accuracy levels .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "automatically building face datasets of new domains from weakly labeled data with pretrained models", "abstract": "training data are critical in face recognition systems . however , labeling a large scale face data for a particular domain is very tedious . in this paper , we propose a method to automatically and incrementally construct datasets from massive weakly labeled data of the target domain which are readily available on the internet under the help of a pretrained face model . more specifically , given a large scale weakly labeled dataset in which each face image is associated with a label , i.e . the name of an identity , we create a graph for each identity with edges linking matched faces verified by the existing model under a tight threshold . then we use the maximal subgraph as the cleaned data for that identity . with the cleaned dataset , we update the existing face model and use the new model to filter the original dataset to get a larger cleaned dataset . we collect a large weakly labeled dataset containing 530,560 asian face images of 7,962 identities from the internet , which will be published for the study of face recognition . by running the filtering process , we obtain a cleaned datasets ( 99.7+ % purity ) of size 223,767 ( recall 70.9 % ) . on our testing dataset of asian faces , the model trained by the cleaned dataset achieves recognition rate 93.1 % , which obviously outperforms the model trained by the public dataset casia whose recognition rate is 85.9 % .", "topics": ["test set"]}
{"title": "research on the fast fourier transform of image based on gpu", "abstract": "study of general purpose computation by gpu ( graphics processing unit ) can improve the image processing capability of micro-computer system . this paper studies the parallelism of the different stages of decimation in time radix 2 fft algorithm , designs the butterfly and scramble kernels and implements 2d fft on gpu . the experiment result demonstrates the validity and advantage over general cpu , especially in the condition of large input size . the approach can also be generalized to other transforms alike .", "topics": ["image processing", "computational complexity theory"]}
{"title": "detection of critical number of people in interlocked doors for security access control by exploiting a microwave transceiver-array", "abstract": "counting the number of people is something many security application focus on , when dealing with controlling accesses in restricted areas , as it occurs with banks , airports , railway stations and governmental offices . this paper presents an automated solution for detecting the presence of more than one person into interlocked doors adopted in many accesses . in most cases , interlocked doors are small areas where other pieces of information and sensors are placed in order to detect the presence of guns , explosive , etc . the general goals and the required environmental condition , allowed us to implement a detection system at lower costs and complexity , with respect to other existing techniques . the system consists of a fixed array of microwave transceiver modules , whose received signals are processed to collect information related to a sort of volume occupied in the interlocked door cabin . the proposed solution has been statistically validated by using statistical analysis . the whole solution has been also implemented to be used in a real time environment and thus validated against real experimental measures .", "topics": ["sensor"]}
{"title": "simultaneous safe screening of features and samples in doubly sparse modeling", "abstract": "the problem of learning a sparse model is conceptually interpreted as the process of identifying active features/samples and then optimizing the model over them . recently introduced safe screening allows us to identify a part of non-active features/samples . so far , safe screening has been individually studied either for feature screening or for sample screening . in this paper , we introduce a new approach for safely screening features and samples simultaneously by alternatively iterating feature and sample screening steps . a significant advantage of considering them simultaneously rather than individually is that they have a synergy effect in the sense that the results of the previous safe feature screening can be exploited for improving the next safe sample screening performances , and vice-versa . we first theoretically investigate the synergy effect , and then illustrate the practical advantage through intensive numerical experiments for problems with large numbers of features and samples .", "topics": ["numerical analysis", "sparse matrix"]}
{"title": "nested graph words for object recognition", "abstract": "in this paper , we propose a new , scalable approach for the task of object based image search or object recognition . despite the very large literature existing on the scalability issues in cbir in the sense of retrieval approaches , the scalability of media and scalability of features remain an issue . in our work we tackle the problem of scalability and structural organization of features . the proposed features are nested local graphs built upon sets of surf feature points with delaunay triangulation . a bag-of-visual-words ( bovw ) framework is applied on these graphs , giving birth to a bag-of-graph-words representation . the nested nature of the descriptors consists in scaling from trivial delaunay graphs - isolated feature points - by increasing the number of nodes layer by layer up to graphs with maximal number of nodes . for each layer of graphs its proper visual dictionary is built . the experiments conducted on the sival data set reveal that the graph features at different layers exhibit complementary performances on the same content . the nested approach , the combination of all existing layers , yields significant improvement of the object recognition performance compared to single level approaches .", "topics": ["scalability"]}
{"title": "action recognition with dynamic image networks", "abstract": "we introduce the concept of `` dynamic image '' , a novel compact representation of videos useful for video analysis , particularly in combination with convolutional neural networks ( cnns ) . a dynamic image encodes temporal data such as rgb or optical flow videos by using the concept of `rank pooling ' . the idea is to learn a ranking machine that captures the temporal evolution of the data and to use the parameters of the latter as a representation . when a linear ranking machine is used , the resulting representation is in the form of an image , which we call dynamic because it summarizes the video dynamics in addition of appearance . this is a powerful idea because it allows to convert any video to an image so that existing cnn models pre-trained for the analysis of still images can be immediately extended to videos . we also present an efficient and effective approximate rank pooling operator , accelerating standard rank pooling algorithms by orders of magnitude , and formulate that as a cnn layer . this new layer allows generalizing dynamic images to dynamic feature maps . we demonstrate the power of the new representations on standard benchmarks in action recognition achieving state-of-the-art performance .", "topics": ["approximation algorithm"]}
{"title": "robust low-complexity randomized methods for locating outliers in large matrices", "abstract": "this paper examines the problem of locating outlier columns in a large , otherwise low-rank matrix , in settings where { } { the data } are noisy , or where the overall matrix has missing elements . we propose a randomized two-step inference framework , and establish sufficient conditions on the required sample complexities under which these methods succeed ( with high probability ) in accurately locating the outliers for each task . comprehensive numerical experimental results are provided to verify the theoretical bounds and demonstrate the computational efficiency of the proposed algorithm .", "topics": ["numerical analysis"]}
{"title": "modeling polypharmacy side effects with graph convolutional networks", "abstract": "the use of multiple drugs , termed polypharmacy , is common to treat patients with complex diseases or co-existing medical conditions . however , a major consequence of polypharmacy is a much higher risk of side effects for the patient . polypharmacy side effects emerge because of drug interactions , in which activity of one drug may change , favorably or unfavorably , if taken with another drug . the knowledge of drug interactions is limited because these complex relationships are usually not observed in small clinical testing . discovering polypharmacy side effects thus remains a challenge with significant implications for patient mortality and morbidity . here we introduce decagon , an approach for modeling polypharmacy side effects . the approach constructs a multimodal graph of protein-protein interactions , drug-protein interactions , and the polypharmacy side effects , which are represented as drug-drug interactions , where each side effect is an edge of a different type . decagon is developed specifically to handle such multimodal graphs with a large number of edge types . our approach develops a new graph convolutional neural network for multirelational link prediction in multimodal networks . unlike approaches limited to predicting simple drug-drug interaction values , decagon can predict the exact side effect , if any , through which a given drug combination manifests clinically . decagon accurately predicts polypharmacy side effects , outperforming baselines by up to 69 % . furthermore , decagon models particularly well side effects that have a strong molecular basis , while on predominantly non-molecular side effects , it achieves good performance because of effective sharing of model parameters across edge types . decagon creates an opportunity to use large molecular and patient population data to flag and prioritize polypharmacy side effects for follow-up analysis via formal pharmacological studies .", "topics": ["interaction"]}
{"title": "vconv-dae : deep volumetric shape learning without object labels", "abstract": "with the advent of affordable depth sensors , 3d capture becomes more and more ubiquitous and already has made its way into commercial products . yet , capturing the geometry or complete shapes of everyday objects using scanning devices ( e.g . kinect ) still comes with several challenges that result in noise or even incomplete shapes . recent success in deep learning has shown how to learn complex shape distributions in a data-driven way from large scale 3d cad model collections and to utilize them for 3d processing on volumetric representations and thereby circumventing problems of topology and tessellation . prior work has shown encouraging results on problems ranging from shape completion to recognition . we provide an analysis of such approaches and discover that training as well as the resulting representation are strongly and unnecessarily tied to the notion of object labels . thus , we propose a full convolutional volumetric auto encoder that learns volumetric representation from noisy data by estimating the voxel occupancy grids . the proposed method outperforms prior work on challenging tasks like denoising and shape completion . we also show that the obtained deep embedding gives competitive performance when used for classification and promising results for shape interpolation .", "topics": ["noise reduction", "encoder"]}
{"title": "distribution-independent evolvability of linear threshold functions", "abstract": "valiant 's ( 2007 ) model of evolvability models the evolutionary process of acquiring useful functionality as a restricted form of learning from random examples . linear threshold functions and their various subclasses , such as conjunctions and decision lists , play a fundamental role in learning theory and hence their evolvability has been the primary focus of research on valiant 's framework ( 2007 ) . one of the main open problems regarding the model is whether conjunctions are evolvable distribution-independently ( feldman and valiant , 2008 ) . we show that the answer is negative . our proof is based on a new combinatorial parameter of a concept class that lower-bounds the complexity of learning from correlations . we contrast the lower bound with a proof that linear threshold functions having a non-negligible margin on the data points are evolvable distribution-independently via a simple mutation algorithm . our algorithm relies on a non-linear loss function being used to select the hypotheses instead of 0-1 loss in valiant 's ( 2007 ) original definition . the proof of evolvability requires that the loss function satisfies several mild conditions that are , for example , satisfied by the quadratic loss function studied in several other works ( michael , 2007 ; feldman , 2009 ; valiant , 2010 ) . an important property of our evolution algorithm is monotonicity , that is the algorithm guarantees evolvability without any decreases in performance . previously , monotone evolvability was only shown for conjunctions with quadratic loss ( feldman , 2009 ) or when the distribution on the domain is severely restricted ( michael , 2007 ; feldman , 2009 ; kanade et al . , 2010 )", "topics": ["nonlinear system", "loss function"]}
{"title": "stochastic neural networks with monotonic activation functions", "abstract": "we propose a laplace approximation that creates a stochastic unit from any smooth monotonic activation function , using only gaussian noise . this paper investigates the application of this stochastic approximation in training a family of restricted boltzmann machines ( rbm ) that are closely linked to bregman divergences . this family , that we call exponential family rbm ( exp-rbm ) , is a subset of the exponential family harmoniums that expresses family members through a choice of smooth monotonic non-linearity for each neuron . using contrastive divergence along with our gaussian approximation , we show that exp-rbm can learn useful representations using novel stochastic units .", "topics": ["nonlinear system", "time complexity"]}
{"title": "towards more accurate clustering method by using dynamic time warping", "abstract": "an intrinsic problem of classifiers based on machine learning ( ml ) methods is that their learning time grows as the size and complexity of the training dataset increases . for this reason , it is important to have efficient computational methods and algorithms that can be applied on large datasets , such that it is still possible to complete the machine learning tasks in reasonable time . in this context , we present in this paper a more accurate simple process to speed up ml methods . an unsupervised clustering algorithm is combined with expectation , maximization ( em ) algorithm to develop an efficient hidden markov model ( hmm ) training . the idea of the proposed process consists of two steps . in the first step , training instances with similar inputs are clustered and a weight factor which represents the frequency of these instances is assigned to each representative cluster . dynamic time warping technique is used as a dissimilarity function to cluster similar examples . in the second step , all formulas in the classical hmm training algorithm ( em ) associated with the number of training instances are modified to include the weight factor in appropriate terms . this process significantly accelerates hmm training while maintaining the same initial , transition and emission probabilities matrixes as those obtained with the classical hmm training algorithm . accordingly , the classification accuracy is preserved . depending on the size of the training set , speedups of up to 2200 times is possible when the size is about 100.000 instances . the proposed approach is not limited to training hmms , but it can be employed for a large variety of mls methods .", "topics": ["cluster analysis"]}
{"title": "using hadoop for large scale analysis on twitter : a technical report", "abstract": "sentiment analysis ( or opinion mining ) on twitter data has attracted much attention recently . one of the system 's key features , is the immediacy in communication with other users in an easy , user-friendly and fast way . consequently , people tend to express their feelings freely , which makes twitter an ideal source for accumulating a vast amount of opinions towards a wide diversity of topics . this amount of information offers huge potential and can be harnessed to receive the sentiment tendency towards these topics . however , since none can invest an infinite amount of time to read through these tweets , an automated decision making approach is necessary . nevertheless , most existing solutions are limited in centralized environments only . thus , they can only process at most a few thousand tweets . such a sample , is not representative to define the sentiment polarity towards a topic due to the massive number of tweets published daily . in this paper , we go one step further and develop a novel method for sentiment learning in the mapreduce framework . our algorithm exploits the hashtags and emoticons inside a tweet , as sentiment labels , and proceeds to a classification procedure of diverse sentiment types in a parallel and distributed manner . moreover , we utilize bloom filters to compact the storage size of intermediate data and boost the performance of our algorithm . through an extensive experimental evaluation , we prove that our solution is efficient , robust and scalable and confirm the quality of our sentiment identification .", "topics": ["scalability"]}
{"title": "drift analysis", "abstract": "drift analysis is one of the major tools for analysing evolutionary algorithms and nature-inspired search heuristics . in this chapter we give an introduction to drift analysis and give some examples of how to use it for the analysis of evolutionary algorithms .", "topics": ["heuristic"]}
{"title": "structured signal recovery from quadratic measurements : breaking sample complexity barriers via nonconvex optimization", "abstract": "this paper concerns the problem of recovering an unknown but structured signal $ x \\in r^n $ from $ m $ quadratic measurements of the form $ y_r=| < a_r , x > |^2 $ for $ r=1,2 , ... , m $ . we focus on the under-determined setting where the number of measurements is significantly smaller than the dimension of the signal ( $ m < < n $ ) . we formulate the recovery problem as a nonconvex optimization problem where prior structural information about the signal is enforced through constrains on the optimization variables . we prove that projected gradient descent , when initialized in a neighborhood of the desired signal , converges to the unknown signal at a linear rate . these results hold for any constraint set ( convex or nonconvex ) providing convergence guarantees to the global optimum even when the objective function and constraint set is nonconvex . furthermore , these results hold with a number of measurements that is only a constant factor away from the minimal number of measurements required to uniquely identify the unknown signal . our results provide the first provably tractable algorithm for this data-poor regime , breaking local sample complexity barriers that have emerged in recent literature . in a companion paper we demonstrate favorable properties for the optimization problem that may enable similar results to continue to hold more globally ( over the entire ambient space ) . collectively these two papers utilize and develop powerful tools for uniform convergence of empirical processes that may have broader implications for rigorous understanding of constrained nonconvex optimization heuristics . the mathematical results in this paper also pave the way for a new generation of data-driven phase-less imaging systems that can utilize prior information to significantly reduce acquisition time and enhance image reconstruction , enabling nano-scale imaging at unprecedented speeds and resolutions .", "topics": ["optimization problem", "loss function"]}
{"title": "a situation calculus-based approach to model ubiquitous information services", "abstract": "this paper presents an augmented situation calculus-based approach to model autonomous computing paradigm in ubiquitous information services . to make it practical for commercial development and easier to support autonomous paradigm imposed by ubiquitous information services , we made improvements based on reiter 's standard situation calculus . first we explore the inherent relationship between fluents and evolution : since not all fluents contribute to systems ' evolution and some fluents can be derived from some others , we define those fluents that are sufficient and necessary to determine evolutional potential as decisive fluents , and then we prove that their successor states wrt to deterministic complex actions satisfy markov property . then , within the calculus framework we build , we introduce validity theory to model the autonomous services with application-specific validity requirements , including : validity fluents to axiomatize validity requirements , heuristic multiple alternative service choices ranging from complete acceptance , partial acceptance , to complete rejection , and validity-ensured policy to comprise such alternative service choices into organic , autonomously-computable services . our approach is demonstrated by a ubiquitous calendaring service , acs , throughout the paper .", "topics": ["heuristic", "autonomous car"]}
{"title": "identification of gaussian process state space models", "abstract": "the gaussian process state space model ( gpssm ) is a non-linear dynamical system , where unknown transition and/or measurement mappings are described by gps . most research in gpssms has focussed on the state estimation problem , i.e . , computing a posterior of the latent state given the model . however , the key challenge in gpssms has not been satisfactorily addressed yet : system identification , i.e . , learning the model . to address this challenge , we impose a structured gaussian variational posterior distribution over the latent states , which is parameterised by a recognition model in the form of a bi-directional recurrent neural network . inference with this structure allows us to recover a posterior smoothed over sequences of data . we provide a practical algorithm for efficiently computing a lower bound on the marginal likelihood using the reparameterisation trick . this further allows for the use of arbitrary kernels within the gpssm . we demonstrate that the learnt gpssm can efficiently generate plausible future trajectories of the identified system after only observing a small number of episodes from the true system .", "topics": ["calculus of variations", "recurrent neural network"]}
{"title": "bayesian group nonnegative matrix factorization for eeg analysis", "abstract": "we propose a generative model of a group eeg analysis , based on appropriate kernel assumptions on eeg data . we derive the variational inference update rule using various approximation techniques . the proposed model outperforms the current state-of-the-art algorithms in terms of common pattern extraction . the validity of the proposed model is tested on the bci competition dataset .", "topics": ["generative model", "calculus of variations"]}
{"title": "performance evaluation of machine learning classifiers in sentiment mining", "abstract": "in recent years , the use of machine learning classifiers is of great value in solving a variety of problems in text classification . sentiment mining is a kind of text classification in which , messages are classified according to sentiment orientation such as positive or negative . this paper extends the idea of evaluating the performance of various classifiers to show their effectiveness in sentiment mining of online product reviews . the product reviews are collected from amazon reviews . to evaluate the performance of classifiers various evaluation methods like random sampling , linear sampling and bootstrap sampling are used . our results shows that support vector machine with bootstrap sampling method outperforms others classifiers and sampling methods in terms of misclassification rate .", "topics": ["sampling ( signal processing )", "data mining"]}
{"title": "speech recognition for medical conversations", "abstract": "in this paper we document our experiences with developing speech recognition for medical transcription - a system that automatically transcribes doctor-patient conversations . towards this goal , we built a system along two different methodological lines - a connectionist temporal classification ( ctc ) phoneme based model and a listen attend and spell ( las ) grapheme based model . to train these models we used a corpus of anonymized conversations representing approximately 14,000 hours of speech . because of noisy transcripts and alignments in the corpus , a significant amount of effort was invested in data cleaning issues . we describe a two-stage strategy we followed for segmenting the data . the data cleanup and development of a matched language model was essential to the success of the ctc based models . the las based models , however were found to be resilient to alignment and transcript noise and did not require the use of language models . ctc models were able to achieve a word error rate of 20.1 % , and the las models were able to achieve 18.3 % . our analysis shows that both models perform well on important medical utterances and therefore can be practical for transcribing medical conversations .", "topics": ["speech recognition", "text corpus"]}
{"title": "semantic object parsing with local-global long short-term memory", "abstract": "semantic object parsing is a fundamental task for understanding objects in detail in computer vision community , where incorporating multi-level contextual information is critical for achieving such fine-grained pixel-level recognition . prior methods often leverage the contextual information through post-processing predicted confidence maps . in this work , we propose a novel deep local-global long short-term memory ( lg-lstm ) architecture to seamlessly incorporate short-distance and long-distance spatial dependencies into the feature learning over all pixel positions . in each lg-lstm layer , local guidance from neighboring positions and global guidance from the whole image are imposed on each position to better exploit complex local and global contextual information . individual lstms for distinct spatial dimensions are also utilized to intrinsically capture various spatial layouts of semantic parts in the images , yielding distinct hidden and memory cells of each position for each dimension . in our parsing approach , several lg-lstm layers are stacked and appended to the intermediate convolutional layers to directly enhance visual features , allowing network parameters to be learned in an end-to-end way . the long chains of sequential computation by stacked lg-lstm layers also enable each pixel to sense a much larger region for inference benefiting from the memorization of previous dependencies in all positions along all dimensions . comprehensive evaluations on three public datasets well demonstrate the significant superiority of our lg-lstm over other state-of-the-art methods .", "topics": ["feature learning", "computer vision"]}
{"title": "automatic extraction of tagset mappings from parallel-annotated corpora", "abstract": "this paper describes some of the recent work of project amalgam ( automatic mapping among lexico-grammatical annotation models ) . we are investigating ways to map between the leading corpus annotation schemes in order to improve their resuability . collation of all the included corpora into a single large annotated corpus will provide a more detailed language model to be developed for tasks such as speech and handwriting recognition . in particular , we focus here on a method of extracting mappings from corpora that have been annotated according to more than one annotation scheme .", "topics": ["text corpus", "parsing"]}
{"title": "universal dependencies parsing for colloquial singaporean english", "abstract": "singlish can be interesting to the acl community both linguistically as a major creole based on english , and computationally for information extraction and sentiment analysis of regional social media . we investigate dependency parsing of singlish by constructing a dependency treebank under the universal dependencies scheme , and then training a neural network model by integrating english syntactic knowledge into a state-of-the-art parser trained on the singlish treebank . results show that english knowledge can lead to 25 % relative error reduction , resulting in a parser of 84.47 % accuracies . to the best of our knowledge , we are the first to use neural stacking to improve cross-lingual dependency parsing on low-resource languages . we make both our annotation and parser available for further research .", "topics": ["parsing"]}
{"title": "the evaluation of causal effects in studies with an unobserved exposure/outcome variable : bounds and identification", "abstract": "this paper deals with the problem of evaluating the causal effect using observational data in the presence of an unobserved exposure/ outcome variable , when cause-effect relationships between variables can be described as a directed acyclic graph and the corresponding recursive factorization of a joint distribution . first , we propose identifiability criteria for causal effects when an unobserved exposure/outcome variable is considered to contain more than two categories . next , when unmeasured variables exist between an unobserved outcome variable and its proxy variables , we provide the tightest bounds based on the potential outcome approach . the results of this paper are helpful to evaluate causal effects in the case where it is difficult or expensive to observe an exposure/ outcome variable in many practical fields .", "topics": ["causality"]}
{"title": "autonomous quadrotor landing using deep reinforcement learning", "abstract": "landing an unmanned aerial vehicle ( uav ) on a ground marker is an open problem despite the effort of the research community . previous attempts mostly focused on the analysis of hand-crafted geometric features and the use of external sensors in order to allow the vehicle to approach the land-pad . in this article , we propose a method based on deep reinforcement learning that only requires low-resolution images taken from a down-looking camera in order to identify the position of the marker and land the uav on it . the proposed approach is based on a hierarchy of deep q-networks ( dqns ) used as high-level control policy for the navigation toward the marker . we implemented different technical solutions , such as the combination of vanilla and double dqns , and a partitioned buffer replay . using domain randomization we trained the vehicle on uniform textures and we tested it on a large variety of simulated and real-world environments . the overall performance is comparable with a state-of-the-art algorithm and human pilots .", "topics": ["high- and low-level", "reinforcement learning"]}
{"title": "differentiable genetic programming", "abstract": "we introduce the use of high order automatic differentiation , implemented via the algebra of truncated taylor polynomials , in genetic programming . using the cartesian genetic programming encoding we obtain a high-order taylor representation of the program output that is then used to back-propagate errors during learning . the resulting machine learning framework is called differentiable cartesian genetic programming ( dcgp ) . in the context of symbolic regression , dcgp offers a new approach to the long unsolved problem of constant representation in gp expressions . on several problems of increasing complexity we find that dcgp is able to find the exact form of the symbolic expression as well as the constants values . we also demonstrate the use of dcgp to solve a large class of differential equations and to find prime integrals of dynamical systems , presenting , in both cases , results that confirm the efficacy of our approach .", "topics": ["polynomial"]}
{"title": "divide-and-conquer with sequential monte carlo", "abstract": "we propose a novel class of sequential monte carlo ( smc ) algorithms , appropriate for inference in probabilistic graphical models . this class of algorithms adopts a divide-and-conquer approach based upon an auxiliary tree-structured decomposition of the model of interest , turning the overall inferential task into a collection of recursively solved sub-problems . the proposed method is applicable to a broad class of probabilistic graphical models , including models with loops . unlike a standard smc sampler , the proposed divide-and-conquer smc employs multiple independent populations of weighted particles , which are resampled , merged , and propagated as the method progresses . we illustrate empirically that this approach can outperform standard methods in terms of the accuracy of the posterior expectation and marginal likelihood approximations . divide-and-conquer smc also opens up novel parallel implementation options and the possibility of concentrating the computational effort on the most challenging sub-problems . we demonstrate its performance on a markov random field and on a hierarchical logistic regression problem .", "topics": ["sampling ( signal processing )", "graphical model"]}
{"title": "bas : beetle antennae search algorithm for optimization problems", "abstract": "meta-heuristic algorithms have become very popular because of powerful performance on the optimization problem . a new algorithm called beetle antennae search algorithm ( bas ) is proposed in the paper inspired by the searching behavior of longhorn beetles . the bas algorithm imitates the function of antennae and the random walking mechanism of beetles in nature , and then two main steps of detecting and searching are implemented . finally , the algorithm is benchmarked on 2 well-known test functions , in which the numerical results validate the efficacy of the proposed bas algorithm .", "topics": ["optimization problem", "numerical analysis"]}
{"title": "the one hundred layers tiramisu : fully convolutional densenets for semantic segmentation", "abstract": "state-of-the-art approaches for semantic image segmentation are built on convolutional neural networks ( cnns ) . the typical segmentation architecture is composed of ( a ) a downsampling path responsible for extracting coarse semantic features , followed by ( b ) an upsampling path trained to recover the input image resolution at the output of the model and , optionally , ( c ) a post-processing module ( e.g . conditional random fields ) to refine the model predictions . recently , a new cnn architecture , densely connected convolutional networks ( densenets ) , has shown excellent results on image classification tasks . the idea of densenets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train . in this paper , we extend densenets to deal with the problem of semantic segmentation . we achieve state-of-the-art results on urban scene benchmark datasets such as camvid and gatech , without any further post-processing module nor pretraining . moreover , due to smart construction of the model , our approach has much less parameters than currently published best entries for these datasets . code to reproduce the experiments is available here : https : //github.com/simjeg/fc-densenet/blob/master/train.py", "topics": ["image segmentation", "computer vision"]}
{"title": "pac-bayesian inequalities for martingales", "abstract": "we present a set of high-probability inequalities that control the concentration of weighted averages of multiple ( possibly uncountably many ) simultaneously evolving and interdependent martingales . our results extend the pac-bayesian analysis in learning theory from the i.i.d . setting to martingales opening the way for its application to importance weighted sampling , reinforcement learning , and other interactive learning domains , as well as many other domains in probability theory and statistics , where martingales are encountered . we also present a comparison inequality that bounds the expectation of a convex function of a martingale difference sequence shifted to the [ 0,1 ] interval by the expectation of the same function of independent bernoulli variables . this inequality is applied to derive a tighter analog of hoeffding-azuma 's inequality .", "topics": ["sampling ( signal processing )", "reinforcement learning"]}
{"title": "separators and adjustment sets in causal graphs : complete criteria and an algorithmic framework", "abstract": "principled reasoning about the identifiability of causal effects from non-experimental data is an important application of graphical causal models . we present an algorithmic framework for efficiently testing , constructing , and enumerating $ m $ -separators in ancestral graphs ( ags ) , a class of graphical causal models that can represent uncertainty about the presence of latent confounders . furthermore , we prove a reduction from causal effect identification by covariate adjustment to $ m $ -separation in a subgraph for directed acyclic graphs ( dags ) and maximal ancestral graphs ( mags ) . jointly , these results yield constructive criteria that characterize all adjustment sets as well as all minimal and minimum adjustment sets for identification of a desired causal effect with multivariate exposures and outcomes in the presence of latent confounding . our results extend several existing solutions for special cases of these problems . our efficient algorithms allowed us to empirically quantify the identifiability gap between covariate adjustment and the do-calculus in random dags , covering a wide range of scenarios . implementations of our algorithms are provided in the r package dagitty .", "topics": ["causality"]}
{"title": "causal effect identification in acyclic directed mixed graphs and gated models", "abstract": "we introduce a new family of graphical models that consists of graphs with possibly directed , undirected and bidirected edges but without directed cycles . we show that these models are suitable for representing causal models with additive error terms . we provide a set of sufficient graphical criteria for the identification of arbitrary causal effects when the new models contain directed and undirected edges but no bidirected edge . we also provide a necessary and sufficient graphical criterion for the identification of the causal effect of a single variable on the rest of the variables . moreover , we develop an exact algorithm for learning the new models from observational and interventional data via answer set programming . finally , we introduce gated models for causal effect identification , a new family of graphical models that exploits context specific independences to identify additional causal effects .", "topics": ["graphical model", "causality"]}
{"title": "formal ontology learning on factual is-a corpus in english using description logics", "abstract": "ontology learning ( ol ) is the computational task of generating a knowledge base in the form of an ontology given an unstructured corpus whose content is in natural language ( nl ) . several works can be found in this area most of which are limited to statistical and lexico-syntactic pattern matching based techniques light-weight ol . these techniques do not lead to very accurate learning mostly because of several linguistic nuances in nl . formal ol is an alternative ( less explored ) methodology were deep linguistics analysis is made using theory and tools found in computational linguistics to generate formal axioms and definitions instead simply inducing a taxonomy . in this paper we propose `` description logic ( dl ) '' based formal ol framework for learning factual is-a type sentences in english . we claim that semantic construction of is-a sentences is non trivial . hence , we also claim that such sentences requires special studies in the context of ol before any truly formal ol can be proposed . we introduce a learner tool , called dlol_is-a , that generated such ontologies in the owl format . we have adopted `` gold standard '' based ol evaluation on is-a rich wcl v.1.1 dataset and our own community representative is-a dataset . we observed significant improvement of dlol_is-a when compared to the light-weight ol tool text2onto and formal ol tool fred .", "topics": ["natural language"]}
{"title": "learning object location predictors with boosting and grammar-guided feature extraction", "abstract": "we present beamer : a new spatially exploitative approach to learning object detectors which shows excellent results when applied to the task of detecting objects in greyscale aerial imagery in the presence of ambiguous and noisy data . there are four main contributions used to produce these results . first , we introduce a grammar-guided feature extraction system , enabling the exploration of a richer feature space while constraining the features to a useful subset . this is specified with a rule-based generative grammar crafted by a human expert . second , we learn a classifier on this data using a newly proposed variant of adaboost which takes into account the spatially correlated nature of the data . third , we perform another round of training to optimize the method of converting the pixel classifications generated by boosting into a high quality set of ( x , y ) locations . lastly , we carefully define three common problems in object detection and define two evaluation criteria that are tightly matched to these problems . major strengths of this approach are : ( 1 ) a way of randomly searching a broad feature space , ( 2 ) its performance when evaluated on well-matched evaluation criteria , and ( 3 ) its use of the location prediction domain to learn object detectors as well as to generate detections that perform well on several tasks : object counting , tracking , and target detection . we demonstrate the efficacy of beamer with a comprehensive experimental evaluation on a challenging data set .", "topics": ["feature vector", "feature extraction"]}
{"title": "interactive configuration by regular string constraints", "abstract": "a product configurator which is complete , backtrack free and able to compute the valid domains at any state of the configuration can be constructed by building a binary decision diagram ( bdd ) . despite the fact that the size of the bdd is exponential in the number of variables in the worst case , bdds have proved to work very well in practice . current bdd-based techniques can only handle interactive configuration with small finite domains . in this paper we extend the approach to handle string variables constrained by regular expressions . the user is allowed to change the strings by adding letters at the end of the string . we show how to make a data structure that can perform fast valid domain computations given some assignment on the set of string variables . we first show how to do this by using one large dfa . since this approach is too space consuming to be of practical use , we construct a data structure that simulates the large dfa and in most practical cases are much more space efficient . as an example a configuration problem on $ n $ string variables with only one solution in which each string variable is assigned to a value of length of $ k $ the former structure will use $ \\omega ( k^n ) $ space whereas the latter only need $ o ( kn ) $ . we also show how this framework easily can be combined with the recent bdd techniques to allow both boolean , integer and string variables in the configuration problem .", "topics": ["time complexity", "computation"]}
{"title": "fast belief update using order-of-magnitude probabilities", "abstract": "we present an algorithm , called predict , for updating beliefs in causal networks quantified with order-of-magnitude probabilities . the algorithm takes advantage of both the structure and the quantification of the network and presents a polynomial asymptotic complexity . predict exhibits a conservative behavior in that it is always sound but not always complete . we provide sufficient conditions for completeness and present algorithms for testing these conditions and for computing a complete set of plausible values . we propose predict as an efficient method to estimate probabilistic values and illustrate its use in conjunction with two known algorithms for probabilistic inference . finally , we describe an application of predict to plan evaluation , present experimental results , and discuss issues regarding its use with conditional logics of belief , and in the characterization of irrelevance .", "topics": ["relevance", "causality"]}
{"title": "inductive representation learning on large graphs", "abstract": "low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks , from content recommendation to identifying protein functions . however , most existing approaches require that all nodes in the graph are present during training of the embeddings ; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes . here we present graphsage , a general , inductive framework that leverages node feature information ( e.g . , text attributes ) to efficiently generate node embeddings for previously unseen data . instead of training individual embeddings for each node , we learn a function that generates embeddings by sampling and aggregating features from a node 's local neighborhood . our algorithm outperforms strong baselines on three inductive node-classification benchmarks : we classify the category of unseen nodes in evolving information graphs based on citation and reddit post data , and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions .", "topics": ["interaction"]}
{"title": "group $ k $ -means", "abstract": "we study how to learn multiple dictionaries from a dataset , and approximate any data point by the sum of the codewords each chosen from the corresponding dictionary . although theoretically low approximation errors can be achieved by the global solution , an effective solution has not been well studied in practice . to solve the problem , we propose a simple yet effective algorithm \\textit { group $ k $ -means } . specifically , we take each dictionary , or any two selected dictionaries , as a group of $ k $ -means cluster centers , and then deal with the approximation issue by minimizing the approximation errors . besides , we propose a hierarchical initialization for such a non-convex problem . experimental results well validate the effectiveness of the approach .", "topics": ["approximation algorithm", "approximation"]}
{"title": "optimization for compressed sensing : the simplex method and kronecker sparsification", "abstract": "in this paper we present two new approaches to efficiently solve large-scale compressed sensing problems . these two ideas are independent of each other and can therefore be used either separately or together . we consider all possibilities . for the first approach , we note that the zero vector can be taken as the initial basic ( infeasible ) solution for the linear programming problem and therefore , if the true signal is very sparse , some variants of the simplex method can be expected to take only a small number of pivots to arrive at a solution . we implemented one such variant and demonstrate a dramatic improvement in computation time on very sparse signals . the second approach requires a redesigned sensing mechanism in which the vector signal is stacked into a matrix . this allows us to exploit the kronecker compressed sensing ( kcs ) mechanism . we show that the kronecker sensing requires stronger conditions for perfect recovery compared to the original vector problem . however , the kronecker sensing , modeled correctly , is a much sparser linear optimization problem . hence , algorithms that benefit from sparse problem representation , such as interior-point methods , can solve the kronecker sensing problems much faster than the corresponding vector problem . in our numerical studies , we demonstrate a ten-fold improvement in the computation time .", "topics": ["optimization problem", "time complexity"]}
{"title": "sentiment analysis for modern standard arabic and colloquial", "abstract": "the rise of social media such as blogs and social networks has fueled interest in sentiment analysis . with the proliferation of reviews , ratings , recommendations and other forms of online expression , online opinion has turned into a kind of virtual currency for businesses looking to market their products , identify new opportunities and manage their reputations , therefore many are now looking to the field of sentiment analysis . in this paper , we present a feature-based sentence level approach for arabic sentiment analysis . our approach is using arabic idioms/saying phrases lexicon as a key importance for improving the detection of the sentiment polarity in arabic sentences as well as a number of novels and rich set of linguistically motivated features contextual intensifiers , contextual shifter and negation handling ) , syntactic features for conflicting phrases which enhance the sentiment classification accuracy . furthermore , we introduce an automatic expandable wide coverage polarity lexicon of arabic sentiment words . the lexicon is built with gold-standard sentiment words as a seed which is manually collected and annotated and it expands and detects the sentiment orientation automatically of new sentiment words using synset aggregation technique and free online arabic lexicons and thesauruses . our data focus on modern standard arabic ( msa ) and egyptian dialectal arabic tweets and microblogs ( hotel reservation , product reviews , etc . ) . the experimental results using our resources and techniques with svm classifier indicate high performance levels , with accuracies of over 95 % .", "topics": ["support vector machine"]}
{"title": "sim-ce : an advanced simulink platform for studying the brain of caenorhabditis elegans", "abstract": "we introduce sim-ce , an advanced , user-friendly modeling and simulation environment in simulink for performing multi-scale behavioral analysis of the nervous system of caenorhabditis elegans ( c. elegans ) . sim-ce contains an implementation of the mathematical models of c. elegans 's neurons and synapses , in simulink , which can be easily extended and particularized by the user . the simulink model is able to capture both complex dynamics of ion channels and additional biophysical detail such as intracellular calcium concentration . we demonstrate the performance of sim-ce by carrying out neuronal , synaptic and neural-circuit-level behavioral simulations . such environment enables the user to capture unknown properties of the neural circuits , test hypotheses and determine the origin of many behavioral plasticities exhibited by the worm .", "topics": ["simulation"]}
{"title": "spsd matrix approximation vis column selection : theories , algorithms , and extensions", "abstract": "symmetric positive semidefinite ( spsd ) matrix approximation is an important problem with applications in kernel methods . however , existing spsd matrix approximation methods such as the nystr\\ '' om method only have weak error bounds . in this paper we conduct in-depth studies of an spsd matrix approximation model and establish strong relative-error bounds . we call it the prototype model for it has more efficient and effective extensions , and some of its extensions have high scalability . though the prototype model itself is not suitable for large-scale data , it is still useful to study its properties , on which the analysis of its extensions relies . this paper offers novel theoretical analysis , efficient algorithms , and a highly accurate extension . first , we establish a lower error bound for the prototype model and improve the error bound of an existing column selection algorithm to match the lower bound . in this way , we obtain the first optimal column selection algorithm for the prototype model . we also prove that the prototype model is exact under certain conditions . second , we develop a simple column selection algorithm with a provable error bound . third , we propose a so-called spectral shifting model to make the approximation more accurate when the eigenvalues of the matrix decay slowly , and the improvement is theoretically quantified . the spectral shifting method can also be applied to improve other spsd matrix approximation models .", "topics": ["approximation", "scalability"]}
{"title": "text recognition in both ancient and cartographic documents", "abstract": "this paper deals with the recognition and matching of text in both cartographic maps and ancient documents . the purpose of this work is to find similar text regions based on statistical and global features . a phase of normalization is done first , in object to well categorize the same quantity of information . a phase of wordspotting is done next by combining local and global features . we make different experiments by combining the different techniques of extracting features in order to obtain better results in recognition phase . we applied fontspotting on both ancient documents and cartographic ones . we also applied the wordspotting in which we adopted a new technique which tries to compare the images of character and not the entire images words . we present the precision and recall values obtained with three methods for the new method of wordspotting applied on characters only .", "topics": ["map"]}
{"title": "copeland dueling bandit problem : regret lower bound , optimal algorithm , and computationally efficient algorithm", "abstract": "we study the k-armed dueling bandit problem , a variation of the standard stochastic bandit problem where the feedback is limited to relative comparisons of a pair of arms . the hardness of recommending copeland winners , the arms that beat the greatest number of other arms , is characterized by deriving an asymptotic regret bound . we propose copeland winners relative minimum empirical divergence ( cw-rmed ) and derive an asymptotically optimal regret bound for it . however , it is not known whether the algorithm can be efficiently computed or not . to address this issue , we devise an efficient version ( ecw-rmed ) and derive its asymptotic regret bound . experimental comparisons of dueling bandit algorithms show that ecw-rmed significantly outperforms existing ones .", "topics": ["regret ( decision theory )"]}
{"title": "a scale-space theory for text", "abstract": "scale-space theory has been established primarily by the computer vision and signal processing communities as a well-founded and promising framework for multi-scale processing of signals ( e.g . , images ) . by embedding an original signal into a family of gradually coarsen signals parameterized with a continuous scale parameter , it provides a formal framework to capture the structure of a signal at different scales in a consistent way . in this paper , we present a scale space theory for text by integrating semantic and spatial filters , and demonstrate how natural language documents can be understood , processed and analyzed at multiple resolutions , and how this scale-space representation can be used to facilitate a variety of nlp and text analysis tasks .", "topics": ["natural language processing", "computer vision"]}
{"title": "on the difficulty of training recurrent neural networks", "abstract": "there are two widely known issues with properly training recurrent neural networks , the vanishing and the exploding gradient problems detailed in bengio et al . ( 1994 ) . in this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical , a geometric and a dynamical systems perspective . our analysis is used to justify a simple yet effective solution . we propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem . we validate empirically our hypothesis and proposed solutions in the experimental section .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "character-based neural embeddings for tweet clustering", "abstract": "in this paper we show how the performance of tweet clustering can be improved by leveraging character-based neural networks . the proposed approach overcomes the limitations related to the vocabulary explosion in the word-based models and allows for the seamless processing of the multilingual content . our evaluation results and code are available on-line at https : //github.com/vendi12/tweet2vec_clustering", "topics": ["cluster analysis"]}
{"title": "im2flow : motion hallucination from static images for action recognition", "abstract": "existing methods to recognize actions in static images take the images at their face value , learning the appearances -- -objects , scenes , and body poses -- -that distinguish each action class . however , such models are deprived of the rich dynamic structure and motions that also define human activity . we propose an approach that hallucinates the unobserved future motion implied by a single snapshot to help static-image action recognition . the key idea is to learn a prior over short-term dynamics from thousands of unlabeled videos , infer the anticipated optical flow on novel static images , and then train discriminative models that exploit both streams of information . our main contributions are twofold . first , we devise an encoder-decoder convolutional neural network and a novel optical flow encoding that can translate a static image into an accurate flow map . second , we show the power of hallucinated flow for recognition , successfully transferring the learned motion into a standard two-stream network for activity recognition . on seven datasets , we demonstrate the power of the approach . it not only achieves state-of-the-art accuracy for dense optical flow prediction , but also consistently enhances recognition of actions and dynamic scenes .", "topics": ["encoder"]}
{"title": "soft-dtw : a differentiable loss function for time-series", "abstract": "we propose in this paper a differentiable learning loss between time series , building upon the celebrated dynamic time warping ( dtw ) discrepancy . unlike the euclidean distance , dtw can compare time series of variable size and is robust to shifts or dilatations across the time dimension . to compute dtw , one typically solves a minimal-cost alignment problem between two time series using dynamic programming . our work takes advantage of a smoothed formulation of dtw , called soft-dtw , that computes the soft-minimum of all alignment costs . we show in this paper that soft-dtw is a differentiable loss function , and that both its value and gradient can be computed with quadratic time/space complexity ( dtw has quadratic time but linear space complexity ) . we show that this regularization is particularly well suited to average and cluster time series under the dtw geometry , a task for which our proposal significantly outperforms existing baselines . next , we propose to tune the parameters of a machine that outputs time series by minimizing its fit with ground-truth labels in a soft-dtw sense .", "topics": ["baseline ( configuration management )", "time series"]}
{"title": "extrinsic calibration of 3d range finder and camera without auxiliary object or human intervention", "abstract": "fusion of heterogeneous extroceptive sensors is the most effient and effective way to representing the environment precisely , as it overcomes various defects of each homogeneous sensor . the rigid transformation ( aka . extrinsic parameters ) of heterogeneous sensory systems should be available before precisely fusing the multisensor information . researchers have proposed several approaches to estimating the extrinsic parameters . these approaches require either auxiliary objects , like chessboards , or extra help from human to select correspondences . in this paper , we proposed a novel extrinsic calibration approach for the extrinsic calibration of range and image sensors . as far as we know , it is the first automatic approach with no requirement of auxiliary objects or any human interventions . first , we estimate the initial extrinsic parameters from the individual motion of the range finder and the camera . then we extract lines in the image and point-cloud pairs , to refine the line feature associations by the initial extrinsic parameters . at the end , we discussed the degenerate case which may lead to the algorithm failure and validate our approach by simulation . the results indicate high-precision extrinsic calibration results against the ground-truth .", "topics": ["simulation", "sensor"]}
{"title": "phonological modeling for continuous speech recognition in korean", "abstract": "a new scheme to represent phonological changes during continuous speech recognition is suggested . a phonological tag coupled with its morphological tag is designed to represent the conditions of korean phonological changes . a pairwise language model of these morphological and phonological tags is implemented in korean speech recognition system . performance of the model is verified through the tdnn-based speech recognition experiments .", "topics": ["speech recognition"]}
{"title": "following the leader and fast rates in linear prediction : curved constraint sets and other regularities", "abstract": "the follow the leader ( ftl ) algorithm , perhaps the simplest of all online learning algorithms , is known to perform well when the loss functions it is used on are convex and positively curved . in this paper we ask whether there are other `` lucky '' settings when ftl achieves sublinear , `` small '' regret . in particular , we study the fundamental problem of linear prediction over a non-empty convex , compact domain . amongst other results , we prove that the curvature of the boundary of the domain can act as if the losses were curved : in this case , we prove that as long as the mean of the loss vectors have positive lengths bounded away from zero , ftl enjoys a logarithmic growth rate of regret , while , e.g . , for polytope domains and stochastic data it enjoys finite expected regret . building on a previously known meta-algorithm , we also get an algorithm that simultaneously enjoys the worst-case guarantees and the bound available for ftl .", "topics": ["regret ( decision theory )", "loss function"]}
{"title": "an intelligent approach towards automatic shape modeling and object extraction from satellite images using cellular automata based algorithm", "abstract": "automatic feature extraction domain has witnessed the application of many intelligent methodologies over past decade ; however detection accuracy of these approaches were limited as object geometry and contextual knowledge were not given enough consideration . in this paper , we propose a frame work for accurate detection of features along with automatic interpolation , and interpretation by modeling feature shape as well as contextual knowledge using advanced techniques such as svrf , cellular neural network , core set , and maca . developed methodology has been compared with contemporary methods using different statistical measures . investigations over various satellite images revealed that considerable success was achieved with the cnn approach . cnn has been effective in modeling different complex features effectively and complexity of the approach has been considerably reduced using corset optimization . the system has dynamically used spectral and spatial information for representing contextual knowledge using cnn-prolog approach . system has been also proved to be effective in providing intelligent interpolation and interpretation of random features .", "topics": ["feature extraction"]}
{"title": "bayesian logic programs", "abstract": "bayesian networks provide an elegant formalism for representing and reasoning about uncertainty using probability theory . theyare a probabilistic extension of propositional logic and , hence , inherit some of the limitations of propositional logic , such as the difficulties to represent objects and relations . we introduce a generalization of bayesian networks , called bayesian logic programs , to overcome these limitations . in order to represent objects and relations it combines bayesian networks with definite clause logic by establishing a one-to-one mapping between ground atoms and random variables . we show that bayesian logic programs combine the advantages of both definite clause logic and bayesian networks . this includes the separation of quantitative and qualitative aspects of the model . furthermore , bayesian logic programs generalize both bayesian networks as well as logic programs . so , many ideas developed", "topics": ["bayesian network"]}
{"title": "confidence driven tgv fusion", "abstract": "we introduce a novel model for spatially varying variational data fusion , driven by point-wise confidence values . the proposed model allows for the joint estimation of the data and the confidence values based on the spatial coherence of the data . we discuss the main properties of the introduced model as well as suitable algorithms for estimating the solution of the corresponding biconvex minimization problem and their convergence . the performance of the proposed model is evaluated considering the problem of depth image fusion by using both synthetic and real data from publicly available datasets .", "topics": ["calculus of variations", "synthetic data"]}
{"title": "a compact dnn : approaching googlenet-level accuracy of classification and domain adaptation", "abstract": "recently , dnn model compression based on network architecture design , e.g . , squeezenet , attracted a lot attention . no accuracy drop on image classification is observed on these extremely compact networks , compared to well-known models . an emerging question , however , is whether these model compression techniques hurt dnn 's learning ability other than classifying images on a single dataset . our preliminary experiment shows that these compression methods could degrade domain adaptation ( da ) ability , though the classification performance is preserved . therefore , we propose a new compact network architecture and unsupervised da method in this paper . the dnn is built on a new basic module conv-m which provides more diverse feature extractors without significantly increasing parameters . the unified framework of our da method will simultaneously learn invariance across domains , reduce divergence of feature representations , and adapt label prediction . our dnn has 4.1m parameters , which is only 6.7 % of alexnet or 59 % of googlenet . experiments show that our dnn obtains googlenet-level accuracy both on classification and da , and our da method slightly outperforms previous competitive ones . put all together , our da strategy based on our dnn achieves state-of-the-art on sixteen of total eighteen da tasks on popular office-31 and office-caltech datasets .", "topics": ["unsupervised learning", "feature extraction"]}
{"title": "a pyramidal evolutionary algorithm with different inter-agent partnering strategies for scheduling problems", "abstract": "this paper combines the idea of a hierarchical distributed genetic algorithm with different inter-agent partnering strategies . cascading clusters of sub-populations are built from bottom up , with higher-level sub-populations optimising larger parts of the problem . hence higher-level sub-populations search a larger search space with a lower resolution whilst lower-level sub-populations search a smaller search space with a higher resolution . the effects of different partner selection schemes amongst the agents on solution quality are examined for two multiple-choice optimisation problems . it is shown that partnering strategies that exploit problem-specific knowledge are superior and can counter inappropriate ( sub- ) fitness measurements .", "topics": ["mathematical optimization"]}
{"title": "how to train a cat : learning canonical appearance transformations for direct visual localization under illumination change", "abstract": "direct visual localization has recently enjoyed a resurgence in popularity with the increasing availability of cheap mobile computing power . the competitive accuracy and robustness of these algorithms compared to state-of-the-art feature-based methods , as well as their natural ability to yield dense maps , makes them an appealing choice for a variety of mobile robotics applications . however , direct methods remain brittle in the face of appearance change due to their underlying assumption of photometric consistency , which is commonly violated in practice . in this paper , we propose to mitigate this problem by training deep convolutional encoder-decoder models to transform images of a scene such that they correspond to a previously-seen canonical appearance . we validate our method in multiple environments and illumination conditions using high-fidelity synthetic rgb-d datasets , and integrate the trained models into a direct visual localization pipeline , yielding improvements in visual odometry ( vo ) accuracy through time-varying illumination conditions , as well as improved metric relocalization performance under illumination change , where conventional methods normally fail . we further provide a preliminary investigation of transfer learning from synthetic to real environments in a localization context . an open-source implementation of our method using pytorch is available at https : //github.com/utiasstars/cat-net .", "topics": ["synthetic data", "map"]}
{"title": "wide & deep learning for recommender systems", "abstract": "generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs . memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable , while generalization requires more feature engineering effort . with less feature engineering , deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features . however , deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank . in this paper , we present wide & deep learning -- -jointly trained wide linear models and deep neural networks -- -to combine the benefits of memorization and generalization for recommender systems . we productionized and evaluated the system on google play , a commercial mobile app store with over one billion active users and over one million apps . online experiment results show that wide & deep significantly increased app acquisitions compared with wide-only and deep-only models . we have also open-sourced our implementation in tensorflow .", "topics": ["nonlinear system", "interaction"]}
{"title": "multidimensional classification of hippocampal shape features discriminates alzheimer 's disease and mild cognitive impairment from normal aging", "abstract": "we describe a new method to automatically discriminate between patients with alzheimer 's disease ( ad ) or mild cognitive impairment ( mci ) and elderly controls , based on multidimensional classification of hippocampal shape features . this approach uses spherical harmonics ( spharm ) coefficients to model the shape of the hippocampi , which are segmented from magnetic resonance images ( mri ) using a fully automatic method that we previously developed . spharm coefficients are used as features in a classification procedure based on support vector machines ( svm ) . the most relevant features for classification are selected using a bagging strategy . we evaluate the accuracy of our method in a group of 23 patients with ad ( 10 males , 13 females , age $ \\pm $ standard-deviation ( sd ) = 73 $ \\pm $ 6 years , mini-mental score ( mms ) = 24.4 $ \\pm $ 2.8 ) , 23 patients with amnestic mci ( 10 males , 13 females , age $ \\pm $ sd = 74 $ \\pm $ 8 years , mms = 27.3 $ \\pm $ 1.4 ) and 25 elderly healthy controls ( 13 males , 12 females , age $ \\pm $ sd = 64 $ \\pm $ 8 years ) , using leave-one-out cross-validation . for ad vs controls , we obtain a correct classification rate of 94 % , a sensitivity of 96 % , and a specificity of 92 % . for mci vs controls , we obtain a classification rate of 83 % , a sensitivity of 83 % , and a specificity of 84 % . this accuracy is superior to that of hippocampal volumetry and is comparable to recently published svm-based whole-brain classification methods , which relied on a different strategy . this new method may become a useful tool to assist in the diagnosis of alzheimer 's disease .", "topics": ["support vector machine", "support vector machine"]}
{"title": "unsupervised learning of noisy-or bayesian networks", "abstract": "this paper considers the problem of learning the parameters in bayesian networks of discrete variables with known structure and hidden variables . previous approaches in these settings typically use expectation maximization ; when the network has high treewidth , the required expectations might be approximated using monte carlo or variational methods . we show how to avoid inference altogether during learning by giving a polynomial-time algorithm based on the method-of-moments , building upon recent work on learning discrete-valued mixture models . in particular , we show how to learn the parameters for a family of bipartite noisy-or bayesian networks . in our experimental results , we demonstrate an application of our algorithm to learning qmr-dt , a large bayesian network used for medical diagnosis . we show that it is possible to fully learn the parameters of qmr-dt even when only the findings are observed in the training data ( ground truth diseases unknown ) .", "topics": ["test set", "calculus of variations"]}
{"title": "skincure : an innovative smart phone-based application to assist in melanoma early detection and prevention", "abstract": "melanoma spreads through metastasis , and therefore it has been proven to be very fatal . statistical evidence has revealed that the majority of deaths resulting from skin cancer are as a result of melanoma . further investigations have shown that the survival rates in patients depend on the stage of the infection ; early detection and intervention of melanoma implicates higher chances of cure . clinical diagnosis and prognosis of melanoma is challenging since the processes are prone to misdiagnosis and inaccuracies due to doctors subjectivity . this paper proposes an innovative and fully functional smart-phone based application to assist in melanoma early detection and prevention . the application has two major components ; the first component is a real-time alert to help users prevent skin burn caused by sunlight ; a novel equation to compute the time for skin to burn is thereby introduced . the second component is an automated image analysis module which contains image acquisition , hair detection and exclusion , lesion segmentation , feature extraction , and classification . the proposed system exploits ph2 dermoscopy image database from pedro hispano hospital for development and testing purposes . the image database contains a total of 200 dermoscopy images of lesions , including normal , atypical , and melanoma cases . the experimental results show that the proposed system is efficient , achieving classification of the normal , atypical and melanoma images with accuracy of 96.3 % , 95.7 % and 97.5 % , respectively .", "topics": ["feature extraction"]}
{"title": "multi-stage suture detection for robot assisted anastomosis based on deep learning", "abstract": "in robotic surgery , task automation and learning from demonstration combined with human supervision is an emerging trend for many new surgical robot platforms . one such task is automated anastomosis , which requires bimanual needle handling and suture detection . due to the complexity of the surgical environment and varying patient anatomies , reliable suture detection is difficult , which is further complicated by occlusion and thread topologies . in this paper , we propose a multi-stage framework for suture thread detection based on deep learning . fully convolutional neural networks are used to obtain the initial detection and the overlapping status of suture thread , which are later fused with the original image to learn a gradient road map of the thread . based on the gradient road map , multiple segments of the thread are extracted and linked to form the whole thread using a curvilinear structure detector . experiments on two different types of sutures demonstrate the accuracy of the proposed framework .", "topics": ["reinforcement learning", "gradient"]}
{"title": "diversity handling in evolutionary landscape", "abstract": "the search ability of an evolutionary algorithm ( ea ) depends on the variation among the individuals in the population . maintaining an optimal level of diversity in the ea population is imperative to ensure that progress of the ea search is unhindered by premature convergence to suboptimal solutions . clearer understanding of the concept of population diversity , in the context of evolutionary search and premature convergence in particular , is the key to designing efficient eas . to this end , this paper first presents a comprehensive analysis of the ea population diversity issues . next we present an investigation on a counter-niching ea technique that introduces and maintains constructive diversity in the population . the proposed approach uses informed genetic operations to reach promising , but un-explored or under-explored areas of the search space , while discouraging premature local convergence . simulation runs on a number of standard benchmark test functions with genetic algorithm ( ga ) implementation shows promising results .", "topics": ["simulation"]}
{"title": "max-cost discrete function evaluation problem under a budget", "abstract": "we propose novel methods for max-cost discrete function evaluation problem ( dfep ) under budget constraints . we are motivated by applications such as clinical diagnosis where a patient is subjected to a sequence of ( possibly expensive ) tests before a decision is made . our goal is to develop strategies for minimizing max-costs . the problem is known to be np hard and greedy methods based on specialized impurity functions have been proposed . we develop a broad class of \\emph { admissible } impurity functions that admit monomials , classes of polynomials , and hinge-loss functions that allow for flexible impurity design with provably optimal approximation bounds . this flexibility is important for datasets when max-cost can be overly sensitive to `` outliers . '' outliers bias max-cost to a few examples that require a large number of tests for classification . we design admissible functions that allow for accuracy-cost trade-off and result in $ o ( \\log n ) $ guarantees of the optimal cost among trees with corresponding classification accuracy levels .", "topics": ["loss function", "polynomial"]}
{"title": "relaxing exclusive control in boolean games", "abstract": "in the typical framework for boolean games ( bg ) each player can change the truth value of some propositional atoms , while attempting to make her goal true . in standard bg goals are propositional formulas , whereas in iterated bg goals are formulas of linear temporal logic . both notions of bg are characterised by the fact that agents have exclusive control over their set of atoms , meaning that no two agents can control the same atom . in the present contribution we drop the exclusivity assumption and explore structures where an atom can be controlled by multiple agents . we introduce concurrent game structures with shared propositional control ( cgs-spc ) and show that they ac- count for several classes of repeated games , including iterated boolean games , influence games , and aggregation games . our main result shows that , as far as verification is concerned , cgs-spc can be reduced to concurrent game structures with exclusive control . this result provides a polynomial reduction for the model checking problem of specifications in alternating-time temporal logic on cgs-spc .", "topics": ["polynomial"]}
{"title": "sparse learning of maximum likelihood model for optimization of complex loss function", "abstract": "traditional machine learning methods usually minimize a simple loss function to learn a predictive model , and then use a complex performance measure to measure the prediction performance . however , minimizing a simple loss function can not guarantee that an optimal performance . in this paper , we study the problem of optimizing the complex performance measure directly to obtain a predictive model . we proposed to construct a maximum likelihood model for this problem , and to learn the model parameter , we minimize a com- plex loss function corresponding to the desired complex performance measure . to optimize the loss function , we approximate the upper bound of the complex loss . we also propose impose the sparsity to the model parameter to obtain a sparse model . an objective is constructed by combining the upper bound of the loss function and the sparsity of the model parameter , and we develop an iterative algorithm to minimize it by using the fast iterative shrinkage- thresholding algorithm framework . the experiments on optimization on three different complex performance measures , including f-score , receiver operating characteristic curve , and recall precision curve break even point , over three real-world applications , aircraft event recognition of civil aviation safety , in- trusion detection in wireless mesh networks , and image classification , show the advantages of the proposed method over state-of-the-art methods .", "topics": ["approximation algorithm", "mathematical optimization"]}
{"title": "link prediction in graphs with autoregressive features", "abstract": "in the paper , we consider the problem of link prediction in time-evolving graphs . we assume that certain graph features , such as the node degree , follow a vector autoregressive ( var ) model and we propose to use this information to improve the accuracy of prediction . our strategy involves a joint optimization procedure over the space of adjacency matrices and var matrices which takes into account both sparsity and low rank properties of the matrices . oracle inequalities are derived and illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank property . the estimate is computed efficiently using proximal methods through a generalized forward-backward agorithm .", "topics": ["sparse matrix"]}
{"title": "accurate estimators for improving minwise hashing and b-bit minwise hashing", "abstract": "minwise hashing is the standard technique in the context of search and databases for efficiently estimating set ( e.g . , high-dimensional 0/1 vector ) similarities . recently , b-bit minwise hashing was proposed which significantly improves upon the original minwise hashing in practice by storing only the lowest b bits of each hashed value , as opposed to using 64 bits . b-bit hashing is particularly effective in applications which mainly concern sets of high similarities ( e.g . , the resemblance > 0.5 ) . however , there are other important applications in which not just pairs of high similarities matter . for example , many learning algorithms require all pairwise similarities and it is expected that only a small fraction of the pairs are similar . furthermore , many applications care more about containment ( e.g . , how much one object is contained by another object ) than the resemblance . in this paper , we show that the estimators for minwise hashing and b-bit minwise hashing used in the current practice can be systematically improved and the improvements are most significant for set pairs of low resemblance and high containment .", "topics": ["database"]}
{"title": "object recognition with multi-scale pyramidal pooling networks", "abstract": "we present a multi-scale pyramidal pooling network , featuring a novel pyramidal pooling layer at multiple scales and a novel encoding layer . thanks to the former the network does not require all images of a given classification task to be of equal size . the encoding layer improves generalisation performance in comparison to similar neural network architectures , especially when training data is scarce . we evaluate and compare our system to convolutional neural networks and state-of-the-art computer vision methods on various benchmark datasets . we also present results on industrial steel defect classification , where existing architectures are not applicable because of the constraint on equally sized input images . the proposed architecture can be seen as a fully supervised hierarchical bag-of-features extension that is trained online and can be fine-tuned for any given task .", "topics": ["test set", "computer vision"]}
{"title": "interpretable r-cnn", "abstract": "this paper presents a method of learning qualitatively interpretable models in object detection using popular two-stage region-based convnet detection systems ( i.e . , r-cnn ) . r-cnn consists of a region proposal network and a roi ( region-of-interest ) prediction network.by interpretable models , we focus on weakly-supervised extractive rationale generation , that is learning to unfold latent discriminative part configurations of object instances automatically and simultaneously in detection without using any supervision for part configurations . we utilize a top-down hierarchical and compositional grammar model embedded in a directed acyclic and-or graph ( aog ) to explore and unfold the space of latent part configurations of rois . we propose an aogparsing operator to substitute the roipooling operator widely used in r-cnn , so the proposed method is applicable to many state-of-the-art convnet based detection systems . the aogparsing operator aims to harness both the explainable rigor of top-down hierarchical and compositional grammar models and the discriminative power of bottom-up deep neural networks through end-to-end training . in detection , a bounding box is interpreted by the best parse tree derived from the aog on-the-fly , which is treated as the extractive rationale generated for interpreting detection . in learning , we propose a folding-unfolding method to train the aog and convnet end-to-end . in experiments , we build on top of the r-fcn and test the proposed method on the pascal voc 2007 and 2012 datasets with performance comparable to state-of-the-art methods .", "topics": ["object detection", "parsing"]}
{"title": "a new low-rank tensor model for video completion", "abstract": "in this paper , we propose a new low-rank tensor model based on the circulant algebra , namely , twist tensor nuclear norm or t-tnn for short . the twist tensor denotes a 3-way tensor representation to laterally store 2d data slices in order . on one hand , t-tnn convexly relaxes the tensor multi-rank of the twist tensor in the fourier domain , which allows an efficient computation using fft . on the other , t-tnn is equal to the nuclear norm of block circulant matricization of the twist tensor in the original domain , which extends the traditional matrix nuclear norm in a block circulant way . we test the t-tnn model on a video completion application that aims to fill missing values and the experiment results validate its effectiveness , especially when dealing with video recorded by a non-stationary panning camera . the block circulant matricization of the twist tensor can be transformed into a circulant block representation with nuclear norm invariance . this representation , after transformation , exploits the horizontal translation relationship between the frames in a video , and endows the t-tnn model with a more powerful ability to reconstruct panning videos than the existing state-of-the-art low-rank models .", "topics": ["computation"]}
{"title": "transfer in a connectionist model of the acquisition of morphology", "abstract": "the morphological systems of natural languages are replete with examples of the same devices used for multiple purposes : ( 1 ) the same type of morphological process ( for example , suffixation for both noun case and verb tense ) and ( 2 ) identical morphemes ( for example , the same suffix for english noun plural and possessive ) . these sorts of similarity would be expected to convey advantages on language learners in the form of transfer from one morphological category to another . connectionist models of morphology acquisition have been faulted for their supposed inability to represent phonological similarity across morphological categories and hence to facilitate transfer . this paper describes a connectionist model of the acquisition of morphology which is shown to exhibit transfer of this type . the model treats the morphology acquisition problem as one of learning to map forms onto meanings and vice versa . as the network learns these mappings , it makes phonological generalizations which are embedded in connection weights . since these weights are shared by different morphological categories , transfer is enabled . in a set of experiments with artificial stimuli , networks were trained first on one morphological task ( e.g . , tense ) and then on a second ( e.g . , number ) . it is shown that in the context of suffixation , prefixation , and template rules , the second task is facilitated when the second category either makes use of the same forms or the same general process type ( e.g . , prefixation ) as the first .", "topics": ["feature vector", "recurrent neural network"]}
{"title": "facial surface analysis using iso-geodesic curves in three dimensional face recognition system", "abstract": "in this paper , we present an automatic 3d face recognition system . this system is based on the representation of human faces surfaces as collections of iso-geodesic curves ( igc ) using 3d fast marching algorithm . to compare two facial surfaces , we compute a geodesic distance between a pair of facial curves using a riemannian geometry . in the classifying step , we use : neural networks ( nn ) , k-nearest neighbor ( knn ) and support vector machines ( svm ) . to test this method and evaluate its performance , a simulation series of experiments were performed on 3d shape retrieval contest 2008 database ( shrec2008 ) .", "topics": ["support vector machine", "neural networks"]}
{"title": "scalable meta-learning for bayesian optimization", "abstract": "bayesian optimization has become a standard technique for hyperparameter optimization , including data-intensive models such as deep neural networks that may take days or weeks to train . we consider the setting where previous optimization runs are available , and we wish to use their results to warm-start a new optimization run . we develop an ensemble model that can incorporate the results of past optimization runs , while avoiding the poor scaling that comes with putting all results into a single gaussian process model . the ensemble combines models from past runs according to estimates of their generalization performance on the current optimization . results from a large collection of hyperparameter optimization benchmark problems and from optimization of a production computer vision platform at facebook show that the ensemble can substantially reduce the time it takes to obtain near-optimal configurations , and is useful for warm-starting expensive searches or running quick re-optimizations .", "topics": ["mathematical optimization", "computer vision"]}
{"title": "metapad : meta pattern discovery from massive text corpora", "abstract": "mining textual patterns in news , tweets , papers , and many other kinds of text corpora has been an active theme in text mining and nlp research . previous studies adopt a dependency parsing-based pattern discovery approach . however , the parsing results lose rich context around entities in the patterns , and the process is costly for a corpus of large scale . in this study , we propose a novel typed textual pattern structure , called meta pattern , which is extended to a frequent , informative , and precise subsequence pattern in certain context . we propose an efficient framework , called metapad , which discovers meta patterns from massive corpora with three techniques : ( 1 ) it develops a context-aware segmentation method to carefully determine the boundaries of patterns with a learnt pattern quality assessment function , which avoids costly dependency parsing and generates high-quality patterns ; ( 2 ) it identifies and groups synonymous meta patterns from multiple facets -- -their types , contexts , and extractions ; and ( 3 ) it examines type distributions of entities in the instances extracted by each group of patterns , and looks for appropriate type levels to make discovered patterns precise . experiments demonstrate that our proposed framework discovers high-quality typed textual patterns efficiently from different genres of massive corpora and facilitates information extraction .", "topics": ["natural language processing", "entity"]}
{"title": "log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing", "abstract": "this paper describes the submission of the amu ( adam mickiewicz university ) team to the automatic post-editing ( ape ) task of wmt 2016 . we explore the application of neural translation models to the ape problem and achieve good results by treating different models as components in a log-linear model , allowing for multiple inputs ( the mt-output and the source ) that are decoded to the same target language ( post-edited translations ) . a simple string-matching penalty integrated within the log-linear model is used to control for higher faithfulness with regard to the raw machine translation output . to overcome the problem of too little training data , we generate large amounts of artificial data . our submission improves over the uncorrected baseline on the unseen test set by -3.2\\ % ter and +5.5\\ % bleu and outperforms any other system submitted to the shared-task by a large margin .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "deep transform : time-domain audio error correction via probabilistic re-synthesis", "abstract": "in the process of recording , storage and transmission of time-domain audio signals , errors may be introduced that are difficult to correct in an unsupervised way . here , we train a convolutional deep neural network to re-synthesize input time-domain speech signals at its output layer . we then use this abstract transformation , which we call a deep transform ( dt ) , to perform probabilistic re-synthesis on further speech ( of the same speaker ) which has been degraded . using the convolutive dt , we demonstrate the recovery of speech audio that has been subject to extreme degradation . this approach may be useful for correction of errors in communications devices .", "topics": ["unsupervised learning"]}
{"title": "accurate vision-based vehicle localization using satellite imagery", "abstract": "we propose a method for accurately localizing ground vehicles with the aid of satellite imagery . our approach takes a ground image as input , and outputs the location from which it was taken on a georeferenced satellite image . we perform visual localization by estimating the co-occurrence probabilities between the ground and satellite images based on a ground-satellite feature dictionary . the method is able to estimate likelihoods over arbitrary locations without the need for a dense ground image database . we present a ranking-loss based algorithm that learns location-discriminative feature projection matrices that result in further improvements in accuracy . we evaluate our method on the malaga and kitti public datasets and demonstrate significant improvements over a baseline that performs exhaustive search .", "topics": ["baseline ( configuration management )", "dictionary"]}
{"title": "a critical reassessment of evolutionary algorithms on the cryptanalysis of the simplified data encryption standard algorithm", "abstract": "in this paper we analyze the cryptanalysis of the simplified data encryption standard algorithm using meta-heuristics and in particular genetic algorithms . the classic fitness function when using such an algorithm is to compare n-gram statistics of a the decrypted message with those of the target message . we show that using such a function is irrelevant in case of genetic algorithm , simply because there is no correlation between the distance to the real key ( the optimum ) and the value of the fitness , in other words , there is no hidden gradient . in order to emphasize this assumption we experimentally show that a genetic algorithm perform worse than a random search on the cryptanalysis of the simplified data encryption standard algorithm .", "topics": ["relevance", "gradient"]}
{"title": "from maxout to channel-out : encoding information on sparse pathways", "abstract": "motivated by an important insight from neural science , we propose a new framework for understanding the success of the recently proposed `` maxout '' networks . the framework is based on encoding information on sparse pathways and recognizing the correct pathway at inference time . elaborating further on this insight , we propose a novel deep network architecture , called `` channel-out '' network , which takes a much better advantage of sparse pathway encoding . in channel-out networks , pathways are not only formed a posteriori , but they are also actively selected according to the inference outputs from the lower layers . from a mathematical perspective , channel-out networks can represent a wider class of piece-wise continuous functions , thereby endowing the network with more expressive power than that of maxout networks . we test our channel-out networks on several well-known image classification benchmarks , setting new state-of-the-art performance on cifar-100 and stl-10 , which represent some of the `` harder '' image classification benchmarks .", "topics": ["computer vision", "sparse matrix"]}
{"title": "efficient computation in adaptive artificial spiking neural networks", "abstract": "artificial neural networks ( anns ) are bio-inspired models of neural computation that have proven highly effective . still , anns lack a natural notion of time , and neural units in anns exchange analog values in a frame-based manner , a computationally and energetically inefficient form of communication . this contrasts sharply with biological neurons that communicate sparingly and efficiently using binary spikes . while artificial spiking neural networks ( snns ) can be constructed by replacing the units of an ann with spiking neurons , the current performance is far from that of deep anns on hard benchmarks and these snns use much higher firing rates compared to their biological counterparts , limiting their efficiency . here we show how spiking neurons that employ an efficient form of neural coding can be used to construct snns that match high-performance anns and exceed state-of-the-art in snns on important benchmarks , while requiring much lower average firing rates . for this , we use spike-time coding based on the firing rate limiting adaptation phenomenon observed in biological spiking neurons . this phenomenon can be captured in adapting spiking neuron models , for which we derive the effective transfer function . neural units in anns trained with this transfer function can be substituted directly with adaptive spiking neurons , and the resulting adaptive snns ( adsnns ) can carry out inference in deep neural networks using up to an order of magnitude fewer spikes compared to previous snns . adaptive spike-time coding additionally allows for the dynamic control of neural coding precision : we show how a simple model of arousal in adsnns further halves the average required firing rate and this notion naturally extends to other forms of attention . adsnns thus hold promise as a novel and efficient model for neural computation that naturally fits to temporally continuous and asynchronous applications .", "topics": ["neural networks", "computation"]}
{"title": "an efficient , probabilistically sound algorithm for segmentation and word discovery", "abstract": "this paper presents a model-based , unsupervised algorithm for recovering word boundaries in a natural-language text from which they have been deleted . the algorithm is derived from a probability model of the source that generated the text . the fundamental structure of the model is specified abstractly so that the detailed component models of phonology , word-order , and word frequency can be replaced in a modular fashion . the model yields a language-independent , prior probability distribution on all possible sequences of all possible words over a given alphabet , based on the assumption that the input was generated by concatenating words from a fixed but unknown lexicon . the model is unusual in that it treats the generation of a complete corpus , regardless of length , as a single event in the probability space . accordingly , the algorithm does not estimate a probability distribution on words ; instead , it attempts to calculate the prior probabilities of various word sequences that could underlie the observed text . experiments on phonemic transcripts of spontaneous speech by parents to young children suggest that this algorithm is more effective than other proposed algorithms , at least when utterance boundaries are given and the text includes a substantial number of short utterances . keywords : bayesian grammar induction , probability models , minimum description length ( mdl ) , unsupervised learning , cognitive modeling , language acquisition , segmentation", "topics": ["natural language", "text corpus"]}
{"title": "ignoring distractors in the absence of labels : optimal linear projection to remove false positives during anomaly detection", "abstract": "in the anomaly detection setting , the native feature embedding can be a crucial source of bias . we present a technique , feature omission using context in unsupervised settings ( focus ) to learn a feature mapping that is invariant to changes exemplified in training sets while retaining as much descriptive power as possible . while this method could apply to many unsupervised settings , we focus on applications in anomaly detection , where little task-labeled data is available . our algorithm requires only non-anomalous sets of data , and does not require that the contexts in the training sets match the context of the test set . by maximizing within-set variance and minimizing between-set variance , we are able to identify and remove distracting features while retaining fidelity to the descriptiveness needed at test time . in the linear case , our formulation reduces to a generalized eigenvalue problem that can be solved quickly and applied to test sets outside the context of the training sets . this technique allows us to align technical definitions of anomaly detection with human definitions through appropriate mappings of the feature space . we demonstrate that this method is able to remove uninformative parts of the feature space for the anomaly detection setting .", "topics": ["test set", "feature vector"]}
{"title": "density-aware single image de-raining using a multi-stream dense network", "abstract": "single image rain streak removal is an extremely challenging problem due to the presence of non-uniform rain densities in images . we present a novel density-aware multi-stream densely connected convolutional neural network-based algorithm , called did-mdn , for joint rain density estimation and de-raining . the proposed method enables the network itself to automatically determine the rain-density information and then efficiently remove the corresponding rain-streaks guided by the estimated rain-density label . to better characterize rain-streaks with different scales and shapes , a multi-stream densely connected de-raining network is proposed which efficiently leverages features from different scales . furthermore , a new dataset containing images with rain-density labels is created and used to train the proposed density-aware network . extensive experiments on synthetic and real datasets demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art methods . in addition , an ablation study is performed to demonstrate the improvements obtained by different modules in the proposed method . code can be found at : https : //github.com/hezhangsprinter", "topics": ["synthetic data"]}
{"title": "experimenting with innate immunity", "abstract": "in a previous paper the authors argued the case for incorporating ideas from innate immunity into artificial immune systems ( aiss ) and presented an outline for a conceptual framework for such systems . a number of key general properties observed in the biological innate and adaptive immune systems were highlighted , and how such properties might be instantiated in artificial systems was discussed in detail . the next logical step is to take these ideas and build a software system with which aiss with these properties can be implemented and experimentally evaluated . this paper reports on the results of that step - the libtissue system .", "topics": ["sensor"]}
{"title": "critical learning periods in deep neural networks", "abstract": "critical periods are phases in the early development of humans and animals during which experience can affect the structure of neuronal networks irreversibly . in this work , we study the effects of visual stimulus deficits on the training of artificial neural networks ( anns ) . introducing well-characterized visual deficits , such as cataract-like blurring , in the early training phase of a standard deep neural network causes irreversible performance loss that closely mimics that reported in humans and animal models . deficits that do not affect low-level image statistics , such as vertical flipping of the images , have no lasting effect on the ann 's performance and can be rapidly overcome with additional training , as observed in humans . in addition , deeper networks show a more prominent critical period . to better understand this phenomenon , we use techniques from information theory to study the strength of the network connections during training . our analysis suggests that the first few epochs are critical for the allocation of resources across different layers , determined by the initial input data distribution . once such information organization is established , the network resources do not re-distribute through additional training . these findings suggest that the initial rapid learning phase of training of anns , under-scrutinized compared to its asymptotic behavior , plays a key role in defining the final performance of networks .", "topics": ["test set", "high- and low-level"]}
{"title": "saliency-based sequential image attention with multiset prediction", "abstract": "humans process visual scenes selectively and sequentially using attention . central to models of human visual attention is the saliency map . we propose a hierarchical visual architecture that operates on a saliency map and uses a novel attention mechanism to sequentially focus on salient regions and take additional glimpses within those regions . the architecture is motivated by human visual attention , and is used for multi-label image classification on a novel multiset task , demonstrating that it achieves high precision and recall while localizing objects with its attention . unlike conventional multi-label image classification models , the model supports multiset prediction due to a reinforcement-learning based training process that allows for arbitrary label permutation and multiple instances per label .", "topics": ["reinforcement learning", "computer vision"]}
{"title": "training of convolutional networks on multiple heterogeneous datasets for street scene semantic segmentation", "abstract": "we propose a convolutional network with hierarchical classifiers for per-pixel semantic segmentation , which is able to be trained on multiple , heterogeneous datasets and exploit their semantic hierarchy . our network is the first to be simultaneously trained on three different datasets from the intelligent vehicles domain , i.e . cityscapes , gtsdb and mapillary vistas , and is able to handle different semantic level-of-detail , class imbalances , and different annotation types , i.e . dense per-pixel and sparse bounding-box labels . we assess our hierarchical approach , by comparing against flat , non-hierarchical classifiers and we show improvements in mean pixel accuracy of 13.0 % for cityscapes classes and 2.4 % for vistas classes and 32.3 % for gtsdb classes . our implementation achieves inference rates of 17 fps at a resolution of 520 x 706 for 108 classes running on a gpu .", "topics": ["sparse matrix", "pixel"]}
{"title": "personrank : detecting important people in images", "abstract": "always , some individuals in images are more important/attractive than others in some events such as presentation , basketball game or speech . however , it is challenging to find important people among all individuals in images directly based on their spatial or appearance information due to the existence of diverse variations of pose , action , appearance of persons and various changes of occasions . we overcome this difficulty by constructing a multiple hyper-interaction graph to treat each individual in an image as a node and inferring the most active node referring to interactions estimated by various types of clews . we model pairwise interactions between persons as the edge message communicated between nodes , resulting in a bidirectional pairwise-interaction graph . to enrich the personperson interaction estimation , we further introduce a unidirectional hyper-interaction graph that models the consensus of interaction between a focal person and any person in a local region around . finally , we modify the pagerank algorithm to infer the activeness of persons on the multiple hybrid-interaction graph ( hig ) , the union of the pairwise-interaction and hyperinteraction graphs , and we call our algorithm the personrank . in order to provide publicable datasets for evaluation , we have contributed a new dataset called multi-scene important people image dataset and gathered a ncaa basketball image dataset from sports game sequences . we have demonstrated that the proposed personrank outperforms related methods clearly and substantially .", "topics": ["interaction"]}
{"title": "an analysis of iso 26262 : using machine learning safely in automotive software", "abstract": "machine learning ( ml ) plays an ever-increasing role in advanced automotive functionality for driver assistance and autonomous operation ; however , its adequacy from the perspective of safety certification remains controversial . in this paper , we analyze the impacts that the use of ml as an implementation approach has on iso 26262 safety lifecycle and ask what could be done to address them . we then provide a set of recommendations on how to adapt the standard to accommodate ml .", "topics": ["autonomous car"]}
{"title": "duality of graphical models and tensor networks", "abstract": "in this article we show the duality between tensor networks and undirected graphical models with discrete variables . we study tensor networks on hypergraphs , which we call tensor hypernetworks . we show that the tensor hypernetwork on a hypergraph exactly corresponds to the graphical model given by the dual hypergraph . we translate various notions under duality . for example , marginalization in a graphical model is dual to contraction in the tensor network . algorithms also translate under duality . we show that belief propagation corresponds to a known algorithm for tensor network contraction . this article is a reminder that the research areas of graphical models and tensor networks can benefit from interaction .", "topics": ["graphical model"]}
{"title": "larger is better : the effect of learning rates enjoyed by stochastic optimization with progressive variance reduction", "abstract": "in this paper , we propose a simple variant of the original stochastic variance reduction gradient ( svrg ) , where hereafter we refer to as the variance reduced stochastic gradient descent ( vr-sgd ) . different from the choices of the snapshot point and starting point in svrg and its proximal variant , prox-svrg , the two vectors of each epoch in vr-sgd are set to the average and last iterate of the previous epoch , respectively . this setting allows us to use much larger learning rates or step sizes than svrg , e.g . , 3/ ( 7l ) for vr-sgd vs 1/ ( 10l ) for svrg , and also makes our convergence analysis more challenging . in fact , a larger learning rate enjoyed by vr-sgd means that the variance of its stochastic gradient estimator asymptotically approaches zero more rapidly . unlike common stochastic methods such as svrg and proximal stochastic methods such as prox-svrg , we design two different update rules for smooth and non-smooth objective functions , respectively . in other words , vr-sgd can tackle non-smooth and/or non-strongly convex problems directly without using any reduction techniques such as quadratic regularizers . moreover , we analyze the convergence properties of vr-sgd for strongly convex problems , which show that vr-sgd attains a linear convergence rate . we also provide the convergence guarantees of vr-sgd for non-strongly convex problems . experimental results show that the performance of vr-sgd is significantly better than its counterparts , svrg and prox-svrg , and it is also much better than the best known stochastic method , katyusha .", "topics": ["gradient descent", "gradient"]}
{"title": "fast image classification by boosting fuzzy classifiers", "abstract": "this paper presents a novel approach to visual objects classification based on generating simple fuzzy classifiers using local image features to distinguish between one known class and other classes . boosting meta learning is used to find the most representative local features . the proposed approach is tested on a state-of-the-art image dataset and compared with the bag-of-features image representation model combined with the support vector machine classification . the novel method gives better classification accuracy and the time of learning and testing process is more than 30 % shorter .", "topics": ["support vector machine", "reinforcement learning"]}
{"title": "two-stage algorithm for fairness-aware machine learning", "abstract": "algorithmic decision making process now affects many aspects of our lives . standard tools for machine learning , such as classification and regression , are subject to the bias in data , and thus direct application of such off-the-shelf tools could lead to a specific group being unfairly discriminated . removing sensitive attributes of data does not solve this problem because a \\textit { disparate impact } can arise when non-sensitive attributes and sensitive attributes are correlated . here , we study a fair machine learning algorithm that avoids such a disparate impact when making a decision . inspired by the two-stage least squares method that is widely used in the field of economics , we propose a two-stage algorithm that removes bias in the training data . the proposed algorithm is conceptually simple . unlike most of existing fair algorithms that are designed for classification tasks , the proposed method is able to ( i ) deal with regression tasks , ( ii ) combine explanatory attributes to remove reverse discrimination , and ( iii ) deal with numerical sensitive attributes . the performance and fairness of the proposed algorithm are evaluated in simulations with synthetic and real-world datasets .", "topics": ["test set", "statistical classification"]}
{"title": "zero-shot task generalization with multi-task deep reinforcement learning", "abstract": "as a step towards developing zero-shot task generalization capabilities in reinforcement learning ( rl ) , we introduce a new rl problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks . in this problem , we consider two types of generalizations : to previously unseen instructions and to longer sequences of instructions . for generalization over unseen instructions , we propose a new objective which encourages learning correspondences between similar subtasks by making analogies . for generalization over sequential instructions , we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions . to deal with delayed reward , we propose a new neural architecture in the meta controller that learns when to update the subtask , which makes learning more efficient . experimental results on a stochastic 3d domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions .", "topics": ["reinforcement learning"]}
{"title": "production ready chatbots : generate if not retrieve", "abstract": "in this paper , we present a hybrid model that combines a neural conversational model and a rule-based graph dialogue system that assists users in scheduling reminders through a chat conversation . the graph based system has high precision and provides a grammatically accurate response but has a low recall . the neural conversation model can cater to a variety of requests , as it generates the responses word by word as opposed to using canned responses . the hybrid system shows significant improvements over the existing baseline system of rule based approach and caters to complex queries with a domain-restricted neural model . restricting the conversation topic and combination of graph based retrieval system with a neural generative model makes the final system robust enough for a real world application .", "topics": ["baseline ( configuration management )", "generative model"]}
{"title": "teaching uavs to race using sim4cv", "abstract": "automating the navigation of unmanned aerial vehicles ( uavs ) in diverse scenarios has gained much attention in the recent years . however , teaching uavs to fly in challenging environments remains an unsolved problem , mainly due to the lack of data for training . in this paper , we develop a photo-realistic simulator that can afford the generation of large amounts of training data ( both images rendered from the uav camera and its controls ) to teach a uav to autonomously race through challenging tracks . we train a deep neural network to predict uav controls from raw image data for the task of autonomous uav racing . training is done through imitation learning enabled by data augmentation to allow for the correction of navigation mistakes . extensive experiments demonstrate that our trained network ( when sufficient data augmentation is used ) outperforms state-of-the-art methods and flies more consistently than many human pilots .", "topics": ["test set", "simulation"]}
{"title": "deep fishing : gradient features from deep nets", "abstract": "convolutional networks ( convnets ) have recently improved image recognition performance thanks to end-to-end learning of deep feed-forward models from raw pixels . deep learning is a marked departure from the previous state of the art , the fisher vector ( fv ) , which relied on gradient-based encoding of local hand-crafted features . in this paper , we discuss a novel connection between these two approaches . first , we show that one can derive gradient representations from convnets in a similar fashion to the fv . second , we show that this gradient representation actually corresponds to a structured matrix that allows for efficient similarity computation . we experimentally study the benefits of transferring this representation over the outputs of convnet layers , and find consistent improvements on the pascal voc 2007 and 2012 datasets .", "topics": ["computer vision", "computation"]}
{"title": "automatic extraction of subcategorization frames for czech", "abstract": "we present some novel machine learning techniques for the identification of subcategorization information for verbs in czech . we compare three different statistical techniques applied to this problem . we show how the learning algorithm can be used to discover previously unknown subcategorization frames from the czech prague dependency treebank . the algorithm can then be used to label dependents of a verb in the czech treebank as either arguments or adjuncts . using our techniques , we ar able to achieve 88 % precision on unseen parsed text .", "topics": ["parsing"]}
{"title": "stable memory allocation in the hippocampus : fundamental limits and neural realization", "abstract": "it is believed that hippocampus functions as a memory allocator in brain , the mechanism of which remains unrevealed . in valiant 's neuroidal model , the hippocampus was described as a randomly connected graph , the computation on which maps input to a set of activated neuroids with stable size . valiant proposed three requirements for the hippocampal circuit to become a stable memory allocator ( sma ) : stability , continuity and orthogonality . the functionality of sma in hippocampus is essential in further computation within cortex , according to valiant 's model . in this paper , we put these requirements for memorization functions into rigorous mathematical formulation and introduce the concept of capacity , based on the probability of erroneous allocation . we prove fundamental limits for the capacity and error probability of sma , in both data-independent and data-dependent settings . we also establish an example of stable memory allocator that can be implemented via neuroidal circuits . both theoretical bounds and simulation results show that the neural sma functions well .", "topics": ["simulation", "computation"]}
{"title": "learning edge representations via low-rank asymmetric projections", "abstract": "we propose a new method for embedding graphs while preserving directed edge information . learning such continuous-space vector representations ( or embeddings ) of nodes in a graph is an important first step for using network information ( from social networks , user-item graphs , knowledge bases , etc . ) in many machine learning tasks . unlike previous work , we ( 1 ) explicitly model an edge as a function of node embeddings , and we ( 2 ) propose a novel objective , the `` graph likelihood '' , which contrasts information from sampled random walks with non-existent edges . individually , both of these contributions improve the learned representations , especially when there are memory constraints on the total size of the embeddings . when combined , our contributions enable us to significantly improve the state-of-the-art by learning more concise representations that better preserve the graph structure . we evaluate our method on a variety of link-prediction task including social networks , collaboration networks , and protein interactions , showing that our proposed method learn representations with error reductions of up to 76 % and 55 % , on directed and undirected graphs . in addition , we show that the representations learned by our method are quite space efficient , producing embeddings which have higher structure-preserving accuracy but are 10 times smaller .", "topics": ["interaction"]}
{"title": "clustering multi-way data : a novel algebraic approach", "abstract": "in this paper , we develop a method for unsupervised clustering of two-way ( matrix ) data by combining two recent innovations from different fields : the sparse subspace clustering ( ssc ) algorithm [ 10 ] , which groups points coming from a union of subspaces into their respective subspaces , and the t-product [ 18 ] , which was introduced to provide a matrix-like multiplication for third order tensors . our algorithm is analogous to ssc in that an `` affinity '' between different data points is built using a sparse self-representation of the data . unlike ssc , we employ the t-product in the self-representation . this allows us more flexibility in modeling ; infact , ssc is a special case of our method . when using the t-product , three-way arrays are treated as matrices whose elements ( scalars ) are n-tuples or tubes . convolutions take the place of scalar multiplication . this framework allows us to embed the 2-d data into a vector-space-like structure called a free module over a commutative ring . these free modules retain many properties of complex inner-product spaces , and we leverage that to provide theoretical guarantees on our algorithm . we show that compared to vector-space counterparts , ssmc achieves higher accuracy and better able to cluster data with less preprocessing in some image clustering problems . in particular we show the performance of the proposed method on weizmann face database , the extended yale b face database and the mnist handwritten digits database .", "topics": ["cluster analysis", "sparse matrix"]}
{"title": "named entity recognition using web document corpus", "abstract": "this paper introduces a named entity recognition approach in textual corpus . this named entity ( ne ) can be a named : location , person , organization , date , time , etc . , characterized by instances . a ne is found in texts accompanied by contexts : words that are left or right of the ne . the work mainly aims at identifying contexts inducing the ne 's nature . as such , the occurrence of the word `` president '' in a text , means that this word or context may be followed by the name of a president as president `` obama '' . likewise , a word preceded by the string `` footballer '' induces that this is the name of a footballer . ne recognition may be viewed as a classification method , where every word is assigned to a ne class , regarding the context . the aim of this study is then to identify and classify the contexts that are most relevant to recognize a ne , those which are frequently found with the ne . a learning approach using training corpus : web documents , constructed from learning examples is then suggested . frequency representations and modified tf-idf representations are used to calculate the context weights associated to context frequency , learning example frequency , and document frequency in the corpus .", "topics": ["statistical classification", "text corpus"]}
{"title": "applying a hybrid query translation method to japanese/english cross-language patent retrieval", "abstract": "this paper applies an existing query translation method to cross-language patent retrieval . in our method , multiple dictionaries are used to derive all possible translations for an input query , and collocational statistics are used to resolve translation ambiguity . we used japanese/english parallel patent abstracts to perform comparative experiments , where our method outperformed a simple dictionary-based query translation method , and achieved 76 % of monolingual retrieval in terms of average precision .", "topics": ["dictionary"]}
{"title": "relations on fp-soft sets applied to decision making problems", "abstract": "in this work , we first define relations on the fuzzy parametrized soft sets and study their properties . we also give a decision making method based on these relations . in approximate reasoning , relations on the fuzzy parametrized soft sets have shown to be of a primordial importance . finally , the method is successfully applied to a problems that contain uncertainties .", "topics": ["approximation algorithm"]}
{"title": "on context-dependent clustering of bandits", "abstract": "we investigate a novel cluster-of-bandit algorithm cab for collaborative recommendation tasks that implements the underlying feedback sharing mechanism by estimating the neighborhood of users in a context-dependent manner . cab makes sharp departures from the state of the art by incorporating collaborative effects into inference as well as learning processes in a manner that seamlessly interleaving explore-exploit tradeoffs and collaborative steps . we prove regret bounds under various assumptions on the data , which exhibit a crisp dependence on the expected number of clusters over the users , a natural measure of the statistical difficulty of the learning task . experiments on production and real-world datasets show that cab offers significantly increased prediction performance against a representative pool of state-of-the-art methods .", "topics": ["regret ( decision theory )", "cluster analysis"]}
{"title": "improved bilinear pooling with cnns", "abstract": "bilinear pooling of convolutional neural network ( cnn ) features [ 22 , 23 ] , and their compact variants [ 10 ] , have been shown to be effective at fine-grained recognition , scene categorization , texture recognition , and visual question-answering tasks among others . the resulting representation captures second-order statistics of convolutional features in a translationally invariant manner . in this paper we investigate various ways of normalizing these statistics to improve their representation power . in particular we find that the matrix square-root normalization offers significant improvements and outperforms alternative schemes such as the matrix logarithm normalization when combined with elementwise square-root and l2 normalization . this improves the accuracy by 2-3 % on a range of fine-grained recognition datasets leading to a new state of the art . we also investigate how the accuracy of matrix function computations effect network training and evaluation . in particular we compare against a technique for estimating matrix square-root gradients via solving a lyapunov equation that is more numerically accurate than computing gradients via a singular value decomposition ( svd ) . we find that while svd gradients are numerically inaccurate the overall effect on the final accuracy is negligible once boundary cases are handled carefully . we present an alternative scheme for computing gradients that is faster and yet it offers improvements over the baseline model . finally we show that the matrix square-root computed approximately using a few newton iterations is just as accurate for the classification task but allows an order-of-magnitude faster gpu implementation compared to svd decomposition .", "topics": ["baseline ( configuration management )", "numerical analysis"]}
{"title": "dimension correction for hierarchical latent class models", "abstract": "model complexity is an important factor to consider when selecting among graphical models . when all variables are observed , the complexity of a model can be measured by its standard dimension , i.e . the number of independent parameters . when hidden variables are present , however , standard dimension might no longer be appropriate . one should instead use effective dimension ( geiger et al . 1996 ) . this paper is concerned with the computation of effective dimension . first we present an upper bound on the effective dimension of a latent class ( lc ) model . this bound is tight and its computation is easy . we then consider a generalization of lc models called hierarchical latent class ( hlc ) models ( zhang 2002 ) . we show that the effective dimension of an hlc model can be obtained from the effective dimensions of some related lc models . we also demonstrate empirically that using effective dimension in place of standard dimension improves the quality of models learned from data .", "topics": ["graphical model", "computation"]}
{"title": "robust multiple manifolds structure learning", "abstract": "we present a robust multiple manifolds structure learning ( rmmsl ) scheme to robustly estimate data structures under the multiple low intrinsic dimensional manifolds assumption . in the local learning stage , rmmsl efficiently estimates local tangent space by weighted low-rank matrix factorization . in the global learning stage , we propose a robust manifold clustering method based on local structure learning results . the proposed clustering method is designed to get the flattest manifolds clusters by introducing a novel curved-level similarity function . our approach is evaluated and compared to state-of-the-art methods on synthetic data , handwritten digit images , human motion capture data and motorbike videos . we demonstrate the effectiveness of the proposed approach , which yields higher clustering accuracy , and produces promising results for challenging tasks of human motion segmentation and motion flow learning from videos .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "using a support-vector machine for japanese-to-english translation of tense , aspect , and modality", "abstract": "this paper describes experiments carried out using a variety of machine-learning methods , including the k-nearest neighborhood method that was used in a previous study , for the translation of tense , aspect , and modality . it was found that the support-vector machine method was the most precise of all the methods tested .", "topics": ["support vector machine"]}
{"title": "3-sat problem a new memetic-pso algorithm", "abstract": "3-sat problem is of great importance to many technical and scientific applications . this paper presents a new hybrid evolutionary algorithm for solving this satisfiability problem . 3-sat problem has the huge search space and hence it is known as a np-hard problem . so , deterministic approaches are not applicable in this context . thereof , application of evolutionary processing approaches and especially pso will be very effective for solving these kinds of problems . in this paper , we introduce a new evolutionary optimization technique based on pso , memetic algorithm and local search approaches . when some heuristics are mixed , their advantages are collected as well and we can reach to the better outcomes . finally , we test our proposed algorithm over some benchmarks used by some another available algorithms . obtained results show that our new method leads to the suitable results by the appropriate time . thereby , it achieves a better result in compared with the existent approaches such as pure genetic algorithm and some verified types", "topics": ["heuristic"]}
{"title": "artificial neural networks that learn to satisfy logic constraints", "abstract": "logic-based problems such as planning , theorem proving , or puzzles , typically involve combinatoric search and structured knowledge representation . artificial neural networks are very successful statistical learners , however , for many years , they have been criticized for their weaknesses in representing and in processing complex structured knowledge which is crucial for combinatoric search and symbol manipulation . two neural architectures are presented , which can encode structured relational knowledge in neural activation , and store bounded first order logic constraints in connection weights . both architectures learn to search for a solution that satisfies the constraints . learning is done by unsupervised practicing on problem instances from the same domain , in a way that improves the network-solving speed . no teacher exists to provide answers for the problem instances of the training and test sets . however , the domain constraints are provided as prior knowledge to a loss function that measures the degree of constraint violations . iterations of activation calculation and learning are executed until a solution that maximally satisfies the constraints emerges on the output units . as a test case , block-world planning problems are used to train networks that learn to plan in that domain , but the techniques proposed could be used more generally as in integrating prior symbolic knowledge with statistical learning", "topics": ["test set", "unsupervised learning"]}
{"title": "routing networks : adaptive selection of non-linear functions for multi-task learning", "abstract": "multi-task learning ( mtl ) with neural networks leverages commonalities in tasks to improve performance , but often suffers from task interference which reduces the benefits of transfer . to address this issue we introduce the routing network paradigm , a novel neural network and training algorithm . a routing network is a kind of self-organizing neural network consisting of two components : a router and a set of one or more function blocks . a function block may be any neural network - for example a fully-connected or a convolutional layer . given an input the router makes a routing decision , choosing a function block to apply and passing the output back to the router recursively , terminating when a fixed recursion depth is reached . in this way the routing network dynamically composes different function blocks for each input . we employ a collaborative multi-agent reinforcement learning ( marl ) approach to jointly train the router and function blocks . we evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the mnist , mini-imagenet , and cifar-100 datasets . our experiments demonstrate a significant improvement in accuracy , with sharper convergence . in addition , routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks . on cifar-100 ( 20 tasks ) we obtain cross-stitch performance levels with an 85 % reduction in training time .", "topics": ["reinforcement learning", "mnist database"]}
{"title": "grounding symbols in multi-modal instructions", "abstract": "as robots begin to cohabit with humans in semi-structured environments , the need arises to understand instructions involving rich variability -- -for instance , learning to ground symbols in the physical world . realistically , this task must cope with small datasets consisting of a particular users ' contextual assignment of meaning to terms . we present a method for processing a raw stream of cross-modal input -- -i.e . , linguistic instructions , visual perception of a scene and a concurrent trace of 3d eye tracking fixations -- -to produce the segmentation of objects with a correspondent association to high-level concepts . to test our framework we present experiments in a table-top object manipulation scenario . our results show our model learns the user 's notion of colour and shape from a small number of physical demonstrations , generalising to identifying physical referents for novel combinations of the words .", "topics": ["high- and low-level", "robot"]}
{"title": "snapshot difference imaging using time-of-flight sensors", "abstract": "computational photography encompasses a diversity of imaging techniques , but one of the core operations performed by many of them is to compute image differences . an intuitive approach to computing such differences is to capture several images sequentially and then process them jointly . usually , this approach leads to artifacts when recording dynamic scenes . in this paper , we introduce a snapshot difference imaging approach that is directly implemented in the sensor hardware of emerging time-of-flight cameras . with a variety of examples , we demonstrate that the proposed snapshot difference imaging technique is useful for direct-global illumination separation , for direct imaging of spatial and temporal image gradients , for direct depth edge imaging , and more .", "topics": ["pixel"]}
{"title": "learning transformed product distributions", "abstract": "we consider the problem of learning an unknown product distribution $ x $ over $ \\ { 0,1\\ } ^n $ using samples $ f ( x ) $ where $ f $ is a \\emph { known } transformation function . each choice of a transformation function $ f $ specifies a learning problem in this framework . information-theoretic arguments show that for every transformation function $ f $ the corresponding learning problem can be solved to accuracy $ \\eps $ , using $ \\tilde { o } ( n/\\eps^2 ) $ examples , by a generic algorithm whose running time may be exponential in $ n. $ we show that this learning problem can be computationally intractable even for constant $ \\eps $ and rather simple transformation functions . moreover , the above sample complexity bound is nearly optimal for the general problem , as we give a simple explicit linear transformation function $ f ( x ) =w \\cdot x $ with integer weights $ w_i \\leq n $ and prove that the corresponding learning problem requires $ \\omega ( n ) $ samples . as our main positive result we give a highly efficient algorithm for learning a sum of independent unknown bernoulli random variables , corresponding to the transformation function $ f ( x ) = \\sum_ { i=1 } ^n x_i $ . our algorithm learns to $ \\eps $ -accuracy in poly $ ( n ) $ time , using a surprising poly $ ( 1/\\eps ) $ number of samples that is independent of $ n. $ we also give an efficient algorithm that uses $ \\log n \\cdot \\poly ( 1/\\eps ) $ samples but has running time that is only $ \\poly ( \\log n , 1/\\eps ) . $", "topics": ["time complexity", "computational complexity theory"]}
{"title": "space as an invention of biological organisms", "abstract": "the question of the nature of space around us has occupied thinkers since the dawn of humanity , with scientists and philosophers today implicitly assuming that space is something that exists objectively . here we show that this does not have to be the case : the notion of space could emerge when biological organisms seek an economic representation of their sensorimotor flow . the emergence of spatial notions does not necessitate the existence of real physical space , but only requires the presence of sensorimotor invariants called `compensable ' sensory changes . we show mathematically and then in simulations that na\\ '' ive agents making no assumptions about the existence of space are able to learn these invariants and to build the abstract notion that physicists call rigid displacement , which is independent of what is being displaced . rigid displacements may underly perception of space as an unchanging medium within which objects are described by their relative positions . our findings suggest that the question of the nature of space , currently exclusive to philosophy and physics , should also be addressed from the standpoint of neuroscience and artificial intelligence .", "topics": ["simulation", "artificial intelligence"]}
{"title": "sengen : sentence generating neural variational topic model", "abstract": "we present a new topic model that generates documents by sampling a topic for one whole sentence at a time , and generating the words in the sentence using an rnn decoder that is conditioned on the topic of the sentence . we argue that this novel formalism will help us not only visualize and model the topical discourse structure in a document better , but also potentially lead to more interpretable topics since we can now illustrate topics by sampling representative sentences instead of bag of words or phrases . we present a variational auto-encoder approach for learning in which we use a factorized variational encoder that independently models the posterior over topical mixture vectors of documents using a feed-forward network , and the posterior over topic assignments to sentences using an rnn . our preliminary experiments on two different datasets indicate early promise , but also expose many challenges that remain to be addressed .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "a deeper look into sarcastic tweets using deep convolutional neural networks", "abstract": "sarcasm detection is a key task for many natural language processing tasks . in sentiment analysis , for example , sarcasm can flip the polarity of an `` apparently positive '' sentence and , hence , negatively affect polarity detection performance . to date , most approaches to sarcasm detection have treated the task primarily as a text categorization problem . sarcasm , however , can be expressed in very subtle ways and requires a deeper understanding of natural language that standard text categorization techniques can not grasp . in this work , we develop models based on a pre-trained convolutional neural network for extracting sentiment , emotion and personality features for sarcasm detection . such features , along with the network 's baseline features , allow the proposed models to outperform the state of the art on benchmark datasets . we also address the often ignored generalizability issue of classifying data that have not been seen by the models at learning phase .", "topics": ["baseline ( configuration management )", "statistical classification"]}
{"title": "mass-univariate hypothesis testing on meeg data using cross-validation", "abstract": "recent advances in statistical theory , together with advances in the computational power of computers , provide alternative methods to do mass-univariate hypothesis testing in which a large number of univariate tests , can be properly used to compare meeg data at a large number of time-frequency points and scalp locations . one of the major problematic aspects of this kind of mass-univariate analysis is due to high number of accomplished hypothesis tests . hence procedures that remove or alleviate the increased probability of false discoveries are crucial for this type of analysis . here , i propose a new method for mass-univariate analysis of meeg data based on cross-validation scheme . in this method , i suggest a hierarchical classification procedure under k-fold cross-validation to detect which sensors at which time-bin and which frequency-bin contributes in discriminating between two different stimuli or tasks . to achieve this goal , a new feature extraction method based on the discrete cosine transform ( dct ) employed to get maximum advantage of all three data dimensions . employing cross-validation and hierarchy architecture alongside the dct feature space makes this method more reliable and at the same time enough sensitive to detect the narrow effects in brain activities .", "topics": ["feature extraction", "sensor"]}
{"title": "the movie graph argument revisited", "abstract": "in this paper , we reexamine the movie graph argument , which demonstrates a basic incompatibility between computationalism and materialism . we discover that the incompatibility is only manifest in singular classical-like universes . if we accept that we live in a multiverse , then the incompatibility goes away , but in that case another line of argument shows that with computationalism , the fundamental , or primitive materiality has no causal influence on what is observed , which must must be derivable from basic arithmetic properties .", "topics": ["causality"]}
{"title": "unsupervised semantic action discovery from video collections", "abstract": "human communication takes many forms , including speech , text and instructional videos . it typically has an underlying structure , with a starting point , ending , and certain objective steps between them . in this paper , we consider instructional videos where there are tens of millions of them on the internet . we propose a method for parsing a video into such semantic steps in an unsupervised way . our method is capable of providing a semantic `` storyline '' of the video composed of its objective steps . we accomplish this using both visual and language cues in a joint generative model . our method can also provide a textual description for each of the identified semantic steps and video segments . we evaluate our method on a large number of complex youtube videos and show that our method discovers semantically correct instructions for a variety of tasks .", "topics": ["generative model", "cluster analysis"]}
{"title": "image segmentation for fruit detection and yield estimation in apple orchards", "abstract": "ground vehicles equipped with monocular vision systems are a valuable source of high resolution image data for precision agriculture applications in orchards . this paper presents an image processing framework for fruit detection and counting using orchard image data . a general purpose image segmentation approach is used , including two feature learning algorithms ; multi-scale multi-layered perceptrons ( mlp ) and convolutional neural networks ( cnn ) . these networks were extended by including contextual information about how the image data was captured ( metadata ) , which correlates with some of the appearance variations and/or class distributions observed in the data . the pixel-wise fruit segmentation output is processed using the watershed segmentation ( ws ) and circular hough transform ( cht ) algorithms to detect and count individual fruits . experiments were conducted in a commercial apple orchard near melbourne , australia . the results show an improvement in fruit segmentation performance with the inclusion of metadata on the previously benchmarked mlp network . we extend this work with cnns , bringing agrovision closer to the state-of-the-art in computer vision , where although metadata had negligible influence , the best pixel-wise f1-score of $ 0.791 $ was achieved . the ws algorithm produced the best apple detection and counting results , with a detection f1-score of $ 0.858 $ . as a final step , image fruit counts were accumulated over multiple rows at the orchard and compared against the post-harvest fruit counts that were obtained from a grading and counting machine . the count estimates using cnn and ws resulted in the best performance for this dataset , with a squared correlation coefficient of $ r^2=0.826 $ .", "topics": ["feature learning", "image processing"]}
{"title": "nonlocal low-rank tensor factor analysis for image restoration", "abstract": "low-rank signal modeling has been widely leveraged to capture non-local correlation in image processing applications . we propose a new method that employs low-rank tensor factor analysis for tensors generated by grouped image patches . the low-rank tensors are fed into the alternative direction multiplier method ( admm ) to further improve image reconstruction . the motivating application is compressive sensing ( cs ) , and a deep convolutional architecture is adopted to approximate the expensive matrix inversion in cs applications . an iterative algorithm based on this low-rank tensor factorization strategy , called nlr-tfa , is presented in detail . experimental results on noiseless and noisy cs measurements demonstrate the superiority of the proposed approach , especially at low cs sampling rates .", "topics": ["sampling ( signal processing )", "image processing"]}
{"title": "an evolutionary squeaky wheel optimisation approach to personnel scheduling", "abstract": "the quest for robust heuristics that are able to solve more than one problem is ongoing . in this paper , we present , discuss and analyse a technique called evolutionary squeaky wheel optimisation and apply it to two different personnel scheduling problems . evolutionary squeaky wheel optimisation improves the original squeaky wheel optimisation 's effectiveness and execution speed by incorporating two extra steps ( selection and mutation ) for added evolution . in the evolutionary squeaky wheel optimisation , a cycle of analysis-selection-mutation-prioritization-construction continues until stopping conditions are reached . the aim of the analysis step is to identify below average solution components by calculating a fitness value for all components . the selection step then chooses amongst these underperformers and discards some probabilistically based on fitness . the mutation step further discards a few components at random . solutions can become incomplete and thus repairs may be required . the repairs are carried out by using the prioritization to first produce priorities that determine an order by which the following construction step then schedules the remaining components . therefore , improvement in the evolutionary squeaky wheel optimisation is achieved by selective solution disruption mixed with interative improvement and constructive repair . strong experimental results are reported on two different domains of personnel scheduling : bus and rail driver scheduling and hospital nurse scheduling .", "topics": ["mathematical optimization", "iteration"]}
{"title": "perturbative black box variational inference", "abstract": "black box variational inference ( bbvi ) with reparameterization gradients triggered the exploration of divergence measures other than the kullback-leibler ( kl ) divergence , such as alpha divergences . in this paper , we view bbvi with generalized divergences as a form of estimating the marginal likelihood via biased importance sampling . the choice of divergence determines a bias-variance trade-off between the tightness of a bound on the marginal likelihood ( low bias ) and the variance of its gradient estimators . drawing on variational perturbation theory of statistical physics , we use these insights to construct a family of new variational bounds . enumerated by an odd integer order $ k $ , this family captures the standard kl bound for $ k=1 $ , and converges to the exact marginal likelihood as $ k\\to\\infty $ . compared to alpha-divergences , our reparameterization gradients have a lower variance . we show in experiments on gaussian processes and variational autoencoders that the new bounds are more mass covering , and that the resulting posterior covariances are closer to the true posterior and lead to higher likelihoods on held-out data .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "variational dropout and the local reparameterization trick", "abstract": "we investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational bayesian inference ( sgvb ) of a posterior over model parameters , while retaining parallelizability . this local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch . such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size , generally leading to much faster convergence . additionally , we explore a connection with dropout : gaussian dropout objectives correspond to sgvb with local reparameterization , a scale-invariant prior and proportionally fixed posterior variance . our method allows inference of more flexibly parameterized posteriors ; specifically , we propose variational dropout , a generalization of gaussian dropout where the dropout rates are learned , often leading to better models . the method is demonstrated through several experiments .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "decoupled learning for conditional adversarial networks", "abstract": "incorporating encoding-decoding nets with adversarial nets has been widely adopted in image generation tasks . we observe that the state-of-the-art achievements were obtained by carefully balancing the reconstruction loss and adversarial loss , and such balance shifts with different network structures , datasets , and training strategies . empirical studies have demonstrated that an inappropriate weight between the two losses may cause instability , and it is tricky to search for the optimal setting , especially when lacking prior knowledge on the data and network . this paper gives the first attempt to relax the need of manual balancing by proposing the concept of \\textit { decoupled learning } , where a novel network structure is designed that explicitly disentangles the backpropagation paths of the two losses . experimental results demonstrate the effectiveness , robustness , and generality of the proposed method . the other contribution of the paper is the design of a new evaluation metric to measure the image quality of generative models . we propose the so-called \\textit { normalized relative discriminative score } ( nrds ) , which introduces the idea of relative comparison , rather than providing absolute estimates like existing metrics .", "topics": ["generative model"]}
{"title": "interpretable graph-based semi-supervised learning via flows", "abstract": "in this paper , we consider the interpretability of the foundational laplacian-based semi-supervised learning approaches on graphs . we introduce a novel flow-based learning framework that subsumes the foundational approaches and additionally provides a detailed , transparent , and easily understood expression of the learning process in terms of graph flows . as a result , one can visualize and interactively explore the precise subgraph along which the information from labeled nodes flows to an unlabeled node of interest . surprisingly , the proposed framework avoids trading accuracy for interpretability , but in fact leads to improved prediction accuracy , which is supported both by theoretical considerations and empirical results . the flow-based framework guarantees the maximum principle by construction and can handle directed graphs in an out-of-the-box manner .", "topics": ["supervised learning"]}
{"title": "airport gate assignment : new model and implementation", "abstract": "airport gate assignment is of great importance in airport operations . in this paper , we study the airport gate assignment problem ( agap ) , propose a new model and implement the model with optimization programming language ( opl ) . with the objective to minimize the number of conflicts of any two adjacent aircrafts assigned to the same gate , we build a mathematical model with logical constraints and the binary constraints , which can provide an efficient evaluation criterion for the airlines to estimate the current gate assignment . to illustrate the feasibility of the model we construct experiments with the data obtained from continental airlines , houston gorge bush intercontinental airport iah , which indicate that our model is both energetic and effective . moreover , we interpret experimental results , which further demonstrate that our proposed model can provide a powerful tool for airline companies to estimate the efficiency of their current work of gate assignment .", "topics": ["eisenstein 's criterion"]}
{"title": "sliced wasserstein generative models", "abstract": "in the paper , we introduce a model of sliced optimal transport ( sot ) , which measures the distribution affinity with sliced wasserstein distance ( swd ) . since swd enjoys the property of factorizing high-dimensional joint distributions into their multiple one-dimensional marginal distributions , its dual and primal forms can be approximated easier compared to wasserstein distance ( wd ) . thus , we propose two types of differentiable sot blocks to equip modern generative frameworks -- -auto-encoders ( aes ) and generative adversarial networks ( gans ) -- -with the primal and dual forms of swd . the superiority of our swae and swgan over the state-of-the-art generative models is studied both qualitatively and quantitatively on standard benchmarks .", "topics": ["generative model", "loss function"]}
{"title": "mnl-bandit : a dynamic learning approach to assortment selection", "abstract": "we consider a dynamic assortment selection problem , where in every round the retailer offers a subset ( assortment ) of $ n $ substitutable products to a consumer , who selects one of these products according to a multinomial logit ( mnl ) choice model . the retailer observes this choice and the objective is to dynamically learn the model parameters , while optimizing cumulative revenues over a selling horizon of length $ t $ . we refer to this exploration-exploitation formulation as the mnl-bandit problem . existing methods for this problem follow an `` explore-then-exploit '' approach , which estimate parameters to a desired accuracy and then , treating these estimates as if they are the correct parameter values , offers the optimal assortment based on these estimates . these approaches require certain a priori knowledge of `` separability '' , determined by the true parameters of the underlying mnl model , and this in turn is critical in determining the length of the exploration period . ( separability refers to the distinguishability of the true optimal assortment from the other sub-optimal alternatives . ) in this paper , we give an efficient algorithm that simultaneously explores and exploits , achieving performance independent of the underlying parameters . the algorithm can be implemented in a fully online manner , without knowledge of the horizon length $ t $ . furthermore , the algorithm is adaptive in the sense that its performance is near-optimal in both the `` well separated '' case , as well as the general parameter setting where this separation need not hold .", "topics": ["value ( ethics )"]}
{"title": "sentiment analysis of code-mixed indian languages : an overview of sail_code-mixed shared task @ icon-2017", "abstract": "sentiment analysis is essential in many real-world applications such as stance detection , review analysis , recommendation system , and so on . sentiment analysis becomes more difficult when the data is noisy and collected from social media . india is a multilingual country ; people use more than one languages to communicate within themselves . the switching in between the languages is called code-switching or code-mixing , depending upon the type of mixing . this paper presents overview of the shared task on sentiment analysis of code-mixed data pairs of hindi-english and bengali-english collected from the different social media platform . the paper describes the task , dataset , evaluation , baseline and participant 's systems .", "topics": ["baseline ( configuration management )"]}
{"title": "statistically efficient thinning of a markov chain sampler", "abstract": "it is common to subsample markov chain output to reduce the storage burden . geyer ( 1992 ) shows that discarding $ k-1 $ out of every $ k $ observations will not improve statistical efficiency , as quantified through variance in a given computational budget . that observation is often taken to mean that thinning mcmc output can not improve statistical efficiency . here we suppose that it costs one unit of time to advance a markov chain and then $ \\theta > 0 $ units of time to compute a sampled quantity of interest . for a thinned process , that cost $ \\theta $ is incurred less often , so it can be advanced through more stages . here we provide examples to show that thinning will improve statistical efficiency if $ \\theta $ is large and the sample autocorrelations decay slowly enough . if the lag $ \\ell\\ge1 $ autocorrelations of a scalar measurement satisfy $ \\rho_\\ell\\ge\\rho_ { \\ell+1 } \\ge0 $ , then there is always a $ \\theta < \\infty $ at which thinning becomes more efficient for averages of that scalar . many sample autocorrelation functions resemble first order ar ( 1 ) processes with $ \\rho_\\ell =\\rho^ { |\\ell| } $ for some $ -1 < \\rho < 1 $ . for an ar ( 1 ) process it is possible to compute the most efficient subsampling frequency $ k $ . the optimal $ k $ grows rapidly as $ \\rho $ increases towards $ 1 $ . the resulting efficiency gain depends primarily on $ \\theta $ , not $ \\rho $ . taking $ k=1 $ ( no thinning ) is optimal when $ \\rho\\le0 $ . for $ \\rho > 0 $ it is optimal if and only if $ \\theta \\le ( 1-\\rho ) ^2/ ( 2\\rho ) $ . this efficiency gain never exceeds $ 1+\\theta $ . this paper also gives efficiency bounds for autocorrelations bounded between those of two ar ( 1 ) processes .", "topics": ["sampling ( signal processing )", "computation"]}
{"title": "early prediction of the duration of protests using probabilistic latent dirichlet allocation and decision trees", "abstract": "protests and agitations are an integral part of every democratic civil society . in recent years , south africa has seen a large increase in its protests . the objective of this paper is to provide an early prediction of the duration of protests from its free flowing english text description . free flowing descriptions of the protests help us in capturing its various nuances such as multiple causes , courses of actions etc . next we use a combination of unsupervised learning ( topic modeling ) and supervised learning ( decision trees ) to predict the duration of the protests . our results show a high degree ( close to 90 % ) of accuracy in early prediction of the duration of protests.we expect the work to help police and other security services in planning and managing their resources in better handling protests in future .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "topic model based multi-label classification from the crowd", "abstract": "multi-label classification is a common supervised machine learning problem where each instance is associated with multiple classes . the key challenge in this problem is learning the correlations between the classes . an additional challenge arises when the labels of the training instances are provided by noisy , heterogeneous crowdworkers with unknown qualities . we first assume labels from a perfect source and propose a novel topic model where the present as well as the absent classes generate the latent topics and hence the words . we non-trivially extend our topic model to the scenario where the labels are provided by noisy crowdworkers . extensive experimentation on real world datasets reveals the superior performance of the proposed model . the proposed model learns the qualities of the annotators as well , even with minimal training data .", "topics": ["test set", "supervised learning"]}
{"title": "neural networks and continuous time", "abstract": "the fields of neural computation and artificial neural networks have developed much in the last decades . most of the works in these fields focus on implementing and/or learning discrete functions or behavior . however , technical , physical , and also cognitive processes evolve continuously in time . this can not be described directly with standard architectures of artificial neural networks such as multi-layer feed-forward perceptrons . therefore , in this paper , we will argue that neural networks modeling continuous time are needed explicitly for this purpose , because with them the synthesis and analysis of continuous and possibly periodic processes in time are possible ( e.g . for robot behavior ) besides computing discrete classification functions ( e.g . for logical reasoning ) . we will relate possible neural network architectures with ( hybrid ) automata models that allow to express continuous processes .", "topics": ["neural networks", "computation"]}
{"title": "convolutional kernel networks", "abstract": "an important goal in visual recognition is to devise image representations that are invariant to particular transformations . in this paper , we address this goal with a new type of convolutional neural network ( cnn ) whose invariance is encoded by a reproducing kernel . unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task , our network learns to approximate the kernel feature map on training data . such an approach enjoys several benefits over classical ones . first , by teaching cnns to be invariant , we obtain simple network architectures that achieve a similar accuracy to more complex ones , while being easy to train and robust to overfitting . second , we bridge a gap between the neural network literature and kernels , which are natural tools to model invariance . we evaluate our methodology on visual recognition tasks where cnns have proven to perform well , e.g . , digit recognition with the mnist dataset , and the more challenging cifar-10 and stl-10 datasets , where our accuracy is competitive with the state of the art .", "topics": ["kernel ( operating system )", "test set"]}
{"title": "high-dimensional probability estimation with deep density models", "abstract": "one of the fundamental problems in machine learning is the estimation of a probability distribution from data . many techniques have been proposed to study the structure of data , most often building around the assumption that observations lie on a lower-dimensional manifold of high probability . it has been more difficult , however , to exploit this insight to build explicit , tractable density models for high-dimensional data . in this paper , we introduce the deep density model ( ddm ) , a new approach to density estimation . we exploit insights from deep learning to construct a bijective map to a representation space , under which the transformation of the distribution of the data is approximately factorized and has identical and known marginal densities . the simplicity of the latent distribution under the model allows us to feasibly explore it , and the invertibility of the map to characterize contraction of measure across it . this enables us to compute normalized densities for out-of-sample data . this combination of tractability and flexibility allows us to tackle a variety of probabilistic tasks on high-dimensional datasets , including : rapid computation of normalized densities at test-time without evaluating a partition function ; generation of samples without mcmc ; and characterization of the joint entropy of the data .", "topics": ["computation"]}
{"title": "an extended theory of head-driven parsing", "abstract": "we show that more head-driven parsing algorithms can be formulated than those occurring in the existing literature . these algorithms are inspired by a family of left-to-right parsing algorithms from a recent publication . we further introduce a more advanced notion of `` head-driven parsing '' which allows more detailed specification of the processing order of non-head elements in the right-hand side . we develop a parsing algorithm for this strategy , based on lr parsing techniques .", "topics": ["parsing"]}
{"title": "magent : a many-agent reinforcement learning platform for artificial collective intelligence", "abstract": "we introduce magent , a platform to support research and development of many-agent reinforcement learning . unlike previous research platforms on single or multi-agent reinforcement learning , magent focuses on supporting the tasks and the applications that require hundreds to millions of agents . within the interactions among a population of agents , it enables not only the study of learning algorithms for agents ' optimal polices , but more importantly , the observation and understanding of individual agent 's behaviors and social phenomena emerging from the ai society , including communication languages , leaderships , altruism . magent is highly scalable and can host up to one million agents on a single gpu server . magent also provides flexible configurations for ai researchers to design their customized environments and agents . in this demo , we present three environments designed on magent and show emerged collective intelligence by learning from scratch .", "topics": ["reinforcement learning", "interaction"]}
{"title": "sparse matrix factorization", "abstract": "we investigate the problem of factorizing a matrix into several sparse matrices and propose an algorithm for this under randomness and sparsity assumptions . this problem can be viewed as a simplification of the deep learning problem where finding a factorization corresponds to finding edges in different layers and values of hidden units . we prove that under certain assumptions for a sparse linear deep network with $ n $ nodes in each layer , our algorithm is able to recover the structure of the network and values of top layer hidden units for depths up to $ \\tilde o ( n^ { 1/6 } ) $ . we further discuss the relation among sparse matrix factorization , deep learning , sparse recovery and dictionary learning .", "topics": ["sparse matrix", "dictionary"]}
{"title": "rra : recurrent residual attention for sequence learning", "abstract": "in this paper , we propose a recurrent neural network ( rnn ) with residual attention ( rra ) to learn long-range dependencies from sequential data . we propose to add residual connections across timesteps to rnn , which explicitly enhances the interaction between current state and hidden states that are several timesteps apart . this also allows training errors to be directly back-propagated through residual connections and effectively alleviates gradient vanishing problem . we further reformulate an attention mechanism over residual connections . an attention gate is defined to summarize the individual contribution from multiple previous hidden states in computing the current state . we evaluate rra on three tasks : the adding problem , pixel-by-pixel mnist classification and sentiment analysis on the imdb dataset . our experiments demonstrate that rra yields better performance , faster convergence and more stable training compared to a standard lstm network . furthermore , rra shows highly competitive performance to the state-of-the-art methods .", "topics": ["recurrent neural network", "gradient descent"]}
{"title": "fingertip detection : a fast method with natural hand", "abstract": "many vision based applications have used fingertips to track or manipulate gestures in their applications . gesture identification is a natural way to pass the signals to the machine , as the human express its feelings most of the time with hand expressions . here a novel time efficient algorithm has been described for fingertip detection . this method is invariant to hand direction and in preprocessing it cuts only hand part from the full image , hence further computation would be much faster than processing full image . binary silhouette of the input image is generated using hsv color space based skin filter and hand cropping done based on intensity histogram of the hand image", "topics": ["image processing", "image segmentation"]}
{"title": "extension of sparse randomized kaczmarz algorithm for multiple measurement vectors", "abstract": "the kaczmarz algorithm is popular for iteratively solving an overdetermined system of linear equations . the traditional kaczmarz algorithm can approximate the solution in few sweeps through the equations but a randomized version of the kaczmarz algorithm was shown to converge exponentially and independent of number of equations . recently an algorithm for finding sparse solution to a linear system of equations has been proposed based on weighted randomized kaczmarz algorithm . these algorithms solves single measurement vector problem ; however there are applications were multiple-measurements are available . in this work , the objective is to solve a multiple measurement vector problem with common sparse support by modifying the randomized kaczmarz algorithm . we have also modeled the problem of face recognition from video as the multiple measurement vector problem and solved using our proposed technique . we have compared the proposed algorithm with state-of-art spectral projected gradient algorithm for multiple measurement vectors on both real and synthetic datasets . the monte carlo simulations confirms that our proposed algorithm have better recovery and convergence rate than the mmv version of spectral projected gradient algorithm under fairness constraints .", "topics": ["synthetic data", "simulation"]}
{"title": "deep keyphrase generation", "abstract": "keyphrase provides highly-summative information that can be effectively used for understanding , organizing and retrieving text content . though previous studies have provided many workable solutions for automated keyphrase extraction , they commonly divided the to-be-summarized content into multiple text chunks , then ranked and selected the most meaningful ones . these approaches could neither identify keyphrases that do not appear in the text , nor capture the real semantic meaning behind the text . we propose a generative model for keyphrase prediction with an encoder-decoder framework , which can effectively overcome the above drawbacks . we name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of the content with a deep learning method . empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text , but also can generate absent keyphrases based on the semantic meaning of the text . code and dataset are available at https : //github.com/memray/seq2seq-keyphrase .", "topics": ["generative model", "encoder"]}
{"title": "variational walkback : learning a transition operator as a stochastic recurrent net", "abstract": "we propose a novel method to directly learn a stochastic transition operator whose repeated application provides generated samples . traditional undirected graphical models approach this problem indirectly by learning a markov chain model whose stationary distribution obeys detailed balance with respect to a parameterized energy function . the energy function is then modified so the model and data distributions match , with no guarantee on the number of steps required for the markov chain to converge . moreover , the detailed balance condition is highly restrictive : energy based models corresponding to neural networks must have symmetric weights , unlike biological neural circuits . in contrast , we develop a method for directly learning arbitrarily parameterized transition operators capable of expressing non-equilibrium stationary distributions that violate detailed balance , thereby enabling us to learn more biologically plausible asymmetric neural networks and more general non-energy based dynamical systems . the proposed training objective , which we derive via principled variational methods , encourages the transition operator to `` walk back '' in multi-step trajectories that start at data-points , as quickly as possible back to the original data points . we present a series of experimental results illustrating the soundness of the proposed approach , variational walkback ( vw ) , on the mnist , cifar-10 , svhn and celeba datasets , demonstrating superior samples compared to earlier attempts to learn a transition operator . we also show that although each rapid training trajectory is limited to a finite but variable number of steps , our transition operator continues to generate good samples well past the length of such trajectories , thereby demonstrating the match of its non-equilibrium stationary distribution to the data distribution . source code : http : //github.com/anirudh9119/walkback_nips17", "topics": ["graphical model", "calculus of variations"]}
{"title": "a feasible roadmap for developing volumetric probability atlas of localized prostate cancer", "abstract": "a statistical volumetric model , showing the probability map of localized prostate cancer within the host anatomical structure , has been developed from 90 optically-imaged surgical specimens . this master model permits an accurate characterization of prostate cancer distribution patterns and an atlas-informed biopsy sampling strategy . the model is constructed by mapping individual prostate models onto a site model , together with localized tumors . an accurate multi-object non-rigid warping scheme is developed based on a mixture of principal-axis registrations . we report our evaluation and pilot studies on the effectiveness of the method and its application to optimizing needle biopsy strategies .", "topics": ["image processing", "simulation"]}
{"title": "hierarchical classification for spoken arabic dialect identification using prosody : case of algerian dialects", "abstract": "in daily communications , arabs use local dialects which are hard to identify automatically using conventional classification methods . the dialect identification challenging task becomes more complicated when dealing with an under-resourced dialects belonging to a same county/region . in this paper , we start by analyzing statistically algerian dialects in order to capture their specificities related to prosody information which are extracted at utterance level after a coarse-grained consonant/vowel segmentation . according to these analysis findings , we propose a hierarchical classification approach for spoken arabic algerian dialect identification ( hadid ) . it takes advantage from the fact that dialects have an inherent property of naturally structured into hierarchy . within hadid , a top-down hierarchical classification is applied , in which we use deep neural networks ( dnns ) method to build a local classifier for every parent node into the hierarchy dialect structure . our framework is implemented and evaluated on algerian arabic dialects corpus . whereas , the hierarchy dialect structure is deduced from historic and linguistic knowledges . the results reveal that within { \\hd } , the best classifier is dnns compared to support vector machine . in addition , compared with a baseline flat classification system , our hadid gives an improvement of 63.5 % in term of precision . furthermore , overall results evidence the suitability of our prosody-based hadid for speaker independent dialect identification while requiring less than 6s test utterances .", "topics": ["baseline ( configuration management )", "support vector machine"]}
{"title": "the most advantageous bangla keyboard layout using data mining technique", "abstract": "bangla alphabet has a large number of letters , for this it is complicated to type faster using bangla keyboard . the proposed keyboard will maximize the speed of operator as they can type with both hands parallel . association rule of data mining to distribute the bangla characters in the keyboard is used here . the frequencies of data consisting of monograph , digraph and trigraph are analyzed , which are derived from data wire-house , and then used association rule of data mining to distribute the bangla characters in the layout . experimental results on several data show the effectiveness of the proposed approach with better performance . this paper presents an optimal bangla keyboard layout , which distributes the load equally on both hands so that maximizing the ease and minimizing the effort .", "topics": ["data mining"]}
{"title": "multi-objective analysis of computational models", "abstract": "computational models are of increasing complexity and their behavior may in particular emerge from the interaction of different parts . studying such models becomes then more and more difficult and there is a need for methods and tools supporting this process . multi-objective evolutionary algorithms generate a set of trade-off solutions instead of a single optimal solution . the availability of a set of solutions that have the specificity to be optimal relative to carefully chosen objectives allows to perform data mining in order to better understand model features and regularities . we review the corresponding work , propose a unifying framework , and highlight its potential use . typical questions that such a methodology allows to address are the following : what are the most critical parameters of the model ? what are the relations between the parameters and the objectives ? what are the typical behaviors of the model ? two examples are provided to illustrate the capabilities of the methodology . the features of a flapping-wing robot are thus evaluated to find out its speed-energy relation , together with the criticality of its parameters . a neurocomputational model of the basal ganglia brain nuclei is then considered and its most salient features according to this methodology are presented and discussed .", "topics": ["data mining", "optimization problem"]}
{"title": "web spam classification using supervised artificial neural network algorithms", "abstract": "due to the rapid growth in technology employed by the spammers , there is a need of classifiers that are more efficient , generic and highly adaptive . neural network based technologies have high ability of adaption as well as generalization . as per our knowledge , very little work has been done in this field using neural network . we present this paper to fill this gap . this paper evaluates performance of three supervised learning algorithms of artificial neural network by creating classifiers for the complex problem of latest web spam pattern classification . these algorithms are conjugate gradient algorithm , resilient backpropagation learning , and levenberg-marquardt algorithm .", "topics": ["statistical classification", "supervised learning"]}
{"title": "automatic identification of retinal arteries and veins in fundus images using local binary patterns", "abstract": "artery and vein ( av ) classification of retinal images is a key to necessary tasks , such as automated measurement of arteriolar-to-venular diameter ratio ( avr ) . this paper comprehensively reviews the state-of-the art in av classification methods . to improve on previous methods , a new local bi- nary pattern-based method ( lbp ) is proposed . beside its simplicity , lbp is robust against low contrast and low quality fundus images ; and it helps the process by including additional av texture and shape information . experimental results compare the performance of the new method with the state-of-the art ; and also methods with different feature extraction and classification schemas .", "topics": ["feature extraction"]}
{"title": "3d interest point detection via discriminative learning", "abstract": "the task of detecting the interest points in 3d meshes has typically been handled by geometric methods . these methods , while greatly describing human preference , can be ill-equipped for handling the variety and subjectivity in human responses . different tasks have different requirements for interest point detection ; some tasks may necessitate high precision while other tasks may require high recall . sometimes points with high curvature may be desirable , while in other cases high curvature may be an indication of noise . geometric methods lack the required flexibility to adapt to such changes . as a consequence , interest point detection seems to be well suited for machine learning methods that can be trained to match the criteria applied on the annotated training data . in this paper , we formulate interest point detection as a supervised binary classification problem using a random forest as our classifier . among other challenges , we are faced with an imbalanced learning problem due to the substantial difference in the priors between interest and non-interest points . we address this by re-sampling the training set . we validate the accuracy of our method and compare our results to those of five state of the art methods on a new , standard benchmark .", "topics": ["test set"]}
{"title": "zoopt : toolbox for derivative-free optimization", "abstract": "recent advances of derivative-free optimization allow efficient approximating the global optimal solutions of sophisticated functions , such as functions with many local optima , non-differentiable and non-continuous functions . this article describes the zoopt ( https : //github.com/eyounx/zoopt ) toolbox that provides efficient derivative-free solvers and are designed easy to use . zoopt provides a python package for single-thread optimization , and a light-weighted distributed version with the help of the julia language for python described functions . zoopt toolbox particularly focuses on optimization problems in machine learning , addressing high-dimensional , noisy , and large-scale problems . the toolbox is being maintained toward ready-to-use tool in real-world machine learning tasks .", "topics": ["approximation algorithm"]}
{"title": "radiation search operations using scene understanding with autonomous uav and ugv", "abstract": "autonomously searching for hazardous radiation sources requires the ability of the aerial and ground systems to understand the scene they are scouting . in this paper , we present systems , algorithms , and experiments to perform radiation search using unmanned aerial vehicles ( uav ) and unmanned ground vehicles ( ugv ) by employing semantic scene segmentation . the aerial data is used to identify radiological points of interest , generate an orthophoto along with a digital elevation model ( dem ) of the scene , and perform semantic segmentation to assign a category ( e.g . road , grass ) to each pixel in the orthophoto . we perform semantic segmentation by training a model on a dataset of images we collected and annotated , using the model to perform inference on images of the test area unseen to the model , and then refining the results with the dem to better reason about category predictions at each pixel . we then use all of these outputs to plan a path for a ugv carrying a lidar to map the environment and avoid obstacles not present during the flight , and a radiation detector to collect more precise radiation measurements from the ground . results of the analysis for each scenario tested favorably . we also note that our approach is general and has the potential to work for a variety of different sensing tasks .", "topics": ["autonomous car", "pixel"]}
{"title": "can deep reinforcement learning solve erdos-selfridge-spencer games ?", "abstract": "deep reinforcement learning has achieved many recent successes , but our understanding of its strengths and limitations is hampered by the lack of rich environments in which we can fully characterize optimal behavior , and correspondingly diagnose individual actions against such a characterization . here we consider a family of combinatorial games , arising from work of erdos , selfridge , and spencer , and we propose their use as environments for evaluating and comparing different approaches to reinforcement learning . these games have a number of appealing features : they are challenging for current learning approaches , but they form ( i ) a low-dimensional , simply parametrized environment where ( ii ) there is a linear closed form solution for optimal behavior from any state , and ( iii ) the difficulty of the game can be tuned by changing environment parameters in an interpretable way . we use these erdos-selfridge-spencer games not only to compare different algorithms , but test for generalization , make comparisons to supervised learning , analyse multiagent play , and even develop a self play algorithm .", "topics": ["supervised learning", "reinforcement learning"]}
{"title": "benchmarking recognition results on word image datasets", "abstract": "we have benchmarked the maximum obtainable recognition accuracy on various word image datasets using manual segmentation and a currently available commercial ocr . we have developed a matlab program , with graphical user interface , for semi-automated pixel level segmentation of word images . we discuss the advantages of pixel level annotation . we have covered five databases adding up to over 3600 word images . these word images have been cropped from camera captured scene , born-digital and street view images . we recognize the segmented word image using the trial version of nuance omnipage ocr . we also discuss , how the degradations introduced during acquisition or inaccuracies introduced during creation of word images affect the recognition of the word present in the image . word images for different kinds of degradations and correction for slant and curvy nature of words are also discussed . the word recognition rates obtained on icdar 2003 , sign evaluation , street view , born-digital and icdar 2011 datasets are 83.9 % , 89.3 % , 79.6 % , 88.5 % and 86.7 % respectively .", "topics": ["database", "pixel"]}
{"title": "using task descriptions in lifelong machine learning for improved performance and zero-shot transfer", "abstract": "knowledge transfer between tasks can improve the performance of learned models , but requires an accurate estimate of the inter-task relationships to identify the relevant knowledge to transfer . these inter-task relationships are typically estimated based on training data for each task , which is inefficient in lifelong learning settings where the goal is to learn each consecutive task rapidly from as little data as possible . to reduce this burden , we develop a lifelong learning method based on coupled dictionary learning that utilizes high-level task descriptions to model the inter-task relationships . we show that using task descriptors improves the performance of the learned task policies , providing both theoretical justification for the benefit and empirical demonstration of the improvement across a variety of learning problems . given only the descriptor for a new task , the lifelong learner is also able to accurately predict a model for the new task through zero-shot learning using the coupled dictionary , eliminating the need to gather training data before addressing the task .", "topics": ["test set", "high- and low-level"]}
{"title": "bridging neural machine translation and bilingual dictionaries", "abstract": "neural machine translation ( nmt ) has become the new state-of-the-art in several language pairs . however , it remains a challenging problem how to integrate nmt with a bilingual dictionary which mainly contains words rarely or never seen in the bilingual training data . in this paper , we propose two methods to bridge nmt and the bilingual dictionaries . the core idea behind is to design novel models that transform the bilingual dictionaries into adequate sentence pairs , so that nmt can distil latent bilingual mappings from the ample and repetitive phenomena . one method leverages a mixed word/character model and the other attempts at synthesizing parallel sentences guaranteeing massive occurrence of the translation lexicon . extensive experiments demonstrate that the proposed methods can remarkably improve the translation quality , and most of the rare words in the test sentences can obtain correct translations if they are covered by the dictionary .", "topics": ["test set", "machine translation"]}
{"title": "implementation of nlization framework for verbs , pronouns and determiners with eugene", "abstract": "unl system is designed and implemented by a nonprofit organization , undl foundation at geneva in 1999 . unl applications are application softwares that allow end users to accomplish natural language tasks , such as translating , summarizing , retrieving or extracting information , etc . two major web based application softwares are interactive analyzer ( ian ) , which is a natural language analysis system . it represents natural language sentences as semantic networks in the unl format . other application software is deep-to-surface generator ( eugene ) , which is an open-source interactive nlizer . it generates natural language sentences out of semantic networks represented in the unl format . in this paper , nlization framework with eugene is focused , while using unl system for accomplishing the task of machine translation . in whole nlization process , eugene takes a unl input and delivers an output in natural language without any human intervention . it is language-independent and has to be parametrized to the natural language input through a dictionary and a grammar , provided as separate interpretable files . in this paper , it is explained that how unl input is syntactically and semantically analyzed with the unl-nl t-grammar for nlization of unl sentences involving verbs , pronouns and determiners for punjabi natural language .", "topics": ["machine translation", "natural language"]}
{"title": "a note on the spice method", "abstract": "in this article , we analyze the spice method developed in [ 1 ] , and establish its connections with other standard sparse estimation methods such as the lasso and the lad-lasso . this result positions spice as a computationally efficient technique for the calculation of lasso-type estimators . conversely , this connection is very useful for establishing the asymptotic properties of spice under several problem scenarios and for suggesting suitable modifications in cases where the naive version of spice would not work .", "topics": ["computational complexity theory", "sparse matrix"]}
{"title": "towards proving the adversarial robustness of deep neural networks", "abstract": "autonomous vehicles are highly complex systems , required to function reliably in a wide variety of situations . manually crafting software controllers for these vehicles is difficult , but there has been some success in using deep neural networks generated using machine-learning . however , deep neural networks are opaque to human engineers , rendering their correctness very difficult to prove manually ; and existing automated techniques , which were not designed to operate on neural networks , fail to scale to large systems . this paper focuses on proving the adversarial robustness of deep neural networks , i.e . proving that small perturbations to a correctly-classified input to the network can not cause it to be misclassified . we describe some of our recent and ongoing work on verifying the adversarial robustness of networks , and discuss some of the open questions we have encountered and how they might be addressed .", "topics": ["neural networks", "autonomous car"]}
{"title": "why size matters : feature coding as nystrom sampling", "abstract": "recently , the computer vision and machine learning community has been in favor of feature extraction pipelines that rely on a coding step followed by a linear classifier , due to their overall simplicity , well understood properties of linear classifiers , and their computational efficiency . in this paper we propose a novel view of this pipeline based on kernel methods and nystrom sampling . in particular , we focus on the coding of a data point with a local representation based on a dictionary with fewer elements than the number of data points , and view it as an approximation to the actual function that would compute pair-wise similarity to all data points ( often too many to compute in practice ) , followed by a nystrom sampling step to select a subset of all data points . furthermore , since bounds are known on the approximation power of nystrom sampling as a function of how many samples ( i.e . dictionary size ) we consider , we can derive bounds on the approximation of the exact ( but expensive to compute ) kernel matrix , and use it as a proxy to predict accuracy as a function of the dictionary size , which has been observed to increase but also to saturate as we increase its size . this model may help explaining the positive effect of the codebook size and justifying the need to stack more layers ( often referred to as deep learning ) , as flat models empirically saturate as we add more complexity .", "topics": ["sampling ( signal processing )", "feature extraction"]}
{"title": "age progression/regression by conditional adversarial autoencoder", "abstract": "`` if i provide you a face image of mine ( without telling you the actual age when i took the picture ) and a large amount of face images that i crawled ( containing labeled faces of different ages but not necessarily paired ) , can you show me what i would look like when i am 80 or what i was like when i was 5 ? '' the answer is probably a `` no . '' most existing face aging works attempt to learn the transformation between age groups and thus would require the paired samples as well as the labeled query image . in this paper , we look at the problem from a generative modeling perspective such that no paired samples is required . in addition , given an unlabeled image , the generative model can directly produce the image with desired age attribute . we propose a conditional adversarial autoencoder ( caae ) that learns a face manifold , traversing on which smooth age progression and regression can be realized simultaneously . in caae , the face is first mapped to a latent vector through a convolutional encoder , and then the vector is projected to the face manifold conditional on age through a deconvolutional generator . the latent vector preserves personalized face features ( i.e . , personality ) and the age condition controls progression vs. regression . two adversarial networks are imposed on the encoder and generator , respectively , forcing to generate more photo-realistic faces . experimental results demonstrate the appealing performance and flexibility of the proposed framework by comparing with the state-of-the-art and ground truth .", "topics": ["generative model", "ground truth"]}
{"title": "pyramid stereo matching network", "abstract": "recent work has shown that depth estimation from a stereo pair of images can be formulated as a supervised learning task to be resolved with convolutional neural networks ( cnns ) . however , current architectures rely on patch-based siamese networks , lacking the means to exploit context information for finding correspondence in illposed regions . to tackle this problem , we propose psmnet , a pyramid stereo matching network consisting of two main modules : spatial pyramid pooling and 3d cnn . the spatial pyramid pooling module takes advantage of the capacity of global context information by aggregating context in different scales and locations to form a cost volume . the 3d cnn learns to regularize cost volume using stacked multiple hourglass networks in conjunction with intermediate supervision . the proposed approach was evaluated on several benchmark datasets . our method ranked first in the kitti 2012 and 2015 leaderboards before march 18 , 2018 . the codes of psmnet are available at : https : //github.com/jiarenchang/psmnet .", "topics": ["supervised learning"]}
{"title": "rsdnet : learning to predict remaining surgery duration from laparoscopic videos without manual annotations", "abstract": "objective : accurate surgery duration estimation is necessary for optimal or planning , which plays an important role for patient comfort and safety as well as resource optimization . it is however challenging to preoperatively predict surgery duration since it varies significantly depending on the patient condition , surgeon skills , and intraoperative situation . we present an approach for intraoperative estimation of remaining surgery duration , which is well suited for deployment in the or . methods : we propose a deep learning pipeline , named rsdnet , which automatically estimates the remaining surgery duration intraoperatively by using only visual information from laparoscopic videos . an interesting feature of rsdnet is that it does not depend on any manual annotation during training . results : the experimental results show that the proposed network significantly outperforms the method that is frequently used in surgical facilities for estimating surgery duration . further , the generalizability of the approach is demonstrated by testing the pipeline on two large datasets containing different types of surgeries , 120 cholecystectomy and 170 gastric bypass videos . conclusion : creation of manual annotations requires expert knowledge and is a time-consuming process , especially considering the numerous types of surgeries performed in a hospital and the large number of laparoscopic videos available . since the proposed pipeline is not reliant on manual annotation , it is easily scalable to many types of surgeries . significance : an improved or management system could be developed with rsdnet as a result of its superior performance and ability to be efficiently scaled up to many kinds of surgeries .", "topics": ["scalability"]}
{"title": "deep transfer learning for person re-identification", "abstract": "person re-identification ( re-id ) poses a unique challenge to deep learning : how to learn a deep model with millions of parameters on a small training set of few or no labels . in this paper , a number of deep transfer learning models are proposed to address the data sparsity problem . first , a deep network architecture is designed which differs from existing deep re-id models in that ( a ) it is more suitable for transferring representations learned from large image classification datasets , and ( b ) classification loss and verification loss are combined , each of which adopts a different dropout strategy . second , a two-stepped fine-tuning strategy is developed to transfer knowledge from auxiliary datasets . third , given an unlabelled re-id dataset , a novel unsupervised deep transfer learning model is developed based on co-training . the proposed models outperform the state-of-the-art deep re-id models by large margins : we achieve rank-1 accuracy of 85.4\\ % , 83.7\\ % and 56.3\\ % on cuhk03 , market1501 , and viper respectively , whilst on viper , our unsupervised model ( 45.1\\ % ) beats most supervised models .", "topics": ["test set", "computer vision"]}
{"title": "rational deployment of csp heuristics", "abstract": "heuristics are crucial tools in decreasing search effort in varied fields of ai . in order to be effective , a heuristic must be efficient to compute , as well as provide useful information to the search algorithm . however , some well-known heuristics which do well in reducing backtracking are so heavy that the gain of deploying them in a search algorithm might be outweighed by their overhead . we propose a rational metareasoning approach to decide when to deploy heuristics , using csp backtracking search as a case study . in particular , a value of information approach is taken to adaptive deployment of solution-count estimation heuristics for value ordering . empirical results show that indeed the proposed mechanism successfully balances the tradeoff between decreasing backtracking and heuristic computational overhead , resulting in a significant overall search time reduction .", "topics": ["heuristic"]}
{"title": "egocentric height estimation", "abstract": "egocentric , or first-person vision which became popular in recent years with an emerge in wearable technology , is different than exocentric ( third-person ) vision in some distinguishable ways , one of which being that the camera wearer is generally not visible in the video frames . recent work has been done on action and object recognition in egocentric videos , as well as work on biometric extraction from first-person videos . height estimation can be a useful feature for both soft-biometrics and object tracking . here , we propose a method of estimating the height of an egocentric camera without any calibration or reference points . we used both traditional computer vision approaches and deep learning in order to determine the visual cues that results in best height estimation . here , we introduce a framework inspired by two stream networks comprising of two convolutional neural networks , one based on spatial information , and one based on information given by optical flow in a frame . given an egocentric video as an input to the framework , our model yields a height estimate as an output . we also incorporate late fusion to learn a combination of temporal and spatial cues . comparing our model with other methods we used as baselines , we achieve height estimates for videos with a mean average error of 14.04 cm over a range of 103 cm of data , and classification accuracy for relative height ( tall , medium or short ) up to 93.75 % where chance level is 33 % .", "topics": ["baseline ( configuration management )", "computer vision"]}
{"title": "weakly supervised cross-lingual named entity recognition via effective annotation and representation projection", "abstract": "the state-of-the-art named entity recognition ( ner ) systems are supervised machine learning models that require large amounts of manually annotated data to achieve high accuracy . however , annotating ner data by human is expensive and time-consuming , and can be quite difficult for a new language . in this paper , we present two weakly supervised approaches for cross-lingual ner with no human annotation in a target language . the first approach is to create automatically labeled ner data for a target language via annotation projection on comparable corpora , where we develop a heuristic scheme that effectively selects good-quality projection-labeled data from noisy data . the second approach is to project distributed representations of words ( word embeddings ) from a target language to a source language , so that the source-language ner system can be applied to the target language without re-training . we also design two co-decoding schemes that effectively combine the outputs of the two projection-based approaches . we evaluate the performance of the proposed approaches on both in-house and open ner data for several target languages . the results show that the combined systems outperform three other weakly supervised approaches on the conll data .", "topics": ["supervised learning", "text corpus"]}
{"title": "core50 : a new dataset and benchmark for continuous object recognition", "abstract": "continuous/lifelong learning of high-dimensional data streams is a challenging research problem . in fact , fully retraining models each time new data become available is infeasible , due to computational and storage issues , while na\\ '' ive incremental strategies have been shown to suffer from catastrophic forgetting . in the context of real-world object recognition applications ( e.g . , robotic vision ) , where continuous learning is crucial , very few datasets and benchmarks are available to evaluate and compare emerging techniques . in this work we propose a new dataset and benchmark core50 , specifically designed for continuous object recognition , and introduce baseline approaches for different continuous learning scenarios .", "topics": ["baseline ( configuration management )", "robot"]}
{"title": "hybrid batch bayesian optimization", "abstract": "bayesian optimization aims at optimizing an unknown non-convex/concave function that is costly to evaluate . we are interested in application scenarios where concurrent function evaluations are possible . under such a setting , bo could choose to either sequentially evaluate the function , one input at a time and wait for the output of the function before making the next selection , or evaluate the function at a batch of multiple inputs at once . these two different settings are commonly referred to as the sequential and batch settings of bayesian optimization . in general , the sequential setting leads to better optimization performance as each function evaluation is selected with more information , whereas the batch setting has an advantage in terms of the total experimental time ( the number of iterations ) . in this work , our goal is to combine the strength of both settings . specifically , we systematically analyze bayesian optimization using gaussian process as the posterior estimator and provide a hybrid algorithm that , based on the current state , dynamically switches between a sequential policy and a batch policy with variable batch sizes . we provide theoretical justification for our algorithm and present experimental results on eight benchmark bo problems . the results show that our method achieves substantial speedup ( up to % 78 ) compared to a pure sequential policy , without suffering any significant performance loss .", "topics": ["iteration"]}
{"title": "robust deep reinforcement learning with adversarial attacks", "abstract": "this paper proposes adversarial attacks for reinforcement learning ( rl ) and then improves the robustness of deep reinforcement learning algorithms ( drl ) to parameter uncertainties with the help of these attacks . we show that even a naively engineered attack successfully degrades the performance of drl algorithm . we further improve the attack using gradient information of an engineered loss function which leads to further degradation in performance . these attacks are then leveraged during training to improve the robustness of rl within robust control framework . we show that this adversarial training of drl algorithms like deep double q learning and deep deterministic policy gradients leads to significant increase in robustness to parameter variations for rl benchmarks such as cart-pole , mountain car , hopper and half cheetah environment .", "topics": ["reinforcement learning", "loss function"]}
{"title": "tensor-dictionary learning with deep kruskal-factor analysis", "abstract": "a multi-way factor analysis model is introduced for tensor-variate data of any order . each data item is represented as a ( sparse ) sum of kruskal decompositions , a kruskal-factor analysis ( kfa ) . kfa is nonparametric and can infer both the tensor-rank of each dictionary atom and the number of dictionary atoms . the model is adapted for online learning , which allows dictionary learning on large data sets . after kfa is introduced , the model is extended to a deep convolutional tensor-factor analysis , supervised by a bayesian svm . the experiments section demonstrates the improvement of kfa over vectorized approaches ( e.g . , bpfa ) , tensor decompositions , and convolutional neural networks ( cnn ) in multi-way denoising , blind inpainting , and image classification . the improvement in psnr for the inpainting results over other methods exceeds 1db in several cases and we achieve state of the art results on caltech101 image classification .", "topics": ["noise reduction", "computer vision"]}
{"title": "real-world object recognition with off-the-shelf deep conv nets : how many objects can icub learn ?", "abstract": "the ability to visually recognize objects is a fundamental skill for robotics systems . indeed , a large variety of tasks involving manipulation , navigation or interaction with other agents , deeply depends on the accurate understanding of the visual scene . yet , at the time being , robots are lacking good visual perceptual systems , which often become the main bottleneck preventing the use of autonomous agents for real-world applications . lately in computer vision , systems that learn suitable visual representations and based on multi-layer deep convolutional networks are showing remarkable performance in tasks such as large-scale visual recognition and image retrieval . to this regard , it is natural to ask whether such remarkable performance would generalize also to the robotic setting . in this paper we investigate such possibility , while taking further steps in developing a computational vision system to be embedded on a robotic platform , the icub humanoid robot . in particular , we release a new dataset ( { \\sc icubworld28 } ) that we use as a benchmark to address the question : { \\it how many objects can icub recognize ? } our study is developed in a learning framework which reflects the typical visual experience of a humanoid robot like the icub . experiments shed interesting insights on the strength and weaknesses of current computer vision approaches applied in real robotic settings .", "topics": ["computer vision", "autonomous car"]}
{"title": "inter-session modeling for session-based recommendation", "abstract": "in recent years , research has been done on applying recurrent neural networks ( rnns ) as recommender systems . results have been promising , especially in the session-based setting where rnns have been shown to outperform state-of-the-art models . in many of these experiments , the rnn could potentially improve the recommendations by utilizing information about the user 's past sessions , in addition to its own interactions in the current session . a problem for session-based recommendation , is how to produce accurate recommendations at the start of a session , before the system has learned much about the user 's current interests . we propose a novel approach that extends a rnn recommender to be able to process the user 's recent sessions , in order to improve recommendations . this is done by using a second rnn to learn from recent sessions , and predict the user 's interest in the current session . by feeding this information to the original rnn , it is able to improve its recommendations . our experiments on two different datasets show that the proposed approach can significantly improve recommendations throughout the sessions , compared to a single rnn working only on the current session . the proposed model especially improves recommendations at the start of sessions , and is therefore able to deal with the cold start problem within sessions .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "infinite-label learning with semantic output codes", "abstract": "we develop a new statistical machine learning paradigm , named infinite-label learning , to annotate a data point with more than one relevant labels from a candidate set , which pools both the finite labels observed at training and a potentially infinite number of previously unseen labels . the infinite-label learning fundamentally expands the scope of conventional multi-label learning , and better models the practical requirements in various real-world applications , such as image tagging , ads-query association , and article categorization . however , how can we learn a labeling function that is capable of assigning to a data point the labels omitted from the training set ? to answer the question , we seek some clues from the recent work on zero-shot learning , where the key is to represent a class/label by a vector of semantic codes , as opposed to treating them as atomic labels . we validate the infinite-label learning by a pac bound in theory and some empirical studies on both synthetic and real data .", "topics": ["test set", "synthetic data"]}
{"title": "tech report : a fast multiscale spatial regularization for sparse hyperspectral unmixing", "abstract": "sparse hyperspectral unmixing from large spectral libraries has been considered to circumvent limitations of endmember extraction algorithms in many applications . this strategy often leads to ill-posed inverse problems , which can benefit from spatial regularization strategies . while existing spatial regularization methods improve the problem conditioning and promote piecewise smooth solutions , they lead to large nonsmooth optimization problems . thus , efficiently introducing spatial context in the unmixing problem remains a challenge , and a necessity for many real world applications . in this paper , a novel multiscale spatial regularization approach for sparse unmixing is proposed . the method uses a signal-adaptive spatial multiscale decomposition based on superpixels to decompose the unmixing problem into two simpler problems , one in the approximation domain and another in the original domain . simulation results using both synthetic and real data indicate that the proposed method can outperform state-of-the-art total variation-based algorithms with a computation time comparable to that of their unregularized counterparts .", "topics": ["time complexity", "synthetic data"]}
{"title": "residual belief propagation : informed scheduling for asynchronous message passing", "abstract": "inference for probabilistic graphical models is still very much a practical challenge in large domains . the commonly used and effective belief propagation ( bp ) algorithm and its generalizations often do not converge when applied to hard , real-life inference tasks . while it is widely recognized that the scheduling of messages in these algorithms may have significant consequences , this issue remains largely unexplored . in this work , we address the question of how to schedule messages for asynchronous propagation so that a fixed point is reached faster and more often . we first show that any reasonable asynchronous bp converges to a unique fixed point under conditions similar to those that guarantee convergence of synchronous bp . in addition , we show that the convergence rate of a simple round-robin schedule is at least as good as that of synchronous propagation . we then propose residual belief propagation ( rbp ) , a novel , easy-to-implement , asynchronous propagation algorithm that schedules messages in an informed way , that pushes down a bound on the distance from the fixed point . finally , we demonstrate the superiority of rbp over state-of-the-art methods for a variety of challenging synthetic and real-life problems : rbp converges significantly more often than other methods ; and it significantly reduces running time until convergence , even when other methods converge .", "topics": ["graphical model", "time complexity"]}
{"title": "data-oriented language processing . an overview", "abstract": "during the last few years , a new approach to language processing has started to emerge , which has become known under various labels such as `` data-oriented parsing '' , `` corpus-based interpretation '' , and `` tree-bank grammar '' ( cf . van den berg et al . 1994 ; bod 1992-96 ; bod et al . 1996a/b ; bonnema 1996 ; charniak 1996a/b ; goodman 1996 ; kaplan 1996 ; rajman 1995a/b ; scha 1990-92 ; sekine & grishman 1995 ; sima'an et al . 1994 ; sima'an 1995-96 ; tugwell 1995 ) . this approach , which we will call `` data-oriented processing '' or `` dop '' , embodies the assumption that human language perception and production works with representations of concrete past language experiences , rather than with abstract linguistic rules . the models that instantiate this approach therefore maintain large corpora of linguistic representations of previously occurring utterances . when processing a new input utterance , analyses of this utterance are constructed by combining fragments from the corpus ; the occurrence-frequencies of the fragments are used to estimate which analysis is the most probable one . in this paper we give an in-depth discussion of a data-oriented processing model which employs a corpus of labelled phrase-structure trees . then we review some other models that instantiate the dop approach . many of these models also employ labelled phrase-structure trees , but use different criteria for extracting fragments from the corpus or employ different disambiguation strategies ( bod 1996b ; charniak 1996a/b ; goodman 1996 ; rajman 1995a/b ; sekine & grishman 1995 ; sima'an 1995-96 ) ; other models use richer formalisms for their corpus annotations ( van den berg et al . 1994 ; bod et al . , 1996a/b ; bonnema 1996 ; kaplan 1996 ; tugwell 1995 ) .", "topics": ["text corpus", "parsing"]}
{"title": "exploiting structure sparsity for covariance-based visual representation", "abstract": "the past few years have witnessed increasing research interest on covariance-based feature representation . a variety of methods have been proposed to boost its efficacy , with some recent ones resorting to nonlinear kernel technique . noting that the essence of this feature representation is to characterise the underlying structure of visual features , this paper argues that an equally , if not more , important approach to boosting its efficacy shall be to improve the quality of this characterisation . following this idea , we propose to exploit the structure sparsity of visual features in skeletal human action recognition , and compute sparse inverse covariance estimate ( sice ) as feature representation . we discuss the advantage of this new representation on dealing with small sample , high dimensionality , and modelling capability . furthermore , utilising the monotonicity property of sice , we efficiently generate a hierarchy of sice matrices to characterise the structure of visual features at different sparsity levels , and two discriminative learning algorithms are then developed to adaptively integrate them to perform recognition . as demonstrated by extensive experiments , the proposed representation leads to significantly improved recognition performance over the state-of-the-art comparable methods . in particular , as a method fully based on linear technique , it is comparable or even better than those employing nonlinear kernel technique . this result well demonstrates the value of exploiting structure sparsity for covariance-based feature representation .", "topics": ["kernel ( operating system )", "nonlinear system"]}
{"title": "a concave optimization algorithm for matching partially overlapping point sets", "abstract": "point matching refers to the process of finding spatial transformation and correspondences between two sets of points . in this paper , we focus on the case that there is only partial overlap between two point sets . following the approach of the robust point matching method , we model point matching as a mixed linear assignment-least square problem and show that after eliminating the transformation variable , the resulting problem of minimization with respect to point correspondence is a concave optimization problem . furthermore , this problem has the property that the objective function can be converted into a form with few nonlinear terms via a linear transformation . based on these properties , we employ the branch-and-bound ( bnb ) algorithm to optimize the resulting problem where the dimension of the search space is small . to further improve efficiency of the bnb algorithm where computation of the lower bound is the bottleneck , we propose a new lower bounding scheme which has a k-cardinality linear assignment formulation and can be efficiently solved . experimental results show that the proposed algorithm outperforms state-of-the-art methods in terms of robustness to disturbances and point matching accuracy .", "topics": ["optimization problem", "mathematical optimization"]}
{"title": "low-memory gemm-based convolution algorithms for deep neural networks", "abstract": "deep neural networks ( dnns ) require very large amounts of computation both for training and for inference when deployed in the field . a common approach to implementing dnns is to recast the most computationally expensive operations as general matrix multiplication ( gemm ) . however , as we demonstrate in this paper , there are a great many different ways to express dnn convolution operations using gemm . although different approaches all perform the same number of operations , the size of temporary data structures differs significantly . convolution of an input matrix with dimensions $ c \\times h \\times w $ , requires $ o ( k^2chw ) $ additional space using the classical im2col approach . more recently memory-efficient approaches requiring just $ o ( kchw ) $ auxiliary space have been proposed . we present two novel gemm-based algorithms that require just $ o ( mhw ) $ and $ o ( kw ) $ additional space respectively , where $ m $ is the number of channels in the result of the convolution . these algorithms dramatically reduce the space overhead of dnn convolution , making it much more suitable for memory-limited embedded systems . experimental evaluation shows that our low-memory algorithms are just as fast as the best patch-building approaches despite requiring just a fraction of the amount of additional memory . our low-memory algorithms have excellent data locality which gives them a further edge over patch-building algorithms when multiple cores are used . as a result , our low memory algorithms often outperform the best patch-building algorithms using multiple threads .", "topics": ["computation", "convolution"]}
{"title": "representations and ensemble methods for dynamic relational classification", "abstract": "temporal networks are ubiquitous and evolve over time by the addition , deletion , and changing of links , nodes , and attributes . although many relational datasets contain temporal information , the majority of existing techniques in relational learning focus on static snapshots and ignore the temporal dynamics . we propose a framework for discovering temporal representations of relational data to increase the accuracy of statistical relational learning algorithms . the temporal relational representations serve as a basis for classification , ensembles , and pattern mining in evolving domains . the framework includes ( 1 ) selecting the time-varying relational components ( links , attributes , nodes ) , ( 2 ) selecting the temporal granularity , ( 3 ) predicting the temporal influence of each time-varying relational component , and ( 4 ) choosing the weighted relational classifier . additionally , we propose temporal ensemble methods that exploit the temporal-dimension of relational data . these ensembles outperform traditional and more sophisticated relational ensembles while avoiding the issue of learning the most optimal representation . finally , the space of temporal-relational models are evaluated using a sample of classifiers . in all cases , the proposed temporal-relational classifiers outperform competing models that ignore the temporal information . the results demonstrate the capability and necessity of the temporal-relational representations for classification , ensembles , and for mining temporal datasets .", "topics": ["data mining", "graphical model"]}
{"title": "handwritten bangla digit recognition using deep learning", "abstract": "in spite of the advances in pattern recognition technology , handwritten bangla character recognition ( hbcr ) ( such as alpha-numeric and special characters ) remains largely unsolved due to the presence of many perplexing characters and excessive cursive in bangla handwriting . even the best existing recognizers do not lead to satisfactory performance for practical applications . to improve the performance of handwritten bangla digit recognition ( hbdr ) , we herein present a new approach based on deep neural networks which have recently shown excellent performance in many pattern recognition and machine learning applications , but has not been throughly attempted for hbdr . we introduce bangla digit recognition techniques based on deep belief network ( dbn ) , convolutional neural networks ( cnn ) , cnn with dropout , cnn with dropout and gaussian filters , and cnn with dropout and gabor filters . these networks have the advantage of extracting and using feature information , improving the recognition of two dimensional shapes with a high degree of invariance to translation , scaling and other pattern distortions . we systematically evaluated the performance of our method on publicly available bangla numeral image database named cmaterdb 3.1.1 . from experiments , we achieved 98.78 % recognition rate using the proposed method : cnn with gabor features and dropout , which outperforms the state-of-the-art algorithms for hdbr .", "topics": ["speech recognition"]}
{"title": "scalable recollections for continual lifelong learning", "abstract": "given the recent success of deep learning applied to a variety of single tasks , it is natural to consider more human-realistic settings . perhaps the most difficult of these settings is that of continual lifelong learning , where the model must learn online over a continuous stream of non-stationary data . a continual lifelong learning system must have three primary capabilities to succeed : it must learn and adapt over time , it must not forget what it has learned , and it must be efficient in both training time and memory . recent techniques have focused their efforts largely on the first two capabilities while the third capability remains largely unexplored . in this paper , we consider the problem of efficient and effective storage of experiences over very large time-frames . in particular we consider the case where typical experiences are n bits and memories are limited to k bits for k < < n. we present a novel scalable architecture and training algorithm in this challenging domain and provide an extensive evaluation of its performance . our results show that we can achieve considerable gains on top of state-of-the-art methods such as gem .", "topics": ["scalability"]}
{"title": "proportionate gradient updates with percentdelta", "abstract": "deep neural networks are generally trained using iterative gradient updates . magnitudes of gradients are affected by many factors , including choice of activation functions and initialization . more importantly , gradient magnitudes can greatly differ across layers , with some layers receiving much smaller gradients than others . causing some layers to train slower than others and therefore slowing down the overall convergence . we analytically explain this disproportionality . then we propose to explicitly train all layers at the same speed , by scaling the gradient w.r.t . every trainable tensor to be proportional to its current value . in particular , at every batch , we want to update all trainable tensors , such that the relative change of the l1-norm of the tensors is the same , across all layers of the network , throughout training time . experiments on mnist show that our method appropriately scales gradients , such that the relative change in trainable tensors is approximately equal across layers . in addition , measuring the test accuracy with training time , shows that our method trains faster than other methods , giving higher test accuracy given same budget of training steps .", "topics": ["gradient descent", "iteration"]}
{"title": "multiscale residual mixture of pca : dynamic dictionaries for optimal basis learning", "abstract": "in this paper we are interested in the problem of learning an over-complete basis and a methodology such that the reconstruction or inverse problem does not need optimization . we analyze the optimality of the presented approaches , their link to popular already known techniques s.a . artificial neural networks , k-means or oja 's learning rule . finally , we will see that one approach to reach the optimal dictionary is a factorial and hierarchical approach . the derived approach lead to a formulation of a deep oja network . we present results on different tasks and present the resulting very efficient learning algorithm which brings a new vision on the training of deep nets . finally , the theoretical work shows that deep frameworks are one way to efficiently have over-complete ( combinatorially large ) dictionary yet allowing easy reconstruction . we thus present the deep residual oja network ( dron ) . we demonstrate that a recursive deep approach working on the residuals allow exponential decrease of the error w.r.t . the depth .", "topics": ["mathematical optimization", "time complexity"]}
{"title": "semi-supervised learning with ipm-based gans : an empirical study", "abstract": "we present an empirical investigation of a recent class of generative adversarial networks ( gans ) using integral probability metrics ( ipm ) and their performance for semi-supervised learning . ipm-based gans like wasserstein gan , fisher gan and sobolev gan have desirable properties in terms of theoretical understanding , training stability , and a meaningful loss . in this work we investigate how the design of the critic ( or discriminator ) influences the performance in semi-supervised learning . we distill three key take-aways which are important for good ssl performance : ( 1 ) the k+1 formulation , ( 2 ) avoiding batch normalization in the critic and ( 3 ) avoiding gradient penalty constraints on the classification layer .", "topics": ["supervised learning", "gradient"]}
{"title": "towards a sound theory of adaptation for the simple genetic algorithm", "abstract": "the pace of progress in the fields of evolutionary computation and machine learning is currently limited -- in the former field , by the improbability of making advantageous extensions to evolutionary algorithms when their capacity for adaptation is poorly understood , and in the latter by the difficulty of finding effective semi-principled reductions of hard real-world problems to relatively simple optimization problems . in this paper we explain why a theory which can accurately explain the simple genetic algorithm 's remarkable capacity for adaptation has the potential to address both these limitations . we describe what we believe to be the impediments -- historic and analytic -- to the discovery of such a theory and highlight the negative role that the building block hypothesis ( bbh ) has played . we argue based on experimental results that a fundamental limitation which is widely believed to constrain the sga 's adaptive ability ( and is strongly implied by the bbh ) is in fact illusionary and does not exist . the sga therefore turns out to be more powerful than it is currently thought to be . we give conditions under which it becomes feasible to numerically approximate and study the multivariate marginals of the search distribution of an infinite population sga over multiple generations even when its genomes are long , and explain why this analysis is relevant to the riddle of the sga 's remarkable adaptive abilities .", "topics": ["approximation algorithm", "numerical analysis"]}
{"title": "syntactic confluence criteria for positive/negative-conditional term rewriting systems", "abstract": "we study the combination of the following already known ideas for showing confluence of unconditional or conditional term rewriting systems into practically more useful confluence criteria for conditional systems : our syntactical separation into constructor and non-constructor symbols , huet 's introduction and toyama 's generalization of parallel closedness for non-noetherian unconditional systems , the use of shallow confluence for proving confluence of noetherian and non-noetherian conditional systems , the idea that certain kinds of limited confluence can be assumed for checking the fulfilledness or infeasibility of the conditions of conditional critical pairs , and the idea that ( when termination is given ) only prime superpositions have to be considered and certain normalization restrictions can be applied for the substitutions fulfilling the conditions of conditional critical pairs . besides combining and improving already known methods , we present the following new ideas and results : we strengthen the criterion for overlay joinable noetherian systems , and , by using the expressiveness of our syntactical separation into constructor and non-constructor symbols , we are able to present criteria for level confluence that are not criteria for shallow confluence actually and also able to weaken the severe requirement of normality ( stiffened with left-linearity ) in the criteria for shallow confluence of noetherian and non-noetherian conditional systems to the easily satisfied requirement of quasi-normality . finally , the whole paper may also give a practically useful overview of the syntactical means for showing confluence of conditional term rewriting systems .", "topics": ["eisenstein 's criterion"]}
{"title": "delineation of skin strata in reflectance confocal microscopy images using recurrent convolutional networks with toeplitz attention", "abstract": "reflectance confocal microscopy ( rcm ) is an effective , non-invasive pre-screening tool for skin cancer diagnosis , but it requires extensive training and experience to assess accurately . there are few quantitative tools available to standardize image acquisition and analysis , and the ones that are available are not interpretable . in this study , we use a recurrent neural network with attention on convolutional network features . we apply it to delineate skin strata in vertically-oriented stacks of transverse rcm image slices in an interpretable manner . we introduce a new attention mechanism called toeplitz attention , which constrains the attention map to have a toeplitz structure . testing our model on an expert labeled dataset of 504 rcm stacks , we achieve 88.17 % image-wise classification accuracy , which is the current state-of-art .", "topics": ["recurrent neural network"]}
{"title": "a framework for fast face and eye detection", "abstract": "face detection is an essential step in many computer vision applications like surveillance , tracking , medical analysis , facial expression analysis etc . several approaches have been made in the direction of face detection . among them , haar-like features based method is a robust method . in spite of the robustness , haar - like features work with some limitations . however , with some simple modifications in the algorithm , its performance can be made faster and more robust . the present work refers to the increase in speed of operation of the original algorithm by down sampling the frames and its analysis with different scale factors . it also discusses the detection of tilted faces using an affine transformation of the input image .", "topics": ["sampling ( signal processing )", "computer vision"]}
{"title": "application of the sp theory of intelligence to the understanding of natural vision and the development of computer vision", "abstract": "the sp theory of intelligence aims to simplify and integrate concepts in computing and cognition , with information compression as a unifying theme . this article discusses how it may be applied to the understanding of natural vision and the development of computer vision . the theory , which is described quite fully elsewhere , is described here in outline but with enough detail to ensure that the rest of the article makes sense . low level perceptual features such as edges or corners may be identified by the extraction of redundancy in uniform areas in a manner that is comparable with the run-length encoding technique for information compression . the concept of multiple alignment in the sp theory may be applied to the recognition of objects , and to scene analysis , with a hierarchy of parts and sub-parts , and at multiple levels of abstraction . the theory has potential for the unsupervised learning of visual objects and classes of objects , and suggests how coherent concepts may be derived from fragments . as in natural vision , both recognition and learning in the sp system is robust in the face of errors of omission , commission and substitution . the theory suggests how , via vision , we may piece together a knowledge of the three-dimensional structure of objects and of our environment , it provides an account of how we may see things that are not objectively present in an image , and how we recognise something despite variations in the size of its retinal image . and it has things to say about the phenomena of lightness constancy and colour constancy , the role of context in recognition , and ambiguities in visual perception . a strength of the sp theory is that it provides for the integration of vision with other sensory modalities and with other aspects of intelligence .", "topics": ["high- and low-level", "unsupervised learning"]}
{"title": "semantic ambiguity and perceived ambiguity", "abstract": "i explore some of the issues that arise when trying to establish a connection between the underspecification hypothesis pursued in the nlp literature and work on ambiguity in semantics and in the psychological literature . a theory of underspecification is developed `from the first principles ' , i.e . , starting from a definition of what it means for a sentence to be semantically ambiguous and from what we know about the way humans deal with ambiguity . an underspecified language is specified as the translation language of a grammar covering sentences that display three classes of semantic ambiguity : lexical ambiguity , scopal ambiguity , and referential ambiguity . the expressions of this language denote sets of senses . a formalization of defeasible reasoning with underspecified representations is presented , based on default logic . some issues to be confronted by such a formalization are discussed .", "topics": ["natural language processing"]}
{"title": "statistical machine translation by generalized parsing", "abstract": "designers of statistical machine translation ( smt ) systems have begun to employ tree-structured translation models . systems involving tree-structured translation models tend to be complex . this article aims to reduce the conceptual complexity of such systems , in order to make them easier to design , implement , debug , use , study , understand , explain , modify , and improve . in service of this goal , the article extends the theory of semiring parsing to arrive at a novel abstract parsing algorithm with five functional parameters : a logic , a grammar , a semiring , a search strategy , and a termination condition . the article then shows that all the common algorithms that revolve around tree-structured translation models , including hierarchical alignment , inference for parameter estimation , translation , and structured evaluation , can be derived by generalizing two of these parameters -- the grammar and the logic . the article culminates with a recipe for using such generalized parsers to train , apply , and evaluate an smt system that is driven by tree-structured translation models .", "topics": ["machine translation", "parsing"]}
{"title": "exponential family graph matching and ranking", "abstract": "we present a method for learning max-weight matching predictors in bipartite graphs . the method consists of performing maximum a posteriori estimation in exponential families with sufficient statistics that encode permutations and data features . although inference is in general hard , we show that for one very relevant application - web page ranking - exact inference is efficient . for general model instances , an appropriate sampler is readily available . contrary to existing max-margin matching models , our approach is statistically consistent and , in addition , experiments with increasing sample sizes indicate superior improvement over such models . we apply the method to graph matching in computer vision as well as to a standard benchmark dataset for learning web page ranking , in which we obtain state-of-the-art results , in particular improving on max-margin variants . the drawback of this method with respect to max-margin alternatives is its runtime for large graphs , which is comparatively high .", "topics": ["sampling ( signal processing )", "time complexity"]}
{"title": "agent models of political interactions", "abstract": "looks at state interactions from an agent based ai perspective to see state interactions as an example of emergent intelligent behavior . exposes basic principles of game theory .", "topics": ["interaction"]}
{"title": "paraphrase generation with deep reinforcement learning", "abstract": "automatic generation of paraphrases from a given sentence is an important yet challenging task in natural language processing ( nlp ) , and plays a key role in a number of applications such as question answering , search , and dialogue . in this paper , we present a deep reinforcement learning approach to paraphrase generation . specifically , we propose a new framework for the task , which consists of a \\textit { generator } and an \\textit { evaluator } , both of which are learned from data . the generator , built as a sequence-to-sequence learning model , can produce paraphrases given a sentence . the evaluator , constructed as a deep matching model , can judge whether two sentences are paraphrases of each other . the generator is first trained by deep learning and then further fine-tuned by reinforcement learning in which the reward is given by the evaluator . for the learning of the evaluator , we propose two methods based on supervised learning and inverse reinforcement learning respectively , depending on the type of available training data . empirical study shows that the learned evaluator can guide the generator to produce more accurate paraphrases . experimental results demonstrate the proposed models ( the generators ) outperform the state-of-the-art methods in paraphrase generation in both automatic evaluation and human evaluation .", "topics": ["natural language processing", "reinforcement learning"]}
{"title": "human perceptions of fairness in algorithmic decision making : a case study of criminal risk prediction", "abstract": "as algorithms are increasingly used to make important decisions that affect human lives , ranging from social benefit assignment to predicting risk of criminal recidivism , concerns have been raised about the fairness of algorithmic decision making . most prior works on algorithmic fairness normatively prescribe how fair decisions ought to be made . in contrast , here , we descriptively survey users for how they perceive and reason about fairness in algorithmic decision making . a key contribution of this work is the framework we propose to understand why people perceive certain features as fair or unfair to be used in algorithms . our framework identifies eight properties of features , such as relevance , volitionality and reliability , as latent considerations that inform people 's moral judgments about the fairness of feature use in decision-making algorithms . we validate our framework through a series of scenario-based surveys with 576 people . we find that , based on a person 's assessment of the eight latent properties of a feature in our exemplar scenario , we can accurately ( > 85 % ) predict if the person will judge the use of the feature as fair . our findings have important implications . at a high-level , we show that people 's unfairness concerns are multi-dimensional and argue that future studies need to address unfairness concerns beyond discrimination . at a low-level , we find considerable disagreements in people 's fairness judgments . we identify root causes of the disagreements , and note possible pathways to resolve them .", "topics": ["high- and low-level", "relevance"]}
{"title": "improved asymmetric locality sensitive hashing ( alsh ) for maximum inner product search ( mips )", "abstract": "recently it was shown that the problem of maximum inner product search ( mips ) is efficient and it admits provably sub-linear hashing algorithms . asymmetric transformations before hashing were the key in solving mips which was otherwise hard . in the prior work , the authors use asymmetric transformations which convert the problem of approximate mips into the problem of approximate near neighbor search which can be efficiently solved using hashing . in this work , we provide a different transformation which converts the problem of approximate mips into the problem of approximate cosine similarity search which can be efficiently solved using signed random projections . theoretical analysis show that the new scheme is significantly better than the original scheme for mips . experimental evaluations strongly support the theoretical findings .", "topics": ["approximation algorithm"]}
{"title": "chinese restaurant game - part ii : applications to wireless networking , cloud computing , and online social networking", "abstract": "in part i of this two-part paper [ 1 ] , we proposed a new game , called chinese restaurant game , to analyze the social learning problem with negative network externality . the best responses of agents in the chinese restaurant game with imperfect signals are constructed through a recursive method , and the influence of both learning and network externality on the utilities of agents is studied . in part ii of this two-part paper , we illustrate three applications of chinese restaurant game in wireless networking , cloud computing , and online social networking . for each application , we formulate the corresponding problem as a chinese restaurant game and analyze how agents learn and make strategic decisions in the problem . the proposed method is compared with four common-sense methods in terms of agents ' utilities and the overall system performance through simulations . we find that the proposed chinese restaurant game theoretic approach indeed helps agents make better decisions and improves the overall system performance . furthermore , agents with different decision orders have different advantages in terms of their utilities , which also verifies the conclusions drawn in part i of this two-part paper .", "topics": ["simulation"]}
{"title": "netgan : generating graphs via random walks", "abstract": "we propose netgan - the first implicit generative model for graphs able to mimic real-world networks . we pose the problem of graph generation as learning the distribution of biased random walks over the input graph . the proposed model is based on a stochastic neural network that generates discrete output samples and is trained using the wasserstein gan objective . netgan is able to produce graphs that exhibit the well-known network patterns without explicitly specifying them in the model definition . at the same time , our model exhibits strong generalization properties , as highlighted by its competitive link prediction performance , despite not being trained specifically for this task . being the first approach to combine both of these desirable properties , netgan opens exciting further avenues for research .", "topics": ["generative model"]}
{"title": "fair assignment of indivisible objects under ordinal preferences", "abstract": "we consider the discrete assignment problem in which agents express ordinal preferences over objects and these objects are allocated to the agents in a fair manner . we use the stochastic dominance relation between fractional or randomized allocations to systematically define varying notions of proportionality and envy-freeness for discrete assignments . the computational complexity of checking whether a fair assignment exists is studied for these fairness notions . we also characterize the conditions under which a fair assignment is guaranteed to exist . for a number of fairness concepts , polynomial-time algorithms are presented to check whether a fair assignment exists . our algorithmic results also extend to the case of unequal entitlements of agents . our np-hardness result , which holds for several variants of envy-freeness , answers an open question posed by bouveret , endriss , and lang ( ecai 2010 ) . we also propose fairness concepts that always suggest a non-empty set of assignments with meaningful fairness properties . among these concepts , optimal proportionality and optimal weak proportionality appear to be desirable fairness concepts .", "topics": ["computational complexity theory", "time complexity"]}
{"title": "single-image depth perception in the wild", "abstract": "this paper studies single-image depth perception in the wild , i.e . , recovering depth from a single image taken in unconstrained settings . we introduce a new dataset `` depth in the wild '' consisting of images in the wild annotated with relative depth between pairs of random points . we also propose a new algorithm that learns to estimate metric depth using annotations of relative depth . compared to the state of the art , our algorithm is simpler and performs better . experiments show that our algorithm , combined with existing rgb-d data and our new relative depth annotations , significantly improves single-image depth perception in the wild .", "topics": ["pixel"]}
{"title": "automatic processing proper names in texts", "abstract": "this paper shows first the problems raised by proper names in natural language processing . second , it introduces the knowledge representation structure we use based on conceptual graphs . then it explains the techniques which are used to process known and unknown proper names . at last , it gives the performance of the system and the further works we intend to deal with .", "topics": ["natural language processing"]}
{"title": "semantic foggy scene understanding with synthetic data", "abstract": "this work addresses the problem of semantic foggy scene understanding ( sfsu ) . although extensive research has been performed on image dehazing and on semantic scene understanding with weather-clear images , little attention has been paid to sfsu . due to the difficulty of collecting and annotating foggy images , we choose to generate synthetic fog on real images that depict weather-clear outdoor scenes , and then leverage these synthetic data for sfsu by employing state-of-the-art convolutional neural networks ( cnn ) . in particular , a complete pipeline to generate synthetic fog on real , weather-clear images using incomplete depth information is developed . we apply our fog synthesis on the cityscapes dataset and generate foggy cityscapes with 20550 images . sfsu is tackled in two fashions : 1 ) with typical supervised learning , and 2 ) with a novel semi-supervised learning , which combines 1 ) with an unsupervised supervision transfer from weather-clear images to their synthetic foggy counterparts . in addition , this work carefully studies the usefulness of image dehazing for sfsu . for evaluation , we present foggy driving , a dataset with 101 real-world images depicting foggy driving scenes , which come with ground truth annotations for semantic segmentation and object detection . extensive experiments show that 1 ) supervised learning with our synthetic data significantly improves the performance of state-of-the-art cnn for sfsu on foggy driving ; 2 ) our semi-supervised learning strategy further improves performance ; and 3 ) image dehazing marginally benefits sfsu with our learning strategy . the datasets , models and code will be made publicly available to encourage further research in this direction .", "topics": ["supervised learning", "object detection"]}
{"title": "visually grounded word embeddings and richer visual features for improving multimodal neural machine translation", "abstract": "in multimodal neural machine translation ( mnmt ) , a neural model generates a translated sentence that describes an image , given the image itself and one source descriptions in english . this is considered as the multimodal image caption translation task . the images are processed with convolutional neural network ( cnn ) to extract visual features exploitable by the translation model . so far , the cnns used are pre-trained on object detection and localization task . we hypothesize that richer architecture , such as dense captioning models , may be more suitable for mnmt and could lead to improved translations . we extend this intuition to the word-embeddings , where we compute both linguistic and visual representation for our corpus vocabulary . we combine and compare different confi", "topics": ["machine translation", "object detection"]}
{"title": "learning common and specific features for rgb-d semantic segmentation with deconvolutional networks", "abstract": "in this paper , we tackle the problem of rgb-d semantic segmentation of indoor images . we take advantage of deconvolutional networks which can predict pixel-wise class labels , and develop a new structure for deconvolution of multiple modalities . we propose a novel feature transformation network to bridge the convolutional networks and deconvolutional networks . in the feature transformation network , we correlate the two modalities by discovering common features between them , as well as characterize each modality by discovering modality specific features . with the common features , we not only closely correlate the two modalities , but also allow them to borrow features from each other to enhance the representation of shared information . with specific features , we capture the visual patterns that are only visible in one modality . the proposed network achieves competitive segmentation accuracy on nyu depth dataset v1 and v2 .", "topics": ["pixel"]}
{"title": "dempster-shafer clustering using potts spin mean field theory", "abstract": "in this article we investigate a problem within dempster-shafer theory where 2**q - 1 pieces of evidence are clustered into q clusters by minimizing a metaconflict function , or equivalently , by minimizing the sum of weight of conflict over all clusters . previously one of us developed a method based on a hopfield and tank model . however , for very large problems we need a method with lower computational complexity . we demonstrate that the weight of conflict of evidence can , as an approximation , be linearized and mapped to an antiferromagnetic potts spin model . this facilitates efficient numerical solution , even for large problem sizes . optimal or nearly optimal solutions are found for dempster-shafer clustering benchmark tests with a time complexity of approximately o ( n**2 log**2 n ) . furthermore , an isomorphism between the antiferromagnetic potts spin model and a graph optimization problem is shown . the graph model has dynamic variables living on the links , which have a priori probabilities that are directly related to the pairwise conflict between pieces of evidence . hence , the relations between three different models are shown .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "nonconvex sparse spectral clustering by alternating direction method of multipliers and its convergence analysis", "abstract": "spectral clustering ( sc ) is a widely used data clustering method which first learns a low-dimensional embedding $ u $ of data by computing the eigenvectors of the normalized laplacian matrix , and then performs k-means on $ u^\\top $ to get the final clustering result . the sparse spectral clustering ( ssc ) method extends sc with a sparse regularization on $ uu^\\top $ by using the block diagonal structure prior of $ uu^\\top $ in the ideal case . however , encouraging $ uu^\\top $ to be sparse leads to a heavily nonconvex problem which is challenging to solve and the work ( lu , yan , and lin 2016 ) proposes a convex relaxation in the pursuit of this aim indirectly . however , the convex relaxation generally leads to a loose approximation and the quality of the solution is not clear . this work instead considers to solve the nonconvex formulation of ssc which directly encourages $ uu^\\top $ to be sparse . we propose an efficient alternating direction method of multipliers ( admm ) to solve the nonconvex ssc and provide the convergence guarantee . in particular , we prove that the sequences generated by admm always exist a limit point and any limit point is a stationary point . our analysis does not impose any assumptions on the iterates and thus is practical . our proposed admm for nonconvex problems allows the stepsize to be increasing but upper bounded , and this makes it very efficient in practice . experimental analysis on several real data sets verifies the effectiveness of our method .", "topics": ["cluster analysis", "matrix regularization"]}
{"title": "learning hmms with nonparametric emissions via spectral decompositions of continuous matrices", "abstract": "recently , there has been a surge of interest in using spectral methods for estimating latent variable models . however , it is usually assumed that the distribution of the observations conditioned on the latent variables is either discrete or belongs to a parametric family . in this paper , we study the estimation of an $ m $ -state hidden markov model ( hmm ) with only smoothness assumptions , such as h\\ '' olderian conditions , on the emission densities . by leveraging some recent advances in continuous linear algebra and numerical analysis , we develop a computationally efficient spectral algorithm for learning nonparametric hmms . our technique is based on computing an svd on nonparametric estimates of density functions by viewing them as \\emph { continuous matrices } . we derive sample complexity bounds via concentration results for nonparametric density estimation and novel perturbation theory results for continuous matrices . we implement our method using chebyshev polynomial approximations . our method is competitive with other baselines on synthetic and real problems and is also very computationally efficient .", "topics": ["baseline ( configuration management )", "computational complexity theory"]}
{"title": "on the exact relationship between the denoising function and the data distribution", "abstract": "we prove an exact relationship between the optimal denoising function and the data distribution in the case of additive gaussian noise , showing that denoising implicitly models the structure of data allowing it to be exploited in the unsupervised learning of representations . this result generalizes a known relationship [ 2 ] , which is valid only in the limit of small corruption noise .", "topics": ["noise reduction", "unsupervised learning"]}
{"title": "signet : scalable embeddings for signed networks", "abstract": "recent successes in word embedding and document embedding have motivated researchers to explore similar representations for networks and to use such representations for tasks such as edge prediction , node label prediction , and community detection . such network embedding methods are largely focused on finding distributed representations for unsigned networks and are unable to discover embeddings that respect polarities inherent in edges . we propose signet , a fast scalable embedding method suitable for signed networks . our proposed objective function aims to carefully model the social structure implicit in signed networks by reinforcing the principles of social balance theory . our method builds upon the traditional word2vec family of embedding approaches and adds a new targeted node sampling strategy to maintain structural balance in higher-order neighborhoods . we demonstrate the superiority of signet over state-of-the-art methods proposed for both signed and unsigned networks on several real world datasets from different domains . in particular , signet offers an approach to generate a richer vocabulary of features of signed networks to support representation and reasoning .", "topics": ["sampling ( signal processing )", "loss function"]}
{"title": "lifted region-based belief propagation", "abstract": "due to the intractable nature of exact lifted inference , research has recently focused on the discovery of accurate and efficient approximate inference algorithms in statistical relational models ( srms ) , such as lifted first-order belief propagation . fobp simulates propositional factor graph belief propagation without constructing the ground factor graph by identifying and lifting over redundant message computations . in this work , we propose a generalization of fobp called lifted generalized belief propagation , in which both the region structure and the message structure can be lifted . this approach allows more of the inference to be performed intra-region ( in the exact inference step of bp ) , thereby allowing simulation of propagation on a graph structure with larger region scopes and fewer edges , while still maintaining tractability . we demonstrate that the resulting algorithm converges in fewer iterations to more accurate results on a variety of srms .", "topics": ["approximation algorithm", "simulation"]}
{"title": "semi-stochastic gradient descent methods", "abstract": "in this paper we study the problem of minimizing the average of a large number ( $ n $ ) of smooth convex loss functions . we propose a new method , s2gd ( semi-stochastic gradient descent ) , which runs for one or several epochs in each of which a single full gradient and a random number of stochastic gradients is computed , following a geometric law . the total work needed for the method to output an $ \\varepsilon $ -accurate solution in expectation , measured in the number of passes over data , or equivalently , in units equivalent to the computation of a single gradient of the loss , is $ o ( ( \\kappa/n ) \\log ( 1/\\varepsilon ) ) $ , where $ \\kappa $ is the condition number . this is achieved by running the method for $ o ( \\log ( 1/\\varepsilon ) ) $ epochs , with a single gradient evaluation and $ o ( \\kappa ) $ stochastic gradient evaluations in each . the svrg method of johnson and zhang arises as a special case . if our method is limited to a single epoch only , it needs to evaluate at most $ o ( ( \\kappa/\\varepsilon ) \\log ( 1/\\varepsilon ) ) $ stochastic gradients . in contrast , svrg requires $ o ( \\kappa/\\varepsilon^2 ) $ stochastic gradients . to illustrate our theoretical results , s2gd only needs the workload equivalent to about 2.1 full gradient evaluations to find an $ 10^ { -6 } $ -accurate solution for a problem with $ n=10^9 $ and $ \\kappa=10^3 $ .", "topics": ["loss function", "gradient descent"]}
{"title": "decentralized topic modelling with latent dirichlet allocation", "abstract": "privacy preserving networks can be modelled as decentralized networks ( e.g . , sensors , connected objects , smartphones ) , where communication between nodes of the network is not controlled by an all-knowing , central node . for this type of networks , the main issue is to gather/learn global information on the network ( e.g . , by optimizing a global cost function ) while keeping the ( sensitive ) information at each node . in this work , we focus on text information that agents do not want to share ( e.g . , text messages , emails , confidential reports ) . we use recent advances on decentralized optimization and topic models to infer topics from a graph with limited communication . we propose a method to adapt latent dirichlet allocation ( lda ) model to decentralized optimization and show on synthetic data that we still recover similar parameters and similar performance at each node than with stochastic methods accessing to the whole information in the graph .", "topics": ["optimization problem", "synthetic data"]}
{"title": "central moment discrepancy ( cmd ) for domain-invariant representation learning", "abstract": "the learning of domain-invariant representations in the context of domain adaptation with neural networks is considered . we propose a new regularization method that minimizes the discrepancy between domain-specific latent feature representations directly in the hidden activation space . although some standard distribution matching approaches exist that can be interpreted as the matching of weighted sums of moments , e.g . maximum mean discrepancy ( mmd ) , an explicit order-wise matching of higher order moments has not been considered before . we propose to match the higher order central moments of probability distributions by means of order-wise moment differences . our model does not require computationally expensive distance and kernel matrix computations . we utilize the equivalent representation of probability distributions by moment sequences to define a new distance function , called central moment discrepancy ( cmd ) . we prove that cmd is a metric on the set of probability distributions on a compact interval . we further prove that convergence of probability distributions on compact intervals w.r.t . the new metric implies convergence in distribution of the respective random variables . we test our approach on two different benchmark data sets for object recognition ( office ) and sentiment analysis of product reviews ( amazon reviews ) . cmd achieves a new state-of-the-art performance on most domain adaptation tasks of office and outperforms networks trained with mmd , variational fair autoencoders and domain adversarial neural networks on amazon reviews . in addition , a post-hoc parameter sensitivity analysis shows that the new approach is stable w.r.t . parameter changes in a certain interval . the source code of the experiments is publicly available .", "topics": ["matrix regularization", "neural networks"]}
{"title": "how we can control the crack to propagate along the specified path feasibly ?", "abstract": "a controllable crack propagation ( ccp ) strategy is suggested . it is well known that crack always leads the failure by crossing the critical domain in engineering structure . therefore , the ccp method is proposed to control the crack to propagate along the specified path , which is away from the critical domain . to complete this strategy , two optimization methods are engaged . firstly , a back propagation neural network ( bpnn ) assisted particle swarm optimization ( pso ) is suggested . in this method , to improve the efficiency of ccp , the bpnn is used to build the metamodel instead of the forward evaluation . secondly , the popular pso is used . considering the optimization iteration is a time consuming process , an efficient reanalysis based extended finite element methods ( x-fem ) is used to substitute the complete x-fem solver to calculate the crack propagation path . moreover , an adaptive subdomain partition strategy is suggested to improve the fitting accuracy between real crack and specified paths . several typical numerical examples demonstrate that both optimization methods can carry out the ccp . the selection of them should be determined by the tradeoff between efficiency and accuracy .", "topics": ["numerical analysis", "iteration"]}
{"title": "lexicalization and grammar development", "abstract": "in this paper we present a fully lexicalized grammar formalism as a particularly attractive framework for the specification of natural language grammars . we discuss in detail feature-based , lexicalized tree adjoining grammars ( fb-ltags ) , a representative of the class of lexicalized grammars . we illustrate the advantages of lexicalized grammars in various contexts of natural language processing , ranging from wide-coverage grammar development to parsing and machine translation . we also present a method for compact and efficient representation of lexicalized trees .", "topics": ["natural language processing", "natural language"]}
{"title": "from images to sentences through scene description graphs using commonsense reasoning and knowledge", "abstract": "in this paper we propose the construction of linguistic descriptions of images . this is achieved through the extraction of scene description graphs ( sdgs ) from visual scenes using an automatically constructed knowledge base . sdgs are constructed using both vision and reasoning . specifically , commonsense reasoning is applied on ( a ) detections obtained from existing perception methods on given images , ( b ) a `` commonsense '' knowledge base constructed using natural language processing of image annotations and ( c ) lexical ontological knowledge from resources such as wordnet . amazon mechanical turk ( amt ) -based evaluations on flickr8k , flickr30k and ms-coco datasets show that in most cases , sentences auto-constructed from sdgs obtained by our method give a more relevant and thorough description of an image than a recent state-of-the-art image caption based approach . our image-sentence alignment evaluation results are also comparable to that of the recent state-of-the art approaches .", "topics": ["natural language processing", "sensor"]}
{"title": "supervised and unsupervised ensembling for knowledge base population", "abstract": "we present results on combining supervised and unsupervised methods to ensemble multiple systems for two popular knowledge base population ( kbp ) tasks , cold start slot filling ( cssf ) and tri-lingual entity discovery and linking ( tedl ) . we demonstrate that our combined system along with auxiliary features outperforms the best performing system for both tasks in the 2015 competition , several ensembling baselines , as well as the state-of-the-art stacking approach to ensembling kbp systems . the success of our technique on two different and challenging problems demonstrates the power and generality of our combined approach to ensembling .", "topics": ["baseline ( configuration management )", "supervised learning"]}
{"title": "learning deep features for one-class classification", "abstract": "we propose a deep learning-based solution for the problem of feature learning in one-class classification . the proposed method operates on top of a convolutional neural network ( cnn ) of choice and produces descriptive features while maintaining a low intra-class variance in the feature space for the given class . for this purpose two loss functions , compactness loss and descriptiveness loss are proposed along with a parallel cnn architecture . a template matching-based framework is introduced to facilitate the testing process . extensive experiments on publicly available anomaly detection , novelty detection and mobile active authentication datasets show that the proposed deep one-class ( doc ) classification method achieves significant improvements over the state-of-the-art .", "topics": ["feature learning", "feature vector"]}
{"title": "state space decomposition and subgoal creation for transfer in deep reinforcement learning", "abstract": "typical reinforcement learning ( rl ) agents learn to complete tasks specified by reward functions tailored to their domain . as such , the policies they learn do not generalize even to similar domains . to address this issue , we develop a framework through which a deep rl agent learns to generalize policies from smaller , simpler domains to more complex ones using a recurrent attention mechanism . the task is presented to the agent as an image and an instruction specifying the goal . this meta-controller guides the agent towards its goal by designing a sequence of smaller subtasks on the part of the state space within the attention , effectively decomposing it . as a baseline , we consider a setup without attention as well . our experiments show that the meta-controller learns to create subgoals within the attention .", "topics": ["baseline ( configuration management )", "reinforcement learning"]}
{"title": "dcfnet : deep neural network with decomposed convolutional filters", "abstract": "filters in a convolutional neural network ( cnn ) contain model parameters learned from enormous amounts of data . in this paper , we suggest to decompose convolutional filters in cnn as a truncated expansion with pre-fixed bases , namely the decomposed convolutional filters network ( dcfnet ) , where the expansion coefficients remain learned from data . such a structure not only reduces the number of trainable parameters and computation , but also imposes filter regularity by bases truncation . through extensive experiments , we consistently observe that dcfnet maintains accuracy for image classification tasks with a significant reduction of model parameters , particularly with fourier-bessel ( fb ) bases , and even with random bases . theoretically , we analyze the representation stability of dcfnet with respect to input variations , and prove representation stability under generic assumptions on the expansion coefficients . the analysis is consistent with the empirical observations .", "topics": ["computer vision", "computation"]}
{"title": "dynamic pricing under finite space demand uncertainty : a multi-armed bandit with dependent arms", "abstract": "we consider a dynamic pricing problem under unknown demand models . in this problem a seller offers prices to a stream of customers and observes either success or failure in each sale attempt . the underlying demand model is unknown to the seller and can take one of n possible forms . in this paper , we show that this problem can be formulated as a multi-armed bandit with dependent arms . we propose a dynamic pricing policy based on the likelihood ratio test . we show that the proposed policy achieves complete learning , i.e . , it offers a bounded regret where regret is defined as the revenue loss with respect to the case with a known demand model . this is in sharp contrast with the logarithmic growing regret in multi-armed bandit with independent arms .", "topics": ["regret ( decision theory )"]}
{"title": "more robust doubly robust off-policy evaluation", "abstract": "we study the problem of off-policy evaluation ( ope ) in reinforcement learning ( rl ) , where the goal is to estimate the performance of a policy from the data generated by another policy ( ies ) . in particular , we focus on the doubly robust ( dr ) estimators that consist of an importance sampling ( is ) component and a performance model , and utilize the low ( or zero ) bias of is and low variance of the model at the same time . although the accuracy of the model has a huge impact on the overall performance of dr , most of the work on using the dr estimators in ope has been focused on improving the is part , and not much on how to learn the model . in this paper , we propose alternative dr estimators , called more robust doubly robust ( mrdr ) , that learn the model parameter by minimizing the variance of the dr estimator . we first present a formulation for learning the dr model in rl . we then derive formulas for the variance of the dr estimator in both contextual bandits and rl , such that their gradients w.r.t.~the model parameters can be estimated from the samples , and propose methods to efficiently minimize the variance . we prove that the mrdr estimators are strongly consistent and asymptotically optimal . finally , we evaluate mrdr in bandits and rl benchmark problems , and compare its performance with the existing methods .", "topics": ["sampling ( signal processing )", "reinforcement learning"]}
{"title": "an approach to improving edge detection for facial and remotely sensed images using vector order statistics", "abstract": "this paper presents an improved edge detection algorithm for facial and remotely sensed images using vector order statistics . the developed algorithm processes colored images directly without been converted to gray scale . a number of the existing algorithms converts the colored images into gray scale before detection of edges . but this process leads to inaccurate precision of recognized edges , thus producing false and broken edges in the output edge map . facial and remotely sensed images consist of curved edge lines which have to be detected continuously to prevent broken edges . in order to deal with this , a collection of pixel approach is introduced with a view to minimizing the false and broken edges that exists in the generated output edge map of facial and remotely sensed images .", "topics": ["pixel"]}
{"title": "ontologies and information extraction", "abstract": "this report argues that , even in the simplest cases , ie is an ontology-driven process . it is not a mere text filtering method based on simple pattern matching and keywords , because the extracted pieces of texts are interpreted with respect to a predefined partial domain model . this report shows that depending on the nature and the depth of the interpretation to be done for extracting the information , more or less knowledge must be involved . this report is mainly illustrated in biology , a domain in which there are critical needs for content-based exploration of the scientific literature and which becomes a major application domain for ie .", "topics": ["natural language"]}
{"title": "improving landmark localization with semi-supervised learning", "abstract": "we present two techniques to improve landmark localization in images from partially annotated datasets . our primary goal is to leverage the common situation where precise landmark locations are only provided for a small data subset , but where class labels for classification or regression tasks related to the landmarks are more abundantly available . first , we propose the framework of sequential multitasking and explore it here through an architecture for landmark localization where training with class labels acts as an auxiliary signal to guide the landmark localization on unlabeled data . a key aspect of our approach is that errors can be backpropagated through a complete landmark localization model . second , we propose and explore an unsupervised learning technique for landmark localization based on having a model predict equivariant landmarks with respect to transformations applied to the image . we show that these techniques , improve landmark prediction considerably and can learn effective detectors even when only a small fraction of the dataset has landmark labels . we present results on two toy datasets and four real datasets , with hands and faces , and report new state-of-the-art on two datasets in the wild , e.g . with only 5\\ % of labeled images we outperform previous state-of-the-art trained on the aflw dataset .", "topics": ["supervised learning"]}
{"title": "zero-shot relation extraction via reading comprehension", "abstract": "we show that relation extraction can be reduced to answering simple reading comprehension questions , by associating one or more natural-language questions with each relation slot . this reduction has several advantages : we can ( 1 ) learn relation-extraction models by extending recent neural reading-comprehension techniques , ( 2 ) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision , and even ( 3 ) do zero-shot learning by extracting new relation types that are only specified at test-time , for which we have no labeled training examples . experiments on a wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation types with high accuracy , and that zero-shot generalization to unseen relation types is possible , at lower accuracy levels , setting the bar for future work on this task .", "topics": ["natural language"]}
{"title": "bayesian poisson tensor factorization for inferring multilateral relations from sparse dyadic event counts", "abstract": "we present a bayesian tensor factorization model for inferring latent group structures from dynamic pairwise interaction patterns . for decades , political scientists have collected and analyzed records of the form `` country $ i $ took action $ a $ toward country $ j $ at time $ t $ '' -- -known as dyadic events -- -in order to form and test theories of international relations . we represent these event data as a tensor of counts and develop bayesian poisson tensor factorization to infer a low-dimensional , interpretable representation of their salient patterns . we demonstrate that our model 's predictive performance is better than that of standard non-negative tensor factorization methods . we also provide a comparison of our variational updates to their maximum likelihood counterparts . in doing so , we identify a better way to form point estimates of the latent factors than that typically used in bayesian poisson matrix factorization . finally , we showcase our model as an exploratory analysis tool for political scientists . we show that the inferred latent factor matrices capture interpretable multilateral relations that both conform to and inform our knowledge of international affairs .", "topics": ["calculus of variations", "sparse matrix"]}
{"title": "multi-advisor reinforcement learning", "abstract": "we consider tackling a single-agent rl problem by distributing it to $ n $ learners . these learners , called advisors , endeavour to solve the problem from a different focus . their advice , taking the form of action values , is then communicated to an aggregator , which is in control of the system . we show that the local planning method for the advisors is critical and that none of the ones found in the literature is flawless : the egocentric planning overestimates values of states where the other advisors disagree , and the agnostic planning is inefficient around danger zones . we introduce a novel approach called empathic and discuss its theoretical aspects . we empirically examine and validate our theoretical findings on a fruit collection task .", "topics": ["reinforcement learning"]}
{"title": "referenceless quality estimation for natural language generation", "abstract": "traditional automatic evaluation measures for natural language generation ( nlg ) use costly human-authored references to estimate the quality of a system output . in this paper , we propose a referenceless quality estimation ( qe ) approach based on recurrent neural networks , which predicts a quality score for a nlg system output by comparing it to the source meaning representation only . our method outperforms traditional metrics and a constant baseline in most respects ; we also show that synthetic data helps to increase correlation results by 21 % compared to the base system . our results are comparable to results obtained in similar qe tasks despite the more challenging setting .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "outlier-robust moment-estimation via sum-of-squares", "abstract": "we develop efficient algorithms for estimating low-degree moments of unknown distributions in the presence of adversarial outliers . the guarantees of our algorithms improve in many cases significantly over the best previous ones , obtained in recent works of diakonikolas et al , lai et al , and charikar et al . we also show that the guarantees of our algorithms match information-theoretic lower-bounds for the class of distributions we consider . these improved guarantees allow us to give improved algorithms for independent component analysis and learning mixtures of gaussians in the presence of outliers . our algorithms are based on a standard sum-of-squares relaxation of the following conceptually-simple optimization problem : among all distributions whose moments are bounded in the same way as for the unknown distribution , find the one that is closest in statistical distance to the empirical distribution of the adversarially-corrupted sample .", "topics": ["optimization problem"]}
{"title": "comparing fifty natural languages and twelve genetic languages using word embedding language divergence ( weld ) as a quantitative measure of language distance", "abstract": "we introduce a new measure of distance between languages based on word embedding , called word embedding language divergence ( weld ) . weld is defined as divergence between unified similarity distribution of words between languages . using such a measure , we perform language comparison for fifty natural languages and twelve genetic languages . our natural language dataset is a collection of sentence-aligned parallel corpora from bible translations for fifty languages spanning a variety of language families . although we use parallel corpora , which guarantees having the same content in all languages , interestingly in many cases languages within the same family cluster together . in addition to natural languages , we perform language comparison for the coding regions in the genomes of 12 different organisms ( 4 plants , 6 animals , and two human subjects ) . our result confirms a significant high-level difference in the genetic language model of humans/animals versus plants . the proposed method is a step toward defining a quantitative measure of similarity between languages , with applications in languages classification , genre identification , dialect identification , and evaluation of translations .", "topics": ["natural language", "high- and low-level"]}
{"title": "deepm : a deep part-based model for object detection and semantic part localization", "abstract": "in this paper , we propose a deep part-based model ( deepm ) for symbiotic object detection and semantic part localization . for this purpose , we annotate semantic parts for all 20 object categories on the pascal voc 2012 dataset , which provides information on object pose , occlusion , viewpoint and functionality . deepm is a latent graphical model based on the state-of-the-art r-cnn framework , which learns an explicit representation of the object-part configuration with flexible type sharing ( e.g . , a sideview horse head can be shared by a fully-visible sideview horse and a highly truncated sideview horse with head and neck only ) . for comparison , we also present an end-to-end object-part ( op ) r-cnn which learns an implicit feature representation for jointly mapping an image roi to the object and part bounding boxes . we evaluate the proposed methods for both the object and part detection performance on pascal voc 2012 , and show that deepm consistently outperforms op r-cnn in detecting objects and parts . in addition , it obtains superior performance to fast and faster r-cnns in object detection .", "topics": ["object detection", "graphical model"]}
{"title": "online linear optimization with the log-determinant regularizer", "abstract": "we consider online linear optimization over symmetric positive semi-definite matrices , which has various applications including the online collaborative filtering . the problem is formulated as a repeated game between the algorithm and the adversary , where in each round t the algorithm and the adversary choose matrices x_t and l_t , respectively , and then the algorithm suffers a loss given by the frobenius inner product of x_t and l_t . the goal of the algorithm is to minimize the cumulative loss . we can employ a standard framework called follow the regularized leader ( ftrl ) for designing algorithms , where we need to choose an appropriate regularization function to obtain a good performance guarantee . we show that the log-determinant regularization works better than other popular regularization functions in the case where the loss matrices l_t are all sparse . using this property , we show that our algorithm achieves an optimal performance guarantee for the online collaborative filtering . the technical contribution of the paper is to develop a new technique of deriving performance bounds by exploiting the property of strong convexity of the log-determinant with respect to the loss matrices , while in the previous analysis the strong convexity is defined with respect to a norm . intuitively , skipping the norm analysis results in the improved bound . moreover , we apply our method to online linear optimization over vectors and show that the ftrl with the burg entropy regularizer , which is the analogue of the log-determinant regularizer in the vector case , works well .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "simple deep random model ensemble", "abstract": "representation learning and unsupervised learning are two central topics of machine learning and signal processing . deep learning is one of the most effective unsupervised representation learning approach . the main contributions of this paper to the topics are as follows . ( i ) we propose to view the representative deep learning approaches as special cases of the knowledge reuse framework of clustering ensemble . ( ii ) we propose to view sparse coding when used as a feature encoder as the consensus function of clustering ensemble , and view dictionary learning as the training process of the base clusterings of clustering ensemble . ( ii ) based on the above two views , we propose a very simple deep learning algorithm , named deep random model ensemble ( drme ) . it is a stack of random model ensembles . each random model ensemble is a special k-means ensemble that discards the expectation-maximization optimization of each base k-means but only preserves the default initialization method of the base k-means . ( iv ) we propose to select the most powerful representation among the layers by applying drme to clustering where the single-linkage is used as the clustering algorithm . moreover , the drme based clustering can also detect the number of the natural clusters accurately . extensive experimental comparisons with 5 representation learning methods on 19 benchmark data sets demonstrate the effectiveness of drme .", "topics": ["feature learning", "cluster analysis"]}
{"title": "functional centering", "abstract": "based on empirical evidence from a free word order language ( german ) we propose a fundamental revision of the principles guiding the ordering of discourse entities in the forward-looking centers within the centering model . we claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances , i.e . , the distinction between context-bound and unbound discourse elements . this claim is backed up by an empirical evaluation of functional centering .", "topics": ["entity"]}
{"title": "syntactic structures and code parameters", "abstract": "we assign binary and ternary error-correcting codes to the data of syntactic structures of world languages and we study the distribution of code points in the space of code parameters . we show that , while most codes populate the lower region approximating a superposition of thomae functions , there is a substantial presence of codes above the gilbert-varshamov bound and even above the asymptotic bound and the plotkin bound . we investigate the dynamics induced on the space of code parameters by spin glass models of language change , and show that , in the presence of entailment relations between syntactic parameters the dynamics can sometimes improve the code . for large sets of languages and syntactic data , one can gain information on the spin glass dynamics from the induced dynamics in the space of code parameters .", "topics": ["approximation algorithm"]}
{"title": "visualizing linguistic shift", "abstract": "neural network based models are a very powerful tool for creating word embeddings , the objective of these models is to group similar words together . these embeddings have been used as features to improve results in various applications such as document classification , named entity recognition , etc . neural language models are able to learn word representations which have been used to capture semantic shifts across time and geography . the objective of this paper is to first identify and then visualize how words change meaning in different text corpus . we will train a neural language model on texts from a diverse set of disciplines philosophy , religion , fiction etc . each text will alter the embeddings of the words to represent the meaning of the word inside that text . we will present a computational technique to detect words that exhibit significant linguistic shift in meaning and usage . we then use enhanced scatterplots and storyline visualization to visualize the linguistic shift .", "topics": ["text corpus"]}
{"title": "semantic 3d reconstruction with finite element bases", "abstract": "we propose a novel framework for the discretisation of multi-label problems on arbitrary , continuous domains . our work bridges the gap between general fem discretisations , and labeling problems that arise in a variety of computer vision tasks , including for instance those derived from the generalised potts model . starting from the popular formulation of labeling as a convex relaxation by functional lifting , we show that fem discretisation is valid for the most general case , where the regulariser is anisotropic and non-metric . while our findings are generic and applicable to different vision problems , we demonstrate their practical implementation in the context of semantic 3d reconstruction , where such regularisers have proved particularly beneficial . the proposed fem approach leads to a smaller memory footprint as well as faster computation , and it constitutes a very simple way to enable variable , adaptive resolution within the same model .", "topics": ["computer vision", "computation"]}
{"title": "two algorithms for finding $ k $ shortest paths of a weighted pushdown automaton", "abstract": "we introduce efficient algorithms for finding the $ k $ shortest paths of a weighted pushdown automaton ( wpda ) , a compact representation of a weighted set of strings with potential applications in parsing and machine translation . both of our algorithms are derived from the same weighted deductive logic description of the execution of a wpda using different search strategies . experimental results show our algorithm 2 adds very little overhead vs. the single shortest path algorithm , even with a large $ k $ .", "topics": ["time complexity", "machine translation"]}
{"title": "artificial intelligence and asymmetric information theory", "abstract": "when human agents come together to make decisions , it is often the case that one human agent has more information than the other . this phenomenon is called information asymmetry and this distorts the market . often if one human agent intends to manipulate a decision in its favor the human agent can signal wrong or right information . alternatively , one human agent can screen for information to reduce the impact of asymmetric information on decisions . with the advent of artificial intelligence , signaling and screening have been made easier . this paper studies the impact of artificial intelligence on the theory of asymmetric information . it is surmised that artificial intelligent agents reduce the degree of information asymmetry and thus the market where these agents are deployed become more efficient . it is also postulated that the more artificial intelligent agents there are deployed in the market the less is the volume of trades in the market . this is because for many trades to happen the asymmetry of information on goods and services to be traded should exist , creating a sense of arbitrage .", "topics": ["artificial intelligence"]}
{"title": "deep learning for accelerated reliability analysis of infrastructure networks", "abstract": "natural disasters can have catastrophic impacts on the functionality of infrastructure systems and cause severe physical and socio-economic losses . given budget constraints , it is crucial to optimize decisions regarding mitigation , preparedness , response , and recovery practices for these systems . this requires accurate and efficient means to evaluate the infrastructure system reliability . while numerous research efforts have addressed and quantified the impact of natural disasters on infrastructure systems , typically using the monte carlo approach , they still suffer from high computational cost and , thus , are of limited applicability to large systems . this paper presents a deep learning framework for accelerating infrastructure system reliability analysis . in particular , two distinct deep neural network surrogates are constructed and studied : ( 1 ) a classifier surrogate which speeds up the connectivity determination of networks , and ( 2 ) an end-to-end surrogate that replaces a number of components such as roadway status realization , connectivity determination , and connectivity averaging . the proposed approach is applied to a simulation-based study of the two-terminal connectivity of a california transportation network subject to extreme probabilistic earthquake events . numerical results highlight the effectiveness of the proposed approach in accelerating the transportation system two-terminal reliability analysis with extremely high prediction accuracy .", "topics": ["simulation", "end-to-end principle"]}
{"title": "neural gpus learn algorithms", "abstract": "learning an algorithm from examples is a fundamental problem that has been widely studied . recently it has been addressed using neural networks , in particular by neural turing machines ( ntms ) . these are fully differentiable computers that use backpropagation to learn their own programming . despite their appeal ntms have a weakness that is caused by their sequential nature : they are not parallel and are are hard to train due to their large depth when unfolded . we present a neural network architecture to address this problem : the neural gpu . it is based on a type of convolutional gated recurrent unit and , like the ntm , is computationally universal . unlike the ntm , the neural gpu is highly parallel which makes it easier to train and efficient to run . an essential property of algorithms is their ability to handle inputs of arbitrary size . we show that the neural gpu can be trained on short instances of an algorithmic task and successfully generalize to long instances . we verified it on a number of tasks including long addition and long multiplication of numbers represented in binary . we train the neural gpu on numbers with upto 20 bits and observe no errors whatsoever while testing it , even on much longer numbers . to achieve these results we introduce a technique for training deep recurrent networks : parameter sharing relaxation . we also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization .", "topics": ["causality"]}
{"title": "dynamic probabilistic network based human action recognition", "abstract": "this paper examines use of dynamic probabilistic networks ( dpn ) for human action recognition . the actions of lifting objects and walking in the room , sitting in the room and neutral standing pose were used for testing the classification . the research used the dynamic interrelation between various different regions of interest ( roi ) on the human body ( face , body , arms , legs ) and the time series based events related to the these rois . this dynamic links are then used to recognize the human behavioral aspects in the scene . first a model is developed to identify the human activities in an indoor scene and this model is dependent on the key features and interlinks between the various dynamic events using dpns . the sub roi are classified with dpn to associate the combined interlink with a specific human activity . the recognition accuracy performance between indoor ( controlled lighting conditions ) is compared with the outdoor lighting conditions . the accuracy in outdoor scenes was lower than the controlled environment .", "topics": ["test set", "image processing"]}
{"title": "interactive visual data exploration with subjective feedback : an information-theoretic approach", "abstract": "visual exploration of high-dimensional real-valued datasets is a fundamental task in exploratory data analysis ( eda ) . existing methods use predefined criteria to choose the representation of data . there is a lack of methods that ( i ) elicit from the user what she has learned from the data and ( ii ) show patterns that she does not know yet . we construct a theoretical model where identified patterns can be input as knowledge to the system . the knowledge syntax here is intuitive , such as `` this set of points forms a cluster '' , and requires no knowledge of maths . this background knowledge is used to find a maximum entropy distribution of the data , after which the system provides the user data projections in which the data and the maximum entropy distribution differ the most , hence showing the user aspects of the data that are maximally informative given the user 's current knowledge . we provide an open source eda system with tailored interactive visualizations to demonstrate these concepts . we study the performance of the system and present use cases on both synthetic and real data . we find that the model and the prototype system allow the user to learn information efficiently from various data sources and the system works sufficiently fast in practice . we conclude that the information theoretic approach to exploratory data analysis where patterns observed by a user are formalized as constraints provides a principled , intuitive , and efficient basis for constructing an eda system .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "learning structural changes of gaussian graphical models in controlled experiments", "abstract": "graphical models are widely used in scienti fic and engineering research to represent conditional independence structures between random variables . in many controlled experiments , environmental changes or external stimuli can often alter the conditional dependence between the random variables , and potentially produce significant structural changes in the corresponding graphical models . therefore , it is of great importance to be able to detect such structural changes from data , so as to gain novel insights into where and how the structural changes take place and help the system adapt to the new environment . here we report an effective learning strategy to extract structural changes in gaussian graphical model using l1-regularization based convex optimization . we discuss the properties of the problem formulation and introduce an efficient implementation by the block coordinate descent algorithm . we demonstrate the principle of the approach on a numerical simulation experiment , and we then apply the algorithm to the modeling of gene regulatory networks under different conditions and obtain promising yet biologically plausible results .", "topics": ["graphical model", "numerical analysis"]}
{"title": "maximizing non-monotone dr-submodular functions with cardinality constraints", "abstract": "we consider the problem of maximizing a non-monotone dr-submodular function subject to a cardinality constraint . diminishing returns ( dr ) submodularity is a generalization of the diminishing returns property for functions defined over the integer lattice . this generalization can be used to solve many machine learning or combinatorial optimization problems such as optimal budget allocation , revenue maximization , etc . in this work we propose the first polynomial-time approximation algorithms for non-monotone constrained maximization . we implement our algorithms for a revenue maximization problem with a real-world dataset to check their efficiency and performance .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "exploiting symmetry and/or manhattan properties for 3d object structure estimation from single and multiple images", "abstract": "many man-made objects have intrinsic symmetries and manhattan structure . by assuming an orthographic projection model , this paper addresses the estimation of 3d structures and camera projection using symmetry and/or manhattan structure cues , which occur when the input is single- or multiple-image from the same category , e.g . , multiple different cars . specifically , analysis on the single image case implies that manhattan alone is sufficient to recover the camera projection , and then the 3d structure can be reconstructed uniquely exploiting symmetry . however , manhattan structure can be difficult to observe from a single image due to occlusion . to this end , we extend to the multiple-image case which can also exploit symmetry but does not require manhattan axes . we propose a novel rigid structure from motion method , exploiting symmetry and using multiple images from the same category as input . experimental results on the pascal3d+ dataset show that our method significantly outperforms baseline methods .", "topics": ["baseline ( configuration management )"]}
{"title": "decision support with text-based emotion recognition : deep learning for affective computing", "abstract": "emotions widely affect the decision-making of humans . this is taken into account by affective computing with the goal of tailoring decision support to the emotional states of individuals . however , the accurate recognition of emotions within narrative documents presents a challenging undertaking due to the complexity and ambiguity of language . even though deep learning has evolved as the state-of-the-art in text mining , empirical investigations of its benefits for affective computing are scarce . we thus adapt recurrent neural networks from the field of deep learning to emotion recognition . in addition , we propose the use of transfer learning as an inductive knowledge transfer from related tasks in natural language processing . the resulting performance is evaluated in a holistic setting , where we find that both recurrent neural networks and transfer learning consistently outperforms traditional machine learning . altogether , the findings have considerable implications for the use of affective computing in providing decision support .", "topics": ["recurrent neural network"]}
{"title": "context-aware hypergraph construction for robust spectral clustering", "abstract": "spectral clustering is a powerful tool for unsupervised data analysis . in this paper , we propose a context-aware hypergraph similarity measure ( cahsm ) , which leads to robust spectral clustering in the case of noisy data . we construct three types of hypergraph -- -the pairwise hypergraph , the k-nearest-neighbor ( knn ) hypergraph , and the high-order over-clustering hypergraph . the pairwise hypergraph captures the pairwise similarity of data points ; the knn hypergraph captures the neighborhood of each point ; and the clustering hypergraph encodes high-order contexts within the dataset . by combining the affinity information from these three hypergraphs , the cahsm algorithm is able to explore the intrinsic topological information of the dataset . therefore , data clustering using cahsm tends to be more robust . considering the intra-cluster compactness and the inter-cluster separability of vertices , we further design a discriminative hypergraph partitioning criterion ( dhpc ) . using both cahsm and dhpc , a robust spectral clustering algorithm is developed . theoretical analysis and experimental evaluation demonstrate the effectiveness and robustness of the proposed algorithm .", "topics": ["cluster analysis", "eisenstein 's criterion"]}
{"title": "xunit : learning a spatial activation function for efficient image restoration", "abstract": "in recent years , deep neural networks ( dnns ) achieved unprecedented performance in many low-level vision tasks . however , state-of-the-art results are typically achieved by very deep networks , which can reach tens of layers with tens of millions of parameters . to make dnns implementable on platforms with limited resources , it is necessary to weaken the tradeoff between performance and efficiency . in this paper , we propose a new activation unit , which is particularly suitable for image restoration problems . in contrast to the widespread per-pixel activation units , like relus and sigmoids , our unit implements a learnable nonlinear function with spatial connections . this enables the net to capture much more complex features , thus requiring a significantly smaller number of layers in order to reach the same performance . we illustrate the effectiveness of our units through experiments with state-of-the-art nets for denoising , de-raining , and super resolution , which are already considered to be very small . with our approach , we are able to further reduce these models by nearly 50 % without incurring any degradation in performance .", "topics": ["high- and low-level", "nonlinear system"]}
{"title": "graph based classification methods using inaccurate external classifier information", "abstract": "in this paper we consider the problem of collectively classifying entities where relational information is available across the entities . in practice inaccurate class distribution for each entity is often available from another ( external ) classifier . for example this distribution could come from a classifier built using content features or a simple dictionary . given the relational and inaccurate external classifier information , we consider two graph based settings in which the problem of collective classification can be solved . in the first setting the class distribution is used to fix labels to a subset of nodes and the labels for the remaining nodes are obtained like in a transductive setting . in the other setting the class distributions of all nodes are used to define the fitting function part of a graph regularized objective function . we define a generalized objective function that handles both the settings . methods like harmonic gaussian field and local-global consistency ( lgc ) reported in the literature can be seen as special cases . we extend the lgc and weighted vote relational neighbor classification ( wvrn ) methods to support usage of external classifier information . we also propose an efficient least squares regularization ( lsr ) based method and relate it to information regularization methods . all the methods are evaluated on several benchmark and real world datasets . considering together speed , robustness and accuracy , experimental results indicate that the lsr and wvrn-extension methods perform better than other methods .", "topics": ["statistical classification", "optimization problem"]}
{"title": "a visual web tool to perform what-if analysis of optimization approaches", "abstract": "in operation research , practical evaluation is essential to validate the efficacy of optimization approaches . this paper promotes the usage of performance profiles as a standard practice to visualize and analyze experimental results . it introduces a web tool to construct and export performance profiles as svg or html files . in addition , the application relies on a methodology to estimate the benefit of hypothetical solver improvements . therefore , the tool allows one to employ what-if analysis to screen possible research directions , and identify those having the best potential . the approach is showcased on two operation research technologies : constraint programming and mixed integer linear programming .", "topics": ["mathematical optimization"]}
{"title": "few-shot autoregressive density estimation : towards learning to learn distributions", "abstract": "deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as imagenet . however , such models require many thousands of gradient-based weight updates and unique image examples for training . ideally , the models would rapidly learn visual concepts from only a handful of examples , similar to the manner in which humans learns across many vision tasks . in this paper , we show how 1 ) neural attention and 2 ) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation . our proposed modifications to pixelcnn result in state-of-the art few-shot density estimation on the omniglot dataset . furthermore , we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on imagenet and handwriting on omniglot without supervision . finally , we extend the model to natural images and demonstrate few-shot image generation on the stanford online products dataset .", "topics": ["gradient"]}
{"title": "deep class aware denoising", "abstract": "the increasing demand for high image quality in mobile devices brings forth the need for better computational enhancement techniques , and image denoising in particular . at the same time , the images captured by these devices can be categorized into a small set of semantic classes . however simple , this observation has not been exploited in image denoising until now . in this paper , we demonstrate how the reconstruction quality improves when a denoiser is aware of the type of content in the image . to this end , we first propose a new fully convolutional deep neural network architecture which is simple yet powerful as it achieves state-of-the-art performance even without being class-aware . we further show that a significant boost in performance of up to $ 0.4 $ db psnr can be achieved by making our network class-aware , namely , by fine-tuning it for images belonging to a specific semantic class . relying on the hugely successful existing image classifiers , this research advocates for using a class-aware approach in all image enhancement tasks .", "topics": ["image processing", "noise reduction"]}
{"title": "improved learning in evolution strategies via sparser inter-agent network topologies", "abstract": "we draw upon a previously largely untapped literature on human collective intelligence as a source of inspiration for improving deep learning . implicit in many algorithms that attempt to solve deep reinforcement learning ( drl ) tasks is the network of processors along which parameter values are shared . so far , existing approaches have implicitly utilized fully-connected networks , in which all processors are connected . however , the scientific literature on human collective intelligence suggests that complete networks may not always be the most effective information network structures for distributed search through complex spaces . here we show that alternative topologies can improve deep neural network training : we find that sparser networks learn higher rewards faster , leading to learning improvements at lower communication costs .", "topics": ["reinforcement learning"]}
{"title": "decomposition into low-rank plus additive matrices for background/foreground separation : a review for a comparative evaluation with a large-scale dataset", "abstract": "recent research on problem formulations based on decomposition into low-rank plus sparse matrices shows a suitable framework to separate moving objects from the background . the most representative problem formulation is the robust principal component analysis ( rpca ) solved via principal component pursuit ( pcp ) which decomposes a data matrix in a low-rank matrix and a sparse matrix . however , similar robust implicit or explicit decompositions can be made in the following problem formulations : robust non-negative matrix factorization ( rnmf ) , robust matrix completion ( rmc ) , robust subspace recovery ( rsr ) , robust subspace tracking ( rst ) and robust low-rank minimization ( rlrm ) . the main goal of these similar problem formulations is to obtain explicitly or implicitly a decomposition into low-rank matrix plus additive matrices . in this context , this work aims to initiate a rigorous and comprehensive review of the similar problem formulations in robust subspace learning and tracking based on decomposition into low-rank plus additive matrices for testing and ranking existing algorithms for background/foreground separation . for this , we first provide a preliminary review of the recent developments in the different problem formulations which allows us to define a unified view that we called decomposition into low-rank plus additive matrices ( dlam ) . then , we examine carefully each method in each robust subspace learning/tracking frameworks with their decomposition , their loss functions , their optimization problem and their solvers . furthermore , we investigate if incremental algorithms and real-time implementations can be achieved for background/foreground separation . finally , experimental results on a large-scale dataset called background models challenge ( bmc 2012 ) show the comparative performance of 32 different robust subspace learning/tracking methods .", "topics": ["optimization problem", "sparse matrix"]}
{"title": "driving cdcl search", "abstract": "the cdcl algorithm is the leading solution adopted by state-of-the-art solvers for sat , smt , asp , and others . experiments show that the performance of cdcl solvers can be significantly boosted by embedding domain-specific heuristics , especially on large real-world problems . however , a proper integration of such criteria in off-the-shelf cdcl implementations is not obvious . in this paper , we distill the key ingredients that drive the search of cdcl solvers , and propose a general framework for designing and implementing new heuristics . we implemented our strategy in an asp solver , and we experimented on two industrial domains . on hard problem instances , state-of-the-art implementations fail to find any solution in acceptable time , whereas our implementation is very successful and finds all solutions .", "topics": ["heuristic"]}
{"title": "human perception in computer vision", "abstract": "computer vision has made remarkable progress in recent years . deep neural network ( dnn ) models optimized to identify objects in images exhibit unprecedented task-trained accuracy and , remarkably , some generalization ability : new visual problems can now be solved more easily based on previous learning . biological vision ( learned in life and through evolution ) is also accurate and general-purpose . is it possible that these different learning regimes converge to similar problem-dependent optimal computations ? we therefore asked whether the human system-level computation of visual perception has dnn correlates and considered several anecdotal test cases . we found that perceptual sensitivity to image changes has dnn mid-computation correlates , while sensitivity to segmentation , crowding and shape has dnn end-computation correlates . our results quantify the applicability of using dnn computation to estimate perceptual loss , and are consistent with the fascinating theoretical view that properties of human perception are a consequence of architecture-independent visual learning .", "topics": ["computer vision", "computation"]}
{"title": "a linked data scalability challenge : concept reuse leads to semantic decay", "abstract": "the increasing amount of available linked data resources is laying the foundations for more advanced semantic web applications . one of their main limitations , however , remains the general low level of data quality . in this paper we focus on a measure of quality which is negatively affected by the increase of the available resources . we propose a measure of semantic richness of linked data concepts and we demonstrate our hypothesis that the more a concept is reused , the less semantically rich it becomes . this is a significant scalability issue , as one of the core aspects of linked data is the propagation of semantic information on the web by reusing common terms . we prove our hypothesis with respect to our measure of semantic richness and we validate our model empirically . finally , we suggest possible future directions to address this scalability problem .", "topics": ["high- and low-level", "scalability"]}
{"title": "prophit : causal inverse classification for multiple continuously valued treatment policies", "abstract": "inverse classification uses an induced classifier as a queryable oracle to guide test instances towards a preferred posterior class label . the result produced from the process is a set of instance-specific feature perturbations , or recommendations , that optimally improve the probability of the class label . in this work , we adopt a causal approach to inverse classification , eliciting treatment policies ( i.e . , feature perturbations ) for models induced with causal properties . in so doing , we solve a long-standing problem of eliciting multiple , continuously valued treatment policies , using an updated framework and corresponding set of assumptions , which we term the inverse classification potential outcomes framework ( icpof ) , along with a new measure , referred to as the individual future estimated effects ( $ i $ fee ) . we also develop the approximate propensity score ( aps ) , based on gaussian processes , to weight treatments , much like the inverse propensity score weighting used in past works . we demonstrate the viability of our methods on student performance .", "topics": ["causality"]}
{"title": "formal verification of piece-wise linear feed-forward neural networks", "abstract": "we present an approach for the verification of feed-forward neural networks in which all nodes have a piece-wise linear activation function . such networks are often used in deep learning and have been shown to be hard to verify for modern satisfiability modulo theory ( smt ) and integer linear programming ( ilp ) solvers . the starting point of our approach is the addition of a global linear approximation of the overall network behavior to the verification problem that helps with smt-like reasoning over the network behavior . we present a specialized verification algorithm that employs this approximation in a search process in which it infers additional node phases for the non-linear nodes in the network from partial node phase assignments , similar to unit propagation in classical sat solving . we also show how to infer additional conflict clauses and safe node fixtures from the results of the analysis steps performed during the search . the resulting approach is evaluated on collision avoidance and handwritten digit recognition case studies .", "topics": ["nonlinear system"]}
{"title": "dictionary-based concept mining : an application for turkish", "abstract": "in this study , a dictionary-based method is used to extract expressive concepts from documents . so far , there have been many studies concerning concept mining in english , but this area of study for turkish , an agglutinative language , is still immature . we used dictionary instead of wordnet , a lexical database grouping words into synsets that is widely used for concept extraction . the dictionaries are rarely used in the domain of concept mining , but taking into account that dictionary entries have synonyms , hypernyms , hyponyms and other relationships in their meaning texts , the success rate has been high for determining concepts . this concept extraction method is implemented on documents , that are collected from different corpora .", "topics": ["dictionary"]}
{"title": "probabilistic conflict resolution in hierarchical hypothesis spaces", "abstract": "artificial intelligence applications such as industrial robotics , military surveillance , and hazardous environment clean-up , require situation understanding based on partial , uncertain , and ambiguous or erroneous evidence . it is necessary to evaluate the relative likelihood of multiple possible hypotheses of the ( current ) situation faced by the decision making program . often , the evidence and hypotheses are hierarchical in nature . in image understanding tasks , for example , evidence begins with raw imagery , from which ambiguous features are extracted which have multiple possible aggregations providing evidential support for the presence of multiple hypothesis of objects and terrain , which in turn aggregate in multiple ways to provide partial evidence for different interpretations of the ambient scene . information fusion for military situation understanding has a similar evidence/hypothesis hierarchy from multiple sensor through message level interpretations , and also provides evidence at multiple levels of the doctrinal hierarchy of military forces .", "topics": ["computer vision", "artificial intelligence"]}
{"title": "deep image super resolution via natural image priors", "abstract": "single image super-resolution ( sr ) via deep learning has recently gained significant attention in the literature . convolutional neural networks ( cnns ) are typically learned to represent the mapping between low-resolution ( lr ) and high-resolution ( hr ) images/patches with the help of training examples . most existing deep networks for sr produce high quality results when training data is abundant . however , their performance degrades sharply when training is limited . we propose to regularize deep structures with prior knowledge about the images so that they can capture more structural information from the same limited data . in particular , we incorporate in a tractable fashion within the cnn framework , natural image priors which have shown to have much recent success in imaging and vision inverse problems . experimental results show that the proposed deep network with natural image priors is particularly effective in training starved regimes .", "topics": ["test set"]}
{"title": "effective blind source separation based on the adam algorithm", "abstract": "in this paper , we derive a modified infomax algorithm for the solution of blind signal separation ( bss ) problems by using advanced stochastic methods . the proposed approach is based on a novel stochastic optimization approach known as the adaptive moment estimation ( adam ) algorithm . the proposed bss solution can benefit from the excellent properties of the adam approach . in order to derive the new learning rule , the adam algorithm is introduced in the derivation of the cost function maximization in the standard infomax algorithm . the natural gradient adaptation is also considered . finally , some experimental results show the effectiveness of the proposed approach .", "topics": ["loss function", "gradient"]}
{"title": "movie description", "abstract": "audio description ( ad ) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers . such descriptions are by design mainly visual and thus naturally form an interesting data source for computer vision and computational linguistics . in this work we propose a novel dataset which contains transcribed ads , which are temporally aligned to full length movies . in addition we also collected and aligned movie scripts used in prior work and compare the two sources of descriptions . in total the large scale movie description challenge ( lsmdc ) contains a parallel corpus of 118,114 sentences and video clips from 202 movies . first we characterize the dataset by benchmarking different approaches for generating video descriptions . comparing ads to scripts , we find that ads are indeed more visual and describe precisely what is shown rather than what should happen according to the scripts created prior to movie production . furthermore , we present and compare the results of several teams who participated in a challenge organized in the context of the workshop `` describing and understanding video & the large scale movie description challenge ( lsmdc ) '' , at iccv 2015 .", "topics": ["computer vision"]}
{"title": "the effect of biased communications on both trusting and suspicious voters", "abstract": "in recent studies of political decision-making , apparently anomalous behavior has been observed on the part of voters , in which negative information about a candidate strengthens , rather than weakens , a prior positive opinion about the candidate . this behavior appears to run counter to rational models of decision making , and it is sometimes interpreted as evidence of non-rational `` motivated reasoning '' . we consider scenarios in which this effect arises in a model of rational decision making which includes the possibility of deceptive information . in particular , we will consider a model in which there are two classes of voters , which we will call trusting voters and suspicious voters , and two types of information sources , which we will call unbiased sources and biased sources . in our model , new data about a candidate can be efficiently incorporated by a trusting voter , and anomalous updates are impossible ; however , anomalous updates can be made by suspicious voters , if the information source mistakenly plans for an audience of trusting voters , and if the partisan goals of the information source are known by the suspicious voter to be `` opposite '' to his own . our model is based on a formalism introduced by the artificial intelligence community called `` multi-agent influence diagrams '' , which generalize bayesian networks to settings involving multiple agents with distinct goals .", "topics": ["bayesian network", "artificial intelligence"]}
{"title": "agglomerative bregman clustering", "abstract": "this manuscript develops the theory of agglomerative clustering with bregman divergences . geometric smoothing techniques are developed to deal with degenerate clusters . to allow for cluster models based on exponential families with overcomplete representations , bregman divergences are developed for nondifferentiable convex functions .", "topics": ["cluster analysis"]}
{"title": "neural end-to-end learning for computational argumentation mining", "abstract": "we investigate neural techniques for end-to-end computational argumentation mining ( am ) . we frame am both as a token-based dependency parsing and as a token-based sequence tagging problem , including a multi-task learning setup . contrary to models that operate on the argument component level , we find that framing am as dependency parsing leads to subpar performance results . in contrast , less complex ( local ) tagging models based on bilstms perform robustly across classification scenarios , being able to catch long-range dependencies inherent to the am problem . moreover , we find that jointly learning 'natural ' subtasks , in a multi-task learning setup , improves performance .", "topics": ["data mining", "parsing"]}
{"title": "group-sparse embeddings in collective matrix factorization", "abstract": "cmf is a technique for simultaneously learning low-rank representations based on a collection of matrices with shared entities . a typical example is the joint modeling of user-item , item-property , and user-feature matrices in a recommender system . the key idea in cmf is that the embeddings are shared across the matrices , which enables transferring information between them . the existing solutions , however , break down when the individual matrices have low-rank structure not shared with others . in this work we present a novel cmf solution that allows each of the matrices to have a separate low-rank structure that is independent of the other matrices , as well as structures that are shared only by a subset of them . we compare map and variational bayesian solutions based on alternating optimization algorithms and show that the model automatically infers the nature of each factor using group-wise sparsity . our approach supports in a principled way continuous , binary and count observations and is efficient for sparse matrices involving missing data . we illustrate the solution on a number of examples , focusing in particular on an interesting use-case of augmented multi-view learning .", "topics": ["calculus of variations", "sparse matrix"]}
{"title": "fast distribution to real regression", "abstract": "we study the problem of distribution to real-value regression , where one aims to regress a mapping $ f $ that takes in a distribution input covariate $ p\\in \\mathcal { i } $ ( for a non-parametric family of distributions $ \\mathcal { i } $ ) and outputs a real-valued response $ y=f ( p ) + \\epsilon $ . this setting was recently studied , and a `` kernel-kernel '' estimator was introduced and shown to have a polynomial rate of convergence . however , evaluating a new prediction with the kernel-kernel estimator scales as $ \\omega ( n ) $ . this causes the difficult situation where a large amount of data may be necessary for a low estimation risk , but the computation cost of estimation becomes infeasible when the data-set is too large . to this end , we propose the double-basis estimator , which looks to alleviate this big data problem in two ways : first , the double-basis estimator is shown to have a computation complexity that is independent of the number of of instances $ n $ when evaluating new predictions after training ; secondly , the double-basis estimator is shown to have a fast rate of convergence for a general class of mappings $ f\\in\\mathcal { f } $ .", "topics": ["computation", "polynomial"]}
{"title": "representation of a sentence using a polar fuzzy neutrosophic semantic net", "abstract": "a semantic net can be used to represent a sentence . a sentence in a language contains semantics which are polar in nature , that is , semantics which are positive , neutral and negative . neutrosophy is a relatively new field of science which can be used to mathematically represent triads of concepts . these triads include truth , indeterminacy and falsehood , and so also positivity , neutrality and negativity . thus a conventional semantic net has been extended in this paper using neutrosophy into a polar fuzzy neutrosophic semantic net . a polar fuzzy neutrosophic semantic net has been implemented in matlab and has been used to illustrate a polar sentence in english language . the paper demonstrates a method for the representation of polarity in a computers memory . thus , polar concepts can be applied to imbibe a machine such as a robot , with emotions , making machine emotion representation possible .", "topics": ["natural language processing", "robot"]}
{"title": "a deep-reinforcement learning approach for software-defined networking routing optimization", "abstract": "in this paper we design and evaluate a deep-reinforcement learning agent that optimizes routing . our agent adapts automatically to current traffic conditions and proposes tailored configurations that attempt to minimize the network delay . experiments show very promising performance . moreover , this approach provides important operational advantages with respect to traditional optimization algorithms .", "topics": ["reinforcement learning"]}
{"title": "cogscik : clustering for cognitive science motivated decision making", "abstract": "computational models of decisionmaking must contend with the variance of context and any number of possible decisions that a defined strategic actor can make at a given time . relying on cognitive science theory , the authors have created an algorithm that captures the orientation of the actor towards an object and arrays the possible decisions available to that actor based on their given intersubjective orientation . this algorithm , like a traditional k-means clustering algorithm , relies on a core-periphery structure that gives the likelihood of moves as those closest to the cluster 's centroid . the result is an algorithm that enables unsupervised classification of an array of decision points belonging to an actor 's present state and deeply rooted in cognitive science theory .", "topics": ["cluster analysis", "unsupervised learning"]}
{"title": "a method for comparing hedge funds", "abstract": "the paper presents new machine learning methods : signal composition , which classifies time-series regardless of length , type , and quantity ; and self-labeling , a supervised-learning enhancement . the paper describes further the implementation of the methods on a financial search engine system to identify behavioral similarities among time-series representing monthly returns of 11,312 hedge funds operated during approximately one decade ( 2000 - 2010 ) . the presented approach of cross-category and cross-location classification assists the investor to identify alternative investments .", "topics": ["time series", "supervised learning"]}
{"title": "a parallel best-response algorithm with exact line search for nonconvex sparsity-regularized rank minimization", "abstract": "in this paper , we propose a convergent parallel best-response algorithm with the exact line search for the nondifferentiable nonconvex sparsity-regularized rank minimization problem . on the one hand , it exhibits a faster convergence than subgradient algorithms and block coordinate descent algorithms . on the other hand , its convergence to a stationary point is guaranteed , while admm algorithms only converge for convex problems . furthermore , the exact line search procedure in the proposed algorithm is performed efficiently in closed-form to avoid the meticulous choice of stepsizes , which is however a common bottleneck in subgradient algorithms and successive convex approximation algorithms . finally , the proposed algorithm is numerically tested .", "topics": ["approximation algorithm", "numerical analysis"]}
{"title": "parallel dither and dropout for regularising deep neural networks", "abstract": "effective regularisation during training can mean the difference between success and failure for deep neural networks . recently , dither has been suggested as alternative to dropout for regularisation during batch-averaged stochastic gradient descent ( sgd ) . in this article , we show that these methods fail without batch averaging and we introduce a new , parallel regularisation method that may be used without batch averaging . our results for parallel-regularised non-batch-sgd are substantially better than what is possible with batch-sgd . furthermore , our results demonstrate that dither and dropout are complimentary .", "topics": ["gradient descent", "gradient"]}
{"title": "high performance software in multidimensional reduction methods for image processing with application to ancient manuscripts", "abstract": "multispectral imaging is an important technique for improving the readability of written or printed text where the letters have faded , either due to deliberate erasing or simply due to the ravages of time . often the text can be read simply by looking at individual wavelengths , but in some cases the images need further enhancement to maximise the chances of reading the text . there are many possible enhancement techniques and this paper assesses and compares an extended set of dimensionality reduction methods for image processing . we assess 15 dimensionality reduction methods in two different manuscripts . this assessment was performed both subjectively by asking the opinions of scholars who were experts in the languages used in the manuscripts which of the techniques they preferred and also by using the davies-bouldin and dunn indexes for assessing the quality of the resulted image clusters . we found that the canonical variates analysis ( cva ) method which was using a matlab implementation and we have used previously to enhance multispectral images , it was indeed superior to all the other tested methods . however it is very likely that other approaches will be more suitable in specific circumstance so we would still recommend that a range of these techniques are tried . in particular , cva is a supervised clustering technique so it requires considerably more user time and effort than a non-supervised technique such as the much more commonly used principle component analysis approach ( pca ) . if the results from pca are adequate to allow a text to be read then the added effort required for cva may not be justified . for the purposes of comparing the computational times and the image results , a cva method is also implemented in c programming language and using the gnu ( gnus not unix ) scientific library ( gsl ) and the opencv ( open source computer vision ) computer vision programming library .", "topics": ["image processing", "cluster analysis"]}
{"title": "analysis of a play by means of chaplin , the characters and places interaction network software", "abstract": "recently , we have developed a software able of gathering information on social networks from written texts . this software , the characters and places interaction network ( chaplin ) tool , is implemented in visual basic . by means of it , characters and places of a literary work can be extracted from a list of raw words . the software interface helps users to select their names out of this list . setting some parameters , chaplin creates a network where nodes represent characters/places and edges give their interactions . nodes and edges are labelled by performances . in this paper , we propose to use chaplin for the analysis a william shakespeare 's play , the famous 'tragedy of hamlet , prince of denmark ' . performances of characters in the play as a whole and in each act of it are given by graphs .", "topics": ["interaction"]}
{"title": "topic modeling on health journals with regularized variational inference", "abstract": "topic modeling enables exploration and compact representation of a corpus . the caringbridge ( cb ) dataset is a massive collection of journals written by patients and caregivers during a health crisis . topic modeling on the cb dataset , however , is challenging due to the asynchronous nature of multiple authors writing about their health journeys . to overcome this challenge we introduce the dynamic author-persona topic model ( dap ) , a probabilistic graphical model designed for temporal corpora with multiple authors . the novelty of the dap model lies in its representation of authors by a persona -- - where personas capture the propensity to write about certain topics over time . further , we present a regularized variational inference algorithm , which we use to encourage the dap model 's personas to be distinct . our results show significant improvements over competing topic models -- - particularly after regularization , and highlight the dap model 's unique ability to capture common journeys shared by different authors .", "topics": ["graphical model", "calculus of variations"]}
{"title": "spectral methods for correlated topic models", "abstract": "in this paper , we propose guaranteed spectral methods for learning a broad range of topic models , which generalize the popular latent dirichlet allocation ( lda ) . we overcome the limitation of lda to incorporate arbitrary topic correlations , by assuming that the hidden topic proportions are drawn from a flexible class of normalized infinitely divisible ( nid ) distributions . nid distributions are generated through the process of normalizing a family of independent infinitely divisible ( id ) random variables . the dirichlet distribution is a special case obtained by normalizing a set of gamma random variables . we prove that this flexible topic model class can be learned via spectral methods using only moments up to the third order , with ( low order ) polynomial sample and computational complexity . the proof is based on a key new technique derived here that allows us to diagonalize the moments of the nid distribution through an efficient procedure that requires evaluating only univariate integrals , despite the fact that we are handling high dimensional multivariate moments . in order to assess the performance of our proposed latent nid topic model , we use two real datasets of articles collected from new york times and pubmed . our experiments yield improved perplexity on both datasets compared with the baseline .", "topics": ["baseline ( configuration management )", "computational complexity theory"]}
{"title": "a framework for non-monotonic reasoning about probabilistic assumptions", "abstract": "attempts to replicate probabilistic reasoning in expert systems have typically overlooked a critical ingredient of that process . probabilistic analysis typically requires extensive judgments regarding interdependencies among hypotheses and data , and regarding the appropriateness of various alternative models . the application of such models is often an iterative process , in which the plausibility of the results confirms or disconfirms the validity of assumptions made in building the model . in current expert systems , by contrast , probabilistic information is encapsulated within modular rules ( involving , for example , `` certainty factors '' ) , and there is no mechanism for reviewing the overall form of the probability argument or the validity of the judgments entering into it .", "topics": ["iteration"]}
{"title": "canonical trends : detecting trend setters in web data", "abstract": "much information available on the web is copied , reused or rephrased . the phenomenon that multiple web sources pick up certain information is often called trend . a central problem in the context of web data mining is to detect those web sources that are first to publish information which will give rise to a trend . we present a simple and efficient method for finding trends dominating a pool of web sources and identifying those web sources that publish the information relevant to a trend before others . we validate our approach on real data collected from influential technology news feeds .", "topics": ["data mining"]}
{"title": "efficient computation of the well-founded semantics over big data", "abstract": "data originating from the web , sensor readings and social media result in increasingly huge datasets . the so called big data comes with new scientific and technological challenges while creating new opportunities , hence the increasing interest in academia and industry . traditionally , logic programming has focused on complex knowledge structures/programs , so the question arises whether and how it can work in the face of big data . in this paper , we examine how the well-founded semantics can process huge amounts of data through mass parallelization . more specifically , we propose and evaluate a parallel approach using the mapreduce framework . our experimental results indicate that our approach is scalable and that well-founded semantics can be applied to billions of facts . to the best of our knowledge , this is the first work that addresses large scale nonmonotonic reasoning without the restriction of stratification for predicates of arbitrary arity . to appear in theory and practice of logic programming ( tplp ) .", "topics": ["computation", "scalability"]}
{"title": "the information geometry of mirror descent", "abstract": "information geometry applies concepts in differential geometry to probability and statistics and is especially useful for parameter estimation in exponential families where parameters are known to lie on a riemannian manifold . connections between the geometric properties of the induced manifold and statistical properties of the estimation problem are well-established . however developing first-order methods that scale to larger problems has been less of a focus in the information geometry community . the best known algorithm that incorporates manifold structure is the second-order natural gradient descent algorithm introduced by amari . on the other hand , stochastic approximation methods have led to the development of first-order methods for optimizing noisy objective functions . a recent generalization of the robbins-monro algorithm known as mirror descent , developed by nemirovski and yudin is a first order method that induces non-euclidean geometries . however current analysis of mirror descent does not precisely characterize the induced non-euclidean geometry nor does it consider performance in terms of statistical relative efficiency . in this paper , we prove that mirror descent induced by bregman divergences is equivalent to the natural gradient descent algorithm on the dual riemannian manifold . using this equivalence , it follows that ( 1 ) mirror descent is the steepest descent direction along the riemannian manifold of the exponential family ; ( 2 ) mirror descent with log-likelihood loss applied to parameter estimation in exponential families asymptotically achieves the classical cram\\'er-rao lower bound and ( 3 ) natural gradient descent for manifolds corresponding to exponential families can be implemented as a first-order method through mirror descent .", "topics": ["time complexity", "gradient descent"]}
{"title": "heterogeneous strategy particle swarm optimization", "abstract": "pso is a widely recognized optimization algorithm inspired by social swarm . in this brief we present a heterogeneous strategy particle swarm optimization ( hspso ) , in which a proportion of particles adopt a fully informed strategy to enhance the converging speed while the rest are singly informed to maintain the diversity . our extensive numerical experiments show that hspso algorithm is able to obtain satisfactory solutions , outperforming both pso and the fully informed pso . the evolution process is examined from both structural and microscopic points of view . we find that the cooperation between two types of particles can facilitate a good balance between exploration and exploitation , yielding better performance . we demonstrate the applicability of hspso on the filter design problem .", "topics": ["numerical analysis"]}
{"title": "the mean and median criterion for automatic kernel bandwidth selection for support vector data description", "abstract": "support vector data description ( svdd ) is a popular technique for detecting anomalies . the svdd classifier partitions the whole space into an inlier region , which consists of the region near the training data , and an outlier region , which consists of points away from the training data . the computation of the svdd classifier requires a kernel function , and the gaussian kernel is a common choice for the kernel function . the gaussian kernel has a bandwidth parameter , whose value is important for good results . a small bandwidth leads to overfitting , and the resulting svdd classifier overestimates the number of anomalies . a large bandwidth leads to underfitting , and the classifier fails to detect many anomalies . in this paper we present a new automatic , unsupervised method for selecting the gaussian kernel bandwidth . the selected value can be computed quickly , and it is competitive with existing bandwidth selection methods .", "topics": ["kernel ( operating system )", "test set"]}
{"title": "artificial intelligence approaches to ucav autonomy", "abstract": "this paper covers a number of approaches that leverage artificial intelligence algorithms and techniques to aid unmanned combat aerial vehicle ( ucav ) autonomy . an analysis of current approaches to autonomous control is provided followed by an exploration of how these techniques can be extended and enriched with ai techniques including artificial neural networks ( ann ) , ensembling and reinforcement learning ( rl ) to evolve control strategies for ucavs .", "topics": ["reinforcement learning", "neural networks"]}
{"title": "benchmarking denoising algorithms with real photographs", "abstract": "lacking realistic ground truth data , image denoising techniques are traditionally evaluated on images corrupted by synthesized i.i.d . gaussian noise . we aim to obviate this unrealistic setting by developing a methodology for benchmarking denoising techniques on real photographs . we capture pairs of images with different iso values and appropriately adjusted exposure times , where the nearly noise-free low-iso image serves as reference . to derive the ground truth , careful post-processing is needed . we correct spatial misalignment , cope with inaccuracies in the exposure parameters through a linear intensity transform based on a novel heteroscedastic tobit regression model , and remove residual low-frequency bias that stems , e.g . , from minor illumination changes . we then capture a novel benchmark dataset , the darmstadt noise dataset ( dnd ) , with consumer cameras of differing sensor sizes . one interesting finding is that various recent techniques that perform well on synthetic noise are clearly outperformed by bm3d on photographs with real noise . our benchmark delineates realistic evaluation scenarios that deviate strongly from those commonly used in the scientific literature .", "topics": ["value ( ethics )", "synthetic data"]}
{"title": "stream-based online active learning in a contextual multi-armed bandit framework", "abstract": "we study the stream-based online active learning in a contextual multi-armed bandit framework . in this framework , the reward depends on both the arm and the context . in a stream-based active learning setting , obtaining the ground truth of the reward is costly , and the conventional contextual multi-armed bandit algorithm fails to achieve a sublinear regret due to this cost . hence , the algorithm needs to determine whether or not to request the ground truth of the reward at current time slot . in our framework , we consider a stream-based active learning setting in which a query request for the ground truth is sent to the annotator , together with some prior information of the ground truth . depending on the accuracy of the prior information , the query cost varies . our algorithm mainly carries out two operations : the refinement of the context and arm spaces and the selection of actions . in our algorithm , the partitions of the context space and the arm space are maintained for a certain time slots , and then become finer as more information about the rewards accumulates . we use a strategic way to select the arms and to request the ground truth of the reward , aiming to maximize the total reward . we analytically show that the regret is sublinear and in the same order with that of the conventional contextual multi-armed bandit algorithms , where no query cost", "topics": ["regret ( decision theory )", "ground truth"]}
{"title": "the state-of-the-art in web-scale semantic information processing for cloud computing", "abstract": "based on integrated infrastructure of resource sharing and computing in distributed environment , cloud computing involves the provision of dynamically scalable and provides virtualized resources as services over the internet . these applications also bring a large scale heterogeneous and distributed information which pose a great challenge in terms of the semantic ambiguity . it is critical for application services in cloud computing environment to provide users intelligent service and precise information . semantic information processing can help users deal with semantic ambiguity and information overload efficiently through appropriate semantic models and semantic information processing technology . the semantic information processing have been successfully employed in many fields such as the knowledge representation , natural language understanding , intelligent web search , etc . the purpose of this report is to give an overview of existing technologies for semantic information processing in cloud computing environment , to propose a research direction for addressing distributed semantic reasoning and parallel semantic computing by exploiting semantic information newly available in cloud computing environment .", "topics": ["natural language", "scalability"]}
{"title": "multi-evidence filtering and fusion for multi-label classification , object detection and semantic segmentation based on weakly supervised learning", "abstract": "supervised object detection and semantic segmentation require object or even pixel level annotations . when there exist image level labels only , it is challenging for weakly supervised algorithms to achieve accurate predictions . the accuracy achieved by top weakly supervised algorithms is still significantly lower than their fully supervised counterparts . in this paper , we propose a novel weakly supervised curriculum learning pipeline for multi-label object recognition , detection and semantic segmentation . in this pipeline , we first obtain intermediate object localization and pixel labeling results for the training images , and then use such results to train task-specific deep networks in a fully supervised manner . the entire process consists of four stages , including object localization in the training images , filtering and fusing object instances , pixel labeling for the training images , and task-specific network training . to obtain clean object instances in the training images , we propose a novel algorithm for filtering , fusing and classifying object instances collected from multiple solution mechanisms . in this algorithm , we incorporate both metric learning and density-based clustering to filter detected object instances . experiments show that our weakly supervised pipeline achieves state-of-the-art results in multi-label image classification as well as weakly supervised object detection and very competitive results in weakly supervised semantic segmentation on ms-coco , pascal voc 2007 and pascal voc 2012 .", "topics": ["supervised learning", "object detection"]}
{"title": "stochastic optimization algorithms", "abstract": "when looking for a solution , deterministic methods have the enormous advantage that they do find global optima . unfortunately , they are very cpu-intensive , and are useless on untractable np-hard problems that would require thousands of years for cutting-edge computers to explore . in order to get a result , one needs to revert to stochastic algorithms , that sample the search space without exploring it thoroughly . such algorithms can find very good results , without any guarantee that the global optimum has been reached ; but there is often no other choice than using them . this chapter is a short introduction to the main methods used in stochastic optimization .", "topics": ["mathematical optimization"]}
{"title": "sweep distortion removal from thz images via blind demodulation", "abstract": "heavy sweep distortion induced by alignments and inter-reflections of layers of a sample is a major burden in recovering 2d and 3d information in time resolved spectral imaging . this problem can not be addressed by conventional denoising and signal processing techniques as it heavily depends on the physics of the acquisition . here we propose and implement an algorithmic framework based on low-rank matrix recovery and alternating minimization that exploits the forward model for thz acquisition . the method allows recovering the original signal in spite of the presence of temporal-spatial distortions . we address a blind-demodulation problem , where based on several observations of the sample texture modulated by an undesired sweep pattern , the two classes of signals are separated . the performance of the method is examined in both synthetic and experimental data , and the successful reconstructions are demonstrated . the proposed general scheme can be implemented to advance inspection and imaging applications in thz and other time-resolved sensing modalities .", "topics": ["sampling ( signal processing )", "noise reduction"]}
{"title": "incorporating belief function in svm for phoneme recognition", "abstract": "the support vector machine ( svm ) method has been widely used in numerous classification tasks . the main idea of this algorithm is based on the principle of the margin maximization to find an hyperplane which separates the data into two different classes.in this paper , svm is applied to phoneme recognition task . however , in many real-world problems , each phoneme in the data set for recognition problems may differ in the degree of significance due to noise , inaccuracies , or abnormal characteristics ; all those problems can lead to the inaccuracies in the prediction phase . unfortunately , the standard formulation of svm does not take into account all those problems and , in particular , the variation in the speech input . this paper presents a new formulation of svm ( b-svm ) that attributes to each phoneme a confidence degree computed based on its geometric position in the space . then , this degree is used in order to strengthen the class membership of the tested phoneme . hence , we introduce a reformulation of the standard svm that incorporates the degree of belief . experimental performance on timit database shows the effectiveness of the proposed method b-svm on a phoneme recognition problem .", "topics": ["support vector machine"]}
{"title": "distributed statistical machine learning in adversarial settings : byzantine gradient descent", "abstract": "we consider the problem of distributed statistical machine learning in adversarial settings , where some unknown and time-varying subset of working machines may be compromised and behave arbitrarily to prevent an accurate model from being learned . this setting captures the potential adversarial attacks faced by federated learning -- a modern machine learning paradigm that is proposed by google researchers and has been intensively studied for ensuring user privacy . formally , we focus on a distributed system consisting of a parameter server and $ m $ working machines . each working machine keeps $ n/m $ data samples , where $ n $ is the total number of samples . the goal is to collectively learn the underlying true model parameter of dimension $ d $ . in classical batch gradient descent methods , the gradients reported to the server by the working machines are aggregated via simple averaging , which is vulnerable to a single byzantine failure . in this paper , we propose a byzantine gradient descent method based on the geometric median of means of the gradients . we show that our method can tolerate $ q \\le ( m-1 ) /2 $ byzantine failures , and the parameter estimate converges in $ o ( \\log n ) $ rounds with an estimation error of $ \\sqrt { d ( 2q+1 ) /n } $ , hence approaching the optimal error rate $ \\sqrt { d/n } $ in the centralized and failure-free setting . the total computational complexity of our algorithm is of $ o ( ( nd/m ) \\log n ) $ at each working machine and $ o ( md + kd \\log^3 n ) $ at the central server , and the total communication cost is of $ o ( m d \\log n ) $ . we further provide an application of our general results to the linear regression problem . a key challenge arises in the above problem is that byzantine failures create arbitrary and unspecified dependency among the iterations and the aggregated gradients . we prove that the aggregated gradient converges uniformly to the true gradient function .", "topics": ["computational complexity theory", "gradient descent"]}
{"title": "functional distributional semantics", "abstract": "vector space models have become popular in distributional semantics , despite the challenges they face in capturing various semantic phenomena . we propose a novel probabilistic framework which draws on both formal semantics and recent advances in machine learning . in particular , we separate predicates from the entities they refer to , allowing us to perform bayesian inference based on logical forms . we describe an implementation of this framework using a combination of restricted boltzmann machines and feedforward neural networks . finally , we demonstrate the feasibility of this approach by training it on a parsed corpus and evaluating it on established similarity datasets .", "topics": ["entity"]}
{"title": "gradient of probability density functions based contrasts for blind source separation ( bss )", "abstract": "the article derives some novel independence measures and contrast functions for blind source separation ( bss ) application . for the $ k^ { th } $ order differentiable multivariate functions with equal hyper-volumes ( region bounded by hyper-surfaces ) and with a constraint of bounded support for $ k > 1 $ , it proves that equality of any $ k^ { th } $ order derivatives implies equality of the functions . the difference between product of marginal probability density functions ( pdfs ) and joint pdf of a random vector is defined as function difference ( fd ) of a random vector . assuming the pdfs are $ k^ { th } $ order differentiable , the results on generalized functions are applied to the independence condition . this brings new sets of independence measures and bss contrasts based on the $ l^p $ -norm , $ p \\geq 1 $ of - fd , gradient of fd ( gfd ) and hessian of fd ( hfd ) . instead of a conventional two stage indirect estimation method for joint pdf based bss contrast estimation , a single stage direct estimation of the contrasts is desired . the article targets both the efficient estimation of the proposed contrasts and extension of the potential theory for an information field . the potential theory has a concept of reference potential and it is used to derive closed form expression for the relative analysis of potential field . analogous to it , there are introduced concepts of reference information potential ( rip ) and cross reference information potential ( crip ) based on the potential due to kernel functions placed at selected sample points as basis in kernel methods . the quantities are used to derive closed form expressions for information field analysis using least squares . the expressions are used to estimate $ l^2 $ -norm of fd and $ l^2 $ -norm of gfd based contrasts .", "topics": ["kernel ( operating system )", "computation"]}
{"title": "mcmc assisted by belief propagaion", "abstract": "markov chain monte carlo ( mcmc ) and belief propagation ( bp ) are the most popular algorithms for computational inference in graphical models ( gm ) . in principle , mcmc is an exact probabilistic method which , however , often suffers from exponentially slow mixing . in contrast , bp is a deterministic method , which is typically fast , empirically very successful , however in general lacking control of accuracy over loopy graphs . in this paper , we introduce mcmc algorithms correcting the approximation error of bp , i.e . , we provide a way to compensate for bp errors via a consecutive bp-aware mcmc . our framework is based on the loop calculus ( lc ) approach which allows to express the bp error as a sum of weighted generalized loops . although the full series is computationally intractable , it is known that a truncated series , summing up all 2-regular loops , is computable in polynomial-time for planar pair-wise binary gms and it also provides a highly accurate approximation empirically . motivated by this , we first propose a polynomial-time approximation mcmc scheme for the truncated series of general ( non-planar ) pair-wise binary models . our main idea here is to use the worm algorithm , known to provide fast mixing in other ( related ) problems , and then design an appropriate rejection scheme to sample 2-regular loops . furthermore , we also design an efficient rejection-free mcmc scheme for approximating the full series . the main novelty underlying our design is in utilizing the concept of cycle basis , which provides an efficient decomposition of the generalized loops . in essence , the proposed mcmc schemes run on transformed gm built upon the non-trivial bp solution , and our experiments show that this synthesis of bp and mcmc outperforms both direct mcmc and bare bp schemes .", "topics": ["approximation algorithm", "computational complexity theory"]}
{"title": "entropy-sgd : biasing gradient descent into wide valleys", "abstract": "this paper proposes a new optimization algorithm called entropy-sgd for training deep neural networks that is motivated by the local geometry of the energy landscape . local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the hessian with very few positive or negative eigenvalues . we leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape , while avoiding poorly-generalizable solutions located in the sharp valleys . conceptually , our algorithm resembles two nested loops of sgd where we use langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights . we show that the new objective has a smoother energy landscape and show improved generalization over sgd using uniform stability , under certain assumptions . our experiments on convolutional and recurrent networks demonstrate that entropy-sgd compares favorably to state-of-the-art techniques in terms of generalization error and training time .", "topics": ["optimization problem", "recurrent neural network"]}
{"title": "a slice sampler for restricted hierarchical beta process with applications to shared subspace learning", "abstract": "hierarchical beta process has found interesting applications in recent years . in this paper we present a modified hierarchical beta process prior with applications to hierarchical modeling of multiple data sources . the novel use of the prior over a hierarchical factor model allows factors to be shared across different sources . we derive a slice sampler for this model , enabling tractable inference even when the likelihood and the prior over parameters are non-conjugate . this allows the application of the model in much wider contexts without restrictions . we present two different data generative models a linear gaussiangaussian model for real valued data and a linear poisson-gamma model for count data . encouraging transfer learning results are shown for two real world applications text modeling and content based image retrieval .", "topics": ["sampling ( signal processing )"]}
{"title": "analyzer and generator for pali", "abstract": "this work describes a system that performs morphological analysis and generation of pali words . the system works with regular inflectional paradigms and a lexical database . the generator is used to build a collection of inflected and derived words , which in turn is used by the analyzer . generating and storing morphological forms along with the corresponding morphological information allows for efficient and simple look up by the analyzer . indeed , by looking up a word and extracting the attached morphological information , the analyzer does not have to compute this information . as we must , however , assume the lexical database to be incomplete , the system can also work without the dictionary component , using a rule-based approach .", "topics": ["dictionary"]}
{"title": "learning the information divergence", "abstract": "information divergence that measures the difference between two nonnegative matrices or tensors has found its use in a variety of machine learning problems . examples are nonnegative matrix/tensor factorization , stochastic neighbor embedding , topic models , and bayesian network optimization . the success of such a learning task depends heavily on a suitable divergence . a large variety of divergences have been suggested and analyzed , but very few results are available for an objective choice of the optimal divergence for a given task . here we present a framework that facilitates automatic selection of the best divergence among a given family , based on standard maximum likelihood estimation . we first propose an approximated tweedie distribution for the beta-divergence family . selecting the best beta then becomes a machine learning problem solved by maximum likelihood . next , we reformulate alpha-divergence in terms of beta-divergence , which enables automatic selection of alpha by maximum likelihood with reuse of the learning principle for beta-divergence . furthermore , we show the connections between gamma and beta-divergences as well as r\\'enyi and alpha-divergences , such that our automatic selection framework is extended to non-separable divergences . experiments on both synthetic and real-world data demonstrate that our method can quite accurately select information divergence across different learning problems and various divergence families .", "topics": ["synthetic data", "bayesian network"]}
{"title": "cells in multidimensional recurrent neural networks", "abstract": "the transcription of handwritten text on images is one task in machine learning and one solution to solve it is using multi-dimensional recurrent neural networks ( mdrnn ) with connectionist temporal classification ( ctc ) . the rnns can contain special units , the long short-term memory ( lstm ) cells . they are able to learn long term dependencies but they get unstable when the dimension is chosen greater than one . we defined some useful and necessary properties for the one-dimensional lstm cell and extend them in the multi-dimensional case . thereby we introduce several new cells with better stability . we present a method to design cells using the theory of linear shift invariant systems . the new cells are compared to the lstm cell on the ifn/enit and rimes database , where we can improve the recognition rate compared to the lstm cell . so each application where the lstm cells in mdrnns are used could be improved by substituting them by the new developed cells .", "topics": ["recurrent neural network"]}
{"title": "completeness of compositional translation for context-free grammars", "abstract": "a machine translation system is said to be *complete* if all expressions that are correct according to the source-language grammar can be translated into the target language . this paper addresses the completeness issue for compositional machine translation in general , and for compositional machine translation of context-free grammars in particular . conditions that guarantee translation completeness of context-free grammars are presented .", "topics": ["machine translation"]}
{"title": "spatio-temporal co-occurrence characterizations for human action classification", "abstract": "the human action classification task is a widely researched topic and is still an open problem . many state-of-the-arts approaches involve the usage of bag-of-video-words with spatio-temporal local features to construct characterizations for human actions . in order to improve beyond this standard approach , we investigate the usage of co-occurrences between local features . we propose the usage of co-occurrences information to characterize human actions . a trade-off factor is used to define an optimal trade-off between vocabulary size and classification rate . next , a spatio-temporal co-occurrence technique is applied to extract co-occurrence information between labeled local features . novel characterizations for human actions are then constructed . these include a vector quantized correlogram-elements vector , a highly discriminative pca ( principal components analysis ) co-occurrence vector and a haralick texture vector . multi-channel kernel svm ( support vector machine ) is utilized for classification . for evaluation , the well known kth as well as the challenging ucf-sports action datasets are used . we obtained state-of-the-arts classification performance . we also demonstrated that we are able to fully utilize co-occurrence information , and improve the standard bag-of-video-words approach .", "topics": ["support vector machine"]}
{"title": "margins , kernels and non-linear smoothed perceptrons", "abstract": "we focus on the problem of finding a non-linear classification function that lies in a reproducing kernel hilbert space ( rkhs ) both from the primal point of view ( finding a perfect separator when one exists ) and the dual point of view ( giving a certificate of non-existence ) , with special focus on generalizations of two classical schemes - the perceptron ( primal ) and von-neumann ( dual ) algorithms . we cast our problem as one of maximizing the regularized normalized hard-margin ( $ \\rho $ ) in an rkhs and % use the representer theorem to rephrase it in terms of a mahalanobis dot-product/semi-norm associated with the kernel 's ( normalized and signed ) gram matrix . we derive an accelerated smoothed algorithm with a convergence rate of $ \\tfrac { \\sqrt { \\log n } } { \\rho } $ given $ n $ separable points , which is strikingly similar to the classical kernelized perceptron algorithm whose rate is $ \\tfrac1 { \\rho^2 } $ . when no such classifier exists , we prove a version of gordan 's separation theorem for rkhss , and give a reinterpretation of negative margins . this allows us to give guarantees for a primal-dual algorithm that halts in $ \\min\\ { \\tfrac { \\sqrt n } { |\\rho| } , \\tfrac { \\sqrt n } { \\epsilon } \\ } $ iterations with a perfect separator in the rkhs if the primal is feasible or a dual $ \\epsilon $ -certificate of near-infeasibility .", "topics": ["nonlinear system", "iteration"]}
{"title": "iqa : visual question answering in interactive environments", "abstract": "we introduce interactive question answering ( iqa ) , the task of answering questions that require an autonomous agent to interact with a dynamic visual environment . iqa presents the agent with a scene and a question , like : `` are there any apples in the fridge ? '' the agent must navigate around the scene , acquire visual understanding of scene elements , interact with objects ( e.g . open refrigerators ) and plan for a series of actions conditioned on the question . popular reinforcement learning approaches with a single controller perform poorly on iqa owing to the large and diverse state space . we propose the hierarchical interactive memory network ( himn ) consisting of a factorized set of controllers , allowing the system to operate at multiple levels of temporal abstraction , reducing the diversity of the action space available to each controller and enabling an easier training paradigm . we introduce iqadata , a new interactive question answering dataset built upon ai2-thor , a simulated photo-realistic environment of configurable indoor scenes with interactive objects . iqadata has 75,000 questions , each paired with a unique scene configuration . our experiments show that our proposed model outperforms popular single controller based methods on iqadata .", "topics": ["reinforcement learning", "simulation"]}
{"title": "protodash : fast interpretable prototype selection", "abstract": "in this paper we propose an efficient algorithm protodash for selecting prototypical examples from complex datasets . our generalizes the learn to criticize ( l2c ) work by kim et al . ( 2016 ) to not only select prototypes for a given sparsity level $ m $ but also to associate non-negative ( for interpretability ) weights with each of them indicative of the importance of each prototype . this extension provides a single coherent framework under which both prototypes and criticisms can be found . furthermore , our framework works for any symmetric positive definite kernel thus addressing one of the key open questions laid out in kim et al . ( 2016 ) . our additional requirement of learning non-negative weights no longer maintains submodularity of the objective as in the previous work , however , we show that the problem is weakly submodular and derive approximation guarantees for our fast protodash algorithm . we demonstrate the efficacy of our method on diverse domains such as retail , digit recognition ( mnist ) and on publicly available 40 health questionnaires obtained from the center for disease control ( cdc ) website maintained by the us dept . of health . we validate the results quantitatively as well as qualitatively based on expert feedback and recently published scientific studies on public health , thus showcasing the power of our method in providing actionability ( for retail ) , utility ( for mnist ) and insight ( on cdc datasets ) , which presumably are the hallmark of an effective interpretable method .", "topics": ["feature vector", "sparse matrix"]}
{"title": "generalized sparse precision matrix selection for fitting multivariate gaussian random fields to large data sets", "abstract": "we present a new method for estimating multivariate , second-order stationary gaussian random field ( grf ) models based on the sparse precision matrix selection ( sps ) algorithm , proposed by davanloo et al . ( 2015 ) for estimating scalar grf models . theoretical convergence rates for the estimated between-response covariance matrix and for the estimated parameters of the underlying spatial correlation function are established . numerical tests using simulated and real datasets validate our theoretical findings . data segmentation is used to handle large data sets .", "topics": ["simulation", "sparse matrix"]}
{"title": "feature-guided black-box safety testing of deep neural networks", "abstract": "despite the improved accuracy of deep neural networks , the discovery of adversarial examples has raised serious safety concerns . most existing approaches for crafting adversarial examples necessitate some knowledge ( architecture , parameters , etc . ) of the network at hand . in this paper , we focus on image classifiers and propose a feature-guided black-box approach to test the safety of deep neural networks that requires no such knowledge . our algorithm employs object detection techniques such as sift ( scale invariant feature transform ) to extract features from an image . these features are converted into a mutable saliency distribution , where high probability is assigned to pixels that affect the composition of the image with respect to the human visual system . we formulate the crafting of adversarial examples as a two-player turn-based stochastic game , where the first player 's objective is to minimise the distance to an adversarial example by manipulating the features , and the second player can be cooperative , adversarial , or random . we show that , theoretically , the two-player game can con- verge to the optimal strategy , and that the optimal strategy represents a globally minimal adversarial image . for lipschitz networks , we also identify conditions that provide safety guarantees that no adversarial examples exist . using monte carlo tree search we gradually explore the game state space to search for adversarial examples . our experiments show that , despite the black-box setting , manipulations guided by a perception-based saliency distribution are competitive with state-of-the-art methods that rely on white-box saliency matrices or sophisticated optimization procedures . finally , we show how our method can be used to evaluate robustness of neural networks in safety-critical applications such as traffic sign recognition in self-driving cars .", "topics": ["object detection", "neural networks"]}
{"title": "segregating event streams and noise with a markov renewal process model", "abstract": "we describe an inference task in which a set of timestamped event observations must be clustered into an unknown number of temporal sequences with independent and varying rates of observations . various existing approaches to multi-object tracking assume a fixed number of sources and/or a fixed observation rate ; we develop an approach to inferring structure in timestamped data produced by a mixture of an unknown and varying number of similar markov renewal processes , plus independent clutter noise . the inference simultaneously distinguishes signal from noise as well as clustering signal observations into separate source streams . we illustrate the technique via a synthetic experiment as well as an experiment to track a mixture of singing birds .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "novel and automatic parking inventory system based on pattern recognition and directional chain code", "abstract": "the objective of this paper is to design an efficient vehicle license plate recognition system and to implement it for automatic parking inventory system . the system detects the vehicle first and then captures the image of the front view of the vehicle . vehicle license plate is localized and characters are segmented . for finding the place of plate , a novel and real time method is expressed . a new and robust technique based on directional chain code is used for character recognition . the resulting vehicle number is then compared with the available database of all the vehicles so as to come up with information about the vehicle type and to charge entrance cost accordingly . the system is then allowed to open parking barrier for the vehicle and generate entrance cost receipt . the vehicle information ( such as entrance time , date , and cost amount ) is also stored in the database to maintain the record . the hardware and software integrated system is implemented and a working prototype model is developed . under the available database , the average accuracy of locating vehicle license plate obtained 100 % . using 70 % samples of character for training , we tested our scheme on whole samples and obtained 100 % correct recognition rate . further we tested our character recognition stage on persian vehicle data set and we achieved 99 % correct recognition .", "topics": ["database"]}
{"title": "learning the synthesizability of dynamic texture samples", "abstract": "a dynamic texture ( dt ) refers to a sequence of images that exhibit temporal regularities and has many applications in computer vision and graphics . given an exemplar of dynamic texture , it is a dynamic but challenging task to generate new samples with high quality that are perceptually similar to the input exemplar , which is known to be { \\em example-based dynamic texture synthesis ( edts ) } . numerous approaches have been devoted to this problem , in the past decades , but none them are able to tackle all kinds of dynamic textures equally well . in this paper , we investigate the synthesizability of dynamic texture samples : { \\em given a dynamic texture sample , how synthesizable it is by using edts , and which edts method is the most suitable to synthesize it ? } to this end , we propose to learn regression models to connect dynamic texture samples with synthesizability scores , with the help of a compiled dynamic texture dataset annotated in terms of synthesizability . more precisely , we first define the synthesizability of dt samples and characterize them by a set of spatiotemporal features . based on these features and an annotated dynamic texture dataset , we then train regression models to predict the synthesizability scores of texture samples and learn classifiers to select the most suitable edts methods . we further complete the selection , partition and synthesizability prediction of dynamic texture samples in a hierarchical scheme . we finally apply the learned synthesizability to detecting synthesizable regions in videos . the experiments demonstrate that our method can effectively learn and predict the synthesizability of dt samples .", "topics": ["computer vision"]}
{"title": "evolving fuzzy image segmentation with self-configuration", "abstract": "current image segmentation techniques usually require that the user tune several parameters in order to obtain maximum segmentation accuracy , a computationally inefficient approach , especially when a large number of images must be processed sequentially in daily practice . the use of evolving fuzzy systems for designing a method that automatically adjusts parameters to segment medical images according to the quality expectation of expert users has been proposed recently ( evolving fuzzy image segmentation efis ) . however , efis suffers from a few limitations when used in practice mainly due to some fixed parameters . for instance , efis depends on auto-detection of the object of interest for feature calculation , a task that is highly application-dependent . this shortcoming limits the applicability of efis , which was proposed with the ultimate goal of offering a generic but adjustable segmentation scheme . in this paper , a new version of efis is proposed to overcome these limitations . the new efis , called self-configuring efis ( sc-efis ) , uses available training data to self-estimate the parameters that are fixed in efis . as well , the proposed sc-efis relies on a feature selection process that does not require auto-detection of an roi . the proposed sc-efis was evaluated using the same segmentation algorithms and the same dataset as for efis . the results show that sc-efis can provide the same results as efis but with a higher level of automation .", "topics": ["test set", "image segmentation"]}
{"title": "personalized medical treatments using novel reinforcement learning algorithms", "abstract": "in both the fields of computer science and medicine there is very strong interest in developing personalized treatment policies for patients who have variable responses to treatments . in particular , i aim to find an optimal personalized treatment policy which is a non-deterministic function of the patient specific covariate data that maximizes the expected survival time or clinical outcome . i developed an algorithmic framework to solve multistage decision problem with a varying number of stages that are subject to censoring in which the `` rewards '' are expected survival times . in specific , i developed a novel q-learning algorithm that dynamically adjusts for these parameters . furthermore , i found finite upper bounds on the generalized error of the treatment paths constructed by this algorithm . i have also shown that when the optimal q-function is an element of the approximation space , the anticipated survival times for the treatment regime constructed by the algorithm will converge to the optimal treatment path . i demonstrated the performance of the proposed algorithmic framework via simulation studies and through the analysis of chronic depression data and a hypothetical clinical trial . the censored q-learning algorithm i developed is more effective than the state of the art clinical decision support systems and is able to operate in environments when many covariate parameters may be unobtainable or censored .", "topics": ["reinforcement learning", "simulation"]}
{"title": "ontology based pivoted normalization using vector based approach for information retrieval", "abstract": "the proposed methodology is procedural i.e . it follows finite number of steps that extracts relevant documents according to users query . it is based on principles of data mining for analyzing web data . data mining first adapts integration of data to generate warehouse . then , it extracts useful information with the help of algorithm . the task of representing extracted documents is done by using vector based statistical approach that represents each document in set of terms .", "topics": ["data mining", "relevance"]}
{"title": "data clustering using a hybrid of fuzzy c-means and quantum-behaved particle swarm optimization", "abstract": "fuzzy clustering has become a widely used data mining technique and plays an important role in grouping , traversing and selectively using data for user specified applications . the deterministic fuzzy c-means ( fcm ) algorithm may result in suboptimal solutions when applied to multidimensional data in real-world , time-constrained problems . in this paper the quantum-behaved particle swarm optimization ( qpso ) with a fully connected topology is coupled with the fuzzy c-means clustering algorithm and is tested on a suite of datasets from the uci machine learning repository . the global search ability of the qpso algorithm helps in avoiding stagnation in local optima while the soft clustering approach of fcm helps to partition data based on membership probabilities . clustering performance indices such as f-measure , accuracy , quantization error , intercluster and intracluster distances are reported for competitive techniques such as pso k-means , qpso k-means and qpso fcm over all datasets considered . experimental results indicate that qpso fcm provides comparable and in most cases superior results when compared to the others .", "topics": ["cluster analysis", "data mining"]}
{"title": "neural network ensembles : evaluation of aggregation algorithms", "abstract": "ensembles of artificial neural networks show improved generalization capabilities that outperform those of single networks . however , for aggregation to be effective , the individual networks must be as accurate and diverse as possible . an important problem is , then , how to tune the aggregate members in order to have an optimal compromise between these two conflicting conditions . we present here an extensive evaluation of several algorithms for ensemble construction , including new proposals and comparing them with standard methods in the literature . we also discuss a potential problem with sequential aggregation algorithms : the non-frequent but damaging selection through their heuristics of particularly bad ensemble members . we introduce modified algorithms that cope with this problem by allowing individual weighting of aggregate members . our algorithms and their weighted modifications are favorably tested against other methods in the literature , producing a sensible improvement in performance on most of the standard statistical databases used as benchmarks .", "topics": ["heuristic", "database"]}
{"title": "automated analysis and prediction of job interview performance", "abstract": "we present a computational framework for automatically quantifying verbal and nonverbal behaviors in the context of job interviews . the proposed framework is trained by analyzing the videos of 138 interview sessions with 69 internship-seeking undergraduates at the massachusetts institute of technology ( mit ) . our automated analysis includes facial expressions ( e.g . , smiles , head gestures , facial tracking points ) , language ( e.g . , word counts , topic modeling ) , and prosodic information ( e.g . , pitch , intonation , and pauses ) of the interviewees . the ground truth labels are derived by taking a weighted average over the ratings of 9 independent judges . our framework can automatically predict the ratings for interview traits such as excitement , friendliness , and engagement with correlation coefficients of 0.75 or higher , and can quantify the relative importance of prosody , language , and facial expressions . by analyzing the relative feature weights learned by the regression models , our framework recommends to speak more fluently , use less filler words , speak as `` we '' ( vs. `` i '' ) , use more unique words , and smile more . we also find that the students who were rated highly while answering the first interview question were also rated highly overall ( i.e . , first impression matters ) . finally , our mit interview dataset will be made available to other researchers to further validate and expand our findings .", "topics": ["ground truth", "coefficient"]}
{"title": "management of uncertainty in the multi-level monitoring and diagnosis of the time of flight scintillation array", "abstract": "we present a general architecture for the monitoring and diagnosis of large scale sensor-based systems with real time diagnostic constraints . this architecture is multileveled , combining a single monitoring level based on statistical methods with two model based diagnostic levels . at each level , sources of uncertainty are identified , and integrated methodologies for uncertainty management are developed . the general architecture was applied to the monitoring and diagnosis of a specific nuclear physics detector at lawrence berkeley national laboratory that contained approximately 5000 components and produced over 500 channels of output data . the general architecture is scalable , and work is ongoing to apply it to detector systems one and two orders of magnitude more complex .", "topics": ["scalability"]}
{"title": "a hybrid lp-rpg heuristic for modelling numeric resource flows in planning", "abstract": "although the use of metric fluents is fundamental to many practical planning problems , the study of heuristics to support fully automated planners working with these fluents remains relatively unexplored . the most widely used heuristic is the relaxation of metric fluents into interval-valued variables -- - an idea first proposed a decade ago . other heuristics depend on domain encodings that supply additional information about fluents , such as capacity constraints or other resource-related annotations . a particular challenge to these approaches is in handling interactions between metric fluents that represent exchange , such as the transformation of quantities of raw materials into quantities of processed goods , or trading of money for materials . the usual relaxation of metric fluents is often very poor in these situations , since it does not recognise that resources , once spent , are no longer available to be spent again . we present a heuristic for numeric planning problems building on the propositional relaxed planning graph , but using a mathematical program for numeric reasoning . we define a class of producer -- consumer planning problems and demonstrate how the numeric constraints in these can be modelled in a mixed integer program ( mip ) . this mip is then combined with a metric relaxed planning graph ( rpg ) heuristic to produce an integrated hybrid heuristic . the mip tracks resource use more accurately than the usual relaxation , but relaxes the ordering of actions , while the rpg captures the causal propositional aspects of the problem . we discuss how these two components interact to produce a single unified heuristic and go on to explore how further numeric features of planning problems can be integrated into the mip . we show that encoding a limited subset of the propositional problem to augment the mip can yield more accurate guidance , partly by exploiting structure such as propositional landmarks and propositional resources . our results show that the use of this heuristic enhances scalability on problems where numeric resource interaction is key in finding a solution .", "topics": ["feature extraction", "interaction"]}
{"title": "stochastic rank-1 bandits", "abstract": "we propose stochastic rank- $ 1 $ bandits , a class of online learning problems where at each step a learning agent chooses a pair of row and column arms , and receives the product of their values as a reward . the main challenge of the problem is that the individual values of the row and column are unobserved . we assume that these values are stochastic and drawn independently . we propose a computationally-efficient algorithm for solving our problem , which we call rank1elim . we derive a $ o ( ( k + l ) ( 1 / \\delta ) \\log n ) $ upper bound on its $ n $ -step regret , where $ k $ is the number of rows , $ l $ is the number of columns , and $ \\delta $ is the minimum of the row and column gaps ; under the assumption that the mean row and column rewards are bounded away from zero . to the best of our knowledge , we present the first bandit algorithm that finds the maximum entry of a rank- $ 1 $ matrix whose regret is linear in $ k + l $ , $ 1 / \\delta $ , and $ \\log n $ . we also derive a nearly matching lower bound . finally , we evaluate rank1elim empirically on multiple problems . we observe that it leverages the structure of our problems and can learn near-optimal solutions even if our modeling assumptions are mildly violated .", "topics": ["regret ( decision theory )", "value ( ethics )"]}
{"title": "training rnns as fast as cnns", "abstract": "common recurrent neural network architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations . in this work , we propose the simple recurrent unit ( sru ) architecture , a recurrent unit that simplifies the computation and exposes more parallelism . in sru , the majority of computation for each step is independent of the recurrence and can be easily parallelized . sru is as fast as a convolutional layer and 5-10x faster than an optimized lstm implementation . we study srus on a wide range of applications , including classification , question answering , language modeling , translation and speech recognition . our experiments demonstrate the effectiveness of sru and the trade-off it enables between speed and performance . we open source our implementation in pytorch and cntk .", "topics": ["recurrent neural network", "computation"]}
{"title": "exploration of the scalability of locfaults", "abstract": "a model checker can produce a trace of counterexample , for an erroneous program , which is often long and difficult to understand . in general , the part about the loops is the largest among the instructions in this trace . this makes the location of errors in loops critical , to analyze errors in the overall program . in this paper , we explore the scalability capabilities of locfaults , our error localization approach exploiting paths of cfg ( control flow graph ) from a counterexample to calculate the mcds ( minimal correction deviations ) , and mcss ( minimal correction subsets ) from each found mcd . we present the times of our approach on programs with while-loops unfolded b times , and a number of deviated conditions ranging from 0 to n. our preliminary results show that the times of our approach , constraint-based and flow-driven , are better compared to bugassist which is based on sat and transforms the entire program to a boolean formula , and further the information provided by locfaults is more expressive for the user .", "topics": ["scalability"]}
{"title": "simulated autonomous driving on realistic road networks using deep reinforcement learning", "abstract": "using deep reinforcement learning ( drl ) can be a promising approach to handle tasks in the field of ( simulated ) autonomous driving , whereby recent publications only consider learning in unusual driving environments . this paper outlines a developed software , which instead can be used for evaluating drl algorithms based on realistic road networks and therefore in more usual driving environments . furthermore , we identify difficulties when drl algorithms are applied to tasks , in which it is not only important to reach a goal , but also how this goal is reached . we conclude this paper by presenting the results of an application of a new drl algorithm , which can partly solve these problems .", "topics": ["reinforcement learning", "simulation"]}
{"title": "fuzzy modeling of electrical impedance tomography image of the lungs", "abstract": "electrical impedance tomography ( eit ) is a functional imaging method that is being developed for bedside use in critical care medicine . aiming at improving the chest anatomical resolution of eit images we developed a fuzzy model based on eit high temporal resolution and the functional information contained in the pulmonary perfusion and ventilation signals . eit data from an experimental animal model were collected during normal ventilation and apnea while an injection of hypertonic saline was used as a reference . the fuzzy model was elaborated in three parts : a modeling of the heart , a pulmonary map from ventilation images and , a pulmonary map from perfusion images . image segmentation was performed using a threshold method and a ventilation/perfusion map was generated . eit images treated by the fuzzy model were compared with the hypertonic saline injection method and ct-scan images , presenting good results in both qualitative ( the image obtained by the model was very similar to that of the ct-scan ) and quantitative ( the roc curve provided an area equal to 0.93 ) point of view . undoubtedly , these results represent an important step in the eit images area , since they open the possibility of developing eit-based bedside clinical methods , which are not available nowadays . these achievements could serve as the base to develop eit diagnosis system for some life-threatening diseases commonly found in critical care medicine .", "topics": ["image segmentation"]}
{"title": "beyond log-concavity : provable guarantees for sampling multi-modal distributions using simulated tempering langevin monte carlo", "abstract": "a key task in bayesian statistics is sampling from distributions that are only specified up to a partition function ( i.e . , constant of proportionality ) . however , without any assumptions , sampling ( even approximately ) can be # p-hard , and few works have provided `` beyond worst-case '' guarantees for such settings . for log-concave distributions , classical results going back to bakry and \\'emery ( 1985 ) show that natural continuous-time markov chains called langevin diffusions mix in polynomial time . the most salient feature of log-concavity violated in practice is uni-modality : commonly , the distributions we wish to sample from are multi-modal . in the presence of multiple deep and well-separated modes , langevin diffusion suffers from torpid mixing . we address this problem by combining langevin diffusion with simulated tempering . the result is a markov chain that mixes more rapidly by transitioning between different temperatures of the distribution . we analyze this markov chain for the canonical multi-modal distribution : a mixture of gaussians ( of equal variance ) . the algorithm based on our markov chain provably samples from distributions that are close to mixtures of gaussians , given access to the gradient of the log-pdf . for the analysis , we use a spectral decomposition theorem for graphs ( gharan and trevisan , 2014 ) and a markov chain decomposition technique ( madras and randall , 2002 ) .", "topics": ["sampling ( signal processing )", "time complexity"]}
{"title": "switched linear encoding with rectified linear autoencoders", "abstract": "several recent results in machine learning have established formal connections between autoencoders -- -artificial neural network models that attempt to reproduce their inputs -- -and other coding models like sparse coding and k-means . this paper explores in depth an autoencoder model that is constructed using rectified linear activations on its hidden units . our analysis builds on recent results to further unify the world of sparse linear coding models . we provide an intuitive interpretation of the behavior of these coding models and demonstrate this intuition using small , artificial datasets with known distributions .", "topics": ["sparse matrix", "autoencoder"]}
{"title": "associative adversarial networks", "abstract": "we propose a higher-level associative memory for learning adversarial networks . generative adversarial network ( gan ) framework has a discriminator and a generator network . the generator ( g ) maps white noise ( z ) to data samples while the discriminator ( d ) maps data samples to a single scalar . to do so , g learns how to map from high-level representation space to data space , and d learns to do the opposite . we argue that higher-level representation spaces need not necessarily follow a uniform probability distribution . in this work , we use restricted boltzmann machines ( rbms ) as a higher-level associative memory and learn the probability distribution for the high-level features generated by d. the associative memory samples its underlying probability distribution and g learns how to map these samples to data space . the proposed associative adversarial networks ( aans ) are generative models in the higher-levels of the learning , and use adversarial non-stochastic models d and g for learning the mapping between data and higher-level representation spaces . experiments show the potential of the proposed networks .", "topics": ["generative model", "high- and low-level"]}
{"title": "a dual-source approach for 3d pose estimation from a single image", "abstract": "one major challenge for 3d pose estimation from a single rgb image is the acquisition of sufficient training data . in particular , collecting large amounts of training data that contain unconstrained images and are annotated with accurate 3d poses is infeasible . we therefore propose to use two independent training sources . the first source consists of images with annotated 2d poses and the second source consists of accurate 3d motion capture data . to integrate both sources , we propose a dual-source approach that combines 2d pose estimation with efficient and robust 3d pose retrieval . in our experiments , we show that our approach achieves state-of-the-art results and is even competitive when the skeleton structure of the two sources differ substantially .", "topics": ["test set"]}
{"title": "an end-to-end 3d convolutional neural network for action detection and segmentation in videos", "abstract": "in this paper , we propose an end-to-end 3d cnn for action detection and segmentation in videos . the proposed architecture is a unified deep network that is able to recognize and localize action based on 3d convolution features . a video is first divided into equal length clips and next for each clip a set of tube proposals are generated based on 3d cnn features . finally , the tube proposals of different clips are linked together and spatio-temporal action detection is performed using these linked video proposals . this top-down action detection approach explicitly relies on a set of good tube proposals to perform well and training the bounding box regression usually requires a large number of annotated samples . to remedy this , we further extend the 3d cnn to an encoder-decoder structure and formulate the localization problem as action segmentation . the foreground regions ( i.e . action regions ) for each frame are segmented first then the segmented foreground maps are used to generate the bounding boxes . this bottom-up approach effectively avoids tube proposal generation by leveraging the pixel-wise annotations of segmentation . the segmentation framework also can be readily applied to a general problem of video object segmentation . extensive experiments on several video datasets demonstrate the superior performance of our approach for action detection and video object segmentation compared to the state-of-the-arts .", "topics": ["object detection", "computer vision"]}
{"title": "extreme learning machine for land cover classification", "abstract": "this paper explores the potential of extreme learning machine based supervised classification algorithm for land cover classification . in comparison to a backpropagation neural network , which requires setting of several user-defined parameters and may produce local minima , extreme learning machine require setting of one parameter and produce a unique solution . etm+ multispectral data set ( england ) was used to judge the suitability of extreme learning machine for remote sensing classifications . a back propagation neural network was used to compare its performance in term of classification accuracy and computational cost . results suggest that the extreme learning machine perform equally well to back propagation neural network in term of classification accuracy with this data set . the computational cost using extreme learning machine is very small in comparison to back propagation neural network .", "topics": ["supervised learning"]}
{"title": "a theory of local matching : sift and beyond", "abstract": "why has sift been so successful ? why its extension , dsp-sift , can further improve sift ? is there a theory that can explain both ? how can such theory benefit real applications ? can it suggest new algorithms with reduced computational complexity or new descriptors with better accuracy for matching ? we construct a general theory of local descriptors for visual matching . our theory relies on concepts in energy minimization and heat diffusion . we show that sift and dsp-sift approximate the solution the theory suggests . in particular , dsp-sift gives a better approximation to the theoretical solution ; justifying why dsp-sift outperforms sift . using the developed theory , we derive new descriptors that have fewer parameters and are potentially better in handling affine deformations .", "topics": ["approximation algorithm", "computational complexity theory"]}
{"title": "progressive boosting for class imbalance", "abstract": "pattern recognition applications often suffer from skewed data distributions between classes , which may vary during operations w.r.t . the design data . two-class classification systems designed using skewed data tend to recognize the majority class better than the minority class of interest . several data-level techniques have been proposed to alleviate this issue by up-sampling minority samples or under-sampling majority samples . however , some informative samples may be neglected by random under-sampling and adding synthetic positive samples through up-sampling adds to training complexity . in this paper , a new ensemble learning algorithm called progressive boosting ( pboost ) is proposed that progressively inserts uncorrelated groups of samples into a boosting procedure to avoid loss of information while generating a diverse pool of classifiers . base classifiers in this ensemble are generated from one iteration to the next , using subsets from a validation set that grows gradually in size and imbalance . consequently , pboost is more robust to unknown and variable levels of skew in operational data , and has lower computation complexity than boosting ensembles in literature . in pboost , a new loss factor is proposed to avoid bias of performance towards the negative class . using this loss factor , the weight update of samples and classifier contribution in final predictions are set based on the ability to recognize both classes . using the proposed loss factor instead of standard accuracy can avoid biasing performance in any boosting ensemble . the proposed approach was validated and compared using synthetic data , videos from the fia dataset that emulates face re-identification applications , and keel collection of datasets . results show that pboost can outperform state of the art techniques in terms of both accuracy and complexity over different levels of imbalance and overlap between classes .", "topics": ["sampling ( signal processing )", "test set"]}
{"title": "statistical learning of arbitrary computable classifiers", "abstract": "statistical learning theory chiefly studies restricted hypothesis classes , particularly those with finite vapnik-chervonenkis ( vc ) dimension . the fundamental quantity of interest is the sample complexity : the number of samples required to learn to a specified level of accuracy . here we consider learning over the set of all computable labeling functions . since the vc-dimension is infinite and a priori ( uniform ) bounds on the number of samples are impossible , we let the learning algorithm decide when it has seen sufficient samples to have learned . we first show that learning in this setting is indeed possible , and develop a learning algorithm . we then show , however , that bounding sample complexity independently of the distribution is impossible . notably , this impossibility is entirely due to the requirement that the learning algorithm be computable , and not due to the statistical nature of the problem .", "topics": ["computational complexity theory", "reinforcement learning"]}
{"title": "extracting core claims from scientific articles", "abstract": "the number of scientific articles has grown rapidly over the years and there are no signs that this growth will slow down in the near future . because of this , it becomes increasingly difficult to keep up with the latest developments in a scientific field . to address this problem , we present here an approach to help researchers learn about the latest developments and findings by extracting in a normalized form core claims from scientific articles . this normalized representation is a controlled natural language of english sentences called aida , which has been proposed in previous work as a method to formally structure and organize scientific findings and discourse . we show how such aida sentences can be automatically extracted by detecting the core claim of an article , checking for aida compliance , and - if necessary - transforming it into a compliant sentence . while our algorithm is still far from perfect , our results indicate that the different steps are feasible and they support the claim that aida sentences might be a promising approach to improve scientific communication in the future .", "topics": ["natural language"]}
{"title": "dotmark - a benchmark for discrete optimal transport", "abstract": "the wasserstein metric or earth mover 's distance ( emd ) is a useful tool in statistics , machine learning and computer science with many applications to biological or medical imaging , among others . especially in the light of increasingly complex data , the computation of these distances via optimal transport is often the limiting factor . inspired by this challenge , a variety of new approaches to optimal transport has been proposed in recent years and along with these new methods comes the need for a meaningful comparison . in this paper , we introduce a benchmark for discrete optimal transport , called dotmark , which is designed to serve as a neutral collection of problems , where discrete optimal transport methods can be tested , compared to one another , and brought to their limits on large-scale instances . it consists of a variety of grayscale images , in various resolutions and classes , such as several types of randomly generated images , classical test images and real data from microscopy . along with the dotmark we present a survey and a performance test for a cross section of established methods ranging from more traditional algorithms , such as the transportation simplex , to recently developed approaches , such as the shielding neighborhood method , and including also a comparison with commercial solvers .", "topics": ["computation"]}
{"title": "a causal framework for explaining the predictions of black-box sequence-to-sequence models", "abstract": "we interpret the predictions of any black-box structured input-structured output model around a specific input-output pair . our method returns an `` explanation '' consisting of groups of input-output tokens that are causally related . these dependencies are inferred by querying the black-box model with perturbed inputs , generating a graph over tokens from the responses , and solving a partitioning problem to select the most relevant components . we focus the general approach on sequence-to-sequence problems , adopting a variational autoencoder to yield meaningful input perturbations . we test our method across several nlp sequence generation tasks .", "topics": ["calculus of variations", "natural language processing"]}
{"title": "learning to disambiguate by asking discriminative questions", "abstract": "the ability to ask questions is a powerful tool to gather information in order to learn about the world and resolve ambiguities . in this paper , we explore a novel problem of generating discriminative questions to help disambiguate visual instances . our work can be seen as a complement and new extension to the rich research studies on image captioning and question answering . we introduce the first large-scale dataset with over 10,000 carefully annotated images-question tuples to facilitate benchmarking . in particular , each tuple consists of a pair of images and 4.6 discriminative questions ( as positive samples ) and 5.9 non-discriminative questions ( as negative samples ) on average . in addition , we present an effective method for visual discriminative question generation . the method can be trained in a weakly supervised manner without discriminative images-question tuples but just existing visual question answering datasets . promising results are shown against representative baselines through quantitative evaluations and user studies .", "topics": ["baseline ( configuration management )"]}
{"title": "diffusion adaptation over networks", "abstract": "adaptive networks are well-suited to perform decentralized information processing and optimization tasks and to model various types of self-organized and complex behavior encountered in nature . adaptive networks consist of a collection of agents with processing and learning abilities . the agents are linked together through a connection topology , and they cooperate with each other through local interactions to solve distributed optimization , estimation , and inference problems in real-time . the continuous diffusion of information across the network enables agents to adapt their performance in relation to streaming data and network conditions ; it also results in improved adaptation and learning performance relative to non-cooperative agents . this article provides an overview of diffusion strategies for adaptation and learning over networks . the article is divided into several sections : 1 . motivation ; 2 . mean-square-error estimation ; 3 . distributed optimization via diffusion strategies ; 4 . adaptive diffusion strategies ; 5 . performance of steepest-descent diffusion strategies ; 6 . performance of adaptive diffusion strategies ; 7 . comparing the performance of cooperative strategies ; 8 . selecting the combination weights ; 9 . diffusion with noisy information exchanges ; 10 . extensions and further considerations ; appendix a : properties of kronecker products ; appendix b : graph laplacian and network connectivity ; appendix c : stochastic matrices ; appendix d : block maximum norm ; appendix e : comparison with consensus strategies ; references .", "topics": ["interaction", "gradient descent"]}
{"title": "task-guided and path-augmented heterogeneous network embedding for author identification", "abstract": "in this paper , we study the problem of author identification under double-blind review setting , which is to identify potential authors given information of an anonymized paper . different from existing approaches that rely heavily on feature engineering , we propose to use network embedding approach to address the problem , which can automatically represent nodes into lower dimensional feature vectors . however , there are two major limitations in recent studies on network embedding : ( 1 ) they are usually general-purpose embedding methods , which are independent of the specific tasks ; and ( 2 ) most of these approaches can only deal with homogeneous networks , where the heterogeneity of the network is ignored . hence , challenges faced here are two folds : ( 1 ) how to embed the network under the guidance of the author identification task , and ( 2 ) how to select the best type of information due to the heterogeneity of the network . to address the challenges , we propose a task-guided and path-augmented heterogeneous network embedding model . in our model , nodes are first embedded as vectors in latent feature space . embeddings are then shared and jointly trained according to task-specific and network-general objectives . we extend the existing unsupervised network embedding to incorporate meta paths in heterogeneous networks , and select paths according to the specific task . the guidance from author identification task for network embedding is provided both explicitly in joint training and implicitly during meta path selection . our experiments demonstrate that by using path-augmented network embedding with task guidance , our model can obtain significantly better accuracy at identifying the true authors comparing to existing methods .", "topics": ["feature vector", "unsupervised learning"]}
{"title": "hierarchical models for neural population dynamics in the presence of non-stationarity", "abstract": "neural population activity often exhibits rich variability and temporal structure . this variability is thought to arise from single-neuron stochasticity , neural dynamics on short time-scales , as well as from modulations of neural firing properties on long time-scales , often referred to as `` non-stationarity '' . to better understand the nature of co-variability in neural circuits and their impact on cortical information processing , we need statistical models that are able to capture multiple sources of variability on different time-scales . here , we introduce a hierarchical statistical model of neural population activity which models both neural population dynamics as well as inter-trial modulations in firing rates . in addition , we extend the model to allow us to capture non-stationarities in the population dynamics itself ( i.e . , correlations across neurons ) . we develop variational inference methods for learning model parameters , and demonstrate that the method can recover non-stationarities in both average firing rates and correlation structure . applied to neural population recordings from anesthetized macaque primary visual cortex , our models provide a better account of the structure of neural firing than stationary dynamics models .", "topics": ["calculus of variations"]}
{"title": "stabilizing training of generative adversarial networks through regularization", "abstract": "deep generative models based on generative adversarial networks ( gans ) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture , parameter initialization , and selection of hyper-parameters . this fragility is in part due to a dimensional mismatch or non-overlapping support between the model distribution and the data distribution , causing their density ratio and the associated f-divergence to be undefined . we overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable gan training procedure . we demonstrate the effectiveness of this regularizer across several architectures trained on common benchmark image generation tasks . our regularization turns gan models into reliable building blocks for deep learning .", "topics": ["matrix regularization"]}
{"title": "global neural ccg parsing with optimality guarantees", "abstract": "we introduce the first global recursive neural parsing model with optimality guarantees during decoding . to support global features , we give up dynamic programs and instead search directly in the space of all possible subtrees . although this space is exponentially large in the sentence length , we show it is possible to learn an efficient a* parser . we augment existing parsing models , which have informative bounds on the outside score , with a global model that has loose bounds but only needs to model non-local phenomena . the global model is trained with a new objective that encourages the parser to explore a tiny fraction of the search space . the approach is applied to ccg parsing , improving state-of-the-art accuracy by 0.4 f1 . the parser finds the optimal parse for 99.9 % of held-out sentences , exploring on average only 190 subtrees .", "topics": ["parsing"]}
{"title": "circle detection by harmony search optimization", "abstract": "automatic circle detection in digital images has received considerable attention over the last years in computer vision as several efforts have aimed for an optimal circle detector . this paper presents an algorithm for automatic detection of circular shapes that considers the overall process as an optimization problem . the approach is based on the harmony search algorithm ( hsa ) , a derivative free meta-heuristic optimization algorithm inspired by musicians while improvising new harmonies . the algorithm uses the encoding of three points as candidate circles ( harmonies ) over the edge-only image . an objective function evaluates ( harmony quality ) if such candidate circles are actually present in the edge image . guided by the values of this objective function , the set of encoded candidate circles are evolved using the hsa so that they can fit to the actual circles on the edge map of the image ( optimal harmony ) . experimental results from several tests on synthetic and natural images with a varying complexity range have been included to validate the efficiency of the proposed technique regarding accuracy , speed and robustness .", "topics": ["optimization problem", "computer vision"]}
{"title": "fine-grained event learning of human-object interaction with lstm-crf", "abstract": "event learning is one of the most important problems in ai . however , notwithstanding significant research efforts , it is still a very complex task , especially when the events involve the interaction of humans or agents with other objects , as it requires modeling human kinematics and object movements . this study proposes a methodology for learning complex human-object interaction ( hoi ) events , involving the recording , annotation and classification of event interactions . for annotation , we allow multiple interpretations of a motion capture by slicing over its temporal span , for classification , we use long-short term memory ( lstm ) sequential models with conditional randon field ( crf ) for constraints of outputs . using a setup involving captures of human-object interaction as three dimensional inputs , we argue that this approach could be used for event types involving complex spatio-temporal dynamics .", "topics": ["interaction"]}
{"title": "paired comparisons-based interactive differential evolution", "abstract": "we propose interactive differential evolution ( ide ) based on paired comparisons for reducing user fatigue and evaluate its convergence speed in comparison with interactive genetic algorithms ( iga ) and tournament iga . user interface and convergence performance are two big keys for reducing interactive evolutionary computation ( iec ) user fatigue . unlike iga and conventional ide , users of the proposed ide and tournament iga do not need to compare whole individuals each other but compare pairs of individuals , which largely decreases user fatigue . in this paper , we design a pseudo-iec user and evaluate another factor , iec convergence performance , using iec simulators and show that our proposed ide converges significantly faster than iga and tournament iga , i.e . our proposed one is superior to others from both user interface and convergence performance points of view .", "topics": ["simulation", "computation"]}
{"title": "unsupervised k-nearest neighbor regression", "abstract": "in many scientific disciplines structures in high-dimensional data have to be found , e.g . , in stellar spectra , in genome data , or in face recognition tasks . in this work we present a novel approach to non-linear dimensionality reduction . it is based on fitting k-nearest neighbor regression to the unsupervised regression framework for learning of low-dimensional manifolds . similar to related approaches that are mostly based on kernel methods , unsupervised k-nearest neighbor ( unn ) regression optimizes latent variables w.r.t . the data space reconstruction error employing the k-nearest neighbor heuristic . the problem of optimizing latent neighborhoods is difficult to solve , but the unn formulation allows the design of efficient strategies that iteratively embed latent points to fixed neighborhood topologies . unn is well appropriate for sorting of high-dimensional data . the iterative variants are analyzed experimentally .", "topics": ["heuristic"]}
{"title": "mahotas : open source software for scriptable computer vision", "abstract": "mahotas is a computer vision library for python . it contains traditional image processing functionality such as filtering and morphological operations as well as more modern computer vision functions for feature computation , including interest point detection and local descriptors . the interface is in python , a dynamic programming language , which is very appropriate for fast development , but the algorithms are implemented in c++ and are tuned for speed . the library is designed to fit in with the scientific software ecosystem in this language and can leverage the existing infrastructure developed in that language . mahotas is released under a liberal open source license ( mit license ) and is available from ( http : //github.com/luispedro/mahotas ) and from the python package index ( http : //pypi.python.org/pypi/mahotas ) .", "topics": ["image processing", "computer vision"]}
{"title": "a logic of knowing how", "abstract": "in this paper , we propose a single-agent modal logic framework for reasoning about goal-direct `` knowing how '' based on ideas from linguistics , philosophy , modal logic and automated planning . we first define a modal language to express `` i know how to guarantee phi given psi '' with a semantics not based on standard epistemic models but labelled transition systems that represent the agent 's knowledge of his own abilities . a sound and complete proof system is given to capture the valid reasoning patterns about `` knowing how '' where the most important axiom suggests its compositional nature .", "topics": ["interaction"]}
{"title": "language modeling by clustering with word embeddings for text readability assessment", "abstract": "we present a clustering-based language model using word embeddings for text readability prediction . presumably , an euclidean semantic space hypothesis holds true for word embeddings whose training is done by observing word co-occurrences . we argue that clustering with word embeddings in the metric space should yield feature representations in a higher semantic space appropriate for text regression . also , by representing features in terms of histograms , our approach can naturally address documents of varying lengths . an empirical evaluation using the common core standards corpus reveals that the features formed on our clustering-based language model significantly improve the previously known results for the same corpus in readability prediction . we also evaluate the task of sentence matching based on semantic relatedness using the wiki-simplewiki corpus and find that our features lead to superior matching performance .", "topics": ["cluster analysis"]}
{"title": "a deep learning based 6 degree-of-freedom localization method for endoscopic capsule robots", "abstract": "we present a robust deep learning based 6 degrees-of-freedom ( dof ) localization system for endoscopic capsule robots . our system mainly focuses on localization of endoscopic capsule robots inside the gi tract using only visual information captured by a mono camera integrated to the robot . the proposed system is a 23-layer deep convolutional neural network ( cnn ) that is capable to estimate the pose of the robot in real time using a standard cpu . the dataset for the evaluation of the system was recorded inside a surgical human stomach model with realistic surface texture , softness , and surface liquid properties so that the pre-trained cnn architecture can be transferred confidently into a real endoscopic scenario . an average error of 7:1 % and 3:4 % for translation and rotation has been obtained , respectively . the results accomplished from the experiments demonstrate that a cnn pre-trained with raw 2d endoscopic images performs accurately inside the gi tract and is robust to various challenges posed by reflection distortions , lens imperfections , vignetting , noise , motion blur , low resolution , and lack of unique landmarks to track .", "topics": ["robot"]}
{"title": "kernel alignment inspired linear discriminant analysis", "abstract": "kernel alignment measures the degree of similarity between two kernels . in this paper , inspired from kernel alignment , we propose a new linear discriminant analysis ( lda ) formulation , kernel alignment lda ( kalda ) . we first define two kernels , data kernel and class indicator kernel . the problem is to find a subspace to maximize the alignment between subspace-transformed data kernel and class indicator kernel . surprisingly , the kernel alignment induced kalda objective function is very similar to classical lda and can be expressed using between-class and total scatter matrices . this can be extended to multi-label data . we use a stiefel-manifold gradient descent algorithm to solve this problem . we perform experiments on 8 single-label and 6 multi-label data sets . results show that kalda has very good performance on many single-label and multi-label problems .", "topics": ["kernel ( operating system )", "loss function"]}
{"title": "analysis of watson 's strategies for playing jeopardy !", "abstract": "major advances in question answering technology were needed for ibm watson to play jeopardy ! at championship level -- the show requires rapid-fire answers to challenging natural language questions , broad general knowledge , high precision , and accurate confidence estimates . in addition , jeopardy ! features four types of decision making carrying great strategic importance : ( 1 ) daily double wagering ; ( 2 ) final jeopardy wagering ; ( 3 ) selecting the next square when in control of the board ; ( 4 ) deciding whether to attempt to answer , i.e . , `` buzz in . '' using sophisticated strategies for these decisions , that properly account for the game state and future event probabilities , can significantly boost a players overall chances to win , when compared with simple `` rule of thumb '' strategies . this article presents our approach to developing watsons game-playing strategies , comprising development of a faithful simulation model , and then using learning and monte-carlo methods within the simulator to optimize watsons strategic decision-making . after giving a detailed description of each of our game-strategy algorithms , we then focus in particular on validating the accuracy of the simulators predictions , and documenting performance improvements using our methods . quantitative performance benefits are shown with respect to both simple heuristic strategies , and actual human contestant performance in historical episodes . we further extend our analysis of human play to derive a number of valuable and counterintuitive examples illustrating how human contestants may improve their performance on the show .", "topics": ["simulation", "natural language"]}
{"title": "a computational model for situated task learning with interactive instruction", "abstract": "learning novel tasks is a complex cognitive activity requiring the learner to acquire diverse declarative and procedural knowledge . prior act-r models of acquiring task knowledge from instruction focused on learning procedural knowledge from declarative instructions encoded in semantic memory . in this paper , we identify the requirements for designing compu- tational models that learn task knowledge from situated task- oriented interactions with an expert and then describe and evaluate a model of learning from situated interactive instruc- tion that is implemented in the soar cognitive architecture .", "topics": ["interaction"]}
{"title": "deep learning for causal inference", "abstract": "in this paper , we propose deep learning techniques for econometrics , specifically for causal inference and for estimating individual as well as average treatment effects . the contribution of this paper is twofold : 1 . for generalized neighbor matching to estimate individual and average treatment effects , we analyze the use of autoencoders for dimensionality reduction while maintaining the local neighborhood structure among the data points in the embedding space . this deep learning based technique is shown to perform better than simple k nearest neighbor matching for estimating treatment effects , especially when the data points have several features/covariates but reside in a low dimensional manifold in high dimensional space . we also observe better performance than manifold learning methods for neighbor matching . 2 . propensity score matching is one specific and popular way to perform matching in order to estimate average and individual treatment effects . we propose the use of deep neural networks ( dnns ) for propensity score matching , and present a network called propensitynet for this . this is a generalization of the logistic regression technique traditionally used to estimate propensity scores and we show empirically that dnns perform better than logistic regression at propensity score matching . code for both methods will be made available shortly on github at : https : //github.com/vikas84bf", "topics": ["cluster analysis", "value ( ethics )"]}
{"title": "a game-theoretic approach to design secure and resilient distributed support vector machines", "abstract": "distributed support vector machines ( dsvm ) have been developed to solve large-scale classification problems in networked systems with a large number of sensors and control units . however , the systems become more vulnerable as detection and defense are increasingly difficult and expensive . this work aims to develop secure and resilient dsvm algorithms under adversarial environments in which an attacker can manipulate the training data to achieve his objective . we establish a game-theoretic framework to capture the conflicting interests between an adversary and a set of distributed data processing units . the nash equilibrium of the game allows predicting the outcome of learning algorithms in adversarial environments , and enhancing the resilience of the machine learning through dynamic distributed learning algorithms . we prove that the convergence of the distributed algorithm is guaranteed without assumptions on the training data or network topologies . numerical experiments are conducted to corroborate the results . we show that network topology plays an important role in the security of dsvm . networks with fewer nodes and higher average degrees are more secure . moreover , a balanced network is found to be less vulnerable to attacks .", "topics": ["test set", "support vector machine"]}
{"title": "unsupervised low-dimensional vector representations for words , phrases and text that are transparent , scalable , and produce similarity metrics that are complementary to neural embeddings", "abstract": "neural embeddings are a popular set of methods for representing words , phrases or text as a low dimensional vector ( typically 50-500 dimensions ) . however , it is difficult to interpret these dimensions in a meaningful manner , and creating neural embeddings requires extensive training and tuning of multiple parameters and hyperparameters . we present here a simple unsupervised method for representing words , phrases or text as a low dimensional vector , in which the meaning and relative importance of dimensions is transparent to inspection . we have created a near-comprehensive vector representation of words , and selected bigrams , trigrams and abbreviations , using the set of titles and abstracts in pubmed as a corpus . this vector is used to create several novel implicit word-word and text-text similarity metrics . the implicit word-word similarity metrics correlate well with human judgement of word pair similarity and relatedness , and outperform or equal all other reported methods on a variety of biomedical benchmarks , including several implementations of neural embeddings trained on pubmed corpora . our implicit word-word metrics capture different aspects of word-word relatedness than word2vec-based metrics and are only partially correlated ( rho = ~0.5-0.8 depending on task and corpus ) . the vector representations of words , bigrams , trigrams , abbreviations , and pubmed title+abstracts are all publicly available from http : //arrowsmith.psych.uic.edu for release under cc-by-nc license . several public web query interfaces are also available at the same site , including one which allows the user to specify a given word and view its most closely related terms according to direct co-occurrence as well as different implicit similarity metrics .", "topics": ["unsupervised learning", "text corpus"]}
{"title": "revealed preference at scale : learning personalized preferences from assortment choices", "abstract": "we consider the problem of learning the preferences of a heterogeneous population by observing choices from an assortment of products , ads , or other offerings . our observation model takes a form common in assortment planning applications : each arriving customer is offered an assortment consisting of a subset of all possible offerings ; we observe only the assortment and the customer 's single choice . in this paper we propose a mixture choice model with a natural underlying low-dimensional structure , and show how to estimate its parameters . in our model , the preferences of each customer or segment follow a separate parametric choice model , but the underlying structure of these parameters over all the models has low dimension . we show that a nuclear-norm regularized maximum likelihood estimator can learn the preferences of all customers using a number of observations much smaller than the number of item-customer combinations . this result shows the potential for structural assumptions to speed up learning and improve revenues in assortment planning and customization . we provide a specialized factored gradient descent algorithm and study the success of the approach empirically .", "topics": ["gradient descent", "gradient"]}
{"title": "eliciting worker preference for task completion", "abstract": "current crowdsourcing platforms provide little support for worker feedback . workers are sometimes invited to post free text describing their experience and preferences in completing tasks . they can also use forums such as turker nation1 to exchange preferences on tasks and requesters . in fact , crowdsourcing platforms rely heavily on observing workers and inferring their preferences implicitly . in this work , we believe that asking workers to indicate their preferences explicitly improve their experience in task completion and hence , the quality of their contributions . explicit elicitation can indeed help to build more accurate worker models for task completion that captures the evolving nature of worker preferences . we design a worker model whose accuracy is improved iteratively by requesting preferences for task factors such as required skills , task payment , and task relevance . we propose a generic framework , develop efficient solutions in realistic scenarios , and run extensive experiments that show the benefit of explicit preference elicitation over implicit ones with statistical significance .", "topics": ["relevance"]}
{"title": "focal loss for dense object detection", "abstract": "the highest accuracy object detectors to date are based on a two-stage approach popularized by r-cnn , where a classifier is applied to a sparse set of candidate object locations . in contrast , one-stage detectors that are applied over a regular , dense sampling of possible object locations have the potential to be faster and simpler , but have trailed the accuracy of two-stage detectors thus far . in this paper , we investigate why this is the case . we discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause . we propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples . our novel focal loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training . to evaluate the effectiveness of our loss , we design and train a simple dense detector we call retinanet . our results show that when trained with the focal loss , retinanet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors . code is at : https : //github.com/facebookresearch/detectron .", "topics": ["sampling ( signal processing )", "object detection"]}
{"title": "supervised topic models for clinical interpretability", "abstract": "supervised topic models can help clinical researchers find interpretable cooccurence patterns in count data that are relevant for diagnostics . however , standard formulations of supervised latent dirichlet allocation have two problems . first , when documents have many more words than labels , the influence of the labels will be negligible . second , due to conditional independence assumptions in the graphical model the impact of supervised labels on the learned topic-word probabilities is often minimal , leading to poor predictions on heldout data . we investigate penalized optimization methods for training slda that produce interpretable topic-word parameters and useful heldout predictions , using recognition networks to speed-up inference . we report preliminary results on synthetic data and on predicting successful anti-depressant medication given a patient 's diagnostic history .", "topics": ["graphical model", "synthetic data"]}
{"title": "joint extraction of events and entities within a document context", "abstract": "events and entities are closely related ; entities are often actors or participants in events and events without entities are uncommon . the interpretation of events and entities is highly contextually dependent . existing work in information extraction typically models events separately from entities , and performs inference at the sentence level , ignoring the rest of the document . in this paper , we propose a novel approach that models the dependencies among variables of events , entities , and their relations , and performs joint inference of these variables across a document . the goal is to enable access to document-level contextual information and facilitate context-aware predictions . we demonstrate that our approach substantially outperforms the state-of-the-art methods for event extraction as well as a strong baseline for entity extraction .", "topics": ["baseline ( configuration management )", "entity"]}
{"title": "a comprehensive analysis of deep learning based representation for face recognition", "abstract": "deep learning based approaches have been dominating the face recognition field due to the significant performance improvement they have provided on the challenging wild datasets . these approaches have been extensively tested on such unconstrained datasets , on the labeled faces in the wild and youtube faces , to name a few . however , their capability to handle individual appearance variations caused by factors such as head pose , illumination , occlusion , and misalignment has not been thoroughly assessed till now . in this paper , we present a comprehensive study to evaluate the performance of deep learning based face representation under several conditions including the varying head pose angles , upper and lower face occlusion , changing illumination of different strengths , and misalignment due to erroneous facial feature localization . two successful and publicly available deep learning models , namely vgg-face and lightened cnn have been utilized to extract face representations . the obtained results show that although deep learning provides a powerful representation for face recognition , it can still benefit from preprocessing , for example , for pose and illumination normalization in order to achieve better performance under various conditions . particularly , if these variations are not included in the dataset used to train the deep learning model , the role of preprocessing becomes more crucial . experimental results also show that deep learning based representation is robust to misalignment and can tolerate facial feature localization errors up to 10 % of the interocular distance .", "topics": ["causality"]}
{"title": "n-gram and neural language models for discriminating similar languages", "abstract": "this paper describes our submission ( named clac ) to the 2016 discriminating similar languages ( dsl ) shared task . we participated in the closed sub-task 1 ( set a ) with two separate machine learning techniques . the first approach is a character based convolution neural network with a bidirectional long short term memory ( bilstm ) layer ( clstm ) , which achieved an accuracy of 78.45 % with minimal tuning . the second approach is a character-based n-gram model . this last approach achieved an accuracy of 88.45 % which is close to the accuracy of 89.38 % achieved by the best submission , and allowed us to rank # 7 overall .", "topics": ["convolution"]}
{"title": "storage of natural language sentences in a hopfield network", "abstract": "this paper look at how the hopfield neural network can be used to store and recall patterns constructed from natural language sentences . as a pattern recognition and storage tool , the hopfield neural network has received much attention . this attention however has been mainly in the field of statistical physics due to the model 's simple abstraction of spin glass systems . a discussion is made of the differences , shown as bias and correlation , between natural language sentence patterns and the randomly generated ones used in previous experiments . results are given for numerical simulations which show the auto-associative competence of the network when trained with natural language patterns .", "topics": ["numerical analysis", "natural language"]}
{"title": "knowledge embedding and retrieval strategies in an informledge system", "abstract": "informledge system ( ils ) is a knowledge network with autonomous nodes and intelligent links that integrate and structure the pieces of knowledge . in this paper , we put forward the strategies for knowledge embedding and retrieval in an ils . ils is a powerful knowledge network system dealing with logical storage and connectivity of information units to form knowledge using autonomous nodes and multi-lateral links . in ils , the autonomous nodes known as knowledge network nodes ( knn ) s play vital roles which are not only used in storage , parsing and in forming the multi-lateral linkages between knowledge points but also in helping the realization of intelligent retrieval of linked information units in the form of knowledge . knowledge built in to the ils forms the shape of sphere . the intelligence incorporated into the links of a knn helps in retrieving various knowledge threads from a specific set of knns . a developed entity of information realized through knn forms in to the shape of a knowledge cone", "topics": ["parsing", "autonomous car"]}
{"title": "deep generative adversarial networks for compressed sensing automates mri", "abstract": "magnetic resonance image ( mri ) reconstruction is a severely ill-posed linear inverse task demanding time and resource intensive computations that can substantially trade off { \\it accuracy } for { \\it speed } in real-time imaging . in addition , state-of-the-art compressed sensing ( cs ) analytics are not cognizant of the image { \\it diagnostic quality } . to cope with these challenges we put forth a novel cs framework that permeates benefits from generative adversarial networks ( gan ) to train a ( low-dimensional ) manifold of diagnostic-quality mr images from historical patients . leveraging a mixture of least-squares ( ls ) gans and pixel-wise $ \\ell_1 $ cost , a deep residual network with skip connections is trained as the generator that learns to remove the { \\it aliasing } artifacts by projecting onto the manifold . lsgan learns the texture details , while $ \\ell_1 $ controls the high-frequency noise . a multilayer convolutional neural network is then jointly trained based on diagnostic quality images to discriminate the projection quality . the test phase performs feed-forward propagation over the generator network that demands a very low computational overhead . extensive evaluations are performed on a large contrast-enhanced mr dataset of pediatric patients . in particular , images rated based on expert radiologists corroborate that gancs retrieves high contrast images with detailed texture relative to conventional cs , and pixel-wise schemes . in addition , it offers reconstruction under a few milliseconds , two orders of magnitude faster than state-of-the-art cs-mri schemes .", "topics": ["computation", "pixel"]}
{"title": "an automatic machine translation evaluation metric based on dependency parsing model", "abstract": "most of the syntax-based metrics obtain the similarity by comparing the sub-structures extracted from the trees of hypothesis and reference . these sub-structures are defined by human and ca n't express all the information in the trees because of the limited length of sub-structures . in addition , the overlapped parts between these sub-structures are computed repeatedly . to avoid these problems , we propose a novel automatic evaluation metric based on dependency parsing model , with no need to define sub-structures by human . first , we train a dependency parsing model by the reference dependency tree . then we generate the hypothesis dependency tree and the corresponding probability by the dependency parsing model . the quality of the hypothesis can be judged by this probability . in order to obtain the lexicon similarity , we also introduce the unigram f-score to the new metric . experiment results show that the new metric gets the state-of-the-art performance on system level , and is comparable with meteor on sentence level .", "topics": ["machine translation", "parsing"]}
{"title": "interpretable facial relational network using relational importance", "abstract": "human face analysis is an important task in computer vision . according to cognitive-psychological studies , facial dynamics could provide crucial cues for face analysis . in particular , the motion of facial local regions in facial expression is related to the motion of other facial regions . in this paper , a novel deep learning approach which exploits the relations of facial local dynamics has been proposed to estimate facial traits from expression sequence . in order to exploit the relations of facial dynamics in local regions , the proposed network consists of a facial local dynamic feature encoding network and a facial relational network . the facial relational network is designed to be interpretable . relational importance is automatically encoded and facial traits are estimated by combining relational features based on the relational importance . the relations of facial dynamics for facial trait estimation could be interpreted by using the relational importance . by comparative experiments , the effectiveness of the proposed method has been validated . experimental results show that the proposed method outperforms the state-of-the-art methods in gender and age estimation .", "topics": ["computer vision"]}
{"title": "true global optimality of the pressure vessel design problem : a benchmark for bio-inspired optimisation algorithms", "abstract": "the pressure vessel design problem is a well-known design benchmark for validating bio-inspired optimization algorithms . however , its global optimality is not clear and there has been no mathematical proof put forward . in this paper , a detailed mathematical analysis of this problem is provided that proves that 6059.714335048436 is the global minimum . the lagrange multiplier method is also used as an alternative proof and this method is extended to find the global optimum of a cantilever beam design problem .", "topics": ["mathematical optimization"]}
{"title": "generalized end-to-end loss for speaker verification", "abstract": "in this paper , we propose a new loss function called generalized end-to-end ( ge2e ) loss , which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end ( te2e ) loss function . unlike te2e , the ge2e loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process . additionally , the ge2e loss does not require an initial stage of example selection . with these properties , our model with the new loss function decreases speaker verification eer by more than 10 % , while reducing the training time by 60 % at the same time . we also introduce the multireader technique , which allows us to do domain adaptation - training a more accurate model that supports multiple keywords ( i.e . `` ok google '' and `` hey google '' ) as well as multiple dialects .", "topics": ["loss function", "end-to-end principle"]}
{"title": "design of intelligent agents based system for commodity market simulation with jade", "abstract": "a market of potato commodity for industry scale usage is engaging several types of actors . they are farmers , middlemen , and industries . a multi-agent system has been built to simulate these actors into agent entities , based on manually given parameters within a simulation scenario file . each type of agents has its own fuzzy logic representing actual actors ' knowledge , to be used to interpreting values and take appropriated decision of it while on simulation . the system will simulate market activities with programmed behaviors then produce the results as spreadsheet and chart graph files . these results consist of each agent 's yearly finance and commodity data . the system will also predict each of next value from these outputs .", "topics": ["value ( ethics )", "simulation"]}
{"title": "metalearning for feature selection", "abstract": "a general formulation of optimization problems in which various candidate solutions may use different feature-sets is presented , encompassing supervised classification , automated program learning and other cases . a novel characterization of the concept of a `` good quality feature '' for such an optimization problem is provided ; and a proposal regarding the integration of quality based feature selection into metalearning is suggested , wherein the quality of a feature for a problem is estimated using knowledge about related features in the context of related problems . results are presented regarding extensive testing of this `` feature metalearning '' approach on supervised text classification problems ; it is demonstrated that , in this context , feature metalearning can provide significant and sometimes dramatic speedup over standard feature selection heuristics .", "topics": ["supervised learning", "optimization problem"]}
{"title": "pilot study for the cost action `` reassembling the republic of letters '' : language-driven network analysis of letters from the hartlib 's papers", "abstract": "the present report summarizes an exploratory study which we carried out in the context of the cost action is1310 `` reassembling the republic of letters , 1500-1800 '' , and which is relevant to the activities of working group 3 `` texts and topics '' and working group 2 `` people and networks '' . in this study we investigated the use of natural language processing ( nlp ) and network text analysis on a small sample of seventeenth-century letters selected from hartlib papers , whose records are in one of the catalogues of early modern letters online ( emlo ) and whose online edition is available on the website of the humanities research institute at the university of sheffield ( http : //www.hrionline.ac.uk/hartlib/ ) . we outline the nlp pipeline used to automatically process the texts into a network representation , in order to identify the texts ' `` narrative centrality '' , i.e . the most central entities in the texts , and the relations between them .", "topics": ["natural language processing", "numerical analysis"]}
{"title": "theorem proving based on semantics of dna strand graph", "abstract": "because of several technological limitations of traditional silicon based computing , for past few years a paradigm shift , from silicon to carbon , is occurring in computational world . dna computing has been considered to be quite promising in solving computational and reasoning problems by using dna strands . resolution , an important aspect of automated theorem proving and mathematical logic , is a rule of inference which leads to proof by contradiction technique for sentences in propositional logic and first-order logic . this can also be called refutation theorem-proving . in this paper we have shown how the theorem proving with resolution refutation by dna computation can be represented by the semantics of process calculus and strand graph .", "topics": ["computation"]}
{"title": "geometric vlad for large scale image search", "abstract": "we present a novel compact image descriptor for large scale image search . our proposed descriptor - geometric vlad ( gvlad ) is an extension of vlad ( vector of locally aggregated descriptors ) that incorporates weak geometry information into the vlad framework . the proposed geometry cues are derived as a membership function over keypoint angles which contain evident and informative information but yet often discarded . a principled technique for learning the membership function by clustering angles is also presented . further , to address the overhead of iterative codebook training over real-time datasets , a novel codebook adaptation strategy is outlined . finally , we demonstrate the efficacy of proposed gvlad based retrieval framework where we achieve more than 15 % improvement in map over existing benchmarks .", "topics": ["cluster analysis"]}
{"title": "evolutionary optimization for decision making under uncertainty", "abstract": "optimizing decision problems under uncertainty can be done using a variety of solution methods . soft computing and heuristic approaches tend to be powerful for solving such problems . in this overview article , we survey evolutionary optimization techniques to solve stochastic programming problems - both for the single-stage and multi-stage case .", "topics": ["heuristic"]}
{"title": "algorithmic optimisations for iterative deconvolution methods", "abstract": "we investigate possibilities to speed up iterative algorithms for non-blind image deconvolution . we focus on algorithms in which convolution with the point-spread function to be deconvolved is used in each iteration , and aim at accelerating these convolution operations as they are typically the most expensive part of the computation . we follow two approaches : first , for some practically important specific point-spread functions , algorithmically efficient sliding window or list processing techniques can be used . in some constellations this allows faster computation than via the fourier domain . second , as iterations progress , computation of convolutions can be restricted to subsets of pixels . for moderate thinning rates this can be done with almost no impact on the reconstruction quality . both approaches are demonstrated in the context of richardson-lucy deconvolution but are not restricted to this method .", "topics": ["computation", "iteration"]}
{"title": "the generalized reparameterization gradient", "abstract": "the reparameterization gradient has become a widely used method to obtain monte carlo gradients to optimize the variational objective . however , this technique does not easily apply to commonly used distributions such as beta or gamma without further approximations , and most practical applications of the reparameterization gradient fit gaussian distributions . in this paper , we introduce the generalized reparameterization gradient , a method that extends the reparameterization gradient to a wider class of variational distributions . generalized reparameterizations use invertible transformations of the latent variables which lead to transformed distributions that weakly depend on the variational parameters . this results in new monte carlo gradients that combine reparameterization gradients and score function gradients . we demonstrate our approach on variational inference for two complex probabilistic models . the generalized reparameterization is effective : even a single sample from the variational distribution is enough to obtain a low-variance gradient .", "topics": ["calculus of variations", "approximation"]}
{"title": "towards a simulation-based programming paradigm for ai applications", "abstract": "we present initial ideas for a programming paradigm based on simulation that is targeted towards applications of artificial intelligence ( ai ) . the approach aims at integrating techniques from different areas of ai and is based on the idea that simulated entities may freely exchange data and behavioural patterns . we define basic notions of a simulation-based programming paradigm and show how it can be used for implementing ai applications .", "topics": ["simulation", "entity"]}
{"title": "lsh microbatches for stochastic gradients : value in rearrangement", "abstract": "metric embeddings are immensely useful representation of interacting entities such as videos , users , search queries , online resources , words , and more . embeddings are computed by optimizing a loss function of the form of a sum over provided associations so that relation of embedding vectors reflects strength of association . moreover , the resulting embeddings allow us to predict the strength of unobserved associations . typically , the optimization performs stochastic gradient updates on minibatches of associations that are arranged independently at random . we propose and study here the antithesis of { \\em coordinated } arrangements , which we obtain efficiently through { \\em lsh microbatching } , where similar associations are grouped together . coordinated arrangements leverage the similarity of entities evident from their association vectors . we experimentally study the benefit of tunable minibatch arrangements , demonstrating consistent reductions of 3-15\\ % in training . arrangement emerges as a powerful performance knob for sgd that is orthogonal and compatible with other tuning methods , and thus is a candidate for wide deployment .", "topics": ["loss function", "entity"]}
{"title": "quantum-like uncertain conditionals for text analysis", "abstract": "simple representations of documents based on the occurrences of terms are ubiquitous in areas like information retrieval , and also frequent in natural language processing . in this work we propose a logical-probabilistic approach to the analysis of natural language text based in the concept of uncertain conditional , on top of a formulation of lexical measurements inspired in the theoretical concept of ideal quantum measurements . the proposed concept can be used for generating topic-specific representations of text , aiming to match in a simple way the perception of a user with a pre-established idea of what the usage of terms in the text should be . a simple example is developed with two versions of a text in two languages , showing how regularities in the use of terms are detected and easily represented .", "topics": ["natural language processing", "natural language"]}
{"title": "nonlinear tensor product approximation of functions", "abstract": "we are interested in approximation of a multivariate function $ f ( x_1 , \\dots , x_d ) $ by linear combinations of products $ u^1 ( x_1 ) \\cdots u^d ( x_d ) $ of univariate functions $ u^i ( x_i ) $ , $ i=1 , \\dots , d $ . in the case $ d=2 $ it is a classical problem of bilinear approximation . in the case of approximation in the $ l_2 $ space the bilinear approximation problem is closely related to the problem of singular value decomposition ( also called schmidt expansion ) of the corresponding integral operator with the kernel $ f ( x_1 , x_2 ) $ . there are known results on the rate of decay of errors of best bilinear approximation in $ l_p $ under different smoothness assumptions on $ f $ . the problem of multilinear approximation ( nonlinear tensor product approximation ) in the case $ d\\ge 3 $ is more difficult and much less studied than the bilinear approximation problem . we will present results on best multilinear approximation in $ l_p $ under mixed smoothness assumption on $ f $ .", "topics": ["approximation", "nonlinear system"]}
{"title": "acceleration of the pdhgm on strongly convex subspaces", "abstract": "we propose several variants of the primal-dual method due to chambolle and pock . without requiring full strong convexity of the objective functions , our methods are accelerated on subspaces with strong convexity . this yields mixed rates , $ o ( 1/n^2 ) $ with respect to initialisation and $ o ( 1/n ) $ with respect to the dual sequence , and the residual part of the primal sequence . we demonstrate the efficacy of the proposed methods on image processing problems lacking strong convexity , such as total generalised variation denoising and total variation deblurring .", "topics": ["image processing", "noise reduction"]}
{"title": "transition-based dependency parsing with pluggable classifiers", "abstract": "in principle , the design of transition-based dependency parsers makes it possible to experiment with any general-purpose classifier without other changes to the parsing algorithm . in practice , however , it often takes substantial software engineering to bridge between the different representations used by two software packages . here we present extensions to maltparser that allow the drop-in use of any classifier conforming to the interface of the weka machine learning package , a wrapper for the timbl memory-based learner to this interface , and experiments on multilingual dependency parsing with a variety of classifiers . while earlier work had suggested that memory-based learners might be a good choice for low-resource parsing scenarios , we can not support that hypothesis in this work . we observed that support-vector machines give better parsing performance than the memory-based learner , regardless of the size of the training set .", "topics": ["parsing"]}
{"title": "generic global constraints based on mdds", "abstract": "constraint programming ( cp ) has been successfully applied to both constraint satisfaction and constraint optimization problems . a wide variety of specialized global constraints provide critical assistance in achieving a good model that can take advantage of the structure of the problem in the search for a solution . however , a key outstanding issue is the representation of 'ad-hoc ' constraints that do not have an inherent combinatorial nature , and hence are not modeled well using narrowly specialized global constraints . we attempt to address this issue by considering a hybrid of search and compilation . specifically we suggest the use of reduced ordered multi-valued decision diagrams ( romdds ) as the supporting data structure for a generic global constraint . we give an algorithm for maintaining generalized arc consistency ( gac ) on this constraint that amortizes the cost of the gac computation over a root-to-leaf path in the search tree without requiring asymptotically more space than used for the mdd . furthermore we present an approach for incrementally maintaining the reduced property of the mdd during the search , and show how this can be used for providing domain entailment detection . finally we discuss how to apply our approach to other similar data structures such as aomdds and case dags . the technique used can be seen as an extension of the gac algorithm for the regular language constraint on finite length input .", "topics": ["computation"]}
{"title": "a more general robust loss function", "abstract": "we present a two-parameter loss function which can be viewed as a generalization of many popular loss functions used in robust statistics : the cauchy/lorentzian , geman-mcclure , welsch/leclerc , and generalized charbonnier loss functions ( and by transitivity the l2 , l1 , l1-l2 , and pseudo-huber/charbonnier loss functions ) . if this penalty is viewed as a negative log-likelihood , it yields a general probability distribution that includes normal and cauchy distributions as special cases . we describe and visualize this loss and its corresponding distribution , and document several of their useful properties .", "topics": ["loss function", "image processing"]}
{"title": "axiomatizing causal reasoning", "abstract": "causal models defined in terms of a collection of equations , as defined by pearl , are axiomatized here . axiomatizations are provided for three successively more general classes of causal models : ( 1 ) the class of recursive theories ( those without feedback ) , ( 2 ) the class of theories where the solutions to the equations are unique , ( 3 ) arbitrary theories ( where the equations may not have solutions and , if they do , they are not necessarily unique ) . it is shown that to reason about causality in the most general third class , we must extend the language used by galles and pearl . in addition , the complexity of the decision procedures is examined for all the languages and classes of models considered .", "topics": ["causality"]}
{"title": "jointly modeling topics and intents with global order structure", "abstract": "modeling document structure is of great importance for discourse analysis and related applications . the goal of this research is to capture the document intent structure by modeling documents as a mixture of topic words and rhetorical words . while the topics are relatively unchanged through one document , the rhetorical functions of sentences usually change following certain orders in discourse . we propose gmm-lda , a topic modeling based bayesian unsupervised model , to analyze the document intent structure cooperated with order information . our model is flexible that has the ability to combine the annotations and do supervised learning . additionally , entropic regularization can be introduced to model the significant divergence between topics and intents . we perform experiments in both unsupervised and supervised settings , results show the superiority of our model over several state-of-the-art baselines .", "topics": ["baseline ( configuration management )", "supervised learning"]}
{"title": "predicting station-level hourly demands in a large-scale bike-sharing network : a graph convolutional neural network approach", "abstract": "bike sharing is a vital piece in a modern multi-modal transportation system . however , it suffers from the bike unbalancing problem due to fluctuating spatial and temporal demands . accurate bike sharing demand predictions can help operators to make optimal routes and schedules for bike redistributions , and therefore enhance the system efficiency . in this study , we propose a novel graph convolutional neural network with data-driven graph filter ( gcnn-ddgf ) model to predict station-level hourly demands in a large-scale bike-sharing network . with each station as a vertex in the network , the new proposed gcnn-ddgf model is able to automatically learn the hidden correlations between stations , and thus overcomes a common issue reported in the previous studies , i.e . , the quality and performance of gcnn models rely on the predefinition of the adjacency matrix . to show the performance of the proposed model , this study compares the gcnn-ddgf model with four gcnns models , whose adjacency matrices are from different bike sharing system matrices including the spatial distance matrix ( sd ) , the demand matrix ( de ) , the average trip duration matrix ( atd ) and the demand correlation matrix ( dc ) , respectively . the five types of gcnn models and the classic support vector regression model are built on a citi bike dataset from new york city which includes 272 stations and over 28 million transactions from 2013 to 2016 . results show that the gcnn-ddgf model has the lowest root mean square error , followed by the gcnn-dc model , and the gcnn-atd model has the worst performance . through a further examination , we find the learned ddgf captures some similar information embedded in the sd , de and dc matrices , and it also uncovers more hidden heterogeneous pairwise correlations between stations that are not revealed by any of those matrices .", "topics": ["support vector machine"]}
{"title": "a novel architecture for computing approximate radon transform", "abstract": "radon transform is a type of transform which is used in image processing to transfer the image into intercept-slope coordinate . its diagonal properties made it appropriate for some applications which need processes in different degrees . radon transform computation needs a lot of arithmetic operations which makes it a compute-intensive algorithm . in literature an approximate algorithm for computing radon transform is introduces which reduces the complexity of computations . but this algorithm is complex and need arbitrary accesses to memory . in this paper we proposed an algorithm which accesses to memory sequentially . in the following an architecture is introduced which uses pipeline to reduce the time complexity of algorithm .", "topics": ["image processing", "computational complexity theory"]}
{"title": "tensor object classification via multilinear discriminant analysis network", "abstract": "this paper proposes a multilinear discriminant analysis network ( mldanet ) for the recognition of multidimensional objects , known as tensor objects . the mldanet is a variation of linear discriminant analysis network ( ldanet ) and principal component analysis network ( pcanet ) , both of which are the recently proposed deep learning algorithms . the mldanet consists of three parts : 1 ) the encoder learned by mlda from tensor data . 2 ) features maps ob-tained from decoder . 3 ) the use of binary hashing and histogram for feature pooling . a learning algorithm for mldanet is described . evaluations on ucf11 database indicate that the proposed mldanet outperforms the pcanet , ldanet , mpca + lda , and mlda in terms of classification for tensor objects .", "topics": ["map", "encoder"]}
{"title": "deep face deblurring", "abstract": "blind deblurring consists a long studied task , however the outcomes of generic methods are not effective in real world blurred images . domain-specific methods for deblurring targeted object categories , e.g . text or faces , frequently outperform their generic counterparts , hence they are attracting an increasing amount of attention . in this work , we develop such a domain-specific method to tackle deblurring of human faces , henceforth referred to as face deblurring . studying faces is of tremendous significance in computer vision , however face deblurring has yet to demonstrate some convincing results . this can be partly attributed to the combination of i ) poor texture and ii ) highly structure shape that yield the contour/gradient priors ( that are typically used ) sub-optimal . in our work instead of making assumptions over the prior , we adopt a learning approach by inserting weak supervision that exploits the well-documented structure of the face . namely , we utilise a deep network to perform the deblurring and employ a face alignment technique to pre-process each face . we additionally surpass the requirement of the deep network for thousands training samples , by introducing an efficient framework that allows the generation of a large dataset . we utilised this framework to create 2mf2 , a dataset of over two million frames . we conducted experiments with real world blurred facial images and report that our method returns a result close to the sharp natural latent image .", "topics": ["computer vision", "gradient"]}
{"title": "genetic optimization of keywords subset in the classification analysis of texts authorship", "abstract": "the genetic selection of keywords set , the text frequencies of which are considered as attributes in text classification analysis , has been analyzed . the genetic optimization was performed on a set of words , which is the fraction of the frequency dictionary with given frequency limits . the frequency dictionary was formed on the basis of analyzed text array of texts of english fiction . as the fitness function which is minimized by the genetic algorithm , the error of nearest k neighbors classifier was used . the obtained results show high precision and recall of texts classification by authorship categories on the basis of attributes of keywords set which were selected by the genetic algorithm from the frequency dictionary .", "topics": ["mathematical optimization", "dictionary"]}
{"title": "soccer field localization from a single image", "abstract": "in this work , we propose a novel way of efficiently localizing a soccer field from a single broadcast image of the game . related work in this area relies on manually annotating a few key frames and extending the localization to similar images , or installing fixed specialized cameras in the stadium from which the layout of the field can be obtained . in contrast , we formulate this problem as a branch and bound inference in a markov random field where an energy function is defined in terms of field cues such as grass , lines and circles . moreover , our approach is fully automatic and depends only on single images from the broadcast video of the game . we demonstrate the effectiveness of our method by applying it to various games and obtain promising results . finally , we posit that our approach can be applied easily to other sports such as hockey and basketball .", "topics": ["mathematical optimization"]}
{"title": "a hybrid intelligent model for software cost estimation", "abstract": "accurate software development effort estimation is critical to the success of software projects . although many techniques and algorithmic models have been developed and implemented by practitioners , accurate software development effort prediction is still a challenging endeavor in the field of software engineering , especially in handling uncertain and imprecise inputs and collinear characteristics . in this paper , a hybrid in-telligent model combining a neural network model integrated with fuzzy model ( neuro-fuzzy model ) has been used to improve the accuracy of estimating software cost . the performance of the proposed model is assessed by designing and conducting evaluation with published project and industrial data . results have shown that the proposed model demonstrates the ability of improving the estimation accuracy by 18 % based on the mean magnitude of relative error ( mmre ) criterion .", "topics": ["eisenstein 's criterion"]}
{"title": "performance prediction and optimization of solar water heater via a knowledge-based machine learning method", "abstract": "measuring the performance of solar energy and heat transfer systems requires a lot of time , economic cost and manpower . meanwhile , directly predicting their performance is challenging due to the complicated internal structures . fortunately , a knowledge-based machine learning method can provide a promising prediction and optimization strategy for the performance of energy systems . in this chapter , the authors will show how they utilize the machine learning models trained from a large experimental database to perform precise prediction and optimization on a solar water heater ( swh ) system . a new energy system optimization strategy based on a high-throughput screening ( hts ) process is proposed . this chapter consists of : i ) comparative studies on varieties of machine learning models ( artificial neural networks ( anns ) , support vector machine ( svm ) and extreme learning machine ( elm ) ) to predict the performances of swhs ; ii ) development of an ann-based software to assist the quick prediction and iii ) introduction of a computational hts method to design a high-performance swh system .", "topics": ["support vector machine", "mathematical optimization"]}
{"title": "unsupervised cnn for single view depth estimation : geometry to the rescue", "abstract": "a significant weakness of most current deep convolutional neural networks is the need to train them using vast amounts of manu- ally labelled data . in this work we propose a unsupervised framework to learn a deep convolutional neural network for single view depth predic- tion , without requiring a pre-training stage or annotated ground truth depths . we achieve this by training the network in a manner analogous to an autoencoder . at training time we consider a pair of images , source and target , with small , known camera motion between the two such as a stereo pair . we train the convolutional encoder for the task of predicting the depth map for the source image . to do so , we explicitly generate an inverse warp of the target image using the predicted depth and known inter-view displacement , to reconstruct the source image ; the photomet- ric error in the reconstruction is the reconstruction loss for the encoder . the acquisition of this training data is considerably simpler than for equivalent systems , requiring no manual annotation , nor calibration of depth sensor to camera . we show that our network trained on less than half of the kitti dataset ( without any further augmentation ) gives com- parable performance to that of the state of art supervised methods for single view depth estimation .", "topics": ["test set", "supervised learning"]}
{"title": "application of methods for syntax analysis of context-free languages to query evaluation of logic programs", "abstract": "my research goal is to employ a parser generation algorithm based on the earley parsing algorithm to the evaluation and compilation of queries to logic programs , especially to deductive databases . by means of partial deduction , from a query to a logic program a parameterized automaton is to be generated that models the evaluation of this query . this automaton can be compiled to executable code ; thus we expect a speedup in runtime of query evaluation . an extended abstract/ full version of a paper accepted to be presented at the doctoral consortium of the 30th international conference on logic programming ( iclp 2014 ) , july 19-22 , vienna , austria", "topics": ["parsing", "database"]}
{"title": "made : masked autoencoder for distribution estimation", "abstract": "there has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples . we introduce a simple modification for autoencoder neural networks that yields powerful generative models . our method masks the autoencoder 's parameters to respect autoregressive constraints : each input is reconstructed only from previous inputs in a given ordering . constrained this way , the autoencoder outputs can be interpreted as a set of conditional probabilities , and their product , the full joint probability . we can also train a single network that can decompose the joint probability in multiple different orderings . our simple framework can be applied to multiple architectures , including deep ones . vectorized implementations , such as on gpus , are simple and fast . experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators . at test time , the method is significantly faster and scales better than other autoregressive estimators .", "topics": ["generative model", "autoencoder"]}
{"title": "improving variational auto-encoders using householder flow", "abstract": "variational auto-encoders ( vae ) are scalable and powerful generative models . however , the choice of the variational posterior determines tractability and flexibility of the vae . commonly , latent variables are modeled using the normal distribution with a diagonal covariance matrix . this results in computational efficiency but typically it is not flexible enough to match the true posterior distribution . one fashion of enriching the variational posterior distribution is application of normalizing flows , i.e . , a series of invertible transformations to latent variables with a simple posterior . in this paper , we follow this line of thinking and propose a volume-preserving flow that uses a series of householder transformations . we show empirically on mnist dataset and histopathology data that the proposed flow allows to obtain more flexible variational posterior and competitive results comparing to other normalizing flows .", "topics": ["generative model", "calculus of variations"]}
{"title": "efficient convolutional auto-encoding via random convexification and frequency-domain minimization", "abstract": "the omnipresence of deep learning architectures such as deep convolutional neural networks ( cnn ) s is fueled by the synergistic combination of ever-increasing labeled datasets and specialized hardware . despite the indisputable success , the reliance on huge amounts of labeled data and specialized hardware can be a limiting factor when approaching new applications . to help alleviating these limitations , we propose an efficient learning strategy for layer-wise unsupervised training of deep cnns on conventional hardware in acceptable time . our proposed strategy consists of randomly convexifying the reconstruction contractive auto-encoding ( rcae ) learning objective and solving the resulting large-scale convex minimization problem in the frequency domain via coordinate descent ( cd ) . the main advantages of our proposed learning strategy are : ( 1 ) single tunable optimization parameter ; ( 2 ) fast and guaranteed convergence ; ( 3 ) possibilities for full parallelization . numerical experiments show that our proposed learning strategy scales ( in the worst case ) linearly with image size , number of filters and filter size .", "topics": ["unsupervised learning"]}
{"title": "warped mixtures for nonparametric cluster shapes", "abstract": "a mixture of gaussians fit to a single curved or heavy-tailed cluster will report that the data contains many clusters . to produce more appropriate clusterings , we introduce a model which warps a latent mixture of gaussians to produce nonparametric cluster shapes . the possibly low-dimensional latent mixture model allows us to summarize the properties of the high-dimensional clusters ( or density manifolds ) describing the data . the number of manifolds , as well as the shape and dimension of each manifold is automatically inferred . we derive a simple inference scheme for this model which analytically integrates out both the mixture parameters and the warping function . we show that our model is effective for density estimation , performs better than infinite gaussian mixture models at recovering the true number of clusters , and produces interpretable summaries of high-dimensional datasets .", "topics": ["cluster analysis"]}
{"title": "dependency parsing with dynamic bayesian network", "abstract": "exact parsing with finite state automata is deemed inappropriate because of the unbounded non-locality languages overwhelmingly exhibit . we propose a way to structure the parsing task in order to make it amenable to local classification methods . this allows us to build a dynamic bayesian network which uncovers the syntactic dependency structure of english sentences . experiments with the wall street journal demonstrate that the model successfully learns from labeled data .", "topics": ["parsing", "bayesian network"]}
{"title": "combating corrupt messages in sparse clustered associative memories", "abstract": "in this paper we analyze and extend the neural network based associative memory proposed by gripon and berrou . this associative memory resembles the celebrated willshaw model with an added partite cluster structure . in the literature , two retrieving schemes have been proposed for the network dynamics , namely sum-of-sum and sum-of-max . they both offer considerably better performance than willshaw and hopfield networks , when comparable retrieval scenarios are considered . former discussions and experiments concentrate on the erasure scenario , where a partial message is used as a probe to the network , in the hope of retrieving the full message . in this regard , sum-of-max outperforms sum-of-sum in terms of retrieval rate by a large margin . however , we observe that when noise and errors are present and the network is queried by a corrupt probe , sum-of-max faces a severe limitation as its stringent activation rule prevents a neuron from reviving back into play once deactivated . in this manuscript , we categorize and analyze different error scenarios so that both the erasure and the corrupt scenarios can be treated consistently . we make an amendment to the network structure to improve the retrieval rate , at the cost of an extra scalar per neuron . afterwards , five different approaches are proposed to deal with corrupt probes . as a result , we extend the network capability , and also increase the robustness of the retrieving procedure . we then experimentally compare all these proposals and discuss pros and cons of each approach under different types of errors . simulation results show that if carefully designed , the network is able to preserve both a high retrieval rate and a low running time simultaneously , even when queried by a corrupt probe .", "topics": ["time complexity", "simulation"]}
{"title": "sensor-type classification in buildings", "abstract": "many sensors/meters are deployed in commercial buildings to monitor and optimize their performance . however , because sensor metadata is inconsistent across buildings , software-based solutions are tightly coupled to the sensor metadata conventions ( i.e . schemas and naming ) for each building . running the same software across buildings requires significant integration effort . metadata normalization is critical for scaling the deployment process and allows us to decouple building-specific conventions from the code written for building applications . it also allows us to deal with missing metadata . one important aspect of normalization is to differentiate sensors by the typeof phenomena being observed . in this paper , we propose a general , simple , yet effective classification scheme to differentiate sensors in buildings by type . we perform ensemble learning on data collected from over 2000 sensor streams in two buildings . our approach is able to achieve more than 92 % accuracy for classification within buildings and more than 82 % accuracy for across buildings . we also introduce a method for identifying potential misclassified streams . this is important because it allows us to identify opportunities to attain more input from experts -- input that could help improve classification accuracy when ground truth is unavailable . we show that by adjusting a threshold value we are able to identify at least 30 % of the misclassified instances .", "topics": ["scalability", "ground truth"]}
{"title": "comparing fixed and adaptive computation time for recurrent neural networks", "abstract": "adaptive computation time for recurrent neural networks ( act ) is one of the most promising architectures for variable computation . act adapts to the input sequence by being able to look at each sample more than once , and learn how many times it should do it . in this paper , we compare act to repeat-rnn , a novel architecture based on repeating each sample a fixed number of times . we found surprising results , where repeat-rnn performs as good as act in the selected tasks . source code in tensorflow and pytorch is publicly available at https : //imatge-upc.github.io/danifojo-2018-repeatrnn/", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "fearnet : brain-inspired model for incremental learning", "abstract": "incremental class learning involves sequentially learning classes in bursts of examples from the same class . this violates the assumptions that underlie methods for training standard deep neural networks , and will cause them to suffer from catastrophic forgetting . arguably , the best method for incremental class learning is icarl , but it requires storing training examples for each class , making it challenging to scale . here , we propose fearnet for incremental class learning . fearnet is a generative model that does not store previous examples , making it memory efficient . fearnet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex . memory consolidation is inspired by mechanisms that occur during sleep . fearnet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall . fearnet achieves state-of-the-art performance at incremental class learning on image ( cifar-100 , cub-200 ) and audio classification ( audioset ) benchmarks .", "topics": ["generative model", "reinforcement learning"]}
{"title": "discrepancy-based algorithms for non-stationary rested bandits", "abstract": "we study the multi-armed bandit problem where the rewards are realizations of general non-stationary stochastic processes , a setting that generalizes many existing lines of work and analyses . in particular , we present a theoretical analysis and derive regret guarantees for rested bandits in which the reward distribution of each arm changes only when we pull that arm . remarkably , our regret bounds are logarithmic in the number of rounds under several natural conditions . we introduce a new algorithm based on classical ucb ideas combined with the notion of weighted discrepancy , a useful tool for measuring the non-stationarity of a stochastic process . we show that the notion of discrepancy can be used to design very general algorithms and a unified framework for the analysis of multi-armed rested bandit problems with non-stationary rewards . in particular , we show that we can recover the regret guarantees of many specific instances of bandit problems with non-stationary rewards that have been studied in the literature . we also provide experiments demonstrating that our algorithms can enjoy a significant improvement in practice compared to standard benchmarks .", "topics": ["regret ( decision theory )"]}
{"title": "reconstructing pompeian households", "abstract": "a database of objects discovered in houses in the roman city of pompeii provides a unique view of ordinary life in an ancient city . experts have used this collection to study the structure of roman households , exploring the distribution and variability of tasks in architectural spaces , but such approaches are necessarily affected by modern cultural assumptions . in this study we present a data-driven approach to household archeology , treating it as an unsupervised labeling problem . this approach scales to large data sets and provides a more objective complement to human interpretation .", "topics": ["unsupervised learning"]}
{"title": "large-margin knn classification using a deep encoder network", "abstract": "knn is one of the most popular classification methods , but it often fails to work well with inappropriate choice of distance metric or due to the presence of numerous class-irrelevant features . linear feature transformation methods have been widely applied to extract class-relevant information to improve knn classification , which is very limited in many applications . kernels have been used to learn powerful non-linear feature transformations , but these methods fail to scale to large datasets . in this paper , we present a scalable non-linear feature mapping method based on a deep neural network pretrained with restricted boltzmann machines for improving knn classification in a large-margin framework , which we call dnet-knn . dnet-knn can be used for both classification and for supervised dimensionality reduction . the experimental results on two benchmark handwritten digit datasets show that dnet-knn has much better performance than large-margin knn using a linear mapping and knn based on a deep autoencoder pretrained with retricted boltzmann machines .", "topics": ["nonlinear system", "scalability"]}
{"title": "decoupling learning rules from representations", "abstract": "in the artificial intelligence field , learning often corresponds to changing the parameters of a parameterized function . a learning rule is an algorithm or mathematical expression that specifies precisely how the parameters should be changed . when creating an artificial intelligence system , we must make two decisions : what representation should be used ( i.e . , what parameterized function should be used ) and what learning rule should be used to search through the resulting set of representable functions . using most learning rules , these two decisions are coupled in a subtle ( and often unintentional ) way . that is , using the same learning rule with two different representations that can represent the same sets of functions can result in two different outcomes . after arguing that this coupling is undesirable , particularly when using artificial neural networks , we present a method for partially decoupling these two decisions for a broad class of learning rules that span unsupervised learning , reinforcement learning , and supervised learning .", "topics": ["supervised learning", "reinforcement learning"]}
{"title": "learnable pooling with context gating for video classification", "abstract": "current methods for video analysis often extract frame-level features using pre-trained convolutional neural networks ( cnns ) . such features are then aggregated over time e.g . , by simple temporal averaging or more sophisticated recurrent neural networks such as long short-term memory ( lstm ) or gated recurrent units ( gru ) . in this work we revise existing video representations and study alternative methods for temporal aggregation . we first explore clustering-based aggregation layers and propose a two-stream architecture aggregating audio and visual features . we then introduce a learnable non-linear unit , named context gating , aiming to model interdependencies among network activations . our experimental results show the advantage of both improvements for the task of video classification . in particular , we evaluate our method on the large-scale multi-modal youtube-8m v2 dataset and outperform all other methods in the youtube 8m large-scale video understanding challenge .", "topics": ["nonlinear system"]}
{"title": "learning hidden structures with relational models by adequately involving rich information in a network", "abstract": "effectively modelling hidden structures in a network is very practical but theoretically challenging . existing relational models only involve very limited information , namely the binary directional link data , embedded in a network to learn hidden networking structures . there is other rich and meaningful information ( e.g . , various attributes of entities and more granular information than binary elements such as `` like '' or `` dislike '' ) missed , which play a critical role in forming and understanding relations in a network . in this work , we propose an informative relational model ( infrm ) framework to adequately involve rich information and its granularity in a network , including metadata information about each entity and various forms of link data . firstly , an effective metadata information incorporation method is employed on the prior information from relational models mmsb and lfrm . this is to encourage the entities with similar metadata information to have similar hidden structures . secondly , we propose various solutions to cater for alternative forms of link data . substantial efforts have been made towards modelling appropriateness and efficiency , for example , using conjugate priors . we evaluate our framework and its inference algorithms in different datasets , which shows the generality and effectiveness of our models in capturing implicit structures in networks .", "topics": ["entity"]}
{"title": "application of a hybrid bi-lstm-crf model to the task of russian named entity recognition", "abstract": "named entity recognition ( ner ) is one of the most common tasks of the natural language processing . the purpose of ner is to find and classify tokens in text documents into predefined categories called tags , such as person names , quantity expressions , percentage expressions , names of locations , organizations , as well as expression of time , currency and others . although there is a number of approaches have been proposed for this task in russian language , it still has a substantial potential for the better solutions . in this work , we studied several deep neural network models starting from vanilla bi-directional long short-term memory ( bi-lstm ) then supplementing it with conditional random fields ( crf ) as well as highway networks and finally adding external word embeddings . all models were evaluated across three datasets : gareev 's dataset , person-1000 , factrueval-2016 . we found that extension of bi-lstm model with crf significantly increased the quality of predictions . encoding input tokens with external word embeddings reduced training time and allowed to achieve state of the art for the russian ner task .", "topics": ["natural language processing"]}
{"title": "on tsallis entropy bias and generalized maximum entropy models", "abstract": "in density estimation task , maximum entropy model ( maxent ) can effectively use reliable prior information via certain constraints , i.e . , linear constraints without empirical parameters . however , reliable prior information is often insufficient , and the selection of uncertain constraints becomes necessary but poses considerable implementation complexity . improper setting of uncertain constraints can result in overfitting or underfitting . to solve this problem , a generalization of maxent , under tsallis entropy framework , is proposed . the proposed method introduces a convex quadratic constraint for the correction of ( expected ) tsallis entropy bias ( teb ) . specifically , we demonstrate that the expected tsallis entropy of sampling distributions is smaller than the tsallis entropy of the underlying real distribution . this expected entropy reduction is exactly the ( expected ) teb , which can be expressed by a closed-form formula and act as a consistent and unbiased correction . teb indicates that the entropy of a specific sampling distribution should be increased accordingly . this entails a quantitative re-interpretation of the maxent principle . by compensating teb and meanwhile forcing the resulting distribution to be close to the sampling distribution , our generalized tebc maxent can be expected to alleviate the overfitting and underfitting . we also present a connection between teb and lidstone estimator . as a result , teb-lidstone estimator is developed by analytically identifying the rate of probability correction in lidstone . extensive empirical evaluation shows promising performance of both tebc maxent and teb-lidstone in comparison with various state-of-the-art density estimation methods .", "topics": ["sampling ( signal processing )"]}
{"title": "learning graph convolution filters from data manifold", "abstract": "convolution neural network ( cnn ) has gained tremendous success in computer vision tasks with its outstanding ability to capture the local latent features . recently , there has been an increasing interest in extending cnns to the general spatial domain . although various types of graph and geometric convolution methods have been proposed , their connections to traditional 2d-convolution are not well-understood . in this paper , we show that depthwise separable convolution is the key to close the gap , based on which we derive a novel depthwise separable graph convolution that subsumes existing graph convolution methods as special cases of our formulation . experiments show that the proposed approach consistently outperforms other graph and geometric convolution baselines on benchmark datasets in multiple domains .", "topics": ["computer vision", "convolution"]}
{"title": "how to escape saddle points efficiently", "abstract": "this paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically on dimension ( i.e . , it is almost `` dimension-free '' ) . the convergence rate of this procedure matches the well-known convergence rate of gradient descent to first-order stationary points , up to log factors . when all saddle points are non-degenerate , all second-order stationary points are local minima , and our result thus shows that perturbed gradient descent can escape saddle points almost for free . our results can be directly applied to many machine learning applications , including deep learning . as a particular concrete example of such an application , we show that our results can be used directly to establish sharp global convergence rates for matrix factorization . our results rely on a novel characterization of the geometry around saddle points , which may be of independent interest to the non-convex optimization community .", "topics": ["gradient descent", "gradient"]}
{"title": "sgan : an alternative training of generative adversarial networks", "abstract": "the generative adversarial networks ( gans ) have demonstrated impressive performance for data synthesis , and are now used in a wide range of computer vision tasks . in spite of this success , they gained a reputation for being difficult to train , what results in a time-consuming and human-involved development process to use them . we consider an alternative training process , named sgan , in which several adversarial `` local '' pairs of networks are trained independently so that a `` global '' supervising pair of networks can be trained against them . the goal is to train the global pair with the corresponding ensemble opponent for improved performances in terms of mode coverage . this approach aims at increasing the chances that learning will not stop for the global pair , preventing both to be trapped in an unsatisfactory local minimum , or to face oscillations often observed in practice . to guarantee the latter , the global pair never affects the local ones . the rules of sgan training are thus as follows : the global generator and discriminator are trained using the local discriminators and generators , respectively , whereas the local networks are trained with their fixed local opponent . experimental results on both toy and real-world problems demonstrate that this approach outperforms standard training in terms of better mitigating mode collapse , stability while converging and that it surprisingly , increases the convergence speed as well .", "topics": ["computer vision"]}
{"title": "the optimal reward baseline for gradient-based reinforcement learning", "abstract": "there exist a number of reinforcement learning algorithms which learnby climbing the gradient of expected reward . their long-runconvergence has been proved , even in partially observableenvironments with non-deterministic actions , and without the need fora system model . however , the variance of the gradient estimator hasbeen found to be a significant practical problem . recent approacheshave discounted future rewards , introducing a bias-variance trade-offinto the gradient estimate . we incorporate a reward baseline into thelearning system , and show that it affects variance without introducingfurther bias . in particular , as we approach the zero-bias , high-variance parameterization , the optimal ( or variance minimizing ) constant reward baseline is equal to the long-term average expectedreward . modified policy-gradient algorithms are presented , and anumber of experiments demonstrate their improvement over previous work .", "topics": ["baseline ( configuration management )", "reinforcement learning"]}
{"title": "sentiment analysis of review datasets using naive bayes and k-nn classifier", "abstract": "the advent of web 2.0 has led to an increase in the amount of sentimental content available in the web . such content is often found in social media web sites in the form of movie or product reviews , user comments , testimonials , messages in discussion forums etc . timely discovery of the sentimental or opinionated web content has a number of advantages , the most important of all being monetization . understanding of the sentiments of human masses towards different entities and products enables better services for contextual advertisements , recommendation systems and analysis of market trends . the focus of our project is sentiment focussed web crawling framework to facilitate the quick discovery of sentimental contents of movie reviews and hotel reviews and analysis of the same . we use statistical methods to capture elements of subjective style and the sentence polarity . the paper elaborately discusses two supervised machine learning algorithms : k-nearest neighbour ( k-nn ) and naive bayes and compares their overall accuracy , precisions as well as recall values . it was seen that in case of movie reviews naive bayes gave far better results than k-nn but for hotel reviews these algorithms gave lesser , almost same accuracies .", "topics": ["supervised learning", "entity"]}
{"title": "building emotional machines : recognizing image emotions through deep neural networks", "abstract": "an image is a very effective tool for conveying emotions . many researchers have investigated in computing the image emotions by using various features extracted from images . in this paper , we focus on two high level features , the object and the background , and assume that the semantic information of images is a good cue for predicting emotion . an object is one of the most important elements that define an image , and we find out through experiments that there is a high correlation between the object and the emotion in images . even with the same object , there may be slight difference in emotion due to different backgrounds , and we use the semantic information of the background to improve the prediction performance . by combining the different levels of features , we build an emotion based feed forward deep neural network which produces the emotion values of a given image . the output emotion values in our framework are continuous values in the 2-dimensional space ( valence and arousal ) , which are more effective than using a few number of emotion categories in describing emotions . experiments confirm the effectiveness of our network in predicting the emotion of images .", "topics": ["neural networks"]}
{"title": "what 's in a patch , ii : visualizing generic surfaces", "abstract": "we continue the development of a linear algebraic framework for the shape-from-shading problem , exploiting the manner in which tensors arise when scalar ( e.g . image ) and vector ( e.g . surface normal ) fields are differentiated multiple times . in this paper we apply that framework to develop taylor expansions of the normal field and build a boot-strapping algorithm to find these polynomial surface solutions ( under any light source ) consistent with a given patch to arbitrary order . a generic constraint on the image derivatives restricts these solutions to a 2-d subspace , plus an unknown rotation matrix . the parameters for the subspace and rotation matrix encapsulate the ambiguity in the shading problem .", "topics": ["polynomial"]}
{"title": "korean to english translation using synchronous tags", "abstract": "it is often argued that accurate machine translation requires reference to contextual knowledge for the correct treatment of linguistic phenomena such as dropped arguments and accurate lexical selection . one of the historical arguments in favor of the interlingua approach has been that , since it revolves around a deep semantic representation , it is better able to handle the types of linguistic phenomena that are seen as requiring a knowledge-based approach . in this paper we present an alternative approach , exemplified by a prototype system for machine translation of english and korean which is implemented in synchronous tags . this approach is essentially transfer based , and uses semantic feature unification for accurate lexical selection of polysemous verbs . the same semantic features , when combined with a discourse model which stores previously mentioned entities , can also be used for the recovery of topicalized arguments . in this paper we concentrate on the translation of korean to english .", "topics": ["machine translation", "entity"]}
{"title": "phrase-based image captioning with hierarchical lstm model", "abstract": "automatic generation of caption to describe the content of an image has been gaining a lot of research interests recently , where most of the existing works treat the image caption as pure sequential data . natural language , however possess a temporal hierarchy structure , with complex dependencies between each subsequence . in this paper , we propose a phrase-based hierarchical long short-term memory ( phi-lstm ) model to generate image description . in contrast to the conventional solutions that generate caption in a pure sequential manner , our proposed model decodes image caption from phrase to sentence . it consists of a phrase decoder at the bottom hierarchy to decode noun phrases of variable length , and an abbreviated sentence decoder at the upper hierarchy to decode an abbreviated form of the image description . a complete image caption is formed by combining the generated phrases with sentence during the inference stage . empirically , our proposed model shows a better or competitive result on the flickr8k , flickr30k and ms-coco datasets in comparison to the state-of-the art models . we also show that our proposed model is able to generate more novel captions ( not seen in the training data ) which are richer in word contents in all these three datasets .", "topics": ["test set", "natural language"]}
{"title": "modeling browser-based distributed evolutionary computation systems", "abstract": "from the era of big science we are back to the `` do it yourself '' , where you do not have any money to buy clusters or subscribe to grids but still have algorithms that crave many computing nodes and need them to measure scalability . fortunately , this coincides with the era of big data , cloud computing , and browsers that include javascript virtual machines . those are the reasons why this paper will focus on two different aspects of volunteer or freeriding computing : first , the pragmatic : where to find those resources , which ones can be used , what kind of support you have to give them ; and then , the theoretical : how evolutionary algorithms can be adapted to an environment in which nodes come and go , have different computing capabilities and operate in complete asynchrony of each other . we will examine the setup needed to create a very simple distributed evolutionary algorithm using javascript and then find a model of how users react to it by collecting data from several experiments featuring different classical benchmark functions .", "topics": ["computation", "scalability"]}
{"title": "a novel scheme for generating secure face templates using bda", "abstract": "in identity management system , frequently used biometric recognition system needs awareness towards issue of protecting biometric template as far as more reliable solution is apprehensive . in sight of this biometric template protection algorithm should gratify the basic requirements viz . security , discriminability and cancelability . as no single template protection method is capable of satisfying these requirements , a novel scheme for face template generation and protection is proposed . the novel scheme is proposed to provide security and accuracy in new user enrolment and authentication process . this novel scheme takes advantage of both the hybrid approach and the binary discriminant analysis algorithm . this algorithm is designed on the basis of random projection , binary discriminant analysis and fuzzy commitment scheme . publicly available benchmark face databases ( feret , frgc , cmu-pie ) and other datasets are used for evaluation . the proposed novel scheme enhances the discriminability and recognition accuracy in terms of matching score of the face images for each stage and provides high security against potential attacks namely brute force and smart attacks . in this paper , we discuss results viz . averages matching score , computation time and security for hybrid approach and novel approach .", "topics": ["time complexity", "computation"]}
{"title": "distilling model knowledge", "abstract": "top-performing machine learning systems , such as deep neural networks , large ensembles and complex probabilistic graphical models , can be expensive to store , slow to evaluate and hard to integrate into larger systems . ideally , we would like to replace such cumbersome models with simpler models that perform equally well . in this thesis , we study knowledge distillation , the idea of extracting the knowledge contained in a complex model and injecting it into a more convenient model . we present a general framework for knowledge distillation , whereby a convenient model of our choosing learns how to mimic a complex model , by observing the latter 's behaviour and being penalized whenever it fails to reproduce it . we develop our framework within the context of three distinct machine learning applications : ( a ) model compression , where we compress large discriminative models , such as ensembles of neural networks , into models of much smaller size ; ( b ) compact predictive distributions for bayesian inference , where we distil large bags of mcmc samples into compact predictive distributions in closed form ; ( c ) intractable generative models , where we distil unnormalizable models such as rbms into tractable models such as nades . we contribute to the state of the art with novel techniques and ideas . in model compression , we describe and implement derivative matching , which allows for better distillation when data is scarce . in compact predictive distributions , we introduce online distillation , which allows for significant savings in memory . finally , in intractable generative models , we show how to use distilled models to robustly estimate intractable quantities of the original model , such as its intractable partition function .", "topics": ["generative model", "graphical model"]}
{"title": "conformal predictors for compound activity prediction", "abstract": "the paper presents an application of conformal predictors to a chemoinformatics problem of identifying activities of chemical compounds . the paper addresses some specific challenges of this domain : a large number of compounds ( training examples ) , high-dimensionality of feature space , sparseness and a strong class imbalance . a variant of conformal predictors called inductive mondrian conformal predictor is applied to deal with these challenges . results are presented for several non-conformity measures ( ncm ) extracted from underlying algorithms and different kernels . a number of performance measures are used in order to demonstrate the flexibility of inductive mondrian conformal predictors in dealing with such a complex set of data . keywords : conformal prediction , confidence estimation , chemoinformatics , non-conformity measure .", "topics": ["feature vector"]}
{"title": "image identification using sift algorithm : performance analysis against different image deformations", "abstract": "image identification is one of the most challenging tasks in different areas of computer vision . scale-invariant feature transform is an algorithm to detect and describe local features in images to further use them as an image matching criteria . in this paper , the performance of the sift matching algorithm against various image distortions such as rotation , scaling , fisheye and motion distortion are evaluated and false and true positive rates for a large number of image pairs are calculated and presented . we also evaluate the distribution of the matched keypoint orientation difference for each image deformation .", "topics": ["computer vision"]}
{"title": "metrics for deep generative models", "abstract": "neural samplers such as variational autoencoders ( vaes ) or generative adversarial networks ( gans ) approximate distributions by transforming samples from a simple random source -- -the latent space -- -to samples from a more complex distribution represented by a dataset . while the manifold hypothesis implies that the density induced by a dataset contains large regions of low density , the training criterions of vaes and gans will make the latent space densely covered . consequently points that are separated by low-density regions in observation space will be pushed together in latent space , making stationary distances poor proxies for similarity . we transfer ideas from riemannian geometry to this setting , letting the distance between two points be the shortest path on a riemannian manifold induced by the transformation . the method yields a principled distance measure , provides a tool for visual inspection of deep generative models , and an alternative to linear interpolation in latent space . in addition , it can be applied for robot movement generalization using previously learned skills . the method is evaluated on a synthetic dataset with known ground truth ; on a simulated robot arm dataset ; on human motion capture data ; and on a generative model of handwritten digits .", "topics": ["generative model", "calculus of variations"]}
{"title": "deep bcd-net using identical encoding-decoding cnn structures for iterative image recovery", "abstract": "in `` extreme '' computational imaging that collects extremely undersampled or noisy measurements , obtaining an accurate image within a reasonable computing time is challenging . incorporating image mapping convolutional neural networks ( cnn ) to iterative image recovery has great potential to resolve this issue . this paper 1 ) incorporates image mapping cnn using identical convolutional kernels in both encoders and decoders into block coordinate descent ( bcd ) optimization method -- referred to bcd-net using identical encoding-decoding cnn structures -- and 2 ) applies alternating direction method of multipliers to train the proposed bcd-net . numerical experiments show that , for a ) denoising moderately low signal-to-noise-ratio images and b ) extremely undersampled magnetic resonance imaging , the proposed bcd-net achieves ( significantly ) more accurate image recovery , compared to bcd-net using distinct encoding-decoding structures and/or the conventional image recovery model using both wavelets and total variation .", "topics": ["kernel ( operating system )", "noise reduction"]}
{"title": "solving mixed model workplace time-dependent assembly line balancing problem with fss algorithm", "abstract": "balancing assembly lines , a family of optimization problems commonly known as assembly line balancing problem , is notoriously np-hard . they comprise a set of problems of enormous practical interest to manufacturing industry due to the relevant frequency of this type of production paradigm . for this reason , many researchers on computational intelligence and industrial engineering have been conceiving algorithms for tackling different versions of assembly line balancing problems utilizing different methodologies . in this article , it was proposed a problem version referred as mixed model workplace time-dependent assembly line balancing problem with the intention of including pressing issues of real assembly lines in the optimization problem , to which four versions were conceived . heuristic search procedures were used , namely two swarm intelligence algorithms from the fish school search family : the original version , named `` vanilla '' , and a special variation including a stagnation avoidance routine . either approaches solved the newly posed problem achieving good results when compared to particle swarm optimization algorithm .", "topics": ["optimization problem", "heuristic"]}
{"title": "evaluating discourse processing algorithms", "abstract": "in order to take steps towards establishing a methodology for evaluating natural language systems , we conducted a case study . we attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues . we present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general . we illustrate the general difficulties encountered with quantitative evaluation . these are problems with : ( a ) allowing for underlying assumptions , ( b ) determining how to handle underspecifications , and ( c ) evaluating the contribution of false positives and error chaining .", "topics": ["natural language"]}
{"title": "dynamic sampling convolutional neural networks", "abstract": "we present dynamic sampling convolutional neural networks ( dscnn ) , where the position-specific kernels learn from not only the current position but also multiple sampled neighbour regions . during sampling , residual learning is introduced to ease training and an attention mechanism is applied to fuse features from different samples . and the kernels are further factorized to reduce parameters . the multiple sampling strategy enlarges the effective receptive fields significantly without requiring more parameters . while dscnns inherit the advantages of dfn , namely avoiding feature map blurring by position-specific kernels while keeping translation invariance , it also efficiently alleviates the overfitting issue caused by much more parameters than normal cnns . our model is efficient and can be trained end-to-end via standard back-propagation . we demonstrate the merits of our dscnns on both sparse and dense prediction tasks involving object detection and flow estimation . our results show that dscnns enjoy stronger recognition abilities and achieve 81.7 % in voc2012 detection dataset . also , dscnns obtain much sharper responses in flow estimation on flyingchairs dataset compared to multiple flownet models ' baselines .", "topics": ["sampling ( signal processing )", "object detection"]}
{"title": "a novel space-time representation on the positive semidefinite con for facial expression recognition", "abstract": "in this paper , we study the problem of facial expression recognition using a novel space-time geometric representation . we describe the temporal evolution of facial landmarks as parametrized trajectories on the riemannian manifold of positive semidefinite matrices of fixed-rank . our representation has the advantage to bring naturally a second desirable quantity when comparing shapes -- the spatial covariance -- in addition to the conventional affine-shape representation . we derive then geometric and computational tools for rate-invariant analysis and adaptive re-sampling of trajectories , grounding on the riemannian geometry of the manifold . specifically , our approach involves three steps : 1 ) facial landmarks are first mapped into the riemannian manifold of positive semidefinite matrices of rank 2 , to build time-parameterized trajectories ; 2 ) a temporal alignment is performed on the trajectories , providing a geometry-aware ( dis- ) similarity measure between them ; 3 ) finally , pairwise proximity function svm ( ppfsvm ) is used to classify them , incorporating the latter ( dis- ) similarity measure into the kernel function . we show the effectiveness of the proposed approach on four publicly available benchmarks ( ck+ , mmi , oulu-casia , and afew ) . the results of the proposed approach are comparable to or better than the state-of-the-art methods when involving only facial landmarks .", "topics": ["sampling ( signal processing )"]}
{"title": "optimizing affinity-based binary hashing using auxiliary coordinates", "abstract": "in supervised binary hashing , one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes , for application to fast image retrieval . this typically results in a difficult optimization problem , nonconvex and nonsmooth , because of the discrete variables involved . much work has simply relaxed the problem during training , solving a continuous optimization , and truncating the codes a posteriori . this gives reasonable results but is quite suboptimal . recent work has tried to optimize the objective directly over the binary codes and achieved better results , but the hash function was still learned a posteriori , which remains suboptimal . we propose a general framework for learning hash functions using affinity-based loss functions that uses auxiliary coordinates . this closes the loop and optimizes jointly over the hash functions and the binary codes so that they gradually match each other . the resulting algorithm can be seen as a corrected , iterated version of the procedure of optimizing first over the codes and then learning the hash function . compared to this , our optimization is guaranteed to obtain better hash functions while being not much slower , as demonstrated experimentally in various supervised datasets . in addition , our framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions .", "topics": ["approximation algorithm", "supervised learning"]}
{"title": "efficient model-based deep reinforcement learning with variational state tabulation", "abstract": "modern reinforcement learning algorithms reach super-human performance in many board and video games , but they are sample inefficient , i.e . they typically require significantly more playing experience than humans to reach an equal performance level . to improve sample efficiency , an agent may build a model of the environment and use planning methods to update its policy . in this article we introduce vast ( variational state tabulation ) , which maps an environment with a high-dimensional state space ( e.g . the space of visual inputs ) to an abstract tabular environment . prioritized sweeping with small backups , a highly efficient planning method , can then be used to update state-action values . we show how vast can rapidly learn to maximize reward in tasks like 3d navigation and efficiently adapt to sudden changes in rewards or transition probabilities .", "topics": ["value ( ethics )", "reinforcement learning"]}
{"title": "fusion of stereo and still monocular depth estimates in a self-supervised learning context", "abstract": "we study how autonomous robots can learn by themselves to improve their depth estimation capability . in particular , we investigate a self-supervised learning setup in which stereo vision depth estimates serve as targets for a convolutional neural network ( cnn ) that transforms a single still image to a dense depth map . after training , the stereo and mono estimates are fused with a novel fusion method that preserves high confidence stereo estimates , while leveraging the cnn estimates in the low-confidence regions . the main contribution of the article is that it is shown that the fused estimates lead to a higher performance than the stereo vision estimates alone . experiments are performed on the kitti dataset , and on board of a parrot slamdunk , showing that even rather limited cnns can help provide stereo vision equipped robots with more reliable depth maps for autonomous navigation .", "topics": ["supervised learning", "map"]}
{"title": "neural machine transliteration : preliminary results", "abstract": "machine transliteration is the process of automatically transforming the script of a word from a source language to a target language , while preserving pronunciation . sequence to sequence learning has recently emerged as a new paradigm in supervised learning . in this paper a character-based encoder-decoder model has been proposed that consists of two recurrent neural networks . the encoder is a bidirectional recurrent neural network that encodes a sequence of symbols into a fixed-length vector representation , and the decoder generates the target sequence using an attention-based recurrent neural network . the encoder , the decoder and the attention mechanism are jointly trained to maximize the conditional probability of a target sequence given a source sequence . our experiments on different datasets show that the proposed encoder-decoder model is able to achieve significantly higher transliteration quality over traditional statistical models .", "topics": ["recurrent neural network", "supervised learning"]}
{"title": "predicting human eye fixations via an lstm-based saliency attentive model", "abstract": "data-driven saliency has recently gained a lot of attention thanks to the use of convolutional neural networks for predicting gaze fixations . in this paper we go beyond standard approaches to saliency prediction , in which gaze maps are computed with a feed-forward network , and we present a novel model which can predict accurate saliency maps by incorporating neural attentive mechanisms . the core of our solution is a convolutional lstm that focuses on the most salient regions of the input image to iteratively refine the predicted saliency map . additionally , to tackle the center bias present in human eye fixations , our model can learn a set of prior maps generated with gaussian functions . we show , through an extensive evaluation , that the proposed architecture overcomes the current state of the art on two public saliency prediction datasets . we further study the contribution of each key components to demonstrate their robustness on different scenarios .", "topics": ["neural networks", "map"]}
{"title": "segmentation , indexing , and visualization of extended instructional videos", "abstract": "we present a new method for segmenting , and a new user interface for indexing and visualizing , the semantic content of extended instructional videos . given a series of key frames from the video , we generate a condensed view of the data by clustering frames according to media type and visual similarities . using various visual filters , key frames are first assigned a media type ( board , class , computer , illustration , podium , and sheet ) . key frames of media type board and sheet are then clustered based on contents via an algorithm with near-linear cost . a novel user interface , the result of two user studies , displays related topics using icons linked topologically , allowing users to quickly locate semantically related portions of the video . we analyze the accuracy of the segmentation tool on 17 instructional videos , each of which is from 75 to 150 minutes in duration ( a total of 40 hours ) ; the classification accuracy exceeds 96 % .", "topics": ["cluster analysis"]}
{"title": "mimo graph filters for convolutional neural networks", "abstract": "superior performance and ease of implementation have fostered the adoption of convolutional neural networks ( cnns ) for a wide array of inference and reconstruction tasks . cnns implement three basic blocks : convolution , pooling and pointwise nonlinearity . since the two first operations are well-defined only on regular-structured data such as audio or images , application of cnns to contemporary datasets where the information is defined in irregular domains is challenging . this paper investigates cnns architectures to operate on signals whose support can be modeled using a graph . architectures that replace the regular convolution with a so-called linear shift-invariant graph filter have been recently proposed . this paper goes one step further and , under the framework of multiple-input multiple-output ( mimo ) graph filters , imposes additional structure on the adopted graph filters , to obtain three new ( more parsimonious ) architectures . the proposed architectures result in a lower number of model parameters , reducing the computational complexity , facilitating the training , and mitigating the risk of overfitting . simulations show that the proposed simpler architectures achieve similar performance as more complex models .", "topics": ["computational complexity theory", "nonlinear system"]}
{"title": "fast and accurate entity recognition with iterated dilated convolutions", "abstract": "today when many practitioners run basic nlp on the entire web and large-volume traffic , faster methods are paramount to saving time and energy costs . recent advances in gpu hardware have led to the emergence of bi-directional lstms as a standard method for obtaining per-token vector representations serving as input to labeling tasks such as ner ( often followed by prediction in a linear-chain crf ) . though expressive and accurate , these models fail to fully exploit gpu parallelism , limiting their computational efficiency . this paper proposes a faster alternative to bi-lstms for ner : iterated dilated convolutional neural networks ( id-cnns ) , which have better capacity than traditional cnns for large context and structured prediction . unlike lstms whose sequential processing on sentences of length n requires o ( n ) time even in the face of parallelism , id-cnns permit fixed-depth convolutions to run in parallel across entire documents . we describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14-20x test-time speedups while retaining accuracy comparable to the bi-lstm-crf . moreover , id-cnns trained to aggregate context from the entire document are even more accurate while maintaining 8x faster test time speeds .", "topics": ["natural language processing", "convolution"]}
{"title": "diagonalwise refactorization : an efficient training method for depthwise convolutions", "abstract": "depthwise convolutions provide significant performance benefits owing to the reduction in both parameters and mult-adds . however , training depthwise convolution layers with gpus is slow in current deep learning frameworks because their implementations can not fully utilize the gpu capacity . to address this problem , in this paper we present an efficient method ( called diagonalwise refactorization ) for accelerating the training of depthwise convolution layers . our key idea is to rearrange the weight vectors of a depthwise convolution into a large diagonal weight matrix so as to convert the depthwise convolution into one single standard convolution , which is well supported by the cudnn library that is highly-optimized for gpu computations . we have implemented our training method in five popular deep learning frameworks . evaluation results show that our proposed method gains $ 15.4\\times $ training speedup on darknet , $ 8.4\\times $ on caffe , $ 5.4\\times $ on pytorch , $ 3.5\\times $ on mxnet , and $ 1.4\\times $ on tensorflow , compared to their original implementations of depthwise convolutions .", "topics": ["convolution", "computation"]}
{"title": "a hierarchical distributed processing framework for big image data", "abstract": "this paper introduces an effective processing framework nominated icp ( image cloud processing ) to powerfully cope with the data explosion in image processing field . while most previous researches focus on optimizing the image processing algorithms to gain higher efficiency , our work dedicates to providing a general framework for those image processing algorithms , which can be implemented in parallel so as to achieve a boost in time efficiency without compromising the results performance along with the increasing image scale . the proposed icp framework consists of two mechanisms , i.e . sicp ( static icp ) and dicp ( dynamic icp ) . specifically , sicp is aimed at processing the big image data pre-stored in the distributed system , while dicp is proposed for dynamic input . to accomplish sicp , two novel data representations named p-image and big-image are designed to cooperate with mapreduce to achieve more optimized configuration and higher efficiency . dicp is implemented through a parallel processing procedure working with the traditional processing mechanism of the distributed system . representative results of comprehensive experiments on the challenging imagenet dataset are selected to validate the capacity of our proposed icp framework over the traditional state-of-the-art methods , both in time efficiency and quality of results .", "topics": ["image processing"]}
{"title": "instrumenting an smt solver to solve hybrid network reachability problems", "abstract": "pddl+ planning has its semantics rooted in hybrid automata ( ha ) and recent work has shown that it can be modeled as a network of has . addressing the complexity of nonlinear pddl+ planning as has requires both space and time efficient reasoning . unfortunately , existing solvers either do not address nonlinear dynamics or do not natively support networks of automata . we present a new algorithm , called hnsolve , which guides the variable selection of the dreal satisfiability modulo theories ( smt ) solver while reasoning about network encodings of nonlinear pddl+ planning as has . hnsolve tightly integrates with dreal by solving a discrete abstraction of the ha network . hnsolve finds composite runs on the ha network that ignore continuous variables , but respect mode jumps and synchronization labels . hnsolve admissibly detects dead-ends in the discrete abstraction , and posts conflict clauses that prune the smt solver 's search . we evaluate the benefits of our hnsolve algorithm on pddl+ benchmark problems and demonstrate its performance with respect to prior work .", "topics": ["nonlinear system"]}
{"title": "estimating the hessian by back-propagating curvature", "abstract": "in this work we develop curvature propagation ( cp ) , a general technique for efficiently computing unbiased approximations of the hessian of any function that is computed using a computational graph . at the cost of roughly two gradient evaluations , cp can give a rank-1 approximation of the whole hessian , and can be repeatedly applied to give increasingly precise unbiased estimates of any or all of the entries of the hessian . of particular interest is the diagonal of the hessian , for which no general approach is known to exist that is both efficient and accurate . we show in experiments that cp turns out to work well in practice , giving very accurate estimates of the hessian of neural networks , for example , with a relatively small amount of work . we also apply cp to score matching , where a diagonal of a hessian plays an integral role in the score matching objective , and where it is usually computed exactly using inefficient algorithms which do not scale to larger and more complex models .", "topics": ["approximation", "gradient descent"]}
{"title": "machine learning approximation algorithms for high-dimensional fully nonlinear partial differential equations and second-order backward stochastic differential equations", "abstract": "high-dimensional partial differential equations ( pde ) appear in a number of models from the financial industry , such as in derivative pricing models , credit valuation adjustment ( cva ) models , or portfolio optimization models . the pdes in such applications are high-dimensional as the dimension corresponds to the number of financial assets in a portfolio . moreover , such pdes are often fully nonlinear due to the need to incorporate certain nonlinear phenomena in the model such as default risks , transaction costs , volatility uncertainty ( knightian uncertainty ) , or trading constraints in the model . such high-dimensional fully nonlinear pdes are exceedingly difficult to solve as the computational effort for standard approximation methods grows exponentially with the dimension . in this work we propose a new method for solving high-dimensional fully nonlinear second-order pdes . our method can in particular be used to sample from high-dimensional nonlinear expectations . the method is based on ( i ) a connection between fully nonlinear second-order pdes and second-order backward stochastic differential equations ( 2bsdes ) , ( ii ) a merged formulation of the pde and the 2bsde problem , ( iii ) a temporal forward discretization of the 2bsde and a spatial approximation via deep neural nets , and ( iv ) a stochastic gradient descent-type optimization procedure . numerical results obtained using $ { \\rm t { \\small ensor } f { \\small low } } $ in $ { \\rm p { \\small ython } } $ illustrate the efficiency and the accuracy of the method in the cases of a $ 100 $ -dimensional black-scholes-barenblatt equation , a $ 100 $ -dimensional hamilton-jacobi-bellman equation , and a nonlinear expectation of a $ 100 $ -dimensional $ g $ -brownian motion .", "topics": ["value ( ethics )", "approximation algorithm"]}
{"title": "towards cognitive exploration through deep reinforcement learning for mobile robots", "abstract": "exploration in an unknown environment is the core functionality for mobile robots . learning-based exploration methods , including convolutional neural networks , provide excellent strategies without human-designed logic for the feature extraction . but the conventional supervised learning algorithms cost lots of efforts on the labeling work of datasets inevitably . scenes not included in the training set are mostly unrecognized either . we propose a deep reinforcement learning method for the exploration of mobile robots in an indoor environment with the depth information from an rgb-d sensor only . based on the deep q-network framework , the raw depth image is taken as the only input to estimate the q values corresponding to all moving commands . the training of the network weights is end-to-end . in arbitrarily constructed simulation environments , we show that the robot can be quickly adapted to unfamiliar scenes without any man-made labeling . besides , through analysis of receptive fields of feature representations , deep reinforcement learning motivates the convolutional networks to estimate the traversability of the scenes . the test results are compared with the exploration strategies separately based on deep learning or reinforcement learning . even trained only in the simulated environment , experimental results in real-world environment demonstrate that the cognitive ability of robot controller is dramatically improved compared with the supervised method . we believe it is the first time that raw sensor information is used to build cognitive exploration strategy for mobile robots through end-to-end deep reinforcement learning .", "topics": ["supervised learning", "feature extraction"]}
{"title": "deep recurrent neural networks for acoustic modelling", "abstract": "we present a novel deep recurrent neural network ( rnn ) model for acoustic modelling in automatic speech recognition ( asr ) . we term our contribution as a tc-dnn-blstm-dnn model , the model combines a deep neural network ( dnn ) with time convolution ( tc ) , followed by a bidirectional long short-term memory ( blstm ) , and a final dnn . the first dnn acts as a feature processor to our model , the blstm then generates a context from the sequence acoustic signal , and the final dnn takes the context and models the posterior probabilities of the acoustic states . we achieve a 3.47 wer on the wall street journal ( wsj ) eval92 task or more than 8 % relative improvement over the baseline dnn models .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "artificial generation of big data for improving image classification : a generative adversarial network approach on sar data", "abstract": "very high spatial resolution ( vhsr ) large-scale sar image databases are still an unresolved issue in the remote sensing field . in this work , we propose such a dataset and use it to explore patch-based classification in urban and periurban areas , considering 7 distinct semantic classes . in this context , we investigate the accuracy of large cnn classification models and pre-trained networks for sar imaging systems . furthermore , we propose a generative adversarial network ( gan ) for sar image generation and test , whether the synthetic data can actually improve classification accuracy .", "topics": ["synthetic data", "database"]}
{"title": "learning social affordance for human-robot interaction", "abstract": "in this paper , we present an approach for robot learning of social affordance from human activity videos . we consider the problem in the context of human-robot interaction : our approach learns structural representations of human-human ( and human-object-human ) interactions , describing how body-parts of each agent move with respect to each other and what spatial relations they should maintain to complete each sub-event ( i.e . , sub-goal ) . this enables the robot to infer its own movement in reaction to the human body motion , allowing it to naturally replicate such interactions . we introduce the representation of social affordance and propose a generative model for its weakly supervised learning from human demonstration videos . our approach discovers critical steps ( i.e . , latent sub-events ) in an interaction and the typical motion associated with them , learning what body-parts should be involved and how . the experimental results demonstrate that our markov chain monte carlo ( mcmc ) based learning algorithm automatically discovers semantically meaningful interactive affordance from rgb-d videos , which allows us to generate appropriate full body motion for an agent .", "topics": ["generative model", "supervised learning"]}
{"title": "variational inference : a review for statisticians", "abstract": "one of the core problems of modern statistics is to approximate difficult-to-compute probability densities . this problem is especially important in bayesian statistics , which frames all inference about unknown quantities as a calculation involving the posterior density . in this paper , we review variational inference ( vi ) , a method from machine learning that approximates probability densities through optimization . vi has been used in many applications and tends to be faster than classical methods , such as markov chain monte carlo sampling . the idea behind vi is to first posit a family of densities and then to find the member of that family which is close to the target . closeness is measured by kullback-leibler divergence . we review the ideas behind mean-field variational inference , discuss the special case of vi applied to exponential family models , present a full example with a bayesian mixture of gaussians , and derive a variant that uses stochastic optimization to scale up to massive data . we discuss modern research in vi and highlight important open problems . vi is powerful , but it is not yet well understood . our hope in writing this paper is to catalyze statistical research on this class of algorithms .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "a novel embedding model for knowledge base completion based on convolutional neural network", "abstract": "in this paper , we propose a novel embedding model , named convkb , for knowledge base completion . our model convkb advances state-of-the-art models by employing a convolutional neural network , so that it can capture global relationships and transitional characteristics between entities and relations in knowledge bases . in convkb , each triple ( head entity , relation , tail entity ) is represented as a 3-column matrix where each column vector represents a triple element . this 3-column matrix is then fed to a convolution layer where multiple filters are operated on the matrix to generate different feature maps . these feature maps are then concatenated into a single feature vector representing the input triple . the feature vector is multiplied with a weight vector via a dot product to return a score . this score is then used to predict whether the triple is valid or not . experiments show that convkb achieves better link prediction performance than previous state-of-the-art embedding models on two benchmark datasets wn18rr and fb15k-237 .", "topics": ["feature vector", "map"]}
{"title": "a simple expression for the map of asplund 's distances with the multiplicative logarithmic image processing ( lip ) law", "abstract": "we introduce a simple expression for the map of asplund 's distances with the multiplicative logarithmic image processing ( lip ) law . it is a difference between a morphological dilation and a morphological erosion with an additive structuring function which corresponds to a morphological gradient .", "topics": ["image processing", "gradient"]}
{"title": "grouping words using statistical context", "abstract": "this paper ( cmp-lg/yymmnnn ) has been accepted for publication in the student session of eacl-95 . it outlines ongoing work using statistical and unsupervised neural network methods for clustering words in untagged corpora . such approaches are of interest when attempting to understand the development of human intuitive categorization of language as well as for trying to improve computational methods in natural language understanding . some preliminary results using a simple statistical approach are described , along with work using an unsupervised neural network to distinguish between the sense classes into which words fall .", "topics": ["cluster analysis", "text corpus"]}
{"title": "less is more : a comprehensive framework for the number of components of ensemble classifiers", "abstract": "the number of component classifiers chosen for an ensemble has a great impact on its prediction ability . in this paper , we use a geometric framework for a priori determining the ensemble size , applicable to most of the existing batch and online ensemble classifiers . there are only a limited number of studies on the ensemble size considering majority voting ( mv ) and weighted majority voting ( wmv ) . almost all of them are designed for batch-mode , barely addressing online environments . the big data dimensions and resource limitations in terms of time and memory make the determination of the ensemble size crucial , especially for online environments . our framework proves , for the mv aggregation rule , that the more strong components we can add to the ensemble the more accurate predictions we can achieve . on the other hand , for the wmv aggregation rule , we prove the existence of an ideal number of components equal to the number of class labels , with the premise that components are completely independent of each other and strong enough . while giving the exact definition for a strong and independent classifier in the context of an ensemble is a challenging task , our proposed geometric framework provides a theoretical explanation of diversity and its impact on the accuracy of predictions . we conduct an experimental evaluation with two different scenarios to show the practical value of our theorems .", "topics": ["statistical classification"]}
{"title": "super-samples from kernel herding", "abstract": "we extend the herding algorithm to continuous spaces by using the kernel trick . the resulting `` kernel herding '' algorithm is an infinite memory deterministic process that learns to approximate a pdf with a collection of samples . we show that kernel herding decreases the error of expectations of functions in the hilbert space at a rate o ( 1/t ) which is much faster than the usual o ( 1/pt ) for iid random samples . we illustrate kernel herding by approximating bayesian predictive distributions .", "topics": ["kernel ( operating system )", "approximation algorithm"]}
{"title": "maximum mutual information regularized classification", "abstract": "in this paper , a novel pattern classification approach is proposed by regularizing the classifier learning to maximize mutual information between the classification response and the true class label . we argue that , with the learned classifier , the uncertainty of the true class label of a data sample should be reduced by knowing its classification response as much as possible . the reduced uncertainty is measured by the mutual information between the classification response and the true class label . to this end , when learning a linear classifier , we propose to maximize the mutual information between classification responses and true class labels of training samples , besides minimizing the classification error and reduc- ing the classifier complexity . an objective function is constructed by modeling mutual information with entropy estimation , and it is optimized by a gradi- ent descend method in an iterative algorithm . experiments on two real world pattern classification problems show the significant improvements achieved by maximum mutual information regularization .", "topics": ["statistical classification", "loss function"]}
{"title": "deep learning in finance", "abstract": "we explore the use of deep learning hierarchical models for problems in financial prediction and classification . financial prediction problems -- such as those presented in designing and pricing securities , constructing portfolios , and risk management -- often involve large data sets with complex data interactions that currently are difficult or impossible to specify in a full economic model . applying deep learning methods to these problems can produce more useful results than standard methods in finance . in particular , deep learning can detect and exploit interactions in the data that are , at least currently , invisible to any existing financial economic theory .", "topics": ["interaction", "bayesian network"]}
{"title": "rivalry of two families of algorithms for memory-restricted streaming pca", "abstract": "we study the problem of recovering the subspace spanned by the first $ k $ principal components of $ d $ -dimensional data under the streaming setting , with a memory bound of $ o ( kd ) $ . two families of algorithms are known for this problem . the first family is based on the framework of stochastic gradient descent . nevertheless , the convergence rate of the family can be seriously affected by the learning rate of the descent steps and deserves more serious study . the second family is based on the power method over blocks of data , but setting the block size for its existing algorithms is not an easy task . in this paper , we analyze the convergence rate of a representative algorithm with decayed learning rate ( oja and karhunen , 1985 ) in the first family for the general $ k > 1 $ case . moreover , we propose a novel algorithm for the second family that sets the block sizes automatically and dynamically with faster convergence rate . we then conduct empirical studies that fairly compare the two families on real-world data . the studies reveal the advantages and disadvantages of these two families .", "topics": ["gradient descent", "gradient"]}
{"title": "kernel principal component analysis and its applications in face recognition and active shape models", "abstract": "principal component analysis ( pca ) is a popular tool for linear dimensionality reduction and feature extraction . kernel pca is the nonlinear form of pca , which better exploits the complicated spatial structure of high-dimensional features . in this paper , we first review the basic ideas of pca and kernel pca . then we focus on the reconstruction of pre-images for kernel pca . we also give an introduction on how pca is used in active shape models ( asms ) , and discuss how kernel pca can be applied to improve traditional asms . then we show some experimental results to compare the performance of kernel pca and standard pca for classification problems . we also implement the kernel pca-based asms , and use it to construct human face models .", "topics": ["kernel ( operating system )", "feature extraction"]}
{"title": "a robust regression approach for background/foreground segmentation", "abstract": "background/foreground segmentation has a lot of applications in image and video processing . in this paper , a segmentation algorithm is proposed which is mainly designed for text and line extraction in screen content . the proposed method makes use of the fact that the background in each block is usually smoothly varying and can be modeled well by a linear combination of a few smoothly varying basis functions , while the foreground text and graphics create sharp discontinuity . the algorithm separates the background and foreground pixels by trying to fit pixel values in the block into a smooth function using a robust regression method . the inlier pixels that can fit well will be considered as background , while remaining outlier pixels will be considered foreground . this algorithm has been extensively tested on several images from hevc standard test sequences for screen content coding , and is shown to have superior performance over other methods , such as the k-means clustering based segmentation algorithm in djvu . this background/foreground segmentation can be used in different applications such as : text extraction , separate coding of background and foreground for compression of screen content and mixed content documents , principle line extraction from palmprint and crease detection in fingerprint images .", "topics": ["cluster analysis", "pixel"]}
{"title": "fast and unsupervised methods for multilingual cognate clustering", "abstract": "in this paper we explore the use of unsupervised methods for detecting cognates in multilingual word lists . we use online em to train sound segment similarity weights for computing similarity between two words . we tested our online systems on geographically spread sixteen different language groups of the world and show that the online pmi system ( pointwise mutual information ) outperforms a hmm based system and two linguistically motivated systems : lexstat and aline . our results suggest that a pmi system trained in an online fashion can be used by historical linguists for fast and accurate identification of cognates in not so well-studied language families .", "topics": ["cluster analysis", "unsupervised learning"]}
{"title": "the topological fusion of bayes nets", "abstract": "bayes nets are relatively recent innovations . as a result , most of their theoretical development has focused on the simplest class of single-author models . the introduction of more sophisticated multiple-author settings raises a variety of interesting questions . one such question involves the nature of compromise and consensus . posterior compromises let each model process all data to arrive at an independent response , and then split the difference . prior compromises , on the other hand , force compromise to be reached on all points before data is observed . this paper introduces prior compromises in a bayes net setting . it outlines the problem and develops an efficient algorithm for fusing two directed acyclic graphs into a single , consensus structure , which may then be used as the basis of a prior compromise .", "topics": ["bayesian network"]}
{"title": "scalable and robust sparse subspace clustering using randomized clustering and multilayer graphs", "abstract": "sparse subspace clustering ( ssc ) is one of the current state-of-the-art methods for partitioning data points into the union of subspaces , with strong theoretical guarantees . however , it is not practical for large data sets as it requires solving a lasso problem for each data point , where the number of variables in each lasso problem is the number of data points . to improve the scalability of ssc , we propose to select a few sets of anchor points using a randomized hierarchical clustering method , and , for each set of anchor points , solve the lasso problems for each data point allowing only anchor points to have a non-zero weight ( this reduces drastically the number of variables ) . this generates a multilayer graph where each layer corresponds to a different set of anchor points . using the grassmann manifold of orthogonal matrices , the shared connectivity among the layers is summarized within a single subspace . finally , we use $ k $ -means clustering within that subspace to cluster the data points , similarly as done by spectral clustering in ssc . we show on both synthetic and real-world data sets that the proposed method not only allows ssc to scale to large-scale data sets , but that it is also much more robust as it performs significantly better on noisy data and on data with close susbspaces and outliers , while it is not prone to oversegmentation .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "phased lstm : accelerating recurrent network training for long or event-based sequences", "abstract": "recurrent neural networks ( rnns ) have become the state-of-the-art choice for extracting patterns from temporal sequences . however , current rnn models are ill-suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons . such data can occur , for example , when the input comes from novel event-driven artificial sensors that generate sparse , asynchronous streams of events or from multiple conventional sensors with different update intervals . in this work , we introduce the phased lstm model , which extends the lstm unit by adding a new time gate . this gate is controlled by a parametrized oscillation with a frequency range that produces updates of the memory cell only during a small percentage of the cycle . even with the sparse updates imposed by the oscillation , the phased lstm network achieves faster convergence than regular lstms on tasks which require learning of long sequences . the model naturally integrates inputs from sensors of arbitrary sampling rates , thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information . it also greatly improves the performance of lstms in standard rnn applications , and does so with an order-of-magnitude fewer computes at runtime .", "topics": ["sampling ( signal processing )", "recurrent neural network"]}
{"title": "output reachable set estimation and verification for multi-layer neural networks", "abstract": "in this paper , the output reachable estimation and safety verification problems for multi-layer perceptron neural networks are addressed . first , a conception called maximum sensitivity in introduced and , for a class of multi-layer perceptrons whose activation functions are monotonic functions , the maximum sensitivity can be computed via solving convex optimization problems . then , using a simulation-based method , the output reachable set estimation problem for neural networks is formulated into a chain of optimization problems . finally , an automated safety verification is developed based on the output reachable set estimation result . an application to the safety verification for a robotic arm model with two joints is presented to show the effectiveness of proposed approaches .", "topics": ["neural networks", "simulation"]}
{"title": "veni vidi vici , a three-phase scenario for parameter space analysis in image analysis and visualization", "abstract": "automatic analysis of the enormous sets of images is a critical task in life sciences . this faces many challenges such as : algorithms are highly parameterized , significant human input is intertwined , and lacking a standard meta-visualization approach . this paper proposes an alternative iterative approach for optimizing input parameters , saving time by minimizing the user involvement , and allowing for understanding the workflow of algorithms and discovering new ones . the main focus is on developing an interactive visualization technique that enables users to analyze the relationships between sampled input parameters and corresponding output . this technique is implemented as a prototype called veni vidi vici , or `` i came , i saw , i conquered . '' this strategy is inspired by the mathematical formulas of numbering computable functions and is developed atop imagej , a scientific image processing program . a case study is presented to investigate the proposed framework . finally , the paper explores some potential future issues in the application of the proposed approach in parameter space analysis in visualization .", "topics": ["image processing"]}
{"title": "learning representations by maximizing compression", "abstract": "we give an algorithm that learns a representation of data through compression . the algorithm 1 ) predicts bits sequentially from those previously seen and 2 ) has a structure and a number of computations similar to an autoencoder . the likelihood under the model can be calculated exactly , and arithmetic coding can be used directly for compression . when training on digits the algorithm learns filters similar to those of restricted boltzman machines and denoising autoencoders . independent samples can be drawn from the model by a single sweep through the pixels . the algorithm has a good compression performance when compared to other methods that work under random ordering of pixels .", "topics": ["noise reduction", "computation"]}
{"title": "natural scene recognition based on superpixels and deep boltzmann machines", "abstract": "the deep boltzmann machines ( dbm ) is a state-of-the-art unsupervised learning model , which has been successfully applied to handwritten digit recognition and , as well as object recognition . however , the dbm is limited in scene recognition due to the fact that natural scene images are usually very large . in this paper , an efficient scene recognition approach is proposed based on superpixels and the dbms . first , a simple linear iterative clustering ( slic ) algorithm is employed to generate superpixels of input images , where each superpixel is regarded as an input of a learning model . then , a two-layer dbm model is constructed by stacking two restricted boltzmann machines ( rbms ) , and a greedy layer-wise algorithm is applied to train the dbm model . finally , a softmax regression is utilized to categorize scene images . the proposed technique can effectively reduce the computational complexity and enhance the performance for large natural image recognition . the approach is verified and evaluated by extensive experiments , including the fifteen-scene categories dataset the uiuc eight-sports dataset , and the sift flow dataset , are used to evaluate the proposed method . the experimental results show that the proposed approach outperforms other state-of-the-art methods in terms of recognition rate .", "topics": ["cluster analysis", "unsupervised learning"]}
{"title": "active learning algorithms for graphical model selection", "abstract": "the problem of learning the structure of a high dimensional graphical model from data has received considerable attention in recent years . in many applications such as sensor networks and proteomics it is often expensive to obtain samples from all the variables involved simultaneously . for instance , this might involve the synchronization of a large number of sensors or the tagging of a large number of proteins . to address this important issue , we initiate the study of a novel graphical model selection problem , where the goal is to optimize the total number of scalar samples obtained by allowing the collection of samples from only subsets of the variables . we propose a general paradigm for graphical model selection where feedback is used to guide the sampling to high degree vertices , while obtaining only few samples from the ones with the low degrees . we instantiate this framework with two specific active learning algorithms , one of which makes mild assumptions but is computationally expensive , while the other is more computationally efficient but requires stronger ( nevertheless standard ) assumptions . whereas the sample complexity of passive algorithms is typically a function of the maximum degree of the graph , we show that the sample complexity of our algorithms is provable smaller and that it depends on a novel local complexity measure that is akin to the average degree of the graph . we finally demonstrate the efficacy of our framework via simulations .", "topics": ["sampling ( signal processing )", "graphical model"]}
{"title": "sequential convolutional neural networks for slot filling in spoken language understanding", "abstract": "we investigate the usage of convolutional neural networks ( cnns ) for the slot filling task in spoken language understanding . we propose a novel cnn architecture for sequence labeling which takes into account the previous context words with preserved order information and pays special attention to the current word with its surrounding context . moreover , it combines the information from the past and the future words for classification . our proposed cnn architecture outperforms even the previously best ensembling recurrent neural network model and achieves state-of-the-art results with an f1-score of 95.61 % on the atis benchmark dataset without using any additional linguistic knowledge and resources .", "topics": ["recurrent neural network"]}
{"title": "efficient unimodality test in clustering by signature testing", "abstract": "this paper provides a new unimodality test with application in hierarchical clustering methods . the proposed method denoted by signature test ( sigtest ) , transforms the data based on its statistics . the transformed data has much smaller variation compared to the original data and can be evaluated in a simple proposed unimodality test . compared with the existing unimodality tests , sigtest is more accurate in detecting the overlapped clusters and has a much less computational complexity . simulation results demonstrate the efficiency of this statistic test for both real and synthetic data sets .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "time resolution dependence of information measures for spiking neurons : atoms , scaling , and universality", "abstract": "the mutual information between stimulus and spike-train response is commonly used to monitor neural coding efficiency , but neuronal computation broadly conceived requires more refined and targeted information measures of input-output joint processes . a first step towards that larger goal is to develop information measures for individual output processes , including information generation ( entropy rate ) , stored information ( statistical complexity ) , predictable information ( excess entropy ) , and active information accumulation ( bound information rate ) . we calculate these for spike trains generated by a variety of noise-driven integrate-and-fire neurons as a function of time resolution and for alternating renewal processes . we show that their time-resolution dependence reveals coarse-grained structural properties of interspike interval statistics ; e.g . , $ \\tau $ -entropy rates that diverge less quickly than the firing rate indicate interspike interval correlations . we also find evidence that the excess entropy and regularized statistical complexity of different types of integrate-and-fire neurons are universal in the continuous-time limit in the sense that they do not depend on mechanism details . this suggests a surprising simplicity in the spike trains generated by these model neurons . interestingly , neurons with gamma-distributed isis and neurons whose spike trains are alternating renewal processes do not fall into the same universality class . these results lead to two conclusions . first , the dependence of information measures on time resolution reveals mechanistic details about spike train generation . second , information measures can be used as model selection tools for analyzing spike train processes .", "topics": ["computation"]}
{"title": "how should we evaluate supervised hashing ?", "abstract": "hashing produces compact representations for documents , to perform tasks like classification or retrieval based on these short codes . when hashing is supervised , the codes are trained using labels on the training data . this paper first shows that the evaluation protocols used in the literature for supervised hashing are not satisfactory : we show that a trivial solution that encodes the output of a classifier significantly outperforms existing supervised or semi-supervised methods , while using much shorter codes . we then propose two alternative protocols for supervised hashing : one based on retrieval on a disjoint set of classes , and another based on transfer learning to new classes . we provide two baseline methods for image-related tasks to assess the performance of ( semi- ) supervised hashing : without coding and with unsupervised codes . these baselines give a lower- and upper-bound on the performance of a supervised hashing scheme .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "classification of sparsely labeled spatio-temporal data through semi-supervised adversarial learning", "abstract": "in recent years , generative adversarial networks ( gan ) have emerged as a powerful method for learning the mapping from noisy latent spaces to realistic data samples in high-dimensional space . so far , the development and application of gans have been predominantly focused on spatial data such as images . in this project , we aim at modeling of spatio-temporal sensor data instead , i.e . dynamic data over time . the main goal is to encode temporal data into a global and low-dimensional latent vector that captures the dynamics of the spatio-temporal signal . to this end , we incorporate auto-regressive rnns , wasserstein gan loss , spectral norm weight constraints and a semi-supervised learning scheme into infogan , a method for retrieval of meaningful latents in adversarial learning . to demonstrate the modeling capability of our method , we encode full-body skeletal human motion from a large dataset representing 60 classes of daily activities , recorded in a multi-kinect setup . initial results indicate competitive classification performance of the learned latent representations , compared to direct cnn/rnn inference . in future work , we plan to apply this method on a related problem in the medical domain , i.e . on recovery of meaningful latents in gait analysis of patients with vertigo and balance disorders .", "topics": ["supervised learning"]}
{"title": "safe and efficient off-policy reinforcement learning", "abstract": "in this work , we take a fresh look at some old and new algorithms for off-policy , return-based reinforcement learning . expressing these in a common form , we derive a novel algorithm , retrace ( $ \\lambda $ ) , with three desired properties : ( 1 ) it has low variance ; ( 2 ) it safely uses samples collected from any behaviour policy , whatever its degree of `` off-policyness '' ; and ( 3 ) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies . we analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms . we believe this is the first return-based off-policy control algorithm converging a.s. to $ q^* $ without the glie assumption ( greedy in the limit with infinite exploration ) . as a corollary , we prove the convergence of watkins ' q ( $ \\lambda $ ) , which was an open problem since 1989 . we illustrate the benefits of retrace ( $ \\lambda $ ) on a standard suite of atari 2600 games .", "topics": ["reinforcement learning"]}
{"title": "a syllable-based technique for word embeddings of korean words", "abstract": "word embedding has become a fundamental component to many nlp tasks such as named entity recognition and machine translation . however , popular models that learn such embeddings are unaware of the morphology of words , so it is not directly applicable to highly agglutinative languages such as korean . we propose a syllable-based learning model for korean using a convolutional neural network , in which word representation is composed of trained syllable vectors . our model successfully produces morphologically meaningful representation of korean words compared to the original skip-gram embeddings . the results also show that it is quite robust to the out-of-vocabulary problem .", "topics": ["natural language processing", "machine translation"]}
{"title": "arabic keyphrase extraction using linguistic knowledge and machine learning techniques", "abstract": "in this paper , a supervised learning technique for extracting keyphrases of arabic documents is presented . the extractor is supplied with linguistic knowledge to enhance its efficiency instead of relying only on statistical information such as term frequency and distance . during analysis , an annotated arabic corpus is used to extract the required lexical features of the document words . the knowledge also includes syntactic rules based on part of speech tags and allowed word sequences to extract the candidate keyphrases . in this work , the abstract form of arabic words is used instead of its stem form to represent the candidate terms . the abstract form hides most of the inflections found in arabic words . the paper introduces new features of keyphrases based on linguistic knowledge , to capture titles and subtitles of a document . a simple anova test is used to evaluate the validity of selected features . then , the learning model is built using the lda - linear discriminant analysis - and training documents . although , the presented system is trained using documents in the it domain , experiments carried out show that it has a significantly better performance than the existing arabic extractor systems , where precision and recall values reach double their corresponding values in the other systems especially for lengthy and non-scientific articles .", "topics": ["supervised learning", "value ( ethics )"]}
{"title": "salientdso : bringing attention to direct sparse odometry", "abstract": "although cluttered indoor scenes have a lot of useful high-level semantic information which can be used for mapping and localization , most visual odometry ( vo ) algorithms rely on the usage of geometric features such as points , lines and planes . lately , driven by this idea , the joint optimization of semantic labels and obtaining odometry has gained popularity in the robotics community . the joint optimization is good for accurate results but is generally very slow . at the same time , in the vision community , direct and sparse approaches for vo have stricken the right balance between speed and accuracy . we merge the successes of these two communities and present a way to incorporate semantic information in the form of visual saliency to direct sparse odometry - a highly successful direct sparse vo algorithm . we also present a framework to filter the visual saliency based on scene parsing . our framework , salientdso , relies on the widely successful deep learning based approaches for visual saliency and scene parsing which drives the feature selection for obtaining highly-accurate and robust vo even in the presence of as few as 40 point features per frame . we provide extensive quantitative evaluation of salientdso on the icl-nuim and tum monovo datasets and show that we outperform dso and orb-slam - two very popular state-of-the-art approaches in the literature . we also collect and publicly release a cvl-umd dataset which contains two indoor cluttered sequences on which we show qualitative evaluations . to our knowledge this is the first paper to use visual saliency and scene parsing to drive the feature selection in direct vo .", "topics": ["high- and low-level", "parsing"]}
{"title": "tricolor dags for machine translation", "abstract": "machine translation ( mt ) has recently been formulated in terms of constraint-based knowledge representation and unification theories , but it is becoming more and more evident that it is not possible to design a practical mt system without an adequate method of handling mismatches between semantic representations in the source and target languages . in this paper , we introduce the idea of `` information-based '' mt , which is considerably more flexible than interlingual mt or the conventional transfer-based mt .", "topics": ["machine translation"]}
{"title": "sparse camera network for visual surveillance -- a comprehensive survey", "abstract": "technological advances in sensor manufacture , communication , and computing are stimulating the development of new applications that are transforming traditional vision systems into pervasive intelligent camera networks . the analysis of visual cues in multi-camera networks enables a wide range of applications , from smart home and office automation to large area surveillance and traffic surveillance . while dense camera networks - in which most cameras have large overlapping fields of view - are well studied , we are mainly concerned with sparse camera networks . a sparse camera network undertakes large area surveillance using as few cameras as possible , and most cameras have non-overlapping fields of view with one another . the task is challenging due to the lack of knowledge about the topological structure of the network , variations in the appearance and motion of specific tracking targets in different views , and the difficulties of understanding composite events in the network . in this review paper , we present a comprehensive survey of recent research results to address the problems of intra-camera tracking , topological structure learning , target appearance modeling , and global activity understanding in sparse camera networks . a number of current open research issues are discussed .", "topics": ["sparse matrix"]}
{"title": "distributed deep reinforcement learning : learn how to play atari games in 21 minutes", "abstract": "we present a study in distributed deep reinforcement learning ( ddrl ) focused on scalability of a state-of-the-art deep reinforcement learning algorithm known as batch asynchronous advantage actorcritic ( ba3c ) . we show that using the adam optimization algorithm with a batch size of up to 2048 is a viable choice for carrying out large scale machine learning computations . this , combined with careful reexamination of the optimizer 's hyperparameters , using synchronous training on the node level ( while keeping the local , single node part of the algorithm asynchronous ) and minimizing the memory footprint of the model , allowed us to achieve linear scaling for up to 64 cpu nodes . this corresponds to a training time of 21 minutes on 768 cpu cores , as opposed to 10 hours when using a single node with 24 cores achieved by a baseline single-node implementation .", "topics": ["baseline ( configuration management )", "reinforcement learning"]}
{"title": "on sgd 's failure in practice : characterizing and overcoming stalling", "abstract": "stochastic gradient descent ( sgd ) is widely used in machine learning problems to efficiently perform empirical risk minimization , yet , in practice , sgd is known to stall before reaching the actual minimizer of the empirical risk . sgd stalling has often been attributed to its sensitivity to the conditioning of the problem ; however , as we demonstrate , sgd will stall even when applied to a simple linear regression problem with unity condition number for standard learning rates . thus , in this work , we numerically demonstrate and mathematically argue that stalling is a crippling and generic limitation of sgd and its variants in practice . once we have established the problem of stalling , we generalize an existing framework for hedging against its effects , which ( 1 ) deters sgd and its variants from stalling , ( 2 ) still provides convergence guarantees , and ( 3 ) makes sgd and its variants more practical methods for minimization .", "topics": ["numerical analysis", "gradient descent"]}
{"title": "noise in structured-light stereo depth cameras : modeling and its applications", "abstract": "depth maps obtained from commercially available structured-light stereo based depth cameras , such as the kinect , are easy to use but are affected by significant amounts of noise . this paper is devoted to a study of the intrinsic noise characteristics of such depth maps , i.e . the standard deviation of noise in estimated depth varies quadratically with the distance of the object from the depth camera . we validate this theoretical model against empirical observations and demonstrate the utility of this noise model in three popular applications : depth map denoising , volumetric scan merging for 3d modeling , and identification of 3d planes in depth maps .", "topics": ["noise reduction", "map"]}
{"title": "deductive algorithmic knowledge", "abstract": "the framework of algorithmic knowledge assumes that agents use algorithms to compute the facts they explicitly know . in many cases of interest , a deductive system , rather than a particular algorithm , captures the formal reasoning used by the agents to compute what they explicitly know . we introduce a logic for reasoning about both implicit and explicit knowledge with the latter defined with respect to a deductive system formalizing a logical theory for agents . the highly structured nature of deductive systems leads to very natural axiomatizations of the resulting logic when interpreted over any fixed deductive system . the decision problem for the logic , in the presence of a single agent , is np-complete in general , no harder than propositional logic . it remains np-complete when we fix a deductive system that is decidable in nondeterministic polynomial time . these results extend in a straightforward way to multiple agents .", "topics": ["time complexity", "polynomial"]}
{"title": "data augmentation generative adversarial networks", "abstract": "effective training of neural networks requires much data . in the low-data regime , parameters are underdetermined , and learnt networks generalise poorly . data augmentation alleviates this by using existing data more effectively . however standard data augmentation produces only limited plausible alternative data . given there is potential to generate a much broader set of augmentations , we design and train a generative model to do data augmentation . the model , based on image conditional generative adversarial networks , takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items . as this generative process does not depend on the classes themselves , it can be applied to novel unseen classes of data . we show that a data augmentation generative adversarial network ( dagan ) augments standard vanilla classifiers well . we also show a dagan can enhance few-shot learning systems such as matching networks . we demonstrate these approaches on omniglot , on emnist having learnt the dagan on omniglot , and vgg-face data . in our experiments we can see over 13 % increase in accuracy in the low-data regime experiments in omniglot ( from 69 % to 82 % ) , emnist ( 73.9 % to 76 % ) and vgg-face ( 4.5 % to 12 % ) ; in matching networks for omniglot we observe an increase of 0.5 % ( from 96.9 % to 97.4 % ) and an increase of 1.8 % in emnist ( from 59.5 % to 61.3 % ) .", "topics": ["generative model"]}
{"title": "conditional generative adversarial nets", "abstract": "generative adversarial nets [ 8 ] were recently introduced as a novel way to train generative models . in this work we introduce the conditional version of generative adversarial nets , which can be constructed by simply feeding the data , y , we wish to condition on to both the generator and discriminator . we show that this model can generate mnist digits conditioned on class labels . we also illustrate how this model could be used to learn a multi-modal model , and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels .", "topics": ["generative model", "mnist database"]}
{"title": "information-theoretic bounded rationality", "abstract": "bounded rationality , that is , decision-making and planning under resource limitations , is widely regarded as an important open problem in artificial intelligence , reinforcement learning , computational neuroscience and economics . this paper offers a consolidated presentation of a theory of bounded rationality based on information-theoretic ideas . we provide a conceptual justification for using the free energy functional as the objective function for characterizing bounded-rational decisions . this functional possesses three crucial properties : it controls the size of the solution space ; it has monte carlo planners that are exact , yet bypass the need for exhaustive search ; and it captures model uncertainty arising from lack of evidence or from interacting with other agents having unknown intentions . we discuss the single-step decision-making case , and show how to extend it to sequential decisions using equivalence transformations . this extension yields a very general class of decision problems that encompass classical decision rules ( e.g . expectimax and minimax ) as limit cases , as well as trust- and risk-sensitive planning .", "topics": ["regret ( decision theory )", "calculus of variations"]}
{"title": "pac-bayesian majority vote for late classifier fusion", "abstract": "a lot of attention has been devoted to multimedia indexing over the past few years . in the literature , we often consider two kinds of fusion schemes : the early fusion and the late fusion . in this paper we focus on late classifier fusion , where one combines the scores of each modality at the decision level . to tackle this problem , we investigate a recent and elegant well-founded quadratic program named mincq coming from the machine learning pac-bayes theory . mincq looks for the weighted combination , over a set of real-valued functions seen as voters , leading to the lowest misclassification rate , while making use of the voters ' diversity . we provide evidence that this method is naturally adapted to late fusion procedure . we propose an extension of mincq by adding an order- preserving pairwise loss for ranking , helping to improve mean averaged precision measure . we confirm the good behavior of the mincq-based fusion approaches with experiments on a real image benchmark .", "topics": ["bayesian network"]}
{"title": "there and back again : a general approach to learning sparse models", "abstract": "we propose a simple and efficient approach to learning sparse models . our approach consists of ( 1 ) projecting the data into a lower dimensional space , ( 2 ) learning a dense model in the lower dimensional space , and then ( 3 ) recovering the sparse model in the original space via compressive sensing . we apply this approach to non-negative matrix factorization ( nmf ) , tensor decomposition and linear classification -- -showing that it obtains $ 10\\times $ compression with negligible loss in accuracy on real data , and obtains up to $ 5\\times $ speedups . our main theoretical contribution is to show the following result for nmf : if the original factors are sparse , then their projections are the sparsest solutions to the projected nmf problem . this explains why our method works for nmf and shows an interesting new property of random projections : they can preserve the solutions of non-convex optimization problems such as nmf .", "topics": ["sparse matrix"]}
{"title": "computing human-understandable strategies", "abstract": "algorithms for equilibrium computation generally make no attempt to ensure that the computed strategies are understandable by humans . for instance the strategies for the strongest poker agents are represented as massive binary files . in many situations , we would like to compute strategies that can actually be implemented by humans , who may have computational limitations and may only be able to remember a small number of features or components of the strategies that have been computed . we study poker games where private information distributions can be arbitrary . we create a large training set of game instances and solutions , by randomly selecting the information probabilities , and present algorithms that learn from the training instances in order to perform well in games with unseen information distributions . we are able to conclude several new fundamental rules about poker strategy that can be easily implemented by humans .", "topics": ["computation"]}
{"title": "investigating the working of text classifiers", "abstract": "text classification is one of the most widely studied task in natural language processing . recently , larger and larger multilayer neural network models are employed for the task motivated by the principle of compositionality . almost all of the methods reported use discriminative approaches for the task . discriminative approaches come with a caveat that if there is no proper capacity control , it might latch on to any signal even though it might not generalize . with use of various state-of-the-art approaches for text classifiers , we want to explore if the models actually learn to compose meaning of the sentences or still just use some key lexicons . to test our hypothesis , we construct datasets where the train and test split have no direct overlap of such lexicons . we study various text classifiers and observe that there is a big performance drop on these datasets . finally , we show that even simple regularization techniques can improve performance on these datasets .", "topics": ["natural language processing", "matrix regularization"]}
{"title": "face parsing via recurrent propagation", "abstract": "face parsing is an important problem in computer vision that finds numerous applications including recognition and editing . recently , deep convolutional neural networks ( cnns ) have been applied to image parsing and segmentation with the state-of-the-art performance . in this paper , we propose a face parsing algorithm that combines hierarchical representations learned by a cnn , and accurate label propagations achieved by a spatially variant recurrent neural network ( rnn ) . the rnn-based propagation approach enables efficient inference over a global space with the guidance of semantic edges generated by a local convolutional model . since the convolutional architecture can be shallow and the spatial rnn can have few parameters , the framework is much faster and more light-weighted than the state-of-the-art cnns for the same task . we apply the proposed model to coarse-grained and fine-grained face parsing . for fine-grained face parsing , we develop a two-stage approach by first identifying the main regions and then segmenting the detail components , which achieves better performance in terms of accuracy and efficiency . with a single gpu , the proposed algorithm parses face images accurately at 300 frames per second , which facilitates real-time applications .", "topics": ["feature learning", "recurrent neural network"]}
{"title": "a ga based window selection methodology to enhance window based multi wavelet transformation and thresholding aided ct image denoising technique", "abstract": "image denoising is getting more significance , especially in computed tomography ( ct ) , which is an important and most common modality in medical imaging . this is mainly due to that the effectiveness of clinical diagnosis using ct image lies on the image quality . the denoising technique for ct images using window-based multi-wavelet transformation and thresholding shows the effectiveness in denoising , however , a drawback exists in selecting the closer windows in the process of window-based multi-wavelet transformation and thresholding . generally , the windows of the duplicate noisy image that are closer to each window of original noisy image are obtained by the checking them sequentially . this leads to the possibility of missing out very closer windows and so enhancement is required in the aforesaid process of the denoising technique . in this paper , we propose a ga-based window selection methodology to include the denoising technique . with the aid of the ga-based window selection methodology , the windows of the duplicate noisy image that are very closer to every window of the original noisy image are extracted in an effective manner . by incorporating the proposed ga-based window selection methodology , the denoising the ct image is performed effectively . eventually , a comparison is made between the denoising technique with and without the proposed ga-based window selection methodology .", "topics": ["noise reduction"]}
{"title": "believe it or not : adding belief annotations to databases", "abstract": "we propose a database model that allows users to annotate data with belief statements . our motivation comes from scientific database applications where a community of users is working together to assemble , revise , and curate a shared data repository . as the community accumulates knowledge and the database content evolves over time , it may contain conflicting information and members can disagree on the information it should store . for example , alice may believe that a tuple should be in the database , whereas bob disagrees . he may also insert the reason why he thinks alice believes the tuple should be in the database , and explain what he thinks the correct tuple should be instead . we propose a formal model for belief databases that interprets users ' annotations as belief statements . these annotations can refer both to the base data and to other annotations . we give a formal semantics based on a fragment of multi-agent epistemic logic and define a query language over belief databases . we then prove a key technical result , stating that every belief database can be encoded as a canonical kripke structure . we use this structure to describe a relational representation of belief databases , and give an algorithm for translating queries over the belief database into standard relational queries . finally , we report early experimental results with our prototype implementation on synthetic data .", "topics": ["synthetic data", "database"]}
{"title": "multi-objective active control policy design for commensurate and incommensurate fractional order chaotic financial systems", "abstract": "in this paper , an active control policy design for a fractional order ( fo ) financial system is attempted , considering multiple conflicting objectives . an active control template as a nonlinear state feedback mechanism is developed and the controller gains are chosen within a multi-objective optimization ( moo ) framework to satisfy the conditions of asymptotic stability , derived analytically . the moo gives a set of solutions on the pareto optimal front for the multiple conflicting objectives that are considered . it is shown that there is a trade-off between the multiple design objectives and a better performance in one objective can only be obtained at the cost of performance deterioration in the other objectives . the multi-objective controller design has been compared using three different moo techniques viz . non dominated sorting genetic algorithm-ii ( nsga-ii ) , epsilon variable multi-objective genetic algorithm ( ev-moga ) , and multi objective evolutionary algorithm with decomposition ( moea/d ) . the robustness of the same control policy designed with the nominal system settings have been investigated also for gradual decrease in the commensurate and incommensurate fractional orders of the financial system .", "topics": ["nonlinear system"]}
{"title": "learning and querying fast generative models for reinforcement learning", "abstract": "a key challenge in model-based reinforcement learning ( rl ) is to synthesize computationally efficient and accurate environment models . we show that carefully designed generative models that learn and operate on compact state representations , so-called state-space models , substantially reduce the computational costs for predicting outcomes of sequences of actions . extensive experiments establish that state-space models accurately capture the dynamics of atari games from the arcade learning environment from raw pixels . the computational speed-up of state-space models while maintaining high accuracy makes their application in rl feasible : we demonstrate that agents which query these models for decision making outperform strong model-free baselines on the game mspacman , demonstrating the potential of using learned environment models for planning .", "topics": ["generative model", "computational complexity theory"]}
{"title": "advancing acoustic-to-word ctc model", "abstract": "the acoustic-to-word model based on the connectionist temporal classification ( ctc ) criterion was shown as a natural end-to-end ( e2e ) model directly targeting words as output units . however , the word-based ctc model suffers from the out-of-vocabulary ( oov ) issue as it can only model limited number of words in the output layer and maps all the remaining words into an oov output node . hence , such a word-based ctc model can only recognize the frequent words modeled by the network output nodes . our first attempt to improve the acoustic-to-word model is a hybrid ctc model which consults a letter-based ctc when the word-based ctc model emits oov tokens during testing time . then , we propose a much better solution by training a mixed-unit ctc model which decomposes all the oov words into sequences of frequent words and multi-letter units . evaluated on a 3400 hours microsoft cortana voice assistant task , the final acoustic-to-word solution improves the baseline word-based ctc by relative 12.09 % word error rate ( wer ) reduction when combined with our proposed attention ctc . such an e2e model without using any language model ( lm ) or complex decoder outperforms the traditional context-dependent phoneme ctc which has strong lm and decoder by relative 6.79 % .", "topics": ["baseline ( configuration management )", "map"]}
{"title": "greedy algorithms for cone constrained optimization with convergence guarantees", "abstract": "greedy optimization methods such as matching pursuit ( mp ) and frank-wolfe ( fw ) algorithms regained popularity in recent years due to their simplicity , effectiveness and theoretical guarantees . mp and fw address optimization over the linear span and the convex hull of a set of atoms , respectively . in this paper , we consider the intermediate case of optimization over the convex cone , parametrized as the conic hull of a generic atom set , leading to the first principled definitions of non-negative mp algorithms for which we give explicit convergence rates and demonstrate excellent empirical performance . in particular , we derive sublinear ( $ \\mathcal { o } ( 1/t ) $ ) convergence on general smooth and convex objectives , and linear convergence ( $ \\mathcal { o } ( e^ { -t } ) $ ) on strongly convex objectives , in both cases for general sets of atoms . furthermore , we establish a clear correspondence of our algorithms to known algorithms from the mp and fw literature . our novel algorithms and analyses target general atom sets and general objective functions , and hence are directly applicable to a large variety of learning settings .", "topics": ["mathematical optimization"]}
{"title": "neural machine translation training in a multi-domain scenario", "abstract": "in this paper , we explore alternative ways to train a neural machine translation system in a multi-domain scenario . we investigate data concatenation ( with fine tuning ) , model stacking ( multi-level fine tuning ) , data selection and weighted ensemble . we evaluate these methods based on three criteria : i ) translation quality , ii ) training time , and iii ) robustness towards out-of-domain tests . our findings on arabic-english and german-english language pairs show that the best translation quality can be achieved by building an initial system on a concatenation of available out-of-domain data and then fine-tuning it on in-domain data . model stacking works best when training begins with the furthest out-of-domain data and the model is incrementally fine-tuned with the next furthest domain and so on . data selection did not give the best results , but can be considered as a decent compromise between training time and translation quality . a weighted ensemble of different individual models performed better than data selection . it is beneficial in a scenario when there is no time for fine-tuning .", "topics": ["machine translation"]}
{"title": "convergence of stochastic gradient descent for pca", "abstract": "we consider the problem of principal component analysis ( pca ) in a streaming stochastic setting , where our goal is to find a direction of approximate maximal variance , based on a stream of i.i.d . data points in $ \\reals^d $ . a simple and computationally cheap algorithm for this is stochastic gradient descent ( sgd ) , which incrementally updates its estimate based on each new data point . however , due to the non-convex nature of the problem , analyzing its performance has been a challenge . in particular , existing guarantees rely on a non-trivial eigengap assumption on the covariance matrix , which is intuitively unnecessary . in this paper , we provide ( to the best of our knowledge ) the first eigengap-free convergence guarantees for sgd in the context of pca . this also partially resolves an open problem posed in \\cite { hardt2014noisy } . moreover , under an eigengap assumption , we show that the same techniques lead to new sgd convergence guarantees with better dependence on the eigengap .", "topics": ["approximation algorithm", "gradient descent"]}
{"title": "recycling computed answers in rewrite systems for abduction", "abstract": "in rule-based systems , goal-oriented computations correspond naturally to the possible ways that an observation may be explained . in some applications , we need to compute explanations for a series of observations with the same domain . the question whether previously computed answers can be recycled arises . a yes answer could result in substantial savings of repeated computations . for systems based on classic logic , the answer is yes . for nonmonotonic systems however , one tends to believe that the answer should be no , since recycling is a form of adding information . in this paper , we show that computed answers can always be recycled , in a nontrivial way , for the class of rewrite procedures that we proposed earlier for logic programs with negation . we present some experimental results on an encoding of the logistics domain .", "topics": ["computation"]}
{"title": "anomaly detection and motif discovery in symbolic representations of time series", "abstract": "the advent of the big data hype and the consistent recollection of event logs and real-time data from sensors , monitoring software and machine configuration has generated a huge amount of time-varying data in about every sector of the industry . rule-based processing of such data has ceased to be relevant in many scenarios where anomaly detection and pattern mining have to be entirely accomplished by the machine . since the early 2000s , the de-facto standard for representing time series has been the symbolic aggregate approximation ( sax ) .in this document , we present a few algorithms using this representation for anomaly detection and motif discovery , also known as pattern mining , in such data . we propose a benchmark of anomaly detection algorithms using data from cloud monitoring software .", "topics": ["data mining", "time series"]}
{"title": "face recognition using hough peaks extracted from the significant blocks of the gradient image", "abstract": "this paper proposes a new technique for automatic face recognition using integrated peaks of the hough transformed significant blocks of the binary gradient image . in this approach firstly the gradient of an image is calculated and a threshold is set to obtain a binary gradient image , which is less sensitive to noise and illumination changes . secondly , significant blocks are extracted from the absolute gradient image , to extract pertinent information with the idea of dimension reduction . finally the best fitted hough peaks are extracted from the hough transformed significant blocks for efficient face recognition . then these hough peaks are concatenated together , which are used as feature in classification process . the efficiency of the proposed method is demonstrated by the experiment on 1100 images from the frav2d face database , 2200 images from the feret database , where the images vary in pose , expression , illumination and scale and 400 images from the orl face database , where the images slightly vary in pose . our method has shown 93.3 % , 88.5 % and 99 % recognition accuracy for the frav2d , feret and the orl database respectively .", "topics": ["gradient"]}
{"title": "hierarchical manifold clustering on diffusion maps for connectomics ( mit 18.s096 final project )", "abstract": "in this paper , we introduce a novel algorithm for segmentation of imperfect boundary probability maps ( bpm ) in connectomics . our algorithm can be a considered as an extension of spectral clustering . instead of clustering the diffusion maps with traditional clustering algorithms , we learn the manifold and compute an estimate of the minimum normalized cut . we proceed by divide and conquer . we also introduce a novel criterion for determining if further splits are necessary in a component based on it 's topological properties . our algorithm complements the currently popular agglomeration approaches in connectomics , which overlook the geometrical aspects of this segmentation problem .", "topics": ["cluster analysis", "map"]}
{"title": "improving robustness of feature representations to image deformations using powered convolution in cnns", "abstract": "in this work , we address the problem of improvement of robustness of feature representations learned using convolutional neural networks ( cnns ) to image deformation . we argue that higher moment statistics of feature distributions could be shifted due to image deformations , and the shift leads to degrade of performance and can not be reduced by ordinary normalization methods as observed in experimental analyses . in order to attenuate this effect , we apply additional non-linearity in cnns by combining power functions with learnable parameters into convolution operation . in the experiments , we observe that cnns which employ the proposed method obtain remarkable boost in both the generalization performance and the robustness under various types of deformations using large scale benchmark datasets . for instance , a model equipped with the proposed method obtains 3.3\\ % performance boost in map on pascal voc object detection task using deformed images , compared to the reference model , while both models provide the same performance using original images . to the best of our knowledge , this is the first work that studies robustness of deep features learned using cnns to a wide range of deformations for object recognition and detection .", "topics": ["object detection", "nonlinear system"]}
{"title": "efficient learning by directed acyclic graph for resource constrained prediction", "abstract": "we study the problem of reducing test-time acquisition costs in classification systems . our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction . we model our system as a directed acyclic graph ( dag ) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements . this problem can be naturally posed as an empirical risk minimization over training data . rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes , we propose an efficient algorithm motivated by dynamic programming . we learn node policies in the dag by reducing the global objective to a series of cost sensitive learning problems . our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture . in addition , we present an extension to map other budgeted learning problems with large number of sensors to our dag architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors .", "topics": ["test set", "computational complexity theory"]}
{"title": "confidentcare : a clinical decision support system for personalized breast cancer screening", "abstract": "breast cancer screening policies attempt to achieve timely diagnosis by the regular screening of apparently healthy women . various clinical decisions are needed to manage the screening process ; those include : selecting the screening tests for a woman to take , interpreting the test outcomes , and deciding whether or not a woman should be referred to a diagnostic test . such decisions are currently guided by clinical practice guidelines ( cpgs ) , which represent a one-size-fits-all approach that are designed to work well on average for a population , without guaranteeing that it will work well uniformly over that population . since the risks and benefits of screening are functions of each patients features , personalized screening policies that are tailored to the features of individuals are needed in order to ensure that the right tests are recommended to the right woman . in order to address this issue , we present confidentcare : a computer-aided clinical decision support system that learns a personalized screening policy from the electronic health record ( ehr ) data . confidentcare operates by recognizing clusters of similar patients , and learning the best screening policy to adopt for each cluster . a cluster of patients is a set of patients with similar features ( e.g . age , breast density , family history , etc . ) , and the screening policy is a set of guidelines on what actions to recommend for a woman given her features and screening test scores . confidentcare algorithm ensures that the policy adopted for every cluster of patients satisfies a predefined accuracy requirement with a high level of confidence . we show that our algorithm outperforms the current cpgs in terms of cost-efficiency and false positive rates .", "topics": ["cluster analysis", "supervised learning"]}
{"title": "shadows and headless shadows : a worlds-based , autobiographical approach to reasoning", "abstract": "many cognitive systems deploy multiple , closed , individually consistent models which can represent interpretations of the present state of the world , moments in the past , possible futures or alternate versions of reality . while they appear under different names , these structures can be grouped under the general term of worlds . the xapagy architecture is a story-oriented cognitive system which relies exclusively on the autobiographical memory implemented as a raw collection of events organized into world-type structures called { \\em scenes } . the system performs reasoning by shadowing current events with events from the autobiography . the shadows are then extrapolated into headless shadows corresponding to predictions , hidden events or inferred relations .", "topics": ["artificial intelligence"]}
{"title": "improving discourse relation projection to build discourse annotated corpora", "abstract": "the naive approach to annotation projection is not effective to project discourse annotations from one language to another because implicit discourse relations are often changed to explicit ones and vice-versa in the translation . in this paper , we propose a novel approach based on the intersection between statistical word-alignment models to identify unsupported discourse annotations . this approach identified 65 % of the unsupported annotations in the english-french parallel sentences from europarl . by filtering out these unsupported annotations , we induced the first pdtb-style discourse annotated corpus for french from europarl . we then used this corpus to train a classifier to identify the discourse-usage of french discourse connectives and show a 15 % improvement of f1-score compared to the classifier trained on the non-filtered annotations .", "topics": ["text corpus"]}
{"title": "learning non-deterministic representations with energy-based ensembles", "abstract": "the goal of a generative model is to capture the distribution underlying the data , typically through latent variables . after training , these variables are often used as a new representation , more effective than the original features in a variety of learning tasks . however , the representations constructed by contemporary generative models are usually point-wise deterministic mappings from the original feature space . thus , even with representations robust to class-specific transformations , statistically driven models trained on them would not be able to generalize when the labeled data is scarce . inspired by the stochasticity of the synaptic connections in the brain , we introduce energy-based stochastic ensembles . these ensembles can learn non-deterministic representations , i.e . , mappings from the feature space to a family of distributions in the latent space . these mappings are encoded in a distribution over a ( possibly infinite ) collection of models . by conditionally sampling models from the ensemble , we obtain multiple representations for every input example and effectively augment the data . we propose an algorithm similar to contrastive divergence for training restricted boltzmann stochastic ensembles . finally , we demonstrate the concept of the stochastic representations on a synthetic dataset as well as test them in the one-shot learning scenario on mnist .", "topics": ["sampling ( signal processing )", "generative model"]}
{"title": "being negative but constructively : lessons learnt from creating better visual question answering datasets", "abstract": "visual question answering ( qa ) has attracted a lot of attention lately , seen essentially as a form of ( visual ) turing test that artificial intelligence should strive to achieve . in this paper , we study a crucial component of this task : how can we design good datasets for the task ? we focus on the design of multiple-choice based datasets where the learner has to select the right answer from a set of candidate ones including the target ( i.e . the correct one ) and the decoys ( i.e . the incorrect ones ) . through careful analysis of the results attained by state-of-the-art learning models and human annotators on existing datasets , we show the design of the decoy answers has a significant impact on how and what the learning models learn from the datasets . in particular , the resulting learner can ignore the visual information , the question , or the both while still doing well on the task . inspired by this , we propose automatic procedures to remedy such design deficiencies . we apply the procedures to re-construct decoy answers for two popular visual qa datasets as well as to create a new visual qa dataset from the visual genome project , resulting in the largest dataset for this task . extensive empirical studies show that the design deficiencies have been alleviated in the remedied datasets and the performance on them is likely a more faithful indicator of the difference among learning models . the datasets are released and publicly available via http : //www.teds.usc.edu/website_vqa/ .", "topics": ["artificial intelligence"]}
{"title": "visual tracking by reinforced decision making", "abstract": "one of the major challenges of model-free visual tracking problem has been the difficulty originating from the unpredictable and drastic changes in the appearance of objects we target to track . existing methods tackle this problem by updating the appearance model on-line in order to adapt to the changes in the appearance . despite the success of these methods however , inaccurate and erroneous updates of the appearance model result in a tracker drift . in this paper , we introduce a novel visual tracking algorithm based on a template selection strategy constructed by deep reinforcement learning methods . the tracking algorithm utilizes this strategy to choose the best template for tracking a given frame . the template selection strategy is self-learned by utilizing a simple policy gradient method on numerous training episodes randomly generated from a tracking benchmark dataset . our proposed reinforcement learning framework is generally applicable to other confidence map based tracking algorithms . the experiment shows that our tracking algorithm effectively decides the best template for visual tracking .", "topics": ["reinforcement learning"]}
{"title": "linear models of computation and program learning", "abstract": "we consider two classes of computations which admit taking linear combinations of execution runs : probabilistic sampling and generalized animation . we argue that the task of program learning should be more tractable for these architectures than for conventional deterministic programs . we look at the recent advances in the `` sampling the samplers '' paradigm in higher-order probabilistic programming . we also discuss connections between partial inconsistency , non-monotonic inference , and vector semantics .", "topics": ["computation"]}
{"title": "finding optimal combination of kernels using genetic programming", "abstract": "in computer vision , problem of identifying or classifying the objects present in an image is called object categorization . it is a challenging problem , especially when the images have clutter background , occlusions or different lighting conditions . many vision features have been proposed which aid object categorization even in such adverse conditions . past research has shown that , employing multiple features rather than any single features leads to better recognition . multiple kernel learning ( mkl ) framework has been developed for learning an optimal combination of features for object categorization . existing mkl methods use linear combination of base kernels which may not be optimal for object categorization . real-world object categorization may need to consider complex combination of kernels ( non-linear ) and not only linear combination . evolving non-linear functions of base kernels using genetic programming is proposed in this report . experiment results show that non-kernel generated using genetic programming gives good accuracy as compared to linear combination of kernels .", "topics": ["nonlinear system", "computer vision"]}
{"title": "price trackers inspired by immune memory", "abstract": "in this paper we outline initial concepts for an immune inspired algorithm to evaluate price time series data . the proposed solution evolves a short term pool of trackers dynamically through a process of proliferation and mutation , with each member attempting to map to trends in price movements . successful trackers feed into a long term memory pool that can generalise across repeating trend patterns . tests are performed to examine the algorithm 's ability to successfully identify trends in a small data set . the influence of the long term memory pool is then examined . we find the algorithm is able to identify price trends presented successfully and efficiently .", "topics": ["time series"]}
{"title": "causal decision trees", "abstract": "uncovering causal relationships in data is a major objective of data analytics . causal relationships are normally discovered with designed experiments , e.g . randomised controlled trials , which , however are expensive or infeasible to be conducted in many cases . causal relationships can also be found using some well designed observational studies , but they require domain experts ' knowledge and the process is normally time consuming . hence there is a need for scalable and automated methods for causal relationship exploration in data . classification methods are fast and they could be practical substitutes for finding causal signals in data . however , classification methods are not designed for causal discovery and a classification method may find false causal signals and miss the true ones . in this paper , we develop a causal decision tree where nodes have causal interpretations . our method follows a well established causal inference framework and makes use of a classic statistical test . the method is practical for finding causal signals in large data sets .", "topics": ["scalability", "causality"]}
{"title": "screening rules for overlapping group lasso", "abstract": "recently , to solve large-scale lasso and group lasso problems , screening rules have been developed , the goal of which is to reduce the problem size by efficiently discarding zero coefficients using simple rules independently of the others . however , screening for overlapping group lasso remains an open challenge because the overlaps between groups make it infeasible to test each group independently . in this paper , we develop screening rules for overlapping group lasso . to address the challenge arising from groups with overlaps , we take into account overlapping groups only if they are inclusive of the group being tested , and then we derive screening rules , adopting the dual polytope projection approach . this strategy allows us to screen each group independently of each other . in our experiments , we demonstrate the efficiency of our screening rules on various datasets .", "topics": ["coefficient"]}
{"title": "two remarkable computational competencies of the simple genetic algorithm", "abstract": "since the inception of genetic algorithmics the identification of computational efficiencies of the simple genetic algorithm ( sga ) has been an important goal . in this paper we distinguish between a computational competency of the sga -- an efficient , but narrow computational ability -- and a computational proficiency of the sga -- a computational ability that is both efficient and broad . till date , attempts to deduce a computational proficiency of the sga have been unsuccessful . it may , however , be possible to inductively infer a computational proficiency of the sga from a set of related computational competencies that have been deduced . with this in mind we deduce two computational competencies of the sga . these competencies , when considered together , point toward a remarkable computational proficiency of the sga . this proficiency is pertinent to a general problem that is closely related to a well-known statistical problem at the cutting edge of computational genetics .", "topics": ["data mining"]}
{"title": "the euler-poincare theory of metamorphosis", "abstract": "in the pattern matching approach to imaging science , the process of `` metamorphosis '' is template matching with dynamical templates . here , we recast the metamorphosis equations of into the euler-poincare variational framework of and show that the metamorphosis equations contain the equations for a perfect complex fluid \\cite { ho2002 } . this result connects the ideas underlying the process of metamorphosis in image matching to the physical concept of order parameter in the theory of complex fluids . after developing the general theory , we reinterpret various examples , including point set , image and density metamorphosis . we finally discuss the issue of matching measures with metamorphosis , for which we provide existence theorems for the initial and boundary value problems .", "topics": ["calculus of variations"]}
{"title": "online action detection", "abstract": "in online action detection , the goal is to detect the start of an action in a video stream as soon as it happens . for instance , if a child is chasing a ball , an autonomous car should recognize what is going on and respond immediately . this is a very challenging problem for four reasons . first , only partial actions are observed . second , there is a large variability in negative data . third , the start of the action is unknown , so it is unclear over what time window the information should be integrated . finally , in real world data , large within-class variability exists . this problem has been addressed before , but only to some extent . our contributions to online action detection are threefold . first , we introduce a realistic dataset composed of 27 episodes from 6 popular tv series . the dataset spans over 16 hours of footage annotated with 30 action classes , totaling 6,231 action instances . second , we analyze and compare various baseline methods , showing this is a challenging problem for which none of the methods provides a good solution . third , we analyze the change in performance when there is a variation in viewpoint , occlusion , truncation , etc . we introduce an evaluation protocol for fair comparison . the dataset , the baselines and the models will all be made publicly available to encourage ( much needed ) further research on online action detection on realistic data .", "topics": ["baseline ( configuration management )", "computer vision"]}
{"title": "computational limits for matrix completion", "abstract": "matrix completion is the problem of recovering an unknown real-valued low-rank matrix from a subsample of its entries . important recent results show that the problem can be solved efficiently under the assumption that the unknown matrix is incoherent and the subsample is drawn uniformly at random . are these assumptions necessary ? it is well known that matrix completion in its full generality is np-hard . however , little is known if make additional assumptions such as incoherence and permit the algorithm to output a matrix of slightly higher rank . in this paper we prove that matrix completion remains computationally intractable even if the unknown matrix has rank $ 4 $ but we are allowed to output any constant rank matrix , and even if additionally we assume that the unknown matrix is incoherent and are shown $ 90 % $ of the entries . this result relies on the conjectured hardness of the $ 4 $ -coloring problem . we also consider the positive semidefinite matrix completion problem . here we show a similar hardness result under the standard assumption that $ \\mathrm { p } \\ne \\mathrm { np } . $ our results greatly narrow the gap between existing feasibility results and computational lower bounds . in particular , we believe that our results give the first complexity-theoretic justification for why distributional assumptions are needed beyond the incoherence assumption in order to obtain positive results . on the technical side , we contribute several new ideas on how to encode hard combinatorial problems in low-rank optimization problems . we hope that these techniques will be helpful in further understanding the computational limits of matrix completion and related problems .", "topics": ["computational complexity theory", "optimization problem"]}
{"title": "harassment detection : a benchmark on the # hackharassment dataset", "abstract": "online harassment has been a problem to a greater or lesser extent since the early days of the internet . previous work has applied anti-spam techniques like machine-learning based text classification ( reynolds , 2011 ) to detecting harassing messages . however , existing public datasets are limited in size , with labels of varying quality . the # hackharassment initiative ( an alliance of 1 tech companies and ngos devoted to fighting bullying on the internet ) has begun to address this issue by creating a new dataset superior to its predecssors in terms of both size and quality . as we ( # hackharassment ) complete further rounds of labelling , later iterations of this dataset will increase the available samples by at least an order of magnitude , enabling corresponding improvements in the quality of machine learning models for harassment detection . in this paper , we introduce the first models built on the # hackharassment dataset v1.0 ( a new open dataset , which we are delighted to share with any interested researcherss ) as a benchmark for future research .", "topics": ["iteration"]}
{"title": "variable forgetting in reasoning about knowledge", "abstract": "in this paper , we investigate knowledge reasoning within a simple framework called knowledge structure . we use variable forgetting as a basic operation for one agent to reason about its own or other agents\\ knowledge . in our framework , two notions namely agents\\ observable variables and the weakest sufficient condition play important roles in knowledge reasoning . given a background knowledge base and a set of observable variables for each agent , we show that the notion of an agent knowing a formula can be defined as a weakest sufficient condition of the formula under background knowledge base . moreover , we show how to capture the notion of common knowledge by using a generalized notion of weakest sufficient condition . also , we show that public announcement operator can be conveniently dealt with via our notion of knowledge structure . further , we explore the computational complexity of the problem whether an epistemic formula is realized in a knowledge structure . in the general case , this problem is pspace-hard ; however , for some interesting subcases , it can be reduced to co-np . finally , we discuss possible applications of our framework in some interesting domains such as the automated analysis of the well-known muddy children puzzle and the verification of the revised needham-schroeder protocol . we believe that there are many scenarios where the natural presentation of the available information about knowledge is under the form of a knowledge structure . what makes it valuable compared with the corresponding multi-agent s5 kripke structure is that it can be much more succinct .", "topics": ["computational complexity theory"]}
{"title": "real time clustering of time series using triangular potentials", "abstract": "motivated by the problem of computing investment portfolio weightings we investigate various methods of clustering as alternatives to traditional mean-variance approaches . such methods can have significant benefits from a practical point of view since they remove the need to invert a sample covariance matrix , which can suffer from estimation error and will almost certainly be non-stationary . the general idea is to find groups of assets which share similar return characteristics over time and treat each group as a single composite asset . we then apply inverse volatility weightings to these new composite assets . in the course of our investigation we devise a method of clustering based on triangular potentials and we present associated theoretical results as well as various examples based on synthetic data .", "topics": ["cluster analysis", "time series"]}
{"title": "amplitude-based approach to evidence accumulation", "abstract": "we point out the need to use probability amplitudes rather than probabilities to model evidence accumulation in decision processes involving real physical sensors . optical information processing systems are given as typical examples of systems that naturally gather evidence in this manner . we derive a new , amplitude-based generalization of the hough transform technique used for object recognition in machine vision . we argue that one should use complex hough accumulators and square their magnitudes to get a proper probabilistic interpretation of the likelihood that an object is present . finally , we suggest that probability amplitudes may have natural applications in connectionist models , as well as in formulating knowledge-based reasoning problems .", "topics": ["sensor"]}
{"title": "applications of data mining to electronic commerce", "abstract": "electronic commerce is emerging as the killer domain for data mining technology . the following are five desiderata for success . seldom are they they all present in one data mining application . 1 . data with rich descriptions . for example , wide customer records with many potentially useful fields allow data mining algorithms to search beyond obvious correlations . 2 . a large volume of data . the large model spaces corresponding to rich data demand many training instances to build reliable models . 3 . controlled and reliable data collection . manual data entry and integration from legacy systems both are notoriously problematic ; fully automated collection is considerably better . 4 . the ability to evaluate results . substantial , demonstrable return on investment can be very convincing . 5 . ease of integration with existing processes . even if pilot studies show potential benefit , deploying automated solutions to previously manual processes is rife with pitfalls . building a system to take advantage of the mined knowledge can be a substantial undertaking . furthermore , one often must deal with social and political issues involved in the automation of a previously manual business process .", "topics": ["data mining", "artificial intelligence"]}
{"title": "soil data analysis using classification techniques and soil attribute prediction", "abstract": "agricultural research has been profited by technical advances such as automation , data mining . today , data mining is used in a vast areas and many off-the-shelf data mining system products and domain specific data mining application soft wares are available , but data mining in agricultural soil datasets is a relatively a young research field . the large amounts of data that are nowadays virtually harvested along with the crops have to be analyzed and should be used to their full extent . this research aims at analysis of soil dataset using data mining techniques . it focuses on classification of soil using various algorithms available . another important purpose is to predict untested attributes using regression technique , and implementation of automated soil sample classification .", "topics": ["data mining"]}
{"title": "enhancing transparency of black-box soft-margin svm by integrating data-based prior information", "abstract": "the lack of transparency often makes the black-box models difficult to be applied to many practical domains . for this reason , the current work , from the black-box model input port , proposes to incorporate data-based prior information into the black-box soft-margin svm model to enhance its transparency . the concept and incorporation mechanism of data-based prior information are successively developed , based on which the transparent or partly transparent svm optimization model is designed and then solved through handily rewriting the optimization problem as a nonlinear quadratic programming problem . an algorithm for mining data-based linear prior information from data set is also proposed , which generates a linear expression with respect to two appropriate inputs identified from all inputs of system . at last , the proposed transparency strategy is applied to eight benchmark examples and two real blast furnace examples for effectiveness exhibition .", "topics": ["optimization problem", "support vector machine"]}
{"title": "on pairwise clustering with side information", "abstract": "pairwise clustering , in general , partitions a set of items via a known similarity function . in our treatment , clustering is modeled as a transductive prediction problem . thus rather than beginning with a known similarity function , the function instead is hidden and the learner only receives a random sample consisting of a subset of the pairwise similarities . an additional set of pairwise side-information may be given to the learner , which then determines the inductive bias of our algorithms . we measure performance not based on the recovery of the hidden similarity function , but instead on how well we classify each item . we give tight bounds on the number of misclassifications . we provide two algorithms . the first algorithm saca is a simple agglomerative clustering algorithm which runs in near linear time , and which serves as a baseline for our analyses . whereas the second algorithm , rgca , enables the incorporation of side-information which may lead to improved bounds at the cost of a longer running time .", "topics": ["baseline ( configuration management )", "cluster analysis"]}
{"title": "new trends in neutrosophic theory and applications", "abstract": "neutrosophic theory and applications have been expanding in all directions at an astonishing rate especially after the introduction the journal entitled neutrosophic sets and systems . new theories , techniques , algorithms have been rapidly developed . one of the most striking trends in the neutrosophic theory is the hybridization of neutrosophic set with other potential sets such as rough set , bipolar set , soft set , hesitant fuzzy set , etc . the different hybrid structure such as rough neutrosophic set , single valued neutrosophic rough set , bipolar neutrosophic set , single valued neutrosophic hesitant fuzzy set , etc . are proposed in the literature in a short period of time . neutrosophic set has been a very important tool in all various areas of data mining , decision making , e-learning , engineering , medicine , social science , and some more . the book new trends in neutrosophic theories and applications focuses on theories , methods , algorithms for decision making and also applications involving neutrosophic information . some topics deal with data mining , decision making , e-learning , graph theory , medical diagnosis , probability theory , topology , and some more .", "topics": ["data mining", "supervised learning"]}
{"title": "articulated motion discovery using pairs of trajectories", "abstract": "we propose an unsupervised approach for discovering characteristic motion patterns in videos of highly articulated objects performing natural , unscripted behaviors , such as tigers in the wild . we discover consistent patterns in a bottom-up manner by analyzing the relative displacements of large numbers of ordered trajectory pairs through time , such that each trajectory is attached to a different moving part on the object . the pairs of trajectories descriptor relies entirely on motion and is more discriminative than state-of-the-art features that employ single trajectories . our method generates temporal video intervals , each automatically trimmed to one instance of the discovered behavior , and clusters them by type ( e.g . , running , turning head , drinking water ) . we present experiments on two datasets : dogs from youtube-objects and a new dataset of national geographic tiger videos . results confirm that our proposed descriptor outperforms existing appearance- and trajectory-based descriptors ( e.g . , hog and dtfs ) on both datasets and enables us to segment unconstrained animal video into intervals containing single behaviors .", "topics": ["unsupervised learning"]}
{"title": "sequential lifted bayesian filtering in multiset rewriting systems", "abstract": "bayesian filtering for plan and activity recognition is challenging for scenarios that contain many observation equivalent entities ( i.e . entities that produce the same observations ) . this is due to the combinatorial explosion in the number of hypotheses that need to be tracked . however , this class of problems exhibits a certain symmetry that can be exploited for state space representation and inference . we analyze current state of the art methods and find that none of them completely fits the requirements arising in this problem class . we sketch a novel inference algorithm that provides a solution by incorporating concepts from lifted inference algorithms , probabilistic multiset rewriting systems , and computational state space models . two experiments confirm that this novel algorithm has the potential to perform efficient probabilistic inference on this problem class .", "topics": ["entity"]}
{"title": "neural skill transfer from supervised language tasks to reading comprehension", "abstract": "reading comprehension is a challenging task in natural language processing and requires a set of skills to be solved . while current approaches focus on solving the task as a whole , in this paper , we propose to use a neural network `skill ' transfer approach . we transfer knowledge from several lower-level language tasks ( skills ) including textual entailment , named entity recognition , paraphrase detection and question type classification into the reading comprehension model . we conduct an empirical evaluation and show that transferring language skill knowledge leads to significant improvements for the task with much fewer steps compared to the baseline model . we also show that the skill transfer approach is effective even with small amounts of training data . another finding of this work is that using token-wise deep label supervision for text classification improves the performance of transfer learning .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "processing of test matrices with guessing correction", "abstract": "it is suggested to insert into test matrix 1s for correct responses , 0s for response refusals , and negative corrective elements for incorrect responses . with the classical test theory approach test scores of examinees and items are calculated traditionally as sums of matrix elements , organized in rows and columns . correlation coefficients are estimated using correction coefficients . in item response theory approach examinee and item logits are estimated using maximum likelihood method and probabilities of all matrix elements .", "topics": ["coefficient"]}
{"title": "learning deep structured multi-scale features using attention-gated crfs for contour prediction", "abstract": "recent works have shown that exploiting multi-scale representations deeply learned via convolutional neural networks ( cnn ) is of tremendous importance for accurate contour detection . this paper presents a novel approach for predicting contours which advances the state of the art in two fundamental aspects , i.e . multi-scale feature generation and fusion . different from previous works directly consider- ing multi-scale feature maps obtained from the inner layers of a primary cnn architecture , we introduce a hierarchical deep model which produces more rich and complementary representations . furthermore , to refine and robustly fuse the representations learned at different scales , the novel attention-gated conditional random fields ( ag-crfs ) are proposed . the experiments ran on two publicly available datasets ( bsds500 and nyudv2 ) demonstrate the effectiveness of the latent ag-crf model and of the overall hierarchical framework .", "topics": ["map"]}
{"title": "is there a role for qualitative risk assessment ?", "abstract": "classically , risk is characterized by a point value probability indicating the likelihood of occurrence of an adverse effect . however , there are domains where the attainability of objective numerical risk characterizations is increasingly being questioned . this paper reviews the arguments in favour of extending classical techniques of risk assessment to incorporate meaningful qualitative and weak quantitative risk characterizations . a technique in which linguistic uncertainty terms are defined in terms of patterns of argument is then proposed . the technique is demonstrated using a prototype computer-based system for predicting the carcinogenic risk due to novel chemical compounds .", "topics": ["numerical analysis"]}
{"title": "video captioning via hierarchical reinforcement learning", "abstract": "video captioning is the task of automatically generating a textual description of the actions in a video . although previous work ( e.g . sequence-to-sequence model ) has shown promising results in abstracting a coarse description of a short video , it is still very challenging to caption a video containing multiple fine-grained actions with a detailed description . this paper aims to address the challenge by proposing a novel hierarchical reinforcement learning framework for video captioning , where a high-level manager module learns to design sub-goals and a low-level worker module recognizes the primitive actions to fulfill the sub-goal . with this compositional framework to reinforce video captioning at different levels , our approach significantly outperforms all the baseline methods on a newly introduced large-scale dataset for fine-grained video captioning . furthermore , our non-ensemble model has already achieved the state-of-the-art results on the widely-used msr-vtt dataset .", "topics": ["baseline ( configuration management )", "high- and low-level"]}
{"title": "an evolutionary model with turing machines", "abstract": "the development of a large non-coding fraction in eukaryotic dna and the phenomenon of the code-bloat in the field of evolutionary computations show a striking similarity . this seems to suggest that ( in the presence of mechanisms of code growth ) the evolution of a complex code ca n't be attained without maintaining a large inactive fraction . to test this hypothesis we performed computer simulations of an evolutionary toy model for turing machines , studying the relations among fitness and coding/non-coding ratio while varying mutation and code growth rates . the results suggest that , in our model , having a large reservoir of non-coding states constitutes a great ( long term ) evolutionary advantage .", "topics": ["simulation", "computation"]}
{"title": "swipe mosaics from video", "abstract": "a panoramic image mosaic is an attractive visualization for viewing many overlapping photos , but its images must be both captured and processed correctly to produce an acceptable composite . we propose swipe mosaics , an interactive visualization that places the individual video frames on a 2d planar map that represents the layout of the physical scene . compared to traditional panoramic mosaics , our capture is easier because the user can both translate the camera center and film moving subjects . processing and display degrade gracefully if the footage lacks distinct , overlapping , non-repeating texture . our proposed visual odometry algorithm produces a distribution over ( x , y ) translations for image pairs . inferring a distribution of possible camera motions allows us to better cope with parallax , lack of texture , dynamic scenes , and other phenomena that hurt deterministic reconstruction techniques . robustness is obtained by training on synthetic scenes with known camera motions . we show that swipe mosaics are easy to generate , support a wide range of difficult scenes , and are useful for documenting a scene for closer inspection .", "topics": ["synthetic data"]}
{"title": "spatial aggregation of holistically-nested networks for automated pancreas segmentation", "abstract": "accurate automatic organ segmentation is an important yet challenging problem for medical image analysis . the pancreas is an abdominal organ with very high anatomical variability . this inhibits traditional segmentation methods from achieving high accuracies , especially compared to other organs such as the liver , heart or kidneys . in this paper , we present a holistic learning approach that integrates semantic mid-level cues of deeply-learned organ interior and boundary maps via robust spatial aggregation using random forest . our method generates boundary preserving pixel-wise class labels for pancreas segmentation . quantitative evaluation is performed on ct scans of 82 patients in 4-fold cross-validation . we achieve a ( mean $ \\pm $ std . dev . ) dice similarity coefficient of 78.01 % $ \\pm $ 8.2 % in testing which significantly outperforms the previous state-of-the-art approach of 71.8 % $ \\pm $ 10.7 % under the same evaluation criterion .", "topics": ["map", "eisenstein 's criterion"]}
{"title": "abstractions for ai-based user interfaces and systems", "abstract": "novel user interfaces based on artificial intelligence , such as natural-language agents , present new categories of engineering challenges . these systems need to cope with uncertainty and ambiguity , interface with machine learning algorithms , and compose information from multiple users to make decisions . we propose to treat these challenges as language-design problems . we describe three programming language abstractions for three core problems in intelligent system design . first , hypothetical worlds support nondeterministic search over spaces of alternative actions . second , a feature type system abstracts the interaction between applications and learning algorithms . finally , constructs for collaborative execution extend hypothetical worlds across multiple machines while controlling access to private data . we envision these features as first steps toward a complete language for implementing ai-based interfaces and applications .", "topics": ["natural language", "artificial intelligence"]}
{"title": "simultaneous object detection , tracking , and event recognition", "abstract": "the common internal structure and algorithmic organization of object detection , detection-based tracking , and event recognition facilitates a general approach to integrating these three components . this supports multidirectional information flow between these components allowing object detection to influence tracking and event recognition and event recognition to influence tracking and object detection . the performance of the combination can exceed the performance of the components in isolation . this can be done with linear asymptotic complexity .", "topics": ["object detection", "computational complexity theory"]}
{"title": "generalization properties and implicit regularization for multiple passes sgm", "abstract": "we study the generalization properties of stochastic gradient methods for learning with convex loss functions and linearly parameterized functions . we show that , in the absence of penalizations or constraints , the stability and approximation properties of the algorithm can be controlled by tuning either the step-size or the number of passes over the data . in this view , these parameters can be seen to control a form of implicit regularization . numerical results complement the theoretical findings .", "topics": ["numerical analysis", "matrix regularization"]}
{"title": "theoretical and practical advances on smoothing for extensive-form games", "abstract": "sparse iterative methods , in particular first-order methods , are known to be among the most effective in solving large-scale two-player zero-sum extensive-form games . the convergence rates of these methods depend heavily on the properties of the distance-generating function that they are based on . we investigate the acceleration of first-order methods for solving extensive-form games through better design of the dilated entropy function -- -a class of distance-generating functions related to the domains associated with the extensive-form games . by introducing a new weighting scheme for the dilated entropy function , we develop the first distance-generating function for the strategy spaces of sequential games that has no dependence on the branching factor of the player . this result improves the convergence rate of several first-order methods by a factor of $ \\omega ( b^dd ) $ , where $ b $ is the branching factor of the player , and $ d $ is the depth of the game tree . thus far , counterfactual regret minimization methods have been faster in practice , and more popular , than first-order methods despite their theoretically inferior convergence rates . using our new weighting scheme and practical tuning we show that , for the first time , the excessive gap technique can be made faster than the fastest counterfactual regret minimization algorithm , cfr+ , in practice .", "topics": ["regret ( decision theory )", "sparse matrix"]}
{"title": "memexqa : visual memex question answering", "abstract": "this paper proposes a new task , memexqa : given a collection of photos or videos from a user , the goal is to automatically answer questions that help users recover their memory about events captured in the collection . towards solving the task , we 1 ) present the memexqa dataset , a large , realistic multimodal dataset consisting of real personal photos and crowd-sourced questions/answers , 2 ) propose memexnet , a unified , end-to-end trainable network architecture for image , text and video question answering . experimental results on the memexqa dataset demonstrate that memexnet outperforms strong baselines and yields the state-of-the-art on this novel and challenging task . the promising results on textqa and videoqa suggest memexnet 's efficacy and scalability across various qa tasks .", "topics": ["baseline ( configuration management )", "end-to-end principle"]}
{"title": "unicalc.lin : a linear constraint solver for the unicalc system", "abstract": "in this short paper we present a linear constraint solver for the unicalc system , an environment for reliable solution of mathematical modeling problems .", "topics": ["mathematical optimization"]}
{"title": "iterative multi-document neural attention for multiple answer prediction", "abstract": "people have information needs of varying complexity , which can be solved by an intelligent agent able to answer questions formulated in a proper way , eventually considering user context and preferences . in a scenario in which the user profile can be considered as a question , intelligent agents able to answer questions can be used to find the most relevant answers for a given user . in this work we propose a novel model based on artificial neural networks to answer questions with multiple answers by exploiting multiple facts retrieved from a knowledge base . the model is evaluated on the factoid question answering and top-n recommendation tasks of the babi movie dialog dataset . after assessing the performance of the model on both tasks , we try to define the long-term goal of a conversational recommender system able to interact using natural language and to support users in their information seeking processes in a personalized way .", "topics": ["natural language", "neural networks"]}
{"title": "protein secondary structure prediction with long short term memory networks", "abstract": "prediction of protein secondary structure from the amino acid sequence is a classical bioinformatics problem . common methods use feed forward neural networks or svms combined with a sliding window , as these models does not naturally handle sequential data . recurrent neural networks are an generalization of the feed forward neural network that naturally handle sequential data . we use a bidirectional recurrent neural network with long short term memory cells for prediction of secondary structure and evaluate using the cb513 dataset . on the secondary structure 8-class problem we report better performance ( 0.674 ) than state of the art ( 0.664 ) . our model includes feed forward networks between the long short term memory cells , a path that can be further explored .", "topics": ["recurrent neural network"]}
{"title": "on calibration of modern neural networks", "abstract": "confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications . we discover that modern neural networks , unlike those from a decade ago , are poorly calibrated . through extensive experiments , we observe that depth , width , weight decay , and batch normalization are important factors influencing calibration . we evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets . our analysis and experiments not only offer insights into neural network learning , but also provide a simple and straightforward recipe for practical settings : on most datasets , temperature scaling -- a single-parameter variant of platt scaling -- is surprisingly effective at calibrating predictions .", "topics": ["neural networks"]}
{"title": "a unified view of regularized dual averaging and mirror descent with implicit updates", "abstract": "we study three families of online convex optimization algorithms : follow-the-proximally-regularized-leader ( ftrl-proximal ) , regularized dual averaging ( rda ) , and composite-objective mirror descent . we first prove equivalence theorems that show all of these algorithms are instantiations of a general ftrl update . this provides theoretical insight on previous experimental observations . in particular , even though the fobos composite mirror descent algorithm handles l1 regularization explicitly , it has been observed that rda is even more effective at producing sparsity . our results demonstrate that fobos uses subgradient approximations to the l1 penalty from previous rounds , leading to less sparsity than rda , which handles the cumulative penalty in closed form . the ftrl-proximal algorithm can be seen as a hybrid of these two , and outperforms both on a large , real-world dataset . our second contribution is a unified analysis which produces regret bounds that match ( up to logarithmic terms ) or improve the best previously known bounds . this analysis also extends these algorithms in two important ways : we support a more general type of composite objective and we analyze implicit updates , which replace the subgradient approximation of the current loss function with an exact optimization .", "topics": ["regret ( decision theory )", "loss function"]}
{"title": "the importance of quantum decoherence in brain processes", "abstract": "based on a calculation of neural decoherence rates , we argue that that the degrees of freedom of the human brain that relate to cognitive processes should be thought of as a classical rather than quantum system , i.e . , that there is nothing fundamentally wrong with the current classical approach to neural network simulations . we find that the decoherence timescales ~10^ { -13 } -10^ { -20 } seconds are typically much shorter than the relevant dynamical timescales ( ~0.001-0.1 seconds ) , both for regular neuron firing and for kink-like polarization excitations in microtubules . this conclusion disagrees with suggestions by penrose and others that the brain acts as a quantum computer , and that quantum coherence is related to consciousness in a fundamental way .", "topics": ["simulation"]}
{"title": "disease trajectory maps", "abstract": "medical researchers are coming to appreciate that many diseases are in fact complex , heterogeneous syndromes composed of subpopulations that express different variants of a related complication . time series data extracted from individual electronic health records ( ehr ) offer an exciting new way to study subtle differences in the way these diseases progress over time . in this paper , we focus on answering two questions that can be asked using these databases of time series . first , we want to understand whether there are individuals with similar disease trajectories and whether there are a small number of degrees of freedom that account for differences in trajectories across the population . second , we want to understand how important clinical outcomes are associated with disease trajectories . to answer these questions , we propose the disease trajectory map ( dtm ) , a novel probabilistic model that learns low-dimensional representations of sparse and irregularly sampled time series . we propose a stochastic variational inference algorithm for learning the dtm that allows the model to scale to large modern medical datasets . to demonstrate the dtm , we analyze data collected on patients with the complex autoimmune disease , scleroderma . we find that dtm learns meaningful representations of disease trajectories and that the representations are significantly associated with important clinical outcomes .", "topics": ["calculus of variations", "sparse matrix"]}
{"title": "smiles enumeration as data augmentation for neural network modeling of molecules", "abstract": "simplified molecular input line entry system ( smiles ) is a single line text representation of a unique molecule . one molecule can however have multiple smiles strings , which is a reason that canonical smiles have been defined , which ensures a one to one correspondence between smiles string and molecule . here the fact that multiple smiles represent the same molecule is explored as a technique for data augmentation of a molecular qsar dataset modeled by a long short term memory ( lstm ) cell based neural network . the augmented dataset was 130 times bigger than the original . the network trained with the augmented dataset shows better performance on a test set when compared to a model built with only one canonical smiles string per molecule . the correlation coefficient r2 on the test set was improved from 0.56 to 0.66 when using smiles enumeration , and the root mean square error ( rms ) likewise fell from 0.62 to 0.55 . the technique also works in the prediction phase . by taking the average per molecule of the predictions for the enumerated smiles a further improvement to a correlation coefficient of 0.68 and a rms of 0.52 was found .", "topics": ["test set", "coefficient"]}
{"title": "generalized product of experts for automatic and principled fusion of gaussian process predictions", "abstract": "in this work , we propose a generalized product of experts ( gpoe ) framework for combining the predictions of multiple probabilistic models . we identify four desirable properties that are important for scalability , expressiveness and robustness , when learning and inferring with a combination of multiple models . through analysis and experiments , we show that gpoe of gaussian processes ( gp ) have these qualities , while no other existing combination schemes satisfy all of them at the same time . the resulting gp-gpoe is highly scalable as individual gp experts can be independently learned in parallel ; very expressive as the way experts are combined depends on the input rather than fixed ; the combined prediction is still a valid probabilistic model with natural interpretation ; and finally robust to unreliable predictions from individual experts .", "topics": ["scalability"]}
{"title": "learning strips operators from noisy and incomplete observations", "abstract": "agents learning to act autonomously in real-world domains must acquire a model of the dynamics of the domain in which they operate . learning domain dynamics can be challenging , especially where an agent only has partial access to the world state , and/or noisy external sensors . even in standard strips domains , existing approaches can not learn from noisy , incomplete observations typical of real-world domains . we propose a method which learns strips action models in such domains , by decomposing the problem into first learning a transition function between states in the form of a set of classifiers , and then deriving explicit strips rules from the classifiers ' parameters . we evaluate our approach on simulated standard planning domains from the international planning competition , and show that it learns useful domain descriptions from noisy , incomplete observations .", "topics": ["simulation", "sensor"]}
{"title": "single parameter galaxy classification : the principal curve through the multi-dimensional space of galaxy properties", "abstract": "we propose to describe the variety of galaxies from sdss by using only one affine parameter . to this aim , we build the principal curve ( p-curve ) passing through the spine of the data point cloud , considering the eigenspace derived from principal component analysis of morphological , physical and photometric galaxy properties . thus , galaxies can be labeled , ranked and classified by a single arc length value of the curve , measured at the unique closest projection of the data points on the p-curve . we find that the p-curve has a `` w '' letter shape with 3 turning points , defining 4 branches that represent distinct galaxy populations . this behavior is controlled mainly by 2 properties , namely u-r and sfr . we further present the variations of several galaxy properties as a function of arc length . luminosity functions variate from steep schechter fits at low arc length , to double power law and ending in log-normal fits at high arc length . galaxy clustering shows increasing autocorrelation power at large scales as arc length increases . pca analysis allowed to find peculiar galaxy populations located apart from the main cloud of data points , such as small red galaxies dominated by a disk , of relatively high stellar mass-to-light ratio and surface mass density . the p-curve allows not only dimensionality reduction , but also provides supporting evidence for relevant physical models and scenarios in extragalactic astronomy : 1 ) evidence for the hierarchical merging scenario in the formation of a selected group of red massive galaxies . these galaxies present a log-normal r-band luminosity function , which might arise from multiplicative processes involved in this scenario . 2 ) connection between the onset of agn activity and star formation quenching , which appears in green galaxies when transitioning from blue to red populations . ( full abstract in downloadable version )", "topics": ["cluster analysis"]}
{"title": "expectation-maximization technique and spatial-adaptation applied to pel-recursive motion estimation", "abstract": "pel-recursive motion estimation isa well-established approach . however , in the presence of noise , it becomes an ill-posed problem that requires regularization . in this paper , motion vectors are estimated in an iterative fashion by means of the expectation-maximization ( em ) algorithm and a gaussian data model . our proposed algorithm also utilizes the local image properties of the scene to improve the motion vector estimates following a spatially adaptive approach . numerical experiments are presented that demonstrate the merits of our method .", "topics": ["numerical analysis", "matrix regularization"]}
{"title": "whitehead method and genetic algorithms", "abstract": "in this paper we discuss a genetic version ( gwa ) of the whitehead 's algorithm , which is one of the basic algorithms in combinatorial group theory . it turns out that gwa is surprisingly fast and outperforms the standard whitehead 's algorithm in free groups of rank > = 5 . experimenting with gwa we collected an interesting numerical data that clarifies the time-complexity of the whitehead 's problem in general . these experiments led us to several mathematical conjectures . if confirmed they will shed light on hidden mechanisms of whitehead method and geometry of automorphic orbits in free groups .", "topics": ["time complexity", "numerical analysis"]}
{"title": "multi-task learning for continuous control", "abstract": "reliable and effective multi-task learning is a prerequisite for the development of robotic agents that can quickly learn to accomplish related , everyday tasks . however , in the reinforcement learning domain , multi-task learning has not exhibited the same level of success as in other domains , such as computer vision . in addition , most reinforcement learning research on multi-task learning has been focused on discrete action spaces , which are not used for robotic control in the real-world . in this work , we apply multi-task learning methods to continuous action spaces and benchmark their performance on a series of simulated continuous control tasks . most notably , we show that multi-task learning outperforms our baselines and alternative knowledge sharing methods .", "topics": ["baseline ( configuration management )", "reinforcement learning"]}
{"title": "distantly supervised road segmentation", "abstract": "we present an approach for road segmentation that only requires image-level annotations at training time . we leverage distant supervision , which allows us to train our model using images that are different from the target domain . using large publicly available image databases as distant supervisors , we develop a simple method to automatically generate weak pixel-wise road masks . these are used to iteratively train a fully convolutional neural network , which produces our final segmentation model . we evaluate our method on the cityscapes dataset , where we compare it with a fully supervised approach . further , we discuss the trade-off between annotation cost and performance . overall , our distantly supervised approach achieves 93.8 % of the performance of the fully supervised approach , while using orders of magnitude less annotation work .", "topics": ["supervised learning", "database"]}
{"title": "extended dynamic programming and fast multidimensional search algorithm for energy minization in stereo and motion", "abstract": "this paper presents a novel extended dynamic programming approach for energy minimization ( edp ) to solve the correspondence problem for stereo and motion . a significant speedup is achieved using a recursive minimum search strategy ( rms ) . the mentioned speedup is particularly important if the disparity space is 2d as well as 3d . the proposed rms can also be applied in the well-known dynamic programming ( dp ) approach for stereo and motion . in this case , the general 2d problem of the global discrete energy minimization is reduced to several mutually independent sub-problems of the one-dimensional minimization . the edp method is used when the approximation of the general 2d discrete energy minimization problem is considered . then the rms algorithm is an essential part of the edp method . using the edp algorithm we obtain a lower energy bound than the graph cuts ( gc ) expansion technique on stereo and motion problems . the proposed calculation scheme possesses natural parallelism and can be realized on graphics processing unit ( gpu ) platforms , and can be potentially restricted further by the number of scanlines in the image plane . furthermore , the rms and edp methods can be used in any optimization problem where the objective function meets specific conditions in the smoothness term .", "topics": ["optimization problem", "loss function"]}
{"title": "can we unify monocular detectors for autonomous driving by using the pixel-wise semantic segmentation of cnns ?", "abstract": "autonomous driving is a challenging topic that requires complex solutions in perception tasks such as recognition of road , lanes , traffic signs or lights , vehicles and pedestrians . through years of research , computer vision has grown capable of tackling these tasks with monocular detectors that can provide remarkable detection rates with relatively low processing times . however , the recent appearance of convolutional neural networks ( cnns ) has revolutionized the computer vision field and has made possible approaches to perform full pixel-wise semantic segmentation in times close to real time ( even on hardware that can be carried on a vehicle ) . in this paper , we propose to use full image segmentation as an approach to simplify and unify most of the detection tasks required in the perception module of an autonomous vehicle , analyzing major concerns such as computation time and detection performance .", "topics": ["image segmentation", "computer vision"]}
{"title": "a convex parametrization of a new class of universal kernel functions for use in kernel learning", "abstract": "we propose a new class of universal kernel functions which admit a linear parametrization using positive semidefinite matrices . these kernels are generalizations of the sobolev kernel and are defined by piecewise-polynomial functions . the class of kernels is termed `` tessellated '' as the resulting discriminant is defined piecewise with hyper-rectangular domains whose corners are determined by the training data . the kernels have scalable complexity , but each instance is universal in the sense that its hypothesis space is dense in $ l_2 $ . using numerical testing , we show that for the soft margin svm , this class can eliminate the need for gaussian kernels . furthermore , we demonstrate that when the ratio of the number of training data to features is high , this method will significantly outperform other kernel learning algorithms . finally , to reduce the complexity associated with sdp-based kernel learning methods , we use a randomized basis for the positive matrices to integrate with existing multiple kernel learning algorithms such as simplemkl .", "topics": ["kernel ( operating system )", "test set"]}
{"title": "an empirical evaluation of visual question answering for novel objects", "abstract": "we study the problem of answering questions about images in the harder setting , where the test questions and corresponding images contain novel objects , which were not queried about in the training data . such setting is inevitable in real world-owing to the heavy tailed distribution of the visual categories , there would be some objects which would not be annotated in the train set . we show that the performance of two popular existing methods drop significantly ( up to 28 % ) when evaluated on novel objects cf . known objects . we propose methods which use large existing external corpora of ( i ) unlabeled text , i.e . books , and ( ii ) images tagged with classes , to achieve novel object based visual question answering . we do systematic empirical studies , for both an oracle case where the novel objects are known textually , as well as a fully automatic case without any explicit knowledge of the novel objects , but with the minimal assumption that the novel objects are semantically related to the existing objects in training . the proposed methods for novel object based visual question answering are modular and can potentially be used with many visual question answering architectures . we show consistent improvements with the two popular architectures and give qualitative analysis of the cases where the model does well and of those where it fails to bring improvements .", "topics": ["test set", "text corpus"]}
{"title": "content based document recommender using deep learning", "abstract": "with the recent advancements in information technology there has been a huge surge in amount of data available . but information retrieval technology has not been able to keep up with this pace of information generation resulting in over spending of time for retrieving relevant information . even though systems exist for assisting users to search a database along with filtering and recommending relevant information , but recommendation system which uses content of documents for recommendation still have a long way to mature . here we present a deep learning based supervised approach to recommend similar documents based on the similarity of content . we combine the c-dssm model with word2vec distributed representations of words to create a novel model to classify a document pair as relevant/irrelavant by assigning a score to it . using our model retrieval of documents can be done in o ( 1 ) time and the memory complexity is o ( n ) , where n is number of documents .", "topics": ["supervised learning"]}
{"title": "spatio-temporal graphical model selection", "abstract": "we consider the problem of estimating the topology of spatial interactions in a discrete state , discrete time spatio-temporal graphical model where the interactions affect the temporal evolution of each agent in a network . among other models , the susceptible , infected , recovered ( $ sir $ ) model for interaction events fall into this framework . we pose the problem as a structure learning problem and solve it using an $ \\ell_1 $ -penalized likelihood convex program . we evaluate the solution on a simulated spread of infectious over a complex network . our topology estimates outperform those of a standard spatial markov random field graphical model selection using $ \\ell_1 $ -regularized logistic regression .", "topics": ["graphical model", "interaction"]}
{"title": "robust and fast decoding of high-capacity color qr codes for mobile applications", "abstract": "the use of color in qr codes brings extra data capacity , but also inflicts tremendous challenges on the decoding process due to chromatic distortion , cross-channel color interference and illumination variation . particularly , we further discover a new type of chromatic distortion in high-density color qr codes , cross-module color interference , caused by the high density which also makes the geometric distortion correction more challenging . to address these problems , we propose two approaches , namely , lsvm-cmi and qda-cmi , which jointly model these different types of chromatic distortion . extended from svm and qda , respectively , both lsvm-cmi and qda-cmi optimize over a particular objective function to learn a color classifier . furthermore , a robust geometric transformation method and several pipeline refinements are proposed to boost the decoding performance for mobile applications . we put forth and implement a framework for high-capacity color qr codes equipped with our methods , called hiq . to evaluate the performance of hiq , we collect a challenging large-scale color qr code dataset , cuhk-cqrc , which consists of 5390 high-density color qr code samples . the comparison with the baseline method [ 2 ] on cuhk-cqrc shows that hiq at least outperforms [ 2 ] by 188 % in decoding success rate and 60 % in bit error rate . our implementation of hiq in ios and android also demonstrates the effectiveness of our framework in real-world applications .", "topics": ["baseline ( configuration management )", "loss function"]}
{"title": "deep learning approximation for stochastic control problems", "abstract": "many real world stochastic control problems suffer from the `` curse of dimensionality '' . to overcome this difficulty , we develop a deep learning approach that directly solves high-dimensional stochastic control problems based on monte-carlo sampling . we approximate the time-dependent controls as feedforward neural networks and stack these networks together through model dynamics . the objective function for the control problem plays the role of the loss function for the deep neural network . we test this approach using examples from the areas of optimal trading and energy storage . our results suggest that the algorithm presented here achieves satisfactory accuracy and at the same time , can handle rather high dimensional problems .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "nddr-cnn : layer-wise feature fusing in multi-task cnn by neural discriminative dimensionality reduction", "abstract": "state-of-the-art convolutional neural network ( cnn ) benefits much from multi-task learning ( mtl ) , which learns multiple related tasks simultaneously to obtain shared or mutually related representations for different tasks . the most widely used mtl cnn structure is based on an empirical or heuristic split on a specific layer ( e.g . , the last convolutional layer ) to minimize multiple task-specific losses . however , this heuristic sharing/splitting strategy may be harmful to the final performance of one or multiple tasks . in this paper , we propose a novel cnn structure for mtl , which enables automatic feature fusing at every layer . specifically , we first concatenate features from different tasks according to their channel dimension , and then formulate the feature fusing problem as discriminative dimensionality reduction . we show that this discriminative dimensionality reduction can be fulfilled by 1x1 convolution , batch normalization , and weight decay in one cnn , which we refer to as neural discriminative dimensionality reduction ( nddr ) . we perform detailed ablation analysis for different configurations in training the proposed nddr-cnn network . the experiments carried out on different network structures and different task sets demonstrate the promising performance and desirable generalizability of our proposed method .", "topics": ["convolution", "heuristic"]}
{"title": "a bayesian network approach to county-level corn yield prediction using historical data and expert knowledge", "abstract": "crop yield forecasting is the methodology of predicting crop yields prior to harvest . the availability of accurate yield prediction frameworks have enormous implications from multiple standpoints , including impact on the crop commodity futures markets , formulation of agricultural policy , as well as crop insurance rating . the focus of this work is to construct a corn yield predictor at the county scale . corn yield ( forecasting ) depends on a complex , interconnected set of variables that include economic , agricultural , management and meteorological factors . conventional forecasting is either knowledge-based computer programs ( that simulate plant-weather-soil-management interactions ) coupled with targeted surveys or statistical model based . the former is limited by the need for painstaking calibration , while the latter is limited to univariate analysis or similar simplifying assumptions that fail to capture the complex interdependencies affecting yield . in this paper , we propose a data-driven approach that is `` gray box '' i.e . that seamlessly utilizes expert knowledge in constructing a statistical network model for corn yield forecasting . our multivariate gray box model is developed on bayesian network analysis to build a directed acyclic graph ( dag ) between predictors and yield . starting from a complete graph connecting various carefully chosen variables and yield , expert knowledge is used to prune or strengthen edges connecting variables . subsequently the structure ( connectivity and edge weights ) of the dag that maximizes the likelihood of observing the training data is identified via optimization . we curated an extensive set of historical data ( 1948-2012 ) for each of the 99 counties in iowa as data to train the model .", "topics": ["test set", "data mining"]}
{"title": "generalized denoising auto-encoders as generative models", "abstract": "recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density , in the case where the corruption noise is gaussian , the reconstruction error is the squared error , and the data is continuous-valued . this has led to various proposals for sampling from this implicitly learned density function , using langevin and metropolis-hastings mcmc . however , it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data-generating distribution when the data are discrete , or using other forms of corruption process and reconstruction errors . another issue is the mathematical justification which is only valid in the limit of small corruption noise . we propose here a different attack on the problem , which deals with all these issues : arbitrary ( but noisy enough ) corruption , arbitrary reconstruction loss ( seen as a log-likelihood ) , handling both discrete and continuous-valued variables , and removing the bias due to non-infinitesimal corruption noise ( or non-infinitesimal contractive penalty ) .", "topics": ["sampling ( signal processing )", "noise reduction"]}
{"title": "slice sampling covariance hyperparameters of latent gaussian models", "abstract": "the gaussian process ( gp ) is a popular way to specify dependencies between random variables in a probabilistic model . in the bayesian framework the covariance structure can be specified using unknown hyperparameters . integrating over these hyperparameters considers different possible explanations for the data when making predictions . this integration is often performed using markov chain monte carlo ( mcmc ) sampling . however , with non-gaussian observations standard hyperparameter sampling approaches require careful tuning and may converge slowly . in this paper we present a slice sampling approach that requires little tuning while mixing well in both strong- and weak-data regimes .", "topics": ["sampling ( signal processing )", "markov chain"]}
{"title": "self-attentive residual decoder for neural machine translation", "abstract": "neural sequence-to-sequence networks with attention have achieved remarkable performance for machine translation . one of the reasons for their effectiveness is their ability to capture relevant source-side contextual information at each time-step prediction through an attention mechanism . however , the target-side context is solely based on the sequence model which , in practice , is prone to a recency bias and lacks the ability to capture effectively non-sequential dependencies among words . to address this limitation , we propose a target-side-attentive residual recurrent network for decoding , where attention over previous words contributes directly to the prediction of the next word . the residual learning facilitates the flow of information from the distant past and is able to emphasize any of the previously translated words , hence it gains access to a wider context . the proposed model outperforms a neural mt baseline as well as a memory and self-attention network on three language pairs . the analysis of the attention learned by the decoder confirms that it emphasizes a wider context , and that it captures syntactic-like structures .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "combogan : unrestrained scalability for image domain translation", "abstract": "this year alone has seen unprecedented leaps in the area of learning-based image translation , namely cyclegan , by zhu et al . but experiments so far have been tailored to merely two domains at a time , and scaling them to more would require an quadratic number of models to be trained . and with two-domain models taking days to train on current hardware , the number of domains quickly becomes limited by the time and resources required to process them . in this paper , we propose a multi-component image translation model and training scheme which scales linearly - both in resource consumption and time required - with the number of domains . we demonstrate its capabilities on a dataset of paintings by 14 different artists and on images of the four different seasons in the alps . note that 14 data groups would need ( 14 choose 2 ) = 91 different cyclegan models : a total of 182 generator/discriminator pairs ; whereas our model requires only 14 generator/discriminator pairs .", "topics": ["scalability"]}
{"title": "generalized emphatic temporal difference learning : bias-variance analysis", "abstract": "we consider the off-policy evaluation problem in markov decision processes with function approximation . we propose a generalization of the recently introduced \\emph { emphatic temporal differences } ( etd ) algorithm \\citep { suttonmw15 } , which encompasses the original etd ( $ \\lambda $ ) , as well as several other off-policy evaluation algorithms as special cases . we call this framework \\etd , where our introduced parameter $ \\beta $ controls the decay rate of an importance-sampling term . we study conditions under which the projected fixed-point equation underlying \\etd\\ involves a contraction operator , allowing us to present the first asymptotic error bounds ( bias ) for \\etd . our results show that the original etd algorithm always involves a contraction operator , and its bias is bounded . moreover , by controlling $ \\beta $ , our proposed generalization allows trading-off bias for variance reduction , thereby achieving a lower total error .", "topics": ["markov chain"]}
{"title": "the application of bayesian optimization and classifier systems in nurse scheduling", "abstract": "two ideas taken from bayesian optimization and classifier systems are presented for personnel scheduling based on choosing a suitable scheduling rule from a set for each persons assignment . unlike our previous work of using genetic algorithms whose learning is implicit , the learning in both approaches is explicit , i.e . we are able to identify building blocks directly . to achieve this target , the bayesian optimization algorithm builds a bayesian network of the joint probability distribution of the rules used to construct solutions , while the adapted classifier system assigns each rule a strength value that is constantly updated according to its usefulness in the current situation . computational results from 52 real data instances of nurse scheduling demonstrate the success of both approaches . it is also suggested that the learning mechanism in the proposed approaches might be suitable for other scheduling problems .", "topics": ["bayesian network"]}
{"title": "development of a hybrid learning system based on svm , anfis and domain knowledge : dkfis", "abstract": "this paper presents the development of a hybrid learning system based on support vector machines ( svm ) , adaptive neuro-fuzzy inference system ( anfis ) and domain knowledge to solve prediction problem . the proposed two-stage domain knowledge based fuzzy information system ( dkfis ) improves the prediction accuracy attained by anfis alone . the proposed framework has been implemented on a noisy and incomplete dataset acquired from a hydrocarbon field located at western part of india . here , oil saturation has been predicted from four different well logs i.e . gamma ray , resistivity , density , and clay volume . in the first stage , depending on zero or near zero and non-zero oil saturation levels the input vector is classified into two classes ( class 0 and class 1 ) using svm . the classification results have been further fine-tuned applying expert knowledge based on the relationship among predictor variables i.e . well logs and target variable - oil saturation . second , an anfis is designed to predict non-zero ( class 1 ) oil saturation values from predictor logs . the predicted output has been further refined based on expert knowledge . it is apparent from the experimental results that the expert intervention with qualitative judgment at each stage has rendered the prediction into the feasible and realistic ranges . the performance analysis of the prediction in terms of four performance metrics such as correlation coefficient ( cc ) , root mean square error ( rmse ) , and absolute error mean ( aem ) , scatter index ( si ) has established dkfis as a useful tool for reservoir characterization .", "topics": ["support vector machine", "coefficient"]}
{"title": "a semantic qa-based approach for text summarization evaluation", "abstract": "many natural language processing and computational linguistics applications involves the generation of new texts based on some existing texts , such as summarization , text simplification and machine translation . however , there has been a serious problem haunting these applications for decades , that is , how to automatically and accurately assess quality of these applications . in this paper , we will present some preliminary results on one especially useful and challenging problem in nlp system evaluation : how to pinpoint content differences of two text passages ( especially for large pas-sages such as articles and books ) . our idea is intuitive and very different from existing approaches . we treat one text passage as a small knowledge base , and ask it a large number of questions to exhaustively identify all content points in it . by comparing the correctly answered questions from two text passages , we will be able to compare their content precisely . the experiment using 2007 duc summarization corpus clearly shows promising results .", "topics": ["natural language processing", "machine translation"]}
{"title": "new alignment methods for discriminative book summarization", "abstract": "we consider the unsupervised alignment of the full text of a book with a human-written summary . this presents challenges not seen in other text alignment problems , including a disparity in length and , consequent to this , a violation of the expectation that individual words and phrases should align , since large passages and chapters can be distilled into a single summary phrase . we present two new methods , based on hidden markov models , specifically targeted to this problem , and demonstrate gains on an extractive book summarization task . while there is still much room for improvement , unsupervised alignment holds intrinsic value in offering insight into what features of a book are deemed worthy of summarization .", "topics": ["unsupervised learning"]}
{"title": "online learning without prior information", "abstract": "the vast majority of optimization and online learning algorithms today require some prior information about the data ( often in the form of bounds on gradients or on the optimal parameter value ) . when this information is not available , these algorithms require laborious manual tuning of various hyperparameters , motivating the search for algorithms that can adapt to the data with no prior information . we describe a frontier of new lower bounds on the performance of such algorithms , reflecting a tradeoff between a term that depends on the optimal parameter value and a term that depends on the gradients ' rate of growth . further , we construct a family of algorithms whose performance matches any desired point on this frontier , which no previous algorithm reaches .", "topics": ["regret ( decision theory )", "loss function"]}
{"title": "action and perception for spatiotemporal patterns", "abstract": "this is a contribution to the formalization of the concept of agents in multivariate markov chains . agents are commonly defined as entities that act , perceive , and are goal-directed . in a multivariate markov chain ( e.g . a cellular automaton ) the transition matrix completely determines the dynamics . this seems to contradict the possibility of acting entities within such a system . here we present definitions of actions and perceptions within multivariate markov chains based on entity-sets . entity-sets represent a largely independent choice of a set of spatiotemporal patterns that are considered as all the entities within the markov chain . for example , the entity-set can be chosen according to operational closure conditions or complete specific integration . importantly , the perception-action loop also induces an entity-set and is a multivariate markov chain . we then show that our definition of actions leads to non-heteronomy and that of perceptions specialize to the usual concept of perception in the perception-action loop .", "topics": ["entity", "markov chain"]}
{"title": "color image denoising by chromatic edges based vector valued diffusion", "abstract": "in this letter we propose to denoise digital color images via an improved geometric diffusion scheme . by introducing edges detected from all three color channels into the diffusion the proposed scheme avoids color smearing artifacts . vector valued diffusion is used to control the smoothing and the geometry of color images are taken into consideration . color edge strength function computed from different planes is introduced and it stops the diffusion spread across chromatic edges . experimental results indicate that the scheme achieves good denoising with edge preservation when compared to other related schemes .", "topics": ["noise reduction"]}
{"title": "discovering underlying plans based on shallow models", "abstract": "plan recognition aims to discover target plans ( i.e . , sequences of actions ) behind observed actions , with history plan libraries or domain models in hand . previous approaches either discover plans by maximally `` matching '' observed actions to plan libraries , assuming target plans are from plan libraries , or infer plans by executing domain models to best explain the observed actions , assuming that complete domain models are available . in real world applications , however , target plans are often not from plan libraries , and complete domain models are often not available , since building complete sets of plans and complete domain models are often difficult or expensive . in this paper we view plan libraries as corpora and learn vector representations of actions using the corpora , we then discover target plans based on the vector representations . specifically , we propose two approaches , dup and rnnplanner , to discover target plans based on vector representations of actions . dup explores the em-style framework to capture local contexts of actions and discover target plans by optimizing the probability of target plans , while rnnplanner aims to leverage long-short term contexts of actions based on rnns ( recurrent neural networks ) framework to help recognize target plans . in the experiments , we empirically show that our approaches are capable of discovering underlying plans that are not from plan libraries , without requiring domain models provided . we demonstrate the effectiveness of our approaches by comparing its performance to traditional plan recognition approaches in three planning domains . we also compare dup and rnnplanner to see their advantages and disadvantages .", "topics": ["recurrent neural network", "text corpus"]}
{"title": "breathrnnet : breathing based authentication on resource-constrained iot devices using rnns", "abstract": "recurrent neural networks ( rnns ) have shown promising results in audio and speech processing applications due to their strong capabilities in modelling sequential data . in many applications , rnns tend to outperform conventional models based on gmm/ubms and i-vectors . increasing popularity of iot devices makes a strong case for implementing rnn based inferences for applications such as acoustics based authentication , voice commands , and edge analytics for smart homes . nonetheless , the feasibility and performance of rnn based inferences on resources-constrained iot devices remain largely unexplored . in this paper , we investigate the feasibility of using rnns for an end-to-end authentication system based on breathing acoustics . we evaluate the performance of rnn models on three types of devices ; smartphone , smartwatch , and raspberry pi and show that unlike cnn models , rnn models can be easily ported onto resource-constrained devices without a significant loss in accuracy .", "topics": ["recurrent neural network", "end-to-end principle"]}
{"title": "a comparative study of gaussian mixture model and radial basis function for voice recognition", "abstract": "a comparative study of the application of gaussian mixture model ( gmm ) and radial basis function ( rbf ) in biometric recognition of voice has been carried out and presented . the application of machine learning techniques to biometric authentication and recognition problems has gained a widespread acceptance . in this research , a gmm model was trained , using expectation maximization ( em ) algorithm , on a dataset containing 10 classes of vowels and the model was used to predict the appropriate classes using a validation dataset . for experimental validity , the model was compared to the performance of two different versions of rbf model using the same learning and validation datasets . the results showed very close recognition accuracy between the gmm and the standard rbf model , but with gmm performing better than the standard rbf by less than 1 % and the two models outperformed similar models reported in literature . the dtreg version of rbf outperformed the other two models by producing 94.8 % recognition accuracy . in terms of recognition time , the standard rbf was found to be the fastest among the three models .", "topics": ["artificial intelligence"]}
{"title": "understanding human behaviors in crowds by imitating the decision-making process", "abstract": "crowd behavior understanding is crucial yet challenging across a wide range of applications , since crowd behavior is inherently determined by a sequential decision-making process based on various factors , such as the pedestrians ' own destinations , interaction with nearby pedestrians and anticipation of upcoming events . in this paper , we propose a novel framework of social-aware generative adversarial imitation learning ( sa-gail ) to mimic the underlying decision-making process of pedestrians in crowds . specifically , we infer the latent factors of human decision-making process in an unsupervised manner by extending the generative adversarial imitation learning framework to anticipate future paths of pedestrians . different factors of human decision making are disentangled with mutual information maximization , with the process modeled by collision avoidance regularization and social-aware lstms . experimental results demonstrate the potential of our framework in disentangling the latent decision-making factors of pedestrians and stronger abilities in predicting future trajectories .", "topics": ["unsupervised learning", "matrix regularization"]}
{"title": "bayesian network learning by compiling to weighted max-sat", "abstract": "the problem of learning discrete bayesian networks from data is encoded as a weighted max-sat problem and the maxwalksat local search algorithm is used to address it . for each dataset , the per-variable summands of the ( bdeu ) marginal likelihood for different choices of parents ( 'family scores ' ) are computed prior to applying maxwalksat . each permissible choice of parents for each variable is encoded as a distinct propositional atom and the associated family score encoded as a 'soft ' weighted single-literal clause . two approaches to enforcing acyclicity are considered : either by encoding the ancestor relation or by attaching a total order to each graph and encoding that . the latter approach gives better results . learning experiments have been conducted on 21 synthetic datasets sampled from 7 bns . the largest dataset has 10,000 datapoints and 60 variables producing ( for the 'ancestor ' encoding ) a weighted cnf input file with 19,932 atoms and 269,367 clauses . for most datasets , maxwalksat quickly finds bns with higher bdeu score than the 'true ' bn . the effect of adding prior information is assessed . it is further shown that bayesian model averaging can be effected by collecting bns generated during the search .", "topics": ["synthetic data", "bayesian network"]}
{"title": "algorithmic linearly constrained gaussian processes", "abstract": "we algorithmically construct multi-output gaussian process priors which satisfy linear differential equations . our approach attempts to parametrize all solutions of the equations using gr\\ '' obner bases . if successful , a push forward gaussian process along the paramerization is the desired prior . we consider several examples , among them the full inhomogeneous system of maxwell 's equations . by bringing together stochastic learning and computeralgebra in a novel way , we combine noisy observations with precise algebraic computations .", "topics": ["computation"]}
{"title": "method for aspect-based sentiment annotation using rhetorical analysis", "abstract": "this paper fills a gap in aspect-based sentiment analysis and aims to present a new method for preparing and analysing texts concerning opinion and generating user-friendly descriptive reports in natural language . we present a comprehensive set of techniques derived from rhetorical structure theory and sentiment analysis to extract aspects from textual opinions and then build an abstractive summary of a set of opinions . moreover , we propose aspect-aspect graphs to evaluate the importance of aspects and to filter out unimportant ones from the summary . additionally , the paper presents a prototype solution of data flow with interesting and valuable results . the proposed method 's results proved the high accuracy of aspect detection when applied to the gold standard dataset .", "topics": ["natural language"]}
{"title": "a study of local binary pattern method for facial expression detection", "abstract": "face detection is a basic task for expression recognition . the reliability of face detection & face recognition approach has a major role on the performance and usability of the entire system . there are several ways to undergo face detection & recognition . we can use image processing operations , various classifiers , filters or virtual machines for the former . various strategies are being available for facial expression detection . the field of facial expression detection can have various applications along with its importance & can be interacted between human being & computer . many few options are available to identify a face in an image in accurate & efficient manner . local binary pattern ( lbp ) based texture algorithms have gained popularity in these years . lbp is an effective approach to have facial expression recognition & is a feature-based approach .", "topics": ["image processing"]}
{"title": "neural baby talk", "abstract": "we introduce a novel framework for image captioning that can produce natural language explicitly grounded in entities that object detectors find in the image . our approach reconciles classical slot filling approaches ( that are generally better grounded in images ) with modern neural captioning approaches ( that are generally more natural sounding and accurate ) . our approach first generates a sentence `template ' with slot locations explicitly tied to specific image regions . these slots are then filled in by visual concepts identified in the regions by object detectors . the entire architecture ( sentence template generation and slot filling with object detectors ) is end-to-end differentiable . we verify the effectiveness of our proposed model on different image captioning tasks . on standard image captioning and novel object captioning , our model reaches state-of-the-art on both coco and flickr30k datasets . we also demonstrate that our model has unique advantages when the train and test distributions of scene compositions -- and hence language priors of associated captions -- are different . code has been made available at : https : //github.com/jiasenlu/neuralbabytalk", "topics": ["natural language", "entity"]}
{"title": "pixel recursive super resolution", "abstract": "we present a pixel recursive super resolution model that synthesizes realistic details into images while enhancing their resolution . a low resolution image may correspond to multiple plausible high resolution images , thus modeling the super resolution process with a pixel independent conditional model often results in averaging different details -- hence blurry edges . by contrast , our model is able to represent a multimodal conditional distribution by properly modeling the statistical dependencies among the high resolution image pixels , conditioned on a low resolution input . we employ a pixelcnn architecture to define a strong prior over natural images and jointly optimize this prior with a deep conditioning convolutional network . human evaluations indicate that samples from our proposed model look more photo realistic than a strong l2 regression baseline .", "topics": ["pixel"]}
{"title": "a computer vision system for attention mapping in slam based 3d models", "abstract": "the study of human factors in the frame of interaction studies has been relevant for usability engi-neering and ergonomics for decades . today , with the advent of wearable eye-tracking and google glasses , monitoring of human factors will soon become ubiquitous . this work describes a computer vision system that enables pervasive mapping and monitoring of human attention . the key contribu-tion is that our methodology enables full 3d recovery of the gaze pointer , human view frustum and associated human centred measurements directly into an automatically computed 3d model in real-time . we apply rgb-d slam and descriptor matching methodologies for the 3d modelling , locali-zation and fully automated annotation of rois ( regions of interest ) within the acquired 3d model . this innovative methodology will open new avenues for attention studies in real world environments , bringing new potential into automated processing for human factors technologies .", "topics": ["computer vision"]}
{"title": "multichannel compressive sensing mri using noiselet encoding", "abstract": "the incoherence between measurement and sparsifying transform matrices and the restricted isometry property ( rip ) of measurement matrix are two of the key factors in determining the performance of compressive sensing ( cs ) . in cs-mri , the randomly under-sampled fourier matrix is used as the measurement matrix and the wavelet transform is usually used as sparsifying transform matrix . however , the incoherence between the randomly under-sampled fourier matrix and the wavelet matrix is not optimal , which can deteriorate the performance of cs-mri . using the mathematical result that noiselets are maximally incoherent with wavelets , this paper introduces the noiselet unitary bases as the measurement matrix to improve the incoherence and rip in cs-mri , and presents a method to design the pulse sequence for the noiselet encoding . this novel encoding scheme is combined with the multichannel compressive sensing ( mcs ) framework to take the advantage of multichannel data acquisition used in mri scanners . an empirical rip analysis is presented to compare the multichannel noiselet and multichannel fourier measurement matrices in mcs . simulations are presented in the mcs framework to compare the performance of noiselet encoding reconstructions and fourier encoding reconstructions at different acceleration factors . the comparisons indicate that multichannel noiselet measurement matrix has better rip than that of its fourier counterpart , and that noiselet encoded mcs-mri outperforms fourier encoded mcs-mri in preserving image resolution and can achieve higher acceleration factors . to demonstrate the feasibility of the proposed noiselet encoding scheme , two pulse sequences with tailored spatially selective rf excitation pulses was designed and implemented on a 3t scanner to acquire the data in the noiselet domain from a phantom and a human brain .", "topics": ["simulation"]}
{"title": "autonomous extracting a hierarchical structure of tasks in reinforcement learning and multi-task reinforcement learning", "abstract": "reinforcement learning ( rl ) , while often powerful , can suffer from slow learning speeds , particularly in high dimensional spaces . the autonomous decomposition of tasks and use of hierarchical methods hold the potential to significantly speed up learning in such domains . this paper proposes a novel practical method that can autonomously decompose tasks , by leveraging association rule mining , which discovers hidden relationship among entities in data mining . we introduce a novel method called arm-hstrl ( association rule mining to extract hierarchical structure of tasks in reinforcement learning ) . it extracts temporal and structural relationships of sub-goals in rl , and multi-task rl . in particular , it finds sub-goals and relationship among them . it is shown the significant efficiency and performance of the proposed method in two main topics of rl .", "topics": ["data mining", "reinforcement learning"]}
{"title": "learning wasserstein embeddings", "abstract": "the wasserstein distance received a lot of attention recently in the community of machine learning , especially for its principled way of comparing distributions . it has found numerous applications in several hard problems , such as domain adaptation , dimensionality reduction or generative models . however , its use is still limited by a heavy computational cost . our goal is to alleviate this problem by providing an approximation mechanism that allows to break its inherent complexity . it relies on the search of an embedding where the euclidean distance mimics the wasserstein distance . we show that such an embedding can be found with a siamese architecture associated with a decoder network that allows to move from the embedding space back to the original input space . once this embedding has been found , computing optimization problems in the wasserstein space ( e.g . barycenters , principal directions or even archetypes ) can be conducted extremely fast . numerical experiments supporting this idea are conducted on image datasets , and show the wide potential benefits of our method .", "topics": ["generative model", "approximation"]}
{"title": "data classification using the dempster-shafer method", "abstract": "in this paper , the dempster-shafer method is employed as the theoretical basis for creating data classification systems . testing is carried out using three popular ( multiple attribute ) benchmark datasets that have two , three and four classes . in each case , a subset of the available data is used for training to establish thresholds , limits or likelihoods of class membership for each attribute , and hence create mass functions that establish probability of class membership for each attribute of the test data . classification of each data item is achieved by combination of these probabilities via dempster 's rule of combination . results for the first two datasets show extremely high classification accuracy that is competitive with other popular methods . the third dataset is non-numerical and difficult to classify , but good results can be achieved provided the system and mass functions are designed carefully and the right attributes are chosen for combination . in all cases the dempster-shafer method provides comparable performance to other more popular algorithms , but the overhead of generating accurate mass functions increases the complexity with the addition of new attributes . overall , the results suggest that the d-s approach provides a suitable framework for the design of classification systems and that automating the mass function design and calculation would increase the viability of the algorithm for complex classification problems .", "topics": ["numerical analysis"]}
{"title": "markov chain hebbian learning algorithm with ternary synaptic units", "abstract": "in spite of remarkable progress in machine learning techniques , the state-of-the-art machine learning algorithms often keep machines from real-time learning ( online learning ) due in part to computational complexity in parameter optimization . as an alternative , a learning algorithm to train a memory in real time is proposed , which is named as the markov chain hebbian learning algorithm . the algorithm pursues efficient memory use during training in that ( i ) the weight matrix has ternary elements ( -1 , 0 , 1 ) and ( ii ) each update follows a markov chain -- the upcoming update does not need past weight memory . the algorithm was verified by two proof-of-concept tasks ( handwritten digit recognition and multiplication table memorization ) in which numbers were taken as symbols . particularly , the latter bases multiplication arithmetic on memory , which may be analogous to humans ' mental arithmetic . the memory-based multiplication arithmetic feasibly offers the basis of factorization , supporting novel insight into the arithmetic .", "topics": ["markov chain"]}
{"title": "stochastic gradient descent performs variational inference , converges to limit cycles for deep networks", "abstract": "stochastic gradient descent ( sgd ) is widely believed to perform implicit regularization when used to train deep neural networks , but the precise manner in which this occurs has thus far been elusive . we prove that sgd minimizes an average potential over the posterior distribution of weights along with an entropic regularization term . this potential is however not the original loss function in general . so sgd does perform variational inference , but for a different loss than the one used to compute the gradients . even more surprisingly , sgd does not even converge in the classical sense : we show that the most likely trajectories of sgd for deep networks do not behave like brownian motion around critical points . instead , they resemble closed loops with deterministic components . we prove that such `` out-of-equilibrium '' behavior is a consequence of highly non-isotropic gradient noise in sgd ; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1 % of its dimension . we provide extensive empirical validation of these claims , proven in the appendix .", "topics": ["calculus of variations", "loss function"]}
{"title": "hardness of the pricing problem for chains in barter exchanges", "abstract": "kidney exchange is a barter market where patients trade willing but medically incompatible donors . these trades occur via cycles , where each patient-donor pair both gives and receives a kidney , and via chains , which begin with an altruistic donor who does not require a kidney in return . for logistical reasons , the maximum length of a cycle is typically limited to a small constant , while chains can be much longer . given a compatibility graph of patient-donor pairs , altruists , and feasible potential transplants between them , finding even a maximum-cardinality set of vertex-disjoint cycles and chains is np-hard . there has been much work on developing provably optimal solvers that are efficient in practice . one of the leading techniques has been branch and price , where column generation is used to incrementally bring cycles and chains into the optimization model on an as-needed basis . in particular , only positive-price columns need to be brought into the model . we prove that finding a positive-price chain is np-complete . this shows incorrectness of two leading branch-and-price solvers that suggested polynomial-time chain pricing algorithms .", "topics": ["time complexity"]}
{"title": "a framework for constrained and adaptive behavior-based agents", "abstract": "behavior trees are commonly used to model agents for robotics and games , where constrained behaviors must be designed by human experts in order to guarantee that these agents will execute a specific chain of actions given a specific set of perceptions . in such application areas , learning is a desirable feature to provide agents with the ability to adapt and improve interactions with humans and environment , but often discarded due to its unreliability . in this paper , we propose a framework that uses reinforcement learning nodes as part of behavior trees to address the problem of adding learning capabilities in constrained agents . we show how this framework relates to options in hierarchical reinforcement learning , ensuring convergence of nested learning nodes , and we empirically show that the learning nodes do not affect the execution of other nodes in the tree .", "topics": ["reinforcement learning", "interaction"]}
{"title": "anisotropic diffusion for details enhancement in multi-exposure image fusion", "abstract": "we develop a multiexposure image fusion method based on texture features , which exploits the edge preserving and intraregion smoothing property of nonlinear diffusion filters based on partial differential equations ( pde ) . with the captured multiexposure image series , we first decompose images into base layers and detail layers to extract sharp details and fine details , respectively . the magnitude of the gradient of the image intensity is utilized to encourage smoothness at homogeneous regions in preference to inhomogeneous regions . then , we have considered texture features of the base layer to generate a mask ( i.e . , decision mask ) that guides the fusion of base layers in multiresolution fashion . finally , well-exposed fused image is obtained that combines fused base layer and the detail layers at each scale across all the input exposures . proposed algorithm skipping complex high dynamic range image ( hdri ) generation and tone mapping steps to produce detail preserving image for display on standard dynamic range display devices . moreover , our technique is effective for blending flash/no-flash image pair and multifocus images , that is , images focused on different targets .", "topics": ["nonlinear system"]}
{"title": "dropoutdagger : a bayesian approach to safe imitation learning", "abstract": "while imitation learning is becoming common practice in robotics , this approach often suffers from data mismatch and compounding errors . dagger is an iterative algorithm that addresses these issues by continually aggregating training data from both the expert and novice policies , but does not consider the impact of safety . we present a probabilistic extension to dagger , which uses the distribution over actions provided by the novice policy , for a given observation . our method , which we call dropoutdagger , uses dropout to train the novice as a bayesian neural network that provides insight to its confidence . using the distribution over the novice 's actions , we estimate a probabilistic measure of safety with respect to the expert action , tuned to balance exploration and exploitation . the utility of this approach is evaluated on the mujoco halfcheetah and in a simple driving experiment , demonstrating improved performance and safety compared to other dagger variants and classic imitation learning .", "topics": ["test set", "bayesian network"]}
{"title": "exploring nearest neighbor approaches for image captioning", "abstract": "we explore a variety of nearest neighbor baseline approaches for image captioning . these approaches find a set of nearest neighbor images in the training set from which a caption may be borrowed for the query image . we select a caption for the query image by finding the caption that best represents the `` consensus '' of the set of candidate captions gathered from the nearest neighbor images . when measured by automatic evaluation metrics on the ms coco caption evaluation server , these approaches perform as well as many recent approaches that generate novel captions . however , human studies show that a method that generates novel captions is still preferred over the nearest neighbor approach .", "topics": ["baseline ( configuration management )"]}
{"title": "generating motion patterns using evolutionary computation in digital soccer", "abstract": "dribbling an opponent player in digital soccer environment is an important practical problem in motion planning . it has special complexities which can be generalized to most important problems in other similar multi agent systems . in this paper , we propose a hybrid computational geometry and evolutionary computation approach for generating motion trajectories to avoid a mobile obstacle . in this case an opponent agent is not only an obstacle but also one who tries to harden dribbling procedure . one characteristic of this approach is reducing process cost of online stage by transferring it to offline stage which causes increment in agents ' performance . this approach breaks the problem into two offline and online stages . during offline stage the goal is to find desired trajectory using evolutionary computation and saving it as a trajectory plan . a trajectory plan consists of nodes which approximate information of each trajectory plan . in online stage , a linear interpolation along with delaunay triangulation in xy-plan is applied to trajectory plan to retrieve desired action .", "topics": ["computation"]}
{"title": "exploring the syntactic abilities of rnns with multi-task learning", "abstract": "recent work has explored the syntactic abilities of rnns using the subject-verb agreement task , which diagnoses sensitivity to sentence structure . rnns performed this task well in common cases , but faltered in complex sentences ( linzen et al . , 2016 ) . we test whether these errors are due to inherent limitations of the architecture or to the relatively indirect supervision provided by most agreement dependencies in a corpus . we trained a single rnn to perform both the agreement task and an additional task , either ccg supertagging or language modeling . multi-task training led to significantly lower error rates , in particular on complex sentences , suggesting that rnns have the ability to evolve more sophisticated syntactic representations than shown before . we also show that easily available agreement training data can improve performance on other syntactic tasks , in particular when only a limited amount of training data is available for those tasks . the multi-task paradigm can also be leveraged to inject grammatical knowledge into language models .", "topics": ["test set", "text corpus"]}
{"title": "fixing an error in caponnetto and de vito ( 2007 )", "abstract": "the seminal paper of caponnetto and de vito ( 2007 ) provides minimax-optimal rates for kernel ridge regression in a very general setting . its proof , however , contains an error in its bound on the effective dimensionality . in this note , we explain the mistake , provide a correct bound , and show that the main theorem remains true .", "topics": ["matrix regularization"]}
{"title": "interactive image segmentation from a feedback control perspective", "abstract": "image segmentation is a fundamental problem in computational vision and medical imaging . designing a generic , automated method that works for various objects and imaging modalities is a formidable task . instead of proposing a new specific segmentation algorithm , we present a general design principle on how to integrate user interactions from the perspective of feedback control theory . impulsive control and lyapunov stability analysis are employed to design and analyze an interactive segmentation system . then stabilization conditions are derived to guide algorithm design . finally , the effectiveness and robustness of proposed method are demonstrated .", "topics": ["image segmentation", "computer vision"]}
{"title": "fast multiplier methods to optimize non-exhaustive , overlapping clustering", "abstract": "clustering is one of the most fundamental and important tasks in data mining . traditional clustering algorithms , such as k-means , assign every data point to exactly one cluster . however , in real-world datasets , the clusters may overlap with each other . furthermore , often , there are outliers that should not belong to any cluster . we recently proposed the neo-k-means ( non-exhaustive , overlapping k-means ) objective as a way to address both issues in an integrated fashion . optimizing this discrete objective is np-hard , and even though there is a convex relaxation of the objective , straightforward convex optimization approaches are too expensive for large datasets . a practical alternative is to use a low-rank factorization of the solution matrix in the convex formulation . the resulting optimization problem is non-convex , and we can locally optimize the objective function using an augmented lagrangian method . in this paper , we consider two fast multiplier methods to accelerate the convergence of an augmented lagrangian scheme : a proximal method of multipliers and an alternating direction method of multipliers ( admm ) . for the proximal augmented lagrangian or proximal method of multipliers , we show a convergence result for the non-convex case with bound-constrained subproblems . these methods are up to 13 times faster -- -with no change in quality -- -compared with a standard augmented lagrangian method on problems with over 10,000 variables and bring runtimes down from over an hour to around 5 minutes .", "topics": ["optimization problem", "cluster analysis"]}
{"title": "ensemble methods for multi-label classification", "abstract": "ensemble methods have been shown to be an effective tool for solving multi-label classification tasks . in the random k-labelsets ( rakel ) algorithm , each member of the ensemble is associated with a small randomly-selected subset of k labels . then , a single label classifier is trained according to each combination of elements in the subset . in this paper we adopt a similar approach , however , instead of randomly choosing subsets , we select the minimum required subsets of k labels that cover all labels and meet additional constraints such as coverage of inter-label correlations . construction of the cover is achieved by formulating the subset selection as a minimum set covering problem ( scp ) and solving it by using approximation algorithms . every cover needs only to be prepared once by offline algorithms . once prepared , a cover may be applied to the classification of any given multi-label dataset whose properties conform with those of the cover . the contribution of this paper is two-fold . first , we introduce scp as a general framework for constructing label covers while allowing the user to incorporate cover construction constraints . we demonstrate the effectiveness of this framework by proposing two construction constraints whose enforcement produces covers that improve the prediction performance of random selection . second , we provide theoretical bounds that quantify the probabilities of random selection to produce covers that meet the proposed construction criteria . the experimental results indicate that the proposed methods improve multi-label classification accuracy and stability compared with the rakel algorithm and to other state-of-the-art algorithms .", "topics": ["approximation algorithm", "statistical classification"]}
{"title": "using deep convolutional networks for gesture recognition in american sign language", "abstract": "in the realm of multimodal communication , sign language is , and continues to be , one of the most understudied areas . in line with recent advances in the field of deep learning , there are far reaching implications and applications that neural networks can have for sign language interpretation . in this paper , we present a method for using deep convolutional networks to classify images of both the the letters and digits in american sign language .", "topics": ["feature extraction", "computer vision"]}
{"title": "suppressing background radiation using poisson principal component analysis", "abstract": "performance of nuclear threat detection systems based on gamma-ray spectrometry often strongly depends on the ability to identify the part of measured signal that can be attributed to background radiation . we have successfully applied a method based on principal component analysis ( pca ) to obtain a compact null-space model of background spectra using pca projection residuals to derive a source detection score . we have shown the method 's utility in a threat detection system using mobile spectrometers in urban scenes ( tandon et al 2012 ) . while it is commonly assumed that measured photon counts follow a poisson process , standard pca makes a gaussian assumption about the data distribution , which may be a poor approximation when photon counts are low . this paper studies whether and in what conditions pca with a poisson-based loss function ( poisson pca ) can outperform standard gaussian pca in modeling background radiation to enable more sensitive and specific nuclear threat detection .", "topics": ["loss function"]}
{"title": "dense bag-of-temporal-sift-words for time series classification", "abstract": "time series classification is an application of particular interest with the increase of data to monitor . classical techniques for time series classification rely on point-to-point distances . recently , bag-of-words approaches have been used in this context . words are quantized versions of simple features extracted from sliding windows . the sift framework has proved efficient for image classification . in this paper , we design a time series classification scheme that builds on the sift framework adapted to time series to feed a bag-of-words . we then refine our method by studying the impact of normalized bag-of-words , as well as densely extract point descriptors . proposed adjustements achieve better performance . the evaluation shows that our method outperforms classical techniques in terms of classification .", "topics": ["baseline ( configuration management )", "statistical classification"]}
{"title": "neural machine translation for low-resource languages", "abstract": "neural machine translation ( nmt ) approaches have improved the state of the art in many machine translation settings over the last couple of years , but they require large amounts of training data to produce sensible output . we demonstrate that nmt can be used for low-resource languages as well , by introducing more local dependencies and using word alignments to learn sentence reordering during translation . in addition to our novel model , we also present an empirical evaluation of low-resource phrase-based statistical machine translation ( smt ) and nmt to investigate the lower limits of the respective technologies . we find that while smt remains the best option for low-resource settings , our method can produce acceptable translations with only 70000 tokens of training data , a level where the baseline nmt system fails completely .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "on unifying deep generative models", "abstract": "deep generative models have achieved impressive success in recent years . generative adversarial networks ( gans ) and variational autoencoders ( vaes ) , as powerful frameworks for deep generative model learning , have largely been considered as two distinct paradigms and received extensive independent study respectively . this paper establishes formal connections between deep generative modeling approaches through a new formulation of gans and vaes . we show that gans and vaes are essentially minimizing kl divergences of respective posterior and inference distributions with opposite directions , extending the two learning phases of classic wake-sleep algorithm , respectively . the unified view provides a powerful tool to analyze a diverse set of existing model variants , and enables to exchange ideas across research lines in a principled way . for example , we transfer the importance weighting method in vae literatures for improved gan learning , and enhance vaes with an adversarial mechanism for leveraging generated samples . quantitative experiments show generality and effectiveness of the imported extensions .", "topics": ["generative model", "calculus of variations"]}
{"title": "predicting performance during tutoring with models of recent performance", "abstract": "in educational technology and learning sciences , there are multiple uses for a predictive model of whether a student will perform a task correctly or not . for example , an intelligent tutoring system may use such a model to estimate whether or not a student has mastered a skill . we analyze the significance of data recency in making such predictions , i.e . , asking whether relatively more recent observations of a student 's performance matter more than relatively older observations . we develop a new recent-performance factors analysis model that takes data recency into account . the new model significantly improves predictive accuracy over both existing logistic-regression performance models and over novel baseline models in evaluations on real-world and synthetic datasets . as a secondary contribution , we demonstrate how the widely used cross-validation with 0-1 loss is inferior to aic and to cross-validation with l1 prediction error loss as a measure of model performance .", "topics": ["baseline ( configuration management )", "synthetic data"]}
{"title": "synthesis of near-regular natural textures", "abstract": "texture synthesis is widely used in the field of computer graphics , vision , and image processing . in the present paper , a texture synthesis algorithm is proposed for near-regular natural textures with the help of a representative periodic pattern extracted from the input textures using distance matching function . local texture statistics is then analyzed against global texture statistics for non-overlapping windows of size same as periodic pattern size and a representative periodic pattern is extracted from the image and used for texture synthesis , while preserving the global regularity and visual appearance . validation of the algorithm based on experiments with synthetic textures whose periodic pattern sizes are known and containing camouflages / defects proves the strength of the algorithm for texture synthesis and its application in detection of camouflages / defects in textures .", "topics": ["image processing", "synthetic data"]}
{"title": "simultaneous detection and quantification of retinal fluid with deep learning", "abstract": "we propose a new deep learning approach for automatic detection and segmentation of fluid within retinal oct images . the proposed framework utilizes both resnet and encoder-decoder neural network architectures . when training the network , we apply a novel data augmentation method called myopic warping together with standard rotation-based augmentation to increase the training set size to 45 times the original amount . finally , the network output is post-processed with an energy minimization algorithm ( graph cut ) along with a few other knowledge guided morphological operations to finalize the segmentation process . based on oct imaging data and its ground truth from the retouch challenge , the proposed system achieves dice indices of 0.522 , 0.682 , and 0.612 , and average absolute volume differences of 0.285 , 0.115 , and 0.156 mm $ ^3 $ for intaretinal fluid , subretinal fluid , and pigment epithelial detachment respectively .", "topics": ["ground truth", "encoder"]}
{"title": "join-graph propagation algorithms", "abstract": "the paper investigates parameterized approximate message-passing schemes that are based on bounded inference and are inspired by pearl 's belief propagation algorithm ( bp ) . we start with the bounded inference mini-clustering algorithm and then move to the iterative scheme called iterative join-graph propagation ( ijgp ) , that combines both iteration and bounded inference . algorithm ijgp belongs to the class of generalized belief propagation algorithms , a framework that allowed connections with approximate algorithms from statistical physics and is shown empirically to surpass the performance of mini-clustering and belief propagation , as well as a number of other state-of-the-art algorithms on several classes of networks . we also provide insight into the accuracy of iterative bp and ijgp by relating these algorithms to well known classes of constraint propagation schemes .", "topics": ["approximation algorithm", "cluster analysis"]}
{"title": "acyclicity notions for existential rules and their application to query answering in ontologies", "abstract": "answering conjunctive queries ( cqs ) over a set of facts extended with existential rules is a prominent problem in knowledge representation and databases . this problem can be solved using the chase algorithm , which extends the given set of facts with fresh facts in order to satisfy the rules . if the chase terminates , then cqs can be evaluated directly in the resulting set of facts . the chase , however , does not terminate necessarily , and checking whether the chase terminates on a given set of rules and facts is undecidable . numerous acyclicity notions were proposed as sufficient conditions for chase termination . in this paper , we present two new acyclicity notions called model-faithful acyclicity ( mfa ) and model-summarising acyclicity ( msa ) . furthermore , we investigate the landscape of the known acyclicity notions and establish a complete taxonomy of all notions known to us . finally , we show that mfa and msa generalise most of these notions . existential rules are closely related to the horn fragments of the owl 2 ontology language ; furthermore , several prominent owl 2 reasoners implement cq answering by using the chase to materialise all relevant facts . in order to avoid termination problems , many of these systems handle only the owl 2 rl profile of owl 2 ; furthermore , some systems go beyond owl 2 rl , but without any termination guarantees . in this paper we also investigate whether various acyclicity notions can provide a principled and practical solution to these problems . on the theoretical side , we show that query answering for acyclic ontologies is of lower complexity than for general ontologies . on the practical side , we show that many of the commonly used owl 2 ontologies are msa , and that the number of facts obtained by materialisation is not too large . our results thus suggest that principled development of materialisation-based owl 2 reasoners is practically feasible .", "topics": ["database"]}
{"title": "the benefit of sex in noisy evolutionary search", "abstract": "the benefit of sexual recombination is one of the most fundamental questions both in population genetics and evolutionary computation . it is widely believed that recombination helps solving difficult optimization problems . we present the first result , which rigorously proves that it is beneficial to use sexual recombination in an uncertain environment with a noisy fitness function . for this , we model sexual recombination with a simple estimation of distribution algorithm called the compact genetic algorithm ( cga ) , which we compare with the classical $ \\mu+1 $ ea . for a simple noisy fitness function with additive gaussian posterior noise $ \\mathcal { n } ( 0 , \\sigma^2 ) $ , we prove that the mutation-only $ \\mu+1 $ ea typically can not handle noise in polynomial time for $ \\sigma^2 $ large enough while the cga runs in polynomial time as long as the population size is not too small . this shows that in this uncertain environment sexual recombination is provably beneficial . we observe the same behavior in a small empirical study .", "topics": ["time complexity", "computation"]}
{"title": "a mathematical theory of deep convolutional neural networks for feature extraction", "abstract": "deep convolutional neural networks have led to breakthrough results in numerous practical machine learning tasks such as classification of images in the imagenet data set , control-policy-learning to play atari games or the board game go , and image captioning . many of these applications first perform feature extraction and then feed the results thereof into a trainable classifier . the mathematical analysis of deep convolutional neural networks for feature extraction was initiated by mallat , 2012 . specifically , mallat considered so-called scattering networks based on a wavelet transform followed by the modulus non-linearity in each network layer , and proved translation invariance ( asymptotically in the wavelet scale parameter ) and deformation stability of the corresponding feature extractor . this paper complements mallat 's results by developing a theory that encompasses general convolutional transforms , or in more technical parlance , general semi-discrete frames ( including weyl-heisenberg filters , curvelets , shearlets , ridgelets , wavelets , and learned filters ) , general lipschitz-continuous non-linearities ( e.g . , rectified linear units , shifted logistic sigmoids , hyperbolic tangents , and modulus functions ) , and general lipschitz-continuous pooling operators emulating , e.g . , sub-sampling and averaging . in addition , all of these elements can be different in different network layers . for the resulting feature extractor we prove a translation invariance result of vertical nature in the sense of the features becoming progressively more translation-invariant with increasing network depth , and we establish deformation sensitivity bounds that apply to signal classes such as , e.g . , band-limited functions , cartoon functions , and lipschitz functions .", "topics": ["sampling ( signal processing )", "statistical classification"]}
{"title": "tandemnet : distilling knowledge from medical images using diagnostic reports as optional semantic references", "abstract": "in this paper , we introduce the semantic knowledge of medical images from their diagnostic reports to provide an inspirational network training and an interpretable prediction mechanism with our proposed novel multimodal neural network , namely tandemnet . inside tandemnet , a language model is used to represent report text , which cooperates with the image model in a tandem scheme . we propose a novel dual-attention model that facilitates high-level interactions between visual and semantic information and effectively distills useful features for prediction . in the testing stage , tandemnet can make accurate image prediction with an optional report text input . it also interprets its prediction by producing attention on the image and text informative feature pieces , and further generating diagnostic report paragraphs . based on a pathological bladder cancer images and their diagnostic reports ( bcidr ) dataset , sufficient experiments demonstrate that our method effectively learns and integrates knowledge from multimodalities and obtains significantly improved performance than comparing baselines .", "topics": ["high- and low-level", "interaction"]}
